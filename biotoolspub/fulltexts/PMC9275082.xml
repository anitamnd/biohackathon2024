<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9275082</article-id>
    <article-id pub-id-type="publisher-id">4819</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-022-04819-3</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MTAGCN: predicting miRNA-target associations in <italic>Camellia sinensis</italic> var. assamica through graph convolution neural network</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Feng</surname>
          <given-names>Haisong</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Xiang</surname>
          <given-names>Ying</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Xiaosong</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Xue</surname>
          <given-names>Wei</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Yue</surname>
          <given-names>Zhenyu</given-names>
        </name>
        <address>
          <email>zhenyuyue@ahau.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="GRID">grid.411389.6</institution-id><institution-id institution-id-type="ISNI">0000 0004 1760 4804</institution-id><institution>School of Information and Computer, Anhui Provincial Engineering Laboratory for Beidou Precision Agriculture Information, </institution><institution>Anhui Agricultural University, </institution></institution-wrap>Hefei, 230036 Anhui China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>11</day>
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>11</day>
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>23</volume>
    <elocation-id>271</elocation-id>
    <history>
      <date date-type="received">
        <day>18</day>
        <month>11</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>1</day>
        <month>7</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">MircoRNAs (miRNAs) play a central role in diverse biological processes of <italic>Camellia sinensis</italic> var.assamica (CSA) through their associations with target mRNAs, including CSA growth, development and stress response. However, although the experiment methods of CSA miRNA-target identifications are costly and time-consuming, few computational methods have been developed to tackle the CSA miRNA-target association prediction problem.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In this paper, we constructed a heterogeneous network for CSA miRNA and targets by integrating rich biological information, including a miRNA similarity network, a target similarity network, and a miRNA-target association network. We then proposed a deep learning framework of graph convolution networks with layer attention mechanism, named MTAGCN. In particular, MTAGCN uses the attention mechanism to combine embeddings of multiple graph convolution layers, employing the integrated embedding to score the unobserved CSA miRNA-target associations.</p>
      </sec>
      <sec>
        <title>Discussion</title>
        <p id="Par3">Comprehensive experiment results on two tasks (balanced task and unbalanced task) demonstrated that our proposed model achieved better performance than the classic machine learning and existing graph convolution network-based methods. The analysis of these results could offer valuable information for understanding complex CSA miRNA-target association mechanisms and would make a contribution to precision plant breeding.</p>
      </sec>
      <sec>
        <title>Supplementary Information</title>
        <p>The online version contains supplementary material available at 10.1186/s12859-022-04819-3.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>CSA miRNA-target association prediction</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Graph convolution network</kwd>
      <kwd>Layer attention mechanism</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>National Natural Science Foundation of China</institution>
        </funding-source>
        <award-id>62102004</award-id>
        <principal-award-recipient>
          <name>
            <surname>Yue</surname>
            <given-names>Zhenyu</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Natural Science Young Foundation of Anhui</institution>
        </funding-source>
        <award-id>2008085QF293</award-id>
        <principal-award-recipient>
          <name>
            <surname>Yue</surname>
            <given-names>Zhenyu</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Natural Science Young Foundation of Anhui Agricultural University</institution>
        </funding-source>
        <award-id>2019zd12</award-id>
        <principal-award-recipient>
          <name>
            <surname>Yue</surname>
            <given-names>Zhenyu</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Introduction and Stabilization of Talent Project of Anhui Agricultural University</institution>
        </funding-source>
        <award-id>yj2019-32</award-id>
        <principal-award-recipient>
          <name>
            <surname>Yue</surname>
            <given-names>Zhenyu</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par4">Tea, produced from the dried leaves of tea plant, <italic>Camellia sinensis</italic>, is one of the most widely consumed drink in the world, which has large economic, medicinal and cultural significance [<xref ref-type="bibr" rid="CR1">1</xref>]. Many studies demonstrated that the characteristic secondary metabolites in tea leaves such as polyphenols, caffeine, theanine, vitamins, have numerous health and medical benefits for humans [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>]. Plant microRNAs (miRNAs) are highly conserved and play an important role in gene expression regulation by targeting specific mRNA [<xref ref-type="bibr" rid="CR4">4</xref>]. Furthermore, it is proven that miRNAs are involved in the development procedures, stress responses or biosynthesis of the secondary metabolites in <italic>Camellia sinensis</italic> var.assamica (CSA) [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR6">6</xref>]. Thus, the identification of CSA miRNAs can not only improve the understanding of miRNA targeted gene regulation but also the evolution of miRNAs.</p>
    <p id="Par5">Although experiment methods to identifying CSA miRNA-target have high accuracy [<xref ref-type="bibr" rid="CR7">7</xref>], they may suffer from time-consuming, laborious and expensive. As a result, it is necessary to develop computational methods for predicting miRNA-target association. Machine learning or deep learning-based methods have been generally adopted to solve various association pair prediction problems in biology. For example, many classification algorithms regard the associations as samples firstly, and the feature vectors of the edges are used to represent these samples. Then the classifiers are trained to recognize the real-existing associations in the graph [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR9">9</xref>]. Nevertheless, the above machine learning methods are heavily dependent on the negative data sampling and the feature extraction. Therefore, more advanced machine learning methods, such as label propagation [<xref ref-type="bibr" rid="CR10">10</xref>], regularized least squares [<xref ref-type="bibr" rid="CR11">11</xref>], semi-supervised graph cut [<xref ref-type="bibr" rid="CR12">12</xref>], sparse subspace learning [<xref ref-type="bibr" rid="CR13">13</xref>], matrix factorization [<xref ref-type="bibr" rid="CR14">14</xref>] and matrix completion [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>], are introduced to solve these kinds of problems. Matrix completion and matrix factorization methods are popular in community due to their flexibility in aggregating apriori information [<xref ref-type="bibr" rid="CR17">17</xref>]. However, deploying them on high-dimensionality data is challenging because of the high computational complexity of matrix operations.</p>
    <p id="Par6">Deep learning methods have recently shown excellent performance in many fields, such as perception, planning, localization, and control [<xref ref-type="bibr" rid="CR18">18</xref>]. The excellent capabilities of deep learning methods for learning representations from the complicated data make it extremely suitable for predicting association pairs in biology. Graph neural network (GNN) uses different node neighborhood aggregating schemes, representing a significant progress in directly processing network/graph structure data [<xref ref-type="bibr" rid="CR19">19</xref>]. Each node feature can be updated by aggregating features of its neighboring nodes during the layer propagation and the node embedding will naturally capture the graph structure. GNNs have been extensively applied in multifarious problems, achieving superior performance in biological tasks, such as disease-gene association identification [<xref ref-type="bibr" rid="CR20">20</xref>, <xref ref-type="bibr" rid="CR21">21</xref>], drug-drug interaction predictions [<xref ref-type="bibr" rid="CR22">22</xref>, <xref ref-type="bibr" rid="CR23">23</xref>], miRNA-disease association predictions [<xref ref-type="bibr" rid="CR24">24</xref>, <xref ref-type="bibr" rid="CR25">25</xref>], etc. As an extension of convolutional neural network for processing graph data, graph convolution network (GCN) [<xref ref-type="bibr" rid="CR26">26</xref>], an important branch of GNN, has achieved excellent performance in different tasks. It is an end-to-end architecture and captures the graph structural information through messages passed between graph nodes, thereby retains explainability. In recent years, it shows superior performances in biological network analysis [<xref ref-type="bibr" rid="CR27">27</xref>, <xref ref-type="bibr" rid="CR28">28</xref>].</p>
    <p id="Par7">In this paper, we developed a graph convolutional network model (MTAGCN) for predicting CSA miRNA-target associations. At first, we constructed heterogeneous networks by exploiting the CSA miRNA-target associations, miRNA-miRNA similarity matrix and target-target similarity matrix. Next, the graph convolution operation was conducted on the heterogeneous network to learn CSA miRNAs and targets embeddings. Considering that the embeddings from different convolution layers represent the proximity of nodes in the network at different levels [<xref ref-type="bibr" rid="CR29">29</xref>], we introduced the attention mechanism [<xref ref-type="bibr" rid="CR30">30</xref>] to combine useful neighborhoods representation adaptively and dynamically. Finally, we defined a score function which based on the integrated embedding, giving predictive scores for unobserved miRNA-target associations. Comprehensive experiment results on two tasks, i.e. the balanced task and the unbalanced task, showed that our proposed MTAGCN model had a better performance than five machine learning and three existing state-of-the-art methods.</p>
    <p id="Par8">In summary, our main contributions are as follows:<list list-type="bullet"><list-item><p id="Par9">We constructed the heterogeneous network to effectively integrate rich biological information, including CSA miRNA-target associations, CSA miRNA information and CSA target information.</p></list-item><list-item><p id="Par10">We proposed MTAGCN, a novel GCN-based method for predicting CSA miRNA-target associations. To our knowledge, this is the first work to adapt deep learning method for CSA miRNA-target association prediction.</p></list-item><list-item><p id="Par11">We designed the attention mechanism to integrate the embeddings information from multiple convolutional layers, leading to more useful representation from miRNAs and targets.</p></list-item></list></p>
  </sec>
  <sec id="Sec2">
    <title>Methods and materials</title>
    <sec id="Sec3">
      <title>Data</title>
      <p id="Par12">The data we used in this study was collected from the 2020 version of the CSA miRNA-target associations released in the work of Suo et al. [<xref ref-type="bibr" rid="CR7">7</xref>]. This dataset contains 5264 relationships between CSA miRNAs and targets which include 356 miRNAs and 4041 targets. For the lack of some miRNA sequences and target information, we removed the relationships between miRNAs and targets, including 66 miRNAs and 1166 targets. Therefore, the resulting dataset we obtained contains 3745 miRNA-target pairs, including 290 different types of CSA miRNAs and 2876 targets. Then, we acquired the CSA target gene locations from <ext-link ext-link-type="uri" xlink:href="http://teacon.wchoda.com">http://teacon.wchoda.com</ext-link>, a database of gene co-expression network for CSA plant [<xref ref-type="bibr" rid="CR31">31</xref>]. According to the CSA target gene locations, we extracted target sequences from CSA whole genome data in the Tea Plant Information Archive [<xref ref-type="bibr" rid="CR32">32</xref>]. The details are shown in Table <xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Summary of the statistics of the miRNA-target associations</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Data</th><th align="left">MiRNA</th><th align="left">Target</th><th align="left">Association</th></tr></thead><tbody><tr><td align="left">Original</td><td align="left">356</td><td align="left">4041</td><td align="left">5264</td></tr><tr><td align="left">Used</td><td align="left">290</td><td align="left">2876</td><td align="left">3745</td></tr></tbody></table><table-wrap-foot><p>Original, the original data from the work of <italic>Suo </italic>et al., Used, the data after filtering</p></table-wrap-foot></table-wrap></p>
      <p id="Par13">To perform the five-fold cross validation, we developed a balanced and an unbalanced dataset, respectively, to evaluate the CSA miRNA-target prediction models. In the training dataset, four-fifths of the positive samples and all the negative samples are used. As for test set in the unbalanced task, we use the remaining one-fifth of the positive samples (749 positive samples) and draw 20 times number of positive samples as negative samples (14,980 negative samples). Then the class of negative data vastly outnumbers that of positive data, causing a class imbalance problem (Table <xref rid="Tab2" ref-type="table">2</xref>). As for test set in the balanced task, we used the same number of negative samples as positive samples (Table <xref rid="Tab2" ref-type="table">2</xref>). In addition, in order to acquire CSA miRNA similarities and target similarities, Kmer [<xref ref-type="bibr" rid="CR33">33</xref>], an algorithm based on nucleic acid composition, is used to transform the CSA targets sequences and miRNAs sequences into feature vectors.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Summary of the samples on the balanced and unbalanced data</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Data</th><th align="left" colspan="2">Training</th><th align="left" colspan="2">Test</th></tr><tr><th align="left">Positive</th><th align="left">Negative</th><th align="left">Positive</th><th align="left">Negative</th></tr></thead><tbody><tr><td align="left">Balanced</td><td align="left">2996</td><td align="left">All</td><td align="left">749</td><td align="left">749</td></tr><tr><td align="left">Unbalanced</td><td align="left">2996</td><td align="left">All</td><td align="left">749</td><td align="left">14,980</td></tr></tbody></table><table-wrap-foot><p>All, all miRNA-target pairs except for the positive samples</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec4">
      <title>Construction of heterogeneous network</title>
      <sec id="Sec5">
        <title>Construction of similarity network</title>
        <p id="Par14">As mentioned above, we used Kmer to obtain CSA miRNA and target features. For one miRNA or target binary feature vector, each element means whether the feature descriptor is present or absent. In this work, we adopted the Jaccard index to calculate the miRNA-miRNA and target-target similarities. Jaccard index [<xref ref-type="bibr" rid="CR27">27</xref>] is a prevailing measure for calculating similarity based on these features. Thus, we further constructed miRNA similarity matrix and target similarity matrix. The Jaccard index measure between two vectors <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{i}$$\end{document}</tex-math><mml:math id="M2"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{j}$$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq2.gif"/></alternatives></inline-formula> is defined as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{ij}^{r} = \frac{{\left| {x_{i} \cap x_{j} } \right|}}{{\left| {x_{i} \cup x_{j} } \right|}}$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∩</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∪</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2022_4819_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>
where <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left| {x_{i} \cap x_{j} } \right|$$\end{document}</tex-math><mml:math id="M8"><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∩</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq3.gif"/></alternatives></inline-formula> denotes the number of features where both elements in <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{i}$$\end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq4.gif"/></alternatives></inline-formula> and the related ones of <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{j}$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq5.gif"/></alternatives></inline-formula> equal to 1, and <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left| {x_{i} \cup x_{j} } \right|$$\end{document}</tex-math><mml:math id="M14"><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∪</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq6.gif"/></alternatives></inline-formula> denotes the numbers of features where either the elements of <inline-formula id="IEq7"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{i}$$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq7.gif"/></alternatives></inline-formula> or the related ones of <inline-formula id="IEq8"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{j}$$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq8.gif"/></alternatives></inline-formula> equal to 1.</p>
        <p id="Par15">Herein, we also considered other similarity calculation measures to construct similarity network, including cosine similarity, Gaussian kernel-based similarity, and Pearson similarity. These measures are widely used in constructing similarity network and have achieved great performance in many biological prediction tasks [<xref ref-type="bibr" rid="CR22">22</xref>, <xref ref-type="bibr" rid="CR34">34</xref>, <xref ref-type="bibr" rid="CR35">35</xref>].</p>
      </sec>
      <sec id="Sec6">
        <title>Heterogeneous network for CSA miRNAs and targets</title>
        <p id="Par16">The heterogeneous network is constructed based on miRNA-target associations, miRNA-miRNA similarity and target-target similarity.</p>
        <p id="Par17">The miRNA-target associations are denoted as an adjacent matrix <inline-formula id="IEq9"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${ }A \in \left\{ {0,1} \right\}^{M*N}$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mrow/><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mfenced close="}" open="{"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mrow><mml:mi>M</mml:mi><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq9.gif"/></alternatives></inline-formula>, <italic>M</italic> and <italic>N</italic> represent the number of miRNAs and targets, respectively. If a CSA miRNA <inline-formula id="IEq10"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r_{i}$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq10.gif"/></alternatives></inline-formula> is associated with a target <inline-formula id="IEq11"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t_{j}$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq11.gif"/></alternatives></inline-formula>, <inline-formula id="IEq12"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A_{ij}$$\end{document}</tex-math><mml:math id="M26"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq12.gif"/></alternatives></inline-formula> = 1; otherwise <inline-formula id="IEq13"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${ }A_{ij}$$\end{document}</tex-math><mml:math id="M28"><mml:mrow><mml:mrow/><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq13.gif"/></alternatives></inline-formula> = 0. The miRNA-miRNA similarity network is derived from the CSA miRNA similarity matrix <inline-formula id="IEq14"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S^{m}$$\end{document}</tex-math><mml:math id="M30"><mml:msup><mml:mi>S</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq14.gif"/></alternatives></inline-formula> with <inline-formula id="IEq15"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{ij}^{m}$$\end{document}</tex-math><mml:math id="M32"><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq15.gif"/></alternatives></inline-formula> as its (<italic>i,j</italic>)th element. And the target-target similarity network is derived from the CSA target similarity network <inline-formula id="IEq16"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S^{n}$$\end{document}</tex-math><mml:math id="M34"><mml:msup><mml:mi>S</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq16.gif"/></alternatives></inline-formula> with <inline-formula id="IEq17"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{ij}^{n}$$\end{document}</tex-math><mml:math id="M36"><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq17.gif"/></alternatives></inline-formula> as its (<italic>i, j</italic>)th element. Furthermore, we adapt <inline-formula id="IEq18"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sim S^{m} = D_{m}^{{ - \frac{1}{2}}} S^{m} D_{m}^{{ - \frac{1}{2}}}$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mo>∼</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msubsup><mml:msup><mml:mi>S</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq18.gif"/></alternatives></inline-formula> and <inline-formula id="IEq19"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sim S^{n} = D_{n}^{{ - \frac{1}{2}}} S^{n} D_{n}^{{ - \frac{1}{2}}}$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:mo>∼</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msubsup><mml:msup><mml:mi>S</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq19.gif"/></alternatives></inline-formula> to normalize the similarity matrices, where <inline-formula id="IEq20"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D_{m} = {\text{diag}}\left( {\mathop \sum \limits_{j} S_{ij}^{m} } \right)$$\end{document}</tex-math><mml:math id="M42"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>diag</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq20.gif"/></alternatives></inline-formula> and <inline-formula id="IEq21"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D_{n} = {\text{diag}}\left( {\mathop \sum \limits_{j} S_{ij}^{n} } \right)$$\end{document}</tex-math><mml:math id="M44"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>diag</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq21.gif"/></alternatives></inline-formula>. Finally, the heterogeneous network defined by the adjacency matrix comes to be<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A_{H} = \left[ {\begin{array}{*{20}l} {\sim S^{m} } \hfill &amp; A \hfill \\ {A^{T} } \hfill &amp; {\sim S^{n} } \hfill \\ \end{array} } \right]$$\end{document}</tex-math><mml:math id="M46" display="block"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:mtd><mml:mtd><mml:mi>A</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>∼</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_4819_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
      </sec>
    </sec>
    <sec id="Sec7">
      <title>Graph convolution</title>
      <p id="Par18">Regarding known associations between miRNAs and targets as a bipartite graph, the prediction problem in this paper can be defined as a semi-supervised link prediction task on such a graph.</p>
      <p id="Par19">We assume that a bipartite graph <italic>G</italic> = (<italic>ν, ε</italic>) with <italic>ν</italic> = <bold>(</bold><inline-formula id="IEq22"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\nu_{m, } \nu_{t} )$$\end{document}</tex-math><mml:math id="M48"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>ν</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq22.gif"/></alternatives></inline-formula> including <inline-formula id="IEq23"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n_{m }$$\end{document}</tex-math><mml:math id="M50"><mml:msub><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq23.gif"/></alternatives></inline-formula> miRNA nodes and <inline-formula id="IEq24"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n_{t}$$\end{document}</tex-math><mml:math id="M52"><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq24.gif"/></alternatives></inline-formula> target nodes, which have numerical features <inline-formula id="IEq25"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_{m } = \left[ {x_{m}^{1} ,x_{m }^{2} , \ldots ,x_{m}^{{n_{m} }} } \right]^{T} \in R^{{n_{m} *M}}$$\end{document}</tex-math><mml:math id="M54"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mfenced close="]" open="["><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:msubsup></mml:mrow></mml:mfenced><mml:mi>T</mml:mi></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq25.gif"/></alternatives></inline-formula> and <inline-formula id="IEq26"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_{t } = \left[ {x_{t}^{1} ,x_{t }^{2} , \ldots ,x_{t}^{{n_{t} }} } \right]^{T} \in R^{{n_{t} *N}}$$\end{document}</tex-math><mml:math id="M56"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mfenced close="]" open="["><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msubsup></mml:mrow></mml:mfenced><mml:mi>T</mml:mi></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq26.gif"/></alternatives></inline-formula>, respectively. Supposing that partial links (denoted as <italic>ε</italic> in <italic>G</italic>) are given labels, our goal is to predict whether there are any potential links between miRNA and target that have not been determined previously. Thus, how best to effectively utilize both graph topology and the attribute information of the nodes is a problem we need to address.</p>
      <p id="Par20">There have recently been some attempts to use deep learning techniques to graph-based data analyses. A graph convolutional network (GCN) is proposed in Kipf et al. [<xref ref-type="bibr" rid="CR26">26</xref>]. Graph convolution is defined on graph as the multiplication of an input signal with a filter <inline-formula id="IEq27"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g_{\theta }$$\end{document}</tex-math><mml:math id="M58"><mml:msub><mml:mi>g</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq27.gif"/></alternatives></inline-formula> in the Fourier domain [<xref ref-type="bibr" rid="CR19">19</xref>]. Given an adjacent matrix <italic>A</italic> with its Laplacian <italic>L</italic>: <italic>D-A</italic>, and attributes of each node on graph (denoted as <italic>s</italic>), spectral graph convolution tries to decompose <bold>s</bold> on the spectral components. We assume that <italic>L</italic> can be decomposed by <inline-formula id="IEq28"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${ }L = U\Lambda U^{T}$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:mrow/><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mi mathvariant="normal">Λ</mml:mi><mml:msup><mml:mi>U</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq28.gif"/></alternatives></inline-formula>, <italic>U</italic> is eigenvector matrix and <italic>Λ</italic> is the diagonal matrix. Hence, <inline-formula id="IEq29"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g_{\theta } {\text{*s}} = Ug_{\theta } U^{T}$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mtext>*s</mml:mtext><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:msup><mml:mi>U</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq29.gif"/></alternatives></inline-formula> s is a graph Fourier transform of <inline-formula id="IEq30"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$U^{T} s$$\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:msup><mml:mi>U</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>s</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq30.gif"/></alternatives></inline-formula>. Defferrard used a truncated expansion in terms of Chebyshev polynomials [<xref ref-type="bibr" rid="CR36">36</xref>]<inline-formula id="IEq31"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T_{k\left( s \right)}$$\end{document}</tex-math><mml:math id="M66"><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mfenced close=")" open="("><mml:mi>s</mml:mi></mml:mfenced></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq31.gif"/></alternatives></inline-formula> up to <inline-formula id="IEq32"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K^{th}$$\end{document}</tex-math><mml:math id="M68"><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mi mathvariant="italic">th</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq32.gif"/></alternatives></inline-formula> order, approximating the spectral filter in order to avoid the issue of computationally costly eigende-composition of <italic>L</italic><disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g_{\theta } {\text{*s}} \approx { }\mathop \sum \limits_{k = 0}^{K} \theta^{\prime}_{k} T_{k} \left( {L_{N} } \right)s$$\end{document}</tex-math><mml:math id="M70" display="block"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mtext>*s</mml:mtext><mml:mo>≈</mml:mo><mml:mrow/><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mfenced close=")" open="("><mml:msub><mml:mi>L</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mfenced><mml:mi>s</mml:mi></mml:mrow></mml:math><graphic xlink:href="12859_2022_4819_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>
where <inline-formula id="IEq33"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta^{\prime}$$\end{document}</tex-math><mml:math id="M72"><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq33.gif"/></alternatives></inline-formula> is a vector about Chebyshev coefficients and <inline-formula id="IEq34"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T_{k}$$\end{document}</tex-math><mml:math id="M74"><mml:msub><mml:mi>T</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq34.gif"/></alternatives></inline-formula> is the Chebyshev polynomials. A further research simplified this definition by approximating the largest eigenvalue of <italic>L</italic> by Formula (<xref rid="Equ4" ref-type="">4</xref>) [<xref ref-type="bibr" rid="CR26">26</xref>]. The convolution operator is<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g_{\theta } {\text{*s}} = \theta \left( {{\text{I}} + D^{{ - \frac{1}{2 }}} {\text{A}}D^{{ - \frac{1}{2 }}} } \right)s$$\end{document}</tex-math><mml:math id="M76" display="block"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mtext>*s</mml:mtext><mml:mo>=</mml:mo><mml:mi>θ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>+</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mtext>A</mml:mtext><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mi>s</mml:mi></mml:mrow></mml:math><graphic xlink:href="12859_2022_4819_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
    </sec>
    <sec id="Sec8">
      <title>Prediction framework of the proposed MTAGCN</title>
      <p id="Par21">The workflow of our model is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Our proposed MATGCN model consists of three parts, i.e., similarity network integration, encoder construction and decoder construction. We integrate similarity networks by combining rich biological information to construct the heterogeneous network. And the encoder is a GCN model with layer attention mechanism, capturing network structure information using GCN. We design a decoder, a fully connected layer network, to transform features into the original space.<fig id="Fig1"><label>Fig. 1</label><caption><p>The workflow of MTAGCN for CSA miRNA-target association prediction</p></caption><graphic xlink:href="12859_2022_4819_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par22">For graph convolution, we adopted the simplified definition. As mentioned above, the prediction of associations between CSA miRNAs and targets can be considered as a semi-supervised link prediction problem. But current GCN-based approaches tackle node classification problem on homogeneous network and are not applicable to the issue involving prediction of associations. Thus, we extend the current graph convolution idea to solve link prediction problem defined on heterogeneous, bipartite, attributed networks. For this goal, we proposed the GCN-based framework called MTAGCN to solve the novel prediction problem. The Algorithm 1 shows the detailed training steps of the MTAGCN for predicting CSA miRNA-target association.</p>
      <p id="Par23">GCN is a multilayer connected neural network and its propagation rule is defined as follows:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H^{{\left( {l + 1} \right)}} = \sigma \left( {D^{{ - \frac{1}{2 }}} AD^{{ - \frac{1}{2 }}} H^{\left( l \right)} W^{\left( l \right)} } \right) = f\left( {H^{\left( l \right)} ,G} \right)$$\end{document}</tex-math><mml:math id="M78" display="block"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mi>A</mml:mi><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mi>H</mml:mi><mml:mfenced close=")" open="("><mml:mi>l</mml:mi></mml:mfenced></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mfenced close=")" open="("><mml:mi>l</mml:mi></mml:mfenced></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mfenced close=")" open="("><mml:mi>l</mml:mi></mml:mfenced></mml:msup><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_4819_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D = {\text{diag}}\left( {\mathop \sum \limits_{j} G_{ij} } \right)$$\end{document}</tex-math><mml:math id="M80" display="block"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mtext>diag</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_4819_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>
where <italic>σ</italic> is an adjustable activation function, <italic>D</italic> is the diagonal degree matrix, <italic>A</italic> is the adjacency matrix, <inline-formula id="IEq35"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H^{\left( l \right)}$$\end{document}</tex-math><mml:math id="M82"><mml:msup><mml:mi>H</mml:mi><mml:mfenced close=")" open="("><mml:mi>l</mml:mi></mml:mfenced></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq35.gif"/></alternatives></inline-formula> is the nodes embedding in the <italic>l</italic>th layer and <inline-formula id="IEq36"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W^{\left( l \right)}$$\end{document}</tex-math><mml:math id="M84"><mml:msup><mml:mi>W</mml:mi><mml:mfenced close=")" open="("><mml:mi>l</mml:mi></mml:mfenced></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq36.gif"/></alternatives></inline-formula> is the layer-wise trainable weight.</p>
      <p id="Par24">For constructing the encoder of MTAGCN, we consider how to fully use the CSA miRNA-miRNA similarity network, the CSA target-target similarity network and the miRNA-target associations through graph convolution network on the heterogeneous graph <inline-formula id="IEq37"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A_{H}$$\end{document}</tex-math><mml:math id="M86"><mml:msub><mml:mi>A</mml:mi><mml:mi>H</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq37.gif"/></alternatives></inline-formula>. Specifically, we set the input graph <italic>G</italic> as<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G = \left[ {\begin{array}{*{20}l} {\mu \sim S^{m} } \hfill &amp; A \hfill \\ {A^{T} } \hfill &amp; {\mu \sim S^{t} } \hfill \\ \end{array} } \right]$$\end{document}</tex-math><mml:math id="M88" display="block"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∼</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:mtd><mml:mtd><mml:mi>A</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∼</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_4819_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>
where <italic>μ</italic> is a penalty factor that controls the contribution of the similarity in MTAGCN’s propagation process,<inline-formula id="IEq38"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S^{m}$$\end{document}</tex-math><mml:math id="M90"><mml:msup><mml:mi>S</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq38.gif"/></alternatives></inline-formula> is the CSA miRNA-miRNA similarity matrix and <inline-formula id="IEq39"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S^{n}$$\end{document}</tex-math><mml:math id="M92"><mml:msup><mml:mi>S</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq39.gif"/></alternatives></inline-formula> is the target-target similarity matrix. To initialize embeddings, we introduce graph convolution into the latent factor model in the light of the nature ‘miRNA-target’ associations and the embedding matrix is reconstructed as<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H^{\left( 0 \right)} = \left[ {\begin{array}{*{20}l} 0 \hfill &amp; A \hfill \\ {A^{T} } \hfill &amp; 0 \hfill \\ \end{array} } \right]$$\end{document}</tex-math><mml:math id="M94" display="block"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mfenced close=")" open="("><mml:mn>0</mml:mn></mml:mfenced></mml:msup><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>A</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_4819_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>
with the above setting, the MTAGCN encoder for first layer can be defined as<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H^{\left( 1 \right)} = \sigma \left( {D^{{ - \frac{1}{2 }}} GD^{{ - \frac{1}{2 }}} H^{\left( 0 \right)} W^{\left( 0 \right)} } \right)$$\end{document}</tex-math><mml:math id="M96" display="block"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mfenced close=")" open="("><mml:mn>1</mml:mn></mml:mfenced></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mi>G</mml:mi><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mi>H</mml:mi><mml:mfenced close=")" open="("><mml:mn>0</mml:mn></mml:mfenced></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mfenced close=")" open="("><mml:mn>0</mml:mn></mml:mfenced></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_4819_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>
where <inline-formula id="IEq40"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H^{\left( 1 \right)} \in {\text{R}}^{{\left( {M + N} \right)*k}}$$\end{document}</tex-math><mml:math id="M98"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mfenced close=")" open="("><mml:mn>1</mml:mn></mml:mfenced></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mfenced><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq40.gif"/></alternatives></inline-formula> denotes the first-layer node embeddings in the heterogeneous matrix <inline-formula id="IEq41"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A_{H}$$\end{document}</tex-math><mml:math id="M100"><mml:msub><mml:mi>A</mml:mi><mml:mi>H</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq41.gif"/></alternatives></inline-formula>, <italic>k</italic> is the embedding dimensionality and <inline-formula id="IEq42"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W^{\left( 0 \right)} \in {\text{R}}^{{\left( {M + N} \right)*k}}$$\end{document}</tex-math><mml:math id="M102"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mfenced close=")" open="("><mml:mn>0</mml:mn></mml:mfenced></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mtext>R</mml:mtext></mml:mrow><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mfenced><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq42.gif"/></alternatives></inline-formula> is the trainable weight matrix of the first-layer. The MTAGCN encoders for subsequent layers follow the Formula (<xref rid="Equ5" ref-type="">5</xref>) and <italic>G</italic> is defined in Formula (<xref rid="Equ7" ref-type="">7</xref>). Herein, after <italic>L</italic> iteration, <italic>L k</italic>-dimensional CSA miRNA and target embeddings can be obtained. Furthermore, we introduce SELU (scaled exponential linear unit) [<xref ref-type="bibr" rid="CR37">37</xref>] as the activation function used in MTAGCN graph convolution layers to accelerate learning procedure and enhance generalization performance.</p>
      <p id="Par25">Different layers of the embeddings capture different structural information. Such as, the first layer obtains direct edge information and other layers obtain the multi-hop neighbor information by iteratively updating the embeddings [<xref ref-type="bibr" rid="CR38">38</xref>, <xref ref-type="bibr" rid="CR39">39</xref>]. Considering that different embeddings in different layers have various contributions, we introduce a self-attention mechanism, which adaptively combines embeddings and harvests final embeddings of CSA miRNAs and targets as <inline-formula id="IEq43"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left[ {\begin{array}{*{20}c} {H_{I} } \\ {H_{G} } \\ \end{array} } \right] = \sum a_{l} H^{l}$$\end{document}</tex-math><mml:math id="M104"><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>H</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:msub><mml:mi>H</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>∑</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:msup><mml:mi>H</mml:mi><mml:mi>l</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq43.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq44"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H_{I} \in R^{{M{*}k}} { }$$\end{document}</tex-math><mml:math id="M106"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mrow/><mml:mo>∗</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mrow/></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq44.gif"/></alternatives></inline-formula> is the final embeddings of miRNAs,<inline-formula id="IEq45"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${ }H_{G} \in R^{{N{*}k}}$$\end{document}</tex-math><mml:math id="M108"><mml:mrow><mml:mrow/><mml:msub><mml:mi>H</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mrow/><mml:mo>∗</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq45.gif"/></alternatives></inline-formula> is the final embeddings of targets, <inline-formula id="IEq46"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$a_{l}$$\end{document}</tex-math><mml:math id="M110"><mml:msub><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq46.gif"/></alternatives></inline-formula> is auto-learned by a single-layer feed-forward network.</p>
      <p id="Par26">To reconstruct adjacency matrix for CSA miRNA-target associations, a bilinear decoder <inline-formula id="IEq47"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A^{\prime} = {\text{f}}\left( {H_{I} ,H_{G} } \right)$$\end{document}</tex-math><mml:math id="M112"><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mtext>f</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq47.gif"/></alternatives></inline-formula> is built as follows:<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A^{{\prime }} = {\text{sigmoid}}\left( {H_{I} W^{{\prime }} H_{G}^{T} } \right)$$\end{document}</tex-math><mml:math id="M114" display="block"><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mtext>sigmoid</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:msup><mml:mi>W</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_4819_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>
where <inline-formula id="IEq48"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W^{\prime} \in R^{k*k}$$\end{document}</tex-math><mml:math id="M116"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq48.gif"/></alternatives></inline-formula> is the trainable matrix. We denoted <inline-formula id="IEq49"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A_{ij}^{^{\prime}} { }$$\end{document}</tex-math><mml:math id="M118"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow><mml:msup><mml:mrow/><mml:mo>′</mml:mo></mml:msup></mml:msubsup><mml:mrow/></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq49.gif"/></alternatives></inline-formula> as the predicted scores of the CSA miRNA-target association, which is given by corresponding (<italic>i</italic>, <italic>j</italic>)th entry of <inline-formula id="IEq50"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A^{\prime}.$$\end{document}</tex-math><mml:math id="M120"><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq50.gif"/></alternatives></inline-formula><graphic position="anchor" xlink:href="12859_2022_4819_Figa_HTML" id="MO2"/></p>
    </sec>
    <sec id="Sec9">
      <title>Optimization</title>
      <p id="Par27">In the dataset with <italic>M</italic> CSA miRNAs and <italic>N</italic> targets, the miRNA-target association pairs are taken as the set of all positive association pairs <inline-formula id="IEq51"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma^{ + }$$\end{document}</tex-math><mml:math id="M122"><mml:msup><mml:mi>γ</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq51.gif"/></alternatives></inline-formula> and other pairs as the set of negative pairs <inline-formula id="IEq52"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma^{ - }$$\end{document}</tex-math><mml:math id="M124"><mml:msup><mml:mi>γ</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq52.gif"/></alternatives></inline-formula>. Although it is a binary classification problem to differentiate two types of miRNA-target pairs, the number of negative miRNA–target pairs are much higher than that of the positive pairs. Herein, MTAGCN learns parameter by the loss function (weighted cross-entropy):<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{loss}} = - \frac{1}{{\begin{array}{*{20}c} {M*N} \\ \end{array} }}\left( {\lambda {*}\mathop \sum \limits_{{\left( {i,j} \right) \in \gamma^{ + } }} \log A^{\prime}_{ij} + \mathop \sum \limits_{{\left( {i,j} \right) \in \gamma^{ - } }} \log \left( {1 - A_{ij}^{{\prime }} } \right)} \right)$$\end{document}</tex-math><mml:math id="M126" display="block"><mml:mrow><mml:mtext>loss</mml:mtext><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>M</mml:mi><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfrac><mml:mfenced close=")" open="("><mml:mrow><mml:mi>λ</mml:mi><mml:mrow><mml:mrow/><mml:mo>∗</mml:mo></mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mo>∈</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:munder><mml:mo>log</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mo>∈</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:munder><mml:mo>log</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_4819_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula>
where (<italic>i, j</italic>) is the instance for CSA miRNA <inline-formula id="IEq53"><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r_{i}$$\end{document}</tex-math><mml:math id="M128"><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq53.gif"/></alternatives></inline-formula> and target <inline-formula id="IEq54"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t_{j}$$\end{document}</tex-math><mml:math id="M130"><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq54.gif"/></alternatives></inline-formula><italic>,</italic>
<inline-formula id="IEq55"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda = \frac{{\left| {\gamma^{ - } } \right|}}{{\left| {\gamma^{ + } } \right|}}$$\end{document}</tex-math><mml:math id="M132"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mfenced close="|" open="|"><mml:msup><mml:mi>γ</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mfenced><mml:mfenced close="|" open="|"><mml:msup><mml:mi>γ</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mfenced></mml:mfrac></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq55.gif"/></alternatives></inline-formula>, <inline-formula id="IEq56"><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left| {\gamma^{ + } } \right|$$\end{document}</tex-math><mml:math id="M134"><mml:mfenced close="|" open="|"><mml:msup><mml:mi>γ</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq56.gif"/></alternatives></inline-formula> and <inline-formula id="IEq57"><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left| {\gamma^{ - } } \right|$$\end{document}</tex-math><mml:math id="M136"><mml:mfenced close="|" open="|"><mml:msup><mml:mi>γ</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2022_4819_Article_IEq57.gif"/></alternatives></inline-formula> denote the corresponding pairs. The balance factor <italic>λ</italic> emphasizes the known associations and decreases the impact of data imbalance.</p>
      <p id="Par28">The Xaiver initialization method [<xref ref-type="bibr" rid="CR40">40</xref>] is used to randomly initialize all trainable weight matrices. Then, as is shown in the 9 rows of Algorithm 1, we use the Adam optimizer [<xref ref-type="bibr" rid="CR41">41</xref>] for the optimization. In order to balance the training speed and the experimental result, we also use a simple cycle learning rate [<xref ref-type="bibr" rid="CR42">42</xref>] during the optimization, that is making a change from 0.01 to 0.1. Furthermore, we introduce fine-grained edge dropout [<xref ref-type="bibr" rid="CR43">43</xref>] and coarse-grained node dropout [<xref ref-type="bibr" rid="CR44">44</xref>] in the graph convolution layers to prevent over-fitting. The fine-grained edge dropout is applied to convolution layers and dense layers, randomly drops out edges. And the coarse-grained node dropout can efficiently enforce dropout at the node level.<graphic position="anchor" xlink:href="12859_2022_4819_Figb_HTML" id="MO3"/></p>
    </sec>
    <sec id="Sec10">
      <title>Negative sampling</title>
      <p id="Par29">Recent arts usually focus on positive sampling, while the strategy for negative sampling is left insufficiently explored. However, many studies theoretically proved that negative sampling is important as positive sampling in determining the optimization objective and the resulted variance [<xref ref-type="bibr" rid="CR45">45</xref>]. Hence, negative sampling has wide application in many fields for its simplicity and efficiency, such as natural language processing [<xref ref-type="bibr" rid="CR46">46</xref>], computer vision [<xref ref-type="bibr" rid="CR47">47</xref>], recommender system [<xref ref-type="bibr" rid="CR48">48</xref>] and graph embedding [<xref ref-type="bibr" rid="CR49">49</xref>]. Inspired by previous study [<xref ref-type="bibr" rid="CR50">50</xref>], we adopted three strategies of negative sampling, including random negative sampling, sampling by CSA miRNA (SCM) and sampling by CSA target (SCT).</p>
      <p id="Par30">For random negative sampling, the negative samples were generated by randomly drawing from the total negative samples. Furthermore, we proposed two negative sampling methods, SCM and SCT. The two methods are similar in some ways and the details are shown in Algorithm 2. For SCM/SCT, we first computed all numbers of positive sample based on the per miRNA/target. Then the negative sample was drawn based on the corresponding miRNA/target. For the unbalanced task, we executed the SCM/SCT in a loop to get enough negative samples. Compared with the random negative sampling without regularity, the other two sampling methods can be based on one CSA miRNA or target. It is worth pointing out that SCM/SCT can ensure that every miRNA/target be sampled, greatly increasing the sampling range of negative samples. In the following, we will compare the above sampling strategies.</p>
    </sec>
  </sec>
  <sec id="Sec11">
    <title>Results and discussions</title>
    <p id="Par31">In this section, we briefly introduced the experimental setup. Next, we carried out to evaluate the performance of the proposed MTAGCN model and the effect of layer attention mechanism, then demonstrated the performance of our model by comparing with five machine learning methods and three existing link/association prediction methods on balanced and unbalanced tasks (Table <xref rid="Tab2" ref-type="table">2</xref>), respectively.</p>
    <sec id="Sec12">
      <title>Experimental setting</title>
      <p id="Par32">To evaluate the effectiveness of our model, we performed five-fold cross validation on the two tasks. We randomly divided known miRNA-target associations into five subsets with equal size. For five-fold cross-validation, we randomly used the 80% known miRNA-target associations for training and the remaining 20% for test. We employed the AUPR (area under precision-recall curve) and the AUC (area under ROC curve) as primary metrics during cross validation which are widely used for pair-wise link predictions [<xref ref-type="bibr" rid="CR51">51</xref>]. Besides, we also calculated other metrics, i.e. recall, specificity, precision, ACC and F1-score.</p>
      <p id="Par33">We set the embeddings dimensionality <italic>k</italic> as 64 by conducting the parameter sensitivity analysis. The layer number <italic>L</italic>, the initial learning rate <italic>lr</italic>, the coarse-grained node dropout <italic>α</italic> and the fine-grained edge dropout <italic>β</italic>, are respectively set to 3, 0.01, 0.6 and 0.6. In addition, the total training epochs of MTAGCN <italic>γ</italic> was set to 500, and the penalty factor <italic>μ</italic> was set to 0.06. Our experiment code was implanted on the open-source machine learning framework Tensorflow. All experiments were conducted on Ubuntu operating 20.04 system with a NVDIA GeForce GTX3090 GPU and 32G memory.</p>
    </sec>
    <sec id="Sec13">
      <title>The influence of different heterogeneous networks</title>
      <p id="Par34">MTAGCN takes advantage of the CSA miRNA-target heterogeneous network to construct the model. And we built the heterogeneous network by aggregating miRNA-miRNA similarities, target-target similarities and known miRNA-target associations. Since we took into account four similarity measures, MTAGCN could be trained on various heterogeneous networks, which may have a certain effect on the predictive ability.</p>
      <p id="Par35">MTAGCN models based on heterogeneous networks with different miRNA-miRNA and target-target similarities were evaluated by five-fold cross validation, and Table <xref rid="Tab3" ref-type="table">3</xref> shows the corresponding results. The Jaccard index achieved slightly better performance than the other similarity measures we used. And these results reflected that our model is robust. Based on the analysis, we ultimately employed Jaccard index to calculate CSA target-target similarity and CSA miRNA-miRNA similarity. In the following study, the heterogenous network was construct by fusing two similarity networks and CSA miRNA-target associations.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Performances of MTAGCN based on different similarity measures</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Measure</th><th align="left">AUPR</th><th align="left">AUC</th><th align="left">F1</th><th align="left">Accuracy</th><th align="left">Recall</th><th align="left">Specificity</th><th align="left">Precision</th></tr></thead><tbody><tr><td align="left">Jaccard</td><td char="." align="char"><bold>0.9207</bold></td><td char="." align="char"><bold>0.8756</bold></td><td char="." align="char"><bold>0.8555</bold></td><td char="." align="char"><bold>0.8669</bold></td><td char="." align="char"><bold>0.7886</bold></td><td char="." align="char"><bold>0.9453</bold></td><td char="." align="char"><bold>0.9356</bold></td></tr><tr><td align="left">Cosine</td><td char="." align="char">0.8966</td><td char="." align="char">0.8805</td><td char="." align="char">0.8011</td><td char="." align="char">0.8212</td><td char="." align="char">0.7202</td><td char="." align="char">0.9222</td><td char="." align="char">0.9028</td></tr><tr><td align="left">Pearson</td><td char="." align="char">0.8908</td><td char="." align="char">0.8699</td><td char="." align="char">0.7880</td><td char="." align="char">0.8123</td><td char="." align="char">0.7020</td><td char="." align="char">0.9226</td><td char="." align="char">0.9022</td></tr><tr><td align="left">Gaussian</td><td char="." align="char">0.8360</td><td char="." align="char">0.7666</td><td char="." align="char">0.6928</td><td char="." align="char">0.7475</td><td char="." align="char">0.5733</td><td char="." align="char">0.9218</td><td char="." align="char">0.8788</td></tr></tbody></table><table-wrap-foot><p>The maximum value of each metric is bold</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec14">
      <title>Analysis of negative sampling</title>
      <p id="Par36">As mentioned above, we adopted three negative sampling strategies motivated by previous studies. We tested our model on these sampling strategies and then discussed how they influence the performances of MTAGCN. Table <xref rid="Tab4" ref-type="table">4</xref> shows that SCT achieves better results than both SCM and random negative sampling methods. That is maybe due to the number of targets is much more than that of miRNAs, resulting in a larger sampling range and reducing the sampling imbalance. To this end, we performed SCT strategy, setting the ratio of positive and negative samples to a rate of 1:1 (balanced task) and 1:20 (unbalanced task) in the follow-up test sets.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Performances of MTAGCN based on different negative sampling strategies on the balanced task</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Sampling</th><th align="left">AUPR</th><th align="left">AUC</th><th align="left">F1</th><th align="left">Accuracy</th><th align="left">Recall</th><th align="left">Specificity</th><th align="left">Precision</th></tr></thead><tbody><tr><td align="left">SCT</td><td char="." align="char"><bold>0.9207</bold></td><td char="." align="char">0.8756</td><td char="." align="char"><bold>0.8555</bold></td><td char="." align="char"><bold>0.8669</bold></td><td char="." align="char">0.7886</td><td char="." align="char"><bold>0.9453</bold></td><td char="." align="char"><bold>0.9356</bold></td></tr><tr><td align="left">Random</td><td char="." align="char">0.8945</td><td char="." align="char"><bold>0.8824</bold></td><td char="." align="char">0.7893</td><td char="." align="char">0.8110</td><td char="." align="char">0.7081</td><td char="." align="char">0.9140</td><td char="." align="char">0.8919</td></tr><tr><td align="left">SCM</td><td char="." align="char">0.8914</td><td char="." align="char">0.8502</td><td char="." align="char">0.8332</td><td char="." align="char">0.8400</td><td char="." align="char"><bold>0.8003</bold></td><td char="." align="char">0.8796</td><td char="." align="char">0.8695</td></tr></tbody></table><table-wrap-foot><p>The maximum value of each metric is bold. SCT, sampling by CSA target; Random, random negative sampling; SCM, sampling by CSA miRNA</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec15">
      <title>Results of MTAGCN</title>
      <p id="Par37">To develop the MTAGCN, we used the embeddings for diverse layers to construct models which denoted as MTAGCN-L1, MTAGCN-L2 and MTAGCN-L3. Table <xref rid="Tab5" ref-type="table">5</xref> shows the performance of the above models using five-fold cross validation. MTAGCN-L1 and MTAGCN-L2 performed better than MTAGCN-L3, showing that the lower layer captures more information than the higher layer because of the over-smoothing. However, MTAGCN that combines the embeddings for all three layers produced the best results on the balanced task.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Performances of MTAGCN based on different embeddings for the balanced task</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Performance</th><th align="left">AUPR</th><th align="left">AUC</th><th align="left">F1</th><th align="left">Accuracy</th><th align="left">Recall</th><th align="left">Specificity</th><th align="left">Precision</th></tr></thead><tbody><tr><td align="left" colspan="8">Models</td></tr><tr><td align="left"> MTAGCN</td><td char="." align="char"><bold>0.9207</bold></td><td char="." align="char"><bold>0.8756</bold></td><td char="." align="char"><bold>0.8555</bold></td><td char="." align="char"><bold>0.8669</bold></td><td char="." align="char"><bold>0.7886</bold></td><td char="." align="char">0.9453</td><td char="." align="char">0.9356</td></tr><tr><td align="left"> MTAGCN-AVE</td><td char="." align="char">0.9134</td><td char="." align="char">0.8695</td><td char="." align="char">0.8208</td><td char="." align="char">0.8331</td><td char="." align="char">0.7648</td><td char="." align="char">0.9015</td><td char="." align="char">0.8863</td></tr><tr><td align="left"> MTAGCN-CON</td><td char="." align="char">0.9166</td><td char="." align="char">0.8657</td><td char="." align="char">0.8117</td><td char="." align="char">0.8348</td><td char="." align="char">0.7148</td><td char="." align="char"><bold>0.9549</bold></td><td char="." align="char"><bold>0.9416</bold></td></tr><tr><td align="left"> MTAGCN-L1</td><td char="." align="char">0.9052</td><td char="." align="char">0.8516</td><td char="." align="char">0.8347</td><td char="." align="char">0.8498</td><td char="." align="char">0.7583</td><td char="." align="char">0.9413</td><td char="." align="char">0.9284</td></tr><tr><td align="left"> MTAGCN-L2</td><td char="." align="char">0.8638</td><td char="." align="char">0.8288</td><td char="." align="char">0.7884</td><td char="." align="char">0.8084</td><td char="." align="char">0.7143</td><td char="." align="char">0.9025</td><td char="." align="char">0.8799</td></tr><tr><td align="left"> MTAGCN-L3</td><td char="." align="char">0.8499</td><td char="." align="char">0.8274</td><td char="." align="char">0.7409</td><td char="." align="char">0.7696</td><td char="." align="char">0.6617</td><td char="." align="char">0.8774</td><td char="." align="char">0.8438</td></tr></tbody></table><table-wrap-foot><p>The maximum value of each metric is bold</p></table-wrap-foot></table-wrap></p>
      <p id="Par38">The <italic>l</italic>th layer of MTAGCN captures the <italic>l</italic>th-order proximity value between nodes, and the attention weights represent the relative contribution of the corresponding convolution layers. We implemented 20 runs of 5-cv, and the Fig. <xref rid="Fig2" ref-type="fig">2</xref> visualizes the attention weights of diverse convolution layers. Different convolution layers have diverse weights, and that of the lower layer is greater than those of the higher layers, revealing that the lower-order proximity is of more important than the higher. Therefore, it also helps to illustrate the performance of MTAGCN-L1, MTAGCN-L2, MTAGCN-L3 (Table <xref rid="Tab5" ref-type="table">5</xref>).<fig id="Fig2"><label>Fig. 2</label><caption><p>The attention weights of diverse convolution layers in MTAGCN</p></caption><graphic xlink:href="12859_2022_4819_Fig2_HTML" id="MO4"/></fig></p>
      <p id="Par39">Furthermore, we considered MTAGCN-AVE and MTAGCN-CON, which integrate embeddings from different convolution layers. MTAGCN-AVE adopts the average of weights for different embeddings. As to MTAGCN-CON, we stack the embeddings for three layers directly. In Table <xref rid="Tab5" ref-type="table">5</xref>, the results indicate that the MTAGCN with attention mechanism achieved more encouraging performance than MTAGCN-AVE and MTAGCN-CON. Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S1 shows the results under unbalanced task, from which we can obtain similar conclusions.</p>
    </sec>
    <sec id="Sec16">
      <title>Comparison with the machine learning methods</title>
      <p id="Par40">To investigate the performance of our proposed MTAGCN model for CSA miRNA-target association prediction, we compared it with some classic machine learning algorithms, including random forest (RF), extremely randomized tree (ERT), decision tree (DT), Gaussian naïve Bayes (GNBS), deep neural network (DNN). The results for the above machine learning models on the balanced and unbalanced task are shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>.<fig id="Fig3"><label>Fig. 3</label><caption><p>Performance comparison between MTAGCN and five types of classic machine learning methods on (<bold>A</bold>) the balanced task and (<bold>B</bold>) the unbalanced task, including RF (random forest), ERT (extremely randomized tree), DT (decision tree), GNBs (Gaussian naïve Bayes), DNN (deep neural network). F1, Acc, Rec, Spe and Pre represent the F1-score, accuracy, recall, specificity and precision, respectively</p></caption><graphic xlink:href="12859_2022_4819_Fig3_HTML" id="MO5"/></fig></p>
      <p id="Par41">According to the results, MTAGCN outperforms all classic machine learning methods, strikingly for the balanced task in Fig. <xref rid="Fig3" ref-type="fig">3</xref> (A). As to the unbalanced task, the accuracy and the specificity of the MTAGCN are lower than most of the classic machine learning models, but the primary metrics AUPR and AUC are higher than these models. It is believed that both the accuracy and specificity are threshold-based metrics, which are greatly affected by data imbalance [<xref ref-type="bibr" rid="CR52">52</xref>]. Overall, MTAGCN has a better performance than the methods used. These classic machine methods all have a low AUPR, F1, and recall, which means the proposed model produces more robust performances across two tasks.</p>
    </sec>
    <sec id="Sec17">
      <title>Comparison with the state-of-the-art methods</title>
      <p id="Par42">As mentioned before, there has few existing methods developed specifically to solve CSA miRNA-target association prediction problem. Therefore, we compared MTAGCN with three state-of-the-art approaches proposed to address other association prediction tasks in the computational biology.<list list-type="bullet"><list-item><p id="Par43">GCMDR [<xref ref-type="bibr" rid="CR53">53</xref>] constructed a graph convolutional network based model to identify miRNA-drug resistance relationships.</p></list-item><list-item><p id="Par44">GATMDA [<xref ref-type="bibr" rid="CR54">54</xref>] proposed a graph attention networks model with inductive matrix completion to predict human microbe-disease associations.</p></list-item><list-item><p id="Par45">GCNMDA [<xref ref-type="bibr" rid="CR55">55</xref>] deployed a conditional random field on the graph convolution network to predict human microbe-drug associations.</p></list-item></list></p>
      <p id="Par46">We compared them with our proposed model under the same experimental conditions, including balanced and unbalanced tasks. The results are shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. We can observe that among all the methods under the balanced task in Fig. <xref rid="Fig4" ref-type="fig">4</xref> (A), MTAGCN achieves the best performance. For the unbalanced task, although GCMDR, GCNMDA have slightly higher accuracy and specificity values than MTAGCN, overall, MTAGCN has better performance on the other metrics in Fig. <xref rid="Fig4" ref-type="fig">4</xref> (B). As mentioned above, accuracy and specificity are greatly affected by data imbalance. In addition, MTAGCN outperformed all compared deep-learning methods in the most evaluation metrics. Furthermore, we would explain the reason why GCMDR obtained such low F1, recall and precision, finding that predicted scores of the true positive samples are almost close to 0. It is believed that the robustness of the GCMDR model is not good for CSA miRNA-target prediction problem.<fig id="Fig4"><label>Fig. 4</label><caption><p>Performance comparison between MTAGCN and the state-of-the-art methods on (<bold>A</bold>) the balanced task and (<bold>B</bold>) the unbalanced task. F1, Acc, Rec, Spe and Pre represent the F1-score, accuracy, recall, specificity and precision, respectively</p></caption><graphic xlink:href="12859_2022_4819_Fig4_HTML" id="MO6"/></fig></p>
    </sec>
    <sec id="Sec18">
      <title>Parameter sensitivity</title>
      <p id="Par47">There are several important parameters influence our model performance, such as the coarse-grained node dropout rate <italic>α</italic>, fine-grained edge dropout rate <italic>β</italic>, the embedding dimensionality <italic>k</italic> and the total training epoch <italic>T</italic>. In order to assess the parameter sensitivity, we evaluated the influences using five-fold CV for all parameters based on balanced task. The node dropout rate <italic>α</italic> plays an important role in our model. We ranged <italic>α</italic> from 0.1 to 0.6 with a step value of 0.1. As shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>, we can achieve the best performance when <italic>α</italic> = 0.6 and a small value of <italic>α</italic> is not good for the model performance. <italic>β</italic> is the regular dropout rate of the edge. We evaluated the performance of model by varying <italic>β</italic> from 0.1 to 0.6 with a step of 0.1. From Fig. <xref rid="Fig5" ref-type="fig">5</xref>, we could conclude that this parameter has a relatively slight influence on our model performance, which indicates that our model is robust against the regular dropout rate <italic>β</italic>. In addition, we used <italic>k</italic> to control the dimensionality of embeddings. In our experiment, we varied <italic>k</italic> from the range of {8, 16, 32, 64, 128, 256}. It can be observed that the best performance is achieved when <italic>k</italic> is 64 and the performance decreases if the value of <italic>k</italic> further increases in the Fig. <xref rid="Fig5" ref-type="fig">5</xref>. Lastly, we also considered the influence of total training epoch <italic>T</italic>. Results in Fig. <xref rid="Fig5" ref-type="fig">5</xref> show that, our model produces the robust performances to the training epoch, which first slightly increases and then decreases, with epoch = 500 achieving the best performance.<fig id="Fig5"><label>Fig. 5</label><caption><p>Parameter sensitivity under balanced task across node dropout (α), adjdp dropout (β), embedding dimensionality (<italic>k</italic>) and total training epoch (<italic>T</italic>). F1, Acc, Rec, Spe and Pre represent the F1-score, accuracy, recall, specificity and precision, respectively</p></caption><graphic xlink:href="12859_2022_4819_Fig5_HTML" id="MO7"/></fig></p>
    </sec>
    <sec id="Sec19">
      <title>Case study</title>
      <p id="Par48">To verify the performance of the proposed model on CSA miRNA-target prediction task, we conducted case studies for CSA miRNA associated with targets. csn-MIR156j_5p is a conserved miRNA in the leaf and root degradomes of CSA, which plays an important role in organ/tissue-specific physiological and developmental process [<xref ref-type="bibr" rid="CR7">7</xref>]. It has high expression levels with the functions of photosynthesis and transmembrane transport, regulating target CSA019508.1 and CSA015924.1 respectively. Some studies also proved that this miRNA could bind to the target CSA018458.1 to inhibit the secretion of resistance proteins [<xref ref-type="bibr" rid="CR56">56</xref>]. csn-MIR319e_5p is a miRNA that influences the ATPase activity and ATP metabolic process related gene expression. For example, it can combine with the target TEA00633.1 and TEA000574.1 to reduce CSA respiration [<xref ref-type="bibr" rid="CR31">31</xref>]. csn-MIR390b_3p is a conserved miRNA that relates to the structural constituent ribosome and oxygen-containing compound. Recent studies show that there is a close relationship between this miRNA and the CSA photosynthesis when targeting CSA024193.1 and CSA016339.1 [<xref ref-type="bibr" rid="CR56">56</xref>]. However, huge challenges remain to reveal the mechanism of miRNA because of its functional complexity. Table <xref rid="Tab6" ref-type="table">6</xref> lists the results of the three case studies. It is obvious that they all show superior performances, demonstrating that the proposed MTAGCN model is capable of predicting the undiscovered potential miRNA-target associations for CSA miRNAs and targets.<table-wrap id="Tab6"><label>Table 6</label><caption><p>The summary of case studies for csn-MIR156j_5p, csn-MIR319e_5p and csn-miR390b_3p</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">CSA miRNA</th><th align="left">AUPR</th><th align="left">AUC</th><th align="left">F1</th><th align="left">Accuracy</th><th align="left">Recall</th><th align="left">Specificity</th><th align="left">Precision</th></tr></thead><tbody><tr><td align="left">csn-MIR156j_5p</td><td char="." align="char">0.95536</td><td char="." align="char">0.9999</td><td char="." align="char">0.8333</td><td char="." align="char">0.9993</td><td char="." align="char">0.8333</td><td char="." align="char">0.9997</td><td char="." align="char">0.8333</td></tr><tr><td align="left">csn-MIR319e_5p</td><td char="." align="char">0.95536</td><td char="." align="char">0.9999</td><td char="." align="char">0.7692</td><td char="." align="char">0.9990</td><td char="." align="char">0.8333</td><td char="." align="char">0.9993</td><td char="." align="char">0.7143</td></tr><tr><td align="left">csn-miR390b_3p</td><td char="." align="char">0.96621</td><td char="." align="char">0.9998</td><td char="." align="char">0.8333</td><td char="." align="char">0.9986</td><td char="." align="char">0.9091</td><td char="." align="char">0.9990</td><td char="." align="char">0.7692</td></tr></tbody></table></table-wrap></p>
    </sec>
  </sec>
  <sec id="Sec20">
    <title>Conclusion</title>
    <p id="Par49">In this paper, we proposed a novel deep learning framework, named MTAGCN, based on graph convolution network with layer attention for CSA miRNA-target association prediction. Compared with existing methods utilizing the topological graphs, MTAGCN integrates the graph information of the heterogeneous network built from CSA miRNA-target associations, CSA miRNA-miRNA similarity network and CSA target-target similarity network. Furthermore, MTAGCN adaptively combines embeddings at diverse convolution layers. Extensive experimental results demonstrate that MTAGCN outperforms the existing link/association prediction methods in predicting CSA miRNA-target associations.</p>
    <p id="Par50">However, although our model has good prediction performance, there is still room to enhance MTAGCN through further refinement. Due to the noise in the features extracted from similarity networks, our model is far from perfect, and the prediction results can be further improved. As a fast-growing research field, graph construction and multi-source feature fusion methods are boosting the model performance. For later versions of MTAGCN, we aim to further work closely with other study groups and develop the model on more experimentally verified data about CSA miRNA-target link associations.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Information</title>
    <sec id="Sec21">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2022_4819_MOESM1_ESM.docx">
            <caption>
              <p><bold>Additional file 1</bold>. Performance of MTAGCN based on different embeddings for the unbalanced task.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We would like to thank all authors of the cited references.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>HF: data curation, conceptualization, methodology, visualization, writing original draft. YX: methodology, visualization, conceptualization. XW: data curation, methodology. WX: visualization, conceptualization. ZY: supervision and editing, conceptualization, funding acquisition. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was supported by the grants from the National Natural Science Foundation of China (62102004), the Natural Science Young Foundation of Anhui (2008085QF293), the Natural Science Young Foundation of Anhui Agricultural University (2019zd12) and the Introduction and Stabilization of Talent Project of Anhui Agricultural University (yj2019-32).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The source code and processed data are freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/haisonF/MTAGCN">https://github.com/haisonF/MTAGCN</ext-link>. The CSA miRNA information is available at the Supplementary Table 23 in Suo et al., <ext-link ext-link-type="uri" xlink:href="https://ars.els-cdn.com/content/image/1-s2.0-S0888754320320188-mmc1.xlsx">https://ars.els-cdn.com/content/image/1-s2.0-S0888754320320188-mmc1.xlsx</ext-link>. The CSA target information is available online at <ext-link ext-link-type="uri" xlink:href="http://teacon.wchoda.com">http://teacon.wchoda.com</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1">
      <title>Ethics approval and consent to participate</title>
      <p id="Par51">Not applicable.</p>
    </notes>
    <notes id="FPar2">
      <title>Consent for publication</title>
      <p id="Par52">Not applicable.</p>
    </notes>
    <notes id="FPar3" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par53">Authors have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Namita</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Mukesh</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Vijay</surname>
            <given-names>KJ</given-names>
          </name>
        </person-group>
        <article-title><italic>Camellia sinensis</italic> (green tea): a review</article-title>
        <source>Global J Pharmacol</source>
        <year>2012</year>
        <volume>6</volume>
        <fpage>52</fpage>
        <lpage>59</lpage>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gong</surname>
            <given-names>A-D</given-names>
          </name>
          <name>
            <surname>Lian</surname>
            <given-names>S-B</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>N-N</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y-J</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>S-Q</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>L-M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Integrated transcriptomics and metabolomics analysis of catechins, caffeine and theanine biosynthesis in tea plant (<italic>Camellia</italic> sinensis) over the course of seasons</article-title>
        <source>BMC Plant Biol</source>
        <year>2020</year>
        <volume>2</volume>
        <fpage>20</fpage>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Physiological genetics, chemical composition, health benefits and toxicology of tea (<italic>Camellia sinensis</italic> L.) flower: a review</article-title>
        <source>Food Res Int</source>
        <year>2020</year>
        <volume>137</volume>
        <fpage>109584</fpage>
        <pub-id pub-id-type="doi">10.1016/j.foodres.2020.109584</pub-id>
        <pub-id pub-id-type="pmid">33233193</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xia</surname>
            <given-names>E-H</given-names>
          </name>
          <name>
            <surname>Tong</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z-Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Tea plant genomics: achievements, challenges and perspectives</article-title>
        <source>Hortic Res</source>
        <year>2020</year>
        <volume>7</volume>
        <fpage>7</fpage>
        <pub-id pub-id-type="doi">10.1038/s41438-019-0225-4</pub-id>
        <pub-id pub-id-type="pmid">31908810</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Relationship between secondary metabolism and miRNA for important flavor compounds in different tissues of tea plant (<italic>Camellia sinensis</italic>) as revealed by genome-wide miRNA analysis</article-title>
        <source>J Agric Food Chem</source>
        <year>2021</year>
        <volume>69</volume>
        <fpage>2001</fpage>
        <lpage>2012</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jafc.0c07440</pub-id>
        <pub-id pub-id-type="pmid">33538166</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Z-W</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J-X</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhuang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Integrative transcriptome, proteome, and microRNA analysis reveals the effects of nitrogen sufficiency and deficiency conditions on theanine metabolism in the tea plant (<italic>Camellia sinensis</italic>)</article-title>
        <source>Hortic Res</source>
        <year>2020</year>
        <volume>7</volume>
        <fpage>44</fpage>
        <pub-id pub-id-type="doi">10.1038/s41438-020-0266-8</pub-id>
        <pub-id pub-id-type="pmid">32257230</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Suo</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lan</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Pu</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Characterizing microRNAs and their targets in different organs of <italic>Camellia sinensis</italic> var. assamica</article-title>
        <source>Genomics</source>
        <year>2021</year>
        <volume>113</volume>
        <fpage>159</fpage>
        <lpage>70</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ygeno.2020.11.020</pub-id>
        <pub-id pub-id-type="pmid">33253793</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gottlieb</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Stein</surname>
            <given-names>GY</given-names>
          </name>
          <name>
            <surname>Ruppin</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Sharan</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>PREDICT: a method for inferring novel drug indications with application to personalized medicine</article-title>
        <source>Mol Syst Biol</source>
        <year>2011</year>
        <volume>7</volume>
        <fpage>496</fpage>
        <pub-id pub-id-type="doi">10.1038/msb.2011.26</pub-id>
        <pub-id pub-id-type="pmid">21654673</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Oh</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ahn</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yoon</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>A network-based classification model for deriving novel drug-disease associations and assessing their molecular actions</article-title>
        <source>PLoS ONE</source>
        <year>2014</year>
        <volume>9</volume>
        <fpage>e111668</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0111668</pub-id>
        <pub-id pub-id-type="pmid">25356910</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Zhang W, Yue X, Chen Y, Lin W, Li B, Liu F, et al. Predicting drug-disease associations based on the known association bipartite network. In: 2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE; 2017. p. 503–9.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>DR2DI: a powerful computational tool for predicting novel drug-disease associations</article-title>
        <source>J Comput Aided Mol Des</source>
        <year>2018</year>
        <volume>32</volume>
        <fpage>633</fpage>
        <lpage>642</lpage>
        <pub-id pub-id-type="doi">10.1007/s10822-018-0117-y</pub-id>
        <pub-id pub-id-type="pmid">29687309</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Wu G, Liu J, Wang C. Semi-supervised graph cut algorithm for drug repositioning by integrating drug, disease and genomic associations. In: 2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE; 2016. p. 223–8.</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Qu</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>LRSSL: predict and interpret drug-disease associations based on data integration using sparse subspace learning</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <fpage>1187</fpage>
        <lpage>1196</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btw591</pub-id>
        <pub-id pub-id-type="pmid">28096083</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Yue</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>F</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Predicting drug-disease associations by using similarity constrained matrix factorization</article-title>
        <source>BMC Bioinform</source>
        <year>2018</year>
        <volume>19</volume>
        <fpage>233</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-018-2220-4</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>F-X</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Overlap matrix completion for predicting drug-associated indications</article-title>
        <source>PLoS Comput Biol</source>
        <year>2019</year>
        <volume>15</volume>
        <fpage>e1007541</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1007541</pub-id>
        <pub-id pub-id-type="pmid">31869322</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Luo</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Computational drug repositioning using low-rank matrix approximation and randomized algorithms</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <fpage>1904</fpage>
        <lpage>1912</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty013</pub-id>
        <pub-id pub-id-type="pmid">29365057</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>F-X</given-names>
          </name>
          <name>
            <surname>Ngom</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>A review on machine learning principles for multi-view biological data integration</article-title>
        <source>Brief Bioinform</source>
        <year>2018</year>
        <volume>19</volume>
        <fpage>325</fpage>
        <lpage>340</lpage>
        <?supplied-pmid 28011753?>
        <pub-id pub-id-type="pmid">28011753</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Carrio</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sampedro</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Rodriguez-Ramos</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Campoy</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>A review of deep learning methods and applications for unmanned aerial vehicles</article-title>
        <source>J Sens.</source>
        <year>2017</year>
        <volume>2017</volume>
        <fpage>44589</fpage>
        <pub-id pub-id-type="doi">10.1155/2017/3296874</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Long</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>PS</given-names>
          </name>
        </person-group>
        <article-title>A Comprehensive survey on graph neural networks</article-title>
        <source>IEEE Trans Neural Netw Learn Syst</source>
        <year>2021</year>
        <volume>32</volume>
        <fpage>4</fpage>
        <lpage>24</lpage>
        <pub-id pub-id-type="doi">10.1109/TNNLS.2020.2978386</pub-id>
        <pub-id pub-id-type="pmid">32217482</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Han P, Yang P, Zhao P, Shang S, Liu Y, Zhou J, et al. GCN-MF: disease-gene association identification by graph convolutional networks and matrix factorization. In: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2019. p. 705–13.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>CIPHER-SC: disease-gene association inference using graph convolution on a context-aware network with single-cell data</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinform</source>
        <year>2020</year>
        <volume>2</volume>
        <fpage>8886</fpage>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Purkayastha S, Mondal I, Sarkar S, Goyal P, Pillai JK. Drug-drug interactions prediction based on drug embedding and graph auto-encoder. In: 2019 IEEE 19th International Conference on Bioinformatics and Bioengineering (BIBE). IEEE; 2019. p. 547–52.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tran</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Kavuluru</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Kilicoglu</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Attention-gated graph convolutions for extracting drug interaction information from drug labels</article-title>
        <source>ACM Trans Comput Healthc</source>
        <year>2021</year>
        <volume>2</volume>
        <fpage>44589</fpage>
        <pub-id pub-id-type="doi">10.1145/3423209</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Que</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A novel computational model for predicting microRNA–disease associations based on heterogeneous graph convolutional networks</article-title>
        <source>Cells</source>
        <year>2019</year>
        <volume>8</volume>
        <fpage>977</fpage>
        <pub-id pub-id-type="doi">10.3390/cells8090977</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Ning</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Neural inductive matrix completion with graph convolutional networks for miRNA-disease association prediction</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <fpage>2538</fpage>
        <lpage>2546</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btz965</pub-id>
        <pub-id pub-id-type="pmid">31904845</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Kipf TN, Welling M. Semi-Supervised Classification with Graph Convolutional Networks. In: International Conference on Learning Representations (ICLR). 2017.</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Predicting drug–disease associations through layer attention graph convolutional network</article-title>
        <source>Brief Bioinform</source>
        <year>2021</year>
        <volume>22</volume>
        <fpage>243</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbaa243</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Dai</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>MDA-GCNFTG: identifying miRNA-disease associations based on graph convolutional networks via graph sampling through the feature and topology graph</article-title>
        <source>Brief Bioinform</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.1093/bib/bbab165</pub-id>
        <?supplied-pmid 34396388?>
        <pub-id pub-id-type="pmid">34396388</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Yang J-H, Chen C-M, Wang C-J, Tsai M-F. HOP-rec: high-order proximity for implicit recommendation. In: Proceedings of the 12th ACM Conference on Recommender Systems. 2018. p. 140–4.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. In: Advances in Neural Information Processing Systems. 2017. p. 5998–6008.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>TeaCoN: a database of gene co-expression network for tea plant (<italic>Camellia sinensis</italic>)</article-title>
        <source>BMC Genomics</source>
        <year>2020</year>
        <volume>21</volume>
        <fpage>461</fpage>
        <pub-id pub-id-type="doi">10.1186/s12864-020-06839-w</pub-id>
        <pub-id pub-id-type="pmid">32620074</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xia</surname>
            <given-names>E-H</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>F-D</given-names>
          </name>
          <name>
            <surname>Tong</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>P-H</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>H-J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Tea Plant Information Archive: a comprehensive genomics and bioinformatics platform for tea plant</article-title>
        <source>Plant Biotechnol J</source>
        <year>2019</year>
        <volume>17</volume>
        <fpage>1938</fpage>
        <lpage>1953</lpage>
        <pub-id pub-id-type="doi">10.1111/pbi.13111</pub-id>
        <pub-id pub-id-type="pmid">30913342</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Marquez-Lago</surname>
            <given-names>TT</given-names>
          </name>
          <name>
            <surname>Leier</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Revote</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>iLearn: an integrated platform and meta-learner for feature engineering, machine-learning analysis and modeling of DNA, RNA and protein sequence data</article-title>
        <source>Brief Bioinform</source>
        <year>2020</year>
        <volume>21</volume>
        <fpage>1047</fpage>
        <lpage>1057</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbz041</pub-id>
        <pub-id pub-id-type="pmid">31067315</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ni</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Predicting drug-disease associations via multi-task learning based on collective matrix factorization</article-title>
        <source>Front Bioeng Biotechnol</source>
        <year>2020</year>
        <volume>218</volume>
        <fpage>44005</fpage>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>MVGCN: data integration through multi-view graph convolutional network for predicting links in biomedical bipartite networks</article-title>
        <source>Bioinformatics</source>
        <year>2022</year>
        <volume>38</volume>
        <fpage>426</fpage>
        <lpage>434</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btab651</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Defferrard</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Bresson</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Vandergheynst</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Convolutional neural networks on graphs with fast localized spectral filtering</article-title>
        <source>Adv Neural Inf Process Syst</source>
        <year>2016</year>
        <volume>29</volume>
        <fpage>3844</fpage>
        <lpage>3852</lpage>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Trottier L, Giguere P, Chaib-Draa B. Parametric exponential linear unit for deep convolutional neural networks. In: 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA). IEEE; 2017. p. 207–14.</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Wang X, He X, Wang M, Feng F, Chua T-S. Neural graph collaborative filtering. In: Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. 2019. p. 165–74.</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">He X, Deng K, Wang X, Li Y, Zhang Y, Wang M. Lightgcn: Simplifying and powering graph convolution network for recommendation. In: Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. 2020. p. 639–48.</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Glorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks. In: Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings; 2010. p. 249–56.</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J. Adam: A Method for Stochastic Optimization. In: International Conference on Learning Representations (ICLR). 2015. p. 13.</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Smith LN. Cyclical learning rates for training neural networks. In: 2017 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE; 2017. p. 464–72.</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Srivastava</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Salakhutdinov</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</article-title>
        <source>J Mach Learn Res</source>
        <year>2014</year>
        <volume>15</volume>
        <fpage>1929</fpage>
        <lpage>1958</lpage>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">van den Berg R, Kipf TN, Welling M. Graph Convolutional Matrix Completion. 2017.</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Yang Z, Ding M, Zhou C, Yang H, Zhou J, Tang J. Understanding negative sampling in graph representation learning. In: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2020. p. 1666–76.</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Chen L, Yuan F, Jose JM, Zhang W. Improving negative sampling for word representation using self-embedded features. In: Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. 2018. p. 99–107.</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Wu C-Y, Manmatha R, Smola AJ, Krahenbuhl P. Sampling matters in deep embedding learning. In: Proceedings of the IEEE International Conference on Computer Vision. 2017. p. 2840–8.</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other">Ding J, Quan Y, He X, Li Y, Jin D. Reinforced Negative Sampling for Recommendation with Exposure Data. In: IJCAI. 2019. p. 2230–6.</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <mixed-citation publication-type="other">Grover A, Leskovec J. node2vec: Scalable feature learning for networks. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2016. p. 855–64.</mixed-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Chen T, Sun Y, Shi Y, Hong L. On sampling strategies for neural network-based collaborative filtering. In: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2017. p. 767–76.</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Jin</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>LDAI-ISPS: LncRNA–disease associations inference based on integrated space projection scores</article-title>
        <source>Int J Mol Sci</source>
        <year>2020</year>
        <volume>21</volume>
        <fpage>1508</fpage>
        <pub-id pub-id-type="doi">10.3390/ijms21041508</pub-id>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mellor</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Boukir</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Haywood</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Exploring issues of training data imbalance and mislabelling on random forest performance for large area land cover classification using the ensemble margin</article-title>
        <source>ISPRS J Photogramm Remote Sens</source>
        <year>2015</year>
        <volume>105</volume>
        <fpage>155</fpage>
        <lpage>168</lpage>
        <pub-id pub-id-type="doi">10.1016/j.isprsjprs.2015.03.014</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>KCC</given-names>
          </name>
          <name>
            <surname>You</surname>
            <given-names>Z-H</given-names>
          </name>
        </person-group>
        <article-title>Graph convolution for predicting associations between miRNA and drug resistance</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <fpage>851</fpage>
        <lpage>858</lpage>
        <?supplied-pmid 31397851?>
        <pub-id pub-id-type="pmid">31397851</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Long</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Xia</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Predicting human microbe-disease associations via graph attention networks with inductive matrix completion</article-title>
        <source>Brief Bioinform</source>
        <year>2021</year>
        <volume>2</volume>
        <fpage>22</fpage>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Long</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kwoh</surname>
            <given-names>CK</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Predicting human microbe-drug associations via graph convolutional network with conditional random field</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <fpage>4918</fpage>
        <lpage>4927</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa598</pub-id>
        <pub-id pub-id-type="pmid">32597948</pub-id>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Identification and characterization of cold-responsive microRNAs in tea plant (<italic>Camellia sinensis</italic>) and their targets using high-throughput sequencing and degradome analysis</article-title>
        <source>BMC Plant Biol</source>
        <year>2014</year>
        <volume>14</volume>
        <fpage>1</fpage>
        <lpage>18</lpage>
        <pub-id pub-id-type="doi">10.1186/1471-2229-14-1</pub-id>
        <pub-id pub-id-type="pmid">24387633</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
