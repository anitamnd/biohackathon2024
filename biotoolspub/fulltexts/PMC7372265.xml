<?DTDIdentifier.IdentifierValue http://null/schema/dtds/document/fulltext/xcr/xocs-article.xsd?>
<?DTDIdentifier.IdentifierType schema?>
<?SourceDTD.DTDName xocs-article.xsd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName ftrr2jats.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Med Image Anal</journal-id>
    <journal-id journal-id-type="iso-abbrev">Med Image Anal</journal-id>
    <journal-title-group>
      <journal-title>Medical Image Analysis</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1361-8415</issn>
    <issn pub-type="epub">1361-8423</issn>
    <publisher>
      <publisher-name>Elsevier B.V.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7372265</article-id>
    <article-id pub-id-type="publisher-id">S1361-8415(20)30158-4</article-id>
    <article-id pub-id-type="doi">10.1016/j.media.2020.101794</article-id>
    <article-id pub-id-type="publisher-id">101794</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Deep-COVID: Predicting COVID-19 from chest X-ray images using deep transfer learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au0001">
        <name>
          <surname>Minaee</surname>
          <given-names>Shervin</given-names>
        </name>
        <email>sminaee@snap.com</email>
        <xref rid="cor0001" ref-type="corresp">⁎</xref>
        <xref rid="aff0001" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au0002">
        <name>
          <surname>Kafieh</surname>
          <given-names>Rahele</given-names>
        </name>
        <email>rkafieh@amt.mui.ac.ir</email>
        <xref rid="cor0002" ref-type="corresp">⁎⁎</xref>
        <xref rid="aff0002" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author" id="au0003">
        <name>
          <surname>Sonka</surname>
          <given-names>Milan</given-names>
        </name>
        <xref rid="aff0003" ref-type="aff">c</xref>
      </contrib>
      <contrib contrib-type="author" id="au0004">
        <name>
          <surname>Yazdani</surname>
          <given-names>Shakib</given-names>
        </name>
        <xref rid="aff0004" ref-type="aff">d</xref>
      </contrib>
      <contrib contrib-type="author" id="au0005">
        <name>
          <surname>Jamalipour Soufi</surname>
          <given-names>Ghazaleh</given-names>
        </name>
        <xref rid="aff0005" ref-type="aff">e</xref>
      </contrib>
      <aff id="aff0001"><label>a</label>Snap Inc., Seattle, WA, USA</aff>
      <aff id="aff0002"><label>b</label>Medical Image and Signal Processing Research Center, Isfahan University of Medical Sciences, Iran</aff>
      <aff id="aff0003"><label>c</label>Iowa Institute for Biomedical Imaging, The University of Iowa, Iowa City, USA</aff>
      <aff id="aff0004"><label>d</label>ECE Department, Isfahan University of Technology, Iran</aff>
      <aff id="aff0005"><label>e</label>Radiology Department, Isfahan University of Medical Sciences, Isfahan, Iran</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor0001"><label>⁎</label>Corresponding author. <email>sminaee@snap.com</email></corresp>
      <corresp id="cor0002"><label>⁎⁎</label>Corresponding author. <email>rkafieh@amt.mui.ac.ir</email></corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>21</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="ppub">
      <month>10</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>21</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <volume>65</volume>
    <fpage>101794</fpage>
    <lpage>101794</lpage>
    <history>
      <date date-type="received">
        <day>24</day>
        <month>4</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>9</day>
        <month>7</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>17</day>
        <month>7</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2020 Elsevier B.V. All rights reserved.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder/>
      <license>
        <license-p>Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.</license-p>
      </license>
    </permissions>
    <abstract abstract-type="author-highlights" id="absh001">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="lst0001">
          <list-item id="lstitem0001">
            <label>•</label>
            <p id="p0001">Preparing a dataset of around 5000 X-ray images for COVID-19 detection.</p>
          </list-item>
          <list-item id="lstitem0002">
            <label>•</label>
            <p id="p0002">Training 4 state-of-the-art convolutional networks for COVID-19 detection.</p>
          </list-item>
          <list-item id="lstitem0003">
            <label>•</label>
            <p id="p0003">Presenting the sensitivity, specificity, ROC curve, AOC, and confusion matrix for each model.</p>
          </list-item>
          <list-item id="lstitem0004">
            <label>•</label>
            <p id="p0004">Achieving sensitivity and specificity rate of higher than 90% with high confidence interval.</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <abstract abstract-type="graphical" id="absh0002">
      <title>Graphical abstract</title>
      <fig id="fig0015" position="anchor">
        <graphic xlink:href="fx1_lrg"/>
      </fig>
    </abstract>
    <abstract id="abs0001">
      <p>The COVID-19 pandemic is causing a major outbreak in more than 150 countries around the world, having a severe impact on the health and life of many people globally. One of the crucial step in fighting COVID-19 is the ability to detect the infected patients early enough, and put them under special care. Detecting this disease from radiography and radiology images is perhaps one of the fastest ways to diagnose the patients. Some of the early studies showed specific abnormalities in the chest radiograms of patients infected with COVID-19. Inspired by earlier works, we study the application of deep learning models to detect COVID-19 patients from their chest radiography images. We first prepare a dataset of 5000 Chest X-rays from the publicly available datasets. Images exhibiting COVID-19 disease presence were identified by board-certified radiologist. Transfer learning on a subset of 2000 radiograms was used to train four popular convolutional neural networks, including ResNet18, ResNet50, SqueezeNet, and DenseNet-121, to identify COVID-19 disease in the analyzed chest X-ray images. We evaluated these models on the remaining 3000 images, and most of these networks achieved a sensitivity rate of 98% ( ±  3%), while having a specificity rate of around 90%. Besides sensitivity and specificity rates, we also present the receiver operating characteristic (ROC) curve, precision-recall curve, average prediction, and confusion matrix of each model. We also used a technique to generate heatmaps of lung regions potentially infected by COVID-19 and show that the generated heatmaps contain most of the infected areas annotated by our board certified radiologist. While the achieved performance is very encouraging, further analysis is required on a larger set of COVID-19 images, to have a more reliable estimation of accuracy rates. The dataset, model implementations (in PyTorch), and evaluations, are all made publicly available for research community at <ext-link ext-link-type="uri" xlink:href="https://github.com/shervinmin/DeepCovid.git" id="intrrf0002">https://github.com/shervinmin/DeepCovid.git</ext-link></p>
    </abstract>
    <kwd-group id="keys0001">
      <title>Keywords</title>
      <kwd>COVID-19</kwd>
      <kwd>X-ray imaging</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Transfer learning</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec0001">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0005">Since December 2019, a novel corona-virus (SARS-CoV-2) has spread from Wuhan to the whole China, and many other countries. By April 18, more than 2 million confirmed cases, and more than 150,000 deaths were reported in the world (<ext-link ext-link-type="uri" xlink:href="https://www.worldometers.info/coronavirus/" id="intrrf0003">https://www.worldometers.info/coronavirus/</ext-link>). Due to unavailability of therapeutic treatment or vaccine for novel COVID-19 disease, early diagnosis is of real importance to provide the opportunity of immediate isolation of the suspected person and to decrease the chance of infection to healthy population. Reverse transcription polymerase chain reaction (RT-PCR) or gene sequencing for respiratory or blood specimens are introduced as main screening methods for COVID-19 (<xref rid="bib0017" ref-type="bibr">Wang et al., 2020</xref>). However, total positive rate of RT-PCR for throat swab samples is reported to be 30 to 60%, which accordingly yields to un-diagnosed patients, which may contagiously infect a huge population of healthy people (<xref rid="bib0018" ref-type="bibr">Yang et al., 2020</xref>). Chest radiography imaging (e.g., X-ray or computed tomography (CT) imaging) as a routine tool for pneumonia diagnosis is easy to perform with fast diagnosis. Chest CT has a high sensitivity for diagnosis of COVID-19 (<xref rid="bib0001" ref-type="bibr">Ai et al., 2020</xref>) and X-ray images show visual indexes correlated with COVID-19 (<xref rid="bib0010" ref-type="bibr">Kanne et al., 2020</xref>). The reports of chest imaging demonstrated multilobar involvement and peripheral airspace opacities. The opacities most frequently reported are ground-glass (57%) and mixed attenuation (29%) (<xref rid="bib0011" ref-type="bibr">Kong and Agarwal, 2020</xref>). During the early course of COVID-19, ground glass pattern is seen in areas that edges the pulmonary vessels and may be difficult to appreciate visually (<xref rid="bib0005" ref-type="bibr">Hansell et al., 2008</xref>). Asymmetric patchy or diffuse airspace opacities are also reported for COVID-19 (<xref rid="bib0016" ref-type="bibr">Rodrigues, 2020</xref>). Such subtle abnormalities can only be interpreted by expert radiologists. Considering huge rate of suspected people and limited number of trained radiologists, automatic methods for identification of such subtle abnormalities can assist the diagnosis procedure and increase the rate of early diagnosis with high accuracy. Artificial intelligence (AI)/machine learning solutions are potentially powerful tools for solving such problems.</p>
    <p id="p0006">So far, due to the lack of availability of public images of COVID-19 patients, detailed studies reporting solutions for automatic detection of COVID-19 from X-ray (or Chest CT) images are not available. Recently a small dataset of COVID-19 X-ray images was collected, which made it possible for AI researchers to train machine learning models to perform automatic COVID-19 diagnostics from X-ray images (<xref rid="bib0003" ref-type="bibr">Cohen et al., 2020</xref>). These images were extracted from academic publications reporting the results on COVID-19 X-ray and CT images. With the help of a board-certified radiologist, we re-labeled those images, and only kept ones a clear sign of COVID-19 as determined by our radiologist. Three sample images with their corresponding marked areas are shown in <xref rid="fig0001" ref-type="fig">Fig. 1</xref>
. We then used a subset of images from ChexPert (<xref rid="bib0009" ref-type="bibr">Irvin et al., 2019</xref>) dataset, as the negative samples for COVID-19 detection. The combined dataset has around 5000 Chest X-ray images (called COVID-Xray-5k), which is divided into 2000 training, and 3000 testing samples.<fig id="fig0001"><label>Fig. 1</label><caption><p>Three sample COVID-19 images, and the corresponding marked areas by our radiologist.</p></caption><alt-text id="at0001">Fig. 1</alt-text><graphic xlink:href="gr1_lrg"/></fig></p>
    <p id="p0007">A machine a learning framework was employed to predict COVID-19 from Chest X-ray images. Unlike the classical approaches for medical image classification which follow a two-step procedure (hand-crafted feature extraction+recognition), we use an end-to-end deep learning framework which directly predicts the COVID-19 disease from raw images without any need of feature extraction. Deep learning based models (and more specifically convolutional neural networks (CNN)) have been shown to outperform the classical AI approaches in most of computer vision and and medical image analysis tasks in recent years, and have been used in a wide range of problems from classification, segmentation, face recognition, to super-resolution and image enhancement (<xref rid="bib0013" ref-type="bibr">LeCun, et al., 1998</xref>, <xref rid="bib0002" ref-type="bibr">Badrinarayanan, Kendall, Cipolla, 2017</xref>, <xref rid="bib0015" ref-type="bibr">Ren, He, Girshick, Sun, 2015</xref>, <xref rid="bib0004" ref-type="bibr">Dong, et al., 2014</xref>, <xref rid="bib0014" ref-type="bibr">Minaee, Abdolrashidi, Su, Bennamoun, Zhang</xref>).</p>
    <p id="p0008">Here, we train 4 popular convolutional networks which have achieved promising results in several tasks during recent years (including ResNet18, ResNet50, SqueezeNet, and DenseNet-161) on COVID-Xray-5k dataset, and analyze their performance for COVID-19 detection. Since so far there is a limited number of X-ray images publicly available for the COVID-19 class, we cannot simply train these models from scratch. Two strategies were adopted to address the COVID-19 image scarcity issue in this work:<list list-type="simple" id="lst0002"><list-item id="lstitem0005"><label>•</label><p id="p0009">We use data augmentation to create transformed version of COVID-19 images (such as flipping, small rotation, adding small amount of distortions), to increase the number of samples by a factor of 5.</p></list-item><list-item id="lstitem0006"><label>•</label><p id="p0010">Instead of training these models from scratch, we fine-tune the last layer of the pre-trained version of these models on ImageNet. In this way, the model can be trained with less labeled samples from each class.</p></list-item></list>
</p>
    <p id="p0011">The above two strategies helped train these networks with the available images, and achieve reasonable performance on the test set of 3000 images. Since the number of samples for the COVID-19 class is limited, we also calculate the confidence interval of the performance metrics. To report a summarizing performance of these models, we provide the Receiver operating characteristic (ROC) curve, and area under the curve (AUC) for each of these models.</p>
    <p id="p0012">Here are the main contributions of this paper:<list list-type="simple" id="lst0003"><list-item id="lstitem0007"><label>•</label><p id="p0013">We prepared a dataset of 5000 images with binary labels, for COVID-19 detection from Chest X-ray images. This dataset can serve as a benchmark for the research community. The images in COVID-19 class, are labeled by a board-certified radiologist, and only those with a clear sign are used for testing purpose.</p></list-item><list-item id="lstitem0008"><label>•</label><p id="p0014">We trained four promising deep learning models on this dataset, and evaluated their performance on a test set of 3000 images. Our best performing model achieved a sensitivity rate of 98%, while having a specificity of 92%.</p></list-item><list-item id="lstitem0009"><label>•</label><p id="p0015">We provided a detailed experimental analysis on the performance of these models, in terms of sensitivity, specificity, ROC curve, area under the curve, precision-recall curve, and histogram of the predicted scores.</p></list-item><list-item id="lstitem0010"><label>•</label><p id="p0016">We provided the heatmaps of the most likely regions, which are infected due to Covid-19, using a deep visualization technique.</p></list-item><list-item id="lstitem0011"><label>•</label><p id="p0017">We made the dataset, the trained models, and the implementation publicly available.</p></list-item></list>
</p>
    <p id="p0018">It is worth to mention that while very encouraging, given the amount of the labeled data the result of this work is still preliminary and more concrete conclusion requires further experiments on a larger dataset of COVID-19 labeled X-ray images. We believe this work can serve as a benchmark for future works and comparisons.</p>
    <p id="p0019">The structure of the rest of this paper is as follows. <xref rid="sec0002" ref-type="sec">Section 2</xref> provides a summary of the prepared <bold>COVID-Xray-5k Dataset</bold>. <xref rid="sec0003" ref-type="sec">Section 3</xref> presents the description of the overall proposed framework. <xref rid="sec0009" ref-type="sec">Section 4</xref> provides the experimental studies and comparison with previous works. And finally the paper is concluded in <xref rid="sec0017" ref-type="sec">Section 5</xref>.</p>
  </sec>
  <sec id="sec0002">
    <label>2</label>
    <title>COVID-Xray-5k Ddataset</title>
    <p id="p0020">Chest X-ray images from two datasets formed the COVID-Xray-5k dataset that contains 2084 training and 3100 test images.</p>
    <p id="p0021">One of the used datasets is the recently published <bold>Covid-Chestxray-Dataset</bold>, which contains a set of images from publications on COVID-19 topics, collected by <ext-link ext-link-type="uri" xlink:href="https://github.com/ieee8023/covid-chestxray-dataset" id="intrrf0004">https://github.com/ieee8023/covid-chestxray-dataset</ext-link>, <xref rid="bib0003" ref-type="bibr">Cohen et al. (2020)</xref>. This dataset contains a mix of chest X-ray and CT images. As of May 3, 2020, it contained 250 X-ray images of COVID-19 patients, from which 203 images are anterior-posterior view. It is mentioned that this dataset is continuously updated. It also contains some meta-data about each patients, such as sex and age. Our COVID-19 images are all coming from this dataset. Based on our board-certified radiologist advice, only anterior-posterior images are kept for Covid-19 prediction, as the lateral images are not suitable for this purpose. The anterior-posterior images were examined by our board-certified radiologist, and the ones without even the slightest radiographic signs of Covid-19 were removed from dataset. Out of 203 interior-exterior X-ray images of COVID-19, 19 of them were excluded, and 184 images (which showed clear signs of COVID-19) were kept by our radiologist. This way, we can provide the community a more cleanly labeled dataset. Out of these images, we chose 100 COVID-19 images to include in the test set (to meet some maximum confidence interval value), and 84 COVID-19 images for the training set. Data augmentation is applied to the training set to increase the number of COVID-19 samples to 420 as described above. We made sure all images for each patient go only to one of the training or test sets. It is worth mentioning that our radiologist marked the regions with specific signs of Covid-19.</p>
    <p id="p0022">Since the number of Non-Covid images was very small in the (<ext-link ext-link-type="uri" xlink:href="https://github.com/ieee8023/covid-chestxray-dataset" id="intrrf0005">https://github.com/ieee8023/covid-chestxray-dataset</ext-link>) dataset, additional images were employed from the <bold>ChexPert</bold> dataset (<xref rid="bib0009" ref-type="bibr">Irvin et al., 2019</xref>), a large public dataset for chest radiograph interpretation consisting of 224,316 chest radiographs of 65,240 patients, labeled for the presence of 14 sub-categories (no-finding, Edema, Pneumonia, etc.). For the non-COVID samples in the training set, we only used images belonging to a single sub-category, composed of 700 images from the no-finding class and 100 images from each remaining 13 sub-classes, resulting in 2000 non-COVID images.</p>
    <p id="p0023">As for the Non-COVID samples in the test dataset, we selected 1700 images from the no-finding category and around 100 images from each remaining 13 sub-classes in distinct sub-folders, resulting in 3000 images in total. The exact number of images of each class for both training and testing is given in <xref rid="tbl0001" ref-type="table">Table 1</xref>
.<table-wrap position="float" id="tbl0001"><label>Table 1</label><caption><p>Number of images per category in COVID-Xray-5k dataset.</p></caption><alt-text id="at0015">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Split</th><th align="left" valign="top">COVID-19</th><th align="left" valign="top">Non-COVID</th></tr></thead><tbody><tr><td valign="top">Training Set</td><td valign="top">84 (420 after augmentation)</td><td valign="top">2000</td></tr><tr><td valign="top">Test Set</td><td valign="top">100</td><td valign="top">3000</td></tr></tbody></table></table-wrap></p>
    <p id="p0024"><xref rid="fig0002" ref-type="fig">Fig. 2</xref>
shows 16 sample images from COVID-Xray-5k dataset, including 4 COVID-19 images (the first row), 4 normal images from ChexPert (the second row), and 8 images with one of the 13 diseases in ChexPert (third and fourth rows).<fig id="fig0002"><label>Fig. 2</label><caption><p>Sample images from COVID-Xray-5k dataset. The images in the first row show 4 COVID-19 images. The images in the second row are 4 sample images of no-finding category in Non-COVID images from <bold>ChexPert</bold>. The images in the third and fourth rows give 8 sample images from other sub-categotries in <bold>ChexPert</bold>.</p></caption><alt-text id="at0002">Fig. 2</alt-text><graphic xlink:href="gr2_lrg"/></fig></p>
    <p id="p0025">It is worth mentioning that, there is wide variation inn the resolution of images in this dataset. There are some low-resolution images in Covid-19 class (below 400 × 400), and some high resolutions ones (more than 1900 × 1400). This is a positive point for the models that can achieve a reasonable high accuracy on this dataset, despite this variable image resolution and imagery methodology. Collecting all images in a super-controlled environment that results in high-resolution and super-clean images, although desired, is not always doable, and as machine learning field progresses, more and more focus is directed toward models and frameworks that can work reasonably well on variable resolution, quality, and small-scale labeled datasets. Also the images of Covid-19 class are collected from multiple sources by the original provider, and some of them may show a different dynamic range from other ones (and also from ChexPert), but during the training all images are normalized to the same distribution to make model less sensitive to that.</p>
  </sec>
  <sec id="sec0003">
    <label>3</label>
    <title>The proposed framework</title>
    <p id="p0026">To overcome the limited data sizes, transfer learning was used to fine-tune four popular pre-trained deep neural networks on the training images of COVID-Xray-5k dataset.</p>
    <sec id="sec0004">
      <label>3.1</label>
      <title>Transfer learning approach</title>
      <p id="p0027">In transfer learning, a model trained on one task is re-purposed to another related task, usually by some adaptation toward the new task. For example, one can imagine using an image classification model trained on ImageNet (which contains millions of labeled images) to initiate task-specific learning for COVID-19 detection on a smaller dataset. Transfer learning is mainly useful for tasks where enough training samples are not available to train a model from scratch, such as medical image classification for rare or emerging diseases. This is especially the case for models based on deep neural networks, which have a large number of parameters to train. By using transfer learning, the model parameters start with already-good initial values that only need some small modifications to be better curated toward the new task.</p>
      <p id="p0028">There are two main ways in which the pre-trained model is used for a different task. In one approach, the pre-trained model is treated as a feature extractor (i.e., the internal weights of the pre-trained model are not adapted to the new task), and a classifier is trained on top of it to perform classification. In another approach, the whole network, or a subset thereof, is fine-tuned on the new task. Therefore the pre-trained model weights are treated as the initial values for the new task, and are updated during the training stage.</p>
      <p id="p0029">In our case, since the number of images in the COVID-19 category is very limited, we only fine-tune the last layer of the convolutional neural networks, and essentially use the pre-trained models as a feature extractor. We evaluate the performance of four popular pre-trained models, ResNet18 (<xref rid="bib0006" ref-type="bibr">He, 2016</xref>), ResNet50 (<xref rid="bib0006" ref-type="bibr">He, 2016</xref>), SqueezeNet (<xref rid="bib0008" ref-type="bibr">Iandola et al., 2016</xref>), and DenseNet-121 (<xref rid="bib0007" ref-type="bibr">Huang et al., 2017</xref>). In the next section we provide a quick overview of the architecture of these models, and how they are used for COVID-19 recognition.</p>
    </sec>
    <sec id="sec0005">
      <label>3.2</label>
      <title>COVID-19 Detection using residual ConvNet – ResNet18 and ResNet50</title>
      <p id="p0030">One of the models used in this work, is the pre-trained ResNet18, trained on ImageNet dataset. ResNet is one of the most popular CNN architecture, which provides easier gradient flow for more efficient training, and was the winner of the 2015 ImageNet competition. The core idea of ResNet is introducing a so-called <italic>identity shortcut connection</italic> that skips one or more layers. This would help the network to provide a direct path to the very early layers in the network, making the gradient updates for those layers much easier.</p>
      <p id="p0031">The overall block diagram of ResNet18 model, and how it is used for COVID-19 detection is illustrated in <xref rid="fig0003" ref-type="fig">Fig. 3</xref>
. ResNet50 architecture is pretty similar to ResNet18, the main difference being having more layers.<fig id="fig0003"><label>Fig. 3</label><caption><p>The architecture of ResNet18 model (<xref rid="bib0006" ref-type="bibr">He, 2016</xref>).</p></caption><alt-text id="at0003">Fig. 3</alt-text><graphic xlink:href="gr3_lrg"/></fig></p>
    </sec>
    <sec id="sec0006">
      <label>3.3</label>
      <title>COVID-19 Detection using SqueezeNet</title>
      <p id="p0032">SqueezeNet (<xref rid="bib0008" ref-type="bibr">Iandola et al., 2016</xref>) proposed by Iandola et al., is a small CNN architecture, which achieves AlexNet-level (<xref rid="bib0012" ref-type="bibr">Krizhevsky et al., 2012</xref>) accuracy on ImageNet with 50 ×  fewer parameters. Using model compression techniques, the authors were able to compress SqueezeNet to less than 0.5MB, which made it very popular for applications that require light-weight models. They alternate a 1 × 1 layer that “squeezes” the incoming data in the vertical dimension followed by two parallel 1 × 1 and 3 × 3 convolutional layers that “expand” the depth of the data again. Three main strategies used in SqueezeNet includes: replace 3 × 3 filters with 1 × 1 filters, decrease the number of input channels to 3 × 3 filters, Down-sample late in the network so that convolution layers have large activation maps. <xref rid="fig0004" ref-type="fig">Fig. 4</xref>
shows the architecture of a simple SqueezeNet.<fig id="fig0004"><label>Fig. 4</label><caption><p>The architecture of SqueezeNet based on “fire modules”. Courtesy of Google (<ext-link ext-link-type="uri" xlink:href="https://codelabs.developers.google.com/codelabs/keras-flowers-squeezenet/" id="intrrf0001">https://codelabs.developers.google.com/codelabs/keras-flowers-squeezenet/</ext-link>).</p></caption><alt-text id="at0004">Fig. 4</alt-text><graphic xlink:href="gr4_lrg"/></fig></p>
    </sec>
    <sec id="sec0007">
      <label>3.4</label>
      <title>COVID-19 Detection using DenseNet</title>
      <p id="p0033">Dense Convolutional Network (DenseNet) is another popular architecture (<xref rid="bib0007" ref-type="bibr">Huang et al., 2017</xref>), which was the winner of the 2017 ImageNet competition. In DenseNet, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. Each layer is receiving a âǣcollective knowledgeâǥ from all preceding layers. Since each layer receives feature maps from all preceding layers, network can be thinner and compact, i.e., number of channels can be fewer (so, it have higher computational efficiency and memory efficiency). The architecture of sample DenseNet is shown in <xref rid="fig0005" ref-type="fig">Fig. 5</xref>
.<fig id="fig0005"><label>Fig. 5</label><caption><p>The architecture of a DenseNet with 5 layers, with expansion of 4. Courtesy of model (<xref rid="bib0007" ref-type="bibr">Huang et al., 2017</xref>).</p></caption><alt-text id="at0005">Fig. 5</alt-text><graphic xlink:href="gr5_lrg"/></fig></p>
    </sec>
    <sec id="sec0008">
      <label>3.5</label>
      <title>Model training</title>
      <p id="p0034">All employed models are trained with a cross-entropy loss function, which tries to minimize the distance between the predicted probability scores, and the ground truth probabilities (derived from labels), and is defined as:<disp-formula id="eq0001"><label>(1)</label><mml:math id="M1" altimg="si1.svg"><mml:mrow><mml:msub><mml:mi mathvariant="bold-script">L</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>log</mml:mi><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>p<sub>i</sub></italic> and <italic>q<sub>i</sub></italic> denote the ground-truth, and predicted probabilities for each image, respectively. We can then minimize this loss function using stochastic gradient descent algorithm (and its variations). We attempted to add regularization to the loss function, but the resulting model was not exhibiting a better performance.</p>
    </sec>
  </sec>
  <sec id="sec0009">
    <label>4</label>
    <title>Experimental results</title>
    <sec id="sec0010">
      <label>4.1</label>
      <title>Model hyper-parameters</title>
      <p id="p0035">We fine-tuned each model for 100 epochs. The batch size is set to 20, and ADAM optimizer is used to optimize the loss function, with a learning rate of 0.0001. All images are down-sampled to 224 × 224 before being fed to the neural network (as these pre-trained models are usually trained with a specific image resolution). All our implementations are done in PyTorch (<ext-link ext-link-type="uri" xlink:href="https://pytorch.org/" id="intrrf0006">https://pytorch.org/</ext-link>), and are publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/shervinmin/DeepCovid.git" id="intrrf0007">https://github.com/shervinmin/DeepCovid.git</ext-link>.</p>
    </sec>
    <sec id="sec0011">
      <label>4.2</label>
      <title>Evaluation metrics</title>
      <p id="p0036">There are different metrics which can be used for evaluating the performance of classification models, such as classification accuracy, sensitivity, specificity, precision, and F1-score. Since the current test dataset is highly imbalanced (100 COVID-19 images, 3000 Non-COVID image), sensitivity and specificity are two proper metrics which can be used for reporting the model performance:<disp-formula id="eq0002"><label>(2)</label><mml:math id="M2" altimg="si2.svg"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi mathvariant="bold">Sensitivity</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>#</mml:mo><mml:mtext>Images</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>correctly</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>predicted</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>as</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>COVID-19</mml:mtext></mml:mrow><mml:mrow><mml:mo>#</mml:mo><mml:mtext>Total</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>COVID-19</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>Images</mml:mtext></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mi mathvariant="bold">Specificity</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>#</mml:mo><mml:mtext>Images</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>correctly</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>predicted</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>as</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>Non-COVID</mml:mtext></mml:mrow><mml:mrow><mml:mo>#</mml:mo><mml:mtext>Total</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>Non-COVID</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>Images</mml:mtext></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p>
    </sec>
    <sec id="sec0012">
      <label>4.3</label>
      <title>Model predicted scores</title>
      <p id="p0037">As mentioned earlier, we focused on four popular convolutional networks, ResNet18, ResNet50, SqueezeNet, DenseNet121. These models predict a probability score for each image, which shows the likelihood of the image being detected as COVID-19. By comparing this probability with a cut-off threshold, we can derive a binary label showing if the image is COVID-19 or not. An ideal model should predict the probability of all COVID-19 samples close to 1, and non-COVID samples close to 0.</p>
      <p id="p0038"><xref rid="fig0006" ref-type="fig">Figs. 6</xref>
–<xref rid="fig0009" ref-type="fig">9</xref>
show the distribution of predicted probability scores for the images in the test set, by ResNet18, ResNet50, SqueezeNet, and DenseNet-161 respectively. Since Non-COVID class in our study contains both normal cases, as well as other types of diseases, we provide the distribution of predicted scores for three classes: COVID-19, Non-COVID normal, and Non-COVID other diseases. As we can see the Non-Covid images with other disease types have slightly larger scores than the Non-COVID normal cases. This makes sense, since those images are more difficult to distinguish from COVID-19, than normal samples.<fig id="fig0006"><label>Fig. 6</label><caption><p>The predicted probability scores by ResNet18 on the test set.</p></caption><alt-text id="at0006">Fig. 6</alt-text><graphic xlink:href="gr6_lrg"/></fig><fig id="fig0007"><label>Fig. 7</label><caption><p>The predicted probability scores by ResNet50 on the test set.</p></caption><alt-text id="at0007">Fig. 7</alt-text><graphic xlink:href="gr7_lrg"/></fig><fig id="fig0008"><label>Fig. 8</label><caption><p>The predicted probability scores by SqueezeNet on the test set.</p></caption><alt-text id="at0008">Fig. 8</alt-text><graphic xlink:href="gr8_lrg"/></fig><fig id="fig0009"><label>Fig. 9</label><caption><p>The predicted probability scores by DesneNet-121 on the test set.</p></caption><alt-text id="at0009">Fig. 9</alt-text><graphic xlink:href="gr9_lrg"/></fig></p>
      <p id="p0039">COVID-19 patient images are predicted to have much higher probabilities than the Non-COVID images, which is really encouraging, as it shows the model is learning to discriminate COVID-19 from non-COVID images. Among different models, it can be observed that SqueezeNet does a much better job in pushing the predicted scores for COVID-19 and Non-COVID images farther apart from each other.</p>
    </sec>
    <sec id="sec0013">
      <label>4.4</label>
      <title>Model sensitivity and specificity</title>
      <p id="p0040">Each model predicts a probability score showing the chance of the image being COVID-19. We can then compare these scores with a threshold to infer if the image is COVID-19 or not. The predicted labels are used to estimate the sensitivity and specificity of each model. Depending on the value of the cut-off threshold, we can get different sensitivity and specificity rates for each model.</p>
      <p id="p0041"><xref rid="tbl0002" ref-type="table">Table 2</xref>, <xref rid="tbl0003" ref-type="table">Table 3</xref>, <xref rid="tbl0004" ref-type="table">Table 4</xref>, <xref rid="tbl0005" ref-type="table">Table 5</xref>
show the sensitivity and specificity rates for different thresholds, using ResNet18, ResNet50, SqueezeNet, and DenseNet-121 models, respectively. As we can see, all these models achieve very promising results, and the best performing model obtains a sensitivity rate of 98% and specificity rate of 92.9%. SqueezeNet and ResNet18 achieve slightly better performance than the other models.<table-wrap position="float" id="tbl0002"><label>Table 2</label><caption><p>Sensitivity and specificity rates of ResNet18 model, for different threshold values.</p></caption><alt-text id="at0016">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Threshold</th><th align="left" valign="top">Sensitivity</th><th align="left" valign="top">Specificity</th></tr></thead><tbody><tr><td valign="top">0.1</td><td valign="top">100%</td><td valign="top">72.4%</td></tr><tr><td valign="top">0.17</td><td valign="top">98%</td><td valign="top">90.7%</td></tr><tr><td valign="top">0.2</td><td valign="top">95%</td><td valign="top">92.4%</td></tr><tr><td valign="top">0.25</td><td valign="top">91%</td><td valign="top">95.8%</td></tr><tr><td valign="top">0.35</td><td valign="top">85%</td><td valign="top">98.3%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl0003"><label>Table 3</label><caption><p>Sensitivity and specificity rates of ResNet50 model, for different threshold values.</p></caption><alt-text id="at0017">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Threshold</th><th align="left" valign="top">Sensitivity</th><th align="left" valign="top">Specificity</th></tr></thead><tbody><tr><td valign="top">0.15</td><td valign="top">100%</td><td valign="top">78.2%</td></tr><tr><td valign="top">0.205</td><td valign="top">98%</td><td valign="top">89.6%</td></tr><tr><td valign="top">0.25</td><td valign="top">93%</td><td valign="top">94.2%</td></tr><tr><td valign="top">0.3</td><td valign="top">90%</td><td valign="top">97.3%</td></tr><tr><td valign="top">0.35</td><td valign="top">85%</td><td valign="top">98.4%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl0004"><label>Table 4</label><caption><p>Sensitivity and specificity rates of SqueezeNet model, for different threshold values.</p></caption><alt-text id="at0018">Table 4</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Threshold</th><th align="left" valign="top">Sensitivity</th><th align="left" valign="top">Specificity</th></tr></thead><tbody><tr><td valign="top">0.1</td><td valign="top">100%</td><td valign="top">89.9%</td></tr><tr><td valign="top">0.15</td><td valign="top">98%</td><td valign="top">92.9%</td></tr><tr><td valign="top">0.2</td><td valign="top">96.0%</td><td valign="top">94.6%</td></tr><tr><td valign="top">0.4</td><td valign="top">92%</td><td valign="top">97.6%</td></tr><tr><td valign="top">0.5</td><td valign="top">87%</td><td valign="top">98.3%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl0005"><label>Table 5</label><caption><p>Sensitivity and specificity rates of DenseNet-121 model, for different threshold values.</p></caption><alt-text id="at0019">Table 5</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Threshold</th><th align="left" valign="top">Sensitivity</th><th align="left" valign="top">Specificity</th></tr></thead><tbody><tr><td valign="top">0.19</td><td valign="top">98%</td><td valign="top">75.1%</td></tr><tr><td valign="top">0.25</td><td valign="top">95%</td><td valign="top">88.9%</td></tr><tr><td valign="top">0.3</td><td valign="top">90%</td><td valign="top">94.6%</td></tr><tr><td valign="top">0.4</td><td valign="top">79%</td><td valign="top">98.9%</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="sec0014">
      <label>4.5</label>
      <title>Small number of COVID-19 cases and model reliability</title>
      <p id="p0042">It is worth mentioning that since so far the number of reliably labeled COVID-19 X-ray images is very limited, and we only have 100 test images in COVID-19 class, the sensitivity and specificity rates reported above may not be reliable. Ideally more experiments on a larger number of test samples with COVID-19 is needed to derive a more reliable estimation of sensitivity rates. We can however estimate the 95% confidence interval of the reported sensitivity and specificity rates here, to see what is the possible range of these values for the current number of test samples in each class. The confidence interval of the accuracy rates can be calculated as:<disp-formula id="eq0003"><label>(3)</label><mml:math id="M3" altimg="si3.svg"><mml:mrow><mml:mi>r</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>z</mml:mi><mml:msqrt><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:msqrt><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>z</italic> denotes the significance level of the confidence interval (the number of standard deviation of the Gaussian distribution), accuracy is the estimated accuracy (in our cases sensitivity and specificity), and <italic>N</italic> denotes the number of samples for that class. Here we used 95% confidence interval, for which the corresponding value of <italic>z</italic> is 1.96.</p>
      <p id="p0043">As for COVID-19 diagnostic, having a sensitive model is crucial, we choose the cut-off threshold corresponding to a sensitivity rate of 98% for each model, and compare their specificity rates. <xref rid="tbl0006" ref-type="table">Table 6</xref>
provides a comparison of the performance of these four models on the test set. As we can see the confidence interval of specificity rates are small (around 1%), since we have around 3000 samples for this class, whereas for the sensitivity rate we get slightly higher confidence interval (around 2.7%) because of the limited number of samples.<table-wrap position="float" id="tbl0006"><label>Table 6</label><caption><p>Comparison of sensitivity and specificity of four state-of-the-art deep neural networks.</p></caption><alt-text id="at0020">Table 6</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Model</th><th align="left" valign="top">Sensitivity</th><th align="left" valign="top">Specificity</th></tr></thead><tbody><tr><td valign="top">ResNet18</td><td valign="top">98%  ±  2.7%</td><td valign="top">90.7%  ±  1.1%</td></tr><tr><td valign="top">ResNet50</td><td valign="top">98%  ±  2.7%</td><td valign="top">89.6%  ±  1.1%</td></tr><tr><td valign="top">SqueezeNet</td><td valign="top">98%  ±  2.7%</td><td valign="top">92.9%  ±  0.9%</td></tr><tr><td valign="top">Densenet-121</td><td valign="top">98%  ±  2.7%</td><td valign="top">75.1%  ±  1.5%</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="sec0015">
      <label>4.6</label>
      <title>The ROC curve, precision recall curve, and confusion matrix</title>
      <p id="p0044">It is hard to compare different models only based on their sensitivity and specificity rates, since these rates change by varying the cut-off thresholds. To see the overall comparison between these models, we need to look at the comparison for all possible threshold values. One way to do this, is through the precision-recall curve, which provides the precision rate as a function of recall rate. Precision is defined as the true positive images divided by the total number of images flagged as positive by the model, and the recall is the same as sensitivity rate (defined in <xref rid="eq0002" ref-type="disp-formula">Eq. (2)</xref>). The precision-recall curve of these four models is shown in <xref rid="fig0010" ref-type="fig">Fig. 10</xref>
.<fig id="fig0010"><label>Fig. 10</label><caption><p>The precision-recall curve of four CNN architectures on test set.</p></caption><alt-text id="at0010">Fig. 10</alt-text><graphic xlink:href="gr10_lrg"/></fig></p>
      <p id="p0045">Another way to do this, is through the Receiver operating characteristic (ROC) curve, which provides the true positive rate as a function of false positive rate. The ROC curve of these four models is shown in <xref rid="fig0011" ref-type="fig">Fig. 11</xref>
. All models have a similar performance according to the AUC with the SqueezeNet achieving a slightly higher AUC than the other models. It is worth mentioning that for highly imbalanced test sets, the AUC may not be a good indicative of model performance (as it can be very high), and looking at average-precision and precision-recall curve would be a better choice in that case. Here we provided both curves for the sake of completeness.<fig id="fig0011"><label>Fig. 11</label><caption><p>The ROC curve of four CNN architectures on COVID-19 test set.</p></caption><alt-text id="at0011">Fig. 11</alt-text><graphic xlink:href="gr11_lrg"/></fig></p>
      <p id="p0046">To see the exact number of correctly samples as COVID-19 and Non-COVID, the confusion matrices of the two top-performing models – the fine-tuned ResNet18 and SqueezeNet – when classifying the set of 3100 test images are shown in <xref rid="fig0012" ref-type="fig">Figs. 12</xref>
and <xref rid="fig0013" ref-type="fig">13</xref>
.<fig id="fig0012"><label>Fig. 12</label><caption><p>The confusion matrix of the proposed ResNet18 model.</p></caption><alt-text id="at0012">Fig. 12</alt-text><graphic xlink:href="gr12_lrg"/></fig><fig id="fig0013"><label>Fig. 13</label><caption><p>The confusion matrix of the proposed SqueezeNet framework.</p></caption><alt-text id="at0013">Fig. 13</alt-text><graphic xlink:href="gr13_lrg"/></fig></p>
    </sec>
    <sec id="sec0016">
      <label>4.7</label>
      <title>The heatmap of potentially infected regions</title>
      <p id="p0047">We used a simple technique to detect the potentially infected regions, while performing COVID-19 detection. This technique is inspired by the work of <xref rid="bib0019" ref-type="bibr">Zeiler and Fergus (2014)</xref>, to visualize the result of deep convolutional networks. We start from the top-left corner of the image, and each time occluding a square region of size <italic>N</italic> × <italic>N</italic> inside the image, and make a prediction using the trained model on the occluded image. If occluding that region causes the model to mis-classify a COVID-19 image as Non-COVID, that area would be considered as a potentially infected region in chest X-ray images (mainly because removing the information of that part led to model mis-classification). On the other hand, if occluding a region does not impact the model’s prediction, we infer that region is not infected. Once we repeat this procedure for different sliding windows of <italic>N</italic> × <italic>N</italic>, each time shifting them with a stride of <italic>S</italic>, we can get a saliency map of the potentially infected regions in detecting COVID-19. The detected regions for six example COVID-19 images from our test set are shown in <xref rid="fig0014" ref-type="fig">Fig. 14</xref>
. The likely regions of COVID-19 disease marked by our board-certified radiologist are shown in blue on the last row. The generated heatmaps show a good agreement with the radiologist-determined regions of the COVID-19 disease.<fig id="fig0014"><label>Fig. 14</label><caption><p>COVID-19 infected regions detected by our ResNet18 model, in six chest X-ray images from the test set. Vertical sets give the Original images (top row), COVID-19 region heatmap (2nd row), heatmap overlaid on the image (3rd row), and the independent standard of radiologist-marked COVID-19 disease regions (bottom row).</p></caption><alt-text id="at0014">Fig. 14</alt-text><graphic xlink:href="gr14_lrg"/></fig></p>
    </sec>
  </sec>
  <sec id="sec0017">
    <label>5</label>
    <title>Conclusion</title>
    <p id="p0048">We reported a deep learning framework for COVID-19 detection from Chest X-ray images, by fine-tuning four pre-trained convolutional models (ResNet18, ResNet50, SqueezeNet, and DenseNet-121) on our training set. We prepared a dataset of around 5k images, called COVID-Xray-5k (using images from two datasets), with the help of a board-certified radiologist to confirm the COVID-19 labels. We make this dataset publicly available for the research community to use as a benchmark for training and evaluating future machine learning models for COVID-19 binary classification task. We performed a detail experimental analysis evaluating the performance of each of these 4 models on the test set of COVID-Xray-5k Dataset, in terms of sensitivity, specificity, ROC, and AUC. For a sensitivity rate of 98%, these models achieved a specificity rate of around 90% on average. This is really encouraging, as it shows the promise of using X-ray images for COVID-19 diagnostics. This study is conducted on a set of publicly available images, which contains around 200 COVID-19 images, and 5000 non-COVID images. The presented work is reflecting one of the earliest Covid-19 chest X-ray analysis and dataset preparation attempts, which brings time-sensitive relevance in combining these two aspects. However, due to the limited number of COVID-19 images publicly available so far, further experiments are needed on a larger set of cleanly labeled COVID-19 images for a more reliable estimation of the accuracy of these models.</p>
  </sec>
  <sec id="sec0017a">
    <title>CRediT authorship contribution statement</title>
    <p id="p0048a"><bold>Shervin Minaee:</bold> Conceptualization, Methodology, Writing - original draft. <bold>Rahele Kafieh:</bold> Data curation, Writing - original draft. <bold>Milan Sonka:</bold> Supervision, Writing - original draft. <bold>Shakib Yazdani:</bold> Data curation, Formal analysis. <bold>Ghazaleh Jamalipour Soufi:</bold> Data curation.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="p0049">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
  </sec>
</body>
<back>
  <ref-list id="bib001">
    <title>References</title>
    <ref id="bib0001">
      <element-citation publication-type="journal" id="sbref0001">
        <person-group person-group-type="author">
          <name>
            <surname>Ai</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Hou</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Zhan</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Lv</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Tao</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Xia</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Correlation of chest CT and RT-PCR testing in coronavirus disease 2019 (COVID-19) in China: a report of 1014 cases</article-title>
        <source>Radiology</source>
        <year>2020</year>
        <pub-id pub-id-type="doi">10.1148/radiol.2020200642</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0002">
      <element-citation publication-type="journal" id="sbref0002">
        <person-group person-group-type="author">
          <name>
            <surname>Badrinarayanan</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Kendall</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Cipolla</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>SegNet: A deep convolutional encoder-decoder architecture for image segmentation</article-title>
        <source>IEEE Trans. Pattern Anal.Mach. Intell</source>
        <volume>39</volume>
        <issue>12</issue>
        <year>2017</year>
        <fpage>2481</fpage>
        <lpage>2495</lpage>
        <pub-id pub-id-type="pmid">28060704</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0003">
      <mixed-citation publication-type="other" id="othref0001">Cohen, J. P., Morrison, P., Dao, L., 2020. COVID-19 image data collection. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2003.11597" id="intrrf0010">2003.11597</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0004">
      <element-citation publication-type="book" id="sbref0003">
        <person-group person-group-type="author">
          <name>
            <surname>Dong</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <chapter-title>Learning a deep convolutional network for image super-resolution</chapter-title>
        <source>European Conference on Computer Vision</source>
        <year>2014</year>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Cham</publisher-loc>
      </element-citation>
    </ref>
    <ref id="bib0005">
      <element-citation publication-type="journal" id="sbref0004">
        <person-group person-group-type="author">
          <name>
            <surname>Hansell</surname>
            <given-names>D.M.</given-names>
          </name>
          <name>
            <surname>Bankier</surname>
            <given-names>A.A.</given-names>
          </name>
          <name>
            <surname>MacMahon</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>McLoud</surname>
            <given-names>T.C.</given-names>
          </name>
          <name>
            <surname>Muller</surname>
            <given-names>N.L.</given-names>
          </name>
          <name>
            <surname>Remy</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Fleischner society: glossary of terms for thoracic imaging</article-title>
        <source>Radiology</source>
        <volume>246</volume>
        <issue>3</issue>
        <year>2008</year>
        <fpage>697</fpage>
        <lpage>722</lpage>
        <pub-id pub-id-type="pmid">18195376</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0006">
      <element-citation publication-type="book" id="sbref0005">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <chapter-title>Deep residual learning for image recognition</chapter-title>
        <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>
        <year>2016</year>
      </element-citation>
    </ref>
    <ref id="bib0007">
      <element-citation publication-type="book" id="sbref0006">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Maaten</surname>
            <given-names>L.V.D.</given-names>
          </name>
          <name>
            <surname>Weinberger</surname>
            <given-names>K.Q.</given-names>
          </name>
        </person-group>
        <chapter-title>Densely connected convolutional networks</chapter-title>
        <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>
        <year>2017</year>
        <fpage>4700</fpage>
        <lpage>4708</lpage>
      </element-citation>
    </ref>
    <ref id="bib0008">
      <mixed-citation publication-type="other" id="othref0002">Iandola, F. N., Han, S., Moskewicz, M. W., Ashraf, K., Dally, W. J., Keutzer, K., 2016. SqueezeNet: Alexnet-level accuracy with 50 ×  fewer parameters and  &lt; 0.5MB model size. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1602.07360" id="intrrf0011">1602.07360</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0009">
      <element-citation publication-type="book" id="sbref0007">
        <person-group person-group-type="author">
          <name>
            <surname>Irvin</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Rajpurkar</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Ko</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Ciurea-Ilcus</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Chute</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Marklund</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <chapter-title>CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison</chapter-title>
        <source>Proceedings of the AAAI Conference on Artificial Intelligence</source>
        <volume>vol. 33</volume>
        <year>2019</year>
        <fpage>590</fpage>
        <lpage>597</lpage>
      </element-citation>
    </ref>
    <ref id="bib0010">
      <element-citation publication-type="journal" id="sbref0008">
        <person-group person-group-type="author">
          <name>
            <surname>Kanne</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Little</surname>
            <given-names>B.P.</given-names>
          </name>
          <name>
            <surname>Chung</surname>
            <given-names>J.H.</given-names>
          </name>
          <name>
            <surname>Brett</surname>
            <given-names>M.E.</given-names>
          </name>
          <name>
            <surname>Ketai</surname>
            <given-names>L.H.</given-names>
          </name>
        </person-group>
        <article-title>Essentials for radiologists on COVID-19: an update - radiology scientific expert panel</article-title>
        <source>Radiology</source>
        <year>2020</year>
        <pub-id pub-id-type="doi">10.1148/radiol.2020200527</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0011">
      <element-citation publication-type="journal" id="sbref0009">
        <person-group person-group-type="author">
          <name>
            <surname>Kong</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Agarwal</surname>
            <given-names>P.P.</given-names>
          </name>
        </person-group>
        <article-title>Chest imaging appearance of COVID-19 infection</article-title>
        <source>Radiology: Cardiothorac. Imaging</source>
        <volume>2</volume>
        <issue>1</issue>
        <year>2020</year>
        <pub-id pub-id-type="doi">10.1148/ryct.2020200028</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0012">
      <element-citation publication-type="journal" id="sbref0010">
        <person-group person-group-type="author">
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G.E.</given-names>
          </name>
        </person-group>
        <article-title>ImageNet classification with deep convolutional neural networks</article-title>
        <source>Adv. Neural inform. Process.Syst.</source>
        <year>2012</year>
      </element-citation>
    </ref>
    <ref id="bib0013">
      <element-citation publication-type="journal" id="sbref0011">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Gradient-based learning applied to document recognition</article-title>
        <source>Proc. IEEE</source>
        <volume>86</volume>
        <issue>11</issue>
        <year>1998</year>
        <fpage>2278</fpage>
        <lpage>2324</lpage>
      </element-citation>
    </ref>
    <ref id="bib0014">
      <mixed-citation publication-type="other" id="othref0003">Minaee, S., Abdolrashidi, A., Su, H., Bennamoun, M., Zhang, D., 2019. Biometric recognition using deep learning: a survey. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1912.00271" id="intrrf0012">1912.00271</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0015">
      <element-citation publication-type="journal" id="sbref0012">
        <person-group person-group-type="author">
          <name>
            <surname>Ren</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Girshick</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Faster R-CNN: towards real-time object detection with region proposal networks</article-title>
        <source>Adv. Neural Inform. Process.Syst.</source>
        <year>2015</year>
      </element-citation>
    </ref>
    <ref id="bib0016">
      <element-citation publication-type="journal" id="sbref0013">
        <person-group person-group-type="author">
          <name>
            <surname>Rodrigues</surname>
            <given-names>J.C.L.</given-names>
          </name>
        </person-group>
        <article-title>An update on COVID-19 for the radiologist - a British society of thoracic imaging statement</article-title>
        <source>Clin. Radiol.</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="bib0017">
      <element-citation publication-type="journal" id="sbref0014">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Tan</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>Detection of SARS-CoV-2 in different types of clinical specimens</article-title>
        <source>JAMA</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="bib0018">
      <mixed-citation publication-type="other" id="othref0004">Yang, Y., Yang, M., Shen, C., Wang, F., Yuan, J., Li, J., Zhang, M., et al., 2020. Laboratory diagnosis and monitoring the viral shedding of 2019-nCoV infections. MedRxiv.</mixed-citation>
    </ref>
    <ref id="bib0019">
      <element-citation publication-type="book" id="sbref0015">
        <person-group person-group-type="author">
          <name>
            <surname>Zeiler</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Fergus</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <chapter-title>Visualizing and understanding convolutional networks</chapter-title>
        <source>European Conference on Computer Vision</source>
        <year>2014</year>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Cham</publisher-loc>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="sec0018" sec-type="supplementary-material">
    <label>Appendix A</label>
    <title>Supplementary materials</title>
    <p id="p0052">
      <supplementary-material content-type="local-data" id="ecom0001">
        <caption>
          <title>Supplementary Data S1</title>
          <p>Supplementary Raw Research Data. This is open data under the CC BY license <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" id="intrrf0009">http://creativecommons.org/licenses/by/4.0/</ext-link></p>
        </caption>
        <media xlink:href="mmc1.xml">
          <alt-text>Supplementary Data S1</alt-text>
        </media>
      </supplementary-material>
    </p>
  </sec>
  <ack id="ack0001">
    <title>Acknowledgment</title>
    <p>The authors would like to thank Joseph Paul Cohen for collecting the COVID-Chestxray-dataset, and Sean Mullan for helping us with data preparation. We would also like to thank the providers of ChexPert dataset, which are used as the negative samples in our case. Milan Sonka’s research effort supported, in part, by NIH grant R01-EB004640.</p>
  </ack>
  <fn-group>
    <fn id="sec0020" fn-type="supplementary-material">
      <p id="p0051">Supplementary material associated with this article can be found, in the online version, at doi:<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.media.2020.101794" id="intrrf0008">10.1016/j.media.2020.101794</ext-link>.</p>
    </fn>
  </fn-group>
</back>
