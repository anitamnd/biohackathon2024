<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">EJNMMI Phys</journal-id>
    <journal-id journal-id-type="iso-abbrev">EJNMMI Phys</journal-id>
    <journal-title-group>
      <journal-title>EJNMMI Physics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2197-7364</issn>
    <publisher>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7403290</article-id>
    <article-id pub-id-type="publisher-id">316</article-id>
    <article-id pub-id-type="doi">10.1186/s40658-020-00316-9</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>RECOMIA—a cloud-based platform for artificial intelligence research in nuclear medicine and radiology</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7116-303X</contrib-id>
        <name>
          <surname>Trägårdh</surname>
          <given-names>Elin</given-names>
        </name>
        <address>
          <email>elin.tragardh@med.lu.se</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Borrelli</surname>
          <given-names>Pablo</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kaboteh</surname>
          <given-names>Reza</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gillberg</surname>
          <given-names>Tony</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ulén</surname>
          <given-names>Johannes</given-names>
        </name>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Enqvist</surname>
          <given-names>Olof</given-names>
        </name>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Edenbrandt</surname>
          <given-names>Lars</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.411843.b</institution-id><institution-id institution-id-type="ISNI">0000 0004 0623 9987</institution-id><institution>Department of Clinical Physiology and Nuclear Medicine, </institution><institution>Skåne University Hospital, </institution></institution-wrap>Carl Bertil Laurells gata 9, 205 02 Malmö, Sweden </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.4514.4</institution-id><institution-id institution-id-type="ISNI">0000 0001 0930 2361</institution-id><institution>Wallenberg Centre for Molecular Medicine, </institution><institution>Lund University, </institution></institution-wrap>Lund, Sweden </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.1649.a</institution-id><institution-id institution-id-type="ISNI">000000009445082X</institution-id><institution>Department of Clinical Physiology, </institution><institution>Sahlgrenska University Hospital, </institution></institution-wrap>Gothenburg, Sweden </aff>
      <aff id="Aff4"><label>4</label>RECOMIA, Malmö, Sweden </aff>
      <aff id="Aff5"><label>5</label>Eigenvision AB, Malmö, Sweden </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.5371.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 0775 6028</institution-id><institution>Department of Electrical Engineering, </institution><institution>Chalmers University of Technology, </institution></institution-wrap>Gothenburg, Sweden </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="GRID">grid.8761.8</institution-id><institution-id institution-id-type="ISNI">0000 0000 9919 9582</institution-id><institution>Department of Molecular and Clinical Medicine, Institute of Medicine, Sahlgrenska Academy, </institution><institution>University of Gothenburg, </institution></institution-wrap>Gothenburg, Sweden </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>4</day>
      <month>8</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>4</day>
      <month>8</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>12</month>
      <year>2020</year>
    </pub-date>
    <volume>7</volume>
    <elocation-id>51</elocation-id>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>2</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>26</day>
        <month>6</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Artificial intelligence (AI) is about to transform medical imaging. The Research Consortium for Medical Image Analysis (RECOMIA), a not-for-profit organisation, has developed an online platform to facilitate collaboration between medical researchers and AI researchers. The aim is to minimise the time and effort researchers need to spend on technical aspects, such as transfer, display, and annotation of images, as well as legal aspects, such as de-identification. The purpose of this article is to present the RECOMIA platform and its AI-based tools for organ segmentation in computed tomography (CT), which can be used for extraction of standardised uptake values from the corresponding positron emission tomography (PET) image.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">The RECOMIA platform includes modules for (1) local de-identification of medical images, (2) secure transfer of images to the cloud-based platform, (3) display functions available using a standard web browser, (4) tools for manual annotation of organs or pathology in the images, (5) deep learning-based tools for organ segmentation or other customised analyses, (6) tools for quantification of segmented volumes, and (7) an export function for the quantitative results. The AI-based tool for organ segmentation in CT currently handles 100 organs (77 bones and 23 soft tissue organs). The segmentation is based on two convolutional neural networks (CNNs): one network to handle organs with multiple similar instances, such as vertebrae and ribs, and one network for all other organs. The CNNs have been trained using CT studies from 339 patients. Experienced radiologists annotated organs in the CT studies. The performance of the segmentation tool, measured as mean Dice index on a manually annotated test set, with 10 representative organs, was 0.93 for all foreground voxels, and the mean Dice index over the organs were 0.86 (0.82 for the soft tissue organs and 0.90 for the bones).</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par3">The paper presents a platform that provides deep learning-based tools that can perform basic organ segmentations in CT, which can then be used to automatically obtain the different measurement in the corresponding PET image. The RECOMIA platform is available on request at <ext-link ext-link-type="uri" xlink:href="http://www.recomia.org">www.recomia.org</ext-link> for research purposes.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>CNN</kwd>
      <kwd>Artificial intelligence</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Segmentation</kwd>
      <kwd>PET-CT</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004063</institution-id>
            <institution>Knut och Alice Wallenbergs Stiftelse</institution>
          </institution-wrap>
        </funding-source>
        <award-id>-</award-id>
        <principal-award-recipient>
          <name>
            <surname>Trägårdh</surname>
            <given-names>Elin</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Region Skåne</institution>
        </funding-source>
        <award-id>-</award-id>
        <principal-award-recipient>
          <name>
            <surname>Trägårdh</surname>
            <given-names>Elin</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100006738</institution-id>
            <institution>Medicinska Fakulteten, Lunds Universitet</institution>
          </institution-wrap>
        </funding-source>
        <award-id>-</award-id>
        <principal-award-recipient>
          <name>
            <surname>Trägårdh</surname>
            <given-names>Elin</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100005761</institution-id>
            <institution>Sahlgrenska Akademin</institution>
          </institution-wrap>
        </funding-source>
        <award-id>ALFGBG-720751</award-id>
        <principal-award-recipient>
          <name>
            <surname>Edenbrandt</surname>
            <given-names>Lars</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par11">Artificial intelligence (AI) is about to transform the field of medical imaging. Deep learning, a subfield of AI, has become the method of choice for image analysis applications. This technique provides new opportunities in developing tools for automated analysis of 3-dimensional computed tomography (CT), positron emission tomography (PET)/CT, and magnetic resonance imaging. These tools have the potential to improve or substitute current methods of assessing CT, PET/CT, and magnetic resonance imaging in patients with cancer, for example, the Response Evaluation Criteria in Solid Tumors and PET Response Evaluation Criteria in Solid Tumors [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref>]. The development of these approaches is, however, hindered by technical and legal aspects that the researchers need to spend time and effort on. A platform for communication, image transfer, and analysis could minimise these problems.</p>
    <p id="Par12">The Research Consortium for Medical Image Analysis (RECOMIA) is a not-for-profit organisation that aims to promote research in the fields of AI and medical imaging. RECOMIA has developed a cloud-based platform to facilitate collaboration between medical researchers focusing on patient images and the related information, and mathematical researchers developing deep learning-based tools. The aim is to minimise the time and effort researchers need to spend on technical aspects, such as transfer and display of digital imaging and communications in medicine (DICOM) images and image annotations, as well as legal aspects, such as de-identification, and compliance with the General Data Protection Regulation and the Health Insurance Portability and Accountability Act.</p>
    <p id="Par13">Deep learning-based tools can be trained to analyse medical images using images with manual annotations of organs or pathology, such as tumours. The RECOMIA platform provides deep learning-based tools that can perform organ segmentations in CT, detection of lesions in PET/CT, and automated quantitative analysis of the segmented/detected volumes. These tools are freely available for researchers on reasonable request at <ext-link ext-link-type="uri" xlink:href="http://www.recomia.org">www.recomia.org</ext-link>. At present, more than 100 different organs and lesions can be segmented/detected based on training databases consisting of CT and PET/CT studies. This article aims to present the RECOMIA platform and the status of the current deep learning-based CT tools.</p>
  </sec>
  <sec id="Sec2">
    <title>Material and methods</title>
    <sec id="Sec3">
      <title>Platform</title>
      <p id="Par14">The RECOMIA platform is a cloud-based platform running on two separate servers (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). One is a Windows server running the web application handling everything but the AI models; it is written in C# using the ASP.NET Framework. One is a Linux server running Docker handling the AI models. To simplify collaboration between researchers at different universities and hospitals in different countries, the platform requires no installation and all functionality is available from a standard web browser. For security, the platform is deployed in an ISO/IEC 27001-certified data centre, and the recommended hardening, such as IP restrictions, is applied.
<fig id="Fig1"><label>Fig. 1</label><caption><p>RECOMIA platform architecture overview and user interaction</p></caption><graphic xlink:href="40658_2020_316_Fig1_HTML" id="MO1"/></fig></p>
      <sec id="Sec4">
        <title>De-identification and upload</title>
        <p id="Par15">New medical images in DICOM format can be uploaded to the platform using drag and drop. Before leaving your device, the image files are automatically de-identified in accordance with the DICOM standard (Fig. <xref rid="Fig2" ref-type="fig">2</xref>). Transfer to the server is secured using the Transfer Layer Security protocol with currently recommended cipher suites.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Dicom files using drag-and-drop (left). Before leaving your device, the image files are automatically de-identified (right)</p></caption><graphic xlink:href="40658_2020_316_Fig2_HTML" id="MO2"/></fig></p>
      </sec>
      <sec id="Sec5">
        <title>Online viewing and manual segmentation</title>
        <p id="Par16">The platform also allows viewing and annotating images in the browser. Standard features, such as windowing, zoom, and colour scales for PET studies are available, in similar ways as in conventional workstations. For performing detailed manual segmentations of, for example, organs or lesions, several tools are available. These include basic tools, such as a brush, polygon, and bucket fill tools, but also more advanced tools specialised for medical images (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). All tools have full support for multiple labels. Annotation tasks can be administered to different experts via a built-in project management system. Possible tasks include segmenting new labels but also reviewing segmentations performed by other experts. To simplify quality control, it is also possible to view the segmentations in 3D.
<fig id="Fig3"><label>Fig. 3</label><caption><p>The thresholding brush only paints pixels with Hounsfield values inside a specified range. This can speed up annotation significantly for some organs</p></caption><graphic xlink:href="40658_2020_316_Fig3_HTML" id="MO3"/></fig></p>
        <p id="Par17">The resulting annotations can be saved in separate DICOM files with label information stored in the DICOM file following the DICOM standard.</p>
      </sec>
      <sec id="Sec6">
        <title>Online AI tools</title>
        <p id="Par18">Several deep learning-based tools are already available upon request in the RECOMIA platform, among which the most important is the organ segmentation, described in the next section (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). It is also possible to upload your own customised tools and make them available to other researchers. Results from the AI tools can be displayed and corrected if necessary. It is also possible to use the automated results as a starting point for manual annotations.
<fig id="Fig4"><label>Fig. 4</label><caption><p>The AI segmentation tool can be used to segment up to 100 different organs (top left). The automated segmentation results can be viewed as an overlay (top right) or in 3D (bottom)</p></caption><graphic xlink:href="40658_2020_316_Fig4_HTML" id="MO4"/></fig></p>
      </sec>
      <sec id="Sec7">
        <title>Online quantification</title>
        <p id="Par19">Given a segmentation, whether performed manually or by AI, several statistics are available for each label. This includes the label volume, mean and max pixel values, and the number of connected components. For example, for PET images, this allows the computation of standardised uptake values and total lesion uptakes. The results can be exported as a CSV file.</p>
      </sec>
    </sec>
    <sec id="Sec8">
      <title>Deep learning-based organ segmentation</title>
      <p id="Par20">The RECOMIA platform has allowed the collection of a large dataset of annotated CT and PET/CT images. This data has been used to develop several useful AI tools. Here, we will focus on a tool for organ segmentation. Convolutional neural network (CNN)-based organ segmentation in CT images is already becoming standard, but it is normally limited to segmenting a smaller number of organs [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>]. This work takes organ segmentation to the next level by handling 100 different labels, including instance labels, such as vertebrae and ribs, where the number of instances can vary between images.</p>
      <sec id="Sec9">
        <title>The model</title>
        <p id="Par21">The organ segmentation tool is based on two CNNs. One CNN handles vertebrae and ribs labels, where there are multiple instances with similar appearances. The other CNN handles all other labels. Both networks are fully convolutional segmentation networks, with structure loosely inspired by the popular U-Net [<xref ref-type="bibr" rid="CR6">6</xref>], see Fig. <xref rid="Fig5" ref-type="fig">5</xref>. Using valid convolutions, the main memory bottleneck during training is the early layers because of their spatial size. By working on four different resolutions, with full field of view only on the lowest resolution, we significantly reduce the memory used during training. In practice, this is implemented using pooling layers at the start of the network. The final convolutional layer contains one channel per label with SoftMax activation.
<fig id="Fig5"><label>Fig. 5</label><caption><p>The network structure used for both CNNs. The reason for using two different filter sizes is to compensate for anisotropic voxel sizes and producing an approximately cubic field of view</p></caption><graphic xlink:href="40658_2020_316_Fig5_HTML" id="MO5"/></fig></p>
        <p id="Par22">The instance CNN has three SoftMax output channels coding for background, vertebra, and rib. The receptive field size of the networks is 136 × 136 × 72, approximately corresponding to a cube in millimeters (185 × 185 × 216 mm). This is too small for the instance CNN to predict the correct index of a vertebra. Instead, it has three extra output channels with linear activations. For each foreground pixel, these channels predict the centre of the corresponding vertebra. As a postprocessing step, these coordinates are clustered to identify the individual vertebrae. The final postprocessing step for all labels consists of extracting the largest connected component and filling holes in that component.</p>
        <p id="Par23">Both networks use the same pre-processing; the Hounsfield values are clamped to [− 800, 800] and divided by 800, resulting in an input with values in the range [− 1, 1].</p>
      </sec>
      <sec id="Sec10">
        <title>Patients and manual segmentations</title>
        <p id="Par24">The CNN-based organ segmentation in CT studies in RECOMIA has been used in multiple studies [<xref ref-type="bibr" rid="CR7">7</xref>–<xref ref-type="bibr" rid="CR12">12</xref>]. These studies were approved by the Regional Ethical Review Board (#295/08) and were performed following the Declaration of Helsinki. Patients and image acquisition have been described previously [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR11">11</xref>].</p>
        <p id="Par25">A group of experienced radiologists and nuclear medicine physicians manually segmented different organs using the RECOMIA platform. The organs included 77 bones and 23 soft tissue organs (Table <xref rid="Tab1" ref-type="table">1</xref>). Not all organs were annotated in all CT studies, which had to be handled in the training process. A dataset of approximately 13,000 manual organ segmentations in 339 images was used to train the CNNs.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>List of the 100 different organs segmented throughout the studies grouped by type</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Bones</th><th>Organs</th><th>Soft tissue</th><th>Organs</th></tr></thead><tbody><tr><td>Skull</td><td>1</td><td>Adrenal gland</td><td>2</td></tr><tr><td>Mandible</td><td>1</td><td>Brain</td><td>1</td></tr><tr><td>Cervical vertebrae</td><td>7</td><td>Lungs</td><td>2</td></tr><tr><td>Thoracic vertebrae</td><td>12</td><td>Trachea</td><td>1</td></tr><tr><td>Lumbar vertebrae</td><td>5</td><td>Bronchi</td><td>2</td></tr><tr><td>Ribs</td><td>24</td><td>Heart</td><td>1</td></tr><tr><td>Sacrum and coccyx</td><td>1</td><td>Aorta</td><td>1</td></tr><tr><td>Hip bones</td><td>2</td><td>Ventricle</td><td>1</td></tr><tr><td>Scapulae</td><td>2</td><td>Gastrointestinal tract</td><td>1</td></tr><tr><td>Clavicles</td><td>2</td><td>Liver</td><td>1</td></tr><tr><td>Sternum manubrium</td><td>1</td><td>Gallbladder</td><td>1</td></tr><tr><td>Sternum body</td><td>1</td><td>Spleen</td><td>1</td></tr><tr><td>Humerus</td><td>2</td><td>Pancreas</td><td>1</td></tr><tr><td>Radius</td><td>2</td><td>Kidneys</td><td>2</td></tr><tr><td>Ulna</td><td>2</td><td>Urinary bladder</td><td>1</td></tr><tr><td>Hand</td><td>2</td><td>Prostate</td><td>1</td></tr><tr><td>Femur</td><td>2</td><td>Testes</td><td>1</td></tr><tr><td>Tibia</td><td>2</td><td>Musc. gluteus maximus</td><td>2</td></tr><tr><td>Fibula</td><td>2</td><td/><td/></tr><tr><td>Patella</td><td>2</td><td/><td/></tr><tr><td>Foot</td><td>2</td><td/><td/></tr><tr><td><bold>Total</bold></td><td><bold>77</bold></td><td/><td><bold>23</bold></td></tr></tbody></table></table-wrap></p>
        <p id="Par26">A separate test set of 10 patients (5 male/5 female) was used to test the method and obtain data on inter-observer variability. Each test case was segmented independently by two different readers. Ten organs (prostate only for male patients) were segmented in each CT study.</p>
        <p id="Par27">All images used for training, validation, and test had a pixel spacing of 1.36 mm in slices and a distance between slices of 3 mm. Images with different pixel spacing can still be segmented by resampling the images using trilinear interpolation before running the networks. The resulting segmentation is then resampled to the image resolution using the nearest neighbour interpolation.</p>
      </sec>
      <sec id="Sec11">
        <title>Training the networks</title>
        <p id="Par28">The annotated data was divided with 80% in a training set and 20% in a validation set used to control hyperparameters. In theory, training a CNN is a simple question of feeding examples to the backpropagation algorithm. In this case, this means feeding randomly selected patches from images in the training group. These patches were augmented using moderate rotations (− 0.15 to 0.15 radians), scaling (− 10 to + 10%), and intensity shifts (− 100 to +100 HU) to enrich the training data. The model was trained using patches of size 136 × 136 × 72 and a batch size of 50. Categorical cross-entropy was used as the loss function, and the optimisation was performed using the Adam method [<xref ref-type="bibr" rid="CR13">13</xref>] with Nesterov momentum. The networks were developed in Python using the Tensorflow and Keras frameworks. Training and execution were performed on a high-end Linux desktop computer with a GeForce RTX 2080 TI graphics card. The training time for each network was about 48 h. Running the model on a single image took about 2 min on average.</p>
      </sec>
    </sec>
    <sec id="Sec12">
      <title>Statistical methods</title>
      <p id="Par29">The CNN-based segmentation was compared to the manual segmentations. The Sørensen-Dice (Dice) index was used to evaluate the agreement between automated and manual segmentations by analysis of the number of overlapping voxels.</p>
    </sec>
  </sec>
  <sec id="Sec13">
    <title>Results</title>
    <p id="Par30">The model was compared to the manual segmentations (e.g. Fig. <xref rid="Fig4" ref-type="fig">4</xref>) on the validation set and the test set. For all foreground pixels in the validations set, the Dice index was 0.95, recall 0.96, and precision 0.94. Another way to measure accuracy is by the per-organ Dice index. The average Dice index over the 100 organs was 0.88 (0.84 for the soft tissue organs and 0.90 for bones). Per organ metrics are shown in Table <xref rid="Tab2" ref-type="table">2</xref>. For the test set, one of the manual segmentations was randomly chosen to be ground truth. The automatic segmentation had foreground Dice index of 0.93, recall of 0.93, and precision of 0.92. The average Dice index over the 10 organs was 0.86 (0.82 for soft tissue organs and 0.90 for bones). Per organ metrics for all organs are shown in Table <xref rid="Tab3" ref-type="table">3</xref>. The inter-observer foreground Dice index, recall, and precision were all 0.94. The average Dice index over the 10 organs was 0.89 (0.86 for soft tissue organs and 0.92 for bone). Per organ metrics for all organs are shown in Table <xref rid="Tab4" ref-type="table">4</xref>.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Dice index, recall, and precision per organ. Each organ is individually segmented (i.e. the model segmented 24 individual ribs). The metric is presented as the mean over all organs in the same group. The metric for each organ in each group is very similar</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Organ</th><th>Dice</th><th>Recall</th><th>Precision</th></tr></thead><tbody><tr><td>Skull</td><td>0.93</td><td>0.94</td><td>0.92</td></tr><tr><td>Mandible</td><td>0.90</td><td>0.97</td><td>0.85</td></tr><tr><td>Cervical vertebrae</td><td>0.88</td><td>0.88</td><td>0.88</td></tr><tr><td>Thoracic vertebrae</td><td>0.91</td><td>0.91</td><td>0.90</td></tr><tr><td>Lumbar vertebrae</td><td>0.91</td><td>0.91</td><td>0.91</td></tr><tr><td>Ribs</td><td>0.88</td><td>0.92</td><td>0.85</td></tr><tr><td>Sacrum and coccyx</td><td>0.94</td><td>0.96</td><td>0.92</td></tr><tr><td>Hip bones</td><td>0.96</td><td>0.97</td><td>0.94</td></tr><tr><td>Scapulae</td><td>0.95</td><td>0.97</td><td>0.93</td></tr><tr><td>Clavicles</td><td>0.94</td><td>0.98</td><td>0.90</td></tr><tr><td>Sternum manubrium</td><td>0.93</td><td>0.96</td><td>0.90</td></tr><tr><td>Sternum body</td><td>0.92</td><td>0.96</td><td>0.89</td></tr><tr><td>Humerus</td><td>0.92</td><td>0.95</td><td>0.89</td></tr><tr><td>Radius</td><td>0.94</td><td>0.96</td><td>0.92</td></tr><tr><td>Ulna</td><td>0.93</td><td>0.98</td><td>0.89</td></tr><tr><td>Hand</td><td>0.87</td><td>0.91</td><td>0.84</td></tr><tr><td>Femur</td><td>0.96</td><td>0.96</td><td>0.97</td></tr><tr><td>Tibia</td><td>0.96</td><td>0.97</td><td>0.96</td></tr><tr><td>Fibula</td><td>0.96</td><td>0.96</td><td>0.95</td></tr><tr><td>Patella</td><td>0.96</td><td>0.97</td><td>0.95</td></tr><tr><td>Foot</td><td>0.95</td><td>0.95</td><td>0.96</td></tr><tr><td>Adrenal gland</td><td>0.61</td><td>0.74</td><td>0.58</td></tr><tr><td>Brain</td><td>0.98</td><td>0.99</td><td>0.96</td></tr><tr><td>Lungs</td><td>0.98</td><td>0.98</td><td>0.98</td></tr><tr><td>Trachea</td><td>0.89</td><td>0.91</td><td>0.86</td></tr><tr><td>Bronchi</td><td>0.77</td><td>0.86</td><td>0.71</td></tr><tr><td>Heart</td><td>0.92</td><td>0.93</td><td>0.92</td></tr><tr><td>Aorta</td><td>0.87</td><td>0.88</td><td>0.87</td></tr><tr><td>Ventricle</td><td>0.85</td><td>0.88</td><td>0.84</td></tr><tr><td>Gastrointestinal tract</td><td>0.86</td><td>0.85</td><td>0.89</td></tr><tr><td>Liver</td><td>0.96</td><td>0.97</td><td>0.96</td></tr><tr><td>Gallbladder</td><td>0.78</td><td>0.86</td><td>0.75</td></tr><tr><td>Spleen</td><td>0.89</td><td>0.93</td><td>0.88</td></tr><tr><td>Pancreas</td><td>0.57</td><td>0.68</td><td>0.53</td></tr><tr><td>Kidneys</td><td>0.91</td><td>0.95</td><td>0.89</td></tr><tr><td>Urinary bladder</td><td>0.83</td><td>0.88</td><td>0.81</td></tr><tr><td>Prostate</td><td>0.82</td><td>0.84</td><td>0.83</td></tr><tr><td>Testes</td><td>0.58</td><td>0.55</td><td>0.66</td></tr><tr><td>Muscle gluteus maximus</td><td>0.93</td><td>0.93</td><td>0.92</td></tr><tr><td>Average</td><td>0.88</td><td>0.91</td><td>0.87</td></tr></tbody></table></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption><p>Mean Dice index, recall, and precision per organ on an independent test set of 10 patients (5 male/5 female)</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Organ</th><th>Dice</th><th>Recall</th><th>Precision</th></tr></thead><tbody><tr><td>Hip bone left</td><td>0.94</td><td>0.95</td><td>0.94</td></tr><tr><td>Humerus left</td><td>0.88</td><td>0.94</td><td>0.84</td></tr><tr><td>Rib right 5</td><td>0.88</td><td>0.91</td><td>0.84</td></tr><tr><td>Scapula right</td><td>0.91</td><td>0.92</td><td>0.90</td></tr><tr><td>Lumbar vertebrae 3</td><td>0.89</td><td>0.88</td><td>0.90</td></tr><tr><td>Aorta</td><td>0.87</td><td>0.91</td><td>0.84</td></tr><tr><td>Kidney left</td><td>0.92</td><td>0.94</td><td>0.91</td></tr><tr><td>Liver</td><td>0.95</td><td>0.94</td><td>0.95</td></tr><tr><td>Prostate</td><td>0.81</td><td>0.93</td><td>0.72</td></tr><tr><td>Trachea</td><td>0.89</td><td>0.89</td><td>0.88</td></tr><tr><td>Average</td><td>0.90</td><td>0.92</td><td>0.87</td></tr></tbody></table></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>Inter-observer Dice index, recall, and precision for the two readers, per organ on an independent test set of 10 patients (5 male/5 female)</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Organ</th><th>Dice</th><th>Recall</th><th>Precision</th></tr></thead><tbody><tr><td>Hip bone left</td><td>0.96</td><td>0.94</td><td>0.97</td></tr><tr><td>Humerus left</td><td>0.92</td><td>0.93</td><td>0.92</td></tr><tr><td>Rib right 5</td><td>0.90</td><td>0.89</td><td>0.91</td></tr><tr><td>Scapula right</td><td>0.93</td><td>0.91</td><td>0.95</td></tr><tr><td>Lumbar vertebrae 3</td><td>0.88</td><td>0.87</td><td>0.89</td></tr><tr><td>Aorta</td><td>0.89</td><td>0.90</td><td>0.89</td></tr><tr><td>Kidney left</td><td>0.94</td><td>0.94</td><td>0.96</td></tr><tr><td>Liver</td><td>0.95</td><td>0.96</td><td>0.94</td></tr><tr><td>Prostate</td><td>0.84</td><td>0.85</td><td>0.84</td></tr><tr><td>Trachea</td><td>0.94</td><td>0.92</td><td>0.95</td></tr><tr><td>Average</td><td>0.91</td><td>0.91</td><td>0.92</td></tr></tbody></table></table-wrap></p>
  </sec>
  <sec id="Sec14">
    <title>Discussion</title>
    <p id="Par31">AI-based tools can provide highly accurate and reproducible organ segmentation, similar to those obtained manually by radiologists, but much faster (approximate manual segmentation time was 90 min per patient for the 10 organs in the test set). To the best of our knowledge, RECOMIA is the only platform that is freely available for research and can be used to automatically segment a wide selection of organs in CT images and provide PET measurements for the same organs. We continue to train new CNNs to continuously improve performance.</p>
    <p id="Par32">Studying the results in Tables <xref rid="Tab2" ref-type="table">2</xref> and <xref rid="Tab3" ref-type="table">3</xref>, the automatic organ segmentation achieves high Dice scores for most labels. Unsurprisingly, organs that might have low contrast to the surrounding tissue, such as the pancreas, are assigned lower scores. Also, small organs, such as the testes or the adrenal glands, tend to be assigned lower Dice scores. To understand why, note that the difficult pixels are typically found on the organ boundaries, while pixels inside the organ are easier to classify. The number of boundary pixels increases quadratically with organ size, while the total number of organ pixels increases cubically.</p>
    <p id="Par33">Considering the statistical dispersion of Dice indices, it is typically low for organs with high Dice scores. This means standard deviations between 0.01 and 0.05, excluding one outlier case where considerable image noise around the first thoracic vertebra led to an off-by-one error in the numbering of all the subsequent vertebrae and ribs (although well delineated). For the organs with lower average Dice index listed above, the dispersion was also higher with standard deviations between 0.08 and 0.26. Finally, due to large natural variability, the gallbladder, urinary bladder, and ventricle had high standard deviations (0.08 to 0.17) although the average Dice indices were good.</p>
    <p id="Par34">The RECOMIA platform and the deep learning-based tools for organ segmentations have already been used in several studies. Lindgren Belal et al. [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR8">8</xref>] used bone segmentation for quantification of bone metastases PET/CT in patients with prostate cancer. The automatically measured tumour burden to bone was associated with overall survival. The intra-observer volume difference for the segmentation of five selected bones was less with CNN-based than a manual approach, for example, Th7 2% volume difference for CNN-based segmentation vs 14% for segmentation performed by a radiologist.</p>
    <p id="Par35">Mortensen et al. [<xref ref-type="bibr" rid="CR9">9</xref>] and Polymeri et al. [<xref ref-type="bibr" rid="CR10">10</xref>] used automated segmentation of the prostate. A CNN was trained for automated measurements in [18F]-choline PET/CT scans obtained before radical prostatectomy in patients with newly diagnosed prostate cancer [<xref ref-type="bibr" rid="CR9">9</xref>]. Automated standardised uptake values from the PET images were obtained for the prostate. Corresponding manual measurements were performed, and the CNN-based and manual measurements were compared with the weighted surgically removed tissue specimens. The automated CNN segmentation and the PET measurements provided similar measurements to manually derived measurements. Polymeri et al. [<xref ref-type="bibr" rid="CR10">10</xref>] then used the method to explore the potential of automatic PET/CT measurements as prognostic biomarkers. These authors found that automated PET/CT measurements reflecting total lesion uptake were significantly associated with overall survival, whereas age, prostate-specific antigen, and Gleason score were not.</p>
    <p id="Par36">Sadik et al. [<xref ref-type="bibr" rid="CR11">11</xref>] developed automated segmentation of the liver and thoracic aorta as a first step towards an automated method for evaluating treatment response in patients with lymphoma, since those organs are reference organs in the Deauville 5-point scale. The CNN-method showed good agreement with results obtained by experienced radiologists who had manually segmented the CT images. Ly et al. [<xref ref-type="bibr" rid="CR12">12</xref>] then used the method to calculate Deauville scores in patients with lymphoma, to compare Deauville scores obtained from different reconstruction methods.</p>
    <p id="Par37">The platform is currently used by research groups from 20 hospitals/universities in 10 countries and includes both CT, PET/CT, and magnetic resonance imaging applications.</p>
    <p id="Par38">The organ segmentations are based on low dose CT without contrast on adult patients. The scope will be expanded to include also CT of diagnostic quality and with contrast. Future work will also include organ segmentation of CT studies from children.</p>
  </sec>
  <sec id="Sec15">
    <title>Conclusion</title>
    <p id="Par39">The paper presents a platform that provides deep learning-based tools that can perform basic organ segmentations in CT, which can then be used to automatically obtain the different measurements in the corresponding PET image. The tools developed in this project are available on request at <ext-link ext-link-type="uri" xlink:href="http://www.recomia.org/">www.recomia.org</ext-link> for research purposes.</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>AI</term>
        <def>
          <p id="Par4">Artificial intelligence</p>
        </def>
      </def-item>
      <def-item>
        <term>CNN</term>
        <def>
          <p id="Par5">Convolutional neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>CT</term>
        <def>
          <p id="Par6">Computed tomography</p>
        </def>
      </def-item>
      <def-item>
        <term>Dice</term>
        <def>
          <p id="Par7">Sorensen-Dice</p>
        </def>
      </def-item>
      <def-item>
        <term>DICOM</term>
        <def>
          <p id="Par8">Digital imaging and communications in medicine</p>
        </def>
      </def-item>
      <def-item>
        <term>PET</term>
        <def>
          <p id="Par9">Positron emission tomography</p>
        </def>
      </def-item>
      <def-item>
        <term>RECOMIA</term>
        <def>
          <p id="Par10">Research Consortium for Medical Image Analysis</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We express our gratitude to the radiologists and nuclear medicine physicians performing organ and lesion segmentation and for all researchers using the RECOMIA platform.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>All authors made substantial contributions to the conception and design of the work and interpretation of data. ET, PB, RK, and LE segmented images. OE and JU performed the CNNs and performed the statistical analysis. TG programmed the RECOMIA platform. ET, JU, OE, and LE drafted the manuscript. PB, RK, and TG reviewed and revised the manuscript. All authors provided final approval of the version submitted for publication.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>The study was financed by generous support from the Knut and Alice Wallenberg Foundation, Region Skåne, Lund University, and from the Swedish state under the agreement between the Swedish government and the county councils, the ALF-agreement (ALFGBG-720751). Open access funding provided by Lund University.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The <ext-link ext-link-type="uri" xlink:href="http://www.recomia.org">www.recomia.org</ext-link> platform is freely available for research.</p>
  </notes>
  <notes id="FPar1">
    <title>Ethics approval and consent to participate</title>
    <p id="Par40">This study was approved by the Regional Ethical Review Board (#295/08) and was performed following the Declaration of Helsinki. All patients provided written informed consent.</p>
  </notes>
  <notes id="FPar2">
    <title>Consent for publication</title>
    <p id="Par41">All patients provided written informed consent.</p>
  </notes>
  <notes id="FPar3" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par42">JU and OE are board members and stockholders of Eigenvision AB, which is a company working with research and development in automated image analysis, computer vision, and machine learning. The other authors declare that they have no conflict of interest.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schwartz</surname>
            <given-names>LH</given-names>
          </name>
          <name>
            <surname>Litiere</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>de Vries</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Ford</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Gwyther</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Mandrekar</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>RECIST 1.1-update and clarification: from the RECIST committee</article-title>
        <source>Eur J Cancer.</source>
        <year>2016</year>
        <volume>62</volume>
        <fpage>132</fpage>
        <lpage>137</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ejca.2016.03.081</pub-id>
        <pub-id pub-id-type="pmid">27189322</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schwartz</surname>
            <given-names>LH</given-names>
          </name>
          <name>
            <surname>Seymour</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Litiere</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ford</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Gwyther</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Mandrekar</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>RECIST 1.1 - standardisation and disease-specific adaptations: perspectives from the RECIST Working Group</article-title>
        <source>Eur J Cancer.</source>
        <year>2016</year>
        <volume>62</volume>
        <fpage>138</fpage>
        <lpage>145</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ejca.2016.03.082</pub-id>
        <pub-id pub-id-type="pmid">27237360</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Min</surname>
            <given-names>SJ</given-names>
          </name>
          <name>
            <surname>Jang</surname>
            <given-names>HJ</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>JH</given-names>
          </name>
        </person-group>
        <article-title>Comparison of the RECIST and PERCIST criteria in solid tumors: a pooled analysis and review</article-title>
        <source>Oncotarget.</source>
        <year>2016</year>
        <volume>7</volume>
        <issue>19</issue>
        <fpage>27848</fpage>
        <lpage>27854</lpage>
        <pub-id pub-id-type="doi">10.18632/oncotarget.8425</pub-id>
        <pub-id pub-id-type="pmid">27036043</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Roth HR, Shen C, Oda H, Sugino T, Oda M, Hayashi Y, et al., editors. A multi-scale pyramid of 3D fully convolutional networks for abdominal multi-organ segmentation. International conference on medical image computing and computer-assisted intervention; 2018: Springer.</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Fishman</surname>
            <given-names>EK</given-names>
          </name>
          <name>
            <surname>Yuille</surname>
            <given-names>AL</given-names>
          </name>
        </person-group>
        <article-title>Abdominal multi-organ segmentation with organ-attention networks and statistical fusion</article-title>
        <source>Medical image analysis.</source>
        <year>2019</year>
        <volume>55</volume>
        <fpage>88</fpage>
        <lpage>102</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2019.04.005</pub-id>
        <pub-id pub-id-type="pmid">31035060</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Ronneberger O, Fischer P, Brox T, editors. U-net: Convolutional networks for biomedical image segmentation. International Conference on Medical image computing and computer-assisted intervention; 2015: Springer.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lindgren Belal</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sadik</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kaboteh</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Enqvist</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Ulen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Poulsen</surname>
            <given-names>MH</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning for segmentation of 49 selected bones in CT scans: first step in automated PET/CT-based 3D quantification of skeletal metastases</article-title>
        <source>Eur J Radiol.</source>
        <year>2019</year>
        <volume>113</volume>
        <fpage>89</fpage>
        <lpage>95</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ejrad.2019.01.028</pub-id>
        <pub-id pub-id-type="pmid">30927965</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lindgren Belal</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sadik</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kaboteh</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Hasani</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Enqvist</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Svarm</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>3D skeletal uptake of 18F sodium fluoride in PET/CT images is associated with overall survival in patients with prostate cancer</article-title>
        <source>EJNMMI Res.</source>
        <year>2017</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>15</fpage>
        <pub-id pub-id-type="doi">10.1186/s13550-017-0264-5</pub-id>
        <pub-id pub-id-type="pmid">28210997</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mortensen</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Borrelli</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Poulsen</surname>
            <given-names>MH</given-names>
          </name>
          <name>
            <surname>Gerke</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Enqvist</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Ulen</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Artificial intelligence-based versus manual assessment of prostate cancer in the prostate gland: a method comparison study</article-title>
        <source>Clin Physiol Funct Imaging.</source>
        <year>2019</year>
        <volume>39</volume>
        <issue>6</issue>
        <fpage>399</fpage>
        <lpage>406</lpage>
        <pub-id pub-id-type="doi">10.1111/cpf.12592</pub-id>
        <pub-id pub-id-type="pmid">31436365</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Polymeri E, Sadik M, Kaboteh R, Borrelli P, Enqvist O, Ulen J, et al. Deep learning-based quantification of PET/CT prostate gland uptake: association with overall survival. Clin Physiol Funct Imaging. 2019;3.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sadik</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lind</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Polymeri</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Enqvist</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Ulen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Tragardh</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Automated quantification of reference levels in liver and mediastinal blood pool for the Deauville therapy response classification using FDG-PET/CT in Hodgkin and non-Hodgkin lymphomas</article-title>
        <source>Clin Physiol Funct Imaging.</source>
        <year>2019</year>
        <volume>39</volume>
        <issue>1</issue>
        <fpage>78</fpage>
        <lpage>84</lpage>
        <pub-id pub-id-type="doi">10.1111/cpf.12546</pub-id>
        <pub-id pub-id-type="pmid">30284376</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ly</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Minarik</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Edenbrandt</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Wollmer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Tragardh</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>The use of a proposed updated EARL harmonization of (18)F-FDG PET-CT in patients with lymphoma yields significant differences in Deauville score compared with current EARL recommendations</article-title>
        <source>EJNMMI Res.</source>
        <year>2019</year>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>65</fpage>
        <pub-id pub-id-type="doi">10.1186/s13550-019-0536-3</pub-id>
        <pub-id pub-id-type="pmid">31346805</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J. Adam: A method for stochastic optimization. arXiv preprint arXiv:14126980. 2014.</mixed-citation>
    </ref>
  </ref-list>
</back>
