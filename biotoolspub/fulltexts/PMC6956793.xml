<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6956793</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btz470</article-id>
    <article-id pub-id-type="publisher-id">btz470</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Data and Text Mining</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Scaling tree-based automated machine learning to biomedical big data with a feature set selector</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-3737-6565</contrib-id>
        <name>
          <surname>Le</surname>
          <given-names>Trang T</given-names>
        </name>
        <xref ref-type="author-notes" rid="btz470-FM2"/>
        <xref ref-type="aff" rid="btz470-aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-6434-5468</contrib-id>
        <name>
          <surname>Fu</surname>
          <given-names>Weixuan</given-names>
        </name>
        <xref ref-type="author-notes" rid="btz470-FM2"/>
        <xref ref-type="aff" rid="btz470-aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-5015-1099</contrib-id>
        <name>
          <surname>Moore</surname>
          <given-names>Jason H</given-names>
        </name>
        <xref ref-type="corresp" rid="btz470-cor1"/>
        <!--<email>jhmoore@upenn.edu</email>-->
        <xref ref-type="aff" rid="btz470-aff1"/>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Kelso</surname>
          <given-names>Janet</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <aff id="btz470-aff1"><institution>Department of Biostatistics, Epidemiology and Informatics, Institute for Biomedical Informatics, University of Pennsylvania</institution>, Philadelphia, PA 19104, <country country="US">USA</country></aff>
    <author-notes>
      <corresp id="btz470-cor1">To whom correspondence should be addressed. Jason Moore is the only corresponding author. <email>jhmoore@upenn.edu</email></corresp>
      <fn id="btz470-FM2">
        <p>The authors wish it to be known that Trang T. Le and Weixuan Fu contributed equally.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="ppub">
      <day>01</day>
      <month>1</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2019-06-04">
      <day>04</day>
      <month>6</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>04</day>
      <month>6</month>
      <year>2019</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>36</volume>
    <issue>1</issue>
    <fpage>250</fpage>
    <lpage>256</lpage>
    <history>
      <date date-type="received">
        <day>03</day>
        <month>1</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>17</day>
        <month>5</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>02</day>
        <month>6</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="cc-by" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btz470.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Automated machine learning (AutoML) systems are helpful data science assistants designed to scan data for novel features, select appropriate supervised learning models and optimize their parameters. For this purpose, Tree-based Pipeline Optimization Tool (TPOT) was developed using strongly typed genetic programing (GP) to recommend an optimized analysis pipeline for the data scientist’s prediction problem. However, like other AutoML systems, TPOT may reach computational resource limits when working on big data such as whole-genome expression data.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We introduce two new features implemented in TPOT that helps increase the system’s scalability: Feature Set Selector (FSS) and Template. FSS provides the option to specify subsets of the features as separate datasets, assuming the signals come from one or more of these specific data subsets. FSS increases TPOT’s efficiency in application on big data by slicing the entire dataset into smaller sets of features and allowing GP to select the best subset in the final pipeline. Template enforces type constraints with strongly typed GP and enables the incorporation of FSS at the beginning of each pipeline. Consequently, FSS and Template help reduce TPOT computation time and may provide more interpretable results. Our simulations show TPOT-FSS significantly outperforms a tuned XGBoost model and standard TPOT implementation. We apply TPOT-FSS to real RNA-Seq data from a study of major depressive disorder. Independent of the previous study that identified significant association with depression severity of two modules, TPOT-FSS corroborates that one of the modules is largely predictive of the clinical diagnosis of each individual.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>Detailed simulation and analysis code needed to reproduce the results in this study is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/lelaboratoire/tpot-fss">https://github.com/lelaboratoire/tpot-fss</ext-link>. Implementation of the new TPOT operators is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/EpistasisLab/tpot">https://github.com/EpistasisLab/tpot</ext-link>.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">National Institutes of Health</named-content>
          <named-content content-type="funder-identifier">10.13039/100000002</named-content>
        </funding-source>
        <award-id>LM010098</award-id>
        <award-id>LM012601</award-id>
        <award-id>AI116794</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="7"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>For many bioinformatics problems of classifying individuals into clinical categories from high-dimensional biological data, performance of a machine learning (ML) model depends greatly on the problem it is applied to (<xref rid="btz470-B32" ref-type="bibr">Olson <italic>et al.</italic>, 2017</xref>, <xref rid="btz470-B31" ref-type="bibr">2018</xref>). In addition, choosing a classifier is merely one step of the arduous process that leads to predictions. To detect patterns among features (e.g. clinical variables) and their associations with the outcome (e.g. clinical diagnosis), a data scientist typically has to design and test different complex ML frameworks that consist of data exploration, feature engineering, model selection and prediction. Automated machine learning (AutoML) systems were developed to automate this challenging and time-consuming process. These intelligent systems increase the accessibility and scalability of various ML applications by efficiently solving an optimization problem to discover pipelines that yield satisfactory outcomes, such as prediction accuracy. Consequently, AutoML allows data scientists to focus their effort in applying their expertise in other important research components such as developing meaningful hypotheses or communicating the results.</p>
    <p>Grid search, random search (<xref rid="btz470-B2" ref-type="bibr">Bergstra and Bengio, 2012</xref>), Bayesian optimization (<xref rid="btz470-B11" ref-type="bibr">Brochu <italic>et al.</italic>, 2010</xref>) and evolutionary algorithm (EA) (<xref rid="btz470-B9" ref-type="bibr">Eiben and Smith, 2010</xref>) are four common approaches to build AutoML systems for diverse applications. Both grid search and random search could be too computational expensive and impractical to explore all possible combinations of the hyperparameters on a model with high-dimensional search space, e.g. with more than 10 hyperparameters (<xref rid="btz470-B17" ref-type="bibr">Dewancker <italic>et al.</italic>, 2016</xref>). Bayesian optimization is implemented in both auto-sklearn (<xref rid="btz470-B12" ref-type="bibr">Feurer <italic>et al.</italic>, 2015</xref>) and Auto-WEKA (<xref rid="btz470-B6" ref-type="bibr">Thornton <italic>et al.</italic>, 2013</xref>; <xref rid="btz470-B19" ref-type="bibr">Kotthoff <italic>et al.</italic>, 2017</xref>) for model selection and hyperparameter optimization. Although both systems allow simple ML pipelines including data pre-processing, feature engineering and single model prediction, they cannot build more complex pipelines or stacked models which are necessary for complicated prediction problems. On the other hand, EA can generate highly extensible and complex ML pipelines and ensemble models for data scientists. For example, Recipe (<xref rid="btz470-B8" ref-type="bibr">de Sá <italic>et al.</italic>, 2017</xref>) uses grammar-based EA to build and optimize ML pipelines based on a fully configurable grammar. Autostacker (<xref rid="btz470-B4" ref-type="bibr">Chen <italic>et al.</italic>, 2018</xref>) uses basic EA to look for flexible combinations of many ML algorithms that yield better performance. DEvol (<ext-link ext-link-type="uri" xlink:href="https://github.com/joeddav/devol">https://github.com/joeddav/devol</ext-link>) was designed specifically for deep neural networks and can optimize complex model architecture by using EA to tune hyperparameters related to convolutional/dense layers and optimizer. More recently released, GAMA (<xref rid="btz470-B14" ref-type="bibr">Gijsbers and Vanschoren, 2019</xref>) performs automatic ensemble of best ML pipelines evaluated by asynchronous EA instead of simply using a single best pipeline for prediction. Progressively, EA enhances AutoML systems with high flexibility in building pipelines in a large search space of ML algorithms and their hyperparameters.</p>
    <p>Tree-based Pipeline Optimization Tool (TPOT) is a EA-based AutoML system that uses genetic programing (GP) (<xref rid="btz470-B1" ref-type="bibr">Banzhaf <italic>et al.</italic>, 1998</xref>) to optimize a series of feature selectors, pre-processors and ML models with the objective of maximizing classification accuracy. Although most AutoML systems primarily focus on model selection and hyperparameter optimization, TPOT also pays attention to feature selection and feature engineering by evaluating the complete pipelines based on their cross-validated score such as mean squared error or balanced accuracy. Given no a priori knowledge about the problem, TPOT has been shown to frequently outperform standard ML analyses (<xref rid="btz470-B30" ref-type="bibr">Olson <italic>et al.</italic>, 2016</xref>; <xref rid="btz470-B34" ref-type="bibr">Olson and Moore, 2016</xref>). Effort has been made to specialize TPOT for human genetics research, resulting in a useful extended version of TPOT, TPOT–MDR that features Multifactor Dimensionality Reduction and an Expert Knowledge Filter (<xref rid="btz470-B37" ref-type="bibr">Sohn <italic>et al.</italic>, 2017</xref>). However, at the current stage, TPOT still requires great computational expense to analyze large datasets such as in genome-wide association studies (GWASs) or gene expression analyses. Consequently, the application of TPOT on real-world datasets has been limited to small sets of features (<xref rid="btz470-B23" ref-type="bibr">Le <italic>et al.</italic>, 2018a</xref>).</p>
    <p>In this work, we introduce two new features implemented in TPOT that helps increase the system’s scalability. First, the Feature Set Selector (FSS) allows the users to pass specific subsets of the features, reducing the computational expense of TPOT at the beginning of each pipeline to only evaluate on a smaller subset of data rather than the entire dataset. Consequently, FSS increases TPOT’s efficiency in application on large datasets by slicing the data into smaller sets of features (e.g. genes) and allowing a genetic algorithm to select the best subset in the final pipeline. Second, Template enables the option for strongly typed GP, a method to enforce type constraints in GP. By letting users specify a desired structure of the resulting ML pipeline, Template helps reduce TPOT computation time and potentially provide more interpretable results.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <p>We begin with descriptions of the two novel additions to TPOT, FSS and Template. Then, we provide detail of a real-world RNA-Seq dataset and describe a simulation approach to generate data comparable to the RNA-Seq data. Finally, we discuss other methods and performance metrics for comparison. Detailed simulation and analysis code needed to reproduce the results has been made available on the GitHub repository <ext-link ext-link-type="uri" xlink:href="https://github.com/lelaboratoire/tpot-fss">https://github.com/lelaboratoire/tpot-fss</ext-link>.</p>
    <sec>
      <title>2.1. Tree-based pipeline optimization tool</title>
      <p>TPOT automates the laborious process of designing a ML pipeline by representing pipelines as binary expression trees with ML operators as primitives. Pipeline elements include algorithms from the extensive library of scikit-learn (<xref rid="btz470-B33" ref-type="bibr">Pedregosa <italic>et al.</italic>, 2011</xref>) as well as other efficient implementations such as extreme gradient boosting. Applying GP with the NSGA-II Pareto optimization (<xref rid="btz470-B7" ref-type="bibr">Deb <italic>et al.</italic>, 2002</xref>), TPOT optimizes the accuracy achieved by the pipeline while accounting for its complexity. Specifically, to automatically generate and optimize these machine-learning pipelines, TPOT utilizes the Python package DEAP (<xref rid="btz470-B13" ref-type="bibr">Fortin <italic>et al.</italic>, 2012</xref>) to implement the GP algorithm. Implementation details can be found at TPOT’s active Github repository <ext-link ext-link-type="uri" xlink:href="https://github.com/EpistasisLab/tpot">https://github.com/EpistasisLab/tpot</ext-link>.</p>
      <sec>
        <title>2.1.1 Feature set selector</title>
        <p>TPOT’s current operators include sets of feature pre-processors, feature transformers, feature selection techniques and supervised classifiers and regressions. In this study, we introduce a new operator called FSS that enables biologically guided group-level feature selection. From pre-defined subsets of features, the FSS operator allows TPOT to select the best subset that maximizes average accuracy in <italic>k</italic>-fold cross validation (5-fold by default). Specifically, taking place at the very first stage of the pipeline, FSS passes only a specific subset of the features onwards, effectively slicing the large original dataset into smaller ones. Hence, with FSS, users can specify subsets of features of interest to reduce the feature space’s dimension at pipeline initialization.</p>
        <p>For example, in a gene expression analysis of major depressive disorder (MDD), a neuroscientist can specify collections of genes in pathways of interest and identify the important collection that helps predict the depression severity. Similarly, in a GWAS of breast cancer, an analyst may assign variants in the data to different subsets of potentially related variants and detect the subset associated with the breast cancer diagnosis. In general, the FSS operator takes advantage of previous compartmentalization of the feature space to smaller subsets based on a priori expert knowledge about the biomedical dataset. From here, TPOT learns and selects the most relevant group of features for outcome prediction. When compared with TPOT’s existing Selector operators, FSS selects features at the group level instead of individual level.</p>
      </sec>
      <sec>
        <title>2.1.2 Template</title>
        <p>Parallel with the establishment of the FSS operator, we now offer TPOT users the option to define a Template that provides a way to specify a desired structure for the resulting ML pipeline, which will reduce TPOT computation time and potentially provide more interpretable results.</p>
        <p>Current implementation of Template supports linear pipelines, or path graphs, which are trees with two nodes (operators) of vertex degree 1, and the other <italic>n</italic> − 2 nodes of vertex degree 2. Further, Template takes advantage of the strongly typed GP framework that enforces data-type constraints (<xref rid="btz470-B28" ref-type="bibr">Montana, 1995</xref>) and imposes type-based restrictions on which element (i.e. operator) type can be chosen at each node. In strongly typed GP, while the fitness function and parameters remain the same, the initialization procedure and genetic operators (e.g. mutation, crossover) must respect the enhanced legality constraints (<xref rid="btz470-B28" ref-type="bibr">Montana, 1995</xref>). With a Template defined, each node in the tree pipeline is assigned one of the five major operator types: FSS, feature selector, feature transformer, classifier or regressor. Moreover, besides the major operator types, each node can also be assigned more specifically as a method of an operator, such as decision trees for classifier. An example Template is FSS → Feature transform → Decision trees (<xref ref-type="fig" rid="btz470-F1">Fig. 1</xref>). 
</p>
        <fig id="btz470-F1" orientation="portrait" position="float">
          <label>Fig. 1.</label>
          <caption>
            <p>Template’s general workflow with TPOT-FSS and example pipelines. Final pipelines with optimized parameters are shown for simulated data (top, green) and real-world RNA-Seq data (bottom, mauve). The specific operators selected in optimal pipelines include built-in TPOT’s operators (OneHotEncoder, FeatureSetSelector) and functions from the library of scikit-learn (ExtraTreesClassifier, Nystroem) </p>
          </caption>
          <graphic xlink:href="btz470f1"/>
        </fig>
      </sec>
    </sec>
    <sec>
      <title>2.2 Datasets</title>
      <p>We apply TPOT with the new FSS operator on both simulated datasets and a real world RNA expression dataset. With both real-world and simulated data, we hope to acquire a comprehensive view of the strengths and limitations of TPOT in the next generation sequencing domain.</p>
      <sec>
        <title>2.2.1 Simulation methods</title>
        <p>The simulated datasets were generated using the R package privateEC, which was designed to simulate realistic effects to be expected in gene expression or resting-state fMRI data. In this study, to be consistent with the real RNA-Seq dataset (described below), we simulate interaction effect data with <italic>m </italic>=<italic> </italic>200 individuals (100 cases and 100 controls) and <italic>P </italic>=<italic> </italic>5000 real-valued features with 4% functional (true positive association with outcome) for each training and testing set. Full details of the simulation approach can be found in <xref rid="btz470-B21" ref-type="bibr">Lareau <italic>et al.</italic> (2015)</xref> and <xref rid="btz470-B24" ref-type="bibr">Le <italic>et al.</italic> (2018b</xref>). Briefly, the privateEC simulation induces a differential co-expression network of random normal expression levels and permutes the values of targeted features within the cases to generate interactions. Further, by imposing a large number of background features (no association with outcome), we seek to assess TPOT–FSS’s performance in accommodating large numbers of non-predictive features.</p>
        <p>To closely resemble the module size distribution in the RNA-Seq data, we first fit a <inline-formula id="IE1"><mml:math id="IM1"><mml:mi mathvariant="normal">Γ</mml:mi></mml:math></inline-formula> distribution to the observed module sizes then sample from this distribution values for the simulated subset size, before the total number of features reaches 4800 (number of background features). Then, the background features were randomly placed in each subset corresponding to its size. Also, for each subset <inline-formula id="IE2"><mml:math id="IM2"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>i</mml:mi><mml:mo> </mml:mo><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>n</mml:mi></mml:math></inline-formula>, a functional feature <inline-formula id="IE3"><mml:math id="IM3"><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> belongs to the subset with the probability
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>∼</mml:mo><mml:msup><mml:mrow><mml:mn>1.618</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:math></disp-formula>where 1.618 is an approximation of the golden ratio and yields a reasonable distribution of the functional features: they are more likely to be included in the earlier subsets (Subsets 1 and 2) than the later ones.</p>
      </sec>
      <sec>
        <title>2.2.2 Real-world RNA-Seq expression data</title>
        <p>We employ TPOT-FSS on an RNA-Seq expression dataset of 78 individuals with MDD and 79 healthy controls (HCs) from <xref rid="btz470-B24" ref-type="bibr">Le <italic>et al.</italic> (2018b</xref>). RNA expression levels were quantified from reads of 19 968 annotated protein-coding genes and underwent a series of pre-processing steps including low read-count and outlier removal, technical and batch effect adjustment, and coefficient of variation filtering. Consequently, whole blood RNA-Seq measurements of 5912 transcripts were used to identify depression gene modules (DGMs) based on a read alignment protocol that enriched for the expression of antisense RNA. In this study, we apply TPOT-FSS to this processed dataset to verify our method’s ability to select the subset of features that is important for predicting the MDD outcome. The primary antisense-enriched dataset, along with a second pre-processed dataset enriched for gene expression, is available on the Github repository of the original MDD study (<ext-link ext-link-type="uri" xlink:href="https://github.com/insilico/DepressionGeneModules">https://github.com/insilico/DepressionGeneModules</ext-link>). We use the interconnected genes in 23 DGMs identified from the original RNA-Seq gene network module analysis (<xref rid="btz470-B24" ref-type="bibr">Le <italic>et al.</italic>, 2018b</xref>) as input for the FSS operator. We remark that these modules were constructed by an unsupervised ML method with dynamic tree cutting from a co-expression network. As a result, this prior knowledge of the gene structure does not depend on the diagnostic phenotype and thus yields no bias in the downstream analysis of TPOT-FSS.</p>
      </sec>
    </sec>
    <sec>
      <title>2.3 Performance assessment</title>
      <p>For each simulated and real-world dataset, after randomly splitting the entire data in two balanced smaller sets (75% training and 25% holdout), we trained TPOT-FSS with the Template FeatureSetSelector-Transformer-Classifier on training data to predict class (e.g. diagnostic phenotype in real-world data) in the holdout set. We assess the performance of TPOT-FSS by quantifying its ability to correctly select the most important subset (containing most functional features) in 100 replicates of TPOT runs on simulated data with known underlying truth. To prevent potential overfitting, we select the pipeline that is closest to the 90th percentile of the cross-validation accuracy to be optimal. This rationale is motivated by a similar procedure for optimizing the penalty coefficient in regularized regression where the most parsimonious model within one standard error of the minimum cross-validation error is picked (<xref rid="btz470-B15" ref-type="bibr">Hastie <italic>et al.</italic>, 2009</xref>). We compare the holdout (out-of-sample) accuracy of TPOT-FSS’s optimal pipeline on the holdout set with that of standard TPOT (with Transformer-Classifier Template, no FSS operator) and eXtreme Gradient Boosting (<xref rid="btz470-B5" ref-type="bibr">Chen and Guestrin, 2016</xref>), or XGBoost, which is a fast and an efficient implementation of the gradient tree boosting method that has shown much utility in many winning Kaggle solutions (<ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/">https://www.kaggle.com/</ext-link>) and been successfully incorporated in several neural network architectures (<xref rid="btz470-B35" ref-type="bibr">Ren <italic>et al.</italic>, 2017</xref>; <xref rid="btz470-B40" ref-type="bibr">Zheng <italic>et al.</italic>, 2017</xref>). In the family of gradient boosted decision trees, XGBoost accounts for complex non-linear interaction structure among features and leverages gradient descents and boosting (sequential ensemble of weak classifiers) to effectively produce a strong prediction model. To obtain the optimal performance for this baseline model, we tune XGBoost hyperparameters using TPOT Template with only one classifier XGBClassifier, which is imported from the xgboost python package. Because of stochasticity in the optimal pipeline from TPOT-FSS, standard TPOT and the tuned XGBoost model, we fit these models on the training data 100 times and compare 100 holdout accuracy values from each method. We choose accuracy to be the metric for comparison because phenotype is balanced in both simulated data and real-world data.</p>
    </sec>
    <sec>
      <title>2.4 Article drafting</title>
      <p>This article is collaboratively written using Manubot (<xref rid="btz470-B16" ref-type="bibr">Himmelstein <italic>et al.</italic>, 2019</xref>), a software that supports open paper writing via GitHub using the Markdown language. Manubot uses continuous integration to monitor changes and automatically update the article. Consequently, the latest version of this article is always available at <ext-link ext-link-type="uri" xlink:href="https://trang1618.github.io/tpot-fss-ms/">https://trang1618.github.io/tpot-fss-ms/</ext-link>.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <p>Our main goal is to test the performance of methods to identify features that discriminate between groups and optimize the classification accuracy.</p>
    <sec>
      <title>3.1 TPOT-FSS recommends optimal pipelines</title>
      <p>As discussed earlier in Section 2, the optimal pipeline from TPOT–FSS and standard TPOT is selected to be closest to the 90th percentile of the cross-validation accuracy. The optimal model of XGBoost holds properly tuned hyperparameters. For simulated dataset, the optimal pipeline selects subset <inline-formula id="IE4"><mml:math id="IM4"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> then constructs an approximate feature map for a linear kernel with Nystroem, which uses a subset of the data as the basis for the approximation. The final prediction is made with an extra-trees classifier that fits a number of randomized decision trees on various sub-samples of the dataset with the presented optimized parameters (<xref ref-type="fig" rid="btz470-F1">Fig. 1</xref>). For the real-world dataset, the most optimal pipeline selects subset DGM-5, one-hot encode the features, then, similar to simulated data, makes the final prediction with an extra-trees classifier with a different set of optimized parameters (<xref ref-type="fig" rid="btz470-F1">Fig. 1</xref>).</p>
    </sec>
    <sec>
      <title>3.2 Accuracy assessment of optimal pipelines</title>
      <p>We compare the accuracy produced by optimal models from TPOT–FSS, standard TPOT and XGBoost on classifying a simulated dataset with moderate interaction effect. We assign values of the effect size in the simulations to generate adequately challenging datasets so that the methods’ accuracies stay moderate and do not cluster around 0.5 or 1. The resulting accuracy values are comparable to those in real-world data. The dataset is split into 75% training and 25% holdout. The three models are built from the training dataset; then, the trained model is applied to the independent holdout data to obtain the holdout accuracy.</p>
      <p>We also apply the three methods to the RNA-Seq study of 78 MDD subjects and 79 HCs described in (<xref rid="btz470-B24" ref-type="bibr">Le <italic>et al.</italic>, 2018b</xref>). The dataset contains 5912 genes after pre-processing and filtering (see Section 2 for more detail). We excluded 277 genes that did not belong to 23 subsets of interconnected genes (DGMs) so that the dataset remains the same across the three methods. As with simulated data, all models are built from the training dataset (61 HC and 56 MDD), then the trained model is applied to the independent holdout data (18 HC and 22 MDD).</p>
      <p>For the simulated data, across all 100 model fits, the optimal TPOT-FSS pipeline yields an average holdout prediction accuracy of 0.65, while the standard TPOT without FSS and tuned XGBoost models respectively report an average holdout accuracy of 0.48 and 0.49 (<xref ref-type="fig" rid="btz470-F2">Fig. 2</xref>). This overfitting in the performance of these other two models is likely due to the models’ high flexibility that <italic>over-learns</italic> the training data, especially with the presence of many noisy background features.
</p>
      <fig id="btz470-F2" orientation="portrait" position="float">
        <label>Fig. 2.</label>
        <caption>
          <p>Performance comparison of three models: tuned XGBoost, optimal pipeline from standard TPOT and optimal pipeline from TPOT-FSS. In both simulated and real-world expression datasets, TPOT-FSS optimal pipelines significantly outperform those of XGBoost and standard TPOT</p>
        </caption>
        <graphic xlink:href="btz470f2"/>
      </fig>
      <p>Meanwhile, for the real-world RNA-Seq data, the optimal TPOT-FSS pipeline yields an average holdout prediction accuracy of 0.68, while the standard TPOT without FSS and tuned XGBoost models produce average holdout accuracies of 0.60 and 0.59, respectively across all 100 model fits (<xref ref-type="fig" rid="btz470-F2">Fig. 2</xref>). In summary, the optimal models from standard TPOT and XGBoost perform better in real-world data compared with simulated data but still worse than that of TPOT-FSS. In both datasets, separate Welch two-sample one-sided <italic>t</italic>-tests show TPOT-FSS optimal pipelines significantly outperform those of XGBoost and standard TPOT (all <italic>P</italic> values <inline-formula id="IE5"><mml:math id="IM5"><mml:mo>&lt;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>).</p>
    </sec>
    <sec>
      <title>3.3 Consistency in selecting subsets of TPOT-FSS</title>
      <p>Our simulation design produces a reasonable distribution of the functional features in all subsets, of which proportions are shown in <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S1</xref>. According to <xref ref-type="disp-formula" rid="E1">Equation (1)</xref>, the earlier the subset, the more functional features it has. Therefore, our aim is to determine how well TPOT-FSS can identify the first subset (<inline-formula id="IE6"><mml:math id="IM6"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>) that contains the largest number of informative features. In 100 replications, TPOT-FSS correctly selects subset <inline-formula id="IE7"><mml:math id="IM7"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> in 75 resulting pipelines (<xref ref-type="fig" rid="btz470-F3">Fig. 3</xref>), with the highest average holdout accuracy (0.69 across all 75 pipelines).
</p>
      <fig id="btz470-F3" orientation="portrait" position="float">
        <label>Fig. 3.</label>
        <caption>
          <p>TPOT-FSS’s holdout accuracy (vertical) with selected subset (horizontal) in 100 replications on the simulated data. Number of pipeline inclusions of each subset is displayed above the boxplots. Subset <italic>S</italic><sub>1</sub> is the most frequent to be included in the final pipeline and yields the best prediction accuracy in the holdout set. Alternating colors separate adjacent subsets for better visualization</p>
        </caption>
        <graphic xlink:href="btz470f3"/>
      </fig>
      <p>For the RNA-Seq data, in 100 replications, TPOT-FSS selects DGM-5 (291 genes) 64 times to be the subset most predictive of the diagnosis status (<xref ref-type="fig" rid="btz470-F4">Fig. 4</xref>), with the highest average holdout accuracy of 0.636 across 64 pipelines. In the previous study with a modular network approach, we showed that DGM-5 has statistically significant associations with depression severity measured by the Montgomery-Åsberg Depression Scale (MADRS). Although there is no direct link between the top genes of the module (<xref ref-type="fig" rid="btz470-F5">Fig. 5a</xref>) and MDD in the literature, many of these genes interact with other MDD-related genes. For example, NR2C2 interacts with FKBP5 gene whose association with MDD has been strongly suggested (<xref rid="btz470-B3" ref-type="bibr">Binder <italic>et al.</italic>, 2004</xref>; <xref rid="btz470-B22" ref-type="bibr">Lavebratt <italic>et al.</italic>, 2010</xref>; <xref rid="btz470-B38" ref-type="bibr">Tatro <italic>et al.</italic>, 2009</xref>). Many of DGM-5’s top genes, including FAM13A, NR2C2, PP7080 and OXR1, were previously shown to have significant association with the diagnosis phenotype using a Relief-based feature selection method (<xref rid="btz470-B25" ref-type="bibr">Le <italic>et al.</italic>, 2019</xref>). Further, with 82% overlap of DGM-5’s genes in a separate dataset from the RNA-Seq study by Mostafavi <italic>et al.</italic> (2013), this gene collection’s enrichment score was also shown to be significantly associated with the diagnosis status in this independent dataset.
</p>
      <fig id="btz470-F4" orientation="portrait" position="float">
        <label>Fig. 4.</label>
        <caption>
          <p>TPOT-FSS’s holdout accuracy (vertical) with selected subset (horizontal) in 100 replications on the RNA-Seq data. Number of pipeline inclusions of each subset is displayed above the boxplots. Subsets DGM-5 and DGM-13 are the most frequent to be included in the final pipeline. Pipelines that include DGM-5, on average, produce higher MDD prediction accuracies in the holdout set</p>
        </caption>
        <graphic xlink:href="btz470f4"/>
      </fig>
      <fig id="btz470-F5" orientation="portrait" position="float">
        <label>Fig. 5.</label>
        <caption>
          <p>Permutation importance scores of the top twenty features in the optimal pipeline that selects (<bold>a</bold>) DGM-5 and one that selects (<bold>b</bold>) DGM-13. Comprehensive importance scores of the all features computed by permutation from the optimal pipelines are provided in <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S2</xref></p>
        </caption>
        <graphic xlink:href="btz470f5"/>
      </fig>
      <p>After DGM-5, DGM-13 (134 genes) was selected by TPOT-FSS 30 times (<xref ref-type="fig" rid="btz470-F4">Fig. 4</xref>), with an average holdout accuracy of 0.563 across 30 pipelines. The previous network approach did not find statistically significant association between this module’s enrichment score and the MADRS. Although many of the top genes (<xref ref-type="fig" rid="btz470-F5">Fig. 5b</xref>) do not have direct disease association, several have been linked to depression-like behavior in animal studies such as PPP1R16A (<xref rid="btz470-B36" ref-type="bibr">Sibille <italic>et al.</italic>, 2009</xref>) and CASKIN1 (<xref rid="btz470-B18" ref-type="bibr">Katano <italic>et al.</italic>, 2018</xref>). The RGL4 gene, a Ral guanine nucleotide dissociation stimulator, was found to have a rare protein disruptive variant in at least one suicide patient among 60 other mutations (<xref rid="btz470-B39" ref-type="bibr">Tombácz <italic>et al.</italic>, 2017</xref>).</p>
    </sec>
    <sec>
      <title>3.4 Computational expense</title>
      <p>For a dataset of the size simulated in our study (<italic>m </italic>=<italic> </italic>200 samples and <italic>P </italic>=<italic> </italic>5000 attributes), standard TPOT has a 18.5-h runtime on a low performance computing machine with an Intel Xeon E5-2690 2.60 GHz CPU, 28 cores and 256 GB of RAM, whereas TPOT-FSS has a 65-min runtime, ∼17 times faster. On the same low performance computing machine, each replication of standard TPOT on the RNA-Seq data takes on average 13.3 h, whereas TPOT-FSS takes 40 min, ∼20 times faster.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <p>To our knowledge, TPOT-FSS is the first AutoML tool to offer the option of feature selection at the group level. Previously, it was computationally expensive for any AutoML program to process biomedical big data. TPOT-FSS is able to identify the most meaningful group of features to include in the prediction pipeline. We assess TPOT-FSS’s holdout prediction accuracy compared with standard TPOT and XGBoost, another state-of-the-art ML method. We apply TPOT-FSS to real-world RNA-Seq data to demonstrate the identification of biologically relevant groups of genes.</p>
    <p>Implemented with a strongly typed GP, Template provides more flexibility by allowing users to pre-specify a particular pipeline structure based on their knowledge, which speeds up AutoML process and provides potentially more interpretable results. For example, in high-dimensional data, dimensionality reduction or feature selection algorithms are preferably included at the beginning of the pipelines via Template to identify important features and, meanwhile, reduce computation time. For datasets with categorical features, pre-processing operators for encoding those features such as one-hot encoder should be specified in the pipeline structure to improve pipelines’ performance. Template was utilized in this study to specify the FSS as the first step of the pipeline, which enables the comparison between the two TPOT implementations, with and without FSS.</p>
    <p>We simulated data of the similar scale and challenging enough for the models to have similar predictive power as in the real-world RNA-Seq data. TPOT-FSS correctly selects the subset with the most important features in the majority of replications and produces high average holdout accuracy of 0.69. In both simulated and RNASeq data, the final TPOT-FSS pipeline outperforms that of standard TPOT and XGBoost. The low holdout accuracies of standard TPOT and XGBoost are expected because of the few signals in a high-dimenional feature space of the data. Meanwhile, TPOT-FSS finds a more compact feature space to operate on, resulting in higher prediction accuracy and lower computational expense.</p>
    <p>Interestingly enough, TPOT-FSS repeatedly selects DGM-5 to include in the final pipeline. In a previous study, we showed DGM-5 and DGM-17 enrichment scores were significantly associated with depression severity (<xref rid="btz470-B24" ref-type="bibr">Le <italic>et al.</italic>, 2018b</xref>). We also remarked that DGM-5 contains many genes that are biologically relevant or previously associated with mood disorders (<xref rid="btz470-B24" ref-type="bibr">Le <italic>et al.</italic>, 2018b</xref>) and its enriched pathways such as apoptosis indicates a genetic signature of MDD pertaining to shrinkage of brain region-specific volume due to cell loss (<xref rid="btz470-B10" ref-type="bibr">Eilat <italic>et al.</italic>, 1999</xref>; <xref rid="btz470-B27" ref-type="bibr">McKinnon <italic>et al.</italic>, 2009</xref>). TPOT-FSS also selects DGM-13 as a potentially predictive group of features with smaller average holdout accuracy compared with DGM-5 (0.563 &lt; 0.636). The lack of previously found association of these genes with the phenotype is likely because MDD is a complex disorder of heterogeneous etiology (<xref rid="btz470-B26" ref-type="bibr">Levinson <italic>et al.</italic>, 2014</xref>). Hence, the clinical diagnosis is the accumulative result of coordinated variation of many genes in the module, especially ones with high importance scores. Future studies to refine and characterize genes in DGM-13 as well as DGM-5 may deploy expression quantitative trait loci (e-QTL) or interaction QTL analysis to discover disease-associated variants (<xref rid="btz470-B20" ref-type="bibr">Lareau <italic>et al.</italic>, 2016</xref>).</p>
    <p>Complexity–interpretability tradeoff is an important topic to discuss in the context of AutoML. Although arbitrarily shaped pipelines may yield predictions competitive to human-level performance, these pipelines are often too complex to be interpretable. Vice versa, a simpler pipeline with defined steps of operators may be easier to interpret but yield suboptimal prediction accuracy. Finding the balance between pipeline complexity, model interpretation and generalization remains a challenging task for AutoML application in biomedical big data. With FSS, in the terminology of EA, each pipeline individual of a TPOT generation during optimization holds lower complexity due to the selected subset’s lower dimension compared with that of the entire dataset. We hope that, with the complexity reduction from imposing a strongly type GP template and FSS, a small loss in dataset-specific predictive accuracy can be compensated by considerable increase in interpretability and generalizability. In this study, the resulting TPOT-FSS pipelines are more interpretable with only two simple optimized operators after the FSS: a transformer and a classifier. In the case of the expression analysis, these pipelines also highlight two small sets of interconnected genes that contain candidates for MDD and related disorders. Additionally, complexity reduction results in more efficient computation, which is strongly desirable in biomedical big data analysis.</p>
    <p>A limitation of the FSS analysis is the required pre-definition of subsets prior to executing TPOT-FSS. Although this characteristic of an intelligent system is desirable when a prior knowledge on the biomedical data is available, it might pose as a challenge when this knowledge is inadequate, such as when analyzing data of a brand-new disease. Nevertheless, one can perform a clustering method such as <italic>k</italic>-means to group features prior to performing TPOT-FSS on the data. Another limitation of the current implementation of TPOT-FSS is its restricted ability to select only one subset. A future design to support tree structures for Template will enable TPOT-FSS to identify more than one subset that have high predictive power of the outcome. A new operator that combines the data subsets will prove useful in this design. Extensions of TPOT-FSS will also involve overlapping subsets, which will require pipeline complexity reformulation beyond the total number of operators included in a pipeline. Specifically, in the case of overlapping subsets, the number of features in the selected subset(s) is expected to be an element of the complexity calculation. Extension of TPOT-FSS to GWAS is straightforward. However, because of the low predictive power of variants in current GWAS, alternative metrics beside accuracy, balanced accuracy or area under the receiving operator characteristic curve will need to be designed and included in the fitness function of TPOT’s EA.</p>
    <p>In this study, we developed two new operators for TPOT, FSS and Template, to enhance its performance on high-dimensional data by simplifying the pipeline structure and reducing the computational expense. FSS helps users leverage domain knowledge to narrow down important features for further interpretation, and Template largely increases flexibility of TPOT via customizing pipeline structure. Future extension and integration of these two operators have the potential to enrich the application of AutoML on different real world biomedical problems.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the National Institutes of Health grant numbers LM010098, LM012601 and AI116794. </p>
    <p><italic>Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>btz470_Supplementary_Files</label>
      <media xlink:href="btz470_supplementary_files.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btz470-B1">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Banzhaf</surname><given-names>W.</given-names></name></person-group><etal>et al</etal> (<year>1998</year>) <italic>Genetic Programming: An Introduction: on the Automatic Evolution of Computer Programs and Its Applications</italic>. Morgan Kaufmann Publishers Inc., San Francisco, CA.</mixed-citation>
    </ref>
    <ref id="btz470-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bergstra</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name></person-group> (<year>2012</year>) 
<article-title>Random search for hyper-parameter optimization</article-title>. <source>J. Mach. Learn. Res</source>., <volume>13</volume>, <fpage>281</fpage>–<lpage>305</lpage>.</mixed-citation>
    </ref>
    <ref id="btz470-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Binder</surname><given-names>E.B.</given-names></name></person-group><etal>et al</etal> (<year>2004</year>) 
<article-title>Polymorphisms in FKBP5 are associated with increased recurrence of depressive episodes and rapid response to antidepressant treatment</article-title>. <source>Nat. Genet</source>., <volume>36</volume>, <fpage>1319</fpage>–<lpage>1325</lpage>.<pub-id pub-id-type="pmid">15565110</pub-id></mixed-citation>
    </ref>
    <ref id="btz470-B4">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) Autostacker: a compositional evolutionary learning system. In: <italic>Proceedings of the Genetic and Evolutionary Computation Conference, Kyoto, Japan</italic>. ACM, New York, NY, pp. 402–409.</mixed-citation>
    </ref>
    <ref id="btz470-B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Guestrin</surname><given-names>C.</given-names></name></person-group> (<year>2016</year>) XGBoost. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD ’16. ACM Press.</mixed-citation>
    </ref>
    <ref id="btz470-B6">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Thornton</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms. In: <italic>Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Chicago, Illinois, USA</italic>. ACM, New York, NY, pp. 847–855.</mixed-citation>
    </ref>
    <ref id="btz470-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Deb</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2002</year>) 
<article-title>A fast and elitist multiobjective genetic algorithm: NSGA-II</article-title>. <source>IEEE Trans. Evol. Comput</source>., <volume>6</volume>, <fpage>182</fpage>–<lpage>197</lpage>.</mixed-citation>
    </ref>
    <ref id="btz470-B8">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>de Sá</surname><given-names>A.G.C.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) <chapter-title>RECIPE: a grammar-based framework for automatically evolving classification pipelines</chapter-title> In: <source>Lecture Notes in Computer Science</source>. 
<publisher-name>Springer International Publishing</publisher-name>, pp. <fpage>246</fpage>–<lpage>261</lpage>.</mixed-citation>
    </ref>
    <ref id="btz470-B9">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Eiben</surname><given-names>A.E.</given-names></name>, <name name-style="western"><surname>Smith</surname><given-names>J.E.</given-names></name></person-group> (<year>2010</year>) <source>Introduction to Evolutionary Computing 1. ed., Corr. 2. Printing, Softcover Version of Original Hardcover ed. 2003</source>. 
<publisher-name>Springer</publisher-name>, 
<publisher-loc>Berlin</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btz470-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Eilat</surname><given-names>E.</given-names></name></person-group><etal>et al</etal> (<year>1999</year>) 
<article-title>Increased apoptosis in patients with major depression: a preliminary study</article-title>. <source>J. Immunol</source>., <volume>163</volume>, <fpage>533</fpage>–<lpage>534</lpage>.<pub-id pub-id-type="pmid">10384158</pub-id></mixed-citation>
    </ref>
    <ref id="btz470-B11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Brochu</surname><given-names>E.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv.</mixed-citation>
    </ref>
    <ref id="btz470-B12">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Feurer</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) <chapter-title>Efficient and robust automated machine learning</chapter-title> In: <person-group person-group-type="editor"><name name-style="western"><surname>Cortes</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (eds) <source>Advances in Neural Information Processing Systems 28</source>. 
<publisher-name>Curran Associates, Inc</publisher-name>, New York, pp. <fpage>2962</fpage>–<lpage>2970</lpage>.</mixed-citation>
    </ref>
    <ref id="btz470-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fortin</surname><given-names>F.-A.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>DEAP: evolutionary algorithms made easy</article-title>. <source>J. Mach. Learn. Res</source>., <volume>13</volume>, <fpage>2171</fpage>–<lpage>2175</lpage>.</mixed-citation>
    </ref>
    <ref id="btz470-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gijsbers</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Vanschoren</surname><given-names>J.</given-names></name></person-group> (<year>2019</year>) 
<article-title>GAMA: genetic automated machine learning assistant</article-title>. <source>J. Open Source Softw</source>., <volume>4</volume>, <fpage>1132</fpage>.</mixed-citation>
    </ref>
    <ref id="btz470-B15">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Hastie</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) <source>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</source>, <edition>2nd edn</edition>
<publisher-name>Springer</publisher-name>, 
<publisher-loc>New York, NY</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btz470-B16">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Himmelstein</surname><given-names>D.S.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) Open collaborative writing with Manubot.</mixed-citation>
    </ref>
    <ref id="btz470-B17">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Dewancker</surname><given-names>I.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) A stratified analysis of Bayesian optimization methods. arXiv.</mixed-citation>
    </ref>
    <ref id="btz470-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Katano</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Distribution of Caskin1 protein and phenotypic characterization of its knockout mice using a comprehensive behavioral test battery</article-title>. <source>Mol. Brain</source>, <volume>11</volume>, 63.</mixed-citation>
    </ref>
    <ref id="btz470-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kotthoff</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Auto-WEKA 2.0: automatic model selection and hyperparameter optimization in WEKA</article-title>. <source>J. Mach. Learn. Res</source>., <volume>18</volume>, <fpage>1</fpage>–<lpage>5</lpage>.</mixed-citation>
    </ref>
    <ref id="btz470-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lareau</surname><given-names>C.A.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>An interaction quantitative trait loci tool implicates epistatic functional variants in an apoptosis pathway in smallpox vaccine eQTL data</article-title>. <source>Genes Immun</source>., <volume>17</volume>, <fpage>244</fpage>–<lpage>250</lpage>.<pub-id pub-id-type="pmid">27052692</pub-id></mixed-citation>
    </ref>
    <ref id="btz470-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lareau</surname><given-names>C.A.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Differential co-expression network centrality and machine learning feature selection for identifying susceptibility hubs in networks with scale-free structure</article-title>. <source>BioData Min</source>., <volume>8</volume>, <fpage>5</fpage>.<pub-id pub-id-type="pmid">25685197</pub-id></mixed-citation>
    </ref>
    <ref id="btz470-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lavebratt</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>Variations in FKBP5 and BDNF genes are suggestively associated with depression in a Swedish population-based cohort</article-title>. <source>J. Affect. Disord</source>., <volume>125</volume>, <fpage>249</fpage>–<lpage>255</lpage>.<pub-id pub-id-type="pmid">20226536</pub-id></mixed-citation>
    </ref>
    <ref id="btz470-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Le</surname><given-names>T.T.</given-names></name></person-group><etal>et al</etal> (<year>2018a</year>) 
<article-title>Integrated machine learning pipeline for aberrant biomarker enrichment (i-mAB): characterizing clusters of differentiation within a compendium of systemic lupus erythematosus patients</article-title>. <source>AMIA Annu. Symp. Proc</source>., <volume>2018</volume>, <fpage>1358</fpage>–<lpage>1367</lpage>.<pub-id pub-id-type="pmid">30815180</pub-id></mixed-citation>
    </ref>
    <ref id="btz470-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Le</surname><given-names>T.T.</given-names></name></person-group><etal>et al</etal> (<year>2018b</year>) 
<article-title>Identification and replication of RNA-Seq gene network modules associated with depression severity</article-title>. <source>Transl. Psychiatry</source>, <volume>8</volume>, 180.</mixed-citation>
    </ref>
    <ref id="btz470-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Le</surname><given-names>T.T.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>STatistical Inference Relief (STIR) feature selection</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>1358</fpage>–<lpage>1365</lpage>.<pub-id pub-id-type="pmid">30239600</pub-id></mixed-citation>
    </ref>
    <ref id="btz470-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Levinson</surname><given-names>D.F.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Genetic studies of major depressive disorder: why are there no genome-wide association study findings and what can we do about it?</article-title><source>Biol. Psychiatry</source>, <volume>76</volume>, <fpage>510</fpage>–<lpage>512</lpage>.<pub-id pub-id-type="pmid">25201436</pub-id></mixed-citation>
    </ref>
    <ref id="btz470-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>McKinnon</surname><given-names>M.C.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>A meta-analysis examining clinical predictors of hippocampal volume in patients with major depressive disorder</article-title>. <source>J. Psychiatry Neurosci</source>., <volume>34</volume>, <fpage>41</fpage>–<lpage>54</lpage>.<pub-id pub-id-type="pmid">19125212</pub-id></mixed-citation>
    </ref>
    <ref id="btz470-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Montana</surname><given-names>D.J.</given-names></name></person-group> (<year>1995</year>) 
<article-title>Strongly typed genetic programming</article-title>. <source>Evol. Comput</source>., <volume>3</volume>, <fpage>199</fpage>–<lpage>230</lpage>.</mixed-citation>
    </ref>
    <ref id="btz470-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mostafavi</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Type I interferon signaling genes in recurrent major depression: increased expression detected by whole-blood RNA sequencing</article-title>. <source>Mol. Psychiatry</source>, <volume>19</volume>, <fpage>1267</fpage>–<lpage>1274</lpage>.<pub-id pub-id-type="pmid">24296977</pub-id></mixed-citation>
    </ref>
    <ref id="btz470-B30">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Olson</surname><given-names>R.S.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) Evaluation of a tree-based pipeline optimization tool for automating data science. In: <italic>Proceedings of the 2016 on Genetic and Evolutionary Computation Conference - GECCO ’16, Denver, Colorado, USA.</italic> ACM Press New York, NY, pp. 485–492.</mixed-citation>
    </ref>
    <ref id="btz470-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Olson</surname><given-names>R.S.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Data-driven advice for applying machine learning to bioinformatics problems</article-title>. <source>Pac. Symp. Biocomput</source>., <volume>23</volume>, <fpage>192</fpage>–<lpage>203</lpage>.<pub-id pub-id-type="pmid">29218881</pub-id></mixed-citation>
    </ref>
    <ref id="btz470-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Olson</surname><given-names>R.S.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>PMLB: a large benchmark suite for machine learning evaluation and comparison</article-title>. <source>BioData Mining</source>, <volume>10</volume>, 1–36.</mixed-citation>
    </ref>
    <ref id="btz470-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pedregosa</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Scikit-learn: machine Learning in Python</article-title>. <source>J. Mach. Learn. Res.</source>, 12, 2825–2830.</mixed-citation>
    </ref>
    <ref id="btz470-B34">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Olson</surname><given-names>R.S.</given-names></name>, <name name-style="western"><surname>Moore</surname><given-names>J.H.</given-names></name></person-group> (<year>2016</year>) Identifying and harnessing the building blocks of machine learning pipelines for sensible initialization of a data science automation tool. <italic>arXiv</italic>.</mixed-citation>
    </ref>
    <ref id="btz470-B35">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>X.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) <chapter-title>A novel image classification method with CNN-XGBoost model</chapter-title> In: <source>Digital Forensics and Watermarking</source>. 
<publisher-name>Springer International Publishing</publisher-name>, Springer, Cham, pp. <fpage>378</fpage>–<lpage>390</lpage>.</mixed-citation>
    </ref>
    <ref id="btz470-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sibille</surname><given-names>E.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>A molecular signature of depression in the amygdala</article-title>. <source>Am. J. Psychiatry</source>, <volume>166</volume>, <fpage>1011</fpage>–<lpage>1024</lpage>.<pub-id pub-id-type="pmid">19605536</pub-id></mixed-citation>
    </ref>
    <ref id="btz470-B37">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Sohn</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) Toward the automated analysis of complex diseases in genome-wide association studies using genetic programming. In: <italic>Proceedings of the Genetic and Evolutionary Computation Conference on - GECCO ’17</italic>. ACM Press.</mixed-citation>
    </ref>
    <ref id="btz470-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tatro</surname><given-names>E.T.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>Modulation of glucocorticoid receptor nuclear translocation in neurons by immunophilins FKBP51 and FKBP52: implications for major depressive disorder</article-title>. <source>Brain Res</source>., <volume>1286</volume>, <fpage>1</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">19545546</pub-id></mixed-citation>
    </ref>
    <ref id="btz470-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tombácz</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>High-coverage whole-exome sequencing identifies candidate genes for suicide in victims with major depressive disorder</article-title>. <source>Sci. Rep</source>., <volume>7</volume>, 7106.</mixed-citation>
    </ref>
    <ref id="btz470-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Short-term load forecasting using EMD-LSTM neural networks with a Xgboost algorithm for feature importance evaluation</article-title>. <source>Energies</source>, <volume>10</volume>, <fpage>1168.</fpage></mixed-citation>
    </ref>
  </ref-list>
</back>
