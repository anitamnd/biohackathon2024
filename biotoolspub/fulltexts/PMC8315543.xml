<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?subarticle pone.0255030.r001?>
<?properties open_access?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS One</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLoS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8315543</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-20-33961</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0255030</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Data Management</subject>
          <subj-group>
            <subject>Data Visualization</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Preprocessing</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Preprocessing</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Data Management</subject>
          <subj-group>
            <subject>Metadata</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Cognitive Science</subject>
            <subj-group>
              <subject>Cognitive Psychology</subject>
              <subj-group>
                <subject>Perception</subject>
                <subj-group>
                  <subject>Sensory Perception</subject>
                  <subj-group>
                    <subject>Vision</subject>
                  </subj-group>
                </subj-group>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Cognitive Psychology</subject>
            <subj-group>
              <subject>Perception</subject>
              <subj-group>
                <subject>Sensory Perception</subject>
                <subj-group>
                  <subject>Vision</subject>
                </subj-group>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Social Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Cognitive Psychology</subject>
            <subj-group>
              <subject>Perception</subject>
              <subj-group>
                <subject>Sensory Perception</subject>
                <subj-group>
                  <subject>Vision</subject>
                </subj-group>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Sensory Perception</subject>
            <subj-group>
              <subject>Vision</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Data Management</subject>
          <subj-group>
            <subject>Data Processing</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Applied Mathematics</subject>
            <subj-group>
              <subject>Algorithms</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Simulation and Modeling</subject>
          <subj-group>
            <subject>Algorithms</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Computer Architecture</subject>
          <subj-group>
            <subject>Computer Hardware</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Numerical Analysis</subject>
            <subj-group>
              <subject>Interpolation</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ReVisE: Remote visualization environment for large numerical simulation datasets</article-title>
      <alt-title alt-title-type="running-head">ReVisE: Remote visualization environment for large numerical simulation datasets</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1822-3937</contrib-id>
        <name>
          <surname>Orlov</surname>
          <given-names>Stepan</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Conceptualization</role>
        <role content-type="https://casrai.org/credit/">Methodology</role>
        <role content-type="https://casrai.org/credit/">Project administration</role>
        <role content-type="https://casrai.org/credit/">Software</role>
        <role content-type="https://casrai.org/credit/">Writing – original draft</role>
        <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff"/>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3715-5829</contrib-id>
        <name>
          <surname>Kuzin</surname>
          <given-names>Alexey</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Conceptualization</role>
        <role content-type="https://casrai.org/credit/">Formal analysis</role>
        <role content-type="https://casrai.org/credit/">Investigation</role>
        <role content-type="https://casrai.org/credit/">Software</role>
        <role content-type="https://casrai.org/credit/">Validation</role>
        <role content-type="https://casrai.org/credit/">Writing – original draft</role>
        <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9973-1705</contrib-id>
        <name>
          <surname>Zhuravlev</surname>
          <given-names>Alexey</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Investigation</role>
        <role content-type="https://casrai.org/credit/">Software</role>
        <role content-type="https://casrai.org/credit/">Visualization</role>
        <role content-type="https://casrai.org/credit/">Writing – original draft</role>
        <xref rid="aff001" ref-type="aff"/>
        <xref rid="econtrib001" ref-type="author-notes">
          <sup>‡</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3953-3440</contrib-id>
        <name>
          <surname>Reshetnikov</surname>
          <given-names>Vyacheslav</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Investigation</role>
        <role content-type="https://casrai.org/credit/">Software</role>
        <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff"/>
        <xref rid="econtrib001" ref-type="author-notes">
          <sup>‡</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Usik</surname>
          <given-names>Egor</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Software</role>
        <role content-type="https://casrai.org/credit/">Validation</role>
        <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff"/>
        <xref rid="econtrib001" ref-type="author-notes">
          <sup>‡</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kiev</surname>
          <given-names>Vladislav</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Data curation</role>
        <role content-type="https://casrai.org/credit/">Formal analysis</role>
        <role content-type="https://casrai.org/credit/">Investigation</role>
        <role content-type="https://casrai.org/credit/">Resources</role>
        <role content-type="https://casrai.org/credit/">Software</role>
        <role content-type="https://casrai.org/credit/">Validation</role>
        <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff"/>
        <xref rid="econtrib001" ref-type="author-notes">
          <sup>‡</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pyatlin</surname>
          <given-names>Andrey</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Conceptualization</role>
        <role content-type="https://casrai.org/credit/">Formal analysis</role>
        <role content-type="https://casrai.org/credit/">Investigation</role>
        <role content-type="https://casrai.org/credit/">Resources</role>
        <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff"/>
        <xref rid="econtrib001" ref-type="author-notes">
          <sup>‡</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <addr-line>Institute of Machinery, Materials, and Transport, Peter the Great St. Petersburg Polytechnic University, St. Petersburg, Russian Federation</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Lisacek</surname>
          <given-names>Frederique</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Swiss Institute of Bioinformatics, SWITZERLAND</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
      </fn>
      <fn fn-type="other" id="econtrib001">
        <p>‡ These authors also contributed equally to this work.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>majorsteve@mail.ru</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>27</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <volume>16</volume>
    <issue>7</issue>
    <elocation-id>e0255030</elocation-id>
    <history>
      <date date-type="received">
        <day>28</day>
        <month>10</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>8</day>
        <month>7</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 Orlov et al</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Orlov et al</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pdf" xlink:href="pone.0255030.pdf"/>
    <abstract>
      <p>The paper presents a new open-source visualization system, named ReVisE, aimed to provide interactive visualization of large datasets, which are results of complex numerical simulations. These datasets are hosted on a remote server or a supercomputer. The design of the system is briefly described. Dataset representation, proposed for interactive visualization and implemented in the system, is discussed. The effectiveness of our approach is confirmed by results of performance measurements on test and real-life large datasets. A comparison with other visualization systems is presented. Future plans of system development are outlined.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100006769</institution-id>
            <institution>russian science foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>18-11-00245</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1822-3937</contrib-id>
          <name>
            <surname>Orlov</surname>
            <given-names>Stepan</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award002">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100006769</institution-id>
            <institution>russian science foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>18-11-00245</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3715-5829</contrib-id>
          <name>
            <surname>Kuzin</surname>
            <given-names>Alexey</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award003">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100006769</institution-id>
            <institution>russian science foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>18-11-00245</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9973-1705</contrib-id>
          <name>
            <surname>Zhuravlev</surname>
            <given-names>Alexey</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award004">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100006769</institution-id>
            <institution>russian science foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>18-11-00245</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3953-3440</contrib-id>
          <name>
            <surname>Reshetnikov</surname>
            <given-names>Vyacheslav</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award005">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100006769</institution-id>
            <institution>russian science foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>18-11-00245</award-id>
        <principal-award-recipient>
          <name>
            <surname>Usik</surname>
            <given-names>Egor</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award006">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100006769</institution-id>
            <institution>russian science foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>18-11-00245</award-id>
        <principal-award-recipient>
          <name>
            <surname>Pyatlin</surname>
            <given-names>Andrey</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>This work has been funded by Russian Science Foundation URL: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://rscf.ru/en/" ext-link-type="uri">https://rscf.ru/en/</ext-link> Grant No: 18-11-00245 Authors who received award: S. Orlov, A. Kuzin, A. Zhuravlev, V. Reshetnikov, E. Usik, A. Pyatlin The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="12"/>
      <table-count count="4"/>
      <page-count count="24"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>The source code of the ReVisE system is available in a public repository hosted on GitHub under the URL <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/deadmorous/revise" ext-link-type="uri">https://github.com/deadmorous/revise</ext-link> All datasets considered in the paper are available for download at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://ftp.mpksoft.ru/revise_datasets/" ext-link-type="uri">https://ftp.mpksoft.ru/revise_datasets/</ext-link> Step-by step instruction on how to use those datasets and repeat ReVisE performance measurements are available in two public protocols, which are as follows: Build ReVisE on Ubuntu <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.protocols.io/view/build-revise-on-ubuntu-bruwm6xe" ext-link-type="uri">https://www.protocols.io/view/build-revise-on-ubuntu-bruwm6xe</ext-link>
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://dx.doi.org/10.17504/protocols.io.bruwm6xe" ext-link-type="uri">dx.doi.org/10.17504/protocols.io.bruwm6xe</ext-link> Prepare and run test on available dataset <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.protocols.io/view/prepare-and-run-test-on-available-dataset-bruzm6x6" ext-link-type="uri">https://www.protocols.io/view/prepare-and-run-test-on-available-dataset-bruzm6x6</ext-link>
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://dx.doi.org/10.17504/protocols.io.bruzm6x6" ext-link-type="uri">dx.doi.org/10.17504/protocols.io.bruzm6x6</ext-link>.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>The source code of the ReVisE system is available in a public repository hosted on GitHub under the URL <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/deadmorous/revise" ext-link-type="uri">https://github.com/deadmorous/revise</ext-link> All datasets considered in the paper are available for download at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://ftp.mpksoft.ru/revise_datasets/" ext-link-type="uri">https://ftp.mpksoft.ru/revise_datasets/</ext-link> Step-by step instruction on how to use those datasets and repeat ReVisE performance measurements are available in two public protocols, which are as follows: Build ReVisE on Ubuntu <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.protocols.io/view/build-revise-on-ubuntu-bruwm6xe" ext-link-type="uri">https://www.protocols.io/view/build-revise-on-ubuntu-bruwm6xe</ext-link>
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://dx.doi.org/10.17504/protocols.io.bruwm6xe" ext-link-type="uri">dx.doi.org/10.17504/protocols.io.bruwm6xe</ext-link> Prepare and run test on available dataset <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.protocols.io/view/prepare-and-run-test-on-available-dataset-bruzm6x6" ext-link-type="uri">https://www.protocols.io/view/prepare-and-run-test-on-available-dataset-bruzm6x6</ext-link>
<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://dx.doi.org/10.17504/protocols.io.bruzm6x6" ext-link-type="uri">dx.doi.org/10.17504/protocols.io.bruzm6x6</ext-link>.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>1 Introduction</title>
    <p>Today’s supercomputers are often used for numerical simulations of complex problems, such as aerodynamics, hydrodynamics, oil flow through porous media, and many others. Computational meshes (further referred to simply as meshes) used in those simulations can reach quite large size, e.g., 10<sup>8</sup>–10<sup>10</sup> nodes. Therefore, simulation results are represented by quite large datasets, especially in the case of unsteady problems, since the time adds one more dimension.</p>
    <p>In this paper we address the problem of interactive visualization of large datasets resulting from complex simulations. Our contribution to the problem solution is software implementation of a new open-source visualization system, named ReVisE (Remote Visualization Environment), available at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/deadmorous/revise" ext-link-type="uri">https://github.com/deadmorous/revise</ext-link>. The motivation behind the idea to create yet another visualization system is basically that existing widely used systems have serious performance issues, making truly interactive visualization only possible for relatively small datasets (up to 10<sup>6</sup> nodes). Although systems for visualizing large datasets exist, e.g., NVIDIA IndeX (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://developer.nvidia.com/nvidia-index" ext-link-type="uri">https://developer.nvidia.com/nvidia-index</ext-link>) and Sight [<xref rid="pone.0255030.ref001" ref-type="bibr">1</xref>], those are not widely used, in particular, due to their limited availability.</p>
    <p>Our main goal is to design and a common way of data processing for visualizing fields on large meshes. This implies a practical proof that our approach to visualization is feasible. The current state of ReVisE software can be viewed as such a proof. It also shows that our visualization system can run (and give satisfactory results) on different hardware, from PCs to multi-GPU servers.</p>
    <p>To give some context for our investigation, let us briefly discuss major existing software products for the visualization of large datasets.</p>
    <p>Kitware ParaView [<xref rid="pone.0255030.ref002" ref-type="bibr">2</xref>] is an open-source multi-platform visualization system. It was developed to visualize extremely large datasets, with the ability to distribute data processing across many remote computational nodes. It can be deployed on both supercomputers and laptops (with lower capabilities). ParaView supports a mechanism to develop specialized plugins. An example of ParaView usage the system is presented in [<xref rid="pone.0255030.ref003" ref-type="bibr">3</xref>].</p>
    <p>NVIDIA IndeX is another framework for remote visualization. It uses computing capabilities of GPUs to process big data. NVIDIA IndeX is designed for real-time visualization and can run on a GPU-accelerated cluster. In addition, a specialized plugin to ParaView has been developed. The plugin improves visualization performance for large datasets. It is claimed that the framework has the good scalability across GPU-accelerated nodes.</p>
    <p>Sight [<xref rid="pone.0255030.ref001" ref-type="bibr">1</xref>] visualization tool is developed in Oak Ridge National Laboratory (ORNL) and is deployed on Oak Ridge Leadership Computing Facility (OLCF) systems (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.olcf.ornl.gov/olcf-resources/rd-project/sight/" ext-link-type="uri">https://www.olcf.ornl.gov/olcf-resources/rd-project/sight/</ext-link>). It is used for needs of OLCF projects users and is intended to visualize large systems consisting of particles. It is built on client/server architecture, therefore the render server is on HPC cluster and only Web client is on the user’s side. The render server supports rendering both on CPUs with OSPRay [<xref rid="pone.0255030.ref004" ref-type="bibr">4</xref>] and on GPUs with NVIDIA OptiX [<xref rid="pone.0255030.ref005" ref-type="bibr">5</xref>] backends.</p>
    <p>One more tool for visualization of the scientific and engineering data is Tecplot 360 (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.tecplot.com/products/tecplot-360/" ext-link-type="uri">https://www.tecplot.com/products/tecplot-360/</ext-link>), which is mostly used for the visualization of computational fluid dynamics (CFD) simulation results. This is commercial software developed by Tecplot company. The support for big data manipulation is provided by the SZL technology helping to reduce file sizes, processing times, and required operating memory. It is claimed that the visualization using of SZL is about 7.3 times faster and the peak memory usage is 93% less than with the traditional usage of the data files in the PLT format.</p>
    <p>There are domain-specific visualization systems; one of them is Voxler (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.goldensoftware.com/products/voxler" ext-link-type="uri">https://www.goldensoftware.com/products/voxler</ext-link>). It is used mostly by geophysicists, geologists, GIS professionals and other environment science researchers and engineers. Voxler provides specialized tools to visualize different 3D models, such as boreholes, LiDAR data, point clouds, etc. Its usage example is demonstrated in [<xref rid="pone.0255030.ref006" ref-type="bibr">6</xref>]. Another promising open-source visualization system is named FAST [<xref rid="pone.0255030.ref007" ref-type="bibr">7</xref>] and intended for heterogeneous medical image computing and visualization.</p>
    <p>Among middleware related to the rendering of scientific datasets we would like to mention OSPRay [<xref rid="pone.0255030.ref004" ref-type="bibr">4</xref>] (an open-source library, based on a number of libraries by Intel, for parallel ray-tracing complex scenes on CPUs) and NVIDIA OptiX ray-tracing engine running on GPU [<xref rid="pone.0255030.ref005" ref-type="bibr">5</xref>].</p>
    <p>A thorough review of big data visualization and analysis tools is presented in [<xref rid="pone.0255030.ref008" ref-type="bibr">8</xref>].</p>
    <p>It is no surprise that attempts to directly process large amounts data resulting from a complex simulation fail to provide sufficient visualization performance of at least several frames per second (fps), even with the use of supercomputers. For example, the data containing several scalar fields for a single time layer may reach several gigabytes in size, so its fast processing would require hundreds or thousands of CPU cores, plus a very efficient data access solution, like Apache Hadoop/Spark. On the other hand, the image obtained as the output of visualization pipeline is just several megabytes in size. Therefore, we naturally face the question: do we really need so much (about 1 megabyte per pixel for a mesh with 10<sup>8</sup> nodes) data from simulation results for visualization? In this paper, we are going to show that it is in fact not needed. Further we describe the core technology in the basis of our approach to making interactivity possible even for visualizing large datasets, and the ReVisE system built on top of it.</p>
    <p>The paper [<xref rid="pone.0255030.ref009" ref-type="bibr">9</xref>] presents a review of visualization techniques employing volume rendering algorithms, and suggests their categorization, aiming to analyze the problem of scalability with respect to the amount of input data and available computational resources for the visualization. An important idea presented in the paper is that the amount of data processing should depend on the visualization output; the terms <italic toggle="yes">output-sensitive</italic>, <italic toggle="yes">ray-guided</italic>, and <italic toggle="yes">display-aware</italic> are introduced to characterize visualization techniques that realize the idea to some extent. Further, the paper classifies data representation in terms of supporting concepts of bricking, octrees, multi-resolution hierarchies, and efficient storage layout and compression. Visualization strategies providing mechanisms to achieve high scalability are discussed. Those mechanisms include object-space and image-space decomposition, preprocessing, multi-resolution rendering, on-demand processing, and even ray-guided rendering, when the requests to load data come during volume ray casting. Those techniques help to fully utilize computational resources on the one hand, and provide a way to minimize the size of <italic toggle="yes">active working set</italic> on the other hand.</p>
    <p>According to the categorization in [<xref rid="pone.0255030.ref009" ref-type="bibr">9</xref>], ReVisE is an output-sensitive visualization system, employing multi-resolution rendering based on octree data representation with additional bricking that results in a multi-resolution hierarchy of limited-depth octrees. The storage layout is optimized for consecutive reading in order to improve the performance. A number of formats are supported for source datasets, but those are always transformed into ReVisE format at the preprocessing stage.</p>
    <p>The structure of next sections is as follows. Section 2.1 introduces the <italic toggle="yes">sparse 3D mipmaping</italic> technology lying in the core of ReVisE and responsible for data representation, including the data preprocessing stage; it also outlines octree visualization algorithms. Sections 2.2–2.4 present general architecture of the system, including a rendering server, a video streaming service, a web server, and a front-end running in the client browser. In section 3 the performance of the system is tested on a number of different-sized datasets and on different hardware. The results of performance tests and the quality of rendered images are further analyzed and discussed. Section 4.1 presents a comparison of interactive operation, animation, and rendering quality between ReVisE and ParaView. Section 4.2 outlines future work to be done, and section 5 summarizes results obtained.</p>
  </sec>
  <sec sec-type="materials|methods" id="sec002">
    <title>2 Materials and methods</title>
    <sec id="sec003">
      <title>2.1 Sparse 3D mipmapping</title>
      <p>The sparse 3D mipmapping technology has been proposed in [<xref rid="pone.0255030.ref010" ref-type="bibr">10</xref>] and implemented as part of the ReVisE system. This section partly repeats materials from [<xref rid="pone.0255030.ref010" ref-type="bibr">10</xref>] for the sake of easier understanding, additional details can be found therein.</p>
      <p>The basic idea of Sparse 3D mipmapping is to remap all scalar fields defined on the original mesh onto a grid consisting of vertices of all cubes belonging to a sparse octree [<xref rid="pone.0255030.ref011" ref-type="bibr">11</xref>]. There are a number of benefits from this operation. In particular, sparse octree data can easily be transformed into a set of dense 3D textures and used for visualization employing volume rendering algorithms; those algorithms run quite fast on modern GPUs, partly because no geometry primitives need to be generated. In addition, octree data can easily be organized as a set of levels with fixed spatial resolution; due to that, progressive rendering is easily implemented, making visualization application quite responsive to user actions, while still delivering high quality images in additional time. Finally, the size of the octree dataset can be controlled by limiting the maximum spatial resolution.</p>
      <sec id="sec004">
        <title>2.1.1 Using octree for scalar field representation</title>
        <p>Octree data structure can be used to represent a spatial scalar field in the following way. Suppose there is an original mesh—unstructured or structured, consisting of tetrahedra or hexahedra—it does not matter, and a scalar field specified at mesh nodes. Mesh elements provide a way to interpolate field values at arbitrary points of the domain. Importantly, unstructured meshes are often non-homogeneous, in the sense that mesh element size varies significantly across the domain. For a given mesh, an octree can be generated, such that the leaves of the octree have sizes close (in some sense, which is clarified in Subsection 2.1.2) to the sizes of elements they intersect with. We do not use the octree to store any data in its nodes. Instead, we use <italic toggle="yes">grids induced by the octree</italic> (or its part) to represent scalar fields. A given octree induces the grid whose nodes are vertices of all octree cubes, and whose elements are octree cubes. Notice that the induced grid is often sparse because the original mesh is non-homogeneous and because the domain differs from the cube constituting the octree root.</p>
        <p>A scalar field can be interpolated in each node of the induced grid that belongs to the domain, resulting in a real number. Nodes of the induced grid that lie outside the domain are marked by storing NaN (not-a-number) value. As a result, the field is represented as an array of real numbers (some having NaN value). To associate each element of the array with a particular node of the induced grid, an ordering rule has to be established for induced grid nodes. A simple ordering rule can be obtained by recursive depth-first traversal of the octree, assigning successive numbers to the unnumbered vertices of the current cube; other ordering rules could be considered, e.g., those that can be run in parallel. Further we refer to fields defined at nodes of the induced grid as <italic toggle="yes">sparse fields</italic>.</p>
        <p>Once a sparse field is computed, it can be interpolated at a subset of domain points: the interpolation is possible when the smallest cube containing the point does not contain induced mesh nodes with NaN field value. Our implementation of the interpolation algorithm computes field values on the regular <italic toggle="yes">dense grid</italic> (obtained from sparse grid by inserting nodes) and interprets the resulting array of values as a 3D texture at the visualization stage.</p>
        <p><xref rid="pone.0255030.g001" ref-type="fig">Fig 1</xref> illustrates the representation of a field using quadtree in the 2D case, which is similar to the 3D case.</p>
        <fig position="float" id="pone.0255030.g001">
          <object-id pub-id-type="doi">10.1371/journal.pone.0255030.g001</object-id>
          <label>Fig 1</label>
          <caption>
            <title>Using quad tree for scalar field representation.</title>
            <p>A quadtree is generated according to element size distribution in the original mesh. Scalar fields are interpolated at nodes of the sparse grid induced by the octree, then field can be interpolated at subdomain points excluding the marginal area. The size of the marginal area can be reduced by refining the octree near the boundary. A: Original mesh. B: Generated quadtree. C: Field interpolated at nodes of the induced grid. D: Field interpolated at subdomain points. E: Quadtree refined at boundary. F: Field interpolated at subdomain points in the case of refined boundary.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.g001" position="float"/>
        </fig>
        <p>It is worth noticing that when the elements of the original mesh are large enough, the octree nodes are equally large too, which may lead to noticeable “marginal area” near the domain boundary, where field cannot be interpolated (<xref rid="pone.0255030.g001" ref-type="fig">Fig 1D</xref>). To overcome this problem, our octree generation algorithm has a part employing the “virtual refinement” of domain boundary in order to reduce the marginal area. A parameter controls the relative density of refined boundary mesh. As a result, the leaves of the octree become small enough near the domain boundary, which ensures sufficient visualization quality. <xref rid="pone.0255030.g001" ref-type="fig">Fig 1E</xref> shows an example of quad tree refined near the boundary, and <xref rid="pone.0255030.g001" ref-type="fig">Fig 1F</xref>—the resulting scalar field.</p>
      </sec>
      <sec id="sec005">
        <title>2.1.2 Controlling the size and quality of visualization dataset</title>
        <p>As mentioned above, the leaves of generated octree are “close” in size to elements located nearby. It is important to define that closeness in a proper way, because different definitions may lead to very different octree sizes. Importantly, meshes used, e.g., in computational fluid dynamics are often anisotropic, which means they contain very thin elements needed to model boundary layers: orthogonal edges of a hexahedral mesh element may differ in length 100 times and more, as shown in <xref rid="pone.0255030.g002" ref-type="fig">Fig 2B</xref>.</p>
        <fig position="float" id="pone.0255030.g002">
          <object-id pub-id-type="doi">10.1371/journal.pone.0255030.g002</object-id>
          <label>Fig 2</label>
          <caption>
            <title>Quadtree/Octree generation on anisotropic meshes.</title>
            <p>Anisotropic meshes contain thin elements having very different edge sizes. Element size estimation is important for controlling the size of visualization dataset. A: 2D mesh and quadtrees generated for it with different <italic toggle="yes">α</italic>. B: 3D mesh and octrees generated for it with different <italic toggle="yes">α</italic> (smaller octree nodes “shine through” bigger ones). C: Dependency of the octree size and the sparse field size on <italic toggle="yes">α</italic>.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.g002" position="float"/>
        </fig>
        <p>The algorithm of octree generation iterates through mesh elements; for each element, it computes the axis-aligned bounding box and estimates element size. Then it picks closest possible size for octree node and, if necessary, inserts octree nodes to ensure that each point of the element bounding box belongs to an octree node of the chosen size. The element size <italic toggle="yes">s</italic> is bounded between two values, <italic toggle="yes">s</italic><sub>min</sub> and <italic toggle="yes">s</italic><sub>max</sub>. On the one hand, the smallest element edge length, <italic toggle="yes">s</italic><sub>min</sub>, should not be considered element size: for thin elements at the boundary, this would lead to a huge number of octree nodes, because other element edges are much longer. On the other hand, taking the largest element edge length, <italic toggle="yes">s</italic><sub>max</sub>, as element size, might lead to a too coarse octree. Our approach is to take the value <italic toggle="yes">s</italic> = <italic toggle="yes">s</italic><sub>min</sub> + <italic toggle="yes">α</italic>(<italic toggle="yes">s</italic><sub>max</sub> − <italic toggle="yes">s</italic><sub>min</sub>) as the element size, where <italic toggle="yes">α</italic> ∈ [0, 1] is a constant parameter. Positive values of <italic toggle="yes">α</italic> help to prevent the “explosion” of octree size due to thin mesh elements.</p>
        <p><xref rid="pone.0255030.g002" ref-type="fig">Fig 2</xref> shows the results of quadtree and octree generator test runs on simple 2D and 3D meshes respectively, containing thin elements at one face. Practically, the parameter <italic toggle="yes">α</italic> is chosen between 0.1 and 1 to trade-off visualization quality against the size of sparse field, which is typically comparable or less than the size of the field on the original mesh.</p>
        <p>Another idea is to limit the total depth of the octree to a fixed level <italic toggle="yes">d</italic><sub>0</sub>, thus limiting maximum possible octree size. Practically reasonable values of <italic toggle="yes">d</italic><sub>0</sub> are between 10 and 13 and correspond to the maximum resolution between 1024 and 8192 smallest octree nodes in each dimension, which is enough in many practical cases.</p>
      </sec>
      <sec id="sec006">
        <title>2.1.3 Visualization metadata</title>
        <p>As already mentioned, at the visualization stage sparse fields are turned into dense fields represented by 3D textures, which are further rendered on GPUs. But the memory of a single GPU is limited and can only store 3D texture corresponding to an octree of depth no more than 9 or 10, depending on hardware. In the same time, the original mesh can have local refinement that results in sparse octrees of bigger depth. Besides, it is promising to parallelize the rendering across many GPUs. Therefore, a sparse field has to be separated into several smaller sparse fields that can be processed in parallel.</p>
        <p>To achieve this goal, we introduce <italic toggle="yes">metadata</italic> of the visualization dataset. The metadata is constituted by the octree, the block depth parameter <italic toggle="yes">d</italic>, and the <italic toggle="yes">level structure</italic>. Let us define the term <italic toggle="yes">block</italic> as a subset of the octree nodes, consisting of a node (called block root) and all its children up to depth <italic toggle="yes">d</italic> from block root. Thus, a block itself is an octree of depth at most <italic toggle="yes">d</italic>. Metadata level structure consists of blocks. Level <italic toggle="yes">l</italic> consists of blocks whose roots are at distance <italic toggle="yes">l</italic> from the octree root. Therefore, level 0 contains one block whose root is the octree root. Next levels have more blocks. A new level provides a more detailed representation of a scalar field if it has at least one block of depth <italic toggle="yes">d</italic>. Therefore, levels 0, 1, …, <italic toggle="yes">D</italic> − <italic toggle="yes">d</italic> are necessary to represent an octree of depth <italic toggle="yes">D</italic>. <xref rid="pone.0255030.g003" ref-type="fig">Fig 3</xref> shows an example of metadata for the 2D case.</p>
        <fig position="float" id="pone.0255030.g003">
          <object-id pub-id-type="doi">10.1371/journal.pone.0255030.g003</object-id>
          <label>Fig 3</label>
          <caption>
            <title>An example of quadtree and corresponding metadata.</title>
            <p>The quadtree has depth 6, and the metadata contains three levels; blocks of each level have depth at most 4. Level blocks are marked with blue squares. A: Quadtree. B: Metadata level 0 (single block). C: Metadata level 1 (2×2 blocks). D: Metadata level 2 (4×4 blocks).</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.g003" position="float"/>
        </fig>
        <p>In our current implementation, each level has to contain all 2<sup>3<italic toggle="yes">l</italic></sup> blocks, and the number of levels is limited to some <italic toggle="yes">L</italic> (e.g., <italic toggle="yes">d</italic> = 8, <italic toggle="yes">L</italic> = 3, which allows to represent an octree of depth at most 11); to make it possible, we force the octree to have all children up to depth <italic toggle="yes">L</italic>, which has a relatively small storage overhead.</p>
        <p>Sparse fields are computed per-block, and for each scalar field a single file stores sparse fields for all blocks. In addition, metadata stores the location of sparse field for each block in the file.</p>
        <p>This metadata structure allows to easily organize the rendering of a specific level and have the corresponding level of detail in the resulting image. Importantly, sparse field data can be read from a storage system in one operation for the entire level, eliminating the need for seek operations or for reading unnecessary data.</p>
        <p>Practically, an application may choose between rendering a fixed level or doing the progressive rendering to provide a low-detailed image as fast as possible and provide more detailed images as they become available. Importantly, each block can be rendered independently on an SMP machine with multiple GPUs, or on in a distributed environment, such as a cluster.</p>
      </sec>
      <sec id="sec007">
        <title>2.1.4 Octree storage optimization</title>
        <p>Although an octree is a popular data structure, and a number of methods [<xref rid="pone.0255030.ref012" ref-type="bibr">12</xref>–<xref rid="pone.0255030.ref016" ref-type="bibr">16</xref>] have been proposed to optimize its storage, we provide our own solution. Existing methods seem to be quite redundant or inappropriate for our case due to two reasons. Firstly, most often, an octree node is considered to either have all 8 child nodes or be a leaf, i.e., have no children at all. In the ReVisE system, we use a different flavor of this data structure, allowing each node to have any subset of child nodes. Secondly, we do not need to store data at octree nodes, as others do. Instead, as explained above, data items are stored at nodes of induced grids that need to be extracted from the octree.</p>
        <p>According to previous sections, there are two basic usage patterns of the octree data structure. At the octree generation stage, its nodes need to be located by position in space, and new nodes need to be added. At the field interpolation and visualization stages, nodes need to be located by position, and induced grids need to be extracted from limited-depth subtrees. Importantly, the octree is immutable after it is generated; this gives a way to drastically reduce disk and operative memory usage for storing and operating octree.</p>
        <p>At the octree generation stage, it is represented by root node geometry parameters (edge length <italic toggle="yes">L</italic> and center position <bold>R</bold><sub><italic toggle="yes">c</italic></sub>) and a single array containing octets of integer indices. Each octree node has an ordinal number, and the ordinal of the root node is 0. Each octet <italic toggle="yes">C</italic><sub><italic toggle="yes">i</italic></sub> corresponds to <italic toggle="yes">i</italic>-th octree node and contains ordinal numbers of child nodes or zeros indicating the absence of a child.</p>
        <p>For large-scale meshes, the size of the corresponding octree in memory might be quite large. It usually fits into memory for a single subdomain, but the processing of the entire mesh might lead to unacceptably large size.</p>
        <p>Fortunately, the octree data structure can be stored much more efficiently if there is no need to add new nodes. The basic idea is to change ordinal numbers of octree nodes in such a way that they appear in the octet in the ascending order. Therefore, the required storage capacity reduces from 32 or 64 bits necessary to store an integer index to just one bit. During the consecutive reading of the octree, actual indices of child nodes can easily be restored. In addition, all trailing zeros can be omitted, which further reduces the storage size up to 8 times (that is the limit case for full octree). The octree stored in this way is further referred to as <italic toggle="yes">the compressed octree</italic>. Importantly, the compressed octree with search and recursive walk enabled typically fits into 1–20 Gb of random access memory (RAM) even for meshes with 10<sup>10</sup> nodes.</p>
        <p>This algorithm of octrees compression is described in more details in [<xref rid="pone.0255030.ref010" ref-type="bibr">10</xref>].</p>
      </sec>
      <sec id="sec008">
        <title>2.1.5 Preprocessing simulation results</title>
        <p>Within our approach, the data has to be preprocessed before visualization. This needs to be done only once per simulation, and the preprocessing is fully automatic. We did not focus on preprocessing performance, although its duration is reasonable. At the preprocessing stage the original dataset is transformed into the <italic toggle="yes">visualization dataset</italic>.</p>
        <p>The original dataset is assumed to be distributed across nodes of a computer system, further referred to as <italic toggle="yes">data host</italic>. Each node of the data host stores data for a specific subdomain, constituted by corresponding part of the mesh (there are a number of domain decomposition techniques, e.g. those implemented in [<xref rid="pone.0255030.ref017" ref-type="bibr">17</xref>]). The preprocessing takes place on the nodes of the data host; subdomains are processed separately, and further the subdomain data is merged into a single visualization dataset. The preprocessing consists of the following five stages: (1) the calculation of the bounding box containing the entire mesh; (2) octree generation at each subdomain; (3) the interpolation of scalar fields at each subdomain; (4) merging all octrees into one; and (5) merging scalar fields in such a way that they are defined at sparse grids induced by the global octree.</p>
        <p>In order for subdomain parts of visualization dataset to be mergeable into a single dataset, the octree for each subdomain has to be representable as a subset of the global octree. Therefore, the preprocessing starts with computing the bounding box for the entire domain in order to determine the position and size of the cube at the root of the global octree. At stages 2 and 3, this information is necessary for the generation of “compatible” subdomain octrees. Stage 4 merges subdomain octrees pairwise. An algorithm for stage (4) has been proposed in [<xref rid="pone.0255030.ref010" ref-type="bibr">10</xref>] that is applicable to octrees in the compressed format (see previous section) without having to perform node insertion operations, because that would require much more memory. Modest memory requirement gives the proposed algorithm an advantage over other algorithms, such as [<xref rid="pone.0255030.ref018" ref-type="bibr">18</xref>]. For stage (5), a straightforward algorithm can be implemented that merges subdomain sparse fields pairwise. At the moment of paper writing, stages 4 and 5 are not implemented in software and are parts of work in progress.</p>
      </sec>
      <sec id="sec009">
        <title>2.1.6 Visualizing octree data</title>
        <p>The visualization in the ReVisE system is based on volume rendering algorithms that require 3D textures to read field data. Before a rendering algorithm can be run, a sparse field has to be transformed into the corresponding dense field and stored in a 3D texture in GPU memory. Our implementation of this operation employs CUDA to generate the 3D texture with dense field on the GPU. For interactive visualization, the time spent on each operation is important. Since the rendering is done on GPU, one has to have 3D texture in GPU memory. On the other hand, data transfer between a GPU and the host system costs time and has to be minimized. Taking this into account, it sounds reasonable to upload a sparse field to the GPU and then generate the dense field on the GPU. Also it should be noticed that sparse fields are often 10–100 times less in size than corresponding dense fields.</p>
        <p>The algorithm of dense field generation processes the octree block of depth <italic toggle="yes">d</italic> level by level, starting from root and ending at level <italic toggle="yes">d</italic> − 1. To process each level, a CUDA kernel is invoked once. When level <italic toggle="yes">l</italic> is processed, the algorithm assumes that the dense field is correct on the grid induced by the full octree of depth <italic toggle="yes">l</italic> (this is always true for level 0). The objective of kernel running on GPU is to ensure the dense field is correct on the grid induced by the full octree of depth <italic toggle="yes">l</italic> + 1. To imagine the algorithm doing that, consider leaves of a full depth-<italic toggle="yes">l</italic> octree. The geometry of each leaf is a cube. The algorithm interpolates any missing field values in the middles of all cube edges, then in the middles of all cube faces, then in the middles of all cubes. Since the dense field is known at level <italic toggle="yes">l</italic>, it is known at all cube vertices, and the above mentioned interpolation is done easily. An important thing to keep in mind is a special handling of sparse field NaN values indicating that the corresponding points are outside the domain. When such a value is involved in the interpolation, the result of the interpolation has to be NaN. This simple propagation rule for NaN values ensures the correctness of dense field in the domain, excluding the marginal area (see <xref rid="pone.0255030.g001" ref-type="fig">Fig 1D and 1F</xref>).</p>
        <p>Once the dense field is computed, it is interpreted as a 3D texture and used by rendering algorithms of volume raycast type [<xref rid="pone.0255030.ref019" ref-type="bibr">19</xref>], implemented in CUDA. Our implementation of the rendering algorithms is based on source code of OpenGL fragment shaders found in the Visualization Library [<xref rid="pone.0255030.ref020" ref-type="bibr">20</xref>], and extends the functionality of the original shaders. Some examples of algorithms currently implemented are shown in <xref rid="pone.0255030.g004" ref-type="fig">Fig 4</xref>. A rendered image can be attributed to certain metadata level; it is obtained by rendering all level blocks. On multiple GPU systems—including clusters equipped with GPUs—the set of level blocks is split into subsets, each of which is rendered by one GPU. Image parts rendered by each GPU are then blended back-to-front into a final image. To make this approach work, the separation of level block set into subsets has to ensure that subsets can be ordered for correct back-to-front blending. Besides, similar blending takes place within each subset; for this to work, blocks of each subset have to be ordered for back-to-front blending. For a fixed level number and a particular number of GPUs and CPUs available, the rendering algorithm can be represented as a graph of tasks; each task can be executed as soon as all its inputs are available and the corresponding computational resource (CPU or GPU) is waiting for a new task. An example of rendering task graph is shown in <xref rid="pone.0255030.g005" ref-type="fig">Fig 5</xref>.</p>
        <fig position="float" id="pone.0255030.g004">
          <object-id pub-id-type="doi">10.1371/journal.pone.0255030.g004</object-id>
          <label>Fig 4</label>
          <caption>
            <title>Volume rendering algorithms applied to test datasets.</title>
            <p>A: Opaque isosurfaces. B: Field values on isosurfaces of other field; transparent isosurfaces. C: Colormap with transparency. D: Colormap with transparency and per-sample lighting. E: Maximum intensity projection.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.g004" position="float"/>
        </fig>
        <fig position="float" id="pone.0255030.g005">
          <object-id pub-id-type="doi">10.1371/journal.pone.0255030.g005</object-id>
          <label>Fig 5</label>
          <caption>
            <title>An example of rendering task graph.</title>
            <p>The graph consists of 4 parallel rendering tasks, two levels of image parts blending and final composing, where the background color is added.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.g005" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec id="sec010">
      <title>2.2 General architecture of visualization system</title>
      <p>Sparse 3D mipmapping technology lies in the basis of the ReVisE visualization system. The system is a distributed application that performs visualization on a remote system and provides web interface for clients to control and observe the visualization. Visualization system runs code of web server written in node.js; visualization code is written in C++ and is controlled from a native node.js add-on based on the node-addon-api technology. Visualization code is parallelized for use on SMP machines with single or multiple NVIDIA’s GPUs; further plans include developing a parallel version for clusters. Rendering functionality, including parallel execution of the rendering task graph, is available as a separate library.</p>
      <p>Apart from the web server that handles the interaction with client over HTTP, the system contains a video streaming service. The service runs in a separate thread of the web server process and is responsible for serving the sequence of images rendered; the images are transported using the web sockets protocol. Similarly to visualization code, the video streaming service is controlled by web server through an interface exposed basing on node-addon-api.</p>
      <p>Client code runs in a web browser on the client side. It provides user interface elements for choosing among available problems, setting up visualization parameters, moving camera, navigating the timeline (including animation), and sharing video stream. No rendering takes place on the client side, as the client obtains images rendered by the server.</p>
      <p>The architecture of the system is depicted in <xref rid="pone.0255030.g006" ref-type="fig">Fig 6</xref>.</p>
      <fig position="float" id="pone.0255030.g006">
        <object-id pub-id-type="doi">10.1371/journal.pone.0255030.g006</object-id>
        <label>Fig 6</label>
        <caption>
          <title>General architecture of the ReVisE system.</title>
          <p>Automatic preprocessing takes place on the data host; the resulting visualization dataset is copied to the visualization server. Interactive visualization application serves video stream of rendered images and listens to user input.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.g006" position="float"/>
      </fig>
    </sec>
    <sec id="sec011">
      <title>2.3 Video streaming service</title>
      <p>Once the visualizer renders a new final image, it uploads the image to shared memory; the image is accompanied by a header containing frame ordinal number and some additional information, such as rendering duration and metadata level number. Image data is stored in shared memory without any compression.</p>
      <p>Video streaming service checks shared memory contents periodically. When it finds a new frame number in the header, the frame is sent to subscribed clients over connected web sockets. Importantly, the streaming service needs to deliver new frames to the client as fast as possible, otherwise the visualization cannot be interactive. There are a number of approaches to implement interactive video streaming; some of them are discussed in [<xref rid="pone.0255030.ref021" ref-type="bibr">21</xref>] (unfortunately, details of communication protocols used over web sockets are not described). In [<xref rid="pone.0255030.ref022" ref-type="bibr">22</xref>], scaling video streaming across a number of clients is considered. In general, it appears to be tricky to deliver a low-latency video stream to the user in the case of a network with limited bandwidth and a noticeable latency.</p>
      <p>Because visualizers are created by the web server per-client, there can be several shared memory areas for use as video sources. Once a new visualizer is created for a new client, or a visualizer is removed when a client is gone, the web server informs the streaming service about the change of set of available sources. When a client connects to video streaming service, it passes source identifier (previously obtained from web server), in order for service to know which source to use for the client.</p>
      <p>Once connected, the client and the service follow a simple protocol, outlined below. Important features of the protocol are the abilities to trade-off image compression quality against image size and to overcome network latency to some extent.</p>
      <p>The protocol specifies a number of messages; some of them are initiated by the client, and some—by the service; each message expects a reply message. <xref rid="pone.0255030.t001" ref-type="table">Table 1</xref> lists and briefly explains the messages.</p>
      <table-wrap position="float" id="pone.0255030.t001">
        <object-id pub-id-type="doi">10.1371/journal.pone.0255030.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>Messages specified in the video streaming service protocol.</title>
        </caption>
        <alternatives>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.t001" id="pone.0255030.t001g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">initiator</th>
                <th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">message</th>
                <th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">reply message</th>
                <th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">remarks</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">client</td>
                <td align="left" rowspan="1" colspan="1"><monospace>s:</monospace> 〈<italic toggle="yes">sourceId</italic>〉</td>
                <td align="left" rowspan="1" colspan="1"><monospace>Ok</monospace> / <monospace>Unknown source</monospace></td>
                <td align="left" rowspan="1" colspan="1">Instructs service to select the specified source</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">client</td>
                <td align="left" rowspan="1" colspan="1">
                  <monospace>F</monospace>
                </td>
                <td align="left" rowspan="1" colspan="1"><monospace>F:</monospace> 〈<italic toggle="yes">hdr</italic>〉〈<italic toggle="yes">img</italic>〉</td>
                <td align="left" rowspan="1" colspan="1">Request full quality image</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">client</td>
                <td align="left" rowspan="1" colspan="1">
                  <monospace>B</monospace>
                </td>
                <td align="left" rowspan="1" colspan="1">
                  <monospace>Ok</monospace>
                </td>
                <td align="left" rowspan="1" colspan="1">Prefer binary image data</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">client</td>
                <td align="left" rowspan="1" colspan="1">
                  <monospace>T</monospace>
                </td>
                <td align="left" rowspan="1" colspan="1">
                  <monospace>Ok</monospace>
                </td>
                <td align="left" rowspan="1" colspan="1">Prefer base64-encoded image data</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">service</td>
                <td align="left" rowspan="1" colspan="1"><monospace>n:</monospace> 〈hdr〉〈img〉</td>
                <td align="left" rowspan="1" colspan="1"><monospace>n:</monospace> 〈<italic toggle="yes">N</italic><sub><italic toggle="yes">v</italic></sub>〉〈<italic toggle="yes">N</italic><sub><italic toggle="yes">s</italic></sub>〉</td>
                <td align="left" rowspan="1" colspan="1">Service sends new image to client initiatively</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="t001fn001">
            <p><italic toggle="yes">sourceId</italic> is a string identifying video stream source. <italic toggle="yes">hdr</italic> is the image header consisting of five numbers: <italic toggle="yes">N</italic><sub><italic toggle="yes">v</italic></sub> (ordinal number of image rendered by visualizer), <italic toggle="yes">N</italic><sub><italic toggle="yes">s</italic></sub> (ordinal number of image sent by the service to the client), <italic toggle="yes">l</italic> (metadata level corresponding to the image rendered), <italic toggle="yes">T</italic> (rendering task graph execution time), and <italic toggle="yes">Q</italic> (image quality). <italic toggle="yes">img</italic> is image data, in JPEG format (base64-encoded in case of text data).</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Once the client specifies the source by sending the <monospace>s</monospace> message, the video stream service starts sending new images using the <monospace>n</monospace> message. A new image is sent when all of the following conditions hold: (1) time passed since last image was sent is not less than 40 ms; (2) the number of pending images (those sent in <monospace>n</monospace> messages for which replies have not been received yet) does not exceed some constant value (typically 3–10); (3) ordinal image number in source shared memory, <italic toggle="yes">N</italic><sub><italic toggle="yes">v</italic></sub>, is greater than the corresponding number in the header of most recent image sent to the client.</p>
      <p>In addition, the streaming service keeps track of “overflow” events: when the conditions (1) and (2) both hold, it records overflow event status 0. When the condition (1) holds and (2) does not, it records overflow event status 1. A buffer of most recent 20 overflow status values is used to adjust the quality <italic toggle="yes">Q</italic> ∈ [0, 100] of JPEG images sent to the client: once the average status value exceeds a constant threshold (currently 0.5), current image quality is decreased by some step (currently 1), unless it has reached minimum value <italic toggle="yes">Q</italic><sub>0</sub> (currently 10); once the average status value is below another constant threshold (currently 0.4), current image quality is increased by the same step, unless it has reached maximum value 100. This simple algorithm allows to keep reasonable frame rates, on the one hand, and keep reasonably small delays between user actions and visualized images corresponding to those actions, even for low-bandwidth network connections and networks with latency 100–300 ms.</p>
      <p>Client code receives and renders images sent with the <monospace>n</monospace> message in a straightforward manner; it should send reply message immediately after receiving an image, before rendering it. In addition, sometimes client requests a full-quality image using the <monospace>F</monospace> message. This happens, for example, when the last received image has quality lower than 100, and the time passed since receiving that image exceeds some threshold, e.g. 1 second.</p>
    </sec>
    <sec id="sec012">
      <title>2.4 Web application for remote visualization</title>
      <p>The web application is a web server that runs on the visualization server and provides functionality to remotely control the visualization of datasets from a web browser. The server runs a separate copy of visualizer for each client session and provides web API to operate it. HTML pages sent to the user contain client code that interacts with the visualizer in response to user actions, such as the choice of an item from a list, setting a numeric parameter, mouse clicks, and mouse movements. This allows to track all input data for visualization controller, including camera position. As a result, user can fully control the visualization from the web browser. <xref rid="pone.0255030.g007" ref-type="fig">Fig 7</xref> shows a screenshot of HTML page with user interface and a visualized scene.</p>
      <fig position="float" id="pone.0255030.g007">
        <object-id pub-id-type="doi">10.1371/journal.pone.0255030.g007</object-id>
        <label>Fig 7</label>
        <caption>
          <title>ReVisE web interface.</title>
          <p>Area at the top contains drop-down lists to choose a problem and a primary field, timeline navigation elements, camera positioning buttons, video stream sharing and settings buttons. Left panel contains elements to pick visualization algorithm and control its parameters. Central area displays rendered scene. Mouse can be used to pan, zoom, and rotate the scene interactively.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.g007" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec id="sec013">
    <title>3 Results and evaluation</title>
    <p>The ReVisE system has been tested in a number of problems of different scale, including test datasets and real-life problems. First of all, we focused on the visualization performance, aiming to provide as much interactivity as possible; preprocessing times have also been measured. Problem cases presented below are as follows.
<list list-type="order"><list-item><p>Multiscale waves, further referred to as <monospace>mwaves</monospace>—a fully synthetic dataset providing an animated field computed by the formula
<disp-formula id="pone.0255030.e001"><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.e001.jpg" id="pone.0255030.e001g" position="anchor"/><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M1" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>sin</mml:mtext><mml:mfrac><mml:mrow><mml:mi>π</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>sin</mml:mtext><mml:mfrac><mml:mrow><mml:mi>π</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>sin</mml:mtext><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>z</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mi>π</mml:mi><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <inline-formula id="pone.0255030.e002"><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.e002.jpg" id="pone.0255030.e002g" position="anchor"/><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M2" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>≡</mml:mo><mml:mtext>cos</mml:mtext><mml:mfrac><mml:mrow><mml:mi>π</mml:mi><mml:mi>z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mtext>sin</mml:mtext><mml:mfrac><mml:mrow><mml:mi>π</mml:mi><mml:mi>z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mi>y</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, and <inline-formula id="pone.0255030.e003"><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.e003.jpg" id="pone.0255030.e003g" position="anchor"/><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M3" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>≡</mml:mo><mml:mo>−</mml:mo><mml:mtext>sin</mml:mtext><mml:mfrac><mml:mrow><mml:mi>π</mml:mi><mml:mi>z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mtext>cos</mml:mtext><mml:mfrac><mml:mrow><mml:mi>π</mml:mi><mml:mi>z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mi>y</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>; <italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, <italic toggle="yes">z</italic> are Cartesian coordinates, and <italic toggle="yes">t</italic> is the time. Spatial domain is the cube <italic toggle="yes">x</italic> ∈ [−1, 1], <italic toggle="yes">y</italic> ∈ [−1, 1], <italic toggle="yes">z</italic> ∈ [0.05, 2.05]. The field has features with size changing gradually between two opposite faces of the domain (bottom row in <xref rid="pone.0255030.g004" ref-type="fig">Fig 4</xref>). The dataset is computed on the regular grid, which results in a full octree—the worst case for visualization performance.</p></list-item><list-item><p>Simulation of flow around 2d NASA wall-mounted hump [<xref rid="pone.0255030.ref023" ref-type="bibr">23</xref>], further referred to as <monospace>hump</monospace>. The mesh contains 5.26 ⋅ 10<sup>6</sup> nodes; numerical solution contains 100 time steps.</p></list-item><list-item><p>Simulation of flow over high-lifted turbine cascade at low Reynolds numbers [<xref rid="pone.0255030.ref024" ref-type="bibr">24</xref>] using NOISEtte code [<xref rid="pone.0255030.ref025" ref-type="bibr">25</xref>], further referred to as <monospace>cascade</monospace>. The mesh contains 8.98 ⋅ 10<sup>7</sup> nodes; subset of numerical solution for visualization contains 13 time steps.</p></list-item></list></p>
    <p>For those problems, visualization datasets have been obtained and visualization performance has been measured. For performance tests, we used three systems, as shown in <xref rid="pone.0255030.t002" ref-type="table">Table 2</xref>. Machine names <monospace>DGX-1</monospace>, <monospace>Tesla</monospace> and <monospace>GeForce</monospace> are used for reference and should be treated as identifiers. <monospace>DGX-1</monospace> and <monospace>Tesla</monospace> are multi-processor SMP machines and <monospace>GeForce</monospace> is a usual desktop.</p>
    <table-wrap position="float" id="pone.0255030.t002">
      <object-id pub-id-type="doi">10.1371/journal.pone.0255030.t002</object-id>
      <label>Table 2</label>
      <caption>
        <title>Parameters of the computers used for tests of ReVisE.</title>
      </caption>
      <alternatives>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.t002" id="pone.0255030.t002g" position="float"/>
        <table frame="box" rules="all" border="0">
          <colgroup span="1">
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1"/>
              <th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">
                <monospace>DGX-1</monospace>
              </th>
              <th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">
                <monospace>Tesla</monospace>
              </th>
              <th align="left" style="border-bottom-width:thick" rowspan="1" colspan="1">
                <monospace>GeForce</monospace>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" rowspan="1" colspan="1">CPU info</td>
              <td align="left" rowspan="1" colspan="1">Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz</td>
              <td align="left" rowspan="1" colspan="1">Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz</td>
              <td align="left" rowspan="1" colspan="1">Intel i7-8700 CPU @ 3.20GHz</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">CPU count</td>
              <td align="left" rowspan="1" colspan="1">2</td>
              <td align="left" rowspan="1" colspan="1">2</td>
              <td align="left" rowspan="1" colspan="1">1</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">cores per CPU</td>
              <td align="left" rowspan="1" colspan="1">20</td>
              <td align="left" rowspan="1" colspan="1">12</td>
              <td align="left" rowspan="1" colspan="1">6</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">RAM, Gb</td>
              <td align="left" rowspan="1" colspan="1">512</td>
              <td align="left" rowspan="1" colspan="1">128</td>
              <td align="left" rowspan="1" colspan="1">16</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">GPU info</td>
              <td align="left" rowspan="1" colspan="1">V100-SXM2 GPUs (16 GB memory, 5120 CUDA cores per GPU)</td>
              <td align="left" rowspan="1" colspan="1">Tesla V100-PCIE GPUs (32 GB memory, 5120 CUDA cores per GPU)</td>
              <td align="left" rowspan="1" colspan="1">GeForce 1060 GPU (6 GB memory, 1280 CUDA cores)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">GPU count</td>
              <td align="left" rowspan="1" colspan="1">8</td>
              <td align="left" rowspan="1" colspan="1">2</td>
              <td align="left" rowspan="1" colspan="1">1</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">remarks</td>
              <td align="left" rowspan="1" colspan="1"/>
              <td align="left" rowspan="1" colspan="1">Intel Optane 960 storage system</td>
              <td align="left" rowspan="1" colspan="1"/>
            </tr>
          </tbody>
        </table>
      </alternatives>
    </table-wrap>
    <p><monospace>DGX-1</monospace> and <monospace>Tesla</monospace> were used in the remote server mode: they have only visualization server running, as shown in <xref rid="pone.0255030.g006" ref-type="fig">Fig 6</xref>, and interact with remote client via Internet. In the case of <monospace>GeForce</monospace>, the visualization server and web client were running locally.</p>
    <p>The procedure of performance measurements is described in the following protocols: “Build ReVisE on Ubuntu” (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://dx.doi.org/10.17504/protocols.io.bruwm6xe" ext-link-type="uri">https://dx.doi.org/10.17504/protocols.io.bruwm6xe</ext-link>), “Run ReVisE from Docker” (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://dx.doi.org/10.17504/protocols.io.bv5hn836" ext-link-type="uri">https://dx.doi.org/10.17504/protocols.io.bv5hn836</ext-link>), and “Prepare and run test on available dataset” (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://dx.doi.org/10.17504/protocols.io.bruzm6x6" ext-link-type="uri">https://dx.doi.org/10.17504/protocols.io.bruzm6x6</ext-link>).</p>
    <sec id="sec014">
      <title>3.1 Visualization performance</title>
      <p>Here visualization performance tests are described. These tests involve the assessment of rendering speed and scalability. The results of testing are presented in <xref rid="pone.0255030.g008" ref-type="fig">Fig 8</xref>. The chart contains the dependency of rendering duration on the number of GPUs used. The rendering is performed on <monospace>DGX-1</monospace>, for the maximal intensity projection rendering algorithm. The rendering is performed in parallel, and each worker thread is placed on a separate GPU. Therefore, the number of GPUs is equal to the number of worker threads and the case of one GPU corresponds to the sequential rendering.</p>
      <fig position="float" id="pone.0255030.g008">
        <object-id pub-id-type="doi">10.1371/journal.pone.0255030.g008</object-id>
        <label>Fig 8</label>
        <caption>
          <title>Rendering duration on <monospace>DGX-1</monospace>.</title>
          <p>Rendering duration as a function of GPU count used for problem cases described above. Rendering algorithm is maximal intensity projection. Each worker thread uses its dedicated GPU.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.g008" position="float"/>
      </fig>
      <p>The rendering of each dataset was performed separately for levels 0–3. Dataset parameters are presented in <xref rid="pone.0255030.t003" ref-type="table">Table 3</xref>. The blocks of datasets <monospace>hump</monospace> and <monospace>cascade</monospace> have depth 8, while the blocks of <monospace>mwaves</monospace> have depth 7. But the dataset <monospace>mwave</monospace> is dense, therefore, this is the heaviest case with the biggest number of nodes. For instance, the total number of nodes at level 3 is 7.72 ⋅ 10<sup>6</sup> for <monospace>hump</monospace>, 7.96 ⋅ 10<sup>7</sup> for <monospace>cascade</monospace> and 1.1 ⋅ 10<sup>9</sup> for <monospace>mwaves</monospace>.</p>
      <table-wrap position="float" id="pone.0255030.t003">
        <object-id pub-id-type="doi">10.1371/journal.pone.0255030.t003</object-id>
        <label>Table 3</label>
        <caption>
          <title>Parameters of visualization datasets.</title>
        </caption>
        <alternatives>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.t003" id="pone.0255030.t003g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" rowspan="2" style="border-bottom-width:thick" colspan="1"/>
                <th align="center" rowspan="2" style="border-bottom-width:thick" colspan="1">blocks depth,<break/><italic toggle="yes">d</italic></th>
                <th align="center" colspan="4" rowspan="1">total node count at level</th>
              </tr>
              <tr>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">0</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">1</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">2</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">3</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <monospace>mwaves</monospace>
                </td>
                <td align="center" rowspan="1" colspan="1">7</td>
                <td align="center" rowspan="1" colspan="1">2.15 ⋅ 10<sup>6</sup></td>
                <td align="center" rowspan="1" colspan="1">1.72 ⋅ 10<sup>7</sup></td>
                <td align="center" rowspan="1" colspan="1">1.37 ⋅ 10<sup>8</sup></td>
                <td align="center" rowspan="1" colspan="1">1.10 ⋅ 10<sup>9</sup></td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <monospace>cascade</monospace>
                </td>
                <td align="center" rowspan="1" colspan="1">8</td>
                <td align="center" rowspan="1" colspan="1">3.34 ⋅ 10<sup>5</sup></td>
                <td align="center" rowspan="1" colspan="1">2.54 ⋅ 10<sup>6</sup></td>
                <td align="center" rowspan="1" colspan="1">1.75 ⋅ 10<sup>7</sup></td>
                <td align="center" rowspan="1" colspan="1">7.96 ⋅ 10<sup>7</sup></td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <monospace>hump</monospace>
                </td>
                <td align="center" rowspan="1" colspan="1">8</td>
                <td align="center" rowspan="1" colspan="1">2.02 ⋅ 10<sup>5</sup></td>
                <td align="center" rowspan="1" colspan="1">1.46 ⋅ 10<sup>6</sup></td>
                <td align="center" rowspan="1" colspan="1">6.39 ⋅ 10<sup>6</sup></td>
                <td align="center" rowspan="1" colspan="1">7.72 ⋅ 10<sup>6</sup></td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>The number of blocks at each level is a power of 2 (1, 8, 64 and 512 blocks for levels 0–3 respectively), therefore, parallel rendering at 1, 2, 4 and 8 GPUs is ideally balanced, e.g. each worker thread processes the same number of blocks (of course, the density of the blocks may be different and it can produce an imbalance).</p>
      <p>Also it is worth noticing that the dataset at level 0 consists of a single block, therefore the rendering at level 0 is always sequential. Theoretically, it must be a horizontal straight line; as one can see, that is actually the case.</p>
      <p>The dataset at level 1 consists of 8 blocks, therefore, in the case of 8 GPUs used, each worker thread processes a single block. So one can expect that the rendering speed in this case is almost equal to the rendering speed at level 0. As one can see, this actually takes place in the case of <monospace>cascade</monospace> and <monospace>hump</monospace> problems, which means that the scalability of rendering level 1 is good and close to ideal. The result for <monospace>mwaves</monospace> is slightly worse, but also acceptable.</p>
      <p>The results for levels 2 and 3 have scalability issues, especially in the cases of <monospace>cascade</monospace> and <monospace>hump</monospace>. The rendering time stays almost the same as the number of GPUs used grows. A possible cause of this issue could be the following.</p>
      <p>As mentioned above, the whole rendering task consists of parallel rendering of image parts on GPU and image blending on CPU. Image blending appears to run rather fast, occupying about 5% of overall rendering time. Therefore, image blending time can be neglected, and attention should be paid to the time spent by rendering workers on GPU.</p>
      <p>The performance of rendering tasks was investigated with the NVIDIA <monospace>nvprof</monospace> tool on the <monospace>Tesla</monospace> machine for the <monospace>hump</monospace> problem. Tests have been done for the case of rendering on a single GPU (sequential rendering) and on two GPUs (one worker thread per GPU). Unfortunately, we failed to use <monospace>nvprof</monospace> on <monospace>DGX-1</monospace> with larger number of GPUs due to technical reasons. Anyway, it was found that CUDA memory management functions <monospace>cudaMalloc</monospace>, <monospace>cudaFree</monospace> and <monospace>cudaMemcpy</monospace> take about 70–80% of overall rendering time, which can be the reason of such poor scalability.</p>
      <p>The amount of onboard memory per GPU (see <xref rid="pone.0255030.t002" ref-type="table">Table 2</xref>) is in most cases much larger than memory required for one worker thread. Therefore one can try running multiple worker threads per GPU. The results of such tests are presented in <xref rid="pone.0255030.g009" ref-type="fig">Fig 9</xref>. Diagrams contain rendering durations for each problem on each machine. Only one GPU is used on each machine, but there are 1, 2, or 4 worker threads per GPU. As one can see, there is a scalability issue. Almost all results do not show any speedup of the rendering when the number of workers grows. The only appropriate case is level 3 on <monospace>DGX-1</monospace>. This problem is subject for a future investigation.</p>
      <fig position="float" id="pone.0255030.g009">
        <object-id pub-id-type="doi">10.1371/journal.pone.0255030.g009</object-id>
        <label>Fig 9</label>
        <caption>
          <title>Dependency of rendering duration on the number of workers per GPU.</title>
          <p>The rendering is performed on one GPU of each machine, so all worker threads use one GPU. The rendering algorithm is maximal intensity projection. The combination <monospace>GeForce</monospace>-<monospace>mwaves</monospace> is absent because it could not be processed due to insufficient memory—this issue is to be resolved in further versions.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.g009" position="float"/>
      </fig>
    </sec>
    <sec id="sec015">
      <title>3.2 Visualization quality</title>
      <p>Examples of images rendered for datasets <monospace>hump</monospace> and <monospace>cascade</monospace> are shown in <xref rid="pone.0255030.g010" ref-type="fig">Fig 10</xref>. Each of those high quality images is rendered in hundreds of milliseconds on <monospace>DGX-1</monospace> and <monospace>Tesla</monospace>, and in about a second on <monospace>GeForce</monospace>.</p>
      <fig position="float" id="pone.0255030.g010">
        <object-id pub-id-type="doi">10.1371/journal.pone.0255030.g010</object-id>
        <label>Fig 10</label>
        <caption>
          <title>Examples of images rendered by ReVisE.</title>
          <p>All images are rendered at level 3 with block depth 8, which corresponds to 3D texture resolution 2049 in each dimension. A: <monospace>hump</monospace> dataset, maximum intensity projection. B: <monospace>hump</monospace> dataset, colormap with transparency and per-sample lighting. C: <monospace>cascade</monospace> dataset, multiple transparent isosurfaces. D: <monospace>cascade</monospace> dataset, colormap with transparency.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.g010" position="float"/>
      </fig>
      <p>ReVisE implements progressive rendering to provide better interactivity. When visualizer controller input changes due to a user action, a new image is rendered at level 0 and sent to the client; if at that moment the input is still the same, an image is rendered at level 1 and sent to the client, and so on, until all metadata layers are rendered. At user action, any rendering of levels 1 and higher in progress is cancelled, and the process starts again from level 0. As a result, in the progressive rendering mode user sees lower quality images first, followed by higher quality images as soon as they become available. <xref rid="pone.0255030.g011" ref-type="fig">Fig 11</xref> presents examples of images rendered at different levels and demonstrates that, on the one hand, lower-quality images still carry useful information, and, on the other hand, higher-quality images provide more detailed view of visualized fields’ features.</p>
      <fig position="float" id="pone.0255030.g011">
        <object-id pub-id-type="doi">10.1371/journal.pone.0255030.g011</object-id>
        <label>Fig 11</label>
        <caption>
          <title>Images rendered for the <monospace>cascade</monospace> dataset at different levels.</title>
          <p>All images are rendered at block depth 8. A: Maximum intensity projection. B: Colormap with transparency and per-sample lighting. C: Single opaque isosurface. D: Single opaque isosurface, zoomed.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.g011" position="float"/>
      </fig>
      <p>To further assess the quality of images generated by ReVisE, similar images have been also obtained with ParaView. To make it possible, a utility has been created (and added to the ReVisE repository) that converts files from any format readable by ReVisE to the CGNS format readable by ParaView. <xref rid="pone.0255030.g012" ref-type="fig">Fig 12</xref> shows ParaView output on the left, and ReVisE output on the right. Notice that ParaView images have been generated using the original ‘cascade’ mesh data, without any intermediate approximation; therefore, ParaView images should be interpreted as “ground truth”.</p>
      <fig position="float" id="pone.0255030.g012">
        <object-id pub-id-type="doi">10.1371/journal.pone.0255030.g012</object-id>
        <label>Fig 12</label>
        <caption>
          <title>Images rendered for the <monospace>cascade</monospace> dataset by ParaView (A, C) and ReVisE at level 3 (B, D).</title>
          <p>A, B: Field at the domain boundary surface (colormap). C, D: Single opaque isosurface of another field; the same isosurface zoomed.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.g012" position="float"/>
      </fig>
      <p>Comparing ParaView and ReVisE images, one can conclude that the corresponding images are very close to each other, although not identical. The sources of differences are (1) small differences in color transfer function and probably different color transformations in rendering pipelines of ParaView and ReVisE, (2) different light source positions, in the case of isosurfaces, (3) small differences in camera positions, (4) different input data (original mesh for ParaView and preprocessed blocks for ReVisE). Let us notice that the comparison of zoomed isosurface parts in <xref rid="pone.0255030.g012" ref-type="fig">Fig 12</xref> proves that the rendering quality in the presented example is sufficient, and as the detail level increases, the geometry of isosurfaces obtained with ReVisE converges to “ground truth” (notice that the images C, D at level 3 in <xref rid="pone.0255030.g011" ref-type="fig">Fig 11</xref> are the same as D in <xref rid="pone.0255030.g012" ref-type="fig">Fig 12</xref>).</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec016">
    <title>4 Discussion</title>
    <p>The ReVisE system is designed with interactive visualization in mind. To reach this goal, all visualization-specific data processing has been split into two phases, which are the preprocessing and the visualization-time processing. The preprocessing is done once per problem. It contains computationally intensive operations that are done offline, i.e., without user interaction. Further visualization of the resulting dataset does not involve such lengthy operations and can be done in real time. Quality of real-time visualization is determined by the effective 3D texture resolution, which in turn depends on computational resources available at visualization time. As has been shown, medium-quality images can be obtained in real time on a desktop system equipped with a GeForce class GPU.</p>
    <sec id="sec017">
      <title>4.1 Comparison with other systems</title>
      <p>The most recognized visualization system for numerical simulation results is ParaView [<xref rid="pone.0255030.ref002" ref-type="bibr">2</xref>]. Therefore, it is natural to compare visualization performance of ReVisE with that of ParaView.</p>
      <p>Preprocessing stage is done offline in ReVisE, resulting in a persistent dataset for further visualization. Therefore, the preprocessing does not affect user experience during visualization session. At visualization time, ReVisE performance does not strongly depend on the size of the original dataset, because data size at each metadata level is limited. Practically, larger original datasets result in more detailed octrees generated at the preprocessing stage, but the total number of levels sufficient for visualization can be limited explicitly, and the worst possible performance for different levels of a dense dataset is known—see <monospace>mwave</monospace> example above. In contrast to that, ParaView processes the original dataset at each startup, forcing user to wait some initial time that can be quite large and depends on dataset size.</p>
      <p>To obtain reasonable animation performance with ParaView, a specific format of the dataset is required: ParaView volume rendering algorithm works fast only in the case of the “image” data format, which corresponds to a regular Cartesian grid. Therefore, data has to be preprocessed (resampled to image) before animation—trying to preprocess on-the-fly leads to prohibitively low animation frame rates (each time step requires many seconds to render), which is a consequence of having to process the original dataset.</p>
      <p>In order to compare the performance of ReVisE and ParaView, the <monospace>hump</monospace> model has been chosen. The numerical solution contains 100 time frames of unsteady gas flow, therefore, the speed of time history animation can be compared. That has been done on the <monospace>Tesla</monospace> system described in <xref rid="pone.0255030.t002" ref-type="table">Table 2</xref>. The original dataset has been converted into the <monospace>pvti</monospace> image format for visualization in ParaView. Each time frame has been represented by regular grid 1024 × 152 × 67. This size of the grid allows to compare visualization speed in ParaView against visualization speed of level 2 in ReVisE: the s3dmm dataset of level 2 with depth 8 is expanded to the dense grid with 1024 cells per edge. Therefore, dense grids in ParaView and ReVisE visualization datasets are expected to be equal.</p>
      <p>Both ParaView and ReVisE were running on <monospace>Tesla</monospace> in server mode, and the corresponding client side was running on a remote PC connected to server via Internet.</p>
      <p>As a preliminary test, interactive pan/zoom/rotate operations have been tested in ParaView and ReVisE, without animation along the time axis. Volume rendering speed for ParaView and ReVisE during those operations is similar when one GPU is used (we were not able to use more than one GPU in ParaView without installing any plugins). Specifically for the <monospace>hump</monospace> dataset, ParaView reports frame rates between 5 and 10, and it can be seen that a simplified rendering algorithm is used while the user holds the mouse button. In ReVisE, frame rates are higher (about 20 fps), but only when the progressive rendering option is enabled. This provides a bit smoother interaction experience. When the progressive rendering option is disabled, ReVisE renders 4–5 frames per second at level 2 and 1–2 fps at level 3. With larger number of GPUs, ReVisE frame rates are higher, e.g. with two GPUs it is about 10 fps at level 2 and 4–5 fps at level 3.</p>
      <p>As the main test, the total animation duration of all 100 time frames has been measured. Volume rendering mode has been used in the test. The results are presented in <xref rid="pone.0255030.t004" ref-type="table">Table 4</xref>. The second column represents results for ParaView: all 100 frames were played in 828 seconds, which corresponds to frame rate of 0.12 fps. Next two columns represent the same results for ReVisE. Column 3 contains results for the case when 2 GPUs are used for rendering with 4 worker threads per GPU (notation is 2/4). Column 4 corresponds to the case of sequential rendering with only 1 worker thread and 1 GPU used (notation 1/1).</p>
      <table-wrap position="float" id="pone.0255030.t004">
        <object-id pub-id-type="doi">10.1371/journal.pone.0255030.t004</object-id>
        <label>Table 4</label>
        <caption>
          <title>Comparison of animation duration with ParaView and ReVisE.</title>
          <p>Notation 2/4 means that 2 GPUs are used with 4 worker threads per GPU; 1/1 means that 1 GPU with 1 worker thread is used.</p>
        </caption>
        <alternatives>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.t004" id="pone.0255030.t004g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" rowspan="2" style="border-bottom-width:thick" colspan="1"/>
                <th align="center" rowspan="2" style="border-bottom-width:thick" colspan="1">ParaView</th>
                <th align="center" colspan="3" rowspan="1">ReVisE</th>
              </tr>
              <tr>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">2/4</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">1/1</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">preprocessing</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <bold>duration, s</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">828</td>
                <td align="center" rowspan="1" colspan="1">24</td>
                <td align="center" rowspan="1" colspan="1">38</td>
                <td align="center" rowspan="1" colspan="1">583</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <bold>frame rate, fps</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">0.12</td>
                <td align="center" rowspan="1" colspan="1">4.17</td>
                <td align="center" rowspan="1" colspan="1">2.63</td>
                <td align="center" rowspan="1" colspan="1"/>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>First of all, the measurement with <monospace>nvidia-smi</monospace> has shown that ParaView does not load GPU more than 35% and volume rendering is performed very fast.</p>
      <p>Possible reasons for such poor animation frame rate in ParaView are low speed of reading data from HDD and memory allocation/freeing due to VTK objects creation. It is worth noticing that pan/zoom/rotate operations are performed in real time, because they involve volume rendering only and do not involve loading from HDD or VTK objects allocations.</p>
      <p>Therefore, one can see that ReVisE does the animation significantly faster. In our opinion, the main reason for that is the use of sparse visualization dataset, which becomes dense only in GPUs memory.</p>
      <p>The last column of <xref rid="pone.0255030.t004" ref-type="table">Table 4</xref> contains the duration of visualization dataset generation (preprocessing). As one can see, that time is even smaller than the duration of animation playing in ParaView. It should be noticed that the ReVisE visualization dataset is only generated once and further reused multiple times. Also it should be noticed that the generation of <monospace>pvti</monospace> files for animation in ParaView requires much more time than the ReVisE preprocessing: all 100 frames were generated in almost 4 hours. The sizes of these datasets are almost equal: <monospace>pvti</monospace> dataset is about 60 Gb, whereas s3dmm dataset is 70 Gb. It is worth noticing that s3dmm dataset contains levels 0–3, which is redundant for the comparison, since, as already mentioned, ParaView animation is compared against ReVisE animation at level 2. A much smaller s3dmm dataset could be generated for level 2.</p>
      <p>In our opinion, the big difference in animation speed between ReVisE and ParaView is mostly due to non-optimal data flow in ParaView. ReVisE has been specifically designed to be efficient in that respect from day 0, whereas in the case of ParaView the main idea was the flexibility and potentially a wide range of general functionality. ReVisE dataset storage format ensures the ability to read data contiguously, without having to perform many seek operations. The amount of data to be read varies, depending on the desired detail level, which allows to generate preliminary low-resolution image very fast, but still have high data read speed for higher resolution. Once read, ReVisE sparse data needs to be uploaded to GPUs block-by-block, and processed on the GPUs. In the case of ParaView, it may happen that data reading operations are not as efficient, and the data undergoes processing on CPU after loading, which may take considerable time.</p>
      <p>The use of <monospace>pvti</monospace> image files in ParaView has one more problem. While this approach works for medium-size datasets, it does not scale well, because ParaView volume rendering algorithm takes a 3D image and uploads it to the GPU as a whole. Firstly, the entire image may not fit into GPU memory if it is large enough. Secondly, uploading the entire image to GPU takes considerable time; if the original dataset consists of fields on a non-homogeneous mesh, it is more efficient to adopt our approach and upload sparse fields to GPU and then convert those fields into dense ones directly on GPU.</p>
      <p>Another option for ParaView is using a plugin, such as OSPRay or NVIDIA IndeX. The work [<xref rid="pone.0255030.ref026" ref-type="bibr">26</xref>] presents the results of using these two plugins, as well as the built-in volume rendering functionality, to animate a huge dataset with mesh containing 6912×3456×384 (about 9 ⋅ 10<sup>9</sup>) nodes. It is shown that with NVIDIA IndeX, animation frames are generated at 1.48 fps on 8 cluster nodes, each having one GPU; before animation starts rendering, the startup time of 49 s is necessary.</p>
      <p>There are other systems for interactive visualization. FAST is an open-source system that provides algorithms to visualize dataset from the medical domain. According to [<xref rid="pone.0255030.ref007" ref-type="bibr">7</xref>], the system reaches good rendering times for CT images (about 2 seconds for 512×512×426 image); however, FAST seem to be using only dense data, while for many datasets with irregular geometry and non-homogeneous unstructured grids, taking data sparsity into account gives significant performance gain. Also, we did not find any published results on animation performance.</p>
      <p>Sight [<xref rid="pone.0255030.ref001" ref-type="bibr">1</xref>] is a system used to render animated particle data and claimed to reach frame rates of 30 fps and more on <monospace>DGX-1</monospace> for datasets of up to 600 million particles. The system uses OSPRay and NVIDIA OptiX for rendering using both CPU and GPU. We did not find additional details about the Sight system and its performance; it seems to be a closed-source project used internally at ORNL.</p>
    </sec>
    <sec id="sec018">
      <title>4.2 Further work</title>
      <p>According to the results of ReVisE rendering performance measurement, we conclude that the scalability across multiple GPUs of a single computational node is far from ideal. Profiling data show that some operations take longer time when additional GPUs are used. Those operations are memory allocations on GPU and data transfers from host to GPU memory. The duration of these two kinds of operations significantly impacts scalability, which we did not expect. Current design of the rendering code involves quite large number of memory allocations at each rendering operation. We plan to significantly improve this situation by changing the source code such that GPU memory is reused. As a result, rendering times will be less, and the scalability across multiple GPUs is expected to be better. However, the problem of slowing down data transfers when multiple GPUs are used still has to be addressed and needs further investigation. In general, the problem of slow data transfers is partly solved by employing latency hiding; as we have shown, this may work even within a naive attempt to just increase the number of worker threads per GPU; a more elaborate design is expected to give better results. Another idea is distributing the rendering across different computational nodes, each having one GPU (as in [<xref rid="pone.0255030.ref026" ref-type="bibr">26</xref>]). Our further plans include the implementation of this approach to parallelize the rendering on a cluster. One more way to address the problem is to reduce data transfers using a fast compression algorithm, such as [<xref rid="pone.0255030.ref027" ref-type="bibr">27</xref>, <xref rid="pone.0255030.ref028" ref-type="bibr">28</xref>].</p>
      <p>Currently the ReVisE project has reached the proof-of-concept state, demonstrating the feasibility of our approach to the visualization of large time-dependent datasets. Further plans include enhancements of performance, usability, and stability necessary for production usage. Performance and scalability improvements include the development of remote execution components allowing to parallelize visualization server across nodes of a cluster (this is currently work in progress). For further scaling to larger datasets, preprocessing needs to be augmented with the ability to merge octrees and fields obtained separately for subdomains. Another way to enhance visualization performance is to implement a mechanism similar to ray-guided rendering, as proposed in [<xref rid="pone.0255030.ref009" ref-type="bibr">9</xref>], in order to reduce the number of blocks to be rendered. An important feature to implement is the API for in-situ visualization, with ReVisE visualization dataset obtained as the output. One attractive direction of further evolution is the integration with ParaView using its plugin mechanism.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec019">
    <title>5 Conclusion</title>
    <p>A new system, ReVisE, for the remote visualization of time-dependent scalar fields defined on large meshes, has been designed and implemented. The design combines a number of known ideas, such as using octrees, and adds new ones, like efficient data structure to store and use octrees. A number of real-life CFD problems have been considered; tests show that ReVisE outperforms other systems in some cases, especially for large time-dependent datasets, due to its specific design. Nevertheless, there is a big potential for further improvement of performance, especially by optimizing GPU memory operations and parallelizing across several nodes of a cluster. ReVisE has a limited functionality, and this can be addressed in a number of ways, one of which is integration with ParaView.</p>
  </sec>
</body>
<back>
  <ack>
    <p>Authors are thankful to Supercomputer Center “Polytechnic” of Peter the Great St. Petersburg Polytechnic University for providing access to a DGX-1 machine and the “Tornado” supercomputer.</p>
    <p>In memory of professor Nikolay Shabrov, who launched the ReVisE project.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="pone.0255030.ref001">
      <label>1</label>
      <mixed-citation publication-type="other">Hernandez B. Sight: Exploratory Visualization of Particle Data. In: Conference: Oak Ridge Leadership Computing Facility Users Meeting; 2017.</mixed-citation>
    </ref>
    <ref id="pone.0255030.ref002">
      <label>2</label>
      <mixed-citation publication-type="book"><name><surname>Ahrens</surname><given-names>J</given-names></name>, <name><surname>Geveci</surname><given-names>B</given-names></name>, <name><surname>Law</surname><given-names>C</given-names></name>. <chapter-title>ParaView: An End-User Tool for Large-Data Visualization</chapter-title>. In: <source>The Visualization Handbook</source>; <year>2005</year>.</mixed-citation>
    </ref>
    <ref id="pone.0255030.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Rogers</surname><given-names>D</given-names></name>, <name><surname>Geveci</surname><given-names>B</given-names></name>, <name><surname>Eschenbert</surname><given-names>K</given-names></name>, <name><surname>Neundorf</surname><given-names>A</given-names></name>, <name><surname>Marion</surname><given-names>P</given-names></name>, <name><surname>Moreland</surname><given-names>K</given-names></name>, <etal>et al</etal>. <source>Large scale visualization on the Cray XT3 using ParaView</source>.; <year>2008</year>.</mixed-citation>
    </ref>
    <ref id="pone.0255030.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Wald</surname><given-names>I</given-names></name>, <name><surname>Johnson</surname><given-names>G</given-names></name>, <name><surname>Amstutz</surname><given-names>J</given-names></name>, <name><surname>Brownlee</surname><given-names>C</given-names></name>, <name><surname>Knoll</surname><given-names>A</given-names></name>, <name><surname>Jeffers</surname><given-names>J</given-names></name>, <etal>et al</etal>. <article-title>OSPRay—A CPU Ray Tracing Framework for Scientific Visualization</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source>. <year>2017</year>;<volume>23</volume>(<issue>1</issue>):<fpage>931</fpage>–<lpage>940</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TVCG.2016.2599041</pub-id><?supplied-pmid 27875206?><pub-id pub-id-type="pmid">27875206</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0255030.ref005">
      <label>5</label>
      <mixed-citation publication-type="book"><name><surname>Parker</surname><given-names>SG</given-names></name>, <name><surname>Bigler</surname><given-names>J</given-names></name>, <name><surname>Dietrich</surname><given-names>A</given-names></name>, <name><surname>Friedrich</surname><given-names>H</given-names></name>, <name><surname>Hoberock</surname><given-names>J</given-names></name>, <name><surname>Luebke</surname><given-names>D</given-names></name>, <etal>et al</etal>. <chapter-title>OptiX: A General Purpose Ray Tracing Engine</chapter-title>. In: <source>ACM SIGGRAPH 2010 Papers</source>. SIGGRAPH’10. <publisher-loc>New York, NY, USA</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>; <year>2010</year>. Available from: <pub-id pub-id-type="doi">10.1145/1833349.1778803</pub-id>.</mixed-citation>
    </ref>
    <ref id="pone.0255030.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Yasir</surname><given-names>SF</given-names></name>, <name><surname>Jani</surname><given-names>J</given-names></name>, <name><surname>Mukri</surname><given-names>M</given-names></name>. <article-title>A dataset of visualization methods to assessing soil profile using RES2DINV and VOXLER software</article-title>. <source>Data in Brief</source>. <year>2019</year>;<volume>24</volume>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.dib.2019.103821</pub-id><?supplied-pmid 30976635?><pub-id pub-id-type="pmid">30976635</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0255030.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Smistad</surname><given-names>E</given-names></name>, <name><surname>Bozorgi</surname><given-names>M</given-names></name>, <name><surname>Lindseth</surname><given-names>F</given-names></name>. <article-title>FAST: framework for heterogeneous medical image computing and visualization</article-title>. <source>International Journal of Computer Assisted Radiology and Surgery</source>. <year>2015</year>;<volume>10</volume>:<fpage>1811</fpage>–<lpage>1822</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11548-015-1158-5</pub-id><?supplied-pmid 25684594?><pub-id pub-id-type="pmid">25684594</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0255030.ref008">
      <label>8</label>
      <mixed-citation publication-type="other">Caldarola EG, Rinaldi AM. Big Data Visualization Tools: A Survey—The New Paradigms, Methodologies and Tools for Large Data Sets Visualization. In: DATA; 2017.</mixed-citation>
    </ref>
    <ref id="pone.0255030.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Beyer</surname><given-names>J</given-names></name>, <name><surname>Hadwiger</surname><given-names>M</given-names></name>, <name><surname>Pfister</surname><given-names>H</given-names></name>. <article-title>State-of-the-Art in GPU-Based Large-Scale Volume Visualization</article-title>. <source>Comput Graph Forum</source>. <year>2015</year>;<volume>34</volume>(<issue>8</issue>):<fpage>13</fpage>–<lpage>37</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/cgf.12605</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0255030.ref010">
      <label>10</label>
      <mixed-citation publication-type="book"><name><surname>Orlov</surname><given-names>S</given-names></name>, <name><surname>Kuzin</surname><given-names>A</given-names></name>, <name><surname>Zhuravlev</surname><given-names>A</given-names></name>. <chapter-title>Core algorithms of sparse 3D mipmapping visualization technology</chapter-title>. In: <name><surname>Voevodin</surname><given-names>V.</given-names></name>, <name><surname>Sobolev</surname><given-names>S.</given-names></name> (eds) Supercomputing. RuSCDays 2020. <source>Communications in Computer and Information Science</source>. <volume>vol. 1331</volume>. <publisher-name>Springer</publisher-name>, <publisher-loc>Cham</publisher-loc>; <year>2020</year>. p. <fpage>413</fpage>–<lpage>424</lpage>. Available from: <pub-id pub-id-type="doi">10.1007/978-3-030-64616-5_36</pub-id>.</mixed-citation>
    </ref>
    <ref id="pone.0255030.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Jackins</surname><given-names>LC</given-names></name>, <name><surname>Tanimoto</surname><given-names>LS</given-names></name>. <article-title>Oct-trees and their use in representing three-dimensional objects</article-title>. <source>Computer Graphics and Image Processing</source>. <year>1980</year>; p. <fpage>249</fpage>–<lpage>270</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/0146-664X(80)90055-6</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0255030.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Gargantini</surname><given-names>I</given-names></name>. <article-title>An Effective Way to Represent Quadtrees</article-title>. <source>Commun ACM</source>. <year>1982</year>;<volume>25</volume>(<issue>12</issue>):<fpage>905</fpage>–<lpage>910</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/358728.358741</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0255030.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Oliver</surname><given-names>MA</given-names></name>, <name><surname>Wiseman</surname><given-names>NE</given-names></name>. <article-title>Operations on Quadtree Encoded Images</article-title>. <source>The Computer Journal</source>. <year>1983</year>;<volume>26</volume>(<issue>1</issue>):<fpage>83</fpage>–<lpage>91</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/comjnl/26.1.83</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0255030.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Woodwark</surname><given-names>JR</given-names></name>. <article-title>Compressed Quad Trees</article-title>. <source>The Computer Journal</source>. <year>1984</year>;<volume>27</volume>(<issue>3</issue>):<fpage>225</fpage>–<lpage>229</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/comjnl/27.3.225</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0255030.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Choi</surname><given-names>MG</given-names></name>, <name><surname>Ju</surname><given-names>E</given-names></name>, <name><surname>Chang</surname><given-names>JW</given-names></name>, <name><surname>Lee</surname><given-names>J</given-names></name>, <name><surname>Kim</surname><given-names>YJ</given-names></name>. <article-title>Linkless Octree Using Multi-Level Perfect Hashing</article-title>. <source>Computer Graphics Forum</source>. <year>2009</year>;<volume>28</volume>(<issue>7</issue>):<fpage>1773</fpage>–<lpage>1780</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/j.1467-8659.2009.01554.x</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0255030.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Lefebvre</surname><given-names>S</given-names></name>, <name><surname>Hoppe</surname><given-names>H</given-names></name>. <article-title>Perfect Spatial Hashing</article-title>. <source>ACM Trans Graph</source>. <year>2006</year>;<volume>25</volume>(<issue>3</issue>):<fpage>579</fpage>–<lpage>588</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/1141911.1141926</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0255030.ref017">
      <label>17</label>
      <mixed-citation publication-type="other">Karypis G, Schloegel K. PARMETIS: Parallel Graph Partitioning and Sparse Matrix Ordering Library Version 4.0. Minneapolis, MN: University of Minnesota; 2011. Available from: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.lrz.de/services/software/mathematik/metis/parmetis_4_0.pdf" ext-link-type="uri">https://www.lrz.de/services/software/mathematik/metis/parmetis_4_0.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0255030.ref018">
      <label>18</label>
      <mixed-citation publication-type="other">Jessup JP, Givigi S, Beaulieu A. Merging of octree based 3D occupancy grid maps. 2014 IEEE International Systems Conference Proceedings. 2014; p. 371–377.</mixed-citation>
    </ref>
    <ref id="pone.0255030.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Singh</surname><given-names>JM</given-names></name>, <name><surname>Narayanan</surname><given-names>PJ</given-names></name>. <article-title>Real-Time Ray Tracing of Implicit Surfaces on the GPU</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source>. <year>2010</year>;<volume>16</volume>(<issue>2</issue>):<fpage>261</fpage>–<lpage>272</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TVCG.2009.41</pub-id><?supplied-pmid 20075486?><pub-id pub-id-type="pmid">20075486</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0255030.ref020">
      <label>20</label>
      <mixed-citation publication-type="other">Bosi M. Visualization Library: Visualization Library Reference Documentation; 2020. Available from: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://visualizationlibrary.org/" ext-link-type="uri">https://visualizationlibrary.org/</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0255030.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Rodriguez-Gil</surname><given-names>L</given-names></name>, <name><surname>Orduña</surname><given-names>P</given-names></name>, <name><surname>Garcia-Zubia</surname><given-names>J</given-names></name>, <name><surname>López-de Ipiña</surname><given-names>D</given-names></name>. <article-title>Interactive live-streaming technologies and approaches for web-based applications</article-title>. <source>Multimedia Tools and Applications</source>. <year>2017</year>; p. <fpage>1</fpage>–<lpage>32</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11042-017-4556-6</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0255030.ref022">
      <label>22</label>
      <mixed-citation publication-type="other">Zhang YW. A Scalable Streaming Media System. DEStech Transactions on Social Science, Education and Human Science. 2017.</mixed-citation>
    </ref>
    <ref id="pone.0255030.ref023">
      <label>23</label>
      <mixed-citation publication-type="journal"><name><surname>Probst</surname><given-names>A</given-names></name>, <name><surname>Schwamborn</surname><given-names>D</given-names></name>, <name><surname>Garbaruk</surname><given-names>A</given-names></name>, <name><surname>Guseva</surname><given-names>E</given-names></name>, <name><surname>Shur</surname><given-names>M</given-names></name>, <name><surname>Strelets</surname><given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Evaluation of grey area mitigation tools within zonal and non-zonal RANS-LES approaches in flows with pressure induced separation</article-title>. <source>International Journal of Heat and Fluid Flow</source>. <year>2017</year>;<volume>68</volume>:<fpage>237</fpage>–<lpage>247</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.ijheatfluidflow.2017.08.008</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0255030.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Duben</surname><given-names>AP</given-names></name>, <name><surname>Kozubskaya</surname><given-names>TK</given-names></name>, <name><surname>Marakueva</surname><given-names>OV</given-names></name>, <name><surname>Voroshnin</surname><given-names>DV</given-names></name>. <article-title>Simulation of flow over high-lifted turbine cascade at low Reynolds numbers</article-title>. <source>J Phys: Conf Ser</source>. <year>2020</year>;submitted to print.</mixed-citation>
    </ref>
    <ref id="pone.0255030.ref025">
      <label>25</label>
      <mixed-citation publication-type="journal"><name><surname>Gorobets</surname><given-names>A</given-names></name>. <article-title>Parallel Algorithm of the NOISEtte Code for CFD and CAA Simulations</article-title>. <source>Lobachevskii Journal of Mathematics</source>. <year>2018</year>;<volume>39</volume>:<fpage>524</fpage>–<lpage>532</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1134/S1995080218040078</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0255030.ref026">
      <label>26</label>
      <mixed-citation publication-type="other">Favre J, Blass A. Renderings of Sheared Thermal Convection. In: The International Conference for High Performance Computing, Networking, Storage, and Analysis; 2018.</mixed-citation>
    </ref>
    <ref id="pone.0255030.ref027">
      <label>27</label>
      <mixed-citation publication-type="other">Rozenberg E, Boncz P. Faster across the PCIe Bus: A GPU Library for Lightweight Decompression: Including Support for Patched Compression Schemes. In: Proceedings of the 13th International Workshop on Data Management on New Hardware. DAMON’17. New York, NY, USA: Association for Computing Machinery; 2017. Available from: <pub-id pub-id-type="doi">10.1145/3076113.3076122</pub-id>.</mixed-citation>
    </ref>
    <ref id="pone.0255030.ref028">
      <label>28</label>
      <mixed-citation publication-type="book"><name><surname>Iakobovski</surname><given-names>MV</given-names></name>, <name><surname>Karasev</surname><given-names>DE</given-names></name>, <name><surname>Krinov</surname><given-names>PS</given-names></name>, <name><surname>Polyakov</surname><given-names>SV</given-names></name>. In: <name><surname>Uvarova</surname><given-names>LA</given-names></name>, <name><surname>Latyshev</surname><given-names>AV</given-names></name>, editors. <source>Visualisation of Grand Challenge Data on Distributed Systems</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Springer US</publisher-name>; <year>2001</year>. p. <fpage>71</fpage>–<lpage>78</lpage>. Available from: <pub-id pub-id-type="doi">10.1007/978-1-4757-3397-6_7</pub-id>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<sub-article article-type="aggregated-review-documents" id="pone.0255030.r001" specific-use="decision-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0255030.r001</article-id>
    <title-group>
      <article-title>Decision Letter 0</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Lisacek</surname>
          <given-names>Frederique</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2021 Frederique Lisacek</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Frederique Lisacek</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="doi" xlink:href="10.1371/journal.pone.0255030" id="rel-obj001" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>0</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">31 Dec 2020</named-content>
    </p>
    <p>PONE-D-20-33961</p>
    <p>ReVisE: Remote visualization environment for large datasets</p>
    <p>PLOS ONE</p>
    <p>Dear Dr. Orlov,</p>
    <p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
    <p>Please submit your revised manuscript by Feb 14 2021 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
    <p>The manuscript raises concerns about the availability of the software and its open source status which is mandatory for publishing in PLOS One. Secondly, the boundary between the previous and current publications is presently too fuzzy and should be made clear. Finally, the comparison of performance with other software should be established in a more convincing manner.</p>
    <p>Please include the following items when submitting your revised manuscript:</p>
    <p>
      <list list-type="bullet">
        <list-item>
          <p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p>
        </list-item>
        <list-item>
          <p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p>
        </list-item>
        <list-item>
          <p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p>
        </list-item>
      </list>
    </p>
    <p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p>
    <p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">http://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link></p>
    <p>We look forward to receiving your revised manuscript.</p>
    <p>Kind regards,</p>
    <p>Frederique Lisacek</p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
    <p>Journal Requirements:</p>
    <p>When submitting your revision, we need you to address these additional requirements.</p>
    <p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at</p>
    <p><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</ext-link> and</p>
    <p>
      <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</ext-link>
    </p>
    <p>2. We note that you have stated that you will provide repository information for your data at acceptance. Should your manuscript be accepted for publication, we will hold it until you provide the relevant accession numbers or DOIs necessary to access your data. If you wish to make changes to your Data Availability statement, please describe these changes in your cover letter and we will update your Data Availability statement to reflect the information you provide.</p>
    <p>3. We noted in your submission details that a portion of your manuscript may have been presented or published elsewhere:</p>
    <p>'Part of this work is to be published in "Communications in Computer and Information Science" at the end of December, 2020. The paper is titled "Core Algorithms of Sparse 3D Mipmapping Visualization Technology" and is further referred to as s3dmm. Its preprint will be uploaded along with other files. There is some intersection of materials published in that paper with the manuscript being submitted (and is further referred to as ReVisE). Those intersections are as follows.</p>
    <p>The "Preprocessing Pipeline" section of s3dmm has common parts with the "Sparse 3D mipmapping" subsection of ReVisE. In particular, Fig 1 of s3dmm coincides with Fig 4 of ReVisE. But there is no one-to-one correspondence between those two sections, because we focused on different things. In ReVisE, we present a reduced version from s3dmm, but the new version also reflects our experience gained during the months after the s3dmm paper was prepared.</p>
    <p>The "Octree storage optimization" section of ReVisE is close to sections "Octree Generation" and "Optimized Storage of Octree Data Structure". Still we decided to include that section into ReVisE because we find the proposed octree compression algorithm quite interesting and useful.</p>
    <p>However, we do not consider ReVisE as a duplicate of s3dmm. The differences are that</p>
    <p>- s3dmm presents the core technology in more details, compared to ReVisE.</p>
    <p>- The results presented in s3dmm do not cover the visualization system as a whole: we write about the rendering of just one block on a PC equipped with GPU, and that was what we had at the moment of writing.</p>
    <p>- On the other hand, ReVisE presents results of performance tests on a number of architectures (DGX-1, Tesla, GeForce), involving remote operation via web application, using video streaming service, and progressive rendering.</p>
    <p>- ReVisE is focused on the presentation of the new visualization system as a whole thing, while s3dmm is dedicated to core algorithms.'</p>
    <p>Please clarify whether this publication was peer-reviewed and formally published.</p>
    <p>If this work was previously peer-reviewed and published, in the cover letter please provide the reason that this work does not constitute dual publication and should be included in the current manuscript.</p>
    <p>4. Please amend the manuscript submission data (via Edit Submission) to include authors Alexey Kuzin, Alexey Zhuravlev, Vyacheslav Reshetnikov, Egor Usik, Vladislav Kiev and Andrey Pyatlin.</p>
    <p>5. Please ensure that you refer to Figure 4 in your text as, if accepted, production will need this reference to link the reader to the figure.</p>
    <p>[Note: HTML markup is below. Please do not edit.]</p>
    <p>Reviewers' comments:</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <bold>Comments to the Author</bold>
    </p>
    <p>1. Is the manuscript technically sound, and do the data support the conclusions?</p>
    <p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Partly</p>
    <p>**********</p>
    <p>2. Has the statistical analysis been performed appropriately and rigorously? </p>
    <p>Reviewer #1: N/A</p>
    <p>Reviewer #2: N/A</p>
    <p>**********</p>
    <p>3. Have the authors made all data underlying the findings in their manuscript fully available?</p>
    <p>The <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.</p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: No</p>
    <p>**********</p>
    <p>4. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
    <p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>**********</p>
    <p>5. Review Comments to the Author</p>
    <p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
    <p>Reviewer #1: In this manuscript the authors present ReVisE, a new software for interactive visualization of large datasets from complex simulations. The manuscript is well written with a clear introduction to the problem, results and discussions. The design of the system is well described, including data structures, algorithms and software architecture. The performance of their software is evaluated with various datasets.</p>
    <p>A major concern is the availability of the software. It is stated that the source code will be made available at GitHub as soon as the paper is published. However, as it is increasingly acknowledged by the scientific community, the source code of any scientific software funded by public institutions and aimed at open source should be available for review. Besides, having an instance with a simple visualization example in a server as demo with a link provided to the reviewers (and later for potential users would also be beneficial) will make a stronger publication.</p>
    <p>Regarding technical details of implementation and deployment, it would be useful to add in Fig. 8 the programming languages of the main software components. Other helpful additions could be mentioning what are the dependencies if there are any and an estimate of the percentage of new code that was implemented or the one re-used (the authors mentioned that it is based on source code of OpenGL fragment shaders found in the Visualization Library).</p>
    <p>The authors claim that tests show that ReVisE outperforms other systems in some cases, especially for large time-dependent datasets. However, the comparison with other systems does not seem quantitative. Even if numbers are from the respective publications of the other tools, a comparison table indicating dataset, hardware, execution time and some comments specific to each system would be preferred.</p>
    <p>Minor suggestions are made to improve the manuscript:</p>
    <p>• Results: please add details of the deployment used for the tests. Where all servers in the same computer?</p>
    <p>• Line 534: GPU memoy -&gt; GPU memory</p>
    <p>• Line 550: didn’t -&gt; did not (too informal otherwise)</p>
    <p>Reviewer #2: The paper introduces a new visualization system, called ReVisE, that aims at remotely visualizing large---not of the reviewer numerical simulation---datasets. I have appreciated to review this article, that has an overall interesting content, and that takes the pre-processing approach in order to solve a task that is otherwise not reachable at the scale of nodes the author intend to work with in real time. We have nonetheless to wait line 8 to understand the kind of data that are manipulated, also something like "for large numerical simulation datasets" in the title and abstract would immediately reduce the scope of the article, as the purpose is not to visualize other kind of large datasets such as corpus of texts.</p>
    <p>In the introduction, the lack of available visualization software that scales up is emphasized (l.13) to motivate the introduction of their system or with performance up to 10^6 nodes. If the scaling issue is tackled clearly in the rest of the article, does the proposed platform intended to be available Open Source, is there a Demo somewhere or at least a video of the platform?</p>
    <p>The related work relies more on other softwares / products, than on what are the existing technologies, what is the state of the art this Research article is linked to.</p>
    <p>In order to help relates this article to existing work, references such as BEYER 2014 State of the Art in GPU Based Large Scale Volume Visualization or Wang 2020 Portable interactive visualization of large-scale simulations in geotechnical engineering using Unity3D, which introduces not only the different softwares but also the different technologies to motivate deeply their approach, could be used. A beginning of argumentation and a clear introduction of the plan should be made at the end of the introduction.</p>
    <p>Sections, paragraphs and subparagraphs at least should be numbered, as it would really simplify the reading, especially when forward references are made as the size of the police is not sufficient to make a clear distinction between sub-paragraphs and sub-sub-paragraphs. When pointing to Forward or Backward reference in your article, you should refer precisely.</p>
    <p>The task to be solved should be clearly stated as such in the introduction and related to the research question the author want to solve; formulating it properly, would induce the steps to achieve to properly evaluate the proposed platform, in comparison with other existing platforms.</p>
    <p>The article appears to be an extension of the already published article [12] (from line 66 to line 315). It should be stated as such both in the abstract and in the introduction. The new contributions of this article have to be clearly stated from the introduction. To help clarify this point, the paragraph Sparse 3D mipmapping, should not only said that the sparse mipmapping has been proposed in [12] but also mention that the whole paragraph reintroduce the concepts for the sake of clarity. Having an Arxiv version of [12] would definitely help the interested reader. When full paragraphs (l.224 to 248) are taken from [12] they should be quoted and referred as such. Figures from [12] (Fig.3 that corresponds to Fig. 2 and 5 of [12], Fig 4 that is Fig 1 of [12], Fig 5 that is Fig.7 of [12]) cannot be reproduced as such for evident copyright reasons: they should either differ or be cited from [12]: in this case, authors should have the consent of Springer for reusing them in another article.</p>
    <p>L.242 to 270:</p>
    <p>How to compare two things that will not be comparable, with a part of pre-processing with the ReVisE and other system that are synchronous? The only way is to compare at least the full process and show what are the gains / costs of making it asynchronous. The other point, is that the scalar field has to change over time, also how are you going to take that into account if it is in the part of the pre-processing? This paragraph lets understand there is still work to do on that (l.269-270), which would explain that the pre-processing is not taken into account (l.244-245); and in this case how do you evaluate a reasonable duration (l.245)? How do you compare it to existing frameworks? l. 266 to 268: are there corroborating results to assert the advantage of the proposed algorithm as [12] does not show any baseline for comparing?</p>
    <p>Moreover the algorithm for stage (5) should be given.</p>
    <p>The new contributions of the article comes from line 316 and forward, include the introduction of the general architecture of the visualization system, moving to the engineering part of the project, mostly descriptive of the implementation; the level of details is largely sufficient to follow.</p>
    <p>Results part (and I would suggest to rename this part Results and Evaluation):</p>
    <p>To evaluate the performances of the proposed system, the proposed baseline in Comparison with other systems relies mainly on open systems, and particularly Paraview. As it is mentioned Paraview does the pre-processing on the fly, so it is going to be hard to compare two processes that do not rely on the same basis. In order to keep this baseline, results on preprocessing time and visualization processing time should be gathered in one table for both systems. If they are equivalent for the different datasets, then we can conclude that the one that has the pre-processing is going to outperform the one that has to recompute each time. A Table with figures for each stage is expected at that level for at least ReVisE and the baseline, for each proposed dataset and for the same level. Additionaly, l. 403 (preprocessing times have also been measured) contradicts l. 245, and no figures are given.</p>
    <p>In the Visualization performance (first occurrence l.428, as a second paragraph l.557 is named the same) the results are centered on the presented system; some tables presenting the datasets with the number of nodes at each level would be an asset (l.435-440). The approach is interesting. For helping the reading l.465 to 475 should be put before l.457 to 464 in order to avoid a forward reference on l.456.</p>
    <p>In the Visualisation quality, there is the need to evaluate it properly: which task has to be solved? which metric is used to show the improvement on the quality and how do we compare this quality to a baseline (what would have been generated by Paraview)? Only experts of the field could answer about the quality of such visualization and respond to what is asserted in lines 487 to 490, as in fact to restrain to level 3 might not be as assumed in the introduction the level of quality the experts would need to have. This evaluation should be done by mixing those visualization with visualization generated with Paraview.</p>
    <p>In the paragraph Comparison with other systems, you mention l.504 "According to our experience" for the better interactivity: this is very subjective and need to be quantified. Particularly, I would have expected to see results on the number of fps rendered with Revise compared to other systems. Once again a well designed Table with different criteria for comparing the systems would be welcome.</p>
    <p>I would put the second paragraph Visualization performance in Future work (more than Further work) as from line 564 it concerns only Future work.</p>
    <p>A last question has to be raised with the final sentence: why would not have started by integrating the ReVisE in the Paraview? I think a different formulation can be found for this point.</p>
    <p>Additional remarks:</p>
    <p>Direct style (for instance l.60-61), imperative forms (l.110, l.281 for instance) should be avoided in a scientific article. Statements should be accompanied by proovable arguments; particularly when coming to evaluation this is definitely needed. I would have expected the visualization performance to be compared to existing softwares, and particularly to Paraview as far as the number of nodes are comparable (maybe for instance with the hump dataset). It would have included in this case the preprocessing stage and the visualization stage, as it has been partially done in [12], showing the gain that we can expect from the new platform. Coming back to the assertion made on line 60-61 there is no proof (i.e. no comparison on an A/B testing for instance) of the visualization made. A baseline would be again to simplify the problem in a first instance to make two comparable visualization one obtained by Paraview for instance and one obtained by your method, and then scaling, and showing what happens. A video comparing the two could also be an asset. A particular attention to articles (a and the) should be given, particularly when they are missing (for instance l.339: it uploads the image to the shared memory; l.343: the video streaming service checks the shared memory cotents). Bullets should be avoided in an article (it is not a presentation); a table is often a good replacement (l 405 to 437 and l.420 to 427)</p>
    <p>Some remarks on Figures:</p>
    <p>All the figures come at the end of the article and are not included in proper way in the article, which makes things difficult to follow.</p>
    <p>Figure 1: I would let the mesh behind in B (as it is done in C). Moreover, the frame used for the quadtree should be marked as such in A, just to be sure that all Figures from A to F are at the same dimension of representation (which does not seem to be the case).</p>
    <p>Figure 2: I think the mesh chosen for illustrating a bit hard to follow and would let it by transparency under the different quadtrees generated.</p>
    <p>Figure 3: I would add colors to help following the octree, or / and draw the graph tree corresponding.</p>
    <p>Figure 4: having the mesh by transparency under the quadtrees generated would help.</p>
    <p>Figure 5: It differs only from one node from Springer. A legend indicating nodes signification directly on the Figure it self would help (and not in the Legend below the Figure). The Legend below figure 5: the second B should be C.</p>
    <p>Figure 8: two inversions: Numerical solution (oi inverted) and Web server: visualization controller (rt inverted)</p>
    <p>Figure 9: ReVisE web interface and not ReVesE in the legend</p>
    <p>Suggestions of figures:</p>
    <p>A figure similar to Figure 3 of [12] would clarify lines 126-127.</p>
    <p>A figure might help to clarify l.154 - 162.</p>
    <p>Mispelling / other remarks:</p>
    <p>l.403: preformance instead of performance</p>
    <p>l.345: what information does the sentence bring?</p>
    <p>The article would really benefit to wait for further results that are underwork (particularly the pre-processing stage needed to make full comparison) and to be evaluated using a strong baseline to compare to other existing frameworks / existing methods; this is needed to show that there is a real improvement and not an artefact of improvement, by just lowering the quality of the visualization given when the dataset becomes to big. I let it nonetheless in major revisions if those strong objections can be lifted previously to any final acceptation, considering the work of the authors.</p>
    <p>**********</p>
    <p>6. PLOS authors have the option to publish the peer review history of their article (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.</p>
    <p>Reviewer #1: <bold>Yes: </bold>Aivett Bilbao</p>
    <p>Reviewer #2: No</p>
    <p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
    <p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p>
  </body>
</sub-article>
<sub-article article-type="author-comment" id="pone.0255030.r002">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0255030.r002</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 0</article-title>
    </title-group>
    <related-article xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="doi" xlink:href="10.1371/journal.pone.0255030" id="rel-obj002" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">11 Apr 2021</named-content>
    </p>
    <p>1</p>
    <p>To Editors</p>
    <p>1.1</p>
    <p>Q: Please ensure that your manuscript meets PLOS ONE’s style requirements,</p>
    <p>including those for file naming. The PLOS ONE style templates can be found</p>
    <p>at...</p>
    <p>A: Done</p>
    <p>1.2</p>
    <p>Q: We note that you have stated that you will provide repository information</p>
    <p>for your data at acceptance. Should your manuscript be accepted for publica-</p>
    <p>tion, we will hold it until you provide the relevant accession numbers or DOIs</p>
    <p>necessary to access your data. If you wish to make changes to your Data Avail-</p>
    <p>ability statement, please describe these changes in your cover letter and we will</p>
    <p>update your Data Availability statement to reflect the information you provide.</p>
    <p>A: Yes, we have made all the data accessible. First of all, we have uploaded</p>
    <p>Revise’s sources on Github: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/deadmorous/revise" ext-link-type="uri">https://github.com/deadmorous/revise</ext-link>. It is</p>
    <p>distributed under AGPL-3.0 license. Also we have deposited all input data into</p>
    <p>protocols.io. Corresponding DOI are:</p>
    <p>• Build ReVisE on Ubuntu:</p>
    <p>
      <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://dx.doi.org/10.17504/protocols.io.bruwm6xe" ext-link-type="uri">http://dx.doi.org/10.17504/protocols.io.bruwm6xe</ext-link>
    </p>
    <p>• Prepare and run test on available dataset:</p>
    <p>
      <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://dx.doi.org/10.17504/protocols.io.bruzm6x6" ext-link-type="uri">http://dx.doi.org/10.17504/protocols.io.bruzm6x6</ext-link>
    </p>
    <p>1.3</p>
    <p>Q: We noted in your submission details that a portion of your manuscript may</p>
    <p>have been presented or published elsewhere: ...</p>
    <p>Please clarify whether this publication was peer-reviewed and formally pub-</p>
    <p>lished.</p>
    <p>If this work was previously peer-reviewed and published, in the cover letter</p>
    <p>please provide the reason that this work does not constitute dual publication</p>
    <p>and should be included in the current manuscript.</p>
    <p>A: Yes, the article ”Core Algorithms of Sparse 3D Mipmapping Visualization</p>
    <p>Technology” is peer reviewed and published.</p>
    <p>This is the reference:</p>
    <p>Orlov S., Kuzin A., Zhuravlev A. (2020) Core Algorithms of Sparse 3D</p>
    <p>Mipmapping Visualization Technology. In: Voevodin V., Sobolev S. (eds) Su-</p>
    <p>percomputing. RuSCDays 2020. Communications in Computer and Informa-</p>
    <p>tion Science, vol 1331. Springer, Cham.</p>
    <p>
      <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://doi.org/10.1007/978-3-030-64616-5_36" ext-link-type="uri">https://doi.org/10.1007/978-3-030-64616-5_36</ext-link>
    </p>
    <p>The reasons requested are in the cover letter.</p>
    <p>1.4</p>
    <p>Q: Please amend the manuscript submission data (via Edit Submission) to</p>
    <p>include authors Alexey Kuzin, Alexey Zhuravlev, Vyacheslav Reshetnikov, Egor</p>
    <p>Usik, Vladislav Kiev and Andrey Pyatlin.</p>
    <p>A: Done</p>
    <p>1.5</p>
    <p>Q: Please ensure that you refer to Figure 4 in your text as, if accepted, produc-</p>
    <p>tion will need this reference to link the reader to the figure.</p>
    <p>A: The reference to Figure 4 inserted.</p>
    <p>2</p>
    <p>To Reviewer 1</p>
    <p>2.1</p>
    <p>Q: A major concern is the availability of the software. It is stated that the</p>
    <p>source code will be made available at GitHub as soon as the paper is published.</p>
    <p>However, as it is increasingly acknowledged by the scientific community, the</p>
    <p>source code of any scientific software funded by public institutions and aimed</p>
    <p>at open source should be available for review. Besides, having an instance with</p>
    <p>a simple visualization example in a server as demo with a link provided to the</p>
    <p>reviewers (and later for potential users would also be beneficial) will make a</p>
    <p>stronger publication.</p>
    <p>A: The source codes of Revise are now available at GitHub under AGPL-</p>
    <p>3.0 license: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/deadmorous/revise" ext-link-type="uri">https://github.com/deadmorous/revise</ext-link>. Also all input data is</p>
    <p>deposited into protocols.io. Corresponding DOI are:</p>
    <p>• Build ReVisE on Ubuntu:</p>
    <p>
      <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://dx.doi.org/10.17504/protocols.io.bruwm6xe" ext-link-type="uri">http://dx.doi.org/10.17504/protocols.io.bruwm6xe</ext-link>
    </p>
    <p>• Prepare and run test on available dataset:</p>
    <p>
      <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://dx.doi.org/10.17504/protocols.io.bruzm6x6" ext-link-type="uri">http://dx.doi.org/10.17504/protocols.io.bruzm6x6</ext-link>
    </p>
    <p>2.2</p>
    <p>Q: Regarding technical details of implementation and deployment, it would be</p>
    <p>useful to add in Fig. 8 the programming languages of the main software compo-</p>
    <p>nents. Other helpful additions could be mentioning what are the dependencies</p>
    <p>if there are any and an estimate of the percentage of new code that was imple-</p>
    <p>mented or the one re-used (the authors mentioned that it is based on source</p>
    <p>code of OpenGL fragment shaders found in the Visualization Library).</p>
    <p>A: We fixed Fig. 8.</p>
    <p>Presently Revise does not depend on any external modules, besides trivial</p>
    <p>ones (such as Qt, googletest). It also depends on VisualizationLibrary, which</p>
    <p>is used as external library and is loaded as Git submodule. Presently Revise is</p>
    <p>intended to support two implementations of rendering: with CUDA and with</p>
    <p>OpenGL. The article does not span the second one because it is not developed</p>
    <p>well now and, probably, will not be developed in the future due to some diffi-</p>
    <p>culties when multiple GPUs used and due to absence of advantages over CUDA</p>
    <p>implementation. Revise depends on VisualizationLibrary when it is built in</p>
    <p>OpenGL mode because the rendering works via VL API. The dependency on</p>
    <p>VL in CUDA mode is rudimentary.</p>
    <p>When we write that our implementation of rendering is based on source</p>
    <p>code of shaders of VL we mean that we refer to algorithms used but the code is</p>
    <p>almost fully re-written. VL rendering is OpenGL-based and written in GLSL,</p>
    <p>when our rendering is CUDA-based so it has to be written from scratch as</p>
    <p>CUDA kernels. The VL implementation generally contains classical algorithms,</p>
    <p>such as volume raycasting and we used this implementation as reference. The</p>
    <p>only thing that can be treated as some sort of copying from VL is Blinn lighting</p>
    <p>implementation. Here is the comparison of these both sources:</p>
    <p>Revise:</p>
    <p>
      <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/deadmorous/revise/blob/master/src/s3dmm_cuda/" ext-link-type="uri">https://github.com/deadmorous/revise/blob/master/src/s3dmm_cuda/</ext-link>
    </p>
    <p>cuda_lighting.hpp#L27</p>
    <p>
      <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/deadmorous/revise/blob/master/src/s3dmm_cuda/" ext-link-type="uri">https://github.com/deadmorous/revise/blob/master/src/s3dmm_cuda/</ext-link>
    </p>
    <p>cuda_lighting.hpp#L54</p>
    <p>VL:</p>
    <p>
      <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/MicBosi/VisualizationLibrary/blob/master/data/" ext-link-type="uri">https://github.com/MicBosi/VisualizationLibrary/blob/master/data/</ext-link>
    </p>
    <p>glsl/volume_raycast_isosurface_transp.fs#L44</p>
    <p>
      <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/MicBosi/VisualizationLibrary/blob/master/data/" ext-link-type="uri">https://github.com/MicBosi/VisualizationLibrary/blob/master/data/</ext-link>
    </p>
    <p>glsl/volume_raycast_isosurface_transp.fs#L66</p>
    <p>But it is worth noticing that Blinn lighting is well-known classical algorithm</p>
    <p>with well-known implementation.</p>
    <p>2.3</p>
    <p>Q: The authors claim that tests show that ReVisE outperforms other systems</p>
    <p>in some cases, especially for large time-dependent datasets. However, the com-</p>
    <p>parison with other systems does not seem quantitative. Even if numbers are</p>
    <p>from the respective publications of the other tools, a comparison table indi-</p>
    <p>cating dataset, hardware, execution time and some comments specific to each</p>
    <p>system would be preferred.</p>
    <p>A: A comparison of volume rendering of ParaView and ReVisE added.</p>
    <p>2.4</p>
    <p>Q: Minor suggestions are made to improve the manuscript:</p>
    <p>• Results: please add details of the deployment used for the tests. Where</p>
    <p>all servers in the same computer? A: ReVisE can be deployed on a Linux</p>
    <p>system by building it from the source code, or by using a docker image.</p>
    <p>See <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/deadmorous/revise#installing-revise" ext-link-type="uri">https://github.com/deadmorous/revise#installing-revise</ext-link> and</p>
    <p><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/deadmorous/revise#docker" ext-link-type="uri">https://github.com/deadmorous/revise#docker</ext-link>. In addition, an in-</p>
    <p>stallation procedure is described in this protocol: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://dx.doi.org/" ext-link-type="uri">http://dx.doi.org/</ext-link></p>
    <p>10.17504/protocols.io.bruwm6xe</p>
    <p>• Line 534: GPU memoy → GPU memory A: Done</p>
    <p>• Line 550: didn’t → did not (too informal otherwise) A: Done</p>
    <p>3</p>
    <p>To Reviewer 2</p>
    <p>3.1</p>
    <p>Q: The paper introduces a new visualization system, called ReVisE, that aims at</p>
    <p>remotely visualizing large—not of the reviewer numerical simulation—datasets.</p>
    <p>I have appreciated to review this article, that has an overall interesting content,</p>
    <p>and that takes the pre-processing approach in order to solve a task that is</p>
    <p>otherwise not reachable at the scale of nodes the author intend to work with</p>
    <p>in real time. We have nonetheless to wait line 8 to understand the kind of</p>
    <p>data that are manipulated, also something like ”for large numerical simulation</p>
    <p>datasets” in the title and abstract would immediately reduce the scope of the</p>
    <p>article, as the purpose is not to visualize other kind of large datasets such as</p>
    <p>corpus of texts.</p>
    <p>A: Done</p>
    <p>3.2</p>
    <p>Q: In the introduction, the lack of available visualization software that scales</p>
    <p>up is emphasized (l.13) to motivate the introduction of their system or with</p>
    <p>performance up to 10 6 nodes. If the scaling issue is tackled clearly in the rest of</p>
    <p>the article, does the proposed platform intended to be available Open Source,</p>
    <p>is there a Demo somewhere or at least a video of the platform? The related</p>
    <p>work relies more on other softwares / products, than on what are the existing</p>
    <p>technologies, what is the state of the art this Research article is linked to.</p>
    <p>In order to help relates this article to existing work, references such as BEYER</p>
    <p>2014 State of the Art in GPU Based Large Scale Volume Visualization or Wang</p>
    <p>2020 Portable interactive visualization of large-scale simulations in geotechnical</p>
    <p>engineering using Unity3D, which introduces not only the different softwares</p>
    <p>but also the different technologies to motivate deeply their approach, could be</p>
    <p>used. A beginning of argumentation and a clear introduction of the plan should</p>
    <p>be made at the end of the introduction.</p>
    <p>A: Added reference to BEYER 2014; added three paragraphs at the end of</p>
    <p>the introduction that</p>
    <p>• introduce main ideas from BEYER 2014;</p>
    <p>• classify ReVisE in terms of categories defined in BEYER 2014;</p>
    <p>• outline further sections of the paper.</p>
    <p>3.3</p>
    <p>Q: Sections, paragraphs and subparagraphs at least should be numbered, as it</p>
    <p>would really simplify the reading, especially when forward references are made</p>
    <p>as the size of the police is not sufficient to make a clear distinction between sub-</p>
    <p>paragraphs and sub-sub-paragraphs. When pointing to Forward or Backward</p>
    <p>reference in your article, you should refer precisely.</p>
    <p>A: We make the manuscript according to the template of the journal and,</p>
    <p>unfortunately, it does not have sections numbering. We added referencies to the</p>
    <p>sections, where needed.</p>
    <p>3.4</p>
    <p>Q: The task to be solved should be clearly stated as such in the introduction</p>
    <p>and related to the research question the author want to solve; formulating it</p>
    <p>properly, would induce the steps to achieve to properly evaluate the proposed</p>
    <p>platform, in comparison with other existing platforms.</p>
    <p>A: In order to address and resolve those issues, we have tried to improve</p>
    <p>problem formulation in the “Introduction” section. We also added more com-</p>
    <p>parison results in the “Results and Evaluation” section.</p>
    <p>3.5</p>
    <p>Q: The article appears to be an extension of the already published article [12]</p>
    <p>(from line 66 to line 315). It should be stated as such both in the abstract and</p>
    <p>in the introduction. The new contributions of this article have to be clearly</p>
    <p>stated from the introduction. To help clarify this point, the paragraph Sparse</p>
    <p>3D mipmapping, should not only said that the sparse mipmapping has been</p>
    <p>proposed in [12] but also mention that the whole paragraph reintroduce the</p>
    <p>concepts for the sake of clarity. Having an Arxiv version of [12] would definitely</p>
    <p>help the interested reader. When full paragraphs (l.224 to 248) are taken from</p>
    <p>[12] they should be quoted and referred as such. Figures from [12] (Fig.3 that</p>
    <p>corresponds to Fig. 2 and 5 of [12], Fig 4 that is Fig 1 of [12], Fig 5 that is</p>
    <p>Fig.7 of [12]) cannot be reproduced as such for evident copyright reasons: they</p>
    <p>should either differ or be cited from [12]: in this case, authors should have the</p>
    <p>consent of Springer for reusing them in another article.</p>
    <p>A: The section Sparse 3D mipmapping has been refactored and made shorter</p>
    <p>with more tight references to [12]. It is stated that it retells article [12], but does</p>
    <p>not replace, so the reader has to refer to [12] for more details. According to this</p>
    <p>logic Figures 3, 4 and 5 are removed in order to not duplicate corresponding</p>
    <p>figures from [12].</p>
    <p>3.6</p>
    <p>Q: L.242 to 270: How to compare two things that will not be comparable, with a</p>
    <p>part of pre-processing with the ReVisE and other system that are synchronous?</p>
    <p>The only way is to compare at least the full process and show what are the gains</p>
    <p>/ costs of making it asynchronous. The other point, is that the scalar field has to</p>
    <p>change over time, also how are you going to take that into account if it is in the</p>
    <p>part of the pre-processing? This paragraph lets understand there is still work</p>
    <p>to do on that (l.269-270), which would explain that the pre-processing is not</p>
    <p>taken into account (l.244-245); and in this case how do you evaluate a reasonable</p>
    <p>duration (l.245)? How do you compare it to existing frameworks? l. 266 to 268:</p>
    <p>are there corroborating results to assert the advantage of the proposed algorithm</p>
    <p>as [12] does not show any baseline for comparing? Moreover the algorithm for</p>
    <p>stage (5) should be given.</p>
    <p>A: We think, that on comparison of the speed of interactive rendering be-</p>
    <p>tween ReVisE and other systems, we do not need to take into account prepro-</p>
    <p>cessing time in ReVisE. One should compare the time of the rendering of the</p>
    <p>scene: this time is important for the user because it determines the level of</p>
    <p>convenience of usage of the system for interactive visualization. Thus, this time</p>
    <p>may serve as quantitative characteristic of convenience of use of the system.</p>
    <p>Reduction of this time can be achieved, on the one hand, by speeding up all</p>
    <p>stages, and also, by transferring some operations to offline preprocessing. In</p>
    <p>ReVisE we use, first of all, the second approach.</p>
    <p>Changing of the scalar field over the time is also allowed. Visualization</p>
    <p>dataset consists of metadata that is common for all time frames and arrays</p>
    <p>of sparse fields, calculated for each time frame. An evaluation of visualization</p>
    <p>dataset is performed at the preprocessing stage.</p>
    <p>ReVisE preprocessing times have been included into the new version of the</p>
    <p>paper. Also, it turns out that to gain best performance with Paraview, the</p>
    <p>original dataset has to be preprocessed as well (otherwise, the animation per-</p>
    <p>formance is completely unacceptable). We have compared preprocessing times</p>
    <p>and visualization dataset sizes for ReVisE and Paraview.</p>
    <p>We decided not to present the algorithm for stage (5) because it has already</p>
    <p>been given in our published paper [15]. Presenting the algorithm in this paper</p>
    <p>would increase the intersection of paper content with content published else-</p>
    <p>where, which is not desirable. As follows from the algorithm, it is a single-pass</p>
    <p>one, and should scale good enough as dataset size grows; of course, the com-</p>
    <p>parison with other algorithms needs to be done and will be done as soon as we</p>
    <p>implement our algorithm and present it in a future paper.</p>
    <p>3.7</p>
    <p>Q: The new contributions of the article comes from line 316 and forward, include</p>
    <p>the introduction of the general architecture of the visualization system, moving</p>
    <p>to the engineering part of the project, mostly descriptive of the implementation;</p>
    <p>the level of details is largely sufficient to follow.</p>
    <p>A: Does not require answer</p>
    <p>3.8</p>
    <p>Q: Results part (and I would suggest to rename this part Results and Evalua-</p>
    <p>tion):</p>
    <p>To evaluate the performances of the proposed system, the proposed baseline</p>
    <p>in Comparison with other systems relies mainly on open systems, and particu-</p>
    <p>larly Paraview. As it is mentioned Paraview does the pre-processing on the fly,</p>
    <p>so it is going to be hard to compare two processes that do not rely on the same</p>
    <p>basis. In order to keep this baseline, results on preprocessing time and visualiza-</p>
    <p>tion processing time should be gathered in one table for both systems. If they</p>
    <p>are equivalent for the different datasets, then we can conclude that the one that</p>
    <p>has the pre-processing is going to outperform the one that has to recompute</p>
    <p>each time. A Table with figures for each stage is expected at that level for at</p>
    <p>least ReVisE and the baseline, for each proposed dataset and for the same level.</p>
    <p>Additionaly, l. 403 (preprocessing times have also been measured) contradicts</p>
    <p>l. 245, and no figures are given.</p>
    <p>A: We have renamed Results to Results and Evaluation.</p>
    <p>We restyled section Discussion (subsection Comparison with other system),</p>
    <p>therefore there is comparison of animation frame rate for ParaView and Revise.</p>
    <p>We have added preprocessing time of Revise into the table.</p>
    <p>First of all the time of animation is compared. As one can see an animation</p>
    <p>in Revise has higher rate than in PraView’s one. Of course, one can add pre-</p>
    <p>processing time to Revise’s animation, but as one can see in this case it would</p>
    <p>be a little bit faster.</p>
    <p>Also it should be noticed that ParaView also requires preprocessing. We</p>
    <p>have to transform original dataset into pvti format in order to provide higher</p>
    <p>rate in ParaView. As it has mentioned in the article, it requires about 4 hours,</p>
    <p>which is a real preprocessing time in ParaView.</p>
    <p>It should be noticed that this comparison relates to the playing of time</p>
    <p>history. If one does pan/zoom/rotate in ParaView without changing time frame</p>
    <p>the volume rendering is performed very fast.</p>
    <p>3.9</p>
    <p>Q: In the Visualization performance (first occurrence l.428, as a second para-</p>
    <p>graph l.557 is named the same) the results are centered on the presented system;</p>
    <p>some tables presenting the datasets with the number of nodes at each level would</p>
    <p>be an asset (l.435-440). The approach is interesting.</p>
    <p>A: The table with visualization datasets parameters added. Also the second</p>
    <p>subsection with name Visualization performance is united with Future work.</p>
    <p>Q: For helping the reading l.465 to 475 should be put before l.457 to 464 in</p>
    <p>order to avoid a forward reference on l.456.</p>
    <p>A: Corresponding paragraphs are swapped.</p>
    <p>Q: In the Visualisation quality, there is the need to evaluate it properly:</p>
    <p>which task has to be solved? which metric is used to show the improvement on</p>
    <p>the quality and how do we compare this quality to a baseline (what would have</p>
    <p>been generated by Paraview)? Only experts of the field could answer about the</p>
    <p>quality of such visualization and respond to what is asserted in lines 487 to 490,</p>
    <p>as in fact to restrain to level 3 might not be as assumed in the introduction the</p>
    <p>level of quality the experts would need to have. This evaluation should be done</p>
    <p>by mixing those visualization with visualization generated with Paraview.</p>
    <p>A: As far as of the quality of volume rendering, let us notice first that the</p>
    <p>best possible level of detail is achieved as soon as the texel size of the 3D texture</p>
    <p>does not exceed the size of a pixel on the screen. Because of this, the effective</p>
    <p>texture resulution of 2048 that we have in the cascade dataset ensures the best</p>
    <p>possible level of detail on a typical FullHD dispay (1920 x 1080 pixels), unless</p>
    <p>the view is zoomed. Further, it might turn to be good enough to have texel size</p>
    <p>larger than the pixel size, if the gradient of the field being visualized is not too</p>
    <p>high. In general, the desired level of detail in the 3D texture depends on the</p>
    <p>problem at hand and, in particular, on mesh element size (which is respected</p>
    <p>by the ReVisE preprocessor). We would like to emphasize that in ReVisE user</p>
    <p>has full control on the level of detail in the visualization dataset, because the</p>
    <p>maximal number of block level is explicitly set at the preprocessing stage, which</p>
    <p>together with block depth determines the effective resolution of the 3D texture.</p>
    <p>Also there is the following observation about datasets from the CFD area:</p>
    <p>those datasets are often large due to mesh refinement at the boundary, which</p>
    <p>is necessary for the computation; however, it is typically normal to ignore those</p>
    <p>boundary layers during the visualization. Anyways, it is up to the user how to</p>
    <p>choose the desired level of detail by setting the mesh refinement parameter α,</p>
    <p>the boundary refinement parameter, and the limitation on the number of levels.</p>
    <p>3.10</p>
    <p>Q: In the paragraph Comparison with other systems, you mention l.504 ”Ac-</p>
    <p>cording to our experience” for the better interactivity: this is very subjective</p>
    <p>and need to be quantified. Particularly, I would have expected to see results on</p>
    <p>the number of fps rendered with Revise compared to other systems. Once again</p>
    <p>a well designed Table with different criteria for comparing the systems would</p>
    <p>be welcome.</p>
    <p>A: A comparison of frame rates between ParaView and ReVisE are added</p>
    <p>for hump model.</p>
    <p>3.11</p>
    <p>Q: I would put the second paragraph Visualization performance in Future work</p>
    <p>(more than Further work) as from line 564 it concerns only Future work.</p>
    <p>A: Visualization performance subsection is moved to Future work.</p>
    <p>3.12</p>
    <p>Q: A last question has to be raised with the final sentence: why would not</p>
    <p>have started by integrating the ReVisE in the Paraview? I think a different</p>
    <p>formulation can be found for this point.</p>
    <p>A: In the beginning of our work on the project, we tried a different approach</p>
    <p>to the visualization. One of our students was developing a plugin for Paraview.</p>
    <p>Lessons learned were that (a) it is quite difficult to follow all the rules of Par-</p>
    <p>aview plugin development, because of many things to be taken into account; (b)</p>
    <p>Paraview is a large system, and in order to extend it properly with plugins, one</p>
    <p>needs to learn Paraview source code; (c) Paraview visualization pipeline might</p>
    <p>have performance bottlenecks that are not known in advance, and struggling</p>
    <p>against those bottlenecks would take up considerable time. But what we really</p>
    <p>wanted was to concentrate on the proof-of-concept implementation of our ap-</p>
    <p>proach, rather than to learn the details of Paraview implementation. To achieve</p>
    <p>that goal, it was significantly simpler to start from scratch and have full control</p>
    <p>over literally any aspect of the system, than to try extending Paraview and over-</p>
    <p>come unexpected problems we are faced to. And only after our approach proves</p>
    <p>feasibility, it worth trying to build it into Paraview; in this case, the existing</p>
    <p>ReVisE implementation serves as a reference for performance estimation and</p>
    <p>helps to control that the Paraview implementation is being done properly.</p>
    <p>3.13</p>
    <p>Q: Additional remarks: (a lot)</p>
    <p>Direct style (for instance l.60-61), imperative forms (l.110, l.281 for instance)</p>
    <p>should be avoided in a scientific article.</p>
    <p>A: Fixed</p>
    <p>Statements should be accompanied by proovable arguments; particularly</p>
    <p>when coming to evaluation this is definitely needed. I would have expected the</p>
    <p>visualization performance to be compared to existing softwares, and particularly</p>
    <p>to Paraview as far as the number of nodes are comparable (maybe for instance</p>
    <p>with the hump dataset). It would have included in this case the preprocessing</p>
    <p>stage and the visualization stage, as it has been partially done in [12], show-</p>
    <p>ing the gain that we can expect from the new platform. Coming back to the</p>
    <p>assertion made on line 60-61 there is no proof (i.e. no comparison on an A/B</p>
    <p>testing for instance) of the visualization made. A baseline would be again to</p>
    <p>simplify the problem in a first instance to make two comparable visualization</p>
    <p>one obtained by Paraview for instance and one obtained by your method, and</p>
    <p>then scaling, and showing what happens. A video comparing the two could also</p>
    <p>be an asset.</p>
    <p>A: We added comparison of animation rate in ParaView and ReVisE. The</p>
    <p>preprocessing time is also taken into account. See Comparison with other sys-</p>
    <p>tems subsection.</p>
    <p>Q: A particular attention to articles (a and the) should be given, particularly</p>
    <p>when they are missing (for instance l.339: it uploads the image to the shared</p>
    <p>memory; l.343: the video streaming service checks the shared memory cotents).</p>
    <p>A: We tried to take it into account.</p>
    <p>Q: Bullets should be avoided in an article (it is not a presentation); a table</p>
    <p>is often a good replacement (l 405 to 437 and l.420 to 427)</p>
    <p>A: Fixed</p>
    <p>3.14</p>
    <p>Some remarks on Figures:</p>
    <p>Q: All the figures come at the end of the article and are not included in</p>
    <p>proper way in the article, which makes things difficult to follow.</p>
    <p>A: This is requirement of the journal to place all figures separately from the</p>
    <p>main text.</p>
    <p>Q:Figure 1: I would let the mesh behind in B (as it is done in C). Moreover,</p>
    <p>the frame used for the quadtree should be marked as such in A, just to be sure</p>
    <p>that all Figures from A to F are at the same dimension of representation (which</p>
    <p>does not seem to be the case).</p>
    <p>A: In all subfigures the scale is the same. The mesh behind the quadtree</p>
    <p>has been added. Notice that the bounding box of the quadtree is greater than</p>
    <p>that of the original mesh by some “padding” and is obtained by including points</p>
    <p>r + ns, where r is the position of a node of refined boundary mesh, n is the</p>
    <p>unit vector of outer normal vector to the boundary face, and s is the size of the</p>
    <p>refined boundary face. Since in subfigure E we use 10x boundary refinement,</p>
    <p>and in subfigure B we use no (1x) boundary refinement, the quadtree in B has</p>
    <p>10x larger padding than in E. It is correct and I am not sure that we need to</p>
    <p>clarify it in the text. The need for the padding yields from the need to have a</p>
    <p>field whose zero-valued isosurface approximates domain boundary.</p>
    <p>Q: Figure 2: I think the mesh chosen for illustrating a bit hard to follow</p>
    <p>and would let it by transparency under the different quadtrees generated.</p>
    <p>A: Done</p>
    <p>Q: Figure 3: I would add colors to help following the octree, or / and draw</p>
    <p>the graph tree corresponding.</p>
    <p>A: The Figure was removed due to restyling of Sparse 3D mipmapping</p>
    <p>section.</p>
    <p>Q: Figure 4: having the mesh by transparency under the quadtrees generated</p>
    <p>would help.</p>
    <p>A: The Figure was removed due to restyling of Sparse 3D mipmapping</p>
    <p>section.</p>
    <p>Q: Figure 5: It differs only from one node from Springer. A legend indicating</p>
    <p>nodes signification directly on the Figure it self would help (and not in the</p>
    <p>Legend below the Figure). The Legend below figure 5: the second B should be</p>
    <p>C.</p>
    <p>A: The Figure was removed due to restyling of Sparse 3D mipmapping</p>
    <p>section.</p>
    <p>Q: Figure 8: two inversions: Numerical solution (oi inverted) and Web</p>
    <p>server: visualization controller (rt inverted)</p>
    <p>A: Fixed</p>
    <p>Q: Figure 9: ReVisE web interface and not ReVesE in the legend</p>
    <p>A: Fixed</p>
    <p>Q: Suggestions of figures: A figure similar to Figure 3 of [12] would clarify</p>
    <p>lines 126-127.</p>
    <p>A: Now we reduced the section Sparse 3d mipmapping and it is more depen-</p>
    <p>dent on content of [12]. Therefore in order to not overload the article’s content</p>
    <p>it is intended that the reader will refer to [12] for more information.</p>
    <p>Q: A figure might help to clarify l.154 - 162.</p>
    <p>A: Added Fig 3. An example of quadtree and corresponding metadata.</p>
    <p>3.15</p>
    <p>Mispelling / other remarks:</p>
    <p>Q:l.403: preformance instead of performance</p>
    <p>A: Fixed.</p>
    <p>Q: l.345: what information does the sentence bring?</p>
    <p>A: Removed.</p>
    <supplementary-material id="pone.0255030.s001" position="float" content-type="local-data">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">response_to_reviewers.pdf</named-content></p>
      </caption>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.s001.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article article-type="aggregated-review-documents" id="pone.0255030.r003" specific-use="decision-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0255030.r003</article-id>
    <title-group>
      <article-title>Decision Letter 1</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Lisacek</surname>
          <given-names>Frederique</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2021 Frederique Lisacek</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Frederique Lisacek</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="doi" xlink:href="10.1371/journal.pone.0255030" id="rel-obj003" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">17 May 2021</named-content>
    </p>
    <p>PONE-D-20-33961R1</p>
    <p>ReVisE: Remote visualization environment for large numerical simulation datasets</p>
    <p>PLOS ONE</p>
    <p>Dear Dr. Orlov,</p>
    <p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
    <p>The improvement of this revised version is undeniable yet both reviewers pointed remaining minor weaknesses in the discussion regarding the comparison of ReVisE with other software, as well as in explaining interactivity. Please make sure these details are addressed as indicated by reviewers.</p>
    <p>Please submit your revised manuscript by July 1, 2021. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
    <p>Please include the following items when submitting your revised manuscript:</p>
    <p>
      <list list-type="bullet">
        <list-item>
          <p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p>
        </list-item>
        <list-item>
          <p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p>
        </list-item>
        <list-item>
          <p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p>
        </list-item>
      </list>
    </p>
    <p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p>
    <p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">http://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</ext-link>.</p>
    <p>We look forward to receiving your revised manuscript.</p>
    <p>Kind regards,</p>
    <p>Frederique Lisacek</p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
    <p>Journal Requirements:</p>
    <p>Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article’s retracted status in the References list and also include a citation and full reference for the retraction notice.</p>
    <p>[Note: HTML markup is below. Please do not edit.]</p>
    <p>Reviewers' comments:</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <bold>Comments to the Author</bold>
    </p>
    <p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the “Comments to the Author” section, enter your conflict of interest statement in the “Confidential to Editor” section, and submit your "Accept" recommendation.</p>
    <p>Reviewer #2: All comments have been addressed</p>
    <p>Reviewer #3: All comments have been addressed</p>
    <p>**********</p>
    <p>2. Is the manuscript technically sound, and do the data support the conclusions?</p>
    <p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p>
    <p>Reviewer #2: Yes</p>
    <p>Reviewer #3: Yes</p>
    <p>**********</p>
    <p>3. Has the statistical analysis been performed appropriately and rigorously? </p>
    <p>Reviewer #2: Yes</p>
    <p>Reviewer #3: N/A</p>
    <p>**********</p>
    <p>4. Have the authors made all data underlying the findings in their manuscript fully available?</p>
    <p>The <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.</p>
    <p>Reviewer #2: Yes</p>
    <p>Reviewer #3: Yes</p>
    <p>**********</p>
    <p>5. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
    <p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p>
    <p>Reviewer #2: Yes</p>
    <p>Reviewer #3: Yes</p>
    <p>**********</p>
    <p>6. Review Comments to the Author</p>
    <p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
    <p>Reviewer #2: I really enjoyed the reading and the improvement in quality since the first reading. From the remarks made during the first review, most of them have been lifted or answered. I still have one major remark and some additional minor remarks.</p>
    <p>Introducing the baseline comparison suggested during the first review improves the Evaluation part. Nonetheless, from your comments: "It should be noticed that this comparison relates to the playing of time history. If one does pan/zoom/rotate in ParaView without changing time frame the volume rendering is performed very fast.", I have a question: What does happen with Revise in the pan/zoom/rotate case? Do you have additional evaluation of this point compared to paraview, such that the reader can understand all the advantages and potential drawbacks, are generally there is not only one side of the coin.</p>
    <p>More anecdotically, preparing the dataset to be in an acceptable format for the system it is going to be read is required but I would not labelled it as a pre-processing stage as you need to do for ReVise.</p>
    <p>Additionally, I would have expected a figure similar to Figure 11 to compare the two renderings (the one achieved with paraview and the one achieved with Revise for the hump model.</p>
    <p>Additionally, I would put this evaluation with the corresponding dataset on a downlable virtual machine for both Paraview and Revise system in order they can be run out of the box for reviewers and readers.</p>
    <p>Some minor additional remarks:</p>
    <p>l. 15: you start by speaking of NVIDIA Index and Sight. NVIDIA Index is then presented in details in paragraph in l. 28 ... but no more mention of Sight.</p>
    <p>l.59-61: I would reformulate the answer to the question in a less assertive mode, because nothing is proven at this stage, by putting:</p>
    <p>In this paper, we are going to show that it is in fact not needed, otherwise the answer asks for explanation, which is mainly what you are going to present in the article.</p>
    <p>I know you have mentioned that it is an editor constrain but Numbered sections are definitely needed to refer to Sections and Subsections (the Editor mentioned I should put it in my review comments). Additionally, to refer to a Section or Subsection, the rules is similar than the one applied for figures, i.e. the section name should be prefixed by Section / Subsection.</p>
    <p>Avoid of ... of... in a sentence: it is often avoidable by reverting (3 in one sentence line 95 - 96 for instance, and also l.401, but certainly on other places)</p>
    <p>Presenting an additional idea goes with three em-dashes: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.uhv.edu/university-college/student-success-center/resources/a-d/dashes-use/#:~:text=There%20are%20actually%20three%20different,writing%20to%20an%20old%20friend" ext-link-type="uri">https://www.uhv.edu/university-college/student-success-center/resources/a-d/dashes-use/#:~:text=There%20are%20actually%20three%20different,writing%20to%20an%20old%20friend</ext-link>.</p>
    <p>This would be appropriate in l.111-112, l.116-117, l.312, l.418-419, l.497 for instance.</p>
    <p>Websites reference should never been put in a bibliography section but as bottom page notes, unless it is a requirement of the Editor (ref 30, 31, 1, 5, 6) as they are not proper bibliography references.</p>
    <p>Some bullet (or telegraphic) style paragraphs remain: it is never a good idea to put more than one semi-column in a sentence. Additionally, each sub-sentence should have a subject and verb in this -case. (l. 17-20, l.86-91, l.73-77, l.296-299)</p>
    <p>l. 158: roams instead of walks is maybe more appropriate</p>
    <p>l. 162 - l.164: on the one hand is needed in place of importantly. End of line 161: I would add: The element size s is bounded between two values s_min and s_max. On the one hand, the smallest value s_min ...</p>
    <p>l. 177: what is a "typical" visualization? As typical, depends mostly on the level of details you want to achieve.</p>
    <p>l. 233 ... maybe: The interested reader can refer to [] on the way of addressing such elements and for further details.</p>
    <p>[13] should be accessible on Arxiv, as it is intensively refered.</p>
    <p>l. 685: to distribute</p>
    <p>Articles should be checked, particularly when nouns are singular (here an article is often needed) as their non usage---named Zero-marking---is mostly dialectal english or reserved to particular cases.</p>
    <p>Reviewer #3: This is a good paper. It is written well and clearly. The system that the authors developed is well described and references to the code and data are provided. It is indeed a nice system that combines various modern technologies and an octree implementation for volume rendering. The authors were objective in their comparison to ParaView.</p>
    <p>Where the paper is weaker is in the comparison section. The authors correctly identify the performance bottleneck in ParaView as the IO time. They should not have stopped there and should have explained more thoroughly how ReVisE overcomes this issue.</p>
    <p>Furthermore, it would have been good to better compare interaction performance rather than mainly focus on animation. Similarly for IndeX and OSPRay comparisons. Although this is not necessary to accept this paper IMO.</p>
    <p>**********</p>
    <p>7. PLOS authors have the option to publish the peer review history of their article (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.</p>
    <p>Reviewer #2: No</p>
    <p>Reviewer #3: No</p>
    <p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
    <p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p>
  </body>
</sub-article>
<sub-article article-type="author-comment" id="pone.0255030.r004">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0255030.r004</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 1</article-title>
    </title-group>
    <related-article xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="doi" xlink:href="10.1371/journal.pone.0255030" id="rel-obj004" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>2</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">1 Jul 2021</named-content>
    </p>
    <p>Please find answers to reviewers' comments in the file "response_to_reviewers_2.pdf".</p>
    <supplementary-material id="pone.0255030.s002" position="float" content-type="local-data">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">response_to_reviewers_2.pdf</named-content></p>
      </caption>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.s002.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article article-type="editor-report" id="pone.0255030.r005" specific-use="decision-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0255030.r005</article-id>
    <title-group>
      <article-title>Decision Letter 2</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Lisacek</surname>
          <given-names>Frederique</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2021 Frederique Lisacek</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Frederique Lisacek</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="doi" xlink:href="10.1371/journal.pone.0255030" id="rel-obj005" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>2</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">5 Jul 2021</named-content>
    </p>
    <p>PONE-D-20-33961R2</p>
    <p>ReVisE: Remote visualization environment for large numerical simulation datasets</p>
    <p>PLOS ONE</p>
    <p>Dear Dr. Orlov,</p>
    <p>Thank you for submitting your revised manuscript to PLOS ONE. The only motivation for requesting a last minor revision is to answer your own questions regarding reviewer2's comments. To speed up the process I took the liberty of contacting directly this reviewer to get the needed explanations and here are his answers:</p>
    <p>1) The reference to "three em-dashes" applies to a Latex option. The reviewer is suggesting that each time you have side comments in brackets as for instance in this sentence (line 123-124 revised version):</p>
    <p>"Suppose there is an original mesh (unstructured or structured, consisting of tetrahedra or hexahedra — does not matter), and a scalar field specified at mesh nodes."</p>
    <p>you should use the "three em-dashes" option of Latex ( \\textemdash) and write:</p>
    <p>"Suppose there is an original mesh---unstructured or structured, consisting of tetrahedra or hexahedra, it does not matter---, and a scalar field specified at mesh nodes."</p>
    <p>It is ultimately your choice. This is only a suggestion.</p>
    <p>2) Regarding your questions about publishing rights regarding your Arxiv reference. The answer of the reviewer is as follows:</p>
    <p>"You should check the date of the end of the embargo period (generally 12 months), as you could have it on an institutional website; anyway normally you are authorized on a personal website (it should be on the agreement). You can refer to: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.springer.com/gp/open-access/publication-policies/self-archiving-policy" ext-link-type="uri">https://www.springer.com/gp/open-access/publication-policies/self-archiving-policy</ext-link>"</p>
    <p>Otherwise, your last revision matches all other requirements for publication so, please resubmit as soon as you can.</p>
    <p>Please submit your revised manuscript by July 8, 2021. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
    <p>Please include the following items when submitting your revised manuscript:</p>
    <p>
      <list list-type="bullet">
        <list-item>
          <p>A rebuttal letter that responds to these last two items after the explanations of reviewer2. You should upload this letter as a separate file labeled 'Response to Reviewers'.</p>
        </list-item>
        <list-item>
          <p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p>
        </list-item>
        <list-item>
          <p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p>
        </list-item>
      </list>
    </p>
    <p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p>
    <p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">http://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</ext-link>.</p>
    <p>We look forward to receiving your revised manuscript.</p>
    <p>Kind regards,</p>
    <p>Frederique Lisacek</p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
    <p>Journal Requirements:</p>
    <p>Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article’s retracted status in the References list and also include a citation and full reference for the retraction notice.</p>
    <p>[Note: HTML markup is below. Please do not edit.]</p>
    <p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
    <p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p>
  </body>
</sub-article>
<sub-article article-type="author-comment" id="pone.0255030.r006">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0255030.r006</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 2</article-title>
    </title-group>
    <related-article xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="doi" xlink:href="10.1371/journal.pone.0255030" id="rel-obj006" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>3</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">7 Jul 2021</named-content>
    </p>
    <p>Please find answers to reviewers in file response_to_reviewers_3.pdf</p>
    <supplementary-material id="pone.0255030.s003" position="float" content-type="local-data">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">response_to_reviewers_3.pdf</named-content></p>
      </caption>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0255030.s003.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article article-type="editor-report" id="pone.0255030.r007" specific-use="decision-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0255030.r007</article-id>
    <title-group>
      <article-title>Decision Letter 3</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Lisacek</surname>
          <given-names>Frederique</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2021 Frederique Lisacek</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Frederique Lisacek</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="doi" xlink:href="10.1371/journal.pone.0255030" id="rel-obj007" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>3</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">9 Jul 2021</named-content>
    </p>
    <p>ReVisE: Remote visualization environment for large numerical simulation datasets</p>
    <p>PONE-D-20-33961R3</p>
    <p>Dear Dr. Orlov,</p>
    <p>We’re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p>
    <p>Within one week, you’ll receive an e-mail detailing the required amendments. When these have been addressed, you’ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p>
    <p>An invoice for payment will follow shortly after the formal acceptance. To ensure an efficient process, please log into Editorial Manager at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www.editorialmanager.com/pone/" ext-link-type="uri">http://www.editorialmanager.com/pone/</ext-link>, click the 'Update My Information' link at the top of the page, and double check that your user information is up-to-date. If you have any billing related questions, please contact our Author Billing department directly at <email>authorbilling@plos.org</email>.</p>
    <p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they’ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <email>onepress@plos.org</email>.</p>
    <p>Kind regards,</p>
    <p>Frederique Lisacek</p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
    <p>Additional Editor Comments (optional):</p>
    <p>Reviewers' comments:</p>
  </body>
</sub-article>
<sub-article article-type="editor-report" id="pone.0255030.r008" specific-use="acceptance-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0255030.r008</article-id>
    <title-group>
      <article-title>Acceptance letter</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Lisacek</surname>
          <given-names>Frederique</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2021 Frederique Lisacek</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Frederique Lisacek</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="doi" xlink:href="10.1371/journal.pone.0255030" id="rel-obj008" related-article-type="reviewed-article"/>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">16 Jul 2021</named-content>
    </p>
    <p>PONE-D-20-33961R3 </p>
    <p>ReVisE: Remote visualization environment for large numerical simulation datasets </p>
    <p>Dear Dr. Orlov:</p>
    <p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now with our production department. </p>
    <p>If your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information please contact <email>onepress@plos.org</email>.</p>
    <p>If we can help with anything else, please email us at <email>plosone@plos.org</email>. </p>
    <p>Thank you for submitting your work to PLOS ONE and supporting open access. </p>
    <p>Kind regards, </p>
    <p>PLOS ONE Editorial Office Staff</p>
    <p>on behalf of</p>
    <p>Dr. Frederique Lisacek </p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
  </body>
</sub-article>
