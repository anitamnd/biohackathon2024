<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?covid-19-tdm?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Health Inf Sci Syst</journal-id>
    <journal-id journal-id-type="iso-abbrev">Health Inf Sci Syst</journal-id>
    <journal-title-group>
      <journal-title>Health Information Science and Systems</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2047-2501</issn>
    <publisher>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9884736</article-id>
    <article-id pub-id-type="publisher-id">209</article-id>
    <article-id pub-id-type="doi">10.1007/s13755-022-00209-4</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MCA-UNet: multi-scale cross co-attentional U-Net for automatic medical image segmentation</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Haonan</given-names>
        </name>
        <address>
          <email>haonan1wang@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Cao</surname>
          <given-names>Peng</given-names>
        </name>
        <address>
          <email>caopeng@mail.neu.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Yang</surname>
          <given-names>Jinzhu</given-names>
        </name>
        <address>
          <email>yangjinzhu@cse.neu.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zaiane</surname>
          <given-names>Osmar</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.412252.2</institution-id><institution-id institution-id-type="ISNI">0000 0004 0368 6968</institution-id><institution>Computer Science and Engineering, </institution><institution>Northeastern University, </institution></institution-wrap>Shenyang, China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.412252.2</institution-id><institution-id institution-id-type="ISNI">0000 0004 0368 6968</institution-id><institution>Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education, </institution><institution>Northeastern University, </institution></institution-wrap>Shenyang, China </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.17089.37</institution-id><institution-id institution-id-type="ISNI">0000 0001 2190 316X</institution-id><institution>Amii, </institution><institution>University of Alberta, </institution></institution-wrap>Edmonton, AB Canada </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>30</day>
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <volume>11</volume>
    <issue>1</issue>
    <elocation-id>10</elocation-id>
    <history>
      <date date-type="received">
        <day>17</day>
        <month>7</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>1</day>
        <month>10</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s), under exclusive licence to Springer Nature Switzerland AG 2023, Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.</copyright-statement>
      <license>
        <license-p>This article is made available via the PMC Open Access Subset for unrestricted research re-use and secondary analysis in any form or by any means with acknowledgement of the original source. These permissions are granted for the duration of the World Health Organization (WHO) declaration of COVID-19 as a global pandemic.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Medical image segmentation is a challenging task due to the high variation in shape, size and position of infections or lesions in medical images. It is necessary to construct multi-scale representations to capture image contents from different scales. However, it is still challenging for U-Net with a simple skip connection to model the global multi-scale context. To overcome it, we proposed a dense skip-connection with cross co-attention in U-Net to solve the semantic gaps for an accurate automatic medical image segmentation. We name our method MCA-UNet, which enjoys two benefits: (1) it has a strong ability to model the multi-scale features, and (2) it jointly explores the spatial and channel attentions. The experimental results on the COVID-19 and IDRiD datasets suggest that our MCA-UNet produces more precise segmentation performance for the consolidation, ground-glass opacity (GGO), microaneurysms (MA) and hard exudates (EX). The source code of this work will be released via <ext-link ext-link-type="uri" xlink:href="https://github.com/McGregorWwww/MCA-UNet/">https://github.com/McGregorWwww/MCA-UNet/</ext-link>.</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Medical image segmentation</kwd>
      <kwd>U-Net</kwd>
      <kwd>Attention</kwd>
      <kwd>Multi-scale feature fusion</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© Springer Nature Switzerland AG 2023</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">Medical image segmentation [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR5">5</xref>] of target objects provides valuable information for the analysis of pathologies. However, the high variation in shape, size and position of infections or lesions is one of the key challenges in medical image segmentation. As observed in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, the size and shape with irregular and blurred appearances in CT between consolidation and ground-glass opacity (GGO) lesions vary significantly. The microaneurysms and hard exudates in fundus photography are tiny/small and dispersedly distributed, which easily results in the false-negative detection.<fig id="Fig1"><label>Fig. 1</label><caption><p>The major challenges in medical segmentation: the various shape and size of COVID-19 lesions in CT images (<bold>a</bold>, <bold>b</bold>), and small lesion scattered in fundus images (<bold>c</bold>, <bold>d</bold>). The right subfigures are the segmentation results of the left original images. ‘MA’ denotes microaneurysms and ‘EX’ denotes hard exudates</p></caption><graphic xlink:href="13755_2022_209_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par3">Recently, deep learning has shown its strong power of feature learning in image segmentation area. For medical image segmentation, U-Net-like encoder-decoder architectures have shown their power in medical image segmentation applications [<xref ref-type="bibr" rid="CR6">6</xref>]. Although U-shaped networks have achieved good performances in many medical image segmentation applications [<xref ref-type="bibr" rid="CR7">7</xref>–<xref ref-type="bibr" rid="CR9">9</xref>], they still have several key limitations. (1) Insufficient capability of extracting context information for reconstructing the fine-grained segmentation map. The global context information is generally captured by deeper layers of the encoder and is gradually transmitted to shallower layers, which may be progressively diluted. (2) Although skip connection can help recover the spatial information which gets lost through the pooling layers, it is unnecessarily restrictive due to demanding the feature maps fusion of the encoder and decoder of the same level without considering the semantic gap [<xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR11">11</xref>]. Therefore, it raises an important question to the U-Net methods: can we solve the limitation and develop a new framework that can improve over the restrictive skip connections in U-Net that requires fusion of only same-scale feature maps with simply concatenating?</p>
    <p id="Par4">To this end, we propose a U-shaped architecture with a more flexible multi-scale cross co-attention skip connection enabling flexible feature fusion in decoders for automatic segmentation. With the proposed dense connectivity, each node in a decoder is connected with the aggregation of all feature maps from the encoder by relaxing the unnecessarily restrictive skip connections where only the feature maps with the same scale are connected. It is different from UNet++ which fuses only the encoder features from the deeper layers without considering the fusion of the shallower layers (please refer to Fig. <xref rid="Fig2" ref-type="fig">2</xref>b). On the other hand, we design an attention mechanism [<xref ref-type="bibr" rid="CR12">12</xref>, <xref ref-type="bibr" rid="CR13">13</xref>] from both the perspectives of channel-wise and spatial-wise to reduce the semantic gap between the encoder and decoder, termed co-attention mechanism. The co-attention mechanism can not only eliminate the semantic gap in feature fusion but also highlight salient features that are passed through the skip connections. Due to the reuse of feature maps, no extra computations and parameters are required, compared with UNet++ and MultiResUNet which solve the semantic gap by combining a series of convolution blocks. To facilitate the learning of the multi-scale feature fusion with cross co-attention connections, we employ deep supervision to facilitate the feature learning in different stages of the decoder. Our experimental results indicate that the deep supervision mechanism is effective in improving the segmentation performances of U-shaped networks, especially in the cases that the target objects have multiple scales. The performance of deep supervision highly depends on appropriate corresponding task weights. Therefore, we regard it as a multi-task learning task and make the weights learnable through a balanced multi-task dynamic weight (BMTD) optimization algorithm. The contribution of this work are three-folds:<fig id="Fig2"><label>Fig. 2</label><caption><p>The comparison of the different skip connection settings</p></caption><graphic xlink:href="13755_2022_209_Fig2_HTML" id="MO2"/></fig></p>
    <p id="Par5">
      <list list-type="bullet">
        <list-item>
          <p id="Par6">We dissect the skip connections in U-Net and empirically demonstrate appropriate connections are important for segmentation. We propose a multi-scale cross skip connection to boost semantic segmentation by bridging the semantic gaps between low-level and high-level features by an effective feature fusion scheme. Compared with the plain skip connections, the multi-scale cross skip connection improve the receptive field of U-Net by jointly considering the multi-scale features and hence able to extract multi-scale features of the target object and incorporate larger context.</p>
        </list-item>
        <list-item>
          <p id="Par7">While encoders have been studied rigorously, relatively few studies focus on the decoder side. The proposed bi-decoder module differs from the original decoder in three ways: (1) cross co-attention, which bridges the semantic gap between encoder and decoder feature maps by highlighting regions that present a significant interest for the diseases. (2) dual upsampling, which improves the upsampling performance by exploiting the finer spatial recovery in the decoder. (3) deep supervision, which further facilitates the multi-scale features fusion with a direct supervision for each level. Based on a U-shape network, the proposed decoder module can be easily embedded in the frameworks in the medical image segmentation tasks.</p>
        </list-item>
        <list-item>
          <p id="Par8">The proposed MCA-UNet is evaluated on four lesion segmentation tasks of two different datasets with difficulties including large variations of shape/size, blurred boundaries and small lesions, and it is shown that it achieves better performance than the related UNet-based architectures.</p>
        </list-item>
      </list>
    </p>
  </sec>
  <sec id="Sec2">
    <title>Related works</title>
    <p id="Par9">Recently, deep learning has shown their strong power of feature learning in image segmentation applications, for example, brain lesion segmentation [<xref ref-type="bibr" rid="CR14">14</xref>], organ segmentation [<xref ref-type="bibr" rid="CR15">15</xref>], electron microscopy image segmentation [<xref ref-type="bibr" rid="CR6">6</xref>]. U-shaped architectures offer the advantages in medical image segmentation applications [<xref ref-type="bibr" rid="CR6">6</xref>]. However, they still have some limitations such as lack of ability of modeling multi-scale global context and the semantic gap between the encoder and the decoder. To solve the issue, some methods with different skip connections for more flexible feature fusion are proposed, as illustrated in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. Zhou et al. [<xref ref-type="bibr" rid="CR10">10</xref>] propose a nested U-shaped framework, UNet++, with nested dense skip pathways which replace the restrictive skip connections fusing only the same-scale feature maps in U-Net. Ibtehaz et al. [<xref ref-type="bibr" rid="CR11">11</xref>] propose MultiResUNet to incorporate some residual convolutional layers along the skip connections. The study hypothesizes that the features propagating from the encoder stage may balance the possible semantic gaps. Attention-UNet [<xref ref-type="bibr" rid="CR13">13</xref>] is proposed to reduce the semantic gap between the encoder and decoder by a spatial attention mechanism. The advantage of the methods is that they improve the segmentation performance by alleviating the semantic gap and incorporating extra convolution layers or attention mechanism. Despite achieving good performance, the works above are still incapable of effectively exploring sufficient information from full scales due to the designs of the skip connections which ignore the correlation of multiple scale encoder features.</p>
  </sec>
  <sec id="Sec3">
    <title>Methods</title>
    <sec id="Sec4">
      <title>The overall framework of MCA-UNet</title>
      <p id="Par10">
        <fig id="Fig3">
          <label>Fig. 3</label>
          <caption>
            <p>The architecture of the proposed MCA-UNet</p>
          </caption>
          <graphic xlink:href="13755_2022_209_Fig3_HTML" id="MO3"/>
        </fig>
      </p>
      <p id="Par11">Our network consists of three parts: the encoder, the Multi-scale Cross Skip Connection and the Bidirectional Decoder (Bi-decoder) which consists of Dual Upsampling, Cross Co-Attention (CCA) and Deep Supervision. We also employ a BMTD algorithm to optimize the multi-task loss from the deep supervised decoder layers. Figure <xref rid="Fig3" ref-type="fig">3</xref> illustrates the architecture of our proposed MCA-UNet network. To improve the representation capacity of the segmentation network, we replace the original two-layer convolution block with a Residual Block [<xref ref-type="bibr" rid="CR16">16</xref>]. To better fuse features of inconsistent semantics and scales, we propose a cross co-attention guided multi-scale fusion scheme, which addresses the issues that arise when fusing features given at different scales. To effectively fuse the multi-scale features from different encoder levels to produce the final segmentation mask, we proposed a bi-decoder module which is directly enhanced by multi-scale context extracted from the contracting path. The bi-decoder module also involves a dual upsampling process that improves the upsampling performance and a deep supervision scheme to facilitate back-propagation and convergence. We provide details for each step in the following sections.</p>
    </sec>
    <sec id="Sec5">
      <title>Encoder</title>
      <p id="Par12">The encoder of the original U-Net consists of four doubled convolution layers with an activation function, which is insufficient for feature extraction and representation. Thus, we replace each convolution with a Residual Block [<xref ref-type="bibr" rid="CR16">16</xref>] which has been proven to be useful for increasing the ability of learning richer representations and mitigating the degradation problem. The details can be seen in Table. <xref rid="Tab1" ref-type="table">1</xref>.</p>
    </sec>
    <sec id="Sec6">
      <title>Multi-scale cross skip connection</title>
      <p id="Par13">Skip connection was first proposed in U-Net, which transmits the low-level information (textures, shapes, etc.) in the shallower encoder stages to the corresponding stages of the decoder. However, each stage of the decoder can only get feature from one scale through the original skip connection, which may harm the decoder features due to the semantic gaps and lacks the ability of capturing multi-scale context information which has been proven essential for lesion segmentation tasks [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR18">18</xref>]. To solve these problems, we replace the original skip connection scheme with a multi-scale cross skip connection scheme. The proposed scheme transmits the resized (using up-samples or max-pooling) features from all the four encoder stages to each decoder stage, then combines them with a Bi-decoder block which will be introduced through the next section. The cross skip paths between the encoder and the decoder can aggregate features generated by multiple scales thus leads to better segmentation prediction.</p>
    </sec>
    <sec id="Sec7">
      <title>Bidirectional decoder, Bi-decoder</title>
      <p id="Par14">The bi-decoder block is designed as a gating operation of the skip connection based on a learned attention map given to multiple feature maps from encoder. Unlike the traditional decoder, the proposed decoder has two inputs and two outputs. Each decoder block is connected with all encoder blocks via attentional skip connections as in the U-Net architecture. The inputs of bi-decoder involves two parts: multi-scale features from the encoder, and a complementary dual upsampled information from the deeper layers. The bi-decoder processes the two inputs with two directions of horizontal and vertical paths, and then learns a more powerful representation and finer recovery by dealing with the feature learning in both directions. With the different scales inputs, the decoder further encode the feature maps as the inputs for extracting global contexts with attention mechanism to enhance finer details by recovering localized spatial information. The outputs are dual upsampled information to the shallower layers and the direct segmentation prediction with another upsampling to the original resolution.</p>
      <p id="Par15">In summary, we introduce three enhancements to the conventional decoder module in our proposed bi-decoder: (1) Directly concatenating the feature maps from the encoder may cause redundancy, hence we proposed a co-correlation with channel- and spatial-wise attention module to guide the channel and spatial information filtration of the encoder feature maps through skip connections, allowing a fine spatial recovery in the decoder. (2) Both deconvolution and upsampling were added in the splicing process of the high-resolution features in the contraction path to leverage the complementarity between two different upsampling operations. (3) Finally, the incorporation of deep supervision can further facilitate the multi-scale features fusion.</p>
      <sec id="Sec8">
        <title>Dual upsampling</title>
        <p id="Par16">The bi-decoder contains two upsampling components of nearest neighbor upsampling and deconvolution to recover resolution from the previous layers. We argue that the two processions are totally different from each other in terms of operation mode and can be complementary for the following cross correlation. Among the existing algorithms, the upsampling or deconvolution algorithm is seperately used in the decoder. In our work, the upsampling and deconvolution comprises a dual-path decoder. The combination of the upsampling and deconvolution can enhance the performance of the cross co-attention when the multi-feature encoder and decoder are fused.</p>
      </sec>
      <sec id="Sec9">
        <title>Cross co-attention (CCA)</title>
        <p id="Par17">Attention Mechanism for medical image segmentation have also been used recently [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR20">20</xref>], showing great potential in improving the segmentation performance. In our work, we hypothesize that the information from multi-scale encoder blocks are different. We are focusing on the cross correlation between the feature maps from encoder and decoder rather than a self-attention within a single feature map. Hence, to better fuse features of inconsistent semantics and scales, we propose a multi-scale channel-wise and spatial-wise attention module. The proposed module is incorporated into the bi-decoder to guide the channel and spatial information filtration of the encoder features through skip connections and eliminate the ambiguity with the decoder features as signals.<fig id="Fig4"><label>Fig. 4</label><caption><p>The structure of cross co-attention (CCA) module. The process of attention is similar to the self attention. The cross co-attention module takes three inputs, an upsampled tensor <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\hat{X}_U} \in \mathbb {R}^{H \times W \times \hat{C}}$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover><mml:mi mathvariant="bold-italic">U</mml:mi></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi><mml:mo>×</mml:mo><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq1.gif"/></alternatives></inline-formula> as well as a deconvoluted tensor <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\hat{X}_D} \in \mathbb {R}^{H \times W \times \hat{C}}$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover><mml:mi mathvariant="bold-italic">D</mml:mi></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi><mml:mo>×</mml:mo><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq2.gif"/></alternatives></inline-formula> as queries, and an encoder tensor <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{X^\ell } \in \mathbb {R}^{H \times W \times C}$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:msup></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi><mml:mo>×</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq3.gif"/></alternatives></inline-formula> as a key and a value at the same time</p></caption><graphic xlink:href="13755_2022_209_Fig4_HTML" id="MO4"/></fig></p>
        <p id="Par18">Specifically, instead of simply aggregating features from all levels, we propose to learn the attention in four parallel different level features. Unlike the previously proposed attention modules, most of which only explore channel- or spatial-wise attention, the proposed multi-scale cross co-attention module applies attention mechanism of channel- and spatial-wise for high-level and low-level features to exploit the complementary space and channel simultaneously. With the cross co-attention, the decoder can learn the importance of each feature channels which come from multi-level feature maps, and emphasize a meaningful feature selection in the spatial map to locate the critical structures.</p>
        <p id="Par19">Motivated by Squeeze-and-Excitation (SE) block, we extend the self attention mechanism to a cross co-attention in the multi-scale feature fusion to model the interactions of encoder-decoder with different scales for better feature representations. We introduce a cross co-attention module and the process is shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. It involves channel and spatial attention branches. As illustrated in Fig. <xref rid="Fig4" ref-type="fig">4</xref>, the two branches are conducted simultaneously rather than sequentially, thus better feature representations for pixel-level prediction are obtained. It takes the concatenated results of two up-sampled features <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\hat{X}_U}$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover><mml:mi mathvariant="bold-italic">U</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq4.gif"/></alternatives></inline-formula> and <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{ \hat{X}_D}$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover><mml:mi mathvariant="bold-italic">D</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq5.gif"/></alternatives></inline-formula> as query feature <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\hat{X}}$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq6.gif"/></alternatives></inline-formula>, and the encoder features from different scales as key features <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{ X^\ell }$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq7.gif"/></alternatives></inline-formula>, <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ell \in 1,2,3,4$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>∈</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq8.gif"/></alternatives></inline-formula> indicates the level of encoder which the feature is skip-connected from. For the <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ell$$\end{document}</tex-math><mml:math id="M18"><mml:mi>ℓ</mml:mi></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq9.gif"/></alternatives></inline-formula>th level encoder, each pair of feature maps (<inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{ X^\ell }$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq10.gif"/></alternatives></inline-formula>, <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{ \hat{X}}$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq11.gif"/></alternatives></inline-formula> ) are fed into the CCA module.</p>
        <p id="Par20">Mathematically, we consider the encoder feature maps <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{X^\ell }=[\varvec{x}_1^\ell ,\varvec{x}_2^\ell ,\ldots ,\varvec{x}_C^\ell ]$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mn>2</mml:mn><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mi>C</mml:mi><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq12.gif"/></alternatives></inline-formula> and decoder feature maps <inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\hat{X}}=[\varvec{\hat{x}}_1,\varvec{\hat{x}}_2,\ldots ,\varvec{\hat{x}}_C]$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>C</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq13.gif"/></alternatives></inline-formula> as combinations of channels <inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{x}_k\in \mathbb {R}^{H\times W}$$\end{document}</tex-math><mml:math id="M28"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq14.gif"/></alternatives></inline-formula> and <inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\hat{x}}_k\in \mathbb {R}^{H\times W}$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq15.gif"/></alternatives></inline-formula> , where <italic>W</italic> , <italic>H</italic> and <italic>C</italic> indicate width, height and channel dimension, respectively. Let <inline-formula id="IEq16"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\tilde{P}^\ell }\in \mathbb {R}^{C\times 1 \times 1}$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover><mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:msup></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq16.gif"/></alternatives></inline-formula> and <inline-formula id="IEq17"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\tilde{Q}^\ell }\in \mathbb {R}^{1\times H\times W}$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover><mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:msup></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq17.gif"/></alternatives></inline-formula> are the channel and spatial attention mask. A global average pooling layer <inline-formula id="IEq18"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g(\varvec{x}_k)=\frac{1}{H\times W} \sum _{i=1}^H\sum _{j=1}^W \varvec{x}_k(i,j)$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:msubsup><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq18.gif"/></alternatives></inline-formula> is used for Spatial squeezing. This operation embeds the global spatial information in vector <inline-formula id="IEq19"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{P}^\ell$$\end{document}</tex-math><mml:math id="M38"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq19.gif"/></alternatives></inline-formula>. This vector is transformed by<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \varvec{P}^\ell =\varvec{L}_1\cdot \delta (\varvec{L}_1\cdot g (\varvec{x}))+\varvec{L}_2\cdot \delta (\varvec{L}_3\cdot g (\varvec{\hat{x}})) \end{aligned}$$\end{document}</tex-math><mml:math id="M40" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>·</mml:mo><mml:mi>δ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>·</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>·</mml:mo><mml:mi>δ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msub><mml:mo>·</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="13755_2022_209_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq20"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{L}_1\in \mathbb {R}^{\frac{C_{\hat{x}}}{2} \times C_x}$$\end{document}</tex-math><mml:math id="M42"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:msub><mml:mi>C</mml:mi><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:msub><mml:mn>2</mml:mn></mml:mfrac><mml:mo>×</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq20.gif"/></alternatives></inline-formula>, <inline-formula id="IEq21"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{L}_{2}\in \mathbb {R}^{C_x\times \frac{C_{\hat{x}}}{2}}$$\end{document}</tex-math><mml:math id="M44"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mfrac><mml:msub><mml:mi>C</mml:mi><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:msub><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq21.gif"/></alternatives></inline-formula> and <inline-formula id="IEq22"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{L}_{3}\in \mathbb {R}^{C_{\hat{x}}\times \frac{C_{\hat{x}}}{2}}$$\end{document}</tex-math><mml:math id="M46"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:msub><mml:mo>×</mml:mo><mml:mfrac><mml:msub><mml:mi>C</mml:mi><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:msub><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq22.gif"/></alternatives></inline-formula> being weights of three Linear layers and the ReLU operator <inline-formula id="IEq23"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\delta (\cdot )$$\end{document}</tex-math><mml:math id="M48"><mml:mrow><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq23.gif"/></alternatives></inline-formula>.</p>
        <p id="Par21">This operation in Eq. (2) encodes the channel-wise dependencies. The resultant vector is used to recalibrate or excite <inline-formula id="IEq24"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{X^\ell }$$\end{document}</tex-math><mml:math id="M50"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq24.gif"/></alternatives></inline-formula> as follow:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \begin{aligned} \varvec{\tilde{P}^\ell }&amp;=\mathcal {F}_{catt}(\varvec{X^\ell })\\&amp;=[\sigma (P_{1}^\ell )\varvec{x}_1^\ell ,\sigma (P_{2}^\ell )\varvec{x}_2^\ell ,\ldots ,\sigma (P_{C}^\ell )\varvec{x}_C^\ell ] \end{aligned} \end{aligned}$$\end{document}</tex-math><mml:math id="M52" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover><mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:msup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mrow><mml:mi mathvariant="italic">catt</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mn>2</mml:mn><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mi>C</mml:mi><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="13755_2022_209_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where the activation <inline-formula id="IEq25"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma ({P}_i^\ell )$$\end{document}</tex-math><mml:math id="M54"><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mi>i</mml:mi><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq25.gif"/></alternatives></inline-formula> indicates the importance of the <italic>i</italic>th channel, which are rescaled.</p>
        <p id="Par22">The process of modeling the spatial relationship is similar to the channel attention. We consider it as an alternative slicing of the input feature maps <inline-formula id="IEq26"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{X}^\ell =[\varvec{x}_{1,1}^\ell ,\varvec{x}_{1,2}^\ell ,\ldots ,\varvec{x}_{i,j}^\ell ,\ldots ,\varvec{x}_{H,W}^\ell ]$$\end{document}</tex-math><mml:math id="M56"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq26.gif"/></alternatives></inline-formula> and <inline-formula id="IEq27"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\hat{X}}=[\varvec{\hat{x}}_{1,1},\varvec{\hat{x}}_{1,2},\ldots ,\varvec{\hat{x}}_{i,j},\ldots ,\varvec{\hat{x}}_{H,W}]$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq27.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq28"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{x}_{i,j}^\ell$$\end{document}</tex-math><mml:math id="M60"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq28.gif"/></alternatives></inline-formula> and <inline-formula id="IEq29"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\hat{x}}_{i,j}\in \mathbb {R}^{1\times 1 \times C}$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq29.gif"/></alternatives></inline-formula> correspond to the spatial location (<italic>i</italic>, <italic>j</italic>) with <inline-formula id="IEq30"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i\in {1, 2, \ldots , H}$$\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq30.gif"/></alternatives></inline-formula> and <inline-formula id="IEq31"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$j \in {1, 2, \ldots , W }$$\end{document}</tex-math><mml:math id="M66"><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq31.gif"/></alternatives></inline-formula>. The spatial squeeze operation is achieved through a convolution<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \varvec{Q}^\ell =\varvec{W}_1 \cdot \delta (\varvec{W}_2 \varvec{X}^\ell +\varvec{W}_3\hat{\varvec{X}}) \end{aligned}$$\end{document}</tex-math><mml:math id="M68" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>·</mml:mo><mml:mi>δ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msub><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="13755_2022_209_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq32"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{W}_{1}\in \mathbb {R}^{1\times 1 \times C_{\hat{x}}\times 1}$$\end{document}</tex-math><mml:math id="M70"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:msub><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq32.gif"/></alternatives></inline-formula> is the weight of spatial squeeze convolution layer, <inline-formula id="IEq33"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{W}_2\in \mathbb {R}^{C_x\times C_{\hat{x}}}$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq33.gif"/></alternatives></inline-formula> and <inline-formula id="IEq34"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{W}_{3}\in \mathbb {R}^{C_{\hat{x}}\times C_{\hat{x}}}$$\end{document}</tex-math><mml:math id="M74"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq34.gif"/></alternatives></inline-formula> reduce the feature channels of <inline-formula id="IEq35"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{X}^\ell$$\end{document}</tex-math><mml:math id="M76"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq35.gif"/></alternatives></inline-formula> and <inline-formula id="IEq36"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\hat{X}}$$\end{document}</tex-math><mml:math id="M78"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq36.gif"/></alternatives></inline-formula> to the same number <inline-formula id="IEq37"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${C_{\hat{x}}}$$\end{document}</tex-math><mml:math id="M80"><mml:msub><mml:mi>C</mml:mi><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq37.gif"/></alternatives></inline-formula>. Each <inline-formula id="IEq38"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Q_{i,j}^\ell$$\end{document}</tex-math><mml:math id="M82"><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq38.gif"/></alternatives></inline-formula> of the projection represents the linearly combined representation for all channels <italic>C</italic> for a spatial location (<italic>i</italic>, <italic>j</italic>). This projection is passed through a sigmoid layer <inline-formula id="IEq39"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma (.)$$\end{document}</tex-math><mml:math id="M84"><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq39.gif"/></alternatives></inline-formula> to rescale activations to [0, 1].<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \begin{aligned} \varvec{\tilde{Q}^\ell }&amp;=\mathcal {F}_{satt}(\varvec{X}^\ell )\\&amp;=[\sigma (Q_{1,1}^\ell )\varvec{x}_{1,1}^\ell ,\sigma (Q_{1,2}^\ell )\varvec{x}_{1,2}^\ell ,\ldots ,\sigma (Q_{i,j}^\ell )\varvec{x}_{i,j}^\ell ,\\&amp;\cdots , \sigma (Q_{H,W}^\ell )\varvec{x}_{H,W}^\ell ] \end{aligned} \end{aligned}$$\end{document}</tex-math><mml:math id="M86" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover><mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:msup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mrow><mml:mi mathvariant="italic">satt</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="13755_2022_209_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where each value <inline-formula id="IEq40"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma (Q_{i,j}^\ell )$$\end{document}</tex-math><mml:math id="M88"><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq40.gif"/></alternatives></inline-formula> corresponds to the relative importance of a spatial information (<italic>i</italic>, <italic>j</italic>) of a given feature map.</p>
        <p id="Par23">After computing the relevance between decoder and encoder during the fusion with the channel and spatial attention, next, we perform a tensor multiplication between the two attention tensor and the original encoder features. Third, we use an element-wise sum operation between the above tensor and original features to obtain the final representations reflecting effective fusion with skip connections for better segmentation. At last, we aggregate the features from these two attention modules, a cleaned up version is indicated as <inline-formula id="IEq41"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\tilde{X}_{cs}^{\ell }}=\varvec{\tilde{P}^{\ell }}\otimes \varvec{X}^\ell +\varvec{\tilde{Q}^{\ell }}\otimes \varvec{X}^\ell$$\end{document}</tex-math><mml:math id="M90"><mml:mrow><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="bold-italic">cs</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover><mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:msup></mml:mrow><mml:mo>⊗</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover><mml:mi mathvariant="bold-italic">ℓ</mml:mi></mml:msup></mml:mrow><mml:mo>⊗</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq41.gif"/></alternatives></inline-formula>, which is the element-wise addition of the channel and spatial excited features, where <inline-formula id="IEq42"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\otimes$$\end{document}</tex-math><mml:math id="M92"><mml:mo>⊗</mml:mo></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq42.gif"/></alternatives></inline-formula> is the element-wise multiplication. The final output feature is expressed by concatenating all the features: <inline-formula id="IEq43"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\tilde{X}_{out}}=\varvec{Concat}\left[ \varvec{\tilde{X}_{cs}}^{1},\varvec{\tilde{X}_{cs}}^{2},\varvec{\tilde{X}_{cs}}^{3},\varvec{\tilde{X}_{cs}}^{4},\varvec{\hat{X}_U},\varvec{\hat{X}_{D}}\right]$$\end{document}</tex-math><mml:math id="M94"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="bold-italic">out</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">Concat</mml:mi></mml:mrow><mml:mfenced close="]" open="["><mml:msup><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="bold-italic">cs</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="bold-italic">cs</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="bold-italic">cs</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>3</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo mathvariant="bold" stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="bold-italic">cs</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>4</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover><mml:mi mathvariant="bold-italic">U</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover><mml:mi mathvariant="bold-italic">D</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq43.gif"/></alternatives></inline-formula>.</p>
      </sec>
      <sec id="Sec10">
        <title>Deep supervision</title>
        <p id="Par24">To improve the back-propagation and make the decoder more stable, we introduce deep supervision [<xref ref-type="bibr" rid="CR21">21</xref>] to the four stages of the decoder. Deep supervision is capable of guiding the feature learning of the hidden layers directly under the supervision of the loss and labels. We up-sample the features from the first three hidden stages to the size of the last prediction stage and add three more losses to supervise them. The final output of the decoder is then re-scaled to the original input size. The re-scaled output is further fed into a softmax layer to produce the class probability distribution. Note that the deep supervision does not work in the inference stage, we only use the last layer of decoder <bold><italic>Side Output 1</italic></bold> for producing the segmentation prediction.</p>
      </sec>
    </sec>
    <sec id="Sec11">
      <title>Training and inference</title>
      <p id="Par25">For the main idea of enhancing the decoder of U-Net, we add horizontal deep supervision in the four decoder levels. We choose deconvolution with kernel size 2 × 2, 4 × 4 and 8 × 8 to resize the output of every layer in decoder to meet the size of the ground truth. Then we compute the losses of those four layers, and use back propagation to update the weights of them, so we can deploy a direct guidance to the decoder and further improve the accuracy of the reconstruction operation.<table-wrap id="Tab1"><label>Table 1</label><caption><p>The architecture of our segmentation network</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Stage encoder</th><th align="left">Input</th><th align="left">Kernel</th><th align="left">Output</th><th align="left">Stage</th><th align="left">Input</th><th align="left">Kernel</th><th align="left">Output</th></tr><tr><th align="left" colspan="4">Encoder</th><th align="left" colspan="4">Decoder</th></tr></thead><tbody><tr><td align="left">Image</td><td align="left">–</td><td align="left">–</td><td align="left">640<sup>2</sup> × 3</td><td align="left"><inline-formula id="IEq48"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm {D_4}$$\end{document}</tex-math><mml:math id="M96"><mml:msub><mml:mi mathvariant="normal">D</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq48.gif"/></alternatives></inline-formula></td><td align="left">40<sup>2</sup> × 376</td><td align="left">ResBlock × 3 + DU</td><td align="left">80<sup>2</sup> × 128</td></tr><tr><td align="left"><inline-formula id="IEq52"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm {E_1}$$\end{document}</tex-math><mml:math id="M98"><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq52.gif"/></alternatives></inline-formula></td><td align="left">640<sup>2</sup> × 3</td><td align="left">ResBlock[<xref ref-type="bibr" rid="CR16">16</xref>] × 2</td><td align="left">640<sup>2</sup> × 24</td><td align="left"><inline-formula id="IEq56"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm {D_3}$$\end{document}</tex-math><mml:math id="M100"><mml:msub><mml:mi mathvariant="normal">D</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq56.gif"/></alternatives></inline-formula></td><td align="left">80<sup>2</sup> × 248</td><td align="left">ResBlock × 2 + DU</td><td align="left">160<sup>2</sup> × 64</td></tr><tr><td align="left"><inline-formula id="IEq60"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm {E_2}$$\end{document}</tex-math><mml:math id="M102"><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq60.gif"/></alternatives></inline-formula></td><td align="left">640<sup>2</sup> × 24</td><td align="left">ResBlock × 2 + MP</td><td align="left">320<sup>2</sup> × 32</td><td align="left"><inline-formula id="IEq64"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm {D_2}$$\end{document}</tex-math><mml:math id="M104"><mml:msub><mml:mi mathvariant="normal">D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq64.gif"/></alternatives></inline-formula></td><td align="left">160<sup>2</sup> × 184</td><td align="left">ResBlock × 2 + DU</td><td align="left">320<sup>2</sup> × 32</td></tr><tr><td align="left"><inline-formula id="IEq68"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm {E_3}$$\end{document}</tex-math><mml:math id="M106"><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq68.gif"/></alternatives></inline-formula></td><td align="left">320<sup>2</sup> × 32</td><td align="left">ResBlock × 2 + MP</td><td align="left">160<sup>2</sup> × 64</td><td align="left"><inline-formula id="IEq72"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm {D_1}$$\end{document}</tex-math><mml:math id="M108"><mml:msub><mml:mi mathvariant="normal">D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq72.gif"/></alternatives></inline-formula></td><td align="left">320<sup>2</sup> × 168</td><td align="left">ResBlock × 2 + DU</td><td align="left">640<sup>2</sup> × 24</td></tr><tr><td align="left"><inline-formula id="IEq76"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm {E_4}$$\end{document}</tex-math><mml:math id="M110"><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq76.gif"/></alternatives></inline-formula></td><td align="left">160<sup>2</sup> × 64</td><td align="left">ResBlock × 3 + MP</td><td align="left">80<sup>2</sup> × 128</td><td align="left">SO<sub>1</sub></td><td align="left">640<sup>2</sup> × 24</td><td align="left">Conv,1 × 1</td><td align="left">640<sup>2</sup> × 1</td></tr><tr><td align="left"><inline-formula id="IEq84"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm {E_5}$$\end{document}</tex-math><mml:math id="M112"><mml:msub><mml:mi mathvariant="normal">E</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq84.gif"/></alternatives></inline-formula></td><td align="left">80<sup>2</sup> × 128</td><td align="left">ResBlock × 4 + MP</td><td align="left">40<sup>2</sup> × 256</td><td align="left">SO<sub>2</sub></td><td align="left">320<sup>2</sup> × 32</td><td align="left">Deconv,× 2</td><td align="left">640<sup>2</sup> × 1</td></tr><tr><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">SO<sub>3</sub></td><td align="left">160<sup>2</sup> × 64</td><td align="left">Deconv,4 × 4</td><td align="left">640<sup>2</sup> × 1</td></tr><tr><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">SO<sub>4</sub></td><td align="left">80<sup>2</sup> × 128</td><td align="left">Deconv,8 × 8</td><td align="left">640<sup>2</sup> × 1</td></tr></tbody></table><table-wrap-foot><p>The input (output) shapes are represented by (size<sup>2</sup> × channel). ‘MP’ denotes the MaxPooling operation, ‘DU’ denotes the proposed dual upsampling module which is a concatenated result of deconvolution and upsampling and ‘SO<sub>i</sub>’ denotes <italic>i</italic>th Side Output. For simplicity, we omit the upsampling operations in skip connections and the detail of CCA module which can be seen in Fig. <xref rid="Fig4" ref-type="fig">4</xref></p></table-wrap-foot></table-wrap></p>
      <p id="Par26">For each layer, we employ the combined binary cross entropy loss and dice loss as our loss function:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \begin{aligned} \mathcal {L}_{i}(\varvec{Y} ,\varvec{\hat{Y}})&amp;=\mathcal {L}_{bce} + \mathcal {L}_{dice}\\&amp;= -\frac{1}{N}\sum _{n=1}^{N} \left( \varvec{Y}_{n}\cdot log \varvec{\hat{Y}}_{n} + 2\cdot \frac{\varvec{Y}_{n}\cdot \varvec{\hat{Y}}_{n}}{\varvec{Y}_{n}+\varvec{\hat{Y}}_{n}}\right) \end{aligned} \end{aligned}$$\end{document}</tex-math><mml:math id="M114" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi mathvariant="italic">bce</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi mathvariant="italic">dice</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mfenced close=")" open="("><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>·</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="13755_2022_209_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq103"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{Y}$$\end{document}</tex-math><mml:math id="M116"><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq103.gif"/></alternatives></inline-formula> and <inline-formula id="IEq104"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\hat{Y}}$$\end{document}</tex-math><mml:math id="M118"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq104.gif"/></alternatives></inline-formula> denote the ground truth labels and predicted probabilities in the batch, <inline-formula id="IEq105"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{Y}_n$$\end{document}</tex-math><mml:math id="M120"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq105.gif"/></alternatives></inline-formula> and <inline-formula id="IEq106"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\hat{Y}}_n$$\end{document}</tex-math><mml:math id="M122"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq106.gif"/></alternatives></inline-formula> denote the <italic>n</italic>th pixel of <inline-formula id="IEq107"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{Y}$$\end{document}</tex-math><mml:math id="M124"><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq107.gif"/></alternatives></inline-formula> and <inline-formula id="IEq108"><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\hat{Y}}$$\end{document}</tex-math><mml:math id="M126"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq108.gif"/></alternatives></inline-formula>, <italic>N</italic> indicates the number of pixels within one batch. We empirically set the weights of the two terms in Eq. (5) to the same. The overall loss function for MCA-UNet is then defined as the weighted summation of the combined loss from each level of decoder:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \mathcal {L}_{ds}(\varvec{Y} ,\ \varvec{\hat{Y}}_1,\ \varvec{\hat{Y}}_2,\ \varvec{\hat{Y}}_3,\ \varvec{\hat{Y}}_4 )=\sum _{i=1}^4 {w_{i}\cdot \mathcal {L}_{i}}(\varvec{Y} ,\ \varvec{\hat{Y}}_i) \end{aligned}$$\end{document}</tex-math><mml:math id="M128" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi mathvariant="italic">ds</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="13755_2022_209_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where <italic>i</italic> indexes the level of the decoder and <inline-formula id="IEq109"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w_i$$\end{document}</tex-math><mml:math id="M130"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq109.gif"/></alternatives></inline-formula> is the weight of each loss.</p>
      <p id="Par27">The performance of deep supervision highly depends on an appropriate choice of weights among the different tasks. How to appropriately set the weights of different tasks is a key issue in the deep supervision. A naive approach is to assign each individual task with an equal weight. It is not appropriate because the multiple tasks to be optimized have different difficulty levels. In this work, we consider the deep supervision as a multi-task learning formulation and assign different weights for different tasks. We propose a dynamic task weighting algorithm, named BMTD, which helps the model to automatically achieve balanced training by dynamically tuning the weight of each task during the model optimization. The weight of each task changes every batch. Hence, we measure how well the model is trained by considering the loss ratio between the current loss and the initial loss for each task. The task which is not trained well has a larger loss ratio. Hence, the harder tasks are optimized with more priority than the easier tasks.</p>
    </sec>
  </sec>
  <sec id="Sec12">
    <title>Experiment and results</title>
    <p id="Par28">
      <table-wrap id="Tab2">
        <label>Table 2</label>
        <caption>
          <p>The results of effectiveness of the proposed components</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th align="left" rowspan="2">Index</th>
              <th align="left" rowspan="2">Base</th>
              <th align="left" rowspan="2">Res</th>
              <th align="left" rowspan="2">DS</th>
              <th align="left" rowspan="2">MCA</th>
              <th align="left" rowspan="2">BMTD</th>
              <th align="left" colspan="3">Ground glass(%)</th>
              <th align="left" colspan="3">Consolidations (%)</th>
              <th align="left" colspan="3">Average (%)</th>
            </tr>
            <tr>
              <th align="left">Dice</th>
              <th align="left">Prec.</th>
              <th align="left">Sen.</th>
              <th align="left">Dice</th>
              <th align="left">Prec.</th>
              <th align="left">Sen.</th>
              <th align="left">Dice</th>
              <th align="left">Prec.</th>
              <th align="left">Sen.</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left">1</td>
              <td align="left">✓</td>
              <td align="left"/>
              <td align="left"/>
              <td align="left"/>
              <td align="left"/>
              <td align="left">61.15</td>
              <td align="left">59.00</td>
              <td align="left">68.01</td>
              <td align="left">38.69</td>
              <td align="left">38.21</td>
              <td align="left">46.73</td>
              <td align="left">49.92</td>
              <td align="left">48.61</td>
              <td align="left">57.37</td>
            </tr>
            <tr>
              <td align="left">2</td>
              <td align="left">✓</td>
              <td align="left">✓</td>
              <td align="left"/>
              <td align="left"/>
              <td align="left"/>
              <td align="left">62.08</td>
              <td align="left">59.94</td>
              <td align="left">68.00</td>
              <td align="left">38.73</td>
              <td align="left">43.11</td>
              <td align="left">39.62</td>
              <td align="left">50.41</td>
              <td align="left">51.52</td>
              <td align="left">53.81</td>
            </tr>
            <tr>
              <td align="left">3</td>
              <td align="left">✓</td>
              <td align="left">✓</td>
              <td align="left">✓</td>
              <td align="left"/>
              <td align="left"/>
              <td align="left">62.36</td>
              <td align="left">
                <bold>61.49</bold>
              </td>
              <td align="left">67.00</td>
              <td align="left">40.19</td>
              <td align="left">41.53</td>
              <td align="left">45.56</td>
              <td align="left">51.27</td>
              <td align="left">51.51</td>
              <td align="left">56.28</td>
            </tr>
            <tr>
              <td align="left">4</td>
              <td align="left">✓</td>
              <td align="left">✓</td>
              <td align="left"/>
              <td align="left">✓</td>
              <td align="left"/>
              <td align="left">62.72</td>
              <td align="left">61.46</td>
              <td align="left">62.78</td>
              <td align="left">41.27</td>
              <td align="left">44.07</td>
              <td align="left">44.31</td>
              <td align="left">52.00</td>
              <td align="left">52.77</td>
              <td align="left">55.55</td>
            </tr>
            <tr>
              <td align="left">5</td>
              <td align="left">✓</td>
              <td align="left">✓</td>
              <td align="left">✓</td>
              <td align="left">✓</td>
              <td align="left"/>
              <td align="left">62.92</td>
              <td align="left">60.92</td>
              <td align="left">68.82</td>
              <td align="left">42.28</td>
              <td align="left">43.24</td>
              <td align="left">47.77</td>
              <td align="left">52.60</td>
              <td align="left">52.08</td>
              <td align="left">58.30</td>
            </tr>
            <tr>
              <td align="left">6</td>
              <td align="left">✓</td>
              <td align="left">✓</td>
              <td align="left">✓</td>
              <td align="left">✓</td>
              <td align="left">✓</td>
              <td align="left">
                <bold>63.39</bold>
              </td>
              <td align="left">61.24</td>
              <td align="left">
                <bold>68.91</bold>
              </td>
              <td align="left">
                <bold>43.45</bold>
              </td>
              <td align="left">
                <bold>44.91</bold>
              </td>
              <td align="left">
                <bold>49.11</bold>
              </td>
              <td align="left">
                <bold>53.42</bold>
              </td>
              <td align="left">
                <bold>53.07</bold>
              </td>
              <td align="left">
                <bold>59.01</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>Best results are boldfaced</p>
          <p><italic>Res</italic> residual block, <italic>DS</italic> Deep supervision, <italic>MCA</italic> Multi-scale cross corelation Attention block, <italic>BMTD</italic> balanced multi-task dynamic weighting, ‘Prec.’ and ‘Sen.’ Precision and Sensitivity</p>
        </table-wrap-foot>
      </table-wrap>
      <table-wrap id="Tab3">
        <label>Table 3</label>
        <caption>
          <p>Comparison of our method and the state-of-the-art methods on the COVID dataset</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th align="left" rowspan="2">Methods</th>
              <th align="left" colspan="3">Ground glass (%)</th>
              <th align="left" colspan="3">Consolidations (%)</th>
              <th align="left" colspan="3">Average (%)</th>
            </tr>
            <tr>
              <th align="left">Dice</th>
              <th align="left">Prec.</th>
              <th align="left">Sen.</th>
              <th align="left">Dice</th>
              <th align="left">Prec.</th>
              <th align="left">Sen.</th>
              <th align="left">Dice</th>
              <th align="left">Prec.</th>
              <th align="left">Sen.</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left">UNet(Baseline)</td>
              <td align="left">61.15</td>
              <td align="left">59.00</td>
              <td align="left">68.01</td>
              <td align="left">38.69</td>
              <td align="left">38.21</td>
              <td align="left">46.73</td>
              <td align="left">49.92</td>
              <td align="left">48.61</td>
              <td align="left">57.37</td>
            </tr>
            <tr>
              <td align="left">UNet++(Backbone:ResNet-101)</td>
              <td align="left">61.50</td>
              <td align="left">60.32</td>
              <td align="left">66.46</td>
              <td align="left">39.20</td>
              <td align="left">40.14</td>
              <td align="left">45.83</td>
              <td align="left">50.35</td>
              <td align="left">50.23</td>
              <td align="left">56.15</td>
            </tr>
            <tr>
              <td align="left">UNet++(ResBlock)</td>
              <td align="left">62.07</td>
              <td align="left">57.86</td>
              <td align="left">
                <bold>71.56</bold>
              </td>
              <td align="left">40.94</td>
              <td align="left">41.48</td>
              <td align="left">47.39</td>
              <td align="left">51.50</td>
              <td align="left">49.67</td>
              <td align="left">
                <bold>59.48</bold>
              </td>
            </tr>
            <tr>
              <td align="left">MultiResUNet</td>
              <td align="left">61.46</td>
              <td align="left">59.94</td>
              <td align="left">66.99</td>
              <td align="left">40.88</td>
              <td align="left">42.78</td>
              <td align="left">45.66</td>
              <td align="left">51.17</td>
              <td align="left">51.36</td>
              <td align="left">56.33</td>
            </tr>
            <tr>
              <td align="left">Attention-UNet</td>
              <td align="left">62.18</td>
              <td align="left">60.90</td>
              <td align="left">68.25</td>
              <td align="left">39.84</td>
              <td align="left">39.17</td>
              <td align="left">47.59</td>
              <td align="left">51.01</td>
              <td align="left">50.04</td>
              <td align="left">57.92</td>
            </tr>
            <tr>
              <td align="left">MCA-UNet</td>
              <td align="left">
                <bold>63.39</bold>
              </td>
              <td align="left">
                <bold>61.24</bold>
              </td>
              <td align="left">68.91</td>
              <td align="left">
                <bold>43.95</bold>
              </td>
              <td align="left">
                <bold>44.91</bold>
              </td>
              <td align="left">
                <bold>49.11</bold>
              </td>
              <td align="left">
                <bold>53.67</bold>
              </td>
              <td align="left">
                <bold>53.07</bold>
              </td>
              <td align="left">59.01</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>Best results are boldfaced</p>
        </table-wrap-foot>
      </table-wrap>
    </p>
    <sec id="Sec13">
      <title>Implementation details</title>
      <p id="Par29">The proposed architecture is listed in Table <xref rid="Tab1" ref-type="table">1</xref>. We used Adam as the optimizer and set the learning rate and batch size to 5e−3 and 24. To avoid over-fitting, we used early stopping and set the patience as 50 epochs. The final number of training epochs is about 200. For all the compared methods, we used the same parameter settings.</p>
    </sec>
    <sec id="Sec14">
      <title>Data and experimental setting</title>
      <sec id="Sec15">
        <title>COVID-19 lung CT images segmentation</title>
        <p id="Par30">We used the public COVID-19 CT images collected by Italian Society of Medical and Interventional Radiology (SIRM) dataset<xref ref-type="fn" rid="Fn1">1</xref> that contains 100 training and 10 testing images. The ground-truth segmentation was done by a trained radiologist. Raw data are public available.<xref ref-type="fn" rid="Fn2">2</xref> We performed 5-fold cross validation and augmented the data by rotating and rescaling. To improve the computational efficiency of the model, we resized the image to 256<inline-formula id="IEq128"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M132"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq128.gif"/></alternatives></inline-formula>256 pixels. Three evaluation metrics were adopted, including Dice coefficient (Dice), Precision and Recall.</p>
      </sec>
      <sec id="Sec16">
        <title>Retinal microaneurysms segmentation</title>
        <p id="Par33">For this task, we used the Indian Diabetic Retinopathy Image Dataset (IDRiD) [<xref ref-type="bibr" rid="CR22">22</xref>], which contains 81 images including 54 images for training and 27 images for testing. The ground-truth segmentation has precise pixel level annotation of abnormalities associated with DR. We chose microaneurysms (MA) and hard exudates (EX) as the target lesion in our experiment since both lesions are small and dispersedly distributed. We computed the area under the precision-recall curve (AUC-PR), the area under the receiver operating characteristic curve (AUC-ROC) and Dice coefficient (Dice) to quantitatively evaluate the segmentation results. We used online data augmentation including resize, random crop, random rotate and CLAHE.<fig id="Fig5"><label>Fig. 5</label><caption><p>The qualitative comparison of the segmentation results on COVID-19 dataset produced by the comparable models and the proposed MCA-UNet. The red boxes indicate the regions where our method exhibits better segmentation performance than the others</p></caption><graphic xlink:href="13755_2022_209_Fig5_HTML" id="MO11"/></fig></p>
      </sec>
    </sec>
    <sec id="Sec17">
      <title>The comparison on COVID-19 dataset</title>
      <p id="Par34">We carried out experiments on the COVID-19 dataset to evaluate the effectiveness of our method. Note that the comparable models have the same encoder-decoder framework as MCA-UNet, including the number of channels, network depth and training strategies. We chose U-Net with ResBlock as our backbone segmentation architecture. The average Dice, Precision, and Sensitivity of all the methods were listed in Table <xref rid="Tab2" ref-type="table">2</xref>. As shown in Table <xref rid="Tab2" ref-type="table">2</xref>, it shows that these enhancements lead to notable improvements on the two segmentation tasks. Our model yields the overall highest performance, with an increase of 3.66% Dice for GGO segmentation and 12.30% Dice for consolidations segmentation compared to the baseline U-Net. Particularly for Consolidation, the increase of performance is striking. Compared to U-Net, our MCA-UNet improves the performance remarkably. Compared with the U-Net with the residual blocks, the cross co-attention module brings 3.15% improvement. The attention information from different layers in the encoder has complementary features, which obviously improves the segmentation accuracy. Meanwhile, deep supervision module individually outperforms the baseline by 1.71%. Therefore, learning the feature representation with direct supervision in the deeper layers is important. When we integrated the deep supervision and MCA together, the performance further improves to 52.60%, which outperforms the individual component of DS and MCA. With the BMTD optimization algorithm, improvements of 0.47% and 1.27% are achieved in ground glass and consolidations, respectively. These observation shows the crucial role of BMTD optimization. Moreover, it also indicates that the side outputs cannot be simply used with the same weights.</p>
      <p id="Par35">To more comprehensively evaluate our model, we chose some typical methods for further comparison. For the Covid19 dataset, we compared the proposed MCA-UNet to UNet++ (Resblock) [<xref ref-type="bibr" rid="CR10">10</xref>], MultiResUNet [<xref ref-type="bibr" rid="CR11">11</xref>], and Attention-UNet [<xref ref-type="bibr" rid="CR13">13</xref>]. All of the networks have an encoder-decoder based architecture. We also compared to the UNet++ with ResNet-101 as powerful encoder.<table-wrap id="Tab4"><label>Table 4</label><caption><p>The comparison with the state-of-the-art MA segmentation methods on the IDRiD dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Methods</th><th align="left">AUC-PR (%)</th><th align="left">AUC-ROC (%)</th><th align="left">Dice (%)</th></tr></thead><tbody><tr><td align="left">iFLYTEK-MIG (Rank #1)</td><td align="left">50.17</td><td align="left">N/A</td><td align="left">N/A</td></tr><tr><td align="left">VRT (Rank #2)</td><td align="left">49.51</td><td align="left">N/A</td><td align="left">N/A</td></tr><tr><td align="left">PATech (Rank #3)</td><td align="left">47.40</td><td align="left">N/A</td><td align="left">N/A</td></tr><tr><td align="left">DRU-Net [<xref ref-type="bibr" rid="CR23">23</xref>]</td><td align="left">N/A</td><td align="left">98.20</td><td align="left">N/A</td></tr><tr><td align="left">SSCL [<xref ref-type="bibr" rid="CR24">24</xref>]</td><td align="left">49.60</td><td align="left">98.28</td><td align="left">N/A</td></tr><tr><td align="left">DeepLabv3+ [<xref ref-type="bibr" rid="CR25">25</xref>]</td><td align="left">48.65</td><td align="left">98.91</td><td align="left">N/A</td></tr><tr><td align="left">SESV-Dlab [<xref ref-type="bibr" rid="CR26">26</xref>]</td><td align="left">50.99</td><td align="left">99.13</td><td align="left">N/A</td></tr><tr><td align="left">U-Net [<xref ref-type="bibr" rid="CR6">6</xref>](Baseline)</td><td align="left">45.04</td><td align="left">94.46</td><td align="left">16.24</td></tr><tr><td align="left">Attention-UNet [<xref ref-type="bibr" rid="CR13">13</xref>]</td><td align="left">49.01</td><td align="left">98.89</td><td align="left">25.49</td></tr><tr><td align="left">MultiResUNet [<xref ref-type="bibr" rid="CR11">11</xref>]</td><td align="left">49.13</td><td align="left">99.19</td><td align="left">25.51</td></tr><tr><td align="left">UNet++ [<xref ref-type="bibr" rid="CR10">10</xref>](ResNet-101)</td><td align="left">46.92</td><td align="left">98.01</td><td align="left">32.03</td></tr><tr><td align="left">UNet++(ResBlock)</td><td align="left">49.32</td><td align="left"><bold>99.27</bold></td><td align="left">19.45</td></tr><tr><td align="left">MCA-UNet</td><td align="left"><bold>52.06</bold></td><td align="left">99.12</td><td align="left"><bold>38.50</bold></td></tr></tbody></table><table-wrap-foot><p>Best results are boldfaced</p></table-wrap-foot></table-wrap></p>
      <p id="Par36">The experimental results obtained by several state-of-the-art segmentation networks are reported in Table <xref rid="Tab3" ref-type="table">3</xref>. By comparing the results from Table <xref rid="Tab3" ref-type="table">3</xref>, we can observe that the segmentation task achieves better performance in MCA-UNet. Compared to other networks that were proposed in the context of medical image segmentation: UNet++ (ResNet-101), MultiResUNet and Attention-UNet, our network achieves average improvements of 6.59%, 4.89% and 5.21% (in terms of Dice), 5.40%, 3.33% and 6.06% (in terms of Precision) and 5.09%, 4.76% and 1.88% (in terms of Sensitivity), respectively. Except for the sensitivity, our model also obtains improvements of 4.21% and 6.85% in terms of dice and precision compared with UNet++(ResBlock). Based on the above quantitative analysis, we can see that the cross skip connections guided by co-attention mechanisms are helpful for the refinement and fusion of complementary information between multi-scale features. Particularly, the proposed multi-scale guided attention network performs better results than Attention-UNet, which also integrates attention modules. Besides, we visualized the segmentation results of the comparable models in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. The red boxes highlight regions where MCA-UNet performs better than the other methods by making better use of the multi-scale context fusion and attention scheme. It shows that our MCA-UNet generates better segmentation results, which are more similar to the ground truth than the results of the competing models. Through the empirical results, we summarize the following findings:<list list-type="order"><list-item><p>For the 1st and 2nd cases where the boundaries of GGO often have low contrast and blurred appearances, making them difficult to be identified. MCA-UNet predicts finer boundary information and maintain the object coherence, which demonstrates the effectiveness of modeling global context representations. It indicates that the multi-scale fusion help to discover more complete and accurate areas of classes of interest with low contrast.</p></list-item><list-item><p id="Par38">2. Consolidations vary significantly in size and shape and have irregular and ambiguous boundaries. For the 3rd and 4th cases, the consolidations have a narrow shape. It can be seen that the predictions of MCA-UNet captures the boundary well. It is obvious that MCA-UNet keeps more details due to its multi-scale features from different encoder levels. For the 5th case where the lesions contain irregular boundaries, the segmentation results generated by our method are closer to the ground truths. Moreover, it also introduces fewer mislabeled pixels, which leads to better performance than other methods. These visual results indicate that our approach can successfully recover finer segmentation details while avoiding getting distracted in ambiguous regions. Nevertheless, the other networks produce smoother segmentations, resulting in a loss of fine grained details. As UNet++ and UNet++(ResBlock) also employed a multi-scale architecture, these differences suggest that the higher scale incorporation and effective cross co-attention can actually improve the performance of segmentation networks. It can be seen that both methods tend to have over-segmentation problems, which may be caused by the lack of higher resolution features.<table-wrap id="Tab5"><label>Table 5</label><caption><p>The comparison with the state-of-the-art EX segmentation methods on the IDRiD dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Methods</th><th align="left">AUC-PR (%)</th><th align="left">AUC-ROC (%)</th><th align="left">Dice (%)</th></tr></thead><tbody><tr><td align="left">U-Net(Baseline)</td><td align="left">75.52</td><td align="left">99.34</td><td align="left">60.46</td></tr><tr><td align="left">Attention-UNet</td><td align="left">75.33</td><td align="left">99.16</td><td align="left">62.18</td></tr><tr><td align="left">MultiResUNet</td><td align="left">70.76</td><td align="left">98.60</td><td align="left">62.03</td></tr><tr><td align="left">UNet++(ResNet-101)</td><td align="left">74.95</td><td align="left">99.32</td><td align="left">61.91</td></tr><tr><td align="left">UNet++(ResBlock)</td><td align="left">72.82</td><td align="left">96.41</td><td align="left">50.54</td></tr><tr><td align="left">MCA-UNet</td><td align="left"><bold>79.45</bold></td><td align="left"><bold>99.53</bold></td><td align="left"><bold>65.91</bold></td></tr></tbody></table><table-wrap-foot><p>Best results are boldfaced</p></table-wrap-foot></table-wrap></p><p id="Par39"><fig id="Fig6"><label>Fig. 6</label><caption><p>The qualitative comparison of the segmentation results on IDRiD dataset produced by the comparable models and the proposed MCA-UNet</p></caption><graphic xlink:href="13755_2022_209_Fig6_HTML" id="MO12"/></fig></p></list-item></list></p>
      <p id="Par40">In summary, the previous approaches suffer from two main limitations in the segmentation of COVID-19: large variations of consolidation and blurred boundary of GGO. For large variations of consolidation in CT lead to inaccurate prediction for the baseline and the comparable methods due to the insufficient multi-scale feature which fails to deal with such variations. The blurred boundary of GGO leads to inaccurate prediction due to the lack of the high spatial information which is lost or distorted in the pooling and upsampling. Both the quantitative evaluation in Table <xref rid="Tab3" ref-type="table">3</xref> and qualitative comparison in Fig. <xref rid="Fig5" ref-type="fig">5</xref> demonstrate the effectiveness of the proposed MCA-UNet for COVID19 segmentation.</p>
    </sec>
    <sec id="Sec18">
      <title>The comparison on IDRiD dataset</title>
      <p id="Par41">For the IDRiD dataset, we compared MCA-UNet with SESV-DLab [<xref ref-type="bibr" rid="CR26">26</xref>], SSCL [<xref ref-type="bibr" rid="CR24">24</xref>], DRU-Net [<xref ref-type="bibr" rid="CR23">23</xref>], and three top-ranking methods on the IDRiD challenge leaderboard [<xref ref-type="bibr" rid="CR22">22</xref>]. DRU-UNet (Deep Recurrent U-Net) is a model which combines the deep residual model and recurrent convolutional operations into U-Net. SSCL is an advanced semi-supervised collaborative learning (SSCL) model. DeepLabv3+ is an extension of DeepLabv3, which introduces a decoder module to better recover the spatial resolutions and further refine the final segmentation masks. Different from the common methods for constructing a more accurate segmentation model, the aim of SESV-DLab is to predict the segmentation errors produced by an existing model and then correct them.</p>
      <p id="Par42">The performance of these methods is shown in Tables <xref rid="Tab4" ref-type="table">4</xref> and <xref rid="Tab5" ref-type="table">5</xref>. The results show that our model achieves the highest AUC-PR and AUC-ROC, especially for the segmentation of MA in Table <xref rid="Tab4" ref-type="table">4</xref>, our model beats the top-3 ranking methods by 3.77%, 5.15% and 9.83% in terms of AUC-PR, setting the new state of the art. It demonstrates again that our model is able to produce precise and reliable results for medical image segmentation.</p>
      <p id="Par43">Most of the existing U-shaped methods perform well on the large object segmentation, but fail to the detection of the small objects, which are particularly prevalent in the eye diseases. Due to the downsampling and upsampling operations in U-Net, the feature maps in hidden layers are sparser than the original inputs, which causes a loss of image details and results in the comparable segmentation models yield inferior segmentation performance for the small lesions. Figure <xref rid="Fig6" ref-type="fig">6</xref> shows some representative results and the comparable methods to exhibit the superiority of the proposed method on the segmentation of MA and EX. As illustrated in Fig. <xref rid="Fig6" ref-type="fig">6</xref>, from the top three examples, we can find that the comparable segmentation methods are limited in small lesion segmentation and produce amounts of false positives. From the bottom three examples, it can be observed that both UNet++(ResNet-101) and UNet++(ResBlock) have over-segmentation problems. On the contrary, the boundary of the EX is under-segmented by both Attention-UNet and MultiResUNet. All the comparable segmentation models are not capable of precise segmentation of the small lesions. In the medical image domain, the multi-scale information is required to be learned by the segmentation models which then facilitates the target segmentation. It shows that MCA-UNet can significantly reduce the false positives and correct some inaccurately segmented regions by the previous algorithms.</p>
    </sec>
  </sec>
  <sec id="Sec19">
    <title>Discussion</title>
    <sec id="Sec20">
      <title>Discussion on the number of dense skip connections</title>
      <p id="Par44">Multi-scale dense connection and cross co-attention (CCA) are two vital modules in our segmentation model to achieve better segmentation performances. To further investigate the relative contribution of each component, we conduct a series of experiments on the EX segmentation, to investigate the individual contribution by varying the number of skip connections, skip connection schemes, and positions of skip connections. By varying the number of skip connections in the bi-decoder, we explored the influence of different skip connections on the EX segmentation performance. Moreover, to evaluate the segmentation performance of the CCA, we replace CCA in all the bi-decoders with a simple concatenation fusion used in the U-Net. The illustration of the competing models can be referred to Fig. <xref rid="Fig7" ref-type="fig">7</xref>. ‘w/o up’ or ‘w/o down’ means that the up-sampling or down-sampling operation in the skip connection is removed.<fig id="Fig7"><label>Fig. 7</label><caption><p>Comparison among the MCA-UNets with different number of the skip connections</p></caption><graphic xlink:href="13755_2022_209_Fig7_HTML" id="MO13"/></fig><table-wrap id="Tab6"><label>Table 6</label><caption><p>Different mappings from encoder to decoder</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Models with different Layer nums</th><th align="left" colspan="2">Attention</th><th align="left" colspan="2">w/o Attention</th></tr><tr><th align="left">AUPR (%)</th><th align="left">AUC (%)</th><th align="left">AUPR (%)</th><th align="left">AUC (%)</th></tr></thead><tbody><tr><td align="left">MCA-UNet-4</td><td align="left"><bold>79.45</bold></td><td align="left"><bold>99.53</bold></td><td align="left">77.14</td><td align="left">99.27</td></tr><tr><td align="left">MCA-UNet-4(w/o up)</td><td align="left">78.37</td><td align="left">99.09</td><td align="left">77.05</td><td align="left">99.15</td></tr><tr><td align="left">MCA-UNet-3</td><td align="left">77.25</td><td align="left">98.81</td><td align="left">77.09</td><td align="left">97.20</td></tr><tr><td align="left">MCA-UNet-3(w/o up)</td><td align="left">77.35</td><td align="left">99.09</td><td align="left">76.25</td><td align="left">99.27</td></tr><tr><td align="left">MCA-UNet-2(w/o up)</td><td align="left">77.21</td><td align="left">99.06</td><td align="left">77.25</td><td align="left">98.39</td></tr><tr><td align="left">MCA-UNet-2(w/o down)</td><td align="left">76.41</td><td align="left">98.53</td><td align="left">75.41</td><td align="left">98.98</td></tr><tr><td align="left">MCA-UNet-1</td><td align="left">77.78</td><td align="left">99.15</td><td align="left">77.56</td><td align="left">99.31</td></tr></tbody></table><table-wrap-foot><p>Best results are boldfaced</p></table-wrap-foot></table-wrap><table-wrap id="Tab7"><label>Table 7</label><caption><p>The study on different fusion and attention mechanism</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Index</th><th align="left" rowspan="2">Attention structure</th><th align="left" colspan="3">Ground glass (%)</th></tr><tr><th align="left">Dice</th><th align="left">Prec.</th><th align="left">Sen.</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">S (SA in encoder)</td><td align="left">61.59</td><td align="left">60.71</td><td align="left">66.56</td></tr><tr><td align="left">2</td><td align="left">S+C (SA in encoder)</td><td align="left">62.48</td><td align="left">58.29</td><td align="left"><bold>72.50</bold></td></tr><tr><td align="left">3</td><td align="left">S+C (SA in encoder and decoder)</td><td align="left">60.93</td><td align="left">59.99</td><td align="left">66.94</td></tr><tr><td align="left">4</td><td align="left">S+C (sequential en-de CCA)</td><td align="left">62.54</td><td align="left">59.43</td><td align="left">69.42</td></tr><tr><td align="left">5</td><td align="left">S+C (concurrent en-de CCA)</td><td align="left"><bold>62.76</bold></td><td align="left"><bold>60.93</bold></td><td align="left">66.98</td></tr></tbody></table><table-wrap-foot><p>Best results are boldfaced</p><p><italic>S</italic> spatial-wise, <italic>C</italic> channel-wise, <italic>SA</italic> self-attention</p></table-wrap-foot></table-wrap><table-wrap id="Tab8"><label>Table 8</label><caption><p>The performance of the proposed connections with different positions and side outputs</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Positions of encoder and decoder</th><th align="left">AUPR (%)</th><th align="left">AUC (%)</th><th align="left">Side outputs for prediction</th><th align="left">AUPR (%)</th><th align="left">AUC (%)</th></tr></thead><tbody><tr><td align="left"><inline-formula id="IEq129"><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\textbf{E}_1, \textbf{E}_2, \textbf{E}_3, \textbf{E}_4)\rightarrow (\textbf{D}_1, \textbf{D}_2, \textbf{D}_3, \textbf{D}_4)$$\end{document}</tex-math><mml:math id="M134"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">→</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq129.gif"/></alternatives></inline-formula></td><td align="left"><bold>79.45</bold></td><td align="left"><bold> 99.53</bold></td><td align="left"><inline-formula id="IEq130"><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{D}_1$$\end{document}</tex-math><mml:math id="M136"><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq130.gif"/></alternatives></inline-formula></td><td align="left">79.45</td><td align="left"><bold>99.53</bold></td></tr><tr><td align="left"><inline-formula id="IEq131"><alternatives><tex-math id="M137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_1 \rightarrow (\textbf{D}_1, \textbf{D}_2, \textbf{D}_3, \textbf{D}_4)$$\end{document}</tex-math><mml:math id="M138"><mml:mrow><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq131.gif"/></alternatives></inline-formula></td><td align="left"><bold>77.67</bold></td><td align="left">98.35</td><td align="left"><inline-formula id="IEq132"><alternatives><tex-math id="M139">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{D}_2$$\end{document}</tex-math><mml:math id="M140"><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq132.gif"/></alternatives></inline-formula></td><td align="left">78.82</td><td align="left">99.26</td></tr><tr><td align="left"><inline-formula id="IEq133"><alternatives><tex-math id="M141">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_2 \rightarrow (\textbf{D}_1, \textbf{D}_2, \textbf{D}_3, \textbf{D}_4)$$\end{document}</tex-math><mml:math id="M142"><mml:mrow><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq133.gif"/></alternatives></inline-formula></td><td align="left">76.56</td><td align="left">98.65</td><td align="left"><inline-formula id="IEq134"><alternatives><tex-math id="M143">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{D}_3$$\end{document}</tex-math><mml:math id="M144"><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq134.gif"/></alternatives></inline-formula></td><td align="left">78.53</td><td align="left">99.33</td></tr><tr><td align="left"><inline-formula id="IEq135"><alternatives><tex-math id="M145">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_3 \rightarrow (\textbf{D}_1, \textbf{D}_2, \textbf{D}_3, \textbf{D}_4)$$\end{document}</tex-math><mml:math id="M146"><mml:mrow><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq135.gif"/></alternatives></inline-formula></td><td align="left">75.06</td><td align="left">99.17</td><td align="left"><inline-formula id="IEq136"><alternatives><tex-math id="M147">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{D}_4$$\end{document}</tex-math><mml:math id="M148"><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq136.gif"/></alternatives></inline-formula></td><td align="left">73.46</td><td align="left">99.48</td></tr><tr><td align="left"><inline-formula id="IEq137"><alternatives><tex-math id="M149">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_4 \rightarrow (\textbf{D}_1, \textbf{D}_2, \textbf{D}_3, \textbf{D}_4)$$\end{document}</tex-math><mml:math id="M150"><mml:mrow><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq137.gif"/></alternatives></inline-formula></td><td align="left">72.53</td><td align="left">99.06</td><td align="left"><inline-formula id="IEq138"><alternatives><tex-math id="M151">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{D}_1+\textbf{D}_2$$\end{document}</tex-math><mml:math id="M152"><mml:mrow><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq138.gif"/></alternatives></inline-formula></td><td align="left"><bold>79.59</bold></td><td align="left">99.28</td></tr><tr><td align="left"><inline-formula id="IEq139"><alternatives><tex-math id="M153">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\textbf{E}_1, \textbf{E}_2, \textbf{E}_3, \textbf{E}_4)\rightarrow \textbf{D}_1$$\end{document}</tex-math><mml:math id="M154"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq139.gif"/></alternatives></inline-formula></td><td align="left">75.66</td><td align="left"><bold>99.29</bold></td><td align="left"><inline-formula id="IEq140"><alternatives><tex-math id="M155">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{D}_1+\textbf{D}_2+\textbf{D}_3$$\end{document}</tex-math><mml:math id="M156"><mml:mrow><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq140.gif"/></alternatives></inline-formula></td><td align="left">78.46</td><td align="left">99.50</td></tr><tr><td align="left"><inline-formula id="IEq141"><alternatives><tex-math id="M157">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\textbf{E}_1, \textbf{E}_2, \textbf{E}_3, \textbf{E}_4)\rightarrow \textbf{D}_2$$\end{document}</tex-math><mml:math id="M158"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq141.gif"/></alternatives></inline-formula></td><td align="left">76.50</td><td align="left">99.15</td><td align="left"><inline-formula id="IEq142"><alternatives><tex-math id="M159">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{D}_1+\textbf{D}_2+\textbf{D}_3+\textbf{D}_4$$\end{document}</tex-math><mml:math id="M160"><mml:mrow><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq142.gif"/></alternatives></inline-formula></td><td align="left">78.78</td><td align="left">99.37</td></tr><tr><td align="left"><inline-formula id="IEq143"><alternatives><tex-math id="M161">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\textbf{E}_1, \textbf{E}_2, \textbf{E}_3, \textbf{E}_4)\rightarrow \textbf{D}_3$$\end{document}</tex-math><mml:math id="M162"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq143.gif"/></alternatives></inline-formula></td><td align="left">75.86</td><td align="left">98.13</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr><tr><td align="left"><inline-formula id="IEq144"><alternatives><tex-math id="M163">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\textbf{E}_1, \textbf{E}_2, \textbf{E}_3, \textbf{E}_4)\rightarrow \textbf{D}_4$$\end{document}</tex-math><mml:math id="M164"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq144.gif"/></alternatives></inline-formula></td><td align="left">72.90</td><td align="left">99.26</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr></tbody></table><table-wrap-foot><p>Best results are boldfaced</p></table-wrap-foot></table-wrap></p>
      <p id="Par45">As shown in Table <xref rid="Tab6" ref-type="table">6</xref>, our proposed CCA is able to consistently achieve better performance compared with the simple concatenation fusion, which demonstrates its robustness and high flexibility for integrating information from the earlier feature maps. Moreover, it can be seen from Table <xref rid="Tab6" ref-type="table">6</xref> that the segmentation performance of the model improves with the increase of the number of skip connections. For the comparison between models with up-sampled connection remained and ones with up-sampled connection removed, the former is worse when the connection number is the same. For example, MCA-UNet-2 (w/o up) achieves an improvement by 1.05% compared with MCA-UNet-2(w/o down). which indicates that the higher resolution is important for the fine spatial recovery, whereas the connections from the encoders with lower resolution is not helpful for the decoders. Our findings show that the spatial information is more critical for the segmentation of the multi-scale lesion objects, especially for the small lesions. MCA-UNet-2 (w/o down) performs the worst, even worse than MCA-UNet-1. The skip connection scheme in MCA-UNet-2 (w/o down) is similar as the UNet++ where the decoders are connected with the lower resolution feature maps of encoders. Another interesting finding is that MCA-UNet-4 without CCA achieved a relatively poor performance compared to MCA-UNet-1 with CCA in terms of AUPR. The results once again validate that simply connecting the feature maps with same level from the encoder and the decoder is not an optimal solution.</p>
    </sec>
    <sec id="Sec21">
      <title>Discussion on different attention mechanisms</title>
      <p id="Par46">Based on the skip connections for information fusion, we systematically conduct the experiment of different attention mechanisms. The result is shown in Table <xref rid="Tab7" ref-type="table">7</xref>. We conduct a series of comparison including the spatial- and channel-wise CCA vs. the spatial-wise CCA, the self-attention (SA) of encoder or decoder vs. CCA and the sequential CCA vs. the concurrent CCA. The traditional self-attention mechanism is to capture the dependencies within the same feature map from the spatial- and channel-wise perspective. Our CCA is to capture the correlation between two feature maps from the encoder and decoder. It is apparent to see that, the proposed concurrent CCA method obtain improvements upon the traditional self attention methods in terms of Dice and precision. The channel maps help capture the context information for the feature fusion. When we integrate the spatial- and channel-wise together, the performance further improves to 62.48% with respect to Dice. Furthermore, when we compare the sequential and concurrent fashion for the encoder-decoder cross co-attention, the concurrent fashion improves the segmentation performance over the sequential model by 0.35% in terms of Dice.</p>
    </sec>
    <sec id="Sec22">
      <title>Discussion on positions of the proposed dense skip connections</title>
      <p id="Par47">We performed a series of experiments with respect to the positions of the proposed skip connection in Table <xref rid="Tab8" ref-type="table">8</xref>. Figure <xref rid="Fig8" ref-type="fig">8</xref> shows the illustration of the settings. Let <inline-formula id="IEq145"><alternatives><tex-math id="M165">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_i \rightarrow \textbf{D}_j$$\end{document}</tex-math><mml:math id="M166"><mml:mrow><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq145.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq146"><alternatives><tex-math id="M167">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i,j=1,\ldots,4$$\end{document}</tex-math><mml:math id="M168"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq146.gif"/></alternatives></inline-formula>, indicates how the encoder features are connected to the decoders. For example, <inline-formula id="IEq147"><alternatives><tex-math id="M169">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_1 \rightarrow (\textbf{D}_1, \textbf{D}_2, \textbf{D}_3, \textbf{D}_4)$$\end{document}</tex-math><mml:math id="M170"><mml:mrow><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq147.gif"/></alternatives></inline-formula> indicates that <inline-formula id="IEq148"><alternatives><tex-math id="M171">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_1$$\end{document}</tex-math><mml:math id="M172"><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq148.gif"/></alternatives></inline-formula> encoder is connected to the decoders of <inline-formula id="IEq149"><alternatives><tex-math id="M173">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{D}_1, \textbf{D}_2, \textbf{D}_3$$\end{document}</tex-math><mml:math id="M174"><mml:mrow><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq149.gif"/></alternatives></inline-formula> and <inline-formula id="IEq150"><alternatives><tex-math id="M175">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{D}_4$$\end{document}</tex-math><mml:math id="M176"><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq150.gif"/></alternatives></inline-formula>. Although the proposed CCA module contributes to the performance improvement as shown in the previous results, it is interesting to investigate 1) which level of encoder is more important for the decoders; and 2) which layer of decoder is more beneficial for the same combination of multi-scale encoder features. Obviously, MCA-UNet with multiple dense connection leads to improved performance than the other models with the certain connections removed. It can be seen that <inline-formula id="IEq151"><alternatives><tex-math id="M177">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_1 \rightarrow (\textbf{D}_1, \textbf{D}_2, \textbf{D}_3, \textbf{D}_4)$$\end{document}</tex-math><mml:math id="M178"><mml:mrow><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq151.gif"/></alternatives></inline-formula> obtains the best performance in terms of AUPR, which indicates that the low-level features with higher resolution is important. The <inline-formula id="IEq152"><alternatives><tex-math id="M179">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_1 \rightarrow (\textbf{D}_1, \textbf{D}_2, \textbf{D}_3, \textbf{D}_4)$$\end{document}</tex-math><mml:math id="M180"><mml:mrow><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq152.gif"/></alternatives></inline-formula> can take full advantage of the rich spatial information, which can help refine the predicted boundary for the lesions with complex structure. On the contrary, <inline-formula id="IEq153"><alternatives><tex-math id="M181">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_4 \rightarrow (\textbf{D}_1, \textbf{D}_2, \textbf{D}_3, \textbf{D}_4)$$\end{document}</tex-math><mml:math id="M182"><mml:mrow><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq153.gif"/></alternatives></inline-formula> shows the worst performance. The reason may be that the spatial information is lost in the contracting path and semantic gap is too large, resulting in poor fusion performance.<fig id="Fig8"><label>Fig. 8</label><caption><p>The comparison among MCA-UNet with CCA on the different positions</p></caption><graphic xlink:href="13755_2022_209_Fig8_HTML" id="MO14"/></fig></p>
    </sec>
    <sec id="Sec23">
      <title>Deep supervision</title>
      <p id="Par48">To test the effectiveness of the deep supervision scheme, we show the performance of each individual side output. From the Table <xref rid="Tab8" ref-type="table">8</xref>, we observe small difference for the multiple predictions of side outputs except <inline-formula id="IEq154"><alternatives><tex-math id="M183">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{D}_4$$\end{document}</tex-math><mml:math id="M184"><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq154.gif"/></alternatives></inline-formula>. Furthermore, we find the performance of <inline-formula id="IEq155"><alternatives><tex-math id="M185">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{D}_1$$\end{document}</tex-math><mml:math id="M186"><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq155.gif"/></alternatives></inline-formula> are slightly better than <inline-formula id="IEq156"><alternatives><tex-math id="M187">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{D}_2$$\end{document}</tex-math><mml:math id="M188"><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq156.gif"/></alternatives></inline-formula> and <inline-formula id="IEq157"><alternatives><tex-math id="M189">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{D}_3$$\end{document}</tex-math><mml:math id="M190"><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq157.gif"/></alternatives></inline-formula>. We also try to employ an ensemble-based methods, where the multiple side outputs are combined to make a final prediction. We find the ensemble of <inline-formula id="IEq158"><alternatives><tex-math id="M191">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{D}_1+\textbf{D}_2$$\end{document}</tex-math><mml:math id="M192"><mml:mrow><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13755_2022_209_Article_IEq158.gif"/></alternatives></inline-formula> achieves a slightly better performance than the individual performance.</p>
    </sec>
  </sec>
  <sec id="Sec24">
    <title>Conclusion</title>
    <p id="Par49">In this work, we introduced a multi-scale Cross Co-Attentional Skip Connection U-Net architecture for the medical image segmentation. Our MCA-UNet utilized the multi-scale feature fusion strategy to combine semantic information at different levels and the cross co-attention module to aggregate relevant global dependencies. To validate our approach, we conducted experiments on three different segmentation tasks on the two different medical image datasets: consolidation, GGO, Microaneurysms and Hard Exudates, indicating that it can be broadly applied to the other medical images segmentation tasks. We provided extensive experiments to evaluate the impact of the individual components of the proposed architecture. Moreover, we will extend our 2D model to a 3D version for capturing the inter-slice continuity of the lesion in the future work.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p id="Par31">
        <ext-link ext-link-type="uri" xlink:href="https://www.sirm.org/category/senza-categoria/covid-19/">https://www.sirm.org/category/senza-categoria/covid-19/</ext-link>
      </p>
    </fn>
    <fn id="Fn2">
      <label>2</label>
      <p id="Par32">
        <ext-link ext-link-type="uri" xlink:href="http://medicalsegmentation.com/covid19/">http://medicalsegmentation.com/covid19/</ext-link>
      </p>
    </fn>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This research was supported by the National Natural Science Foundation of China (No. 62076059), the Science Project of Liaoning province (2021-MS-105) and the National Natural Science Foundation of China (No. 61971118).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>Data are publicly available.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1" notes-type="COI-statement">
      <title>Conflict of interest</title>
      <p id="Par53">The authors declare that they have no conflict of interest.</p>
    </notes>
    <notes id="FPar2">
      <title>Ethical approval</title>
      <p id="Par54">This article does not contain any studies with human participants or animals performed by any of the authors.</p>
    </notes>
    <notes id="FPar3">
      <title>Informed consent</title>
      <p id="Par55">Not applicable</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Litjens</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Kooi</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Bejnordi</surname>
            <given-names>BE</given-names>
          </name>
          <name>
            <surname>Setio</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Ciompi</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Ghafoorian</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Van Der Laak</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Van Ginneken</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sánchez</surname>
            <given-names>CI</given-names>
          </name>
        </person-group>
        <article-title>A survey on deep learning in medical image analysis</article-title>
        <source>Med Image Anal.</source>
        <year>2017</year>
        <volume>42</volume>
        <fpage>60</fpage>
        <lpage>88</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2017.07.005</pub-id>
        <pub-id pub-id-type="pmid">28778026</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pham</surname>
            <given-names>DL</given-names>
          </name>
          <name>
            <surname>Chenyang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Prince</surname>
            <given-names>JL</given-names>
          </name>
        </person-group>
        <article-title>Current methods in medical image segmentation</article-title>
        <source>Ann Rev Biomed Eng.</source>
        <year>2000</year>
        <volume>2</volume>
        <issue>1</issue>
        <fpage>315</fpage>
        <lpage>337</lpage>
        <pub-id pub-id-type="doi">10.1146/annurev.bioeng.2.1.315</pub-id>
        <pub-id pub-id-type="pmid">11701515</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tan</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Analysis of segmentation of lung parenchyma based on deep learning methods</article-title>
        <source>J X-Ray Sci Technol.</source>
        <year>2021</year>
        <volume>29</volume>
        <issue>6</issue>
        <fpage>945</fpage>
        <lpage>959</lpage>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tan</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Shaoxun</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Segmentation of lung airways based on deep learning methods</article-title>
        <source>IET Image Process.</source>
        <year>2022</year>
        <volume>16</volume>
        <issue>5</issue>
        <fpage>1444</fpage>
        <lpage>1456</lpage>
        <pub-id pub-id-type="doi">10.1049/ipr2.12423</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Juan</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Jiantao</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Automated segmentation of the optic disc from fundus images using an asymmetric deep learning network</article-title>
        <source>Pattern Recognit.</source>
        <year>2021</year>
        <volume>112</volume>
        <fpage>107810</fpage>
        <pub-id pub-id-type="doi">10.1016/j.patcog.2020.107810</pub-id>
        <pub-id pub-id-type="pmid">34354302</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Ronneberger O, Fischer P, Brox T. U-net: convolutional networks for biomedical image segmentation. In: Medical Image Computing and Computer-Assisted Intervention (MICCAI), volume 9351 of LNCS, 2015;234–241. Springer.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Isensee F, Kickingereder P, Wick W, Bendszus M, Maier-Hein KH. Brain Tumor Segmentation and Radiomics Survival Prediction: Contribution to the BRATS 2017 Challenge. In: Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries, Lecture Notes in Computer Science, pp 287–297, Cham, 2018.</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Falk</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Mai</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Bensch</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Ronneberger</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>U-Net: deep learning for cell counting, detection, and morphometry</article-title>
        <source>Nat Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <issue>1</issue>
        <fpage>67</fpage>
        <lpage>70</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-018-0261-2</pub-id>
        <pub-id pub-id-type="pmid">30559429</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qian</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Dai</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Crossover-Net: leveraging vertical-horizontal crossover relation for robust medical image segmentation</article-title>
        <source>Pattern Recognit.</source>
        <year>2021</year>
        <volume>113</volume>
        <fpage>107756</fpage>
        <pub-id pub-id-type="doi">10.1016/j.patcog.2020.107756</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zongwei Zhou</surname>
            <given-names>Md</given-names>
          </name>
          <name>
            <surname>Siddiquee</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Tajbakhsh</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>UNet++: redesigning skip connections to exploit multiscale features in image segmentation</article-title>
        <source>IEEE Trans Med Imaging</source>
        <year>2020</year>
        <volume>39</volume>
        <issue>6</issue>
        <fpage>1856</fpage>
        <lpage>1867</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2019.2959609</pub-id>
        <pub-id pub-id-type="pmid">31841402</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ibtehaz</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Sohel</surname>
            <given-names>RM</given-names>
          </name>
        </person-group>
        <article-title>MultiResUNet : rethinking the u-net architecture for multimodal biomedical image segmentation</article-title>
        <source>Neural Netw.</source>
        <year>2020</year>
        <volume>121</volume>
        <fpage>74</fpage>
        <lpage>87</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neunet.2019.08.025</pub-id>
        <pub-id pub-id-type="pmid">31536901</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. In: Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Oktay O, Schlemper J, Folgoc LL, Lee M, Heinrich M, Misawa K, Mori K, McDonagh S, Hammerla NY, Kainz B, Glocker B, Rueckert D. Attention U-Net: Learning Where to Look for the Pancreas. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1804.03999">arXiv:1804.03999</ext-link> [cs], 2018.</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">ISLES 2015-A public evaluation benchmark for ischemic stroke lesion segmentation from multispectral MRI.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Qi</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Dou</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Chi-Wing</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Heng</surname>
            <given-names>P-A</given-names>
          </name>
        </person-group>
        <article-title>H-DenseUNet: hybrid densely connected UNet for liver and tumor segmentation From CT volumes</article-title>
        <source>IEEE Trans Med Imaging</source>
        <year>2018</year>
        <volume>37</volume>
        <issue>12</issue>
        <fpage>2663</fpage>
        <lpage>2674</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2018.2845918</pub-id>
        <pub-id pub-id-type="pmid">29994201</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp 770–778, Las Vegas, NV, USA, 2016. IEEE.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bo</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Zaiane</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>MSDS-UNet: a multi-scale deeply supervised 3D U-Net for automatic segmentation of lung tumor in CT</article-title>
        <source>Comput Med Imaging Graph.</source>
        <year>2021</year>
        <volume>92</volume>
        <fpage>101957</fpage>
        <pub-id pub-id-type="doi">10.1016/j.compmedimag.2021.101957</pub-id>
        <pub-id pub-id-type="pmid">34325225</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kamnitsas</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Ledig</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Newcombe</surname>
            <given-names>VF</given-names>
          </name>
          <name>
            <surname>Simpson</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Kane</surname>
            <given-names>AD</given-names>
          </name>
          <name>
            <surname>Menon</surname>
            <given-names>DK</given-names>
          </name>
          <name>
            <surname>Rueckert</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Glocker</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation</article-title>
        <source>Med Image Anal.</source>
        <year>2017</year>
        <volume>36</volume>
        <fpage>61</fpage>
        <lpage>78</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2016.10.004</pub-id>
        <pub-id pub-id-type="pmid">27865153</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Xiaowei</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Lequan</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Chi-Wing</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Heng</surname>
            <given-names>P-A</given-names>
          </name>
        </person-group>
        <article-title>CANet: cross-disease attention network for joint diabetic retinopathy and diabetic macular edema grading</article-title>
        <source>IEEE Trans Med Imaging</source>
        <year>2020</year>
        <volume>39</volume>
        <issue>5</issue>
        <fpage>1483</fpage>
        <lpage>1493</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2019.2951844</pub-id>
        <pub-id pub-id-type="pmid">31714219</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Wang Q, Wu B, Zhu P, Li P, Zuo W, Hu Q. ECA-Net: efficient channel attention for deep convolutional neural networks. p 9.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Lee C-Y, Xie S, Gallagher P, Zhang Z, Tu Z. Deeply-supervised nets. In: Guy Lebanon and S. V. N. Vishwanathan, (eds), In: Proceedings of the eighteenth international conference on artificial intelligence and statistics, volume 38 of Proceedings of Machine Learning Research, pp 562–570, San Diego, 2015. PMLR.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Porwal</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Pachade</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kamble</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Kokare</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Deshmukh</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Sahasrabuddhe</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Meriaudeau</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Indian diabetic retinopathy image dataset (IDRiD): a database for diabetic retinopathy screening research</article-title>
        <source>Data</source>
        <year>2018</year>
        <volume>3</volume>
        <issue>3</issue>
        <fpage>25</fpage>
        <pub-id pub-id-type="doi">10.3390/data3030025</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kou</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Zekuan</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hao</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Microaneurysms segmentation with a U-Net based on recurrent residual convolutional neural network</article-title>
        <source>J Med Imaging</source>
        <year>2019</year>
        <volume>6</volume>
        <issue>02</issue>
        <fpage>1</fpage>
        <pub-id pub-id-type="doi">10.1117/1.JMI.6.2.025008</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Zhou Y, He X, Huang L, Liu L, Zhu F, Cui S, Shao L. Collaborative learning of semi-supervised segmentation and classification for medical images. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2079–2088, 2019.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>L-C</given-names>
          </name>
          <name>
            <surname>Papandreou</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Kokkinos</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Murphy</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Yuille</surname>
            <given-names>AL</given-names>
          </name>
        </person-group>
        <article-title>DeepLab: semantic image segmentation with deep convolutional nets, Atrous convolution, and fully connected CRFs</article-title>
        <source>IEEE Trans Pattern Anal Mach Intell</source>
        <year>2018</year>
        <volume>40</volume>
        <issue>4</issue>
        <fpage>834</fpage>
        <lpage>848</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id>
        <pub-id pub-id-type="pmid">28463186</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hao</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Xia</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>SESV: accurate medical image segmentation by predicting and correcting errors</article-title>
        <source>IEEE Trans Med Imaging</source>
        <year>2021</year>
        <volume>40</volume>
        <issue>1</issue>
        <fpage>286</fpage>
        <lpage>296</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2020.3025308</pub-id>
        <pub-id pub-id-type="pmid">32956049</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
