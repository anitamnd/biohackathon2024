<?properties open_access?>
<?subarticle pone.0224243.r001?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-id journal-id-type="pmc">plosone</journal-id>
    <journal-title-group>
      <journal-title>PLoS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6876843</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-19-17145</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0224243</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Brain Mapping</subject>
            <subj-group>
              <subject>Optogenetics</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Bioassays and Physiological Analysis</subject>
          <subj-group>
            <subject>Neurophysiological Analysis</subject>
            <subj-group>
              <subject>Optogenetics</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Behavior</subject>
            <subj-group>
              <subject>Animal Behavior</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Social Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Behavior</subject>
            <subj-group>
              <subject>Animal Behavior</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Zoology</subject>
          <subj-group>
            <subject>Animal Behavior</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Human Factors Engineering</subject>
          <subj-group>
            <subject>Man-Computer Interface</subject>
            <subj-group>
              <subject>Graphical User Interfaces</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Computer Architecture</subject>
          <subj-group>
            <subject>User Interfaces</subject>
            <subj-group>
              <subject>Graphical User Interfaces</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Physics</subject>
          <subj-group>
            <subject>Electromagnetic Radiation</subject>
            <subj-group>
              <subject>Light</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Equipment</subject>
          <subj-group>
            <subject>Optical Equipment</subject>
            <subj-group>
              <subject>Cameras</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Physiology</subject>
          <subj-group>
            <subject>Biological Locomotion</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Physiology</subject>
          <subj-group>
            <subject>Biological Locomotion</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Cell Biology</subject>
          <subj-group>
            <subject>Cellular Types</subject>
            <subj-group>
              <subject>Animal Cells</subject>
              <subj-group>
                <subject>Neurons</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Cellular Neuroscience</subject>
            <subj-group>
              <subject>Neurons</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Computer Architecture</subject>
          <subj-group>
            <subject>Computer Hardware</subject>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MARGO (Massively Automated Real-time GUI for Object-tracking), a platform for high-throughput ethology</article-title>
      <alt-title alt-title-type="running-head">MARGO (Massively Automated Real-time GUI for Object-tracking), a platform for high-throughput ethology</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0611-5864</contrib-id>
        <name>
          <surname>Werkhoven</surname>
          <given-names>Zach</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Data curation</role>
        <role content-type="http://credit.casrai.org/">Formal analysis</role>
        <role content-type="http://credit.casrai.org/">Investigation</role>
        <role content-type="http://credit.casrai.org/">Methodology</role>
        <role content-type="http://credit.casrai.org/">Resources</role>
        <role content-type="http://credit.casrai.org/">Software</role>
        <role content-type="http://credit.casrai.org/">Supervision</role>
        <role content-type="http://credit.casrai.org/">Validation</role>
        <role content-type="http://credit.casrai.org/">Visualization</role>
        <role content-type="http://credit.casrai.org/">Writing – original draft</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4597-9895</contrib-id>
        <name>
          <surname>Rohrsen</surname>
          <given-names>Christian</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Investigation</role>
        <role content-type="http://credit.casrai.org/">Methodology</role>
        <role content-type="http://credit.casrai.org/">Resources</role>
        <role content-type="http://credit.casrai.org/">Validation</role>
        <role content-type="http://credit.casrai.org/">Visualization</role>
        <role content-type="http://credit.casrai.org/">Writing – original draft</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Qin</surname>
          <given-names>Chuan</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Investigation</role>
        <role content-type="http://credit.casrai.org/">Methodology</role>
        <role content-type="http://credit.casrai.org/">Resources</role>
        <role content-type="http://credit.casrai.org/">Software</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7824-7650</contrib-id>
        <name>
          <surname>Brembs</surname>
          <given-names>Björn</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Funding acquisition</role>
        <role content-type="http://credit.casrai.org/">Methodology</role>
        <role content-type="http://credit.casrai.org/">Writing – original draft</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6165-7696</contrib-id>
        <name>
          <surname>de Bivort</surname>
          <given-names>Benjamin</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Formal analysis</role>
        <role content-type="http://credit.casrai.org/">Funding acquisition</role>
        <role content-type="http://credit.casrai.org/">Investigation</role>
        <role content-type="http://credit.casrai.org/">Methodology</role>
        <role content-type="http://credit.casrai.org/">Project administration</role>
        <role content-type="http://credit.casrai.org/">Resources</role>
        <role content-type="http://credit.casrai.org/">Software</role>
        <role content-type="http://credit.casrai.org/">Supervision</role>
        <role content-type="http://credit.casrai.org/">Validation</role>
        <role content-type="http://credit.casrai.org/">Visualization</role>
        <role content-type="http://credit.casrai.org/">Writing – original draft</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
        <xref ref-type="corresp" rid="cor001">*</xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>Dept. of Organismic and Evolutionary Biology &amp; Center for Brain Science, Harvard University, Cambridge, MA, United States of America</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Institut für Zoologie - Neurogenetik, Universität Regensburg, Regensburg, Germany</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Gilestro</surname>
          <given-names>Giorgio F</given-names>
        </name>
        <role>Editor</role>
        <xref ref-type="aff" rid="edit1"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Imperial College London, UNITED KINGDOM</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>BLdB is a scientific advisor of FlySorter, LLC. This does not alter our adherence to PLOS ONE policies on sharing data and materials.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>debivort@oeb.harvard.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>25</day>
      <month>11</month>
      <year>2019</year>
    </pub-date>
    <volume>14</volume>
    <issue>11</issue>
    <elocation-id>e0224243</elocation-id>
    <history>
      <date date-type="received">
        <day>17</day>
        <month>6</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>8</day>
        <month>10</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2019 Werkhoven et al</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Werkhoven et al</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0224243.pdf"/>
    <abstract>
      <p>Fast object tracking in real time allows convenient tracking of very large numbers of animals and closed-loop experiments that control stimuli for many animals in parallel. We developed MARGO, a MATLAB-based, real-time animal tracking suite for custom behavioral experiments. We demonstrated that MARGO can rapidly and accurately track large numbers of animals in parallel over very long timescales, typically when spatially separated such as in multiwell plates. We incorporated control of peripheral hardware, and implemented a flexible software architecture for defining new experimental routines. These features enable closed-loop delivery of stimuli to many individuals simultaneously. We highlight MARGO’s ability to coordinate tracking and hardware control with two custom behavioral assays (measuring phototaxis and optomotor response) and one optogenetic operant conditioning assay. There are currently several open source animal trackers. MARGO’s strengths are 1) fast and accurate tracking, 2) high throughput, 3) an accessible interface and data output and 4) real-time closed-loop hardware control for for sensory and optogenetic stimuli, all of which are optimized for large-scale experiments.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution>US National Science Foundation</institution>
        </funding-source>
        <award-id>GRFP</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0611-5864</contrib-id>
          <name>
            <surname>Werkhoven</surname>
            <given-names>Zach</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award002">
        <funding-source>
          <institution>Universität Regensburg</institution>
        </funding-source>
        <award-id>iPUR</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4597-9895</contrib-id>
          <name>
            <surname>Rohrsen</surname>
            <given-names>Christian</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award003">
        <funding-source>
          <institution>Fulbright Program</institution>
        </funding-source>
        <award-id>Foreign Student Program</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4597-9895</contrib-id>
          <name>
            <surname>Rohrsen</surname>
            <given-names>Christian</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award004">
        <funding-source>
          <institution>US National Science Foundation</institution>
        </funding-source>
        <award-id>IOS-1557913</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6165-7696</contrib-id>
          <name>
            <surname>de Bivort</surname>
            <given-names>Benjamin L</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award005">
        <funding-source>
          <institution>Alfred P Sloan Foundation</institution>
        </funding-source>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6165-7696</contrib-id>
          <name>
            <surname>de Bivort</surname>
            <given-names>Benjamin L</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award006">
        <funding-source>
          <institution>Klingenstein-Simons Foundation</institution>
        </funding-source>
        <award-id>Fellowship Award in Neuroscience</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6165-7696</contrib-id>
          <name>
            <surname>de Bivort</surname>
            <given-names>Benjamin L</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award007">
        <funding-source>
          <institution>Smith Family Foundation</institution>
        </funding-source>
        <award-id>Odyssey Award</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6165-7696</contrib-id>
          <name>
            <surname>de Bivort</surname>
            <given-names>Benjamin L</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>This research was supported by NSF Graduate Research Fellowship (DGE-1144152) to ZW; an iPUR Fellowship and a Fulbright Fellowship to CR; NSF (IOS-1557913), Alfred P. Sloan Foundation, Klingenstein-Simons Fellowship, and Smith Family Foundation to BLdB. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="6"/>
      <table-count count="1"/>
      <page-count count="24"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>MARGO’s code is available in the MARGO repository on github (<ext-link ext-link-type="uri" xlink:href="https://github.com/de-Bivort-Lab/margo">https://github.com/de-Bivort-Lab/margo</ext-link>). All behavioral data is available on Zenodo (<ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/2596143#.XQP1-vlKiRd">https://zenodo.org/record/2596143#.XQP1-vlKiRd</ext-link>). Instrument schematics are available on github at: de Bivort Lab schematics repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/de-Bivort-Lab/dblab-schematics">https://github.com/de-Bivort-Lab/dblab-schematics</ext-link>).</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>MARGO’s code is available in the MARGO repository on github (<ext-link ext-link-type="uri" xlink:href="https://github.com/de-Bivort-Lab/margo">https://github.com/de-Bivort-Lab/margo</ext-link>). All behavioral data is available on Zenodo (<ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/2596143#.XQP1-vlKiRd">https://zenodo.org/record/2596143#.XQP1-vlKiRd</ext-link>). Instrument schematics are available on github at: de Bivort Lab schematics repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/de-Bivort-Lab/dblab-schematics">https://github.com/de-Bivort-Lab/dblab-schematics</ext-link>).</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>Automated animal tracking methods have become commonplace in the study of behavior. They enable large sample sizes, high statistical power, and more rapid inference of mechanisms giving rise to behavior. Existing animal trackers vary in computational complexity and are often specialized for particular imaging configurations or behavioral measurements. Trackers can assist in a wide range of experimental tasks such as monitoring activity, measuring response to stimuli [<xref rid="pone.0224243.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0224243.ref002" ref-type="bibr">2</xref>], and locating body parts over time [<xref rid="pone.0224243.ref003" ref-type="bibr">3</xref>, <xref rid="pone.0224243.ref004" ref-type="bibr">4</xref>]. Some trackers are designed to track and maintain identities of multiple individuals occupying the same arena [<xref rid="pone.0224243.ref005" ref-type="bibr">5</xref>–<xref rid="pone.0224243.ref008" ref-type="bibr">8</xref>] while others measure the collective activity of groups without maintaining identities or rely on physical segregation of animals to ensure trajectories never collide [<xref rid="pone.0224243.ref009" ref-type="bibr">9</xref>–<xref rid="pone.0224243.ref012" ref-type="bibr">12</xref>]. But few of these trackers are designed as platforms for high throughput, hardware control, and flexible experimental reconfiguration.</p>
    <p>Improvements in machine learning and template matching approaches to object localization and classification have made it possible to efficiently train models that accurately track and classify a variety of animal species and visually distinguish identities of individuals across time [<xref rid="pone.0224243.ref005" ref-type="bibr">5</xref>, <xref rid="pone.0224243.ref006" ref-type="bibr">6</xref>, <xref rid="pone.0224243.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0224243.ref013" ref-type="bibr">13</xref>]. Tracking individual identity in groups requires resolving identities through collisions where bodies are overlapping. FlyTracker and idTracker.ai train classifiers to assign identities to individuals in each frame and also extract postural information such head and limb position. In optimized experiments, these trackers can maintain distinct identities over extended periods with minimal human intervention. Other trackers, such as Ctrax ToxTrac, and Tracktor [<xref rid="pone.0224243.ref007" ref-type="bibr">7</xref>, <xref rid="pone.0224243.ref014" ref-type="bibr">14</xref>, <xref rid="pone.0224243.ref015" ref-type="bibr">15</xref>], track animals by segmenting them from the image background and assign identities by stitching traces together across frames based on changes in position. Although the classification accuracy can be quite high under optimal conditions, these methods generally require human intervention to prevent assignment error from propagating over longer timescales even at low error rates (or they are used for analyses where individual identity is not needed).</p>
    <p>Both approaches to identity tracking can be used to study complex social and individual behaviors, but the computational cost of collision resolution means that tracking is generally performed offline on recorded video data [<xref rid="pone.0224243.ref016" ref-type="bibr">16</xref>]. Furthermore, the need to record high-quality, high-resolution video data can make it challenging to track animals over long experiments. Some methods of postural segmentation require manual addition of limb markers [<xref rid="pone.0224243.ref017" ref-type="bibr">17</xref>], splines fit in post-processing [<xref rid="pone.0224243.ref018" ref-type="bibr">18</xref>], or computationally heavy machine vision in post-processing [<xref rid="pone.0224243.ref003" ref-type="bibr">3</xref>, <xref rid="pone.0224243.ref004" ref-type="bibr">4</xref>, <xref rid="pone.0224243.ref008" ref-type="bibr">8</xref>]. In all cases, the need to separate tracking and recording can be rate-limiting for experiments. Real-time tracking offers the benefits of allowing closed-loop stimulus delivery and a small data footprint due to video data not being retained. In general, real-time tracking methods are less capable of tracking individuals through collisions because they cannot use future information to help resolve ambiguities [<xref rid="pone.0224243.ref011" ref-type="bibr">11</xref>]. For that reason, real-time multiple animal trackers can fall back on spatial segregation of animals to distinguish identities or dispense with identity tracking altogether [<xref rid="pone.0224243.ref012" ref-type="bibr">12</xref>, <xref rid="pone.0224243.ref016" ref-type="bibr">16</xref>]. Some existing real-time trackers can track multiple animals (without maintaining their identity through collisions) in parallel and support a variety of features such as modular arena design, and closed-loop stimulus delivery [<xref rid="pone.0224243.ref019" ref-type="bibr">19</xref>–<xref rid="pone.0224243.ref022" ref-type="bibr">22</xref>].</p>
    <p>The tracking algorithms, software interface, hardware configurations, and experimental goals of available trackers vary greatly. Some packages such as Tracktor and FlyWorld use a simple application programming interface (API) and implement tracking through background segmentation and match identities with Hungarian-like Algorithms that minimize frame-to-frame changes in position [<xref rid="pone.0224243.ref007" ref-type="bibr">7</xref>, <xref rid="pone.0224243.ref016" ref-type="bibr">16</xref>, <xref rid="pone.0224243.ref023" ref-type="bibr">23</xref>]. Ethoscopes are an integrated hardware and software solution that take advantage of the small size and low cost of microcomputers such as the Raspberry Pi. They support modular arenas and peripheral hardware for stimulus delivery [<xref rid="pone.0224243.ref019" ref-type="bibr">19</xref>] and can be networked and operated through a web-based interface to conduct experiments remotely and at scale. Ethoscopes provide a hardware template and API for integrating peripheral components into behavioral experiments, but the Ethoscope tracker is not currently designed to operate independent of the hardware module. BioTracker offers a graphical user-interface (GUI) that allows the user to select from different tracking algorithms with easily customized tracking parameters or import and use a custom algorithm [<xref rid="pone.0224243.ref024" ref-type="bibr">24</xref>].</p>
    <p>We wanted a platform that integrated many of the positive features of these trackers into a single software package, while supporting genome-scale screening experiments in a flexible way that would support the needs of labs that study diverse behaviors. We prioritized 1) fast and accurate individual tracking that could be scaled to very large numbers of individuals or experimental groups over very long timescales, 2) flexibility in the user interface that would permit a diversity of organisms, tracking modes, experimental paradigms, and behavioral arenas, 3) integration of peripheral hardware to enable closed-loop sensory and optogenetic stimuli, and 4) a user-friendly interface and data output format.</p>
    <p>We developed MARGO, a MATLAB based tracking suite, with these goals in mind. MARGO can reliably track up to thousands of individuals simultaneously in real-time for days or longer (with limits only set by logistical challenges such as keeping animals fed). MARGO has two tracking modes that allow it to distinguish either individuals or groups of individuals that are spatially segregated. We show that traces acquired in MARGO are comparably accurate to those of other trackers and are robust to noisy images and changing imaging conditions. We also demonstrate that tracking works reliably with nonspecialist equipment (like smart phone cameras). MARGO provides visual feedback on tracking performance that streamlines parameter configuration, making it easy to setup new experiments.</p>
    <p>Additionally, MARGO can control peripheral hardware, enabling closed-loop individual stimulus delivery in high-throughput paradigms. Using adult fruit flies, we demonstrate three closed-loop [<xref rid="pone.0224243.ref025" ref-type="bibr">25</xref>] applications in MARGO for delivering individualized stimuli to multiple animals in parallel. First we measured individual phototactic bias in Y-shaped arenas. Second we quantified individual optomotor response in circular arenas. In the third assay, we configured MARGO to deliver optogenetic stimulation in real-time. Though MARGO was developed and tested with adult fruit flies, we show that it can be used to track many organisms such as fruit fly larvae, nematodes, larval zebrafish and bumblebees. We packaged MARGO with an easy-to-use graphical user interface (GUI) and comprehensive documentation to improve the accessibility of the software and offer it as a resource to the ethology community. Though it does not perform visual identity recognition or postural limb tracking, we believe that MARGO can meet the needs of many large behavioral screens, experiments requiring real-time stimulus delivery, and users looking to run rapid pilot experiments with little setup.</p>
    <sec id="sec002">
      <title>MARGO workflow</title>
      <p>The core experimental workflow of a MARGO experiment (<xref ref-type="fig" rid="pone.0224243.g001">Fig 1A</xref>) can be briefly summarized as follows: 1) define spatial regions of interest (ROIs) in which flies will be tracked, 2) construct a background image used to separate foreground and background, 3) compute statistics on the distribution of the number of foreground pixels under clean tracking conditions to facilitate detection and correction of noisy imaging, 4) perform tracking. We found that constraining the space in which an animal might be located significantly relaxed the computational requirements of multi-animal tracking. Because MARGO is designed for high-throughput experiments, it needs to be convenient to define up to thousands of ROIs. MARGO has two modes for defining ROIs. The first is automated detection that detects and segments regular patterns of high-contrast regions in the image, such as back-lit arenas. The second prompts the user to manually place grids of ROIs of arbitrary size. In practice, we find that ROI definition typically takes a few seconds but can take as long a few minutes.</p>
      <fig id="pone.0224243.g001" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0224243.g001</object-id>
        <label>Fig 1</label>
        <caption>
          <title>MARGO workflow, tracking algorithm, and sample behavioral box.</title>
          <p>A) Diagram of the user workflow to set up a new tracking experiment. Arrow color indicates whether the setup step is required. Before tracking, users define an input source, define ROIs to track, initialize a background image used to separate foreground and background, and sample the image statistics on a reference of clean tracking. Tracking parameters can be customized at multiple points (blue arrows). B) Flowchart depicting the MARGO’s frame-to-frame tracking routine. Each frame consists of image processing (green) to segment foreground from the background, noise estimation (magenta) to assess the quality of foreground segmentation and determine if the current frame can be tracked, and tracking (cyan) of foreground binary blobs. MARGO’s tracking algorithm skips noisy frames and re-acquires the background image if many consecutive frames are deemed too noisy to track. C) Schematic of a typical behavioral box used for tracking. Behavioral arenas are backlit with an LED illuminator and imaged with an overhead camera. The tracking camera is fitted with an infrared filter to allow light visible to the animals to be controlled independently of the tracking illumination. A diffuser panel between the LED backlight and the behavioral arenas makes the illumination even. The camera and illuminator are both connected to a computer for real-time tracking and control via MARGO. D) Representative views of MARGO’s GUI. Blue inset shows the controls for setting tracking parameters, pink inset the menu options for configuring experiments.</p>
        </caption>
        <graphic xlink:href="pone.0224243.g001"/>
      </fig>
      <p>Following ROI definition, a background image is constructed for each ROI separately. Each background image is computed as the mean or median (as configured by the user) image from a rolling stack of background sample images. Tracking is performed by segmenting binary blobs from a thresholded difference image computed by subtracting each frame from an estimate of the background (<xref ref-type="fig" rid="pone.0224243.g001">Fig 1B</xref>). Background subtraction commonly suffers from two issues with opposing solutions. The first is that subtle changes in the background over time introduce error in the difference image, requiring continuous averaging or reacquisition of the background image. The second is that continuous averaging or reacquisition of the background can make inactive animals appear as part of the background rather than foreground, making them undetectable in the thresholded difference image. Constructing the reference for each ROI separately mitigates these concerns by allowing the reference to be constructed in a piece-meal fashion by adding a background sample image only when the animals have moved from the positions they occupied in previous images of the background stack. The time needed to establish a background image depends on the activity level of the animals and the number of images in the reference stack. We typically find that 3-30 seconds are needed to initialize the background image. Once a background image is established, tracking can begin. In each frame, candidate blobs are identified as the blobs that are both 1) between minimum and maximum size threshold and 2) located within the bounds of an ROI. Candidate blobs are subsequently assigned to ROIs by spatial location. Within each ROI, candidate blobs are matched to centroid traces by minimizing the total frame-to-frame changes in position within each ROI. If the number of candidates exceeds the number of traces in a given ROI, only the candidates closest to the last centroid positions of the traces are assigned. If the number of traces exceeds the number of candidates, the candidates are assigned to the closest traces and any remaining traces are assigned no position (i.e., NaN for that frame).</p>
      <p>Degradation of difference image quality over time (due changes in the background, noisy imaging, and physical perturbation of the imaging setup) constitutes a significant barrier to long term tracking [<xref rid="pone.0224243.ref015" ref-type="bibr">15</xref>]. To address this problem, MARGO continuously monitors the quality of the difference image and updates or reacquires the background image when imaging becomes noisy. We refer to this collective process as noise correction. Prior to tracking, MARGO samples the distribution of the total number of above-threshold pixels under clean imaging conditions to serve as a baseline for comparison. During tracking, the software then continuously calculates that distribution on a rolling basis and reacquires a background image when the rolling sample substantially deviates from the baseline distribution.</p>
    </sec>
    <sec id="sec003">
      <title>Tracking accuracy and noise robustness</title>
      <p>We performed a number of experiments and analyses to assess MARGO’s robustness to tracking errors and comparability with other trackers. In these experiments, we tracked individual flies, each alone in a circular arena, so that individual identity was assured by spatial segregation.</p>
      <p>We assessed the ability of MARGO to handle degradation of the difference image by repeatedly shifting the background image by a small amount in a random direction (2px, 2% of the arena diameter, and 0.16% of the width of the image) to mimic situations where an accidental nudge or vibration shifts the arena. MARGO was used to simultaneously record a movie of individual flies walking in circular arenas and track their centroids. These tracks were the ground truth for this misalignment experiment, and background shifting was implemented digitally on the recorded movie. MARGO reliably detects the changes in difference image statistics associated with each of these events and recovers clean tracking by reacquiring the background, typically within 1 second (<xref ref-type="fig" rid="pone.0224243.g002">Fig 2A and 2B</xref>). Forcing reacquisition of the background image has the disadvantage of resetting the reference with a single image, meaning that a normal background image built by median-filtering multiple frames spaced in time cannot be computed immediately (background images made this way have two benefits: lower pixel noise and fewer tracking dead spots because they do not include moving animals). This typically caused a reduction in tracking accuracy that is brief (&lt;2s) and had little effect on the overall correlation of the tracking data to the ground truth (r = 0.9998). Indeed, we found a small effect on tracking error (mean 3.07± 2.5 pixels, which corresponds to 20% of a fly’s body length at our typical imaging resolution) even when shifting the background every 2 seconds. In our experimental set-ups, noise-induced background reacquisition was relatively rare, typically occurring fewer than 10 times over the duration of a two hour experiment.</p>
      <fig id="pone.0224243.g002" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0224243.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>MARGO tracking accuracy and robustness to imaging noise.</title>
          <p>A) Diagram of the background image shifting scheme used to simulate the kind of background inaccuracy that can happen in long experiments. B) Trial-triggered median tracking error centered on reference shifting. C) Median error of tracking performed on the same video at different levels of compression. Below: sample images. D) Median tracking error versus different levels of added noise. Pixel noise was manually added to the binary threshold image downstream. Below: sample images with estimated fly position (red circle). E) Sample trace comparison and F) log distribution of tracking error between traces acquired from the same video in both MARGO and Ctrax. The 95% confidence interval of the above means are shown but are within the line thickness.</p>
        </caption>
        <graphic xlink:href="pone.0224243.g002"/>
      </fig>
      <p>We tested MARGO’s sensitivity to video compression by compressing and tracking a video previously captured during a real-time tracking session. The centroid position error of traces acquired from compressed videos were calculated by comparing them to the ground-truth traces acquired on uncompressed images in real time. MARGO showed sub-pixel median tracking error up to 3000-fold compression (<xref ref-type="fig" rid="pone.0224243.g002">Fig 2C</xref>). We further tested the robustness of MARGO to noisy imaging by digitally injecting pixel noise (by randomly setting each pixel to True with a fixed probability) into the thresholded difference image of each frame of a video previously acquired and tracked under clean conditions. Noise was added downstream of noise correction and upstream of tracking to simulate tracking under conditions where noise correction is poorly calibrated. We observed sub-pixel median tracking error up to 20%pixel noise (<xref ref-type="fig" rid="pone.0224243.g002">Fig 2D</xref>). In practice, we find it easy to create imaging conditions with noise levels &lt;1% pixel noise without the use of expensive hardware.</p>
      <p>To compare the tracking accuracy of MARGO to a widely used animal tracker, we fed uncompressed video captured during a live tracking session in MARGO into Ctrax [<xref rid="pone.0224243.ref014" ref-type="bibr">14</xref>] and measured the discrepancy between the two sets of tracks. Overall we found a high degree of agreement between traces acquired in MARGO and Ctrax (<xref ref-type="fig" rid="pone.0224243.g002">Fig 2E and 2F</xref>). We attribute the majority of discrepancies to minor variations in blob size and shape arising from differences in background segmentation. It is worth noting that although Ctrax flagged many frames for manual inspection and resolution, for comparability we opted not to resolve these frames and instead restricted our analysis to the automatically acquired traces. (Ctrax primarily uses these flags to draw user attention to tracking ambiguities through collisions, which did not happen in our experiment because flies were spatially segregated.) Manual inspection of tracked frames with error larger than 1 pixel revealed that most major discrepancies occurred in one of two ways: 1) short periods between the death and birth of two traces on the same animal in Ctrax, or 2) identity swaps in Ctrax between animals in neighboring arenas. These errors may be attributable to our inartful use of Ctrax.</p>
    </sec>
    <sec id="sec004">
      <title>High-throughput behavioral screens</title>
      <p>We designed MARGO with high-throughput behavioral screens in mind, with hundreds of experimental groups, each potentially containing hundreds or thousands of animals. Many features in MARGO’s GUI have been included to reduce the time needed to establish successful tracking, including automated ROI detection and visualizations of object statistics and the effects of parameters. Configuring tracking for experiments with hundreds of individuals typically took between 2-5 minutes. Additionally, we added the ability to save and load parameter and experimental configurations.</p>
      <p>The speed of the tracking algorithm permits the tracking of very large numbers of animals simultaneously in a single field of view (facilitating certain experimental designs, like testing multiple experimental groups simultaneously). To demonstrate MARGO’s throughput, we continuously tracked 960 flies at 8Hz for more than 6 days (<xref ref-type="supplementary-material" rid="pone.0224243.s001">S1 Video</xref>). Flies were singly housed in bottomless 96-well plates (<xref ref-type="fig" rid="pone.0224243.g003">Fig 3A</xref>) placed on top of food and were imaged by a single overhead camera. The appearance of the arenas changed substantially over 6 days due to evaporation of water from the fly food media, condensation on the well plate lids, and egg laying. Despite these changes, the quality of centroid traces and acquisition rates appeared stable throughout the experiment (<xref ref-type="fig" rid="pone.0224243.g003">Fig 3B</xref>). The overall activity level of flies decreased over the duration of the experiment (<xref ref-type="fig" rid="pone.0224243.g003">Fig 3C</xref>). The flies’ log-speed distributions generally exhibited two distinct modes: a low mode consistent with frame-to-frame tracking noise and a higher mode consistent with movement of the flies (<xref ref-type="fig" rid="pone.0224243.g003">Fig 3D</xref>) [<xref rid="pone.0224243.ref026" ref-type="bibr">26</xref>, <xref rid="pone.0224243.ref027" ref-type="bibr">27</xref>]. Individual flies varied in the relative abundance of these two modes. We defined a movement threshold as the local minimum between these two modes and parsed individual speed trajectories into movement bouts by identifying periods of continuous movement above the threshold. Sorting flies by the average length of their movement bouts revealed a trend of increasing mean and magnitude of the higher “movement” mode (<xref ref-type="fig" rid="pone.0224243.g003">Fig 3D</xref>), i.e., flies that walked longer tended to walk faster.</p>
      <fig id="pone.0224243.g003" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0224243.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>MARGO tracking throughput.</title>
          <p>A) Image of 10 single-fly housing plates from the overhead tracking camera. B) Sample tracks from the same fly on days 1 and 6. C) Fly speeds at three representative scales: heatmap of individual speed over the duration of the experiment (top), heatmap of individual speed from a three hour period (middle), raw speed traces from twenty individuals from a three minute period (bottom). Activity of most flies decreased over the six day duration. D) Individual kernel density estimates of log speed over the duration of the experiment. Column order was sorted by mean individual bout length in ascending order. E) Acquisition frame rate as a function of number of ROIs tracked in a simulated experiment. The acquisition rate decreased exponentially, consistent with a linear increase in inter-frame interval as a function of ROI number.</p>
        </caption>
        <graphic xlink:href="pone.0224243.g003"/>
      </fig>
      <p>To measure MARGO’s performance as a function of the number of ROIs, we recorded the mean real-time tracking rate while varying the number of tracked ROIs from a high-resolution (7.4MP) video composed of the same single-arena video repeated 2400 times in a grid. We found that the frame-to-frame latency scaled linearly as a function of the number of ROIs tracked (<xref ref-type="fig" rid="pone.0224243.g003">Fig 3E</xref>). On modern computer hardware (intel i7 4.0GHz CPU), we measured tracking rates of 160Hz for a single ROI down to 5Hz for 2400 ROIs. MARGO could plausibly track up to 5000 animals at lower rates (1 Hz), potentially fast enough for experiments monitoring changes in activity level changes over long timescales, like circadian experiments.</p>
      <p>Large behavioral screens can potentially generate hundreds of hours of data on thousands of animals and massive data files even without recording videos. We found that experiments tracking many hundreds of animals over multiple days made raw data files too large to hold in memory on typical computers. We designed a custom data container and an API to easily work with data stored in large binary files. MARGO’s raw data API includes methods to batch-process multiple tracking experiments or single datasets too large to hold in memory (see user documentation).</p>
    </sec>
    <sec id="sec005">
      <title>Customization and versatility</title>
      <p>To demonstrate MARGO’s ability to prototype experiments without the need for specialized hardware, we ran a minimal tracking experiment using only commonly available materials. Individual fruit flies were placed into the wells of a standard 48 well culture plate. The plate was put in a cardboard box (to reduce reflections) on a sheet of white paper as a high contrast background. Movies were recorded on a 1.3MP smartphone camera using natural room light as illumination and imported into MARGO for tracking (<xref ref-type="supplementary-material" rid="pone.0224243.s002">S2 Video</xref>). Tracks and movement bouts acquired under these conditions showed no apparent differences to those acquired under our normal experimental conditions (custom arenas over diffused LED illuminators in light-sealed imaging boxes). However, we did find that the lower contrast illumination of this setup increased imaging noise and narrowed the range of parameters that worked for segmentation, but had no apparent effect on the accuracy of traces once calibrated.</p>
      <p>MARGO was developed for high-throughput ethology in fruit flies, but many small organisms used for high-throughput behavior are more translucent than adult flies. To assess MARGO’s tracking robustness on such organisms, we used MARGO to track videos of larval <italic>Danio rerio</italic>, <italic>Caenorhabditis elegans</italic>, larval <italic>Drosophila</italic> (<xref ref-type="supplementary-material" rid="pone.0224243.s003">S3</xref>–<xref ref-type="supplementary-material" rid="pone.0224243.s005">S5</xref> Videos), and also bumblebees (<italic>Bombus impatiens</italic>) (<xref ref-type="supplementary-material" rid="pone.0224243.s006">S6 Video</xref>). As expected, the translucency of these organisms narrowed the functional range of some tracking parameters, but MARGO’s real-time tracking feedback made it easy to dial in these parameters. Sample traces acquired from other organisms were qualitatively similar to those acquired with adult flies, suggesting that MARGO works with a variety of organisms.</p>
      <p>We gave MARGO a graphical user interface (GUI) to make it accessible to users unfamiliar with MATLAB or programming in general (<xref ref-type="fig" rid="pone.0224243.g001">Fig 1D</xref>). We generally find that new users easily learn to use both the core work-flow and parameter customization. The typical setup time of a tracking experiment for trained users ranged between a few seconds (with saved parameter profiles) to a few minutes (under novel imaging conditions). The utility of the GUI extends to customization of analysis, visualization, and input/output sources such as videos, cameras, displays, and COM devices. Descriptions and instructions for these use cases, including defining custom experiments via the API, are available in MARGO’s documentation.</p>
    </sec>
    <sec id="sec006">
      <title>Integrating hardware for closed-loop experiments</title>
      <p>Real-time tracking allows the delivery of closed-loop stimuli that depend on the behavior of animals. MARGO offers native support for the hardware needed for closed-loop experiments including: cameras for real-time image acquisition, projectors/displays for visual stimuli, and serial COM devices for digital control of other peripheral electronics. COM devices include programmable microcontrollers (like Arduinos) that make it relatively simple to control a wide variety of devices. MARGO was designed to detect and communicate with such COM devices devices to integrate real-time feedback from sensors and coordinate closed-loop control of peripheral hardware.</p>
      <p>We ran experiments with a custom circuit board to measure individual phototactic preference (the “LED Y-maze”). In this assay, individual flies explored symmetrical Y-shaped arenas with LEDs at the end of each arm (<xref ref-type="fig" rid="pone.0224243.g004">Fig 4A and 4B</xref>, <xref ref-type="supplementary-material" rid="pone.0224243.s007">S7 Video</xref>). For all arenas in parallel, real-time tracking detected which arm the fly was in at each frame. At the start of each trial, an LED was randomly turned on in one of the unoccupied arms. Once the fly walked into one of these two new arms, MARGO turned off all the LEDs in that arena. Immediately after these choice events, a new trial was initiated by randomly turning on an LED in one of the now unoccupied arms. This process repeated for each fly independently over two hours, and MARGO recorded which turns were toward a lit LED (positive phototaxis) and which were away (negative phototaxis) (<xref ref-type="fig" rid="pone.0224243.g004">Fig 4C</xref>). Tiling many such mazes on a single board yielded the experimental throughput for which MARGO is well-suited. Overall, we recorded choices from over 3,600 individuals, representing more than 830,000 choices in total.</p>
      <fig id="pone.0224243.g004" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0224243.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>High-throughput phototactic assay in Y-shaped arenas.</title>
          <p>A) Schematic of the behavior box with an LED Y-maze array in place. B) Diagram of a single LED Y-maze and trial structure. New trials initiate by turning on (yellow) an LED in one of the two unoccupied maze arms. The trial ends when then animal turns into a new arm and the lit LED is turned off (gray). Each turn is scored for both handedness and phototactic preference. C) Raw turn data for two sample flies. Each individual trial consists of both a phototactic and handedness choice. Individual mean turn biases range from 0 (all left turns) to 1 (all right turns). Light biases range from 0 (all photopositive turns) to 1 (all photonegative turns). D) Comparison of individual average phototactic bias distributions for different wild-type fly lines. Blind flies (NorpA) and flies tested with all LEDs turned off (DGRP-105 dark) are included as negative controls. Horizontal dashed line indicates random bias at p = 0.5. E) Distribution of individual average phototactic biases for the same cohort of flies over the first 8 days post-eclosion. F) Individual mean phototactic and right turn biases calculated on all trials sub-divided by into trials where the lit arm of the maze was to the right or left of the choice point. Data points are colored by either the individual mean right turn bias (left panel) over all trials or the individual mean phototactic bias (right panel) over all trials. The rank orders of both turn bias and phototactic bias are anti-correlated (r = -0.38 and -0.63 respectively) between trials where right or left arm was lit.</p>
        </caption>
        <graphic xlink:href="pone.0224243.g004"/>
      </fig>
      <p>To assess MARGO’s capacity to reveal behavioral differences between genotypes, we tested a variety of wild type strains in the LED Y-maze. All strains exhibited a significant average positive phototactic bias (mean phototactic indices ranging from 0.55 to 0.80, p-values&lt;&lt;10<sup>−6</sup> by t-test). In contrast, blind flies (<italic>Norp-A</italic> mutants) and flies under identical circumstances but with unpowered LEDs, showed mean “preferences” indistinguishable from 0.5, consistent with random choices (<xref ref-type="fig" rid="pone.0224243.g004">Fig 4D</xref>). The wild type lines tested showed significant variation in population mean (one-way ANOVA; F(6,1943) = 118.2, p&lt;&lt;10<sup>−6</sup>) and population variability (one-way ANOVA on Levene-transformed data; F(6,1943) = 19.29, p&lt;&lt;10<sup>−6</sup>).</p>
      <p>We collected LED Y-maze data from a single cohort of wild-type (Berlin-K, n = 144) flies over the first 8 days post-eclosion to profile phototaxis throughout development (<xref ref-type="fig" rid="pone.0224243.g004">Fig 4E</xref>). Flies displayed a significant average negative light bias (0.417, p&lt;&lt;10<sup>−6</sup>) on the day of eclosion but transitioned to a positive light bias of 0.663 (p&lt;&lt;10<sup>−6</sup>) by 7 days post-eclosion. This assay has structural similarities to an assay we previously used to measure locomotor handedness [<xref rid="pone.0224243.ref028" ref-type="bibr">28</xref>], the tendency of individuals to turn left or right when going through the center of the arena. In the LED Y-maze assay, locomotor left-right decisions were made in superposition with light-dark choices. Flies typically make hundreds of choices over the course of an experiment, giving us enough data to examine the turn bias of individuals in all four left-right/light-dark combinations. We divided trials into two groups based on whether the lit LED appeared to the right or left of the choice point. We found that the mean turn bias but not the mean phototactic bias differed between these two conditions (<xref ref-type="fig" rid="pone.0224243.g004">Fig 4F</xref>) [<xref rid="pone.0224243.ref029" ref-type="bibr">29</xref>]. Categorizing trials this way revealed that the rank order of both turn bias and phototactic bias are anti-correlated (r = -0.38 and r = -0.63 respectively) between the two conditions, suggesting that both individual phototactic bias and locomotor handedness bias affect each choice.</p>
      <p>We adapted an optomotor paradigm [<xref rid="pone.0224243.ref030" ref-type="bibr">30</xref>] to a high-throughput configuration to test MARGO’s ability to deliver a precise closed-loop stimulus with low latency. In this paradigm, an optomotor stimulus consisting of a high-contrast, rotating pinwheel, centered on a fly, is projected on the floor of the arena in which it is walking freely. On average, such optomotor stimuli evoke a turn in the direction of the rotation to stabilize the visual motion [<xref rid="pone.0224243.ref031" ref-type="bibr">31</xref>]. The center of the pinwheel follows the position of the fly as it moves around the arena so that the only apparent motion of the stimulus is around the fly. Thus, this stimulus is closed-loop with respect to each animal’s position and open-loop with respect to its rotation velocity.</p>
      <p>To implement this paradigm, we constructed a behavioral platform with a camera and an overhead mounted projector targeting an array of flat circular arenas (<xref ref-type="fig" rid="pone.0224243.g005">Fig 5A</xref>). To target a stimulus to a fly based on its coordinates in the tracking camera, MARGO had to learn the mapping of camera coordinates to projector coordinates. We added a feature to locate small dots displayed by the projector with the camera. From the position of these dots in camera coordinates, we constructed a registration mapping from the camera FOV to the projector display field. Using this mapping, we programmed MARGO to use the real-time positions of flies to project pinwheel stimuli independently to 48 freely moving individuals simultaneously (<xref ref-type="fig" rid="pone.0224243.g005">Fig 5B</xref>, <xref ref-type="supplementary-material" rid="pone.0224243.s008">S8 Video</xref>). To ensure faithful coordination between the tracking and stimulus, the tracking rate was matched to the refresh rate of the display at 60Hz (which is below the flies’ flicker-fusion rate, meaning this stimulus produces beta movement apparent motion [<xref rid="pone.0224243.ref032" ref-type="bibr">32</xref>]; see <xref ref-type="sec" rid="sec008">Discussion</xref>).</p>
      <fig id="pone.0224243.g005" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0224243.g005</object-id>
        <label>Fig 5</label>
        <caption>
          <title>High-throughput optomotor assay implementation in MARGO.</title>
          <p>A) Schematic of the optomotor arenas and behavioral box. B) Diagram of a single arena and optomotor stimulus. Trials begin with a pinwheel stimulus, centered on the fly. For each trial, the rotational direction (red arrow) of the stimulus is randomized. As the animal moves, the pinwheel position is updated to stay centered on the fly. Trials end when the stimulus is removed after 2s. C) Four sample raw individual angular velocity time series. Flies typically respond to optomotor stimuli by turning in the direction of the rotation of the stimulus. Shaded rectangles indicate the direction of pinwheel rotation, line color angular acceleration. D) Trial-triggered average optomotor response across all individuals. Change in body angle (left) is relative to body angle at stimulus onset. Sign indicates turns with (positive) or against (negative) the direction of stimulus rotation. E) Comparison of the observed distribution of individual average optomotor indices (n = 1,860) to the distribution expected under a null model in which all flies turn with identical statistics, generated by bootstrap resampling. F) Population average optomotor index as a function of stimulus contrast (0-1). Pinwheel contrast was randomly varied on a trial-by-trial basis. G) Average optomotor index as a function of stimulus spatial frequency and stimulus angular velocity.</p>
        </caption>
        <graphic xlink:href="pone.0224243.g005"/>
      </fig>
      <p>While optimizing this assay, we observed that optomotor responses could be reliably elicited, provided individuals were already moving when the pinwheel was initiated. This is consistent with previous observations of optomotor responses depending on arousal state [<xref rid="pone.0224243.ref033" ref-type="bibr">33</xref>, <xref rid="pone.0224243.ref034" ref-type="bibr">34</xref>]. We therefore configured MARGO to stimulate with the pinwheel each fly when: 1) it was moving 2), a minimum inter-trial interval had passed, and 3) it was a minimum distance away from the edge of the arena. The inter-trial interval helped prevent behavioral responses from adapting, and provided a baseline measurement period where no stimulus was present. Minimum distance to the edge ensured that the stimulus occupied a significant portion of the animal’s field of view.</p>
      <p>We characterized the optomotor behavior of wild type flies in a two hour experiment with two second pinwheel stimuli and a minimum inter-trial interval of 2s (<xref ref-type="fig" rid="pone.0224243.g005">Fig 5C</xref>). In total, over 300,000 trials were recorded from more than 1,800 flies, assayed in groups of up to 48 flies simultaneously. For each fly, we calculated an optomotor index [<xref rid="pone.0224243.ref035" ref-type="bibr">35</xref>] as the fraction (normalized to [-1,1]) of body angle change that occurred in the same direction as the stimulus rotation over the duration of the stimulus. On average, flies displayed reliable optomotor responses (mean index = 0.358, p&lt;&lt;10<sup>−6</sup>) when stimulated with high-contrast pinwheels (<xref ref-type="fig" rid="pone.0224243.g005">Fig 5D</xref>). We observed significant individual variation in optomotor index (<xref ref-type="fig" rid="pone.0224243.g005">Fig 5E</xref>) as well as the number of trials each fly experienced, reflecting individual variation in the fraction of time walking.</p>
      <p>To characterize the psychometric properties of this behavior, we randomly varied pinwheel contrast, angular velocity, and spatial frequency simultaneously on a trial-by-trial basis. Mean optomotor indices increased with pinwheel contrast, plateauing over much of the dynamic range of the projector, starting around 25% contrast (<xref ref-type="fig" rid="pone.0224243.g005">Fig 5F</xref>). Similarly, optomotor indices increased with both stimulus spatial frequency and angular speed, peaking at 0.18 cycles/degree and 360 degrees/s respectively (<xref ref-type="fig" rid="pone.0224243.g005">Fig 5G</xref>). The population mean optomotor index reversed at high combined values of spatial frequency and angular speed due to the apparent reversal of the stimulus at frequencies higher than the refresh rate of the projector.</p>
    </sec>
    <sec id="sec007">
      <title>High-throughput optogenetic experiments</title>
      <p>To test the versatility of MARGO, we used its API to implement high-throughput closed-loop optogenetic experiments using a digital projector to target individual flies expressing CsChrimson [<xref rid="pone.0224243.ref036" ref-type="bibr">36</xref>, <xref rid="pone.0224243.ref037" ref-type="bibr">37</xref>] with flashing red light contingent on their behavior (<xref ref-type="fig" rid="pone.0224243.g006">Fig 6</xref>). We used a commericial Optoma S310e DLP projector which, when displaying red light ([255 0 0] RGB code), had a spectral range of 570 nm to 720 nm with a peak at 595 nm. Light stimulation frequency was set to the projector refresh rate (60Hz) and its intensity to the maximum, if not otherwise specified.</p>
      <fig id="pone.0224243.g006" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0224243.g006</object-id>
        <label>Fig 6</label>
        <caption>
          <title>Optogenetic closed-loop experiments with MARGO.</title>
          <p>A.1) Top: Fraction of time spent in the arm of a Y-maze which was triggered to optogenetically stimulate flies expressing CsChrimson in bitter taste receptor neurons. Bottom: Portion of arm entries into the reinforced arm. Light green boxes are control flies not fed ATR; dark green experimental flies are fed ATR. Red line indicates chance rates. Individual points are flies. Even at the lowest intensity (50%), flies show a robust avoidance of the reinforced arm in a Y-Maze. Increasing light intensity (x-axis) further decreases (slightly) the lit arm occupancy time and the lit arm entries even further. Here and elsewhere *:p&lt;0.05, **:p&lt;0.01, ***:p&lt;0.001. A.2) As in A.1, but varying the frequency of the optogenetic stimulation. Frequency had little effect on the occupancy or rate of entry into the reinforced arm. A.3) Blind <italic>norpA<sup>P24</sup>;Gr28bd+TrpA1&gt;Chrimson</italic> flies, expressing Chrimson in heat-sensitive neurons, also show decreased occupancy in the lit arm, whereas the fraction of entries into the lit arm appears unchanged compared to control flies not fed ATR. B.1) Example walking speed traces of an individual fly in circular arenas stimulated upon when above or below (depending on trial period) a speed threshold 4 px/s. Line color indicates which reinforcement paradigm was used in each period. Initial (t1) and final (t4) baseline periods are highlighted (see B.3). Green line indicates the speed threshold. B.2) Walking speeds for all periods and all flies. <italic>norpA<sup>P24</sup>;Gr28bd+Trp A1&gt;Chrimson</italic> flies increase their walking speed specifically during periods when stimulation is contingent on slow walking or resting (lit when stop), compared to lit when running periods and controls without the optogenetic effector <italic>norpA<sup>P24</sup>;UAS-Chrimson</italic>. B.3) Walking speed during the initial baseline period did not differ between experimental and control flies (t1). In contrast, after three reinforcement periods, walking speed in experimental flies was significantly lower than in control flies (t4). All flies in B were fed with all-trans-retinal.</p>
        </caption>
        <graphic xlink:href="pone.0224243.g006"/>
      </fig>
      <p>As a first experiment, we tracked the flies in a Y-Maze shaped like that in <xref ref-type="fig" rid="pone.0224243.g004">Fig 4A</xref>, but with no LEDs. Whenever a fly entered a designated arm, MARGO projected red light on it. Flies expressed CsChrimson in bitter-taste receptor neurons using the driver <italic>Gr66a-GAL4</italic>. MARGO recorded the fractional time spent in the lit arm (occupancy) and the number of entries into the lit arm (entries). We observed a modest increase in the aversive effects of optogenetic stimulation (reduced occupancy and entries) with light intensity (<xref ref-type="fig" rid="pone.0224243.g006">Fig 6A.1</xref>), whereas increasing stimulation frequency did not elicit any obvious change in aversion (<xref ref-type="fig" rid="pone.0224243.g006">Fig 6A.2</xref>). To test the robustness of the experiment to changes in the fictive conditioning stimulus, and to exclude the effects of visual cues, we expressed CsChrimson in heat sensitive neurons targeted by <italic>Gr28bd+TrpA1-GAL4</italic> in <italic>norpA<sup>P24</sup></italic> blind flies. This experiment is conceptually analogous to spatial learning in the heat-box, where flies are trained to avoid one side of a dark, heatable chamber [<xref rid="pone.0224243.ref038" ref-type="bibr">38</xref>–<xref rid="pone.0224243.ref045" ref-type="bibr">45</xref>]. While blindness only marginally affected the time spent in the lit arm (the blind flies with Chrimson driven in heat-sensitive neurons still avoided occupying the lit arm at similar rates to seeing flies with Chrimson in bitter-sensitive neurons), the reduction in entries into the lit arm, observed in the seeing flies, was abolished (<xref ref-type="fig" rid="pone.0224243.g006">Fig 6A.3</xref>). These results suggest that vision is a key sensory modality informing the decision to enter an arm, but not for the decision of how much time to spend in an arm, once entered.</p>
      <p>Analogous to a different heat-box experiment [<xref rid="pone.0224243.ref046" ref-type="bibr">46</xref>], optogenetic stimulation was made contingent on locomotor speed rather than position. In the same circular arenas as the optomotor experiments above (<xref ref-type="fig" rid="pone.0224243.g005">Fig 5A</xref>), the red light was switched on under two distinct conditions enforced in separate experimental blocks: 1) whenever the walking speed of the flies exceeded a threshold of 6.8 mm/s and 2) whenever the walking speed fell below that same threshold. The overall 64 minute experimental protocol consisted of 8 periods of 8 minutes each. The periods alternated between a baseline period, where the light was permanently switched off, and the two reinforcement periods where the light was contingent on either fast walking or slow walking/resting, respectively (<xref ref-type="fig" rid="pone.0224243.g006">Fig 6B.1 and 6B.2</xref>). As in the heat-box experiments, flies increased their walking speed when punished for walking too slowly. However, punishing fast walking failed to significantly decrease walking speed. Reminiscent of the induction of ‘learned helplessness’ in yoked control animals in the heat-box [<xref rid="pone.0224243.ref046" ref-type="bibr">46</xref>], flies trained with these conflicting schedules of punishment, significantly reduced their walking speed in the baseline periods without optogenetic stimulation, in comparison to control animals which did not express any CsChrimson (<xref ref-type="fig" rid="pone.0224243.g006">Fig 6B.3</xref>).</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec008">
    <title>Discussion</title>
    <p>We developed MARGO as a platform for a wide variety of behavioral paradigms and organisms, all at high throughput for large-scale experiments (like genetic screens, measuring individuality and characterizing psychometric response curves). MARGO’s tracking algorithm, interface, and data footprint are lightweight, making it perform well in applications like real-time centroid tracking. Conversely, it is not made for harder computational tasks like maintaining the identity of multiple animals in the same compartment. But the ability to rapidly define ROIs, and track individuals in them, enables MARGO to easily coordinate low-latency, closed-loop stimulation for psychometric and optogenetic experiments. Furthermore, by packaging MARGO in a GUI and thoroughly documenting its usage and API, we hope to make it accessible both to new users with little programming experience and advanced users developing custom experimental paradigms.</p>
    <p>When ROI boundaries are drawn along physical barriers, individual identities can be maintained indefinitely through ROI identity, thus removing the requirement for human supervision and intervention. We found that insisting on spatial segregation ultimately relaxes the computational requirements enough that thousands of individuals can be tracked in real time. In the future, real-time tracking that maintains individual identity without physical barriers may be possible, perhaps as an extension of current methodologies that exploit neural networks to track individuals offline [<xref rid="pone.0224243.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0224243.ref013" ref-type="bibr">13</xref>]. MARGO’s interface assists in the automated definition of up to thousands of ROIs. An ROI-based architecture can also be used to distinguish groups rather than individual identities by separating groups into distinct arenas. This configuration therefore allows multiple groups, as well as individuals, to be tested in parallel.</p>
    <p>Long-term automated behavioral measurement has great potential in the fields of sleep, circadian rhythms, pharmacology, and aging, among others. MARGO offers many features useful for activity measurement over long timescales, including rapid experimental setup, small data footprint, and built-in utilities for handling large data sets. For example, over a week we tracked the behavior of 960 flies simultaneously as they walked in the wells of custom 96-well plates (<xref ref-type="fig" rid="pone.0224243.g003">Fig 3</xref>). Such throughput can be applied to comparisons among individuals, genotypes, or treatment groups.</p>
    <p>With built-in hardware support for cameras, displays, and peripheral electronics, MARGO enables open- and closed-loop stimulus-evoked ethology on a large scale. Built-in features supporting projector displays, like camera-projector registration, facilitate a wide variety of visual and optogenetic experiments (Figs <xref ref-type="fig" rid="pone.0224243.g005">5</xref> and <xref ref-type="fig" rid="pone.0224243.g006">6</xref>). Native detection and communication with serial COM devices further extends these capabilities by providing a generic interface for a wide variety of peripheral devices, such as the LED controllers we used for the LED Y-maze (<xref ref-type="fig" rid="pone.0224243.g004">Fig 4</xref>). Taken together, MARGO is a multi-purpose platform for coordinating hardware inputs and outputs for high-throughput ethology.</p>
    <p>Between our two closed-loop visual stimulation experiments (LED Y-maze and optomotor assay), we screened nearly 5,000 animals over hundreds of thousands of trials, allowing the precise characterization of both individual- and population-level behavior. With the experiments themselves representing less than a week of testing, these platforms could be used for large behavioral screens of hundreds of strains. In the LED Y-maze, we showed that individuals displayed idiosyncratic biases in both phototactic preference and locomotor handedness simultaneously, as observed previously in separate assays [<xref rid="pone.0224243.ref028" ref-type="bibr">28</xref>, <xref rid="pone.0224243.ref047" ref-type="bibr">47</xref>]. The wild-type fly lines we screened displayed population level differences in both mean preference and variability in phototactic bias [<xref rid="pone.0224243.ref029" ref-type="bibr">29</xref>]. Furthermore, the mean of one strain (Berlin-K) shifted from negative to positive over the first week post-eclosion, as was reported previously [<xref rid="pone.0224243.ref048" ref-type="bibr">48</xref>]. Interestingly, we observed that flies with a high right-turn probability were more likely to turn toward the light when it was to the right of the choice point and that the opposite was true of flies with a high left-turn probability. We observed a similar but stronger effect of phototactic bias on locomotor handedness (e.g. flies with a high phototactic bias were more likely to turn toward the right when the light was on the right). Together these results demonstrate measurable effects of phototactic bias and handedness in a task that probes both simultaneously. Thus, we found that both individual light and handedness biases influence light/turn behavior on a choice-by-choice basis. As responses to light are ethologically relevant [<xref rid="pone.0224243.ref049" ref-type="bibr">49</xref>], the interplay of individual behavioral biases may have fitness consequences for wild flies.</p>
    <p>In the optomotor experiment, we demonstrated that, using closed-loop stimuli delivered from a projector, MARGO can quantify individual optomotor responses of dozens of flies simultaneously. Consistent with previous findings [<xref rid="pone.0224243.ref033" ref-type="bibr">33</xref>, <xref rid="pone.0224243.ref034" ref-type="bibr">34</xref>], we saw that stationary flies did not exhibit strong optomotor responses, consistent with the idea that this reflexive behavior may be state-dependent [<xref rid="pone.0224243.ref050" ref-type="bibr">50</xref>–<xref rid="pone.0224243.ref054" ref-type="bibr">54</xref>]. While all animals tested exhibited the optomotor response to some degree, we observed a broad distribution of individual optomotor indices, suggesting that individuals respond idiosyncratically to the same stimulus, as has been found previously in other spontaneous and stimulus-evoked behaviors [<xref rid="pone.0224243.ref017" ref-type="bibr">17</xref>, <xref rid="pone.0224243.ref028" ref-type="bibr">28</xref>, <xref rid="pone.0224243.ref047" ref-type="bibr">47</xref>, <xref rid="pone.0224243.ref049" ref-type="bibr">49</xref>, <xref rid="pone.0224243.ref055" ref-type="bibr">55</xref>]. We suspect that the success of this assay may be partially due to tightly centering the pinwheel centered on the animal as it moves, which is possible because of MARGO’s low latency.</p>
    <p>Our optogenetic experiments provide a proof of principle that high-throughput closed-loop manipulation of neural activity is feasible (<xref ref-type="fig" rid="pone.0224243.g006">Fig 6</xref>). Using different driver lines to activate neurons under both spatial (<xref ref-type="fig" rid="pone.0224243.g006">Fig 6A</xref>) and locomotor (<xref ref-type="fig" rid="pone.0224243.g006">Fig 6B</xref>) contingencies, optogenetic stimulation reliably altered fly behavior in the expected directions. These experiments also revealed that flies use visual elements of the projector rig to orient when the stimulus was nominally off, and that optogenetic punishment can induce learning effects outlasting the stimulation itself. These results also remind us about a general limitation of studying freely moving animals: the large number of degrees of freedom that such behavior enables can make it difficult to causally relate biological manipulations to specific mechanisms. For instance, without prior knowledge of the function of the optogenetically targeted neurons, it would not have been immediately clear if our manipulation affected reinforcing neurons or neurons involved in motor control, which could also lead to altered occupancy of the lit arm in the Y-Maze. Likewise, a screen for neurons that are required for non-random entry into optogenetically-reinforced arms of the Y-maze would yield blind flies, as the flies in our assay apparently use visual cues to identify which arms are reinforced before entering them.</p>
    <p>Behavioral experiments are frequently more complex than tracking objects in a dish. Such experiments could require complex arena geometries, data streams from external sensors, control of peripheral hardware, and access to measurements of behavior in real time. Bonsai is an open-source, visual programming framework for combining input and output streams of multiple devices such as cameras, microcontrollers, and other peripheral devices and defining experimental architecture from beginning to end. MARGO can manage these same features, making both programs well suited to implementing new behavioral paradigms. As a point of contrast, MARGO makes assumptions about the core workflow of the experiment and therefore only offers customization at a particular node in the workflow (between tracking and data output on each frame). We designed MARGO around a workflow what we think is likely to be useful to many users, and can be tweaked to meet specific experimental needs, hoping that this structure will simplify implementation of custom experiments. Specifically, MARGO can automatically generate templates for new experiments with custom inputs and outputs within the GUI. We have also included a tutorial for defining custom experiments in MARGO’s documentation. In practice, we find that new experiments can typically be defined in one or two custom functions, given familiarity with the API.</p>
    <p>Animal tracking platforms are evolving to meet the diverse needs of the ethology, neuroscience and behavioral genetics communities. See <xref rid="pone.0224243.t001" ref-type="table">Table 1</xref> for a comparison of features of several contemporary tracking programs. Trackers can be broadly described as falling into one of two categories: 1) real-time trackers [<xref rid="pone.0224243.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0224243.ref012" ref-type="bibr">12</xref>, <xref rid="pone.0224243.ref019" ref-type="bibr">19</xref>, <xref rid="pone.0224243.ref020" ref-type="bibr">20</xref>, <xref rid="pone.0224243.ref022" ref-type="bibr">22</xref>, <xref rid="pone.0224243.ref024" ref-type="bibr">24</xref>] with potential for high throughput and hardware control and 2) offline trackers [<xref rid="pone.0224243.ref005" ref-type="bibr">5</xref>–<xref rid="pone.0224243.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0224243.ref014" ref-type="bibr">14</xref>, <xref rid="pone.0224243.ref015" ref-type="bibr">15</xref>] with the potential to maintain individual identities (without using spatial segregation) and/or track body parts. Hardware integration is a natural extension of real-time trackers since many stimulus paradigms are contingent on behavior. While trackers in the second category are currently unsuitable for real-time applications, they offer the notable benefits of being able to study fine-scale postural and social behaviors. The ability to record video in parallel with tracking and peripheral hardware control means MARGO can be used upstream of offline trackers, making it possible to analyze social dynamics or postural features in response to closed-loop stimuli. Among this array of options, MARGO is optimized for the throughput characteristic of <italic>Drosophila</italic> and other genetic model organisms like <italic>C. elegans</italic>. MARGO has the flexibility to accommodate the experimental diversity of techniques in neuroethology. Thus, we envision MARGO’s niche as a versatile platform for experiments operating at high throughput to measure individual behavior and deliver closed-loop sensory and optogenetic stimuli.</p>
    <table-wrap id="pone.0224243.t001" orientation="portrait" position="float">
      <object-id pub-id-type="doi">10.1371/journal.pone.0224243.t001</object-id>
      <label>Table 1</label>
      <caption>
        <title>Comparison of open-source animal tracking packages.</title>
        <p>Trackers as falling into two rough categories: 1) real-time trackers capable of very high throughput and potential hardware integration, and 2) offline trackers capable of tracking body parts and/or maintaining individual identities without spatial segregation.</p>
      </caption>
      <alternatives>
        <graphic id="pone.0224243.t001g" xlink:href="pone.0224243.t001"/>
        <table frame="box" rules="all" border="0">
          <colgroup span="1">
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th align="center" rowspan="1" colspan="1"/>
              <th align="center" rowspan="1" colspan="1">MARGO</th>
              <th align="center" rowspan="1" colspan="1">Ethoscopes</th>
              <th align="center" rowspan="1" colspan="1">BioTracker</th>
              <th align="center" rowspan="1" colspan="1">Bonsai</th>
              <th align="center" rowspan="1" colspan="1">ToxTrac</th>
              <th align="center" rowspan="1" colspan="1">flyTracker</th>
              <th align="center" rowspan="1" colspan="1">Ctrax</th>
              <th align="center" rowspan="1" colspan="1">idTracker</th>
              <th align="center" rowspan="1" colspan="1">Tracktor</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center" rowspan="1" colspan="1">Real-time</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">real-time or offline</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">real-time</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">real-time or offline</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">real-time or offline</td>
              <td align="center" rowspan="1" colspan="1">offline</td>
              <td align="center" rowspan="1" colspan="1">offline</td>
              <td align="center" rowspan="1" colspan="1">offline</td>
              <td align="center" rowspan="1" colspan="1">offline</td>
              <td align="center" rowspan="1" colspan="1">offline</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Peripheral hardware integration</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" style="background-color:#d9d9d9" rowspan="1" colspan="1">camera only</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">Yes</td>
              <td align="center" rowspan="1" colspan="1">no</td>
              <td align="center" rowspan="1" colspan="1">no</td>
              <td align="center" rowspan="1" colspan="1">no</td>
              <td align="center" rowspan="1" colspan="1">no</td>
              <td align="center" rowspan="1" colspan="1">no</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Supports experimental models</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" rowspan="1" colspan="1">no</td>
              <td align="center" rowspan="1" colspan="1">depends</td>
              <td align="center" rowspan="1" colspan="1">no</td>
              <td align="center" rowspan="1" colspan="1">no</td>
              <td align="center" rowspan="1" colspan="1">no</td>
              <td align="center" rowspan="1" colspan="1">no</td>
              <td align="center" rowspan="1" colspan="1">no</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Custom hardware required</td>
              <td align="center" rowspan="1" colspan="1">none</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">ethoscope</td>
              <td align="center" rowspan="1" colspan="1">none</td>
              <td align="center" rowspan="1" colspan="1">none</td>
              <td align="center" rowspan="1" colspan="1">none</td>
              <td align="center" rowspan="1" colspan="1">none</td>
              <td align="center" rowspan="1" colspan="1">none</td>
              <td align="center" rowspan="1" colspan="1">none</td>
              <td align="center" rowspan="1" colspan="1">none</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Track multiple animals per ROI</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" rowspan="1" colspan="1">depends</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Resolves identity through collisions</td>
              <td align="center" rowspan="1" colspan="1">no</td>
              <td align="center" rowspan="1" colspan="1">no</td>
              <td align="center" rowspan="1" colspan="1">no</td>
              <td align="center" rowspan="1" colspan="1">depends</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" rowspan="1" colspan="1">yes</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" rowspan="1" colspan="1">no</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Limb, head, midline, wing tracking</td>
              <td align="center" rowspan="1" colspan="1">no</td>
              <td align="center" rowspan="1" colspan="1">no</td>
              <td align="center" rowspan="1" colspan="1">no</td>
              <td align="center" rowspan="1" colspan="1">depends</td>
              <td align="center" rowspan="1" colspan="1">no</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" rowspan="1" colspan="1">no</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">GUI</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" rowspan="1" colspan="1">depends</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">yes</td>
              <td align="center" rowspan="1" colspan="1">no</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">ROI definition</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">automatic or grid</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">grid or manual</td>
              <td align="center" rowspan="1" colspan="1">depends</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">automatic or manual</td>
              <td align="center" style="background-color:#bfbfbf" rowspan="1" colspan="1">automatic</td>
              <td align="center" rowspan="1" colspan="1">grid or manual</td>
              <td align="center" rowspan="1" colspan="1">single ROI</td>
              <td align="center" rowspan="1" colspan="1">manual drawing</td>
              <td align="center" rowspan="1" colspan="1">manual definition</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Notes</td>
              <td align="center" rowspan="1" colspan="1">fast tracking with hardware control</td>
              <td align="center" rowspan="1" colspan="1">low-cost, easily-scalable hardware module</td>
              <td align="center" rowspan="1" colspan="1">allows user defined tracking algorithms</td>
              <td align="center" rowspan="1" colspan="1">visual programming to combine many data streams</td>
              <td align="center" rowspan="1" colspan="1">simple setup and UI (MS Windows only)</td>
              <td align="center" rowspan="1" colspan="1">uses feature detectors to track body parts</td>
              <td align="center" rowspan="1" colspan="1">widely used, has behavioral analysis toolbox</td>
              <td align="center" rowspan="1" colspan="1">maintains many individual IDs</td>
              <td align="center" rowspan="1" colspan="1">well-suited to non-static background</td>
            </tr>
          </tbody>
        </table>
      </alternatives>
    </table-wrap>
  </sec>
  <sec sec-type="materials|methods" id="sec009">
    <title>Methods</title>
    <sec id="sec010">
      <title>Repositories</title>
      <p>MARGO’s code is available in the <ext-link ext-link-type="uri" xlink:href="https://github.com/de-Bivort-Lab/margo">MARGO repository</ext-link> on github. All behavioral data is available on <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/2596143.XI2maRNKiRc">Zenodo</ext-link>. Instrument schematics are available on github at: <ext-link ext-link-type="uri" xlink:href="https://github.com/de-Bivort-Lab/dblab-schematics">de Bivort Lab schematics repository</ext-link>.</p>
    </sec>
    <sec id="sec011">
      <title>Software</title>
      <p>The MARGO GUI, tracking algorithm, and all analysis software were written in MATLAB (The Mathworks, Inc, Natick, MA). Detailed descriptions of the functions and use of the MARGO GUI, ROI detection, background referencing, tracking implementation, noise correction, and data output are available in MARGO’s <ext-link ext-link-type="uri" xlink:href="https://github.com/de-Bivort-Lab/margo/wiki">documentation</ext-link>. Optomotor stimuli were crafted and displayed using the Psychtoolbox-3 for MATLAB. Software for control of all custom electronic hardware was written in C using Arduino libraries.</p>
    </sec>
    <sec id="sec012">
      <title>Organism genotypes and rearing</title>
      <p>Unless otherwise specified, the genotype of all fruit flies tested was a strain of Berlin-K that we inbred for 13 generations prior to these experiments. Gr66a-G4 (from the G. Turner lab), <italic>norpA<sup>P24</sup></italic> (from the M. Heisenberg lab), TrpA1-G4 (FlyBase ID: 27593), Gr28bd-G4 (FlyBase ID: 57620), UAS-20xCsChrimson (FlyBase ID: 55135) were the lines used in the optogenetic experiments. Tracking experiments were performed with mixed sex flies 3-5 days post-eclosion unless otherwise noted. Flies were raised on standard conrmeal/dextrose formula media (Harvard Fly Core Facility) under 12 h/12 h light and dark cycle in an incubator at 25°C and 40% humidity. Animals were imaged and singly-housed on food in modified 96 well plates (Fly Plates, FlySorter LLC) for all multi-day tracking experiments. <italic>C. Elegans</italic> were housed in a custom platform on agarose media and were composed of multiple strains as described in the WorMotel publication [<xref rid="pone.0224243.ref056" ref-type="bibr">56</xref>]. <italic>Drosophila</italic> larvae CantonS on 2% agarose media mixed with fructose in a gradient (0-300mM) along one axis. Larval zebrafish were <italic>HC:GCaMP6s</italic>.</p>
    </sec>
    <sec id="sec013">
      <title>Behavioral acquisition</title>
      <p>All real-time tracking images were captured with USB or GigE cameras from Point Grey (Firefly MV 13SC2 and BFLY-PGE-12A2M-CS). Images for the minimal hardware setup were acquired with an iPhone 5s 1.3MP camera. Cameras used in real-time experiments were fitted with a long-pass 87 Kodak Wratten infrared filter with a cutoff frequency of 750nm and illuminated with infrared LEDs centered at 940nm (Knema LLC, Shreveport, LA). Acquisition rates varied by experiment between 5.0 fps for LED Y-maze and 60.0 fps for optomotor response. Flies were imaged at spatial resolutions ranging between 1-4 pixels per mm and we generally found tracking to be stable at 10 pixels per animal and above. Offline video tracking was performed on 1000x compressed AVI video files unless otherwise specified. Tracking and imaging was conducted in Windows 10 on computers with CPUs ranging from intel i3 3.1GHz to intel i7 4.0GHz.</p>
    </sec>
    <sec id="sec014">
      <title>Behavioral instruments</title>
      <p>Unless otherwise specified, all tracking was conducted in custom imaging boxes constructed with laser-cut acrylic and aluminum rails. Schematics of custom behavioral arenas and behavioral boxes were designed in AutoCAD. Arena parts were laser-cut from black and clear acrylic and joined with Plastruct plastic weld. Schematics for behavioral boxes can be found on the <ext-link ext-link-type="uri" xlink:href="https://github.com/de-Bivort-Lab/dblab-schematics">de Bivort Lab schematics repository</ext-link>. Illumination was provided by dual-channel white and infrared LED array panels mounted at the base (Part# BK3301, Knema LLC, Shreveport, LA). Adjacent pairs of white and infrared LEDs were arrayed in a 14x14 grid spaced 2.2cm apart. White and infrared LEDs were wired for independent control by MOSFET transistors and a Teensy 3.2 microcontroller. Two sand-blasted clear acrylic diffusers were placed in between the illuminator and the behavioral arena for smooth backlighting. Additional tracking was performed in standard 48 multi-well culture plates and individual fly storage units (FlyPlates) from FlySorter LLC. Additional details on the behavioral platforms used here are available in the MARGO documentation.</p>
    </sec>
    <sec id="sec015">
      <title>Experimental procedures</title>
      <p>Tracking experiments were conducted between 10AM and 6PM. We saw no time-of-day significant effects on individual behavioral measures from the optomotor and LED Y-maze assays. Flies were anesthetized either on ice or CO<sub>2</sub> and manually loaded into behavioral arenas with an aspirator. Behavioral modules were loaded into tracking boxes and allowed a minimum post-anesthesia recovery period of 20 minutes before tracking. Unless otherwise specified, animals were tracked for 2 hours in an environmental room at 23 C and 40% humidity. Following tracking, flies were returned to individual storage plates where they were housed for further experiments as needed. For the optogenetic experiments, flies were tested for 20 min in the Y-mazes or 64 min in the circular arenas.</p>
    </sec>
    <sec id="sec016">
      <title>Data and statistics</title>
      <p>Unless noted, all reported error bars are 95% confidence intervals computed by bootstrap resampling. Data processing and calculation of behavioral metrics was conducted automatically by MARGO either in real time, or after experiments. 1000 bootstrap replicates were averaged to estimate null distributions and confidence intervals. Reported p-values for phototaxis, optogenetic closed-loop experiments and optomotor behavior were unadjusted for multiple comparisons and were calculated via two-tailed t-tests. Critical values were adjusted for multiple comparisons via Bonferroni correction.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material" id="sec017">
    <title>Supporting information</title>
    <supplementary-material content-type="local-data" id="pone.0224243.s001">
      <label>S1 Video</label>
      <caption>
        <p><ext-link ext-link-type="uri" xlink:href="https://youtu.be/fyG31BAYHE0">https://youtu.be/fyG31BAYHE0</ext-link>. Video from 960 fly experiment with MARGO traces overlaid. Inactive flies are unmarked.</p>
        <p>(MP4)</p>
      </caption>
      <media xlink:href="pone.0224243.s001.mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pone.0224243.s002">
      <label>S2 Video</label>
      <caption>
        <p><ext-link ext-link-type="uri" xlink:href="https://youtu.be/0aFny65wCnM">https://youtu.be/0aFny65wCnM</ext-link>. MARGO tracking of flies in a simple imaging configuration made from a cardboard box and a sheet of paper. Video was captured with a 1.3MP iPhone camera and tracked offline.</p>
        <p>(MP4)</p>
      </caption>
      <media xlink:href="pone.0224243.s002.mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pone.0224243.s003">
      <label>S3 Video</label>
      <caption>
        <p><ext-link ext-link-type="uri" xlink:href="https://youtu.be/M8imxRP92k4">https://youtu.be/M8imxRP92k4</ext-link>. MARGO tracking of 48 larval zebrafish in a multiwell culture plate (2x speed).</p>
        <p>(MP4)</p>
      </caption>
      <media xlink:href="pone.0224243.s003.mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pone.0224243.s004">
      <label>S4 Video</label>
      <caption>
        <p><ext-link ext-link-type="uri" xlink:href="https://youtu.be/kuTM71lHALc">https://youtu.be/kuTM71lHALc</ext-link>. MARGO tracking of <italic>C. Elegans</italic> in WorMotel, a custom 2400 well platform for studying aging.</p>
        <p>(MP4)</p>
      </caption>
      <media xlink:href="pone.0224243.s004.mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pone.0224243.s005">
      <label>S5 Video</label>
      <caption>
        <p><ext-link ext-link-type="uri" xlink:href="https://youtu.be/sxQMXHJoG24">https://youtu.be/sxQMXHJoG24</ext-link>. MARGO tracking of 38 fruit fly larvae in a single ROI in response to a fructose gradient. Individual identities are not maintained through collisions.</p>
        <p>(MP4)</p>
      </caption>
      <media xlink:href="pone.0224243.s005.mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pone.0224243.s006">
      <label>S6 Video</label>
      <caption>
        <p><ext-link ext-link-type="uri" xlink:href="https://youtu.be/FVIXQSdiWx0">https://youtu.be/FVIXQSdiWx0</ext-link>. MARGO tracking of a time-lapsed video of a Bumblebee colony in an artificial nestbox. Due to the low temporal resolution, individual identities are not maintained at all.</p>
        <p>(MP4)</p>
      </caption>
      <media xlink:href="pone.0224243.s006.mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pone.0224243.s007">
      <label>S7 Video</label>
      <caption>
        <p><ext-link ext-link-type="uri" xlink:href="https://youtu.be/PqPJA6hsabE">https://youtu.be/PqPJA6hsabE</ext-link>. Summary video of the image processing, object tracking, and closed-loop control in the LED Y-Maze assay.</p>
        <p>(M4V)</p>
      </caption>
      <media xlink:href="pone.0224243.s007.m4v">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pone.0224243.s008">
      <label>S8 Video</label>
      <caption>
        <p><ext-link ext-link-type="uri" xlink:href="https://youtu.be/uxgswI8jEWY">https://youtu.be/uxgswI8jEWY</ext-link>. Summary video of the image processing, object tracking, and closed-loop control in the Optomotor assay.</p>
        <p>(MP4)</p>
      </caption>
      <media xlink:href="pone.0224243.s008.mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <p>We thank Ed Soucy and Joel Greenwood for help troubleshooting and fabricating the LED Y-maze board; Simon Forsberg, Jamilla Akhund-Zade, and Carolyn Elya for providing crucial beta testing and feedback during MARGO development; Vladislav Susoy and Robert Johnson for donating worms, and fish to test MARGO tracking in other species; Jessleen Kanwal, James Crall and Matthew Churgin for donating movies of larvae, bees and worms for tracking. ZW was supported by an NSF Graduate Research Fellowship (DGE-1144152). CR was supported by iPUR and Fulbright fellowships. BdB was supported by the NSF (IOS-1557913), the Alfred P. Sloan Foundation, The Klingenstein-Simons Fellowship, and the Smith Family Foundation.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="pone.0224243.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Fry</surname><given-names>S</given-names></name>, <name><surname>Rohrseitz</surname><given-names>N</given-names></name>, <name><surname>Straw</surname><given-names>A</given-names></name>, <name><surname>Dickinson</surname><given-names>M</given-names></name>. <article-title>TrackFly: Virtual reality for a behavioral system analysis in free-flying fruit flies</article-title>. <source>J Neurosci Meth</source>. <year>2008</year>;<volume>171</volume>(<issue>1</issue>):<fpage>110</fpage>–<lpage>117</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2008.02.016</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Donelson</surname><given-names>NC</given-names></name>, <name><surname>Kim</surname><given-names>EZ</given-names></name>, <name><surname>Slawson</surname><given-names>JB</given-names></name>, <name><surname>Vecsey</surname><given-names>CG</given-names></name>, <name><surname>Huber</surname><given-names>R</given-names></name>, <name><surname>Griffith</surname><given-names>LC</given-names></name>. <article-title>Correction: High-Resolution Positional Tracking for Long-Term Analysis of Drosophila Sleep and Locomotion Using the “Tracker” Program</article-title>. <source>PLoS ONE</source>. <year>2012</year>;<volume>7</volume>(<issue>8</issue>). <pub-id pub-id-type="doi">10.1371/annotation/4c62d454-931e-4c48-841a-a701cb658a1c</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref003">
      <label>3</label>
      <mixed-citation publication-type="other">Mathis A, Mamidanna P, Cury K, Abe T, Murthy V. DeepLabCut: markerless pose estimation of user-defined body parts with deep learning. 2018.</mixed-citation>
    </ref>
    <ref id="pone.0224243.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Pereira</surname><given-names>TD</given-names></name>, <name><surname>Aldarondo</surname><given-names>DE</given-names></name>, <name><surname>Willmore</surname><given-names>L</given-names></name>, <name><surname>Kislin</surname><given-names>M</given-names></name>, <name><surname>Wang</surname><given-names>SSH</given-names></name>, <name><surname>Murthy</surname><given-names>M</given-names></name>, <etal>et al</etal><article-title>Fast animal pose estimation using deep neural networks</article-title>. <source>Nature Methods</source>. <year>2018</year>;<volume>16</volume>(<issue>1</issue>):<fpage>117</fpage>–<lpage>125</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-018-0234-5</pub-id><?supplied-pmid 30573820?><pub-id pub-id-type="pmid">30573820</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Alfonso</surname><given-names>P</given-names></name>, <name><surname>Julián</surname><given-names>V</given-names></name>, <name><surname>Hinz</surname><given-names>RC</given-names></name>, <name><surname>Arganda</surname><given-names>S</given-names></name>, <name><surname>de Polavieja</surname><given-names>GG</given-names></name>. <article-title>idTracker: tracking individuals in a group by automatic identification of unmarked animals</article-title>. <source>Nat Methods</source>. <year>2014</year>;<volume>11</volume>(<issue>7</issue>):nmeth.2994.</mixed-citation>
    </ref>
    <ref id="pone.0224243.ref006">
      <label>6</label>
      <mixed-citation publication-type="other">Eyjolfsdottir E, Branson S, Burgos-Artizzu XP, Hoopfer ED, Schor J, Anderson DJ, et al. Detecting Social Actions of Fruit Flies. Computer Vision—ECCV 2014 Lecture Notes in Computer Science. 2014; p. 772–787.</mixed-citation>
    </ref>
    <ref id="pone.0224243.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Rodriguez</surname><given-names>A</given-names></name>, <name><surname>Zhang</surname><given-names>H</given-names></name>, <name><surname>Klaminder</surname><given-names>J</given-names></name>, <name><surname>Brodin</surname><given-names>T</given-names></name>, <name><surname>Andersson</surname><given-names>M</given-names></name>. <article-title>ToxId: an efficient algorithm to solve occlusions when tracking multiple animals</article-title>. <source>Scientific Reports</source>. <year>2017</year>;<volume>7</volume>(<issue>1</issue>). <pub-id pub-id-type="doi">10.1038/s41598-017-15104-2</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Romero-Ferrero</surname><given-names>F</given-names></name>, <name><surname>Bergomi</surname><given-names>MG</given-names></name>, <name><surname>Hinz</surname><given-names>RC</given-names></name>, <name><surname>Heras</surname><given-names>FJH</given-names></name>, <name><surname>Polavieja</surname><given-names>GGD</given-names></name>. <article-title>idtracker.ai: tracking all individuals in small or large collectives of unmarked animals</article-title>. <source>Nature Methods</source>. <year>2019</year>;<volume>16</volume>(<issue>2</issue>):<fpage>179</fpage>–<lpage>182</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-018-0295-5</pub-id><?supplied-pmid 30643215?><pub-id pub-id-type="pmid">30643215</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Ramot</surname><given-names>D</given-names></name>, <name><surname>Johnson</surname><given-names>B</given-names></name>, <name><surname>B</surname><given-names>T</given-names><suffix>Jr</suffix></name>, <name><surname>one</surname><given-names>CL</given-names></name>. <article-title>The Parallel Worm Tracker: a platform for measuring average speed and drug-induced paralysis in nematodes</article-title>. <source>PloS one</source>. <year>2008</year><pub-id pub-id-type="doi">10.1371/journal.pone.0002208</pub-id><?supplied-pmid 18493300?><pub-id pub-id-type="pmid">18493300</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Swierczek</surname><given-names>NA</given-names></name>, <name><surname>Giles</surname><given-names>AC</given-names></name>, <name><surname>Rankin</surname><given-names>CH</given-names></name>, <name><surname>Kerr</surname><given-names>RA</given-names></name>. <article-title>High-throughput behavioral analysis in C. elegans</article-title>. <source>Nat Methods</source>. <year>2011</year>;<volume>8</volume>(<issue>7</issue>):<fpage>592</fpage><pub-id pub-id-type="doi">10.1038/nmeth.1625</pub-id><?supplied-pmid 21642964?><pub-id pub-id-type="pmid">21642964</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Itskovits</surname><given-names>E</given-names></name>, <name><surname>Levine</surname><given-names>A</given-names></name>, <name><surname>Cohen</surname><given-names>E</given-names></name>, <name><surname>Zaslaver</surname><given-names>A</given-names></name>. <article-title>A multi-animal tracker for studying complex behaviors</article-title>. <source>BMC Biology</source>. <year>2017</year>;<volume>15</volume>(<issue>1</issue>). <pub-id pub-id-type="doi">10.1186/s12915-017-0363-9</pub-id><?supplied-pmid 28385158?><pub-id pub-id-type="pmid">28385158</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Scaplen</surname><given-names>KM</given-names></name>, <name><surname>Mei</surname><given-names>NJ</given-names></name>, <name><surname>Bounds</surname><given-names>HA</given-names></name>, <name><surname>Song</surname><given-names>SL</given-names></name>, <name><surname>Azanchi</surname><given-names>R</given-names></name>, <name><surname>Kaun</surname><given-names>KR</given-names></name>. <article-title>Automated real-time quantification of group locomotor activity in Drosophila melanogaster</article-title>. <source>Sci Rep</source>. <year>2019</year>;<volume>9</volume>(<issue>1</issue>):<fpage>4427</fpage><pub-id pub-id-type="doi">10.1038/s41598-019-40952-5</pub-id><?supplied-pmid 30872709?><pub-id pub-id-type="pmid">30872709</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Schneider</surname><given-names>J</given-names></name>, <name><surname>Murali</surname><given-names>N</given-names></name>, <name><surname>Taylor</surname><given-names>GW</given-names></name>, <name><surname>Levine</surname><given-names>JD</given-names></name>. <article-title>Can Drosophila melanogaster tell who’s who?</article-title><source>Plos One</source>. <year>2018</year>;<volume>13</volume>(<issue>10</issue>). <pub-id pub-id-type="doi">10.1371/journal.pone.0205043</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Branson</surname><given-names>K</given-names></name>, <name><surname>Robie</surname><given-names>AA</given-names></name>, <name><surname>Bender</surname><given-names>J</given-names></name>, <name><surname>Perona</surname><given-names>P</given-names></name>, <name><surname>Dickinson</surname><given-names>MH</given-names></name>. <article-title>High-throughput ethomics in large groups of Drosophila</article-title>. <source>Nat Methods</source>. <year>2009</year>;<volume>6</volume>(<issue>6</issue>):nmeth.1328. <pub-id pub-id-type="doi">10.1038/nmeth.1328</pub-id><?supplied-pmid 19412169?><pub-id pub-id-type="pmid">19412169</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Sridhar</surname><given-names>VH</given-names></name>, <name><surname>Roche</surname><given-names>DG</given-names></name>, <name><surname>Gingins</surname><given-names>S</given-names></name>. <article-title>Tracktor: Image-based automated tracking of animal movement and behaviour</article-title>. <source>Methods in Ecology and Evolution</source>. <year>2019</year>;<volume>0</volume>(<issue>0</issue>).</mixed-citation>
    </ref>
    <ref id="pone.0224243.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>G</given-names></name>, <name><surname>Nath</surname><given-names>T</given-names></name>, <name><surname>Linneweber</surname><given-names>GA</given-names></name>, <name><surname>Claeys</surname><given-names>A</given-names></name>, <name><surname>Guo</surname><given-names>Z</given-names></name>, <name><surname>Li</surname><given-names>J</given-names></name>, <etal>et al</etal><article-title>A simple computer vision pipeline reveals the effects of isolation on social interaction dynamics in Drosophila</article-title>. <source>Plos Comput Biol</source>. <year>2018</year>;<volume>14</volume>(<issue>8</issue>):<fpage>e1006410</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006410</pub-id><?supplied-pmid 30161262?><pub-id pub-id-type="pmid">30161262</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Kain</surname><given-names>J</given-names></name>, <name><surname>Stokes</surname><given-names>C</given-names></name>, <name><surname>Gaudry</surname><given-names>Q</given-names></name>, <name><surname>Song</surname><given-names>X</given-names></name>, <name><surname>Foley</surname><given-names>J</given-names></name>, <name><surname>Wilson</surname><given-names>R</given-names></name>, <etal>et al</etal><article-title>Leg-tracking and automated behavioural classification in Drosophila</article-title>. <source>Nature Communications</source>. <year>2013</year>;<volume>4</volume>(<issue>1</issue>). <pub-id pub-id-type="doi">10.1038/ncomms2908</pub-id><?supplied-pmid 23715269?><pub-id pub-id-type="pmid">23715269</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Uhlmann</surname><given-names>V</given-names></name>, <name><surname>Ramdya</surname><given-names>P</given-names></name>, <name><surname>Delgado-Gonzalo</surname><given-names>R</given-names></name>, <name><surname>Benton</surname><given-names>R</given-names></name>, <name><surname>Unser</surname><given-names>M</given-names></name>. <article-title>FlyLimbTracker: An active contour based approach for leg segment tracking in unmarked, freely behaving Drosophila</article-title>. <source>Plos One</source>. <year>2017</year>;<volume>12</volume>(<issue>4</issue>). <pub-id pub-id-type="doi">10.1371/journal.pone.0173433</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Geissmann</surname><given-names>Q</given-names></name>, <name><surname>Rodriguez</surname><given-names>L</given-names></name>, <name><surname>Beckwith</surname><given-names>EJ</given-names></name>, <name><surname>French</surname><given-names>AS</given-names></name>, <name><surname>Jamasb</surname><given-names>AR</given-names></name>, <name><surname>Gilestro</surname><given-names>GF</given-names></name>. <article-title>Ethoscopes: An open platform for high-throughput ethomics</article-title>. <source>Plos Biol</source>. <year>2017</year>;<volume>15</volume>(<issue>10</issue>):<fpage>e2003026</fpage><pub-id pub-id-type="doi">10.1371/journal.pbio.2003026</pub-id><?supplied-pmid 29049280?><pub-id pub-id-type="pmid">29049280</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Straw</surname><given-names>AD</given-names></name>, <name><surname>Branson</surname><given-names>K</given-names></name>, <name><surname>Neumann</surname><given-names>TR</given-names></name>, <name><surname>Dickinson</surname><given-names>MH</given-names></name>. <article-title>Multi-camera real-time three-dimensional tracking of multiple flying animals</article-title>. <source>Journal of The Royal Society Interface</source>. <year>2010</year>;<volume>8</volume>(<issue>56</issue>):<fpage>395</fpage>–<lpage>409</lpage>. <pub-id pub-id-type="doi">10.1098/rsif.2010.0230</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Stowers</surname><given-names>JR</given-names></name>, <name><surname>Hofbauer</surname><given-names>M</given-names></name>, <name><surname>Bastien</surname><given-names>R</given-names></name>, <name><surname>Griessner</surname><given-names>J</given-names></name>, <name><surname>Higgins</surname><given-names>P</given-names></name>, <name><surname>Farooqui</surname><given-names>S</given-names></name>, <etal>et al</etal><article-title>Virtual reality for freely moving animals</article-title>. <source>Nature Methods</source>. <year>2017</year>;<volume>14</volume>(<issue>10</issue>):<fpage>995</fpage>–<lpage>1002</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.4399</pub-id><?supplied-pmid 28825703?><pub-id pub-id-type="pmid">28825703</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref022">
      <label>22</label>
      <mixed-citation publication-type="journal"><name><surname>Chagas</surname><given-names>A</given-names></name>, <name><surname>L</surname><given-names>PL</given-names></name>, <name><surname>Arrenberg</surname><given-names>AB</given-names></name>, <name><surname>Baden</surname><given-names>T</given-names></name>. <article-title>The €100 lab: A 3D-printable open-source platform for fluorescence microscopy, optogenetics, and accurate temperature control during behaviour of zebrafish, Drosophila, and Caenorhabditis elegans</article-title>. <source>Plos Biol</source>. <year>2017</year>;<volume>15</volume>(<issue>7</issue>):<fpage>e2002702</fpage><pub-id pub-id-type="doi">10.1371/journal.pbio.2002702</pub-id><pub-id pub-id-type="pmid">28719603</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref023">
      <label>23</label>
      <mixed-citation publication-type="journal"><name><surname>Kuhn</surname><given-names>HW</given-names></name>. <source>The Hungarian method for the assignment problem</source>. <year>1955</year>;<volume>2</volume>(<issue>1 2</issue>):<fpage>83</fpage>–<lpage>97</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0224243.ref024">
      <label>24</label>
      <mixed-citation publication-type="other">Mönck H, Jörg A, Falkenhausen T, Tanke J, Wild B, Dormagen D, et al. BioTracker: An Open-Source Computer Vision Framework for Visual Animal Tracking. 2018.</mixed-citation>
    </ref>
    <ref id="pone.0224243.ref025">
      <label>25</label>
      <mixed-citation publication-type="journal"><name><surname>Heisenberg</surname><given-names>M</given-names></name>, <name><surname>Wolf</surname><given-names>R</given-names></name>. <article-title>Vision in Drosophila</article-title>. <source>Studies of Brain Function</source>. <year>1984</year>.</mixed-citation>
    </ref>
    <ref id="pone.0224243.ref026">
      <label>26</label>
      <mixed-citation publication-type="journal"><name><surname>Berman</surname><given-names>GJ</given-names></name>, <name><surname>Choi</surname><given-names>DM</given-names></name>, <name><surname>Bialek</surname><given-names>W</given-names></name>, <name><surname>Shaevitz</surname><given-names>JW</given-names></name>. <article-title>Mapping the stereotyped behaviour of freely moving fruit flies</article-title>. <source>Journal of The Royal Society Interface</source>. <year>2014</year>;<volume>11</volume>(<issue>99</issue>):<fpage>20140672</fpage>–<lpage>20140672</lpage>. <pub-id pub-id-type="doi">10.1098/rsif.2014.0672</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Crall</surname><given-names>JD</given-names></name>, <name><surname>Souffrant</surname><given-names>AD</given-names></name>, <name><surname>Akandwanaho</surname><given-names>D</given-names></name>, <name><surname>Hescock</surname><given-names>SD</given-names></name>, <name><surname>Callan</surname><given-names>SE</given-names></name>, <name><surname>Coronado</surname><given-names>WM</given-names></name>, <etal>et al</etal><article-title>Social context modulates idiosyncrasy of behaviour in the gregarious cockroach Blaberus discoidalis</article-title>. <source>Animal Behaviour</source>. <year>2016</year>;<volume>111</volume>:<fpage>297</fpage>–<lpage>305</lpage>. <pub-id pub-id-type="doi">10.1016/j.anbehav.2015.10.032</pub-id>.</mixed-citation>
    </ref>
    <ref id="pone.0224243.ref028">
      <label>28</label>
      <mixed-citation publication-type="journal"><name><surname>Buchanan</surname><given-names>SM</given-names></name>, <name><surname>Kain</surname><given-names>JS</given-names></name>, <name><surname>de Bivort</surname><given-names>BL</given-names></name>. <article-title>Neuronal control of locomotor handedness in Drosophila</article-title>. <source>Proc National Acad Sci</source>. <year>2015</year>;<volume>112</volume>(<issue>21</issue>):<fpage>6700</fpage>–<lpage>6705</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1500804112</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref029">
      <label>29</label>
      <mixed-citation publication-type="journal"><name><surname>Ayroles</surname><given-names>JF</given-names></name>, <name><surname>Buchanan</surname><given-names>SM</given-names></name>, <name><surname>Chelsea</surname><given-names>O</given-names></name>, <name><surname>Kyobi</surname><given-names>S</given-names></name>, <name><surname>Grenier</surname><given-names>JK</given-names></name>, <name><surname>Clark</surname><given-names>AG</given-names></name>, <etal>et al</etal><article-title>Behavioral idiosyncrasy reveals genetic control of phenotypic variability</article-title>. <source>Proc National Acad Sci</source>. <year>2015</year>;<volume>112</volume>(<issue>21</issue>):<fpage>6706</fpage>–<lpage>6711</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1503830112</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref030">
      <label>30</label>
      <mixed-citation publication-type="other">Cruz TL, Fujiwara TE, Varela N, Mohammad F, Claridge-Chang A, Chiappe ME. Motor context coordinates visually guided walking in Drosophila. bioRxiv. 2019.</mixed-citation>
    </ref>
    <ref id="pone.0224243.ref031">
      <label>31</label>
      <mixed-citation publication-type="journal"><name><surname>Götz</surname><given-names>KG</given-names></name>, <name><surname>Wenking</surname><given-names>H</given-names></name>. <article-title>Visual control of locomotion in the walking fruitfly Drosophila</article-title>. <source>Journal of Comparative Physiology</source>. <year>1973</year>;<volume>85</volume>(<issue>3</issue>):<fpage>235</fpage>–<lpage>266</lpage>. <pub-id pub-id-type="doi">10.1007/BF00694232</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref032">
      <label>32</label>
      <mixed-citation publication-type="journal"><name><surname>Haag</surname><given-names>J</given-names></name>, <name><surname>Arenz</surname><given-names>A</given-names></name>, <name><surname>Serbe</surname><given-names>E</given-names></name>, <name><surname>Gabbiani</surname><given-names>F</given-names></name>, <name><surname>Borst</surname><given-names>A</given-names></name>. <article-title>Complementary mechanisms create direction selectivity in the fly</article-title>. <source>eLife</source>. <year>2016</year>;<volume>5</volume><pub-id pub-id-type="doi">10.7554/eLife.17421</pub-id><?supplied-pmid 27502554?><pub-id pub-id-type="pmid">27502554</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref033">
      <label>33</label>
      <mixed-citation publication-type="journal"><name><surname>Zhu</surname><given-names>Y</given-names></name>, <name><surname>Nern</surname><given-names>A</given-names></name>, <name><surname>Zipursky</surname><given-names>S</given-names></name>, <name><surname>Biology</surname><given-names>FM</given-names></name>. <article-title>Peripheral visual circuits functionally segregate motion and phototaxis behaviors in the fly</article-title>. <source>Current Biology</source>. <year>2009</year><pub-id pub-id-type="doi">10.1016/j.cub.2009.02.053</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref034">
      <label>34</label>
      <mixed-citation publication-type="journal"><name><surname>Kim</surname><given-names>S</given-names></name>, <name><surname>Tellez</surname><given-names>K</given-names></name>, <name><surname>Buchan</surname><given-names>G</given-names></name>, <name><surname>Lebestky</surname><given-names>T</given-names></name>. <article-title>Fly Stampede 2.0: A Next Generation Optomotor Assay for Walking Behavior in Drosophila Melanogaster</article-title>. <source>Frontiers in Molecular Neuroscience</source>. <year>2016</year>;<volume>9</volume><pub-id pub-id-type="doi">10.3389/fnmol.2016.00148</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref035">
      <label>35</label>
      <mixed-citation publication-type="journal"><name><surname>Seelig</surname><given-names>JD</given-names></name>, <name><surname>Chiappe</surname><given-names>ME</given-names></name>, <name><surname>Lott</surname><given-names>GK</given-names></name>, <name><surname>Dutta</surname><given-names>A</given-names></name>, <name><surname>Osborne</surname><given-names>JE</given-names></name>, <name><surname>Reiser</surname><given-names>MB</given-names></name>, <etal>et al</etal><article-title>Two-photon calcium imaging from head-fixed Drosophila during optomotor walking behavior</article-title>. <source>Nature Methods</source>. <year>2010</year>;<volume>8</volume>(<issue>2</issue>):<fpage>184</fpage>–<lpage>184</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth0211-184b</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref036">
      <label>36</label>
      <mixed-citation publication-type="journal"><name><surname>Griebel</surname><given-names>G</given-names></name>. <article-title>Faculty of 1000 evaluation for Noninvasive optical inhibition with a red-shifted microbial rhodopsin</article-title>. <source>F1000—Post-publication peer review of the biomedical literature</source>. <year>2014</year>.</mixed-citation>
    </ref>
    <ref id="pone.0224243.ref037">
      <label>37</label>
      <mixed-citation publication-type="journal"><name><surname>Klapoetke</surname><given-names>NC</given-names></name>, <name><surname>Murata</surname><given-names>Y</given-names></name>, <name><surname>Kim</surname><given-names>S</given-names></name>, <name><surname>Pulver</surname><given-names>SR</given-names></name>, <name><surname>Amanda</surname><given-names>B</given-names></name>, <name><surname>Cho</surname><given-names>Y</given-names></name>, <etal>et al</etal><article-title>Independent optical excitation of distinct neural populations</article-title>. <source>Nat Methods</source>. <year>2014</year>;<volume>11</volume>(<issue>3</issue>). <pub-id pub-id-type="doi">10.1038/nmeth.2836</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref038">
      <label>38</label>
      <mixed-citation publication-type="journal"><name><surname>Wustmann</surname><given-names>G</given-names></name>, <name><surname>Rein</surname><given-names>K</given-names></name>, <name><surname>Wolf</surname><given-names>R</given-names></name>, <name><surname>Heisenberg</surname><given-names>M</given-names></name>. <article-title>A new paradigm for operant conditioning of Drosophila melanogaster</article-title>. <source>Journal of Comparative Physiology A</source>. <year>1996</year>;<volume>179</volume>(<issue>3</issue>). <pub-id pub-id-type="doi">10.1007/BF00194996</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref039">
      <label>39</label>
      <mixed-citation publication-type="journal"><name><surname>Wustmann</surname><given-names>G</given-names></name>, <name><surname>Heisenberg</surname><given-names>M</given-names></name>. <article-title>Behavioral manipulation of retrieval in a spatial memory task for Drosophila melanogaster</article-title>. <source>Learning &amp; Memory</source>. <year>1997</year>;<volume>4</volume>(<issue>4</issue>):<fpage>328</fpage>–<lpage>336</lpage>. <pub-id pub-id-type="doi">10.1101/lm.4.4.328</pub-id><pub-id pub-id-type="pmid">10706370</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref040">
      <label>40</label>
      <mixed-citation publication-type="journal"><name><surname>Diegelmann</surname><given-names>S</given-names></name>. <article-title>Genetic dissociation of acquisition and memory strength in the heat-box spatial learning paradigm in Drosophila</article-title>. <source>Learning &amp; Memory</source>. <year>2006</year>;<volume>13</volume>(<issue>1</issue>):<fpage>72</fpage>–<lpage>83</lpage>. <pub-id pub-id-type="doi">10.1101/lm.45506</pub-id><pub-id pub-id-type="pmid">16418434</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref041">
      <label>41</label>
      <mixed-citation publication-type="journal"><name><surname>Ostrowski</surname><given-names>D</given-names></name>, <name><surname>Kahsai</surname><given-names>L</given-names></name>, <name><surname>Kramer</surname><given-names>EF</given-names></name>, <name><surname>Knutson</surname><given-names>P</given-names></name>, <name><surname>Zars</surname><given-names>T</given-names></name>. <article-title>Place memory retention in Drosophila</article-title>. <source>Neurobiology of Learning and Memory</source>. <year>2015</year>;<volume>123</volume>:<fpage>217</fpage>–<lpage>224</lpage>. <pub-id pub-id-type="doi">10.1016/j.nlm.2015.06.015</pub-id><?supplied-pmid 26143995?><pub-id pub-id-type="pmid">26143995</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref042">
      <label>42</label>
      <mixed-citation publication-type="journal"><name><surname>Putz</surname><given-names>G</given-names></name>. <article-title>Memories in Drosophila Heat-box Learning</article-title>. <source>Learning &amp; Memory</source>. <year>2002</year>;<volume>9</volume>(<issue>5</issue>):<fpage>349</fpage>–<lpage>359</lpage>. <pub-id pub-id-type="doi">10.1101/lm.50402</pub-id><pub-id pub-id-type="pmid">12359842</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref043">
      <label>43</label>
      <mixed-citation publication-type="journal"><name><surname>Sitaraman</surname><given-names>D</given-names></name>, <name><surname>Zars</surname><given-names>M</given-names></name>, <name><surname>Zars</surname><given-names>T</given-names></name>. <article-title>Reinforcement pre-exposure enhances spatial memory formation in Drosophila</article-title>. <source>Journal of Comparative Physiology A</source>. <year>2007</year>;<volume>193</volume>(<issue>8</issue>):<fpage>903</fpage>–<lpage>908</lpage>. <pub-id pub-id-type="doi">10.1007/s00359-007-0243-9</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref044">
      <label>44</label>
      <mixed-citation publication-type="journal"><name><surname>Sitaraman</surname><given-names>D</given-names></name>, <name><surname>Zars</surname><given-names>M</given-names></name>, <name><surname>Zars</surname><given-names>T</given-names></name>. <article-title>Place memory formation in Drosophila is independent of proper octopamine signaling</article-title>. <source>Journal of Comparative Physiology A</source>. <year>2010</year>;<volume>196</volume>(<issue>4</issue>):<fpage>299</fpage>–<lpage>305</lpage>. <pub-id pub-id-type="doi">10.1007/s00359-010-0517-5</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref045">
      <label>45</label>
      <mixed-citation publication-type="journal"><name><surname>Zars</surname><given-names>M</given-names></name>, <name><surname>Zars</surname><given-names>T</given-names></name>. <article-title>High and low temperatures have unequal reinforcing properties in Drosophila spatial learning</article-title>. <source>Journal of Comparative Physiology A</source>. <year>2006</year>;<volume>192</volume>(<issue>7</issue>):<fpage>727</fpage>–<lpage>735</lpage>. <pub-id pub-id-type="doi">10.1007/s00359-006-0109-6</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref046">
      <label>46</label>
      <mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>Z</given-names></name>, <name><surname>Bertolucci</surname><given-names>F</given-names></name>, <name><surname>Wolf</surname><given-names>R</given-names></name>, <name><surname>Heisenberg</surname><given-names>M</given-names></name>. <article-title>Flies Cope with Uncontrollable Stress by Learned Helplessness</article-title>. <source>Current Biology</source>. <year>2013</year>;<volume>23</volume>(<issue>9</issue>):<fpage>799</fpage>–<lpage>803</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2013.03.054</pub-id>. <?supplied-pmid 23602474?><pub-id pub-id-type="pmid">23602474</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref047">
      <label>47</label>
      <mixed-citation publication-type="journal"><name><surname>Kain</surname><given-names>JS</given-names></name>, <name><surname>Stokes</surname><given-names>C</given-names></name>, <name><surname>de Bivort</surname><given-names>BL</given-names></name>. <article-title>Phototactic personality in fruit flies and its suppression by serotonin and white</article-title>. <source>Proc National Acad Sci</source>. <year>2012</year>;<volume>109</volume>(<issue>48</issue>):<fpage>19834</fpage>–<lpage>19839</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1211988109</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref048">
      <label>48</label>
      <mixed-citation publication-type="journal"><name><surname>Chiang</surname><given-names>H</given-names></name>. <source>Tactic Reactions of Young Adults of Drosophila melanogaster</source>. <year>1963</year>;<volume>70</volume>(<issue>2</issue>):<fpage>329</fpage>.</mixed-citation>
    </ref>
    <ref id="pone.0224243.ref049">
      <label>49</label>
      <mixed-citation publication-type="journal"><name><surname>Kain</surname><given-names>JS</given-names></name>, <name><surname>Zhang</surname><given-names>S</given-names></name>, <name><surname>Jamilla</surname><given-names>A</given-names></name>, <name><surname>Samuel</surname><given-names>AD</given-names></name>, <name><surname>Klein</surname><given-names>M</given-names></name>, <name><surname>Bivort</surname><given-names>BL</given-names></name>. <article-title>Variability in thermal and phototactic preferences in Drosophila may reflect an adaptive bet hedging strategy</article-title>. <source>Evolution</source>. <year>2015</year>;<volume>69</volume>(<issue>12</issue>):<fpage>3171</fpage>–<lpage>3185</lpage>. <pub-id pub-id-type="doi">10.1111/evo.12813</pub-id><?supplied-pmid 26531165?><pub-id pub-id-type="pmid">26531165</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref050">
      <label>50</label>
      <mixed-citation publication-type="journal"><name><surname>Rosner</surname><given-names>R</given-names></name>, <name><surname>Egelhaaf</surname><given-names>M</given-names></name>, <name><surname>Warzecha</surname><given-names>A</given-names></name>. <source>Behavioural state affects motion-sensitive neurones in the fly visual system</source>. <year>2009</year>;<volume>213</volume>(<issue>2</issue>):<fpage>331</fpage>–<lpage>338</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0224243.ref051">
      <label>51</label>
      <mixed-citation publication-type="journal"><name><surname>Rien</surname><given-names>D</given-names></name>, <name><surname>Kern</surname><given-names>R</given-names></name>, <name><surname>Kurtz</surname><given-names>R</given-names></name>. <source>Octopaminergic modulation of a fly visual motion-sensitive neuron during stimulation with naturalistic optic flow</source>. <year>2013</year>;<volume>7</volume>.</mixed-citation>
    </ref>
    <ref id="pone.0224243.ref052">
      <label>52</label>
      <mixed-citation publication-type="journal"><name><surname>Chiappe</surname><given-names>EM</given-names></name>, <name><surname>Seelig</surname><given-names>JD</given-names></name>, <name><surname>Reiser</surname><given-names>MB</given-names></name>, <name><surname>Jayaraman</surname><given-names>V</given-names></name>. <source>Walking Modulates Speed Sensitivity in Drosophila Motion Vision</source>. <year>2010</year>;<volume>20</volume>(<issue>16</issue>):<fpage>1470</fpage>–<lpage>1475</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0224243.ref053">
      <label>53</label>
      <mixed-citation publication-type="journal"><name><surname>Maimon</surname><given-names>G</given-names></name>, <name><surname>Straw</surname><given-names>AD</given-names></name>, <name><surname>Dickinson</surname><given-names>MH</given-names></name>. <source>Active flight increases the gain of visual motion processing in Drosophila</source>. <year>2010</year>;<volume>13</volume>(<issue>3</issue>):<fpage>393</fpage>–<lpage>399</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0224243.ref054">
      <label>54</label>
      <mixed-citation publication-type="journal"><name><surname>Gorostiza</surname><given-names>EA</given-names></name>. <article-title>Does Cognition Have a Role in Plasticity of “Innate Behavior”? A Perspective From Drosophila</article-title>. <source>Frontiers in Psychology</source>. <year>2018</year>;<volume>9</volume>:<fpage>1502</fpage><pub-id pub-id-type="doi">10.3389/fpsyg.2018.01502</pub-id><?supplied-pmid 30233444?><pub-id pub-id-type="pmid">30233444</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref055">
      <label>55</label>
      <mixed-citation publication-type="journal"><name><surname>Todd</surname><given-names>JG</given-names></name>, <name><surname>Kain</surname><given-names>JS</given-names></name>, <name><surname>de Bivort</surname><given-names>BL</given-names></name>. <article-title>Systematic exploration of unsupervised methods for mapping behavior</article-title>. <source>Phys Biol</source>. <year>2017</year>;<volume>14</volume>(<issue>1</issue>):<fpage>015002</fpage><pub-id pub-id-type="doi">10.1088/1478-3975/14/1/015002</pub-id><?supplied-pmid 28166059?><pub-id pub-id-type="pmid">28166059</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0224243.ref056">
      <label>56</label>
      <mixed-citation publication-type="journal"><name><surname>Churgin</surname><given-names>MA</given-names></name>, <name><surname>Jung</surname><given-names>S</given-names></name>, <name><surname>Yu</surname><given-names>C</given-names></name>, <name><surname>Chen</surname><given-names>X</given-names></name>, <name><surname>Raizen</surname><given-names>DM</given-names></name>, <name><surname>Christopher</surname><given-names>F</given-names></name>. <article-title>Longitudinal imaging of Caenorhabditis elegans in a microfabricated device reveals variation in behavioral decline during aging</article-title>. <source>Elife</source>. <year>2017</year>;<volume>6</volume>:<fpage>e26652</fpage><pub-id pub-id-type="doi">10.7554/eLife.26652</pub-id><?supplied-pmid 28537553?><pub-id pub-id-type="pmid">28537553</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<sub-article id="pone.0224243.r001" article-type="editor-report">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0224243.r001</article-id>
    <title-group>
      <article-title>Decision Letter 0</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Gilestro</surname>
          <given-names>Giorgio F</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2019 Giorgio F Gilestro</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Giorgio F Gilestro</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article id="rel-obj001" ext-link-type="doi" xlink:href="10.1371/journal.pone.0224243" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>0</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <boxed-text id="pone-0224926-box001" position="float" specific-use="prior_peer_review_unavailable" orientation="portrait">
      <sec id="sec018">
        <title>Transfer Alert</title>
        <p>This paper was transferred from another journal. As a result, its full editorial history (including decision letters, peer reviews and author responses) may not be present.</p>
      </sec>
    </boxed-text>
    <p>
      <named-content content-type="letter-date">21 Jun 2019</named-content>
    </p>
    <p>PONE-D-19-17145</p>
    <p>MARGO (Massively Automated Real-time GUI for Object-tracking), a platform for high-throughput ethology</p>
    <p>PLOS ONE</p>
    <p>Dear Dr. de Bivort,</p>
    <p>Thank you for submitting your manuscript to PLOS ONE. I have reviewed the comments this manuscript received during its previous submission on the sister journal and I am happy to proceed using the feedback from those reviewers together with my own judgement. Reviewer #3, whose comments are here attached once more for your convenience, raised some questions aimed at clarifying some technical aspects of the work and we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
    <p>We would appreciate receiving your revised manuscript by Aug 05 2019 11:59PM. When you are ready to submit your revision, log on to <ext-link ext-link-type="uri" xlink:href="https://www.editorialmanager.com/pone/">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
    <p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter.</p>
    <p>To enhance the reproducibility of your results, we recommend that if applicable you deposit your laboratory protocols in protocols.io, where a protocol can be assigned its own identifier (DOI) such that it can be cited independently in the future. For instructions see: <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols">http://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link></p>
    <p>Please include the following items when submitting your revised manuscript:</p>
    <p>
      <list list-type="bullet">
        <list-item>
          <p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). This letter should be uploaded as separate file and labeled 'Response to Reviewers'.</p>
        </list-item>
        <list-item>
          <p>A marked-up copy of your manuscript that highlights changes made to the original version. This file should be uploaded as separate file and labeled 'Revised Manuscript with Track Changes'.</p>
        </list-item>
        <list-item>
          <p>An unmarked version of your revised paper without tracked changes. This file should be uploaded as separate file and labeled 'Manuscript'.</p>
        </list-item>
      </list>
    </p>
    <p>Please note while forming your response, if your article is accepted, you may have the opportunity to make the peer review history publicly available. The record will include editor decision letters (with reviews) and your responses to reviewer comments. If eligible, we will contact you to opt in or out.</p>
    <p>We look forward to receiving your revised manuscript.</p>
    <p>Kind regards,</p>
    <p>Giorgio F Gilestro, PhD</p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
    <p>Journal Requirements:</p>
    <p>1. When submitting your revision, we need you to address these additional requirements.</p>
    <p>Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at</p>
    <p><ext-link ext-link-type="uri" xlink:href="http://www.journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf">http://www.journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</ext-link> and <ext-link ext-link-type="uri" xlink:href="http://www.journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf">http://www.journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</ext-link></p>
    <p>2. Thank you for stating the following in the Competing Interests section:</p>
    <p>I have read the journal's policy and the authors of this manuscript have the following competing interests: BdB is a scientific advisor at FlySorter, LLC</p>
    <p>We note that you received funding from a commercial source: [Name of Company]</p>
    <p>Please provide an amended Competing Interests Statement that explicitly states this commercial funder, along with any other <ext-link ext-link-type="uri" xlink:href="http://www.plosone.org/static/competing.action">relevant declarations</ext-link> relating to employment, consultancy, patents, products in development, marketed products, etc.</p>
    <p>Within this Competing Interests Statement, please confirm that this does not alter your adherence to all PLOS ONE policies on sharing data and materials by including the following statement: "This does not alter our adherence to PLOS ONE policies on sharing data and materials.” (as detailed online in our guide for authors <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosone/s/competing-interests">http://journals.plos.org/plosone/s/competing-interests</ext-link>).  If there are restrictions on sharing of data and/or materials, please state these. Please note that we cannot proceed with consideration of your article until this information has been declared.</p>
    <p>Please include your amended Competing Interests Statement within your cover letter. We will change the online submission form on your behalf.</p>
    <p>Please know it is PLOS ONE policy for corresponding authors to declare, on behalf of all authors, all potential competing interests for the purposes of transparency. PLOS defines a competing interest as anything that interferes with, or could reasonably be perceived as interfering with, the full and objective presentation, peer review, editorial decision-making, or publication of research or non-research articles submitted to one of the journals. Competing interests can be financial or non-financial, professional, or personal. Competing interests can arise in relationship to an organization or another person. Please follow this link to our website for more details on competing interests: <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosone/s/competing-interests">http://journals.plos.org/plosone/s/competing-interests</ext-link></p>
    <p>Additional Editor Comments:</p>
    <p>Reviewer #3: (inherited from previous submission)</p>
    <p>The authors propose a system to track many animals independently. It is particularly tuned for high throughput of independent animals.</p>
    <p>Major</p>
    <p>It is not clear to me that Bonsai cannot do what MARGO does. Could you compare?</p>
    <p>I could not find some parts of the algorithm (how it is decided that a blob is noise or animal, how you go from centroids to trajectories)</p>
    <p>Minor</p>
    <p>Authors seem to confuse idTracker with idtrackrer.ai. It is the second one that uses training, while you claim is the first one. Also idtracker.ai seem to be missing in ‘while others measure the collective</p>
    <p>activity of groups without maintaining identities or rely on physical segregation of animals to ensure trajectories never Collide’</p>
    <p>Unclear what ‘sample imaging statistics of clean tracking’ means</p>
    <p>‘Each background image is computed as the mean or median image from a rolling stack of background sample images.’ How do you choose between mean or median?</p>
    <p>Does the tracking start independently for every ROI? How long does it take to construct the BKG image? If an animal never moves, is there a way to extract this information or it should be inferred by the number of tracks obtained?</p>
    <p>Typo: ‘pvalues¡¡</p>
    <p>10−6 by t-test).’ And similar ones. pvalues¡¡ 10−6 by t-test).</p>
    <p>I think the Abstract needs to be more specific. For example: ‘tracking’ is for animals independetly, for example in different wells. Maybe also clarify it is for MATLAB, as this is very informative for users. In main text I learn much better but MARGO is about:</p>
    <p>1) fast and accurate individual tracking that</p>
    <p>could be scaled to very large numbers of individuals or</p>
    <p>experimental groups over very long timescales, 2) flexibility</p>
    <p>in the user interface that would permit a diversity of</p>
    <p>organisms, tracking modes, experimental paradigms, and</p>
    <p>behavioral arenas, 3) integration of peripheral hardware to</p>
    <p>enable closed-loop sensory and optogenetic stimuli, and 4)</p>
    <p>a user-friendly interface and data output format.</p>
    <p>[Note: HTML markup is below. Please do not edit.]</p>
    <p>Reviewers' comments:</p>
    <p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files to be viewed.]</p>
    <p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link ext-link-type="uri" xlink:href="https://pacev2.apexcovantage.com/">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email us at <email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p>
  </body>
</sub-article>
<sub-article id="pone.0224243.r002" article-type="author-comment">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0224243.r002</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 0</article-title>
    </title-group>
    <related-article id="rel-obj002" ext-link-type="doi" xlink:href="10.1371/journal.pone.0224243" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">7 Oct 2019</named-content>
    </p>
    <p>[This is a copy of the response text I uploaded as a component of the revision]</p>
    <p>Please find below a the full set of comments from the previous reviewer and editor (in blue) and our replies (in black). These were helpful comments and we have implemented changes that have improved the manuscript. </p>
    <p>[From the editor:]</p>
    <p>&gt; Thank you for submitting your manuscript to PLOS ONE. I</p>
    <p>&gt; have reviewed the comments this manuscript received during</p>
    <p>&gt; its previous submission on the sister journal and I am</p>
    <p>&gt; happy to proceed using the feedback from those reviewers</p>
    <p>&gt; together with my own judgement. Reviewer #3, whose</p>
    <p>&gt; comments are here attached once more for your convenience,</p>
    <p>&gt; raised some questions aimed at clarifying some technical</p>
    <p>&gt; aspects of the work and we invite you to submit a revised</p>
    <p>&gt; version of the manuscript that addresses the points raised during the review process.</p>
    <p>We thank the editor and reviewer for their constructive feedback, and believe we have made appropriate revisions to the manuscript. </p>
    <p>[From Reviewer 3]</p>
    <p>The authors propose a system to track many animals independently. It is particularly tuned for high throughput of independent animals.</p>
    <p>Major Comments</p>
    <p>It is not clear to me that Bonsai cannot do what MARGO does. Could you compare?</p>
    <p>The reviewer rightly points out that Bonsai is similar to MARGO in the sense that it features support for many hardware devices and a flexible interface for defining custom experiments. We did not originally include Bonsai in our list of trackers because Bonsai is closer to a programming environment than a tracker. We are not experts in Bonsai and cannot definitively say what Bonsai can do, but we believe, given the flexibility of Bonsai and ReactiveX, that most if not all of MARGO’s functionality could be written and implemented in Bonsai. The flexibility offered by Bonsai extends well beyond that of MARGO. We see MARGO as a specific implementation of a tracking interface that includes many features useful for high-throughput and real-time tracking. MARGO does offer some flexibility for defining new experimental regimes within the same overarching schema of parallel tracking and/or stimulus targeting of hundreds or thousands of ROIs. We believe that some users may find it simpler and more efficient to run experiments similar to those pre-packaged with MARGO, particularly users who are unfamiliar with Bonsai’s API and ReactiveX.</p>
    <p>We have now added Bonsai to our table of trackers, noting its generality as a device programming environment. We have also now included it in the discussion. </p>
    <p>I could not find some parts of the algorithm (how it is decided that a blob is noise or animal, how you go from centroids to trajectories)</p>
    <p>We thank the reviewer for pointing out this discrepancy and have added clarifying text. Briefly, in each frame, candidate blobs are identified as the blobs that are both 1) between minimum and maximum size threshold and 2) located within the bounds of an ROI. Candidate blobs are subsequently assigned to ROIs by spatial location. Within each ROI, candidate blobs are matched to centroid traces by minimizing the total frame-to-frame changes in position within each ROI. If the number of candidates exceeds the number of traces in a given ROI, only the candidates closest to the last centroid positions of the traces are assigned. If the number of traces exceeds the number of candidates, the candidates are assigned to the closest traces and any remaining traces are assigned no position (i.e. NaN) for that frame.</p>
    <p>Minor Comments</p>
    <p>Authors seem to confuse idTracker with idtrackrer.ai. It is the second one that uses training, while you claim is the first one. </p>
    <p>We have changed the text and citation to correctly cite idTracker.ai.</p>
    <p>Also idtracker.ai seem to be missing in ‘while others measure the collective activity of groups without maintaining identities or rely on physical segregation of animals to ensure trajectories never Collide’</p>
    <p>We believe that idTracker.ai does not belong on this list, since it is capable of maintaining individual identities without physical separation.</p>
    <p>Unclear what ‘sample imaging statistics of clean tracking’ means.</p>
    <p>We have re-worded this sentence clarify that this step of the MARGO workflow refers to sampling the number of foreground pixels under clean imaging conditions to construct a distribution and compute statistics on that distribution.</p>
    <p>‘Each background image is computed as the mean or median image from a rolling stack of background sample images.’ How do you choose between mean or median?</p>
    <p>The user can choose within the tracking options whether the background image is computed as the mean or median of the sample images. By default, MARGO computes the background image as the median of the sample images since it can compute an accurate representation of how the background should look without the animal with as few as three images (provided that the animal moves between samples). Mean referencing often requires more sampling to remove ghost-like images of the animal from the background. We have reworded the sentence to help clarify that point.</p>
    <p>Does the tracking start independently for every ROI? </p>
    <p>Tracking starts for all ROIs simultaneously once the setup steps are completed (ie. ROI definition, background image, and . </p>
    <p>How long does it take to construct the BKG image?</p>
    <p>Typically we find that it takes between 3-30 seconds to establish the background image for most animals. Importantly, background image updating occurs continuously throughout tracking; so it is not essential that animals move prior to the start of tracking. We have now added this information to the text.</p>
    <p>If an animal never moves, is there a way to extract this information or it should be inferred by the number of tracks obtained?</p>
    <p>The number of traces output by MARGO is specified by the user prior (default) or automatically estimated (if this option is selected by the user) prior to tracking. In either case, the total number of traces is fixed before tracking and are organized by ROIs (the number of traces in each ROI can be a different number for each ROI). Animals that never move and are therefore averaged into the background, will receive no assigned position in each frame (i.e. NaN). Thus there will be a trace for each animal and it is possible to identify which animals are inactive by the total distance traveled for each trace. We’ve clarified the above points in the text.</p>
    <p>I think the Abstract needs to be more specific. For example: ‘tracking’ is for animals independently, for example in different wells. Maybe also clarify it is for MATLAB, as this is very informative for users. In main text I learn much better but MARGO is about:</p>
    <p>1. fast and accurate individual tracking that could be scaled to very large numbers of individuals or experimental groups over very long timescales</p>
    <p>2. flexibility in the user interface that would permit a diversity of organisms, tracking modes, experimental paradigms, and behavioral arenas</p>
    <p>3. integration of peripheral hardware to enable closed-loop sensory and optogenetic stimuli</p>
    <p>4. a user-friendly interface and data output format.</p>
    <p>We have attempted to make the abstract more specific, and adopted the reviewer’s suggestion to indicate that it is a MATLAB package. Since MARGO does track multiple animals in the same well/arena (though without maintaining identity), we didn’t want to imply that it exclusively tracks one animal per well, but we changed the language to emphasize that this is the typical configuration. We also modified the abstract to better highlight the specific features listed above.</p>
    <supplementary-material content-type="local-data" id="pone.0224243.s009">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">MARGO_Response to Reviewers.pdf</named-content></p>
      </caption>
      <media xlink:href="pone.0224243.s009.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article id="pone.0224243.r003" article-type="editor-report">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0224243.r003</article-id>
    <title-group>
      <article-title>Decision Letter 1</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Gilestro</surname>
          <given-names>Giorgio F</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2019 Giorgio F Gilestro</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Giorgio F Gilestro</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article id="rel-obj003" ext-link-type="doi" xlink:href="10.1371/journal.pone.0224243" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">9 Oct 2019</named-content>
    </p>
    <p>MARGO (Massively Automated Real-time GUI for Object-tracking), a platform for high-throughput ethology</p>
    <p>PONE-D-19-17145R1</p>
    <p>Dear Dr. de Bivort,</p>
    <p>We are pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it complies with all outstanding technical requirements.</p>
    <p>Within one week, you will receive an e-mail containing information on the amendments required prior to publication. When all required modifications have been addressed, you will receive a formal acceptance letter and your manuscript will proceed to our production department and be scheduled for publication.</p>
    <p>Shortly after the formal acceptance letter is sent, an invoice for payment will follow. To ensure an efficient production and billing process, please log into Editorial Manager at <ext-link ext-link-type="uri" xlink:href="https://www.editorialmanager.com/pone/">https://www.editorialmanager.com/pone/</ext-link>, click the "Update My Information" link at the top of the page, and update your user information. If you have any billing related questions, please contact our Author Billing department directly at <email>authorbilling@plos.org</email>.</p>
    <p>If your institution or institutions have a press office, please notify them about your upcoming paper to enable them to help maximize its impact. If they will be preparing press materials for this manuscript, you must inform our press team as soon as possible and no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <email>onepress@plos.org</email>.</p>
    <p>With kind regards,</p>
    <p>Giorgio F Gilestro, PhD</p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
    <p>Additional Editor Comments (optional):</p>
    <p>Reviewers' comments:</p>
  </body>
</sub-article>
<sub-article id="pone.0224243.r004" article-type="editor-report">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0224243.r004</article-id>
    <title-group>
      <article-title>Acceptance letter</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Gilestro</surname>
          <given-names>Giorgio F</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2019 Giorgio F Gilestro</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Giorgio F Gilestro</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article id="rel-obj004" ext-link-type="doi" xlink:href="10.1371/journal.pone.0224243" related-article-type="reviewed-article"/>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">6 Nov 2019</named-content>
    </p>
    <p>PONE-D-19-17145R1 </p>
    <p>MARGO (Massively Automated Real-time GUI for Object-tracking), a platform for high-throughput ethology </p>
    <p>Dear Dr. de Bivort:</p>
    <p>I am pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now with our production department. </p>
    <p>If your institution or institutions have a press office, please notify them about your upcoming paper at this point, to enable them to help maximize its impact. If they will be preparing press materials for this manuscript, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information please contact <email>onepress@plos.org</email>.</p>
    <p>For any other questions or concerns, please email <email>plosone@plos.org</email>. </p>
    <p>Thank you for submitting your work to PLOS ONE.</p>
    <p>With kind regards,</p>
    <p>PLOS ONE Editorial Office Staff</p>
    <p>on behalf of</p>
    <p>Dr. Giorgio F Gilestro </p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
  </body>
</sub-article>
