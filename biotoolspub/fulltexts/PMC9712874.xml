<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Digit Imaging</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Digit Imaging</journal-id>
    <journal-title-group>
      <journal-title>Journal of Digital Imaging</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0897-1889</issn>
    <issn pub-type="epub">1618-727X</issn>
    <publisher>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9712874</article-id>
    <article-id pub-id-type="pmid">35995898</article-id>
    <article-id pub-id-type="publisher-id">683</article-id>
    <article-id pub-id-type="doi">10.1007/s10278-022-00683-y</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methods Paper</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Highdicom: a Python Library for Standardized Encoding of Image Annotations and Machine Learning Model Outputs in Pathology and Radiology</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2242-351X</contrib-id>
        <name>
          <surname>Bridge</surname>
          <given-names>Christopher P.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gorman</surname>
          <given-names>Chris</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pieper</surname>
          <given-names>Steven</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Doyle</surname>
          <given-names>Sean W.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lennerz</surname>
          <given-names>Jochen K.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kalpathy-Cramer</surname>
          <given-names>Jayashree</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Clunie</surname>
          <given-names>David A.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff8">8</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fedorov</surname>
          <given-names>Andriy Y.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff7">7</xref>
        <xref ref-type="aff" rid="Aff9">9</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Herrmann</surname>
          <given-names>Markus D.</given-names>
        </name>
        <address>
          <email>mdherrmann@mgh.harvard.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.32224.35</institution-id><institution-id institution-id-type="ISNI">0000 0004 0386 9924</institution-id><institution>Martinos Center for Biomedical Imaging, Massachusetts General Hospital, </institution></institution-wrap>Boston, MA USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.32224.35</institution-id><institution-id institution-id-type="ISNI">0000 0004 0386 9924</institution-id><institution>MGH &amp; BWH Center for Clinical Data Science, Mass General Brigham, </institution></institution-wrap>Boston, MA USA </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.32224.35</institution-id><institution-id institution-id-type="ISNI">0000 0004 0386 9924</institution-id><institution>Computational Pathology, Department of Pathology, Massachusetts General Hospital, </institution></institution-wrap>Boston, MA USA </aff>
      <aff id="Aff4"><label>4</label>Isomics, Inc., Cambridge, MA USA </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.32224.35</institution-id><institution-id institution-id-type="ISNI">0000 0004 0386 9924</institution-id><institution>Center for Integrated Diagnostics, Department of Pathology, Massachusetts General Hospital, </institution></institution-wrap>Boston, MA USA </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.38142.3c</institution-id><institution-id institution-id-type="ISNI">000000041936754X</institution-id><institution>Department of Pathology, Harvard Medical School, </institution></institution-wrap>Boston, MA USA </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="GRID">grid.38142.3c</institution-id><institution-id institution-id-type="ISNI">000000041936754X</institution-id><institution>Department of Radiology, Harvard Medical School, </institution></institution-wrap>Boston, MA USA </aff>
      <aff id="Aff8"><label>8</label>PixelMed Publishing, LLC., Bangor, PA USA </aff>
      <aff id="Aff9"><label>9</label><institution-wrap><institution-id institution-id-type="GRID">grid.62560.37</institution-id><institution-id institution-id-type="ISNI">0000 0004 0378 8294</institution-id><institution>Surgical Planning Laboratory, Department of Radiology, Brigham and Women’s Hospital, </institution></institution-wrap>Boston, MA USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>22</day>
      <month>8</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>22</day>
      <month>8</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <volume>35</volume>
    <issue>6</issue>
    <fpage>1719</fpage>
    <lpage>1737</lpage>
    <history>
      <date date-type="received">
        <day>28</day>
        <month>10</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>20</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>26</day>
        <month>5</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Machine learning (ML) is revolutionizing image-based diagnostics in pathology and radiology. ML models have shown promising results in research settings, but the lack of interoperability between ML systems and enterprise medical imaging systems has been a major barrier for clinical integration and evaluation. The DICOM<sup>®</sup> standard specifies information object definitions (IODs) and services for the representation and communication of digital images and related information, including image-derived annotations and analysis results. However, the complexity of the standard represents an obstacle for its adoption in the ML community and creates a need for software libraries and tools that simplify working with datasets in DICOM format. Here we present the <italic>highdicom</italic> library, which provides a high-level application programming interface (API) for the Python programming language that abstracts low-level details of the standard and enables encoding and decoding of image-derived information in DICOM format in a few lines of Python code. The <italic>highdicom</italic> library leverages NumPy arrays for efficient data representation and ties into the extensive Python ecosystem for image processing and machine learning. Simultaneously, by simplifying creation and parsing of DICOM-compliant files, <italic>highdicom</italic> achieves interoperability with the medical imaging systems that hold the data used to train and run ML models, and ultimately communicate and store model outputs for clinical use. We demonstrate through experiments with slide microscopy and computed tomography imaging, that, by bridging these two ecosystems, <italic>highdicom</italic> enables developers and researchers to train and evaluate state-of-the-art ML models in pathology and radiology while remaining compliant with the DICOM standard and interoperable with clinical systems at all stages. To promote standardization of ML research and streamline the ML model development and deployment process, we made the library available free and open-source at <ext-link ext-link-type="uri" xlink:href="https://github.com/herrmannlab/highdicom">https://github.com/herrmannlab/highdicom</ext-link>.</p>
      <sec>
        <title>Supplementary Information</title>
        <p>The online version contains supplementary material available at 10.1007/s10278-022-00683-y.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>DICOM</kwd>
      <kwd>Python</kwd>
      <kwd>Software</kwd>
      <kwd>Machine learning</kwd>
      <kwd>Segmentations</kwd>
      <kwd>Structured reports</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000054</institution-id>
            <institution>National Cancer Institute</institution>
          </institution-wrap>
        </funding-source>
        <award-id>HHSN26110071</award-id>
        <principal-award-recipient>
          <name>
            <surname>Fedorov</surname>
            <given-names>Andriy Y.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id>
            <institution>National Institutes of Health</institution>
          </institution-wrap>
        </funding-source>
        <award-id>5R01CA235589</award-id>
        <award-id>5P41EB015902</award-id>
        <principal-award-recipient>
          <name>
            <surname>Fedorov</surname>
            <given-names>Andriy Y.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000070</institution-id>
            <institution>National Institute of Biomedical Imaging and Bioengineering</institution>
          </institution-wrap>
        </funding-source>
        <award-id>P41EB028741</award-id>
        <principal-award-recipient>
          <name>
            <surname>Fedorov</surname>
            <given-names>Andriy Y.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000054</institution-id>
            <institution>National Cancer Institute</institution>
          </institution-wrap>
        </funding-source>
        <award-id>U24 CA264044</award-id>
        <principal-award-recipient>
          <name>
            <surname>Fedorov</surname>
            <given-names>Andriy Y.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>National Cancer Institute</institution>
        </funding-source>
        <award-id>R01 CA241817</award-id>
        <principal-award-recipient>
          <name>
            <surname>Fedorov</surname>
            <given-names>Andriy Y.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000054</institution-id>
            <institution>National Cancer Institute</institution>
          </institution-wrap>
        </funding-source>
        <award-id>U01CA242879</award-id>
        <principal-award-recipient>
          <name>
            <surname>Fedorov</surname>
            <given-names>Andriy Y.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) under exclusive licence to Society for Imaging Informatics in Medicine 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par2">Recent breakthroughs in machine learning (ML) and computational processing capabilities have led to the development of ML models that demonstrate unprecedented performance on a variety of highly complex computer vision tasks [<xref ref-type="bibr" rid="CR1">1</xref>]. State-of-the-art convolutional neural network models now regularly achieve near-human or even superhuman performance on a variety of challenging vision tasks and across different imaging modalities, including segmentation and classification of slide microscopy images in pathology [<xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR4">4</xref>] as well as computed tomography or magnetic resonance images in radiology [<xref ref-type="bibr" rid="CR5">5</xref>–<xref ref-type="bibr" rid="CR7">7</xref>]. Over the last couple of years, ML models have evolved technically within the research domain [<xref ref-type="bibr" rid="CR8">8</xref>] and the vision is that these models will soon be applied widely in clinical practice to support pathologists and radiologists in interpretation of images and ultimately improve diagnostic accuracy and efficiency [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR9">9</xref>–<xref ref-type="bibr" rid="CR11">11</xref>]. To realize this vision, healthcare enterprises are now tasked with evaluating model performance in clinical context and integrating the outputs of ML models into clinical workflows. Similarly, development of models may be expedited if image annotations generated by clinical experts and stored within clinical information systems could be directly consumed by ML training and validation pipelines. Unfortunately, this is currently impeded by the lack of standard interfaces for exchange of image annotations and ML model outputs between image analysis, image display, and image management systems.</p>
    <p id="Par3">Digital Imaging and Communication in Medicine (DICOM) is the internationally accepted standard for communication of medical images and related information across a wide range of medical imaging modalities and disciplines. Hospitals around the world have established an extensive enterprise imaging infrastructure, workflows, and software applications based on DICOM [<xref ref-type="bibr" rid="CR12">12</xref>] and pathology and radiology are converging towards using DICOM for communication of digital images [<xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR16">16</xref>]. However, existing pathology as well as radiology systems primarily rely on non-standard formats and interfaces for the storage and exchange of image annotations and computational image analysis results, to which we hereafter collectively refer as <italic>annotations</italic>. Similarly, ML models developed by researchers generally receive and return annotations in a variety of customized formats that are incompatible with clinically available image management and display systems and that lack metadata required for interpretation and use of the information in clinical context. Instead, it would be desirable if ML models were developed according to the FAIR guiding principles [<xref ref-type="bibr" rid="CR17">17</xref>] using standardized metadata to allow for annotations to be findable, accessible, interoperable, and reusable. The DICOM standard provides information object definitions (IODs), such as Segmentations and Structured Reports, for annotations [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR19">19</xref>], and implementation of these IODs to enable interoperable storage and communication of ML model outputs has been proposed by the Integrating the Healthcare Enterprise (IHE) Radiology Technical Committee [<xref ref-type="bibr" rid="CR20">20</xref>].</p>
    <p id="Par4">Python is the de facto standard programming language of data science and provides a rich ecosystem for scientific computing, image processing, and machine learning [<xref ref-type="bibr" rid="CR21">21</xref>–<xref ref-type="bibr" rid="CR24">24</xref>]. The majority of ML models are developed and deployed in the form of Python programs. The <italic>pydicom</italic> library [<xref ref-type="bibr" rid="CR25">25</xref>] provides data structures and routines for storing and accessing data of DICOM datasets (parts 5 and 6 of the DICOM standard) as well as reading and writing DICOM files (part 10 of the DICOM standard). However, <italic>pydicom</italic> has no concept of IODs (parts 3 and 16 of the DICOM standard) and as such leaves it to each developer to set all attributes required by an IOD manually and ensure that they follow all relevant constraints when creating new DICOM objects containing annotations. Similarly, parsing the annotation IODs for the information relevant to a particular ML task using the <italic>pydicom</italic> API is challenging due to their highly nested and interdependent structure. Consequently, both tasks are slow, complex, and error-prone and require considerable knowledge of the DICOM standard. We therefore identified a need for a higher-level abstraction layer between the ML model developer and the low-level encoding rules of the DICOM standard. This motivated us to create the open-source <italic>highdicom</italic> library, which provides a high-level application programming interface for creating and reading annotations in DICOM format using the Python programming language. Our goal in releasing this library is to enable ML processes that achieve interoperability between ML models and clinical information systems throughout the entire model development and deployment lifecycle while avoiding the complexity that this currently entails. Furthermore, we aimed to create a library that is applicable across a range of common ML tasks and imaging domains.</p>
    <p id="Par5">In this article, we first describe the design and implementation of the <italic>highdicom</italic> library to meet this unmet need and then assess the library’s capabilities in encoding and decoding annotations (either generated by human readers or ML models) in DICOM format. We perform experiments that demonstrate the use of the library during ML model training and inference and show how the library enables the development of ML models that are interoperable with established image management and display systems and thus can be readily integrated into an enterprise medical imaging environment. To this end, we consider a variety of clinically relevant computer vision problems and multiple imaging modalities across different medical disciplines, placing a focus on lung tumor detection in slide microscopy images in pathology and computed tomography images in radiology as an illustrative use case.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>Design Overview and Application Programming Interface (API)</title>
      <p id="Par6">The software components responsible for transforming the data input and output from ML models, and thereby ensuring interoperability with adjacent systems, are commonly referred to as data pipelines [<xref ref-type="bibr" rid="CR26">26</xref>, <xref ref-type="bibr" rid="CR27">27</xref>]. During inference, pipelines are responsible for retrieving and preprocessing input images into an in-memory format that can be consumed by the model and encoding the model’s in-memory outputs into a form suitable for communication and storage. During training, they retrieve and preprocess input images and additionally, if required, decode annotations into an in-memory representation of the target for model training. The <italic>highdicom</italic> library is intended to operate within data pipelines that connect clinical infrastructure using the DICOM standard to popular Python ML frameworks such as <italic>PyTorch</italic> [<xref ref-type="bibr" rid="CR28">28</xref>] and <italic>Tensorflow</italic> [<xref ref-type="bibr" rid="CR29">29</xref>], and is focused on annotations rather than the input images themselves. The library’s core functionality is twofold: First, encoding model outputs in the form of <italic>NumPy</italic> arrays together with relevant metadata into annotations in the form of <italic>pydicom</italic> objects (Fig. <xref rid="Fig1" ref-type="fig">1</xref>A). Second, decoding annotations provided as <italic>pydicom</italic> datasets to obtain targets in the form of <italic>NumPy</italic> arrays (Fig. <xref rid="Fig1" ref-type="fig">1</xref>B) by reading and interpreting the included metadata. We chose the n-dimensional <italic>NumPy</italic> array data structure [<xref ref-type="bibr" rid="CR22">22</xref>] as an in-memory representation of model outputs and targets because it is interoperable with <italic>pydicom</italic> as well as <italic>PyTorch</italic> and <italic>Tensorflow</italic> and many other well-established Python image processing libraries (e.g., <italic>OpenCV</italic> [<xref ref-type="bibr" rid="CR30">30</xref>] and <italic>ITK</italic> [<xref ref-type="bibr" rid="CR31">31</xref>]).<fig id="Fig1"><label>Fig. 1</label><caption><p>Intended use of <italic>highdicom</italic> in data pipelines during machine learning model training and inference workflows. <bold>A</bold> Encoding of model outputs upon inference in the postprocessing pipeline. <bold>B</bold> Decoding of image annotations for model training in the preprocessing pipeline</p></caption><graphic xlink:href="10278_2022_683_Fig1_HTML" id="MO1"/></fig></p>
      <sec id="FPar1">
        <title>API Overview</title>
        <p id="Par7">We designed <italic>highdicom</italic> following the object-oriented programming paradigm and modelled the API according to the DICOM Information Model, which specifies different abstract data types that are referred to as information object definitions (IODs) (Fig. <xref rid="Fig2" ref-type="fig">2</xref>). An IOD defines the set of required and optional DICOM attributes that may be included into DICOM objects. We selected various IODs for storage of annotations and implemented each in <italic>highdicom</italic> as a Python class.</p>
        <p>Strictly speaking, each Python class implements a DICOM Storage Service-Object Pair (SOP) Class, which is the data structure within the DICOM standard that stores the attributes defined by an IOD. An instance of such a Python class thus represents a DICOM SOP instance and serves as a container for a DICOM dataset, where each instance attribute holds the value of a DICOM data element.</p>
        <p>The Python classes are ultimately derived from the pydicom.Dataset class from the existing pydicom package and therefore inherit low-level behaviors, such as accessing, setting, iterating over data elements, and reading/writing to/from files that many developers are already familiar with. It further allows developers to retain low-level control over all data elements in order to add to or alter information in objects constructed by <italic>highdicom</italic>. Below pydicom.Dataset in the class hierarchy, there is a common abstract base class called highdicom.SOPClass (Fig. <xref rid="Fig2" ref-type="fig">2</xref>A), which abstracts the attributes that are required by all SOP classes. Specific SOP classes are then implemented by dedicated Python classes that are derived from the abstract base class (Fig. <xref rid="Fig2" ref-type="fig">2</xref>B). In this way, we aim to provide an idiomatic Python interface that abstracts as much of the low-level DICOM encoding and decoding rules as possible while staying close to the standard DICOM terminology to avoid potential ambiguities.</p>
      </sec>
      <sec id="FPar2">
        <title>Encoding of DICOM SOP Instances</title>
        <p id="Par10">The process of encoding information in derived objects is implemented in the constructor methods of the corresponding SOP classes (either in the highdicom.SOPClass abstract base class or in derived IOD-specific classes). For construction of an SOP instance, the developer provides the image-derived information that is outputted by a model (e.g., pixel data or graphic data) together with descriptive contextual information that the standard requires for the corresponding IOD. Attribute values that are static or can be derived from provided arguments are automatically set upon object construction. For example, relevant metadata about the patient, the study, or the specimen are automatically copied from the metadata of provided source images and references to the source images are included in the derived objects (Fig. <xref rid="Fig3" ref-type="fig">3</xref>A–B). Furthermore, the constructor automatically validates the content of created SOP instances through runtime checks to ensure that constructed objects are fully compliant with the relevant IOD in the standard.</p>
        <p>By design, all required information must be passed to the SOP class constructor when creating the object, and thereafter the object remains immutable through the <italic>highdicom</italic> API (though an experienced developer may use the lower-level interface provided by the <italic>pydicom</italic> API to modify the object if required). This means that the constructor can validate all input parameters at once accounting for all interdependencies and conditional logic between attributes. It also reflects the intent of the standard in that DICOM objects are immutable following creation.</p>
      </sec>
      <sec id="FPar3">
        <title>Decoding of DICOM SOP Instances</title>
        <p id="Par12">The <italic>pydicom</italic> library provides a powerful low-level Python interface to developers to access DICOM data elements of a dataset directly, with little abstraction from the details of the data format. While this is appropriate for many image objects, the complexity of the derived objects used for annotations means that accessing the desired information using the <italic>pydicom</italic> API requires a detailed knowledge of the underlying data structures and in our experience results in a verbose, cumbersome, and error-prone process. Therefore, we have endowed <italic>highdicom</italic> SOP classes with additional methods (not in the standard) that provide a means for developers to access, filter, and interpret the content of a DICOM object when preparing image annotations to be used as targets for a training algorithm. In addition, <italic>highdicom</italic> SOP classes implement alternative constructor methods that allow for the creation of <italic>highdicom</italic> SOP instances from existing pydicom.Dataset objects, which were read from a file or retrieved over network, and thereby enhance the objects with additional, modality-specific methods and properties for data access.</p>
      </sec>
      <sec id="FPar4">
        <title>Data Types and Structures</title>
        <p id="Par13">The majority of DICOM metadata attribute values that are passed to and returned from the <italic>highdicom</italic> API upon encoding and decoding of SOP instances have primitive, built-in Python types such as strings (str), integers (int), and floats (float). To further encapsulate closely related metadata of composite DICOM data types (DICOM Sequences or Sequence Items) and to improve code readability and reusability, the <italic>highdicom</italic> API further provides custom Python types, which are implemented in the form of Python classes and are generally derived from either pydicom.Dataset or pydicom.Sequence. DICOM bulkdata values such as pixel data or vector graphic data are passed to and returned from <italic>highdicom</italic> Python classes as NumPy objects (numpy.ndarray).<fig id="Fig2"><label>Fig. 2</label><caption><p>Implementation of the DICOM Information Model in Python. <bold>A</bold> The <italic>highdicom</italic> Python abstract base class highdicom.SOPClass and its relationship to an DICOM information object definition (IOD) and DICOM Storage Service-Object Pair (SOP) Class. <bold>B</bold> A <italic>highdicom</italic> Python class for a specific DICOM IOD and SOP Class (exemplified by highdicom.seg.Segmentation that implements the DICOM Segmentation Storage SOP Class defined by the DICOM Segmentation IOD)</p></caption><graphic xlink:href="10278_2022_683_Fig2_HTML" id="MO2"/></fig></p>
      </sec>
    </sec>
    <sec id="Sec4">
      <title>Storage of Annotations in DICOM Format</title>
      <p id="Par14">Having described the general approach taken by our library, we now begin to discuss the individual IODs that we selected for implementation. The DICOM standard specifies a wide range of IODs for different types of DICOM objects, including images acquired by various modalities (e.g., computed tomography or whole slide microscopy) as well as image-derived information generated by image display, processing, or analysis systems [<xref ref-type="bibr" rid="CR32">32</xref>]. For implementation in the <italic>highdicom</italic> library, we considered standard IODs that provide mechanisms to store image annotations for common ML tasks across pathology and radiology use cases. We thereby focused on the following decision problems and their corresponding annotations (Fig. <xref rid="Fig3" ref-type="fig">3</xref>A) [<xref ref-type="bibr" rid="CR1">1</xref>]: <list list-type="order"><list-item><p id="Par15"><italic>Image classification</italic> — class labels in the form of discrete binary or categorical values and optionally class scores in the form of continuous probabilistic values (Fig. <xref rid="Fig3" ref-type="fig">3</xref>A upper panel)</p></list-item><list-item><p id="Par16"><italic>Image segmentation</italic> — class labels at pixel resolution that identify semantically distinct regions of interest (ROIs) within an image in the form of raster graphics (Fig. <xref rid="Fig3" ref-type="fig">3</xref>A middle panel)</p></list-item><list-item><p id="Par17"><italic>Object detection</italic> — spatial coordinates for individual ROIs in the form of vector graphics (commonly bounding boxes), combined with class labels and detection scores (Fig. <xref rid="Fig3" ref-type="fig">3</xref>A lower panel)</p></list-item></list>We identified three IODs that together allow for the encoding of annotations for these common use cases: the Segmentation IOD and two Structured Report (SR) IODs. The Segmentation IOD was selected to encode ROIs returned by image segmentation models as raster graphics. The Comprehensive SR and Comprehensive 3D SR IODs were chosen to encode vector graphic ROIs returned by object detection models as well as class labels, scores, and measurements returned by image classification and regression models (Fig. <xref rid="Fig3" ref-type="fig">3</xref>A). All three IODs are designed to be agnostic of the imaging modality and able to support use cases across medical disciplines including pathology and radiology.<fig id="Fig3"><label>Fig. 3</label><caption><p>Encoding of machine learning model outputs in DICOM. <bold>A</bold> Information entities and the Python types used to represent machine learning model inputs (images) and outputs (image-derived information) for three common decision problems. <bold>B</bold> Schematic overview of the content of source image objects (exemplified by a DICOM VL Whole Slide Microscopy Image) and derived objects (DICOM Comprehensive 3D SR and DICOM Segmentation). Note that descriptive metadata is copied from source to derived objects and derived objects may reference information contained in source images or other derived objects</p></caption><graphic xlink:href="10278_2022_683_Fig3_HTML" id="MO3"/></fig></p>
      <sec id="FPar5">
        <title>DICOM Segmentation Images</title>
        <p id="Par18">The Segmentation IOD is implemented in highdicom as the highdicom.seg.Segmentation Python class and allows for the encoding of one or more components, which in DICOM are referred to as <italic>segments</italic>. Each segment may represent a pixel class (category) or an individual instance of a given class as generated by semantic segmentation or instance segmentation models [<xref ref-type="bibr" rid="CR33">33</xref>], respectively. Segments may further have binary or fractional type, either representing a mask of Boolean values where non-zero pixels encode class membership or a mask of decimal numbers where pixels encode class probability.</p>
        <p>In order to encode a DICOM Segmentation image, the developer passes to the constructor a mask as a numpy.ndarray (of either Boolean, integer, or floating point data type) along with additional metadata that describe the meaning of each segment within the segmentation (highdicom.seg.SegmentDescription) and the algorithm responsible for producing the segmentation (highdicom.AlgorithmIdentificationSequence).</p>
        <p>To facilitate decoding of DICOM Segmentation images, the highdicom.seg.Segmentation class provides methods that allow developers to filter segments by their label, segmented property category or type, or tracking identifiers. It further provides methods to obtain a segmentation mask as a numpy.ndarray for a given set of segments and source image frames. While conceptually straightforward, in practice, several steps are necessary to achieve this correctly: (i) Determining which frames stored in the Segmentation image are relevant to a given set of segments and source image frames based on the multi-frame dimension indexing information, (ii) Sorting the Segmentation image frames according the query, (iii) Adding in missing pixel values in case of sparse Segmentation images where background image frames were omitted during encoding to save storage space (iv) (Optionally) combining multi binary segments into a multi-class label map.</p>
      </sec>
      <sec id="FPar6">
        <title>DICOM Structured Report Documents</title>
        <p id="Par21">There are various IODs defined by the standard that utilize structured reporting, but we selected the Comprehensive SR (highdicom.sr.ComprehensiveSR) and Comprehensive 3D SR (highdicom.sr.Comprehensive3DSR) IODs for implementation in <italic>highdicom</italic> because they provide the most flexible mechanisms for storing annotations. In addition to the IOD definitions, the standard provides SR templates, which serve as schemas that define how the content of an SR document shall be structured and how the information shall be encoded. A template consists of a sequence of <italic>content items</italic>, each defining a name-value pair (or question-answer pair) that encodes a domain-specific property or concept (Fig. <xref rid="Fig4" ref-type="fig">4</xref>A). Notably, both concept names and values have a composite data type and are each encoded by one or more DICOM attributes. Concept names are coded using standard medical terminologies and ontologies such as the DICOM Controlled Terminology or the Systematized Nomenclature of Medicine Clinical Terms (SNOMED CT) and thereby get endowed with an explicit, domain-specific meaning [<xref ref-type="bibr" rid="CR34">34</xref>]. The structure of the corresponding value depends on the <italic>value type</italic>, which defines a set of DICOM attributes that are included in the SR document to represent the assigned value.</p>
        <p>Within <italic>highdicom</italic>, the highdicom.sr.CodedConcept class is an important data type that encapsulates the DICOM attributes required to code a concept using a standard coding scheme within a single Python object. We further contributed lower-level data types to the underlying <italic>pydicom</italic> library that provide programmatic access to codes included in the DICOM standard, specifically the DICOM Controlled Terminology (DCM), SNOMED-CT (SCT), and Unified Code for Units of Measure (UCUM) coding schemes. These codes that are included in the <italic>pydicom</italic> library are fully compatible with the coded concepts of the <italic>highdicom</italic> library and can generally be used interchangeably throughout the API. Furthermore, for each of the different DICOM content item value types, we have implemented a separate Python class that is derived from pydicom.Dataset and encapsulates both the coded concept name and the corresponding value of the given type (Fig. <xref rid="Fig4" ref-type="fig">4</xref>B).</p>
        <p>Notable content item classes include highdicom.sr.CodeContentItem, which may be used to store class labels as coded values, and highdicom.sr.NumContentItem, which may be used to store a measurement along with its unit. ROIs may be either encoded by value or by reference and stored within or outside of the SR document content, respectively. In the case of vector graphics (including but not limited to bounding boxes), the graphic data may be stored within the SR document and encoded via DICOM content items of value type SCOORD3D, which encodes 3D spatial coordinates of geometric objects in the frame of reference (patient or slide coordinate system). This value type is implemented in <italic>highdicom</italic> by the highdicom.sr.Scoord3DContentItem Python class (Fig. <xref rid="Fig4" ref-type="fig">4</xref>B). In the case of raster graphics, the pixel data of Segmentation images are stored outside of the SR document, but specific segments can be referenced from within the SR document via content items of value type IMAGE. (implemented by the highdicom.sr.ImageContentItem Python class), which includes DICOM identifiers for the referenced image object and segments contained therein.<fig id="Fig4"><label>Fig. 4</label><caption><p>Encoding of annotations as DICOM Structured Reporting (SR) content items and templates for inclusion into an SR document. <bold>A</bold> SR content items of different values types. <bold>B</bold> Implementation of SR content items in <italic>highdicom</italic> by classes that inherit from pydicom.Dataset. <bold>C</bold> SR template TID 1500 “Measurement Report” and included sub-templates. <bold>D</bold> Implementation of SR templates in <italic>highdicom</italic> by classes that inherit from pydicom.Sequence</p></caption><graphic xlink:href="10278_2022_683_Fig4_HTML" id="MO4"/></fig></p>
        <p>The standard provides different SR templates for a variety of common clinical use cases and diagnostics tasks, such as recording X-ray dose exposure or reporting echocardiography findings. We chose to implement the more generic template TID 1500 “Measurement Report” in <italic>highdicom</italic> for encoding annotations, because the template provides standard content items to describe measurements and qualitative evaluations of images as well as individual image ROIs (Fig. <xref rid="Fig4" ref-type="fig">4</xref>C) and because it has already been successfully used for standardized communication of quantitative image analysis results [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR19">19</xref>]. Importantly, sub-templates that can be included in TID 1500 allow for the encoding of annotations of entire images, planar image regions, or volumetric image regions (Fig. <xref rid="Fig4" ref-type="fig">4</xref>C). Within the library’s API, these selected templates are implemented by Python classes, which are derived from an abstract base class highdicom.sr.Template, which is in turn derived from pydicom.Sequence (Fig. <xref rid="Fig4" ref-type="fig">4</xref>D). The constructors of these Python classes require the developer to pass the relevant data via named parameters but then handle its inclusion in the template with the correct concept names as well as ensuring all constraints are satisfied.</p>
        <p>When decoding SR documents, the high degree of nesting in the document tree and the variable order of content items at each level means that finding a particular content item of interest in the tree potentially requires multiple nested loops. Furthermore, as described above, each content item is a collection of data elements that must first be parsed and interpreted as a unit. The Python classes that implement SR templates and individual SR content items provide methods and properties to facilitate data access. Using the provided methods, measurement groups within a highdicom.sr.ComprehensiveSR or highdicom.sr.Comprehensive3DSR object can be filtered by their finding type, finding site, or tracking identifiers. Individual measurements and qualitative evaluations contained within these groups can similarly be filtered by their concept name. Furthermore, <italic>highdicom</italic> classes representing SR templates and content items provide access to their content items or values, respectively, through Python properties that return the data either as a built-in Python type or a custom <italic>highdicom</italic> type (which will typically match the type of the argument passed to the constructor).</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec5">
    <title>Results</title>
    <p id="Par26">Having laid the foundation through the description of the library’s design and implementation, we now proceed to demonstrating the capabilities of the library. We consider a concrete use case of developing machine learning models for lung tumor detection in both pathology and radiology and deploying the models clinically using a common platform and framework that is applicable independent of the medical discipline or imaging modality. In this section, we first describe the steps necessary to encode the annotations in DICOM using <italic>highdicom</italic>, including the description of the detected region of interest, the identified finding, and related measurements and qualitative evaluations. We then show through a series of experiments how <italic>highdicom</italic> can streamline ML model training and inference for this use case.</p>
    <sec id="Sec6">
      <title>Highdicom Facilitates Encoding of Image Annotations in DICOM Format</title>
      <sec id="FPar7">
        <title>Structured Reporting using Standard Medical Terminologies</title>
        <p id="Par27">While the approach of using standardized vocabularies is powerful and important for interoperability, it complicates working with the data. For example, comparing two concepts for equality requires comparison of their code values, coding scheme designators, and coding scheme versions. The highdicom.sr.CodedConcept and the lower-level <italic>pydicom</italic> types facilitate the use of coded concepts for structured reporting of annotations in Python at a high level of abstraction (code snippet 1).<graphic position="anchor" xlink:href="10278_2022_683_Figa_HTML" id="MO5"/></p>
      </sec>
      <sec id="FPar8">
        <title>Describing ROI Evaluations and Measurements</title>
        <p id="Par28">The coded concept type forms the basis for additional higher-level composite data types for DICOM structured reporting such as SR content items. Code snippet 2 demonstrates example content items for the encoding of a tumor image region of interest, the tumor finding, and an associated tumor measurement.<graphic position="anchor" xlink:href="10278_2022_683_Figb_HTML" id="MO6"/></p>
        <p>This demonstrates using the SCT vocabulary built in to <italic>pydicom</italic> to encode a concept name as “Morphology,” and a domain-specific coding scheme, the International Classification of Diseases for Oncology (ICD-O), to specify the exact type of tumor as the concept value. Of note, the area measurement in our example is encoded in a well-defined physical unit, as would be expected for clinical decision-making. The corresponding image region is defined in the same physical space. In DICOM, image regions may be defined by spatial coordinates within either the pixel matrix of an individual image or, as in this example, the frame of reference (the 3D patient- or slide-based physical coordinate system). While the former appears more straightforward, the latter is more general and allows for annotations derived from transformed versions of the original images with arbitrary affine transformations (rotations, scaling, etc.) as well as crops.</p>
      </sec>
      <sec id="FPar9">
        <title>Creation of DICOM Annotation Objects</title>
        <p id="Par30">The computer vision problem of tumor detection could be solved using either an object detection or image segmentation model. Accordingly, the output of these models and the annotations used to train them can be encoded using the highdicom.sr.Comprehensive3DSR (code snippet 3) and highdicom.seg.Segmentation (code snippet 4) classes respectively. In either case, this involves describing the finding and the anatomical site of the finding as well as supplying relevant contextual metadata such as the device or person reporting the observation. However, note that it is not necessary to specify patient, study, or specimen information since <italic>highdicom</italic> copies this metadata directly from the source images provided as evidence to the constructor.<graphic position="anchor" xlink:href="10278_2022_683_Figc_HTML" id="MO7"/><graphic position="anchor" xlink:href="10278_2022_683_Figd_HTML" id="MO8"/></p>
      </sec>
    </sec>
    <sec id="Sec7">
      <title>Highdicom Facilitates Efficient Loading and Decoding of Images and Corresponding Annotations</title>
      <p id="Par31">When it comes to training a model for tumor detection, annotations may be provided in the form of either raster graphics within a Segmentation image or vector graphics within an SR document. In both cases, <italic>highdicom</italic> provides methods that simplify access to, and interpretation of, the relevant content in the annotation SOP instances. If annotations are provided as raster graphics within a Segmentation image, model training may require combining binary bit planes from multiple segments in the Segmentation image to create a single label map, represented as a NumPy array, in which pixels encode tumor identities. If instead annotations are provided as vector graphics within an SR document, the spatial coordinates of image regions will need to be collected from within the document content tree and passed as NumPy arrays to training processes. Snippets 5 and 6 show example usage of the methods that <italic>highdicom</italic> provides for these purposes.<graphic position="anchor" xlink:href="10278_2022_683_Fige_HTML" id="MO9"/><graphic position="anchor" xlink:href="10278_2022_683_Figf_HTML" id="MO10"/></p>
    </sec>
    <sec id="Sec8">
      <title>Highdicom Facilitates Decoding and Encoding of Annotations During Model Training and Inference, Respectively</title>
      <p id="Par32">To establish a proof-of-concept standard-based ML workflow and to demonstrate the utility of the <italic>highdicom</italic> library for ML, we performed a set of experiments on the training and evaluation of deep convolutional neural network (CNN) models using publicly available slide microscopy (SM) and computed tomography (CT) image datasets. We emphasize that our intent is to demonstrate a complete ML workflow for pathology and radiology fully based on DICOM, rather than create models with optimal performance or reach state-of-the-art for a particular task.</p>
      <p id="Par33">For pathology, we trained and evaluated models using lung cancer collections of slide microscopy (SM) images from The Cancer Imaging Archive (TCIA) [<xref ref-type="bibr" rid="CR35">35</xref>] that were acquired as part of The Cancer Genome Atlas (TCGA) Lung Adenocarcima (LUAD) or Lung Squamous Cell Carinoma (LUSC) projects and which we converted into DICOM format as previously described [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR36">36</xref>]. For radiology, we used the collection of CT images of the Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI) (LIDC-IDRI) [<xref ref-type="bibr" rid="CR35">35</xref>, <xref ref-type="bibr" rid="CR37">37</xref>, <xref ref-type="bibr" rid="CR38">38</xref>], which were already available in DICOM format. We used available measurements and qualitative evaluations for these SM and CT files provided by TCIA as image annotations, which we encoded in DICOM SR documents or DICOM Segmentation images using <italic>highdicom</italic> (see <xref rid="MOESM1" ref-type="media">supplementary methods</xref>), resulting in training sets for CT lung nodule detection and SM image classification encoded entirely within DICOM format.</p>
      <p id="Par34">We developed proof-of-concept ML models based on published algorithms and implemeted data pre- and postprocessing pipelines for each model to load model inputs from DICOM SM or CT image instances, annotations from DICOM SR documents or DICOM Segmentation images respectively, and store outputs to DICOM SR instances. For pathology, we implemented a weakly supervised image classification model using multiple instance learning with the objective to classify individual SM image frames of lung tissue sections into slide background, normal lung tissue, lung adenocarcinoma, or lung squamous cell carcinoma similar to prior work described by Coudray et al. [<xref ref-type="bibr" rid="CR39">39</xref>]. To this end, we used a modified version of a ResNet-101 model [<xref ref-type="bibr" rid="CR40">40</xref>], which we initialized with parameters from pre-training on ImageNet [<xref ref-type="bibr" rid="CR41">41</xref>] and further optimized using SM image frames and image annotations from the TCGA collections similar to the algorithms described by Lerousseau et al. [<xref ref-type="bibr" rid="CR42">42</xref>] and Lu et al. [<xref ref-type="bibr" rid="CR43">43</xref>]. During training, each training sample was created by selecting one or more frames of an SM image from a given series (i.e., digital slide) together with the corresponding image-level annotations obtained from the SR document using <italic>highdicom</italic>. During inference, the data postprocessing pipeline collects predicted class probabilities for each image frame, constructs low-resolution probabilistic segmentation mask for each class (with pixels representing class probabilitisties for individual frames), and finally encodes the constructed masks in a DICOM Segmentation image with FRACTIONAL Segmentation Type and PROBABILITY Fractional Segmentation Type (Fig. <xref rid="Fig5" ref-type="fig">5</xref> upper panel). The postprocessing pipeline further thresholds the individual class probability predictions to generate a binary segmentation mask for each class (normal lung tissue, lung adenocarcinoma, or lung squamous carcinoma), performs a connected component analysis and border following to find the contours of ROIs representing class instances, and encodes each detected ROI together with additional measurements and qualitative evaluations in a DICOM Comprehensive 3D SR document (Fig. <xref rid="Fig5" ref-type="fig">5</xref> lower panel).<fig id="Fig5"><label>Fig. 5</label><caption><p>Schematic overview of output post-processing pipelines of the pathology model, which classifies individual image frames of a multi-frame SM image of a lung tissue section specimen. Outputted scores get transformed into a segmentation mask from which bounding boxes of the tumor regions are derived. The coordinates of the bounding box vertices are stored as 3D spatial coordinates in the reference slide coordinate system</p></caption><graphic xlink:href="10278_2022_683_Fig5_HTML" id="MO11"/></fig></p>
      <p id="Par35">For radiology, we implemented an object detection model to detect lung nodules in individual CT slices of the chest. We used an off-the-shelf implementation of the widely used RetinaNet convolutional neural network [<xref ref-type="bibr" rid="CR44">44</xref>] available with the <italic>torchvision</italic> package<xref ref-type="fn" rid="Fn1">1</xref>. Specifically, we used a RetinaNet model with the ResNet-50 backbone [<xref ref-type="bibr" rid="CR40">40</xref>] and initialized the model with weights from pre-training on the ImageNet dataset [<xref ref-type="bibr" rid="CR41">41</xref>]. During training, each training sample was created by selecting a random CT image frame (2D axial slice) from a given series. The annotations encoded in DICOM Segmentation images were read using <italic>highdicom</italic> and the bounding box containing each nodule in the slice was calculated on-the-fly from the contained segments and used as a ground truth label for supervised training of the RetinaNet model. The post-processing pipeline for the chest CT model collected predicted bounding boxes and their detection scores outputted by the RetinaNet model for every frame in the CT series and encoded them in a DICOM Comprehensive 3D SR document, with vector graphics used to represent bounding box coordinates and detection scores encoded as a measurement of the region represented by the bounding box (Fig. <xref rid="Fig6" ref-type="fig">6</xref>).<fig id="Fig6"><label>Fig. 6</label><caption><p>Schematic overview of output post-processing pipeline of the radiology model, which detects lung nodules in image frames of single-frame CT images of the thorax and outputs bounding boxes of lung nodule regions. The coordinates of the bounding box vertices are stored as 3D spatial coordinates in the reference patient coordinate system</p></caption><graphic xlink:href="10278_2022_683_Fig6_HTML" id="MO12"/></fig></p>
      <sec id="FPar10">
        <title>Annotations Generated by Highdicom can be Stored in Image Management Systems using DICOMweb Services and Visualized using DICOM-Compliant Display Systems</title>
        <p id="Par36">After model training, we selected one pathology and radiology model for further clinical evaluation and deployed it into a production-like environment, consisting of an image management system (IMS) with a DICOMweb interface [<xref ref-type="bibr" rid="CR45">45</xref>] and DICOM-compliant image display systems. Specifically, a dcm4chee-arc-light archive<xref ref-type="fn" rid="Fn2">2</xref> served as the IMS and we stored SM and CT images in the IMS via DICOMweb RESTful services using the <italic>dicomweb-client</italic> Python library [<xref ref-type="bibr" rid="CR14">14</xref>].<xref ref-type="fn" rid="Fn3">3</xref> Upon inference, the data preprocessing pipelines retrieved DICOM SM or CT images from the IMS over network using the <italic>dicomweb-client</italic>, read and interpreted the image metadata and pixel data using <italic>pydicom</italic>, and passed the pixel data as inputs to the model as NumPy arrays. Model outputs received as NumPy arrays were encoded as DICOM SR documents in the data postprocessing pipeline using <italic>highdicom</italic> and stored back in the IMS over network using the <italic>dicomweb-client</italic>.</p>
        <p>For radiology, we visualized the ground truth lung nodules using the OHIF <xref ref-type="fn" rid="Fn4">4</xref> viewer, which retrieved the DICOM Segmentation images over network using the <italic>dicomweb-client</italic> library and displayed each segment as a raster graphic on top of the corresponding CT images (supplementary Fig. <xref rid="MOESM1" ref-type="media">S3</xref>B). We additionally visualized detected ROIs using the open-source 3D Slicer <xref ref-type="fn" rid="Fn5">5</xref> software (supplementary Fig. <xref rid="MOESM1" ref-type="media">S3</xref>B).</p>
        <p>For pathology, we visualized detected lung tumor regions using the Slim <xref ref-type="fn" rid="Fn6">6</xref> viewer, which retrieved the DICOM SR documents over network using the <italic>dicomweb-client</italic> JavaScript library <xref ref-type="fn" rid="Fn7">7</xref> and displayed the spatial coordinates of each ROI contained in the SR documents as a vector graphic on top of the corresponding SM images (supplementary Fig. <xref rid="MOESM1" ref-type="media">S4</xref>).</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec9">
    <title>Discussion</title>
    <p id="Par39">The main contributions of this paper are: (i) The demonstration that image annotations can be encoded and exchanged in DICOM format using existing DICOM IODs and services, respectively. (ii) The development of a software library that provides a high-level application programming interface (API) for the Python programming language to facilitate creation of DICOM objects for storage of image-derived information, including image annotations, as well as accessing and interpreting information stored in DICOM objects. (iii) The establishment of a standard-based workflow for ML model training and inference that is generally applicable across different imaging modalities, computer vision problems, and medical disciplines.</p>
    <p id="Par40">In developing the <italic>highdicom</italic> library and establishing an ML workflow based on DICOM, we made several observations that merit further discussion.</p>
    <sec id="Sec10">
      <title>Clinical Use of Machine Learning Model Outputs in Pathology and Radiology Requires Domain-Specific Metadata</title>
      <p id="Par41">Medical images and image annotations must not only contain the actual data, such as the pixel data in case of an image, but require additional metadata that enable interpretation and use of the data. Such metadata can be grouped into information related to data representation, information about the data acquisition process and equipment, and information related to the clinical context in which the data was acquired, including identifying and descriptive information about the patient, study, and specimens. This contextual information that describes how the data relates to the real world is crucial for unambiguous interpretation of medical images as well as any regions of interest, measurements, or qualitative evaluations derived from them.</p>
      <p id="Par42">To ensure that clinical decisions based on this information are made for the right patient and specimen and in the correct clinical setting, real-world entities need to be uniquely identifiable throughout the digital workflow. As such it is desirable to establish an unambiguous association between the digital information (images and image annotations) on the one hand and clinically relevant real-world entities (patients, specimens, etc.) on the other hand by including clinical identifiers into digital objects. This furthermore facilitates exchange of information between departments and institutions upon transfer and referral of patients. DICOM specifies standard information object definitions and attributes to store and exchange digital images and image-derived information together with the relevant clinical identifiers as composite objects. The <italic>highdicom</italic> API facilitates access to and creation of such standard DICOM objects using the Python programming language and thereby enables data engineers and scientists to develop ML models and systems that can receive inputs and return outputs that include relevant identifiers for clinical application. Additionally, in many cases including patient information and references to the source images, <italic>highdicom</italic> will find and copy the relevant metadata from the dataset of the source image to reduce the room for human error as far as possible.</p>
      <p id="Par43">In addition to identifiers, DICOM objects contain descriptive metadata about the imaging target (patient or specimen), the imaging modality and procedure, the anatomical location of the imaging or surgical procedure, and in case of pathology the preparation of the specimen. This information can be critical for the interpretation of images or image annotations by ML systems during model training or inference as well as by other systems that use or interpret model outputs. Most importantly this descriptive metadata allows automated systems to decide whether or not a given information object may be appropriate to use in the context of the intended use or select one of several available objects for analysis or display [<xref ref-type="bibr" rid="CR46">46</xref>]. Descriptive metadata is also useful for performing model validation and error analysis to determine groups of inputs, according to patient demographic information, pre-analytic specimen preparation variables, or image acquisition parameters, upon which models are under-performing. Furthermore, the DICOM standard provides mechanisms for describing the image analysis algorithm (name, version, etc.) as well as the completeness or validity of analysis results at various stages of the clinical decision making process. For example, the DICOM SR IODs include attributes that allow clinical users to verify or, if necessary, complete or correct ML model outputs, to record the verification or modification activity, and to create an audit trail that establishes the relationship between the document containing the verified or modified content and the predecessor document containing the unverified model outputs. These mechanisms are critical for safe clinical application of ML models, since their outputs are generally intended for clinical decision support rather than independent decision making [<xref ref-type="bibr" rid="CR47">47</xref>] and thus require review by a clinical expert before inclusion into the medical record.</p>
      <p id="Par44">The <italic>highdicom</italic> library enables developers to access relevant descriptive information in received DICOM objects upon preprocessing or include such information into generated DICOM object upon post-processing and thereby make it available to downstream clinical systems. The high-level and well-tested abstractions provided by <italic>highdicom</italic> allow developers to achieve this goal with only a few lines of Python code.</p>
    </sec>
    <sec id="Sec11">
      <title>Standard Coding Schemes Enable Unambiguous Interpretation of Image Annotations</title>
      <p id="Par45">Subtle differences in the description of imaging findings can lead to drastically different treatment decisions. To ensure that image annotations can be interpreted unambiguously by both clinicians and devices or automated systems that may act upon the information, the terms used to describe and report annotations need to be well-defined. DICOM structured reporting uses codes of established clinical terminologies and ontologies to describe image-derived information rather than using free text. For example, while many words in English and other languages may be used to refer to a “tumor” as the finding type of the ROI, the concept can be unambiguously represented across languages and domains by the SNOMED-CT code “108369006.” The use of structured reports and standardized codes facilitates interpretation of image annotations by both humans and machines and is therefore critical for enabling structural and semantic interoperability between ML models and clinical systems. The standard-based approach further facilitates the re-use of data beyond the scope of the project or use case for which they were initially created. While there are several advantages to using codes, they are cumbersome to work with and increase the complexity of ML programs and are thus in our experience often frowned on by developers. The <italic>highdicom</italic> library provides data structures and methods that abstract the codes and significantly simplify using and operating on coded concepts.</p>
      <p id="Par46">While codes chosen from well-established coding schemes can significantly improve interoperability, the choice of the appropriate code can still pose a significant challenge to both developers and clinical experts. The <italic>highdicom</italic> library does not (and cannot) fully solve this problem. Indeed, in practice, it may be the case that no standard coded concept accurately describes the annotation and a custom coding scheme is required. DICOM allows, and <italic>pydicom</italic> and <italic>highdicom</italic> support, the definition of such custom coding schemes with the convention of a prefix of “99” followed by an identifying text string. Consumers of custom coded concepts should detect this condition and seek out-of-band information for correct interpretation of the annotations. However, for a large range of common clinical use cases, the library (together with the underlying <italic>pydicom</italic> library) exposes value sets defined in the DICOM standard via abstractions, and by depending on these abstractions throughout its API, encourages developers to choose codes from these predefined sets.</p>
    </sec>
    <sec id="Sec12">
      <title>Encoding Image Regions in a Well-defined Coordinate System in Three-dimensional Physical Space Allows for Clinically Actionable Measurements</title>
      <p id="Par47">Establishing an unambiguous spatial relationship between ROIs and their corresponding source images for display or computational analysis requires a common frame of reference, which defines the coordinate system to uniquely localize both images or image regions with respect to the imaging target (the specimen in pathology or the patient in radiology) with both position and orientation. Many applications simply specify ROIs relative to the pixel matrix of an image in pixel units. However, this simple approach is problematic for interoperability, because the image pixel grid forms an ill-defined coordinate system and the location (offset, rotation, and scale) of an image with respect to the imaging target changes upon spatial transformation of the image. DICOM specifies a frame of reference for both slide-based and patient-based coordinate systems, which enables accurate and precise localization of a ROI with respect to the patient or the specimen on the slide independent of whether affine transformations have been applied to images. Defining ROIs in physical space in millimeter units further has the advantage that spatial ROI measurements such as diameter or area can be readily taken in this frame of reference without the need to transform coordinates, a process that can be error prone and result in incorrect measurements with potentially serious clinical implications. The <italic>highdicom</italic> library enables developers to work with both 2D pixel matrix and 3D frame of reference coordinates and provides developers methods to readily convert coordinates between the different coordinate systems.</p>
    </sec>
    <sec id="Sec13">
      <title>Scaling to Large Numbers of Image Annotations in the Context of Slide Microscopy Imaging in Pathology</title>
      <p id="Par48">As demonstrated in this paper, encoding of ROIs in SR documents works for both pathology and radiology. However, the deeply nested structure of SR documents does not scale well to object detection problems in pathology, where millions of cells or nuclei may be detected per whole slide image. To address this challenge, DICOM Working Group 26 Pathology (WG-26) has developed a supplement for the DICOM standard that proposes the introduction of a <italic>Microscopy Bulk Simple Annotations</italic> IOD and Annotation (ANN) modality specifically designed for the storage and exchange of a large number of image annotations in the form of spatial coordinates [<xref ref-type="bibr" rid="CR48">48</xref>]. The graphic types used in the ANN objects have been harmonized with those in SRs, and their structure is similar to that of SEG images. This supplement was recently approved and incorporated into the DICOM standard and is now implemented in <italic>highdicom</italic> as a highdicom.ann.MicroscopyBulkSimpleAnnotations SOP class, reusing the existing building blocks of the library for coded concepts and spatial coordinates.</p>
    </sec>
    <sec id="Sec14">
      <title>Abstracting the Complexity of the Standard Without Oversimplifying Medical Imaging Use Cases</title>
      <p id="Par49">DICOM is the ubiquitous standard for representation and communication of medical image data and standardizes many aspects of the imaging workflow to enable interoperability in the clinical setting. However, DICOM is often criticized by the biomedical imaging research community for its elaborateness and alternative data formats have emerged in the research setting that are intended to simplify access to and storage of data by researchers that do not want to cope with intricacies of the standard [<xref ref-type="bibr" rid="CR49">49</xref>]. The first step in an image analysis pipeline is thus often the conversion of DICOM objects into an alternative format that is considered more suitable for research use [<xref ref-type="bibr" rid="CR50">50</xref>]. While conversion of clinically acquired DICOM objects into another format may work well within the limited scope of a research project, the reverse, i.e., the conversion of a given research output into standard DICOM representation, is generally not possible, since important contextual information is lost along the way [<xref ref-type="bibr" rid="CR51">51</xref>]. Many of the attributes of DICOM objects that are regarded superfluous by researchers and are readily removed for ease of use are crucial for interoperability with clinical systems and for correct representation and interpretation of the data in clinical practice.</p>
      <p id="Par50">We argue that the discussion regarding the establishment and adoption of standards for clinical deployment of ML models and integration of their outputs into clinical workflows should be guided primarily by the requirements of clinical systems and clinicians for interpretability and clinical decision making, rather than current practices within research communities. The DICOM standard has been evolving over many years through continuous collaboration of an international group of experts and a diverse set of stakeholders based on a considerate and controlled process that takes a variety of use cases as well as legal and regulatory aspects into account. While the comprehensiveness and inclusiveness of the standard has advantages, it has also resulted in significant complexity and demands an implementation that exposes the useful parts of the standards through a layer of abstraction. The <italic>highdicom</italic> library strikes a fine balance, by providing an API that hides as many details of the DICOM standard as possible from model developers, while acknowledging that medical imaging is complex and that efforts aiming for DICOM abstraction should involve technical and domain experts to avoid oversimplification with detrimental effects on interoperability and ultimately patient safety. The result is an API that abstracts the intricate structure of DICOM datasets, but retains full and direct access to all DICOM attributes and stays close to the terminology of the DICOM data models to avoid any ambiguities.</p>
    </sec>
    <sec id="Sec15">
      <title>Bridging the Gap Between Model Development in Research and Model Deployment in Clinical Practice</title>
      <p id="Par51">Researchers, medical device manufacturers, and healthcare providers are generally interested in accelerating the translation of research findings into clinical practice and enable patients to get access to and benefit from diagnostic and therapeutic innovations. However, the incentives for the different stakeholders who participate in the translation process at different time points from model development to deployment are not necessarily well aligned. Currently, the production deployment of an ML model is generally not a major concern to model developers, who primarily operate in a research environment. The developer often does not receive a technical specification against which the model should be developed and is unaware of the environment into which the model should ultimately be deployed for clinical validation. As a consequence, the structure of data outputted by ML models developed in research settings is generally highly customized towards a particular research project and specific use case and lacks identifying or descriptive metadata relevant for clinical application (see above). Furthermore, current ML models store data in a variety of proprietary formats that are incompatible with clinical systems, which generally rely on a DICOM interface for data exchange. Together, these factors impede the deployment of an ML model and its integration into existing clinical workflows for validation or application.</p>
      <p id="Par52">One opportunity for streamlining this process is to rely on DICOM as a common format and interface for data exchange during both model development and deployment. In our experiments, we demonstrated that <italic>highdicom</italic> makes feasible a fully DICOM-based workflow in which all files stored on storage devices are in DICOM format with minimal increase in complexity for the developer. Adapting a model developed in such a workflow for clinical deployment becomes a straightforward task.</p>
      <p id="Par53">A common use for non-DICOM formats is for storage of intermediate results within the input image preprocessing pipeline, such as the results of image registration operations. A limitation of our proposed DICOM-only workflow is that it assumes that model training and inference pre-processing pipelines operate directly on the source images. However, we argue that models developed for eventual clinical deployment must have input preprocessing pipelines that are able to operate efficiently from the raw source data and as such having this constraint in place through model development process simplifies deployment. Furthermore, intermediate results could also be represented in DICOM format (e.g., using the Spatial Registration IOD for image registration results) and future versions of <italic>highdicom</italic> may provide tools to help with the creation and access of intermediate results in DICOM format.</p>
    </sec>
    <sec id="Sec16">
      <title>Common Platforms, Services, and Tools Will Facilitate Enterprise Medical Imaging, Interdisciplinary Research, and Integrated Diagnostics</title>
      <p id="Par54">Standardization of images, image annotations, and model predictions between pathology and radiology opens new avenues for enterprise medical imaging, interdisciplinary quantitative biomedical imaging research, and integrated image-based diagnostics. Despite unique challenges and use cases for image management in digital pathology and radiology, there are opportunities for streamlining the investment into and use of IT infrastructure and platforms across medical disciplines within the enterprise. Given that most hospitals already have an existing medical imaging infrastructure based on DICOM, encoding image annotations in DICOM format may lower the barrier for integration of ML systems into clinical workflows.</p>
      <p id="Par55">Relying on the DICOM standard may further promote interdisciplinary biomedical imaging research by, for example, clearing the way for the use of annotations of slide microscopy images in pathology as ground truth for training ML models for analysis of CT images in radiology or vice versa. Furthermore, leveraging a standard data format and communication interface provides an opportunity to synthesize different imaging modalities and interpret pathology and radiology ML model outputs side-by-side. In this paper, we demonstrate that <italic>highdicom</italic> facilitates the creation and interpretation of image annotations independent of a specific medical imaging modality, discipline, department, or institution. We further show that data can be exchanged and stored using DICOM-compatible image management systems, which already exist in hospitals worldwide and are increasingly being adopted by biomedical imaging research initiatives around the world. For example, the National Cancer Institute’s Imaging Data Commons (IDC) in the USA will make large public collections of pathology and radiology images, image annotations, and image analysis results available in DICOM format [<xref ref-type="bibr" rid="CR52">52</xref>]. The <italic>highdicom</italic> library will allow researchers to leverage these resources and enable them to readily share their results and make them usable by other researchers. We therefore see the potential for <italic>highdicom</italic> to streamline the development and deployment of ML models across departmental boundaries, accelerate the translation of technological innovations from research into clinical practice, and to assist in the realization of AI in healthcare.</p>
    </sec>
  </sec>
  <sec id="Sec17">
    <title>Conclusion</title>
    <p id="Par56">The <italic>highdicom</italic> library abstracts the complexity of the DICOM standard, and exposes medical imaging data to ML model developers via a pythonic interface that ties into the scientific Python ecosystem for machine learning and image processing and allows data scientists to think of imaging data at a high level of abstraction without having to worry about the low-level details and rules of the DICOM standard. Focusing on the use case of detecting lung tumors in slide microscopy images of surgical tissue section specimens as well as in computed tomography images of the chest, we examined examples for the interpretation of DICOM-encoded image annotations during model training and encoding of model outputs during model inference. Through a series of experiments, we have demonstrated the utility of the library for the development of ML models and shown that, by relying on the DICOM standard, the library enables interoperability of the developed ML models with commercially available DICOM-compliant information systems and allows for unambiguous interpretation of model outputs in clinical context independent of the specific medical imaging modality or discipline. By facilitating the use of DICOM throughout the model development and deployment process, <italic>highdicom</italic> has the potential to bridge the gap between research and clinical application and thereby streamline clinical integration and validation of ML models.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Information</title>
    <sec id="Sec18">
      <p>Below is the link to the electronic supplementary material.<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="10278_2022_683_MOESM1_ESM.pdf"><caption><p>Supplementary file1 (PDF 5.84 MB)</p></caption></media></supplementary-material></p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p id="Par59">
        <ext-link ext-link-type="uri" xlink:href="https://pytorch.org/vision/0.8/index.html">https://pytorch.org/vision/0.8/index.html</ext-link>
      </p>
    </fn>
    <fn id="Fn2">
      <label>2</label>
      <p id="Par60">
        <ext-link ext-link-type="uri" xlink:href="https://github.com/dcm4che/dcm4chee-arc-light">https://github.com/dcm4che/dcm4chee-arc-light</ext-link>
      </p>
    </fn>
    <fn id="Fn3">
      <label>3</label>
      <p id="Par61">
        <ext-link ext-link-type="uri" xlink:href="https://github.com/mghcomputationalpathology/dicomweb-client">https://github.com/mghcomputationalpathology/dicomweb-client</ext-link>
      </p>
    </fn>
    <fn id="Fn4">
      <label>4</label>
      <p id="Par62">
        <ext-link ext-link-type="uri" xlink:href="https://github.com/ohif/viewers">https://github.com/ohif/viewers</ext-link>
      </p>
    </fn>
    <fn id="Fn5">
      <label>5</label>
      <p id="Par63">
        <ext-link ext-link-type="uri" xlink:href="https://slicer.org">https://slicer.org</ext-link>
      </p>
    </fn>
    <fn id="Fn6">
      <label>6</label>
      <p id="Par64">
        <ext-link ext-link-type="uri" xlink:href="https://github.com/herrmannlab/slim">https://github.com/herrmannlab/slim</ext-link>
      </p>
    </fn>
    <fn id="Fn7">
      <label>7</label>
      <p id="Par65">
        <ext-link ext-link-type="uri" xlink:href="https://github.com/dcmjs-org/dicomweb-client">https://github.com/dcmjs-org/dicomweb-client</ext-link>
      </p>
    </fn>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors thank Dandan Mo and Aidan Stein for assistance with testing the highdicom library and deployment of image management and display systems. Further thanks are extended to James Alexander Mays, Lida Harriri, and Anthony John Iafrate for discussions and feedback regarding the histomorphologic classification of lung tumors. The authors would also like to express their gratitude to Justin St. Marie, Ira Cooper, and Tom Schultz for support with compute and storage infrastructure and to Bernardo Bizzo for discussions about graphical image annotations and display of model outputs. The results shown here are in part based upon data generated by the TCGA Research Network: <ext-link ext-link-type="uri" xlink:href="http://cancergenome.nih.gov/">http://cancergenome.nih.gov/</ext-link>. The authors acknowledge the National Cancer Institute and the Foundation for the National Institutes of Health, and their critical role in the creation of the free publicly available LIDC/IDRI Database used in this study.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author Contribution</title>
    <p>M.D.H. initiated and designed the study. C.P.B. and M.D.H. wrote initial version of the manuscript. C.P.B., C.G., and M.D.H. conducted experiments, processed and analyzed data, and created the figures. C.P.B., C.G., S.D., A.Y.F., S.P., D.A.C., and M.D.H. designed, implemented, or tested software. All authors contributed to writing of the manuscript.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar11" notes-type="COI-statement">
      <title>Conflict of Interest</title>
      <p id="Par57">Christopher P. Bridge receives research funding from GE Healthcare, Nvidia Corporation, and Bayer, AG. Steven Pieper is in part supported by the National Institute of Biomedical Imaging and Bioengineering (NIBIB) (P41 EB015902) and by the National Cancer Institute, National Institutes of Health, under Task Order No. HHSN26110071 under Contract No. HHSN261201500003l. Jayashree Kalpathy-Cramer is in part supported by the National Cancer Institute CI U01CA242879, and receives research funding from GE Healthcare, Genentech, Inc, and Bayer, AG. Andriy Y. Fedorov is in part supported by the National Institute of Biomedical Imaging and Bioengineering: contracts 75N92020C00008 and 75N92020C00021; grant P41 EB028741, and by the National Cancer Institute: Task Order No. HHSN26110071 under Contract No. HHSN261201500003l; grants U24 CA264044 and R01 CA241817. David A. Clunie receives financial compensation as a consultant of Philips Algotec, as a consultant for Essex Leidos CBIIT NCI, as a consultant for Brigham and Women’s Hospital NCI Imaging Data Commons (IDC), as a consultant for the University of Leeds Northern Pathology Imaging Co-operative (NPIC), and as a contractor for NEMA as DICOM Editor. Markus D. Herrmann is in part supported by the National Cancer Institute: Task Order No. HHSN26110071 under Contract No. HHSN2612015000031. The content of this publication does not necessarily reflect the views or policies of the Department of Health and Human Services, nor does mention of trade names, commercial products, or organizations imply endorsement by the US Government.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <issue>7553</issue>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
        <?supplied-pmid 26017442?>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Campanella</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Hanna</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Geneslaw</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Miraflor</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Werneck Krauss Silva</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Busam</surname>
            <given-names>KJ</given-names>
          </name>
          <name>
            <surname>Brogi</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Reuter</surname>
            <given-names>VE</given-names>
          </name>
          <name>
            <surname>Klimstra</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Fuchs</surname>
            <given-names>TJ</given-names>
          </name>
        </person-group>
        <article-title>Clinical-grade computational pathology using weakly supervised deep learning on whole slide images</article-title>
        <source>Nature Medicine</source>
        <year>2019</year>
        <volume>25</volume>
        <issue>8</issue>
        <fpage>1301</fpage>
        <lpage>1309</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-019-0508-1</pub-id>
        <?supplied-pmid 31308507?>
        <pub-id pub-id-type="pmid">31308507</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lu</surname>
            <given-names>MY</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>TY</given-names>
          </name>
          <name>
            <surname>Williamson</surname>
            <given-names>DFK</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Shady</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lipkova</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Mahmood</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>AI-based pathology predicts origins for cancers of unknown primary</article-title>
        <source>Nature</source>
        <year>2021</year>
        <volume>594</volume>
        <issue>7861</issue>
        <fpage>106</fpage>
        <lpage>110</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-021-03512-4</pub-id>
        <?supplied-pmid 33953404?>
        <pub-id pub-id-type="pmid">33953404</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van der Laak</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Litjens</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Ciompi</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Deep learning in histopathology: the path to the clinic</article-title>
        <source>Nat Med</source>
        <year>2021</year>
        <volume>27</volume>
        <issue>5</issue>
        <fpage>775</fpage>
        <lpage>784</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-021-01343-4</pub-id>
        <?supplied-pmid 33990804?>
        <pub-id pub-id-type="pmid">33990804</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McKinney</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Sieniek</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Godbole</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Godwin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Antropova</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Ashrafian</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Back</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Chesus</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Corrado</surname>
            <given-names>GS</given-names>
          </name>
          <name>
            <surname>Darzi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Etemadi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Garcia-Vicente</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Gilbert</surname>
            <given-names>FJ</given-names>
          </name>
          <name>
            <surname>Halling-Brown</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hassabis</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Jansen</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Karthikesalingam</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kelly</surname>
            <given-names>CJ</given-names>
          </name>
          <name>
            <surname>King</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ledsam</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Melnick</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Mostofi</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Reicher</surname>
            <given-names>JJ</given-names>
          </name>
          <name>
            <surname>Romera-Paredes</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sidebottom</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Suleyman</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Tse</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Young</surname>
            <given-names>KC</given-names>
          </name>
          <name>
            <surname>De Fauw</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Shetty</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>International evaluation of an AI system for breast cancer screening</article-title>
        <source>Nature</source>
        <year>2020</year>
        <volume>577</volume>
        <issue>7788</issue>
        <fpage>89</fpage>
        <lpage>94</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-019-1799-6</pub-id>
        <?supplied-pmid 31894144?>
        <pub-id pub-id-type="pmid">31894144</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Choy</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Khalilzadeh</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Michalski</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Do</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Samir</surname>
            <given-names>AE</given-names>
          </name>
          <name>
            <surname>Pianykh</surname>
            <given-names>OS</given-names>
          </name>
          <name>
            <surname>Geis</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Pandharipande</surname>
            <given-names>PV</given-names>
          </name>
          <name>
            <surname>Brink</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Dreyer</surname>
            <given-names>KJ</given-names>
          </name>
        </person-group>
        <article-title>Current Applications and Future Impact of Machine Learning in Radiology</article-title>
        <source>Radiology</source>
        <year>2018</year>
        <volume>288</volume>
        <issue>2</issue>
        <fpage>318</fpage>
        <lpage>328</lpage>
        <pub-id pub-id-type="doi">10.1148/radiol.2018171820</pub-id>
        <?supplied-pmid 29944078?>
        <pub-id pub-id-type="pmid">29944078</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ardila</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kiraly</surname>
            <given-names>AP</given-names>
          </name>
          <name>
            <surname>Bharadwaj</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Reicher</surname>
            <given-names>JJ</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Tse</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Etemadi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Corrado</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Naidich</surname>
            <given-names>DP</given-names>
          </name>
          <name>
            <surname>Shetty</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography</article-title>
        <source>Nat Med</source>
        <year>2019</year>
        <volume>25</volume>
        <issue>6</issue>
        <fpage>954</fpage>
        <lpage>961</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-019-0447-x</pub-id>
        <?supplied-pmid 31110349?>
        <pub-id pub-id-type="pmid">31110349</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hosny</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Aerts</surname>
            <given-names>HJWL</given-names>
          </name>
        </person-group>
        <article-title>Artificial intelligence for global health</article-title>
        <source>Science</source>
        <year>2019</year>
        <volume>366</volume>
        <issue>6468</issue>
        <fpage>955</fpage>
        <lpage>956</lpage>
        <pub-id pub-id-type="doi">10.1126/science.aay5189</pub-id>
        <?supplied-pmid 31753987?>
        <pub-id pub-id-type="pmid">31753987</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Allen, B., Seltzer, S. E., Langlotz, C. P., Dreyer, K. P., Summers, R. M., Petrick, N., Marinac-Dabic, D., Cruz, M., Alkasab, T. K., Hanisch, R. J., Nilsen, W. J., Burleson, J., Lyman, K., and Kandarpa, K. “A Road Map for Translational Research on Artificial Intelligence in Medical Imaging: From the 2018 National Institutes of Health/RSNA/ACR/The Academy Workshop”. <italic>J Am Coll Radiol</italic> 16.9 Pt A (Sept. 2019), pp. 1179–1189.</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Granter</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Beck</surname>
            <given-names>AH</given-names>
          </name>
          <name>
            <surname>Papke</surname>
            <given-names>DJ</given-names>
          </name>
        </person-group>
        <article-title>AlphaGo, Deep Learning, and the Future of the Human Microscopist</article-title>
        <source>Archives of Pathology &amp; Laboratory Medicine</source>
        <year>2017</year>
        <volume>141</volume>
        <issue>5</issue>
        <fpage>619</fpage>
        <lpage>621</lpage>
        <pub-id pub-id-type="doi">10.5858/arpa.2016-0471-ED</pub-id>
        <pub-id pub-id-type="pmid">28447900</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Abels, E., Pantanowitz, L., Aeffner, F., Zarella, M. D., Laak, J. van der, Bui, M. M., Vemuri, V. N., Parwani, A. V., Gibbs, J., Agosto-Arroyo, E., Beck, A. H., and Kozlowski, C. “Computational pathology definitions, best practices, and recommendations for regulatory guidance: a white paper from the Digital Pathology Association”. <italic>J. Pathol.</italic> (July 2019).</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Roth</surname>
            <given-names>CJ</given-names>
          </name>
          <name>
            <surname>Lannum</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Persons</surname>
            <given-names>KR</given-names>
          </name>
        </person-group>
        <source>A Foundation for Enterprise Imaging: HIMSS-SIIM Collaborative White Paper</source>
        <year>2016</year>
        <volume>29</volume>
        <issue>5</issue>
        <fpage>530</fpage>
        <lpage>538</lpage>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Clunie</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Hosseinzadeh</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Wintell</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>De Mena</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lajara</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Garcia-Rojo</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Bueno</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Saligrama</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Stearrett</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Toomey</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Abels</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Apeldoorn</surname>
            <given-names>FV</given-names>
          </name>
          <name>
            <surname>Langevin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Nichols</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schmid</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Horchner</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Beckwith</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Parwani</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pantanowitz</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Digital Imaging and Communications in Medicine Whole Slide Imaging Connectathon at Digital Pathology Association Pathology Visions 2017</article-title>
        <source>Journal of Pathology Informatics</source>
        <year>2018</year>
        <volume>9</volume>
        <fpage>6</fpage>
        <pub-id pub-id-type="doi">10.4103/jpi.jpi_1_18</pub-id>
        <?supplied-pmid 29619278?>
        <pub-id pub-id-type="pmid">29619278</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Herrmann</surname>
            <given-names>MD</given-names>
          </name>
          <name>
            <surname>Clunie</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Fedorov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Doyle</surname>
            <given-names>SW</given-names>
          </name>
          <name>
            <surname>Pieper</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Klepeis</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>LP</given-names>
          </name>
          <name>
            <surname>Mutter</surname>
            <given-names>GL</given-names>
          </name>
          <name>
            <surname>Milstone</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Schultz</surname>
            <given-names>TJ</given-names>
          </name>
          <name>
            <surname>Kikinis</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Kotecha</surname>
            <given-names>GK</given-names>
          </name>
          <name>
            <surname>Hwang</surname>
            <given-names>DH</given-names>
          </name>
          <name>
            <surname>Andriole</surname>
            <given-names>KP</given-names>
          </name>
          <name>
            <surname>Iafrate</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Brink</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Boland</surname>
            <given-names>GW</given-names>
          </name>
          <name>
            <surname>Dreyer</surname>
            <given-names>KJ</given-names>
          </name>
          <name>
            <surname>Michalski</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Golden</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Louis</surname>
            <given-names>DN</given-names>
          </name>
          <name>
            <surname>Lennerz</surname>
            <given-names>JK</given-names>
          </name>
        </person-group>
        <article-title>Implementing the DICOM Standard for Digital Pathology</article-title>
        <source>Journal of Pathology Informatics</source>
        <year>2018</year>
        <volume>9</volume>
        <fpage>37</fpage>
        <pub-id pub-id-type="doi">10.4103/jpi.jpi_42_18</pub-id>
        <?supplied-pmid 30533276?>
        <pub-id pub-id-type="pmid">30533276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Dash, R., Jones, C., Merrick, R., Haroske, G., Harrison, J., Sayers, C., Haarselhorst, N., Wintell, M., Herrmann, M., and Macary, F. “Integrating the health-care enterprise pathology and laboratory medicine guideline for digital pathology interoperability”. <italic>J Pathol Inform</italic> 12.16 (Mar. 2021).</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">IHE PaLM Technical Committee in collaboration with DICOM WG26. <italic>IHE Pathology and Laboratory Medicine Technical Framework Supplement Digital Pathology Workflow – Image Acquisition (DPIA)</italic>. <ext-link ext-link-type="uri" xlink:href="https://www.ihe.net/uploadedFiles/Documents/PaLM/IHE_PaLM_Suppl_DPIA.pdf">https://www.ihe.net/uploadedFiles/Documents/PaLM/IHE_PaLM_Suppl_DPIA.pdf</ext-link>. Aug. 2020.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wilkinson</surname>
            <given-names>MD</given-names>
          </name>
          <name>
            <surname>Dumontier</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Aalbersberg</surname>
            <given-names>IJ</given-names>
          </name>
          <name>
            <surname>Appleton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Axton</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Baak</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Blomberg</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Boiten</surname>
            <given-names>JW</given-names>
          </name>
          <name>
            <surname>da Silva Santos</surname>
            <given-names>LB</given-names>
          </name>
          <name>
            <surname>Bourne</surname>
            <given-names>PE</given-names>
          </name>
          <name>
            <surname>Bouwman</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Brookes</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Clark</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Crosas</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Dillo</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Dumon</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Edmunds</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Evelo</surname>
            <given-names>CT</given-names>
          </name>
          <name>
            <surname>Finkers</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Gonzalez-Beltran</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gray</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Groth</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Goble</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Grethe</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Heringa</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hoen</surname>
            <given-names>PA</given-names>
          </name>
          <name>
            <surname>Hooft</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Kuhn</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Kok</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Kok</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lusher</surname>
            <given-names>SJ</given-names>
          </name>
          <name>
            <surname>Martone</surname>
            <given-names>ME</given-names>
          </name>
          <name>
            <surname>Mons</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Packer</surname>
            <given-names>AL</given-names>
          </name>
          <name>
            <surname>Persson</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Rocca-Serra</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Roos</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>van Schaik</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Sansone</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Schultes</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Sengstag</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Slater</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Strawn</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Swertz</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Thompson</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>van der Lei</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>van Mulligen</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Velterop</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Waagmeester</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Wittenburg</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Wolstencroft</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Mons</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>The FAIR Guiding Principles for scientific data management and stewardship</article-title>
        <source>Sci Data</source>
        <year>2016</year>
        <volume>3</volume>
        <fpage>160018</fpage>
        <pub-id pub-id-type="doi">10.1038/sdata.2016.18</pub-id>
        <?supplied-pmid 26978244?>
        <pub-id pub-id-type="pmid">26978244</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fedorov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Clunie</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ulrich</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Bauer</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Wahle</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Brown</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Onken</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Riesmeier</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Pieper</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kikinis</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Buatti</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Beichel</surname>
            <given-names>RR</given-names>
          </name>
        </person-group>
        <article-title>DICOM for quantitative imaging biomarker development: a standards based approach to sharing clinical data and structured PET/CT analysis results in head and neck cancer research</article-title>
        <source>PeerJ</source>
        <year>2016</year>
        <volume>4</volume>
        <fpage>e2057</fpage>
        <pub-id pub-id-type="doi">10.7717/peerj.2057</pub-id>
        <?supplied-pmid 27257542?>
        <pub-id pub-id-type="pmid">27257542</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Herz</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Fillion-Robin</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Onken</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Riesmeier</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lasso</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pinter</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Fichtinger</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Pieper</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Clunie</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kikinis</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Fedorov</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>dcmqi: An Open Source Library for Standardized Communication of Quantitative Image Analysis Results Using DICOM</article-title>
        <source>Cancer Research</source>
        <year>2017</year>
        <volume>77</volume>
        <issue>21</issue>
        <fpage>e87</fpage>
        <lpage>e90</lpage>
        <pub-id pub-id-type="doi">10.1158/0008-5472.CAN-17-0336</pub-id>
        <?supplied-pmid 29092948?>
        <pub-id pub-id-type="pmid">29092948</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">IHE Radiology Technical Committee. <italic>IHE Radiology Technical Framework Supplement AI Results (AIR)</italic>. <ext-link ext-link-type="uri" xlink:href="https://www.ihe.net/uploadedFiles/Documents/Radiology/IHE_RAD_Suppl_AIR.pdf">https://www.ihe.net/uploadedFiles/Documents/Radiology/IHE_RAD_Suppl_AIR.pdf</ext-link>. June 2020.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Virtanen, P. et al. “SciPy 1.0: fundamental algorithms for scientific computing in Python”. <italic>Nat Methods</italic> 17.3 (Mar. 2020), pp. 261–272.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Harris</surname>
            <given-names>CR</given-names>
          </name>
          <name>
            <surname>Millman</surname>
            <given-names>KJ</given-names>
          </name>
          <name>
            <surname>van der Walt</surname>
            <given-names>SJ</given-names>
          </name>
          <name>
            <surname>Gommers</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Virtanen</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Cournapeau</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Wieser</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Berg</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>NJ</given-names>
          </name>
          <name>
            <surname>Kern</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Picus</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hoyer</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>van Kerkwijk</surname>
            <given-names>MH</given-names>
          </name>
          <name>
            <surname>Brett</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Haldane</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Del Ró</surname>
            <given-names>JF</given-names>
          </name>
          <name>
            <surname>Wiebe</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Peterson</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Gérard-Marchant</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Sheppard</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Reddy</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Weckesser</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Abbasi</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Gohlke</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Oliphant</surname>
            <given-names>TE</given-names>
          </name>
        </person-group>
        <article-title>Array programming with NumPy</article-title>
        <source>Nature</source>
        <year>2020</year>
        <volume>585</volume>
        <issue>7825</issue>
        <fpage>357</fpage>
        <lpage>362</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id>
        <?supplied-pmid 32939066?>
        <pub-id pub-id-type="pmid">32939066</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Varoquaux</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Gramfort</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Michel</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Thirion</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Grisel</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Blondel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Prettenhofer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Weiss</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Dubourg</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Vanderplas</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Passos</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cournapeau</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Brucher</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Perrot</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Duchesnay</surname>
            <given-names>É</given-names>
          </name>
        </person-group>
        <article-title>Scikit-learn: Machine learning in python</article-title>
        <source>J Mach Learn Res</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>2825</fpage>
        <lpage>2830</lpage>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Walt, S. van der, Schönberger, J., Nunez-Iglesias, J., Boulogne, F., Warner, J., Yager, N., Gouillart, E., Yu, T., and contributors, the scikit-image. “scikit-image: image processing in Python”. <italic>PeerJ</italic> 2 (2014), e453.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Mason, D. “SU-E-T-33: pydicom: an open source DICOM library”. <italic>Medical Physics</italic> 38.6 Part 10 (2011), pp. 3493–3493.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Hapke, H. and Nelson, C. Building machine learning pipelines. O’Reilly Media, 2020.</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Sambasivan, N., Kapania, S., Highfill, H., Akrong, D., Paritosh, P., and Aroyo, L. M. ““Everyone Wants to Do the Model Work, Not the Data Work”: Data Cascades in High-Stakes AI”. <italic>Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</italic>. CHI ’21. Yokohama, Japan: Association for Computing Machinery, 2021.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang L. Bai, J., and Chintala, S. “PyTorch: An Imperative Style, High-Performance Deep Learning Library”. <italic>Advances in Neural Information Processing Systems 32</italic>. Curran Associates, Inc., 2019, pp. 8026–8037.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al. “Tensorflow: A system for large-scale machine learning”. <italic>12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</italic>. 2016, pp. 265–283.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Bradski, G. “The OpenCV Library”. <italic>Dr. Dobb’s Journal of Software Tools</italic> (2000).</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Yoo, T. S., Ackerman, M. J., Lorensen, W. E., Schroeder, W., Chalana, V., Aylward, S., Metaxas, D., and Whitaker, R. “Engineering and algorithm design for an image processing API: a technical report on ITK-the insight toolkit”. <italic>Medicine Meets Virtual Reality 02/10</italic>. IOS press, 2002, pp. 586–592.</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Pianykh</surname>
            <given-names>OS</given-names>
          </name>
        </person-group>
        <source>Digital imaging and communications in medicine (DICOM): a practical introduction and survival guide,</source>
        <year>2008</year>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hafiz</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Bhat</surname>
            <given-names>GM</given-names>
          </name>
        </person-group>
        <article-title>A survey on instance segmentation: state of the art</article-title>
        <source>International Journal of Multimedia Information Retrieval</source>
        <year>2020</year>
        <volume>9</volume>
        <issue>3</issue>
        <fpage>171</fpage>
        <lpage>189</lpage>
        <pub-id pub-id-type="doi">10.1007/s13735-020-00195-x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bidgood</surname>
            <given-names>WD</given-names>
          </name>
        </person-group>
        <article-title>The SNOMED DICOM microglossary: controlled terminology resource for data interchange in biomedical imaging</article-title>
        <source>Methods Inf Med</source>
        <year>1998</year>
        <volume>37</volume>
        <issue>4–5</issue>
        <fpage>404</fpage>
        <lpage>414</lpage>
        <?supplied-pmid 9865038?>
        <pub-id pub-id-type="pmid">9865038</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Clark</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Vendt</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Freymann</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kirby</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Koppel</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Moore</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Phillips</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Maffitt</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Pringle</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Tarbox</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Prior</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository</article-title>
        <source>Journal of Digital Imaging</source>
        <year>2013</year>
        <volume>26</volume>
        <issue>6</issue>
        <fpage>1045</fpage>
        <lpage>1057</lpage>
        <pub-id pub-id-type="doi">10.1007/s10278-013-9622-7</pub-id>
        <?supplied-pmid 23884657?>
        <pub-id pub-id-type="pmid">23884657</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Clunie</surname>
            <given-names>DA</given-names>
          </name>
        </person-group>
        <article-title>Dual-Personality DICOM-TIFF for Whole Slide Images: A Migration Technique for Legacy Software</article-title>
        <source>Journal of Pathology Informatics</source>
        <year>2019</year>
        <volume>10</volume>
        <fpage>12</fpage>
        <pub-id pub-id-type="doi">10.4103/jpi.jpi_93_18</pub-id>
        <?supplied-pmid 31057981?>
        <pub-id pub-id-type="pmid">31057981</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Armato</surname>
            <given-names>SG</given-names>
            <suffix>III</suffix>
          </name>
          <name>
            <surname>McLennan</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Bidaut</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>McNitt-Gray</surname>
            <given-names>MF</given-names>
          </name>
          <name>
            <surname>Meyer</surname>
            <given-names>CR</given-names>
          </name>
          <name>
            <surname>Reeves</surname>
            <given-names>AP</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Aberle</surname>
            <given-names>DR</given-names>
          </name>
          <name>
            <surname>Henschke</surname>
            <given-names>CI</given-names>
          </name>
          <name>
            <surname>Hoffman</surname>
            <given-names>EA</given-names>
          </name>
          <name>
            <surname>Kazerooni</surname>
            <given-names>EA</given-names>
          </name>
          <name>
            <surname>MacMahon</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>van Beek</surname>
            <given-names>EJR</given-names>
          </name>
          <name>
            <surname>Yankelevitz</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Biancardi</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Bland</surname>
            <given-names>PH</given-names>
          </name>
          <name>
            <surname>Brown</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Engelmann</surname>
            <given-names>RM</given-names>
          </name>
          <name>
            <surname>Laderach</surname>
            <given-names>GE</given-names>
          </name>
          <name>
            <surname>Max</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Pais</surname>
            <given-names>RC</given-names>
          </name>
          <name>
            <surname>Qing</surname>
            <given-names>DPY</given-names>
          </name>
          <name>
            <surname>Roberts</surname>
            <given-names>RY</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>AR</given-names>
          </name>
          <name>
            <surname>Starkey</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Batra</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Caligiuri</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Farooqi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gladish</surname>
            <given-names>GW</given-names>
          </name>
          <name>
            <surname>Jude</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Munden</surname>
            <given-names>RF</given-names>
          </name>
          <name>
            <surname>Petkovska</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Quint</surname>
            <given-names>LE</given-names>
          </name>
          <name>
            <surname>Schwartz</surname>
            <given-names>LH</given-names>
          </name>
          <name>
            <surname>Sundaram</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Dodd</surname>
            <given-names>LE</given-names>
          </name>
          <name>
            <surname>Fenimore</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Gur</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Petrick</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Freymann</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kirby</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hughes</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Vande Casteele</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gupte</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sallam</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Heath</surname>
            <given-names>MD</given-names>
          </name>
          <name>
            <surname>Kuhn</surname>
            <given-names>MH</given-names>
          </name>
          <name>
            <surname>Dharaiya</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Burns</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Fryd</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Salganicoff</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Anand</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Shreter</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Vastagh</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Croft</surname>
            <given-names>BY</given-names>
          </name>
          <name>
            <surname>Clarke</surname>
            <given-names>LP</given-names>
          </name>
        </person-group>
        <article-title>The lung image database consortium (lidc) and image database resource initiative (idri): A completed reference database of lung nodules on ct scans</article-title>
        <source>Medical Physics</source>
        <year>2011</year>
        <volume>38</volume>
        <issue>2</issue>
        <fpage>915</fpage>
        <lpage>931</lpage>
        <pub-id pub-id-type="doi">10.1118/1.3528204</pub-id>
        <?supplied-pmid 21452728?>
        <pub-id pub-id-type="pmid">21452728</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Armato</surname>
            <given-names>S</given-names>
            <suffix>III</suffix>
          </name>
          <name>
            <surname>McLennan</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Bidaut</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>McNitt-Gray</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Meyer</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Reeves</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Aberle</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Henschke</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Hoffman</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Kazerooni</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>MacMahon</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>van Beek</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Yankelevitz</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Biancardi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bland</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Brown</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Engelmann</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Laderach</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Max</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Pais</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Qing</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Roberts</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Starkey</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Batra</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Caligiuri</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Farooqi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gladish</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Jude</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Munden</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Petkovska</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Quint</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Schwartz</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Sundaram</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Dodd</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Fenimore</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Gur</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Petrick</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Freymann</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kirby</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hughes</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Casteele</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gupte</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sallam</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Heath</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kuhn</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Dharaiya</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Burns</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Fryd</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Salganicoff</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Anand</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Shreter</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Vastagh</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Croft</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Clarke</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <source>Data from lidc-idri</source>
        <year>2015</year>
        <publisher-loc>The Cancer Imaging Archive</publisher-loc>
        <publisher-name>Tech. rep</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Coudray</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Ocampo</surname>
            <given-names>PS</given-names>
          </name>
          <name>
            <surname>Sakellaropoulos</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Narula</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Snuderl</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Fenyo</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Moreira</surname>
            <given-names>AL</given-names>
          </name>
          <name>
            <surname>Razavian</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Tsirigos</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Classification and mutation prediction from non-small cell lung cancer histopathology images using deep learning</article-title>
        <source>Nature Medicine</source>
        <year>2018</year>
        <volume>24</volume>
        <issue>10</issue>
        <fpage>1559</fpage>
        <lpage>1567</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-018-0177-5</pub-id>
        <?supplied-pmid 30224757?>
        <pub-id pub-id-type="pmid">30224757</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">He, K., Zhang, X., Ren, S., and Sun, J. “Deep Residual Learning for Image Recognition”. <italic>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic>. 2016.</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. “ImageNet: A Large-Scale Hierarchical Image Database”. <italic>CVPR09</italic>. 2009.</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Lerousseau, M., Vakalopoulou, M., Classe, M., Adam, J., Battistella, E., Carré, A., Estienne, T., Henry, T., Deutsch, E., and Paragios, N. “Weakly Supervised Multiple Instance Learning Histopathological Tumor Segmentation”.<italic> Medical Image Computing and Computer Assisted Intervention – MICCAI 2020</italic>. Springer International Publishing, 2020, pp. 470–479.</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Lu, M. Y., Williamson, D. F. K., Chen, T. Y., Chen, R. J., Barbieri, M., and Mahmood, F. “Data-efficient and weakly supervised computational pathology on whole-slide images”. <italic>Nat Biomed Eng</italic> (Mar. 2021).</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Dollár, P. “Focal loss for dense object detection”. <italic>Proceedings of the IEEE international conference on computer vision</italic>. 2017, pp. 2980–2988.</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">DICOM Standards Committee. <italic>DICOM PS3.18 – Web Services</italic>. <ext-link ext-link-type="uri" xlink:href="http://dicom.nema.org/medical/dicom/current/output/chtml/part18/PS3.18.html">http://dicom.nema.org/medical/dicom/current/output/chtml/part18/PS3.18.html</ext-link>. 2021.</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gauriau</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Bridge</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Kitamura</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Tenenholtz</surname>
            <given-names>NA</given-names>
          </name>
          <name>
            <surname>Kirsch</surname>
            <given-names>JE</given-names>
          </name>
          <name>
            <surname>Andriole</surname>
            <given-names>KP</given-names>
          </name>
          <name>
            <surname>Michalski</surname>
            <given-names>MH</given-names>
          </name>
          <name>
            <surname>Bizzo</surname>
            <given-names>BC</given-names>
          </name>
        </person-group>
        <article-title>Using DICOM Metadata for Radiological Image Series Categorization: a Feasibility Study on Large Clinical Brain MRI Datasets</article-title>
        <source>J Digit Imaging</source>
        <year>2020</year>
        <volume>33</volume>
        <issue>3</issue>
        <fpage>747</fpage>
        <lpage>762</lpage>
        <pub-id pub-id-type="doi">10.1007/s10278-019-00308-x</pub-id>
        <?supplied-pmid 31950302?>
        <pub-id pub-id-type="pmid">31950302</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Magrabi, F., Ammenwerth, E., McNair, J. B., De Keizer, N. F., Hypponen, H., Nykänen, P., Rigby, M., Scott, P. J., Vehko, T., Wong, Z. S.-Y., and Georgiou, A. “Artificial Intelligence in Clinical Decision Support: Challenges for Evaluating AI and Practical Implications”. <italic>Yearb Med Inform</italic> 28 (01 2019), pp. 128–134.</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other">DICOM Standards Committee, Working Group 26 (Pathology). <italic>Supplement 222: Microscopy Bulk Simple Annotations Storage SOP Class</italic>. <ext-link ext-link-type="uri" xlink:href="ftp://medical.nema.org/medical/dicom/supps/LB/sup222_lb_WSIAnnotations.pdf">ftp://medical.nema.org/medical/dicom/supps/LB/sup222_lb_WSIAnnotations.pdf</ext-link>. 2021.</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Larobina</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Murino</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Medical image file formats</article-title>
        <source>J Digit Imaging</source>
        <year>2014</year>
        <volume>27</volume>
        <issue>2</issue>
        <fpage>200</fpage>
        <lpage>206</lpage>
        <pub-id pub-id-type="doi">10.1007/s10278-013-9657-9</pub-id>
        <?supplied-pmid 24338090?>
        <pub-id pub-id-type="pmid">24338090</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Morgan</surname>
            <given-names>PS</given-names>
          </name>
          <name>
            <surname>Ashburner</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Rorden</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>The first step for neuroimaging data analysis: DICOM to NIfTI conversion</article-title>
        <source>J Neurosci Methods</source>
        <year>2016</year>
        <volume>264</volume>
        <fpage>47</fpage>
        <lpage>56</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jneumeth.2016.03.001</pub-id>
        <?supplied-pmid 26945974?>
        <pub-id pub-id-type="pmid">26945974</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Roberts</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Driggs</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Thorpe</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gilbey</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yeung</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ursprung</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Aviles-Rivero</surname>
            <given-names>AI</given-names>
          </name>
          <name>
            <surname>Etmann</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>McCague</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Beer</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans</article-title>
        <source>Nature Machine Intelligence</source>
        <year>2021</year>
        <volume>3</volume>
        <issue>3</issue>
        <fpage>199</fpage>
        <lpage>217</lpage>
        <pub-id pub-id-type="doi">10.1038/s42256-021-00307-0</pub-id>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other">Fedorov, A., Longabaugh, W., Pot, W., Clunie, D., Pieper, S., Aerts Hugo, J., Homeyer, A., Lewis, R., Akbarzadeh, A., Bontempi, D., Clifford, D., Herrmann, M., Höfener, H., Octaviano, I., Osborne, C., Paquette, S., Petts, J., Punzo, D., Reyes, M., Schacherer, D., Tian, M., White, G., Ziegler, E., Shmulevich, I., Pihl, T., Wagner, U., Farahani, K., and R, K. “NCI Imaging Data Commons”. <italic>Cancer Research</italic> (2021).</mixed-citation>
    </ref>
  </ref-list>
</back>
