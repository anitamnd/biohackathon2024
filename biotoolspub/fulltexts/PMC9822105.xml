<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?subarticle pcbi.1010793.r001?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS Comput Biol</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLOS Computational Biology</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1553-734X</issn>
    <issn pub-type="epub">1553-7358</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9822105</article-id>
    <article-id pub-id-type="pmid">36548439</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1010793</article-id>
    <article-id pub-id-type="publisher-id">PCOMPBIOL-D-22-01039</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Mathematical and Statistical Techniques</subject>
          <subj-group>
            <subject>Statistical Methods</subject>
            <subj-group>
              <subject>Forecasting</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Statistics</subject>
            <subj-group>
              <subject>Statistical Methods</subject>
              <subj-group>
                <subject>Forecasting</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Cognitive Science</subject>
            <subj-group>
              <subject>Cognitive Psychology</subject>
              <subj-group>
                <subject>Language</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Cognitive Psychology</subject>
            <subj-group>
              <subject>Language</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Social Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Cognitive Psychology</subject>
            <subj-group>
              <subject>Language</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Computational Biology</subject>
          <subj-group>
            <subject>Genome Analysis</subject>
            <subj-group>
              <subject>Gene Ontologies</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Genetics</subject>
          <subj-group>
            <subject>Genomics</subject>
            <subj-group>
              <subject>Genome Analysis</subject>
              <subj-group>
                <subject>Gene Ontologies</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Database and Informatics Methods</subject>
          <subj-group>
            <subject>Bioinformatics</subject>
            <subj-group>
              <subject>Sequence Analysis</subject>
              <subj-group>
                <subject>Sequence Alignment</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and analysis methods</subject>
        <subj-group>
          <subject>Database and informatics methods</subject>
          <subj-group>
            <subject>Bioinformatics</subject>
            <subj-group>
              <subject>Sequence analysis</subject>
              <subj-group>
                <subject>BLAST algorithm</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Molecular Biology</subject>
          <subj-group>
            <subject>Molecular Biology Techniques</subject>
            <subj-group>
              <subject>Sequencing Techniques</subject>
              <subj-group>
                <subject>Protein Sequencing</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Molecular Biology Techniques</subject>
          <subj-group>
            <subject>Sequencing Techniques</subject>
            <subj-group>
              <subject>Protein Sequencing</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Neural Networks</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Neural Networks</subject>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Integrating unsupervised language model with triplet neural networks for protein gene ontology prediction</article-title>
      <alt-title alt-title-type="running-head">Protein function prediction using deep neural networks</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3857-1533</contrib-id>
        <name>
          <surname>Zhu</surname>
          <given-names>Yi-Heng</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Chengxin</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6786-8053</contrib-id>
        <name>
          <surname>Yu</surname>
          <given-names>Dong-Jun</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2739-1916</contrib-id>
        <name>
          <surname>Zhang</surname>
          <given-names>Yang</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff003" ref-type="aff">
          <sup>3</sup>
        </xref>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, People’s Republic of China</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Department of Computational Medicine and Bioinformatics, University of Michigan, Ann Arbor, Michigan, United States of America</addr-line>
    </aff>
    <aff id="aff003">
      <label>3</label>
      <addr-line>Department of Biological Chemistry, University of Michigan, Ann Arbor, Michigan, United States of America</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Friedberg</surname>
          <given-names>Iddo</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Iowa State University College of Veterinary Medicine, UNITED STATES</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>njyudj@njust.edu.cn</email> (D-JY); <email>zhang@zhanggroup.org</email> (YZ)</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>22</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <volume>18</volume>
    <issue>12</issue>
    <elocation-id>e1010793</elocation-id>
    <history>
      <date date-type="received">
        <day>7</day>
        <month>7</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>5</day>
        <month>12</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 Zhu et al</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Zhu et al</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pcbi.1010793.pdf"/>
    <abstract>
      <p>Accurate identification of protein function is critical to elucidate life mechanisms and design new drugs. We proposed a novel deep-learning method, ATGO, to predict Gene Ontology (GO) attributes of proteins through a triplet neural-network architecture embedded with pre-trained language models from protein sequences. The method was systematically tested on 1068 non-redundant benchmarking proteins and 3328 targets from the third Critical Assessment of Protein Function Annotation (CAFA) challenge. Experimental results showed that ATGO achieved a significant increase of the GO prediction accuracy compared to the state-of-the-art approaches in all aspects of molecular function, biological process, and cellular component. Detailed data analyses showed that the major advantage of ATGO lies in the utilization of pre-trained transformer language models which can extract discriminative functional pattern from the feature embeddings. Meanwhile, the proposed triplet network helps enhance the association of functional similarity with feature similarity in the sequence embedding space. In addition, it was found that the combination of the network scores with the complementary homology-based inferences could further improve the accuracy of the predicted models. These results demonstrated a new avenue for high-accuracy deep-learning function prediction that is applicable to large-scale protein function annotations from sequence alone.</p>
    </abstract>
    <abstract abstract-type="summary">
      <title>Author summary</title>
      <p>In the post-genome sequencing era, a major challenge in computational molecular biology is to annotate the biological functions of all genes and gene products, which have been classified, in the context of the widely used Gene Ontology (GO), into three aspects of molecular function, biological process, and cellular component. In this work, we proposed a new open-source deep-learning architecture, ATGO, to deduce GO terms of proteins from the primary amino acid sequence, through the integration of the triplet neural-network with pre-trained language models of protein sequences. Large benchmark tests showed that, when powered with transformer embeddings of the language model, ATGO achieved a significantly improved performance than other state-of-the-art approaches in all the GO aspect predictions. Following the rapid progress of self-attention neural network techniques, which have demonstrated remarkable impacts on natural language processing and multi-sensory data process, and most recently on protein structure prediction, this study showed the significant potential of attention transformer language models on protein function annotations.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100004543</institution-id>
            <institution>China Scholarship Council</institution>
          </institution-wrap>
        </funding-source>
        <award-id>201906840041</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3857-1533</contrib-id>
          <name>
            <surname>Zhu</surname>
            <given-names>Yi-Heng</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award002">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001809</institution-id>
            <institution>National Natural Science Foundation of China</institution>
          </institution-wrap>
        </funding-source>
        <award-id>62072243</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6786-8053</contrib-id>
          <name>
            <surname>Yu</surname>
            <given-names>Dong-Jun</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award003">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001809</institution-id>
            <institution>National Natural Science Foundation of China</institution>
          </institution-wrap>
        </funding-source>
        <award-id>61772273</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6786-8053</contrib-id>
          <name>
            <surname>Yu</surname>
            <given-names>Dong-Jun</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award004">
        <funding-source>
          <institution>Natural Science Foundation of Jiangsu</institution>
        </funding-source>
        <award-id>BK20201304</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6786-8053</contrib-id>
          <name>
            <surname>Yu</surname>
            <given-names>Dong-Jun</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award005">
        <funding-source>
          <institution>Foundation of National Defense Key Laboratory of Science and Technology</institution>
        </funding-source>
        <award-id>JZX7Y202001SY000901</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6786-8053</contrib-id>
          <name>
            <surname>Yu</surname>
            <given-names>Dong-Jun</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award006">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000057</institution-id>
            <institution>National Institute of General Medical Sciences</institution>
          </institution-wrap>
        </funding-source>
        <award-id>GM136422</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2739-1916</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Yang</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award007">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000057</institution-id>
            <institution>National Institute of General Medical Sciences</institution>
          </institution-wrap>
        </funding-source>
        <award-id>S10OD026825</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2739-1916</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Yang</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award008">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000060</institution-id>
            <institution>National Institute of Allergy and Infectious Diseases</institution>
          </institution-wrap>
        </funding-source>
        <award-id>AI134678</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2739-1916</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Yang</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award009">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
            <institution>National Science Foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>IIS1901191</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2739-1916</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Yang</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award010">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
            <institution>National Science Foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>DBI2030790</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2739-1916</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Yang</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award011">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
            <institution>National Science Foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>MTM2025426</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2739-1916</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Yang</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>This work is supported in part by the China Scholarship Council (201906840041 to YHZ), the National Natural Science Foundation of China (62072243 and 61772273 to DJY), the Natural Science Foundation of Jiangsu (BK20201304 to DJY), the Foundation of National Defense Key Laboratory of Science and Technology (JZX7Y202001SY000901 to DJY), the National Institute of General Medical Sciences (GM136422, S10OD026825 to YZ), the National Institute of Allergy and Infectious Diseases (AI134678 to YZ), and the National Science Foundation (IIS1901191, DBI2030790, MTM2025426 to YZ). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="6"/>
      <table-count count="3"/>
      <page-count count="26"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>PLOS Publication Stage</meta-name>
        <meta-value>vor-update-to-uncorrected-proof</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>Publication Update</meta-name>
        <meta-value>2023-01-06</meta-value>
      </custom-meta>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>The benchmark dataset, standalone package, and online server for ATGO are available at <ext-link xlink:href="https://zhanggroup.org/ATGO/" ext-link-type="uri">https://zhanggroup.org/ATGO/</ext-link>.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>The benchmark dataset, standalone package, and online server for ATGO are available at <ext-link xlink:href="https://zhanggroup.org/ATGO/" ext-link-type="uri">https://zhanggroup.org/ATGO/</ext-link>.</p>
  </notes>
</front>
<body>
  <disp-quote>
    <p>This is a <italic toggle="yes">PLOS Computational Biology</italic> Methods paper.</p>
  </disp-quote>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>Proteins are the material basis of life and play many important roles in living organisms, such as catalyzing biochemical reactions, transmitting signals, and maintaining structure of cells [<xref rid="pcbi.1010793.ref001" ref-type="bibr">1</xref>]. To elucidate life mechanisms, it is critical to identify the biological functions of proteins, which have been grouped, in the context of the widely used Gene Ontology (GO), into three aspects of molecular function (MF), biological process (BP), and cellular component (CC) [<xref rid="pcbi.1010793.ref002" ref-type="bibr">2</xref>]. Direct determination of protein functions via biochemical experiments is standard but often time-consuming and incomplete [<xref rid="pcbi.1010793.ref003" ref-type="bibr">3</xref>]. As a result, numerous sequenced proteins have no available function annotation to date. As of June 2022, for example, the UniProt database [<xref rid="pcbi.1010793.ref004" ref-type="bibr">4</xref>] harbored ~230 million protein sequences, but only &lt;1% of them were annotated with known GO terms using experimental evidence. To fill the gap between sequence and function, it is urgent to develop efficient computational methods for protein function prediction [<xref rid="pcbi.1010793.ref005" ref-type="bibr">5</xref>,<xref rid="pcbi.1010793.ref006" ref-type="bibr">6</xref>].</p>
    <p>Existing function prediction methods use at least one of three information sources: template detection [<xref rid="pcbi.1010793.ref007" ref-type="bibr">7</xref>], biological network [<xref rid="pcbi.1010793.ref008" ref-type="bibr">8</xref>], and sequence composition [<xref rid="pcbi.1010793.ref009" ref-type="bibr">9</xref>]. Conventional function prediction methods typically rely on the detection of templates that have similar sequence or structure to the query and therefore suitable for functional inference. For examples, GOtcha [<xref rid="pcbi.1010793.ref010" ref-type="bibr">10</xref>], GoFDR [<xref rid="pcbi.1010793.ref011" ref-type="bibr">11</xref>], and Blast2GO [<xref rid="pcbi.1010793.ref007" ref-type="bibr">7</xref>] identify sequence templates using BLAST or PSI-BLAST alignments [<xref rid="pcbi.1010793.ref012" ref-type="bibr">12</xref>], while COFACTOR [<xref rid="pcbi.1010793.ref013" ref-type="bibr">13</xref>] and ProFunc [<xref rid="pcbi.1010793.ref014" ref-type="bibr">14</xref>] search templates through structure alignment [<xref rid="pcbi.1010793.ref015" ref-type="bibr">15</xref>]. Meanwhile, biological networks established by either protein-protein interaction (PPI) or gene co-expression have been used in more recent function predictors, such as NetGO [<xref rid="pcbi.1010793.ref008" ref-type="bibr">8</xref>], MS-kNN [<xref rid="pcbi.1010793.ref016" ref-type="bibr">16</xref>], and TripletGO [<xref rid="pcbi.1010793.ref017" ref-type="bibr">17</xref>]. The rationale behind PPI networks or gene co-expression is that the proteins with PPI or similar gene expression patterns are more likely to be involved in the same biological pathway or found in the same subcellular location.</p>
    <p>Both template-based and network-based approaches have a common drawback: the accuracy of these methods is contingent upon the availability of readily identifiable and functionally annotated templates or interaction partners. To eliminate such dependence, machine learning-based methods have emerged to directly derive functions from the sequence composition of the query alone [<xref rid="pcbi.1010793.ref008" ref-type="bibr">8</xref>]. This can be achieved by extracting hand-crafted sequence features (e.g., k-mer amino acid coding and matches of protein domain families), which can then be used by machine learning approaches (e.g., support vector machine and logistic regression) to implement function model training and prediction, with typical examples including FFPred [<xref rid="pcbi.1010793.ref018" ref-type="bibr">18</xref>] and GOlabeler [<xref rid="pcbi.1010793.ref019" ref-type="bibr">19</xref>].</p>
    <p>Despite the potential advantage, the prediction accuracy of many early machine learning methods was not satisfactory. One of the major reasons is due to the lack of informative feature representation methods, as most of the approaches are based on simple feature representations, such as amino acid composition, physiochemical properties, and protein family coding, which cannot fully extract the complex pattern of protein functions [<xref rid="pcbi.1010793.ref019" ref-type="bibr">19</xref>,<xref rid="pcbi.1010793.ref020" ref-type="bibr">20</xref>]. To partly overcome this barrier, several methods, e.g., DeepGO [<xref rid="pcbi.1010793.ref009" ref-type="bibr">9</xref>], DeepGOPlus [<xref rid="pcbi.1010793.ref021" ref-type="bibr">21</xref>], and TALE [<xref rid="pcbi.1010793.ref022" ref-type="bibr">22</xref>], utilized deep learning technology to predict protein function. Compared to traditional machine learning approaches, one advantage of deep learning technologies is that they could extract more discriminative feature embeddings from preliminary sequence through designing complex neural networks. Nevertheless, the performance of deep learning methods is often hampered by the limitation and imbalance of annotated function data. Currently, there are only ~130,000 proteins with experimental annotations in the UniProt database, where 81.6% of GO terms are annotated to less than 50 proteins per term. The insufficient experimental data significantly limits the effectiveness of training the deep neural network models.</p>
    <p>To alleviate the issue caused by the lack of annotated data, a promising approach is to utilize protein language models pre-trained through deep-learning networks on large-scale sequence databases which may not have functional annotations. Due to the extensive sequence training and learning, important inter-residue correlation patterns, which are critical for protein functions, can be extracted through the language models and utilized for functional embedding. Several protein language models, such as ProtTrans [<xref rid="pcbi.1010793.ref023" ref-type="bibr">23</xref>], models used in TAPE [<xref rid="pcbi.1010793.ref024" ref-type="bibr">24</xref>], and Bepler &amp; Berger’s approach [<xref rid="pcbi.1010793.ref025" ref-type="bibr">25</xref>], have been recently proposed in the literatures. Especially, an unsupervised protein language model, ESM-1b transformer [<xref rid="pcbi.1010793.ref026" ref-type="bibr">26</xref>], which utilized self-attention networks to learn the evolutionary diversity from 250 millions of protein sequences, has found impressive usefulness in protein contact-map and structure prediction. Meanwhile, the unsupervised language models have also been used, often through supervised learners such as convolutional neural networks (CNNs), for protein function prediction tasks, with examples including the predictions of protein molecular function [<xref rid="pcbi.1010793.ref027" ref-type="bibr">27</xref>], mutation and stability [<xref rid="pcbi.1010793.ref024" ref-type="bibr">24</xref>], subcellular localization [<xref rid="pcbi.1010793.ref023" ref-type="bibr">23</xref>], GO transferals [<xref rid="pcbi.1010793.ref028" ref-type="bibr">28</xref>], and ligand binding [<xref rid="pcbi.1010793.ref029" ref-type="bibr">29</xref>].</p>
    <p>In this work, we proposed a new deep learning model, ATGO, for high accuracy protein function prediction by the integration of the triplet neural-network protocol [<xref rid="pcbi.1010793.ref030" ref-type="bibr">30</xref>] with the language models of protein sequences. First, we utilized an unsupervised self-attention transformer model, ESM-1b transformer [<xref rid="pcbi.1010793.ref026" ref-type="bibr">26</xref>], which has been pre-trained on millions of unannotated sequences, as a feature extractor to generate feature embeddings. Next, a supervised triplet neural-network was extended to train function annotation models from multi-layer transformer feature embeddings, by enhancing the difference between positive and negative samples. To improve prediction accuracy, we further implemented a composite version, ATGO+, by combining ATGO with a sequence homology-based model. Both ATGO and ATGO+ have been systematically tested on a large set of non-redundant proteins, where the results demonstrated significant advantage on accurate GO term prediction over the current state-of-the-art of the field. The standalone package and an online server of ATGO are made freely available through URL <ext-link xlink:href="https://zhanggroup.org/ATGO/" ext-link-type="uri">https://zhanggroup.org/ATGO/</ext-link>.</p>
  </sec>
  <sec sec-type="results" id="sec002">
    <title>Results</title>
    <sec id="sec003">
      <title>Overall performance of ATGO</title>
      <p>ATGO is a deep learning-based approach to protein function annotation with respect to GO terms. As shown in <xref rid="pcbi.1010793.g001" ref-type="fig">Fig 1</xref>, ATGO first extracts three layers of feature embeddings from the attention network-based transformer (ESM-1b), which are then fused by a fully connected neural network. Next, the fused feature embedding is trained by a triplet neural network to generate confidence scores of the predicted GO terms (See ‘<xref rid="sec009" ref-type="sec">Methods and materials</xref>”).</p>
      <fig position="float" id="pcbi.1010793.g001">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010793.g001</object-id>
        <label>Fig 1</label>
        <caption>
          <title>The procedures of ATGO for protein function prediction.</title>
          <p><bold>(a)</bold> The workflow of ATGO. Starting from the input sequence, the ESM-1b transformer is utilized to generate the feature embeddings from the last three layers, which are fused by a fully connected neural network. The fused feature embedding is then fed into a triplet network to create confidence scores of GO models. <bold>(b)</bold> The structure of ESM-1b transformer. For an input sequence, the masking, one-hot encoding, and position embedding are orderly executed to generate the coding matrix, which is then fed into a self-attention block with <italic toggle="yes">n</italic> layers. Each layer can output a feature embedding matrix from an individual evolutionary view through integrating <italic toggle="yes">m</italic> attention heads with a feed-forward network, where the scale dot-product attention is performed in each head. <bold>(c)</bold> The design of a triplet network for assessing feature similarity. The input is a triplet variable (<italic toggle="yes">anc</italic>, <italic toggle="yes">pos</italic>, <italic toggle="yes">neg</italic>), where <italic toggle="yes">anc</italic> is an anchor (baseline) protein, and <italic toggle="yes">pos</italic> (or <italic toggle="yes">neg</italic>) is a positive (or negative) protein with the same (or different) function of <italic toggle="yes">anc</italic>. Each sequence is fed into the designed feature generation model to extract a feature embedding vector, as the input of fully connected layer to output a new embedding vector. Then, the feature dissimilarity between two proteins is measured by Euclidean distance of embedding vectors. Finally, a triplet loss is designed to enhance the relationship between functional similarity and feature similarity in embedding space.</p>
        </caption>
        <graphic xlink:href="pcbi.1010793.g001" position="float"/>
      </fig>
      <p>To examine the efficacy of proposed ATGO, we implemented three GO prediction methods from different biology views as baselines, including a sequence alignment-based GO prediction method (SAGP, see <xref rid="pcbi.1010793.s021" ref-type="supplementary-material">S1 Text</xref>), a PPI-based GO predictor (PPIGP, <xref rid="pcbi.1010793.s022" ref-type="supplementary-material">S2 Text</xref>), and a Naïve-based GO predictor (NGP, see <xref rid="pcbi.1010793.s023" ref-type="supplementary-material">S3 Text</xref>). In addition, we compared ATGO with five state-of-the-art models, including three deep learning-based methods (i.e., DeepGO [<xref rid="pcbi.1010793.ref009" ref-type="bibr">9</xref>], DeepGOCNN [<xref rid="pcbi.1010793.ref021" ref-type="bibr">21</xref>], and TALE [<xref rid="pcbi.1010793.ref022" ref-type="bibr">22</xref>]), two template-based methods (FunFams [<xref rid="pcbi.1010793.ref031" ref-type="bibr">31</xref>] and DIAMONDScore [<xref rid="pcbi.1010793.ref021" ref-type="bibr">21</xref>]), which are driven by protein family search and sequence homology alignment, respectively. Given the complementarity of machine learning and homology-based alignments, we also implemented a composite method, ATGO+, which generates predictions by linearly combining the confidence score of ATGO and SAGP. This will be benchmarked with two other composite methods (DeepGOPlus [<xref rid="pcbi.1010793.ref021" ref-type="bibr">21</xref>] and TALE+ [<xref rid="pcbi.1010793.ref022" ref-type="bibr">22</xref>]), which are weighted combinations of (DeepGOCNN, DIAMONDScore) and (TALE, DIAMONDScore), respectively. For the seven third-party methods, we downloaded the programs and ran them on our test dataset under the default setting.</p>
      <p><xref rid="pcbi.1010793.t001" ref-type="table">Table 1</xref> summarizes the overall results of all 12 GO prediction methods on 1068 non-redundant test proteins (See ‘<xref rid="sec009" ref-type="sec">Methods and materials</xref>”) for three GO aspects, where the performance is measured by maximum F<sub>1</sub>-score (F<sub>max</sub>) [<xref rid="pcbi.1010793.ref032" ref-type="bibr">32</xref>,<xref rid="pcbi.1010793.ref033" ref-type="bibr">33</xref>], area under the precision-recall curve (AUPR) [<xref rid="pcbi.1010793.ref034" ref-type="bibr">34</xref>], and coverage [<xref rid="pcbi.1010793.ref033" ref-type="bibr">33</xref>]. The data shows that ATGO achieves a significantly better performance than all other 8 non-composite control methods. Specifically, compared with the second-best performer SAGP, ATGO gains 9.3% and 69.1% average improvements for F<sub>max</sub> and AUPR, respectively, on the average of three GO aspects, all with <italic toggle="yes">p</italic>-values ≤1.1e-07 on two-sided Student’s t-test.</p>
      <table-wrap position="float" id="pcbi.1010793.t001">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010793.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>The summary of the proposed ATGO/ATGO+ and other 10 competing GO prediction methods on the 1068 benchmark proteins.</title>
          <p><italic toggle="yes">p</italic>-values in parenthesis are calculated between ATGO and other single-based methods and between ATGO+ and other composite methods by two-sided Student’s t-test. Specifically, the proposed ATGO and ATGO+ are repeatedly implemented with 10 times on the benchmark dataset to generate the corresponding performance evaluation indices, which are compared with the fixed evaluation index generated by the competing method to calculate <italic toggle="yes">p</italic>-value using two-sided Student’s t-test. Bold fonts highlight the best performer in each category. Coverage is the ratio of the number of proteins with available prediction scores divided by the total number of test proteins.</p>
        </caption>
        <alternatives>
          <graphic xlink:href="pcbi.1010793.t001" id="pcbi.1010793.t001g" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" colspan="2" rowspan="2">Methods</th>
                <th align="center" colspan="3" rowspan="1">F<sub>max</sub></th>
                <th align="center" colspan="3" rowspan="1">AUPR</th>
                <th align="center" colspan="3" rowspan="1">Coverage</th>
              </tr>
              <tr>
                <th align="center" rowspan="1" colspan="1">MF</th>
                <th align="center" rowspan="1" colspan="1">BP</th>
                <th align="center" rowspan="1" colspan="1">CC</th>
                <th align="center" rowspan="1" colspan="1">MF</th>
                <th align="center" rowspan="1" colspan="1">BP</th>
                <th align="center" rowspan="1" colspan="1">CC</th>
                <th align="center" rowspan="1" colspan="1">MF</th>
                <th align="center" rowspan="1" colspan="1">BP</th>
                <th align="center" rowspan="1" colspan="1">CC</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" rowspan="9" colspan="1">Single algorithms</td>
                <td align="center" rowspan="1" colspan="1">SAGP</td>
                <td align="center" rowspan="1" colspan="1">0.597<break/>(1.1e-07)</td>
                <td align="center" rowspan="1" colspan="1">0.400<break/>(5.5e-10)</td>
                <td align="center" rowspan="1" colspan="1">0.534<break/>(1.5e-14)</td>
                <td align="center" rowspan="1" colspan="1">0.351<break/>(3.1e-17)</td>
                <td align="center" rowspan="1" colspan="1">0.242<break/>(4.8e-17)</td>
                <td align="center" rowspan="1" colspan="1">0.322<break/>(1.6e-19)</td>
                <td align="center" rowspan="1" colspan="1">0.88</td>
                <td align="center" rowspan="1" colspan="1">0.87</td>
                <td align="center" rowspan="1" colspan="1">0.85</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">PPIGP</td>
                <td align="center" rowspan="1" colspan="1">0.224<break/>(1.2e-18)</td>
                <td align="center" rowspan="1" colspan="1">0.303<break/>(3.0e-17)</td>
                <td align="center" rowspan="1" colspan="1">0.467<break/>(7.4e-17)</td>
                <td align="center" rowspan="1" colspan="1">0.103<break/>(4.6e-20)</td>
                <td align="center" rowspan="1" colspan="1">0.181<break/>(8.9e-19)</td>
                <td align="center" rowspan="1" colspan="1">0.340<break/>(2.9e-19)</td>
                <td align="center" rowspan="1" colspan="1">0.52</td>
                <td align="center" rowspan="1" colspan="1">0.63</td>
                <td align="center" rowspan="1" colspan="1">0.63</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">NGP</td>
                <td align="center" rowspan="1" colspan="1">0.224<break/>(1.2e-18)</td>
                <td align="center" rowspan="1" colspan="1">0.254<break/>(1.2e-18)</td>
                <td align="center" rowspan="1" colspan="1">0.481<break/>(1.7e-16)</td>
                <td align="center" rowspan="1" colspan="1">0.103<break/>(4.6e-20)</td>
                <td align="center" rowspan="1" colspan="1">0.151<break/>(2.1e-19)</td>
                <td align="center" rowspan="1" colspan="1">0.355<break/>(5.0e-19)</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">DeepGO</td>
                <td align="center" rowspan="1" colspan="1">0.355<break/>(4.3e-17)</td>
                <td align="center" rowspan="1" colspan="1">0.317<break/>(9.6e-17)</td>
                <td align="center" rowspan="1" colspan="1">0.499<break/>(6.4e-16)</td>
                <td align="center" rowspan="1" colspan="1">0.293<break/>(4.3e-18)</td>
                <td align="center" rowspan="1" colspan="1">0.218<break/>(8.1e-18)</td>
                <td align="center" rowspan="1" colspan="1">0.430<break/>(1.5e-17)</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">FunFams</td>
                <td align="center" rowspan="1" colspan="1">0.476<break/>(1.0e-14)</td>
                <td align="center" rowspan="1" colspan="1">0.315<break/>(7.6e-17)</td>
                <td align="center" rowspan="1" colspan="1">0.424<break/>(7.5e-18)</td>
                <td align="center" rowspan="1" colspan="1">0.294<break/>(4.4e-18)</td>
                <td align="center" rowspan="1" colspan="1">0.152<break/>(2.1e-19)</td>
                <td align="center" rowspan="1" colspan="1">0.236<break/>(1.3e-20)</td>
                <td align="center" rowspan="1" colspan="1">0.66</td>
                <td align="center" rowspan="1" colspan="1">0.62</td>
                <td align="center" rowspan="1" colspan="1">0.58</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">DeepGOCNN</td>
                <td align="center" rowspan="1" colspan="1">0.328<break/>(1.8e-17)</td>
                <td align="center" rowspan="1" colspan="1">0.307<break/>(3.8e-17)</td>
                <td align="center" rowspan="1" colspan="1">0.463<break/>(5.6e-17)</td>
                <td align="center" rowspan="1" colspan="1">0.264<break/>(1.8e-18)</td>
                <td align="center" rowspan="1" colspan="1">0.208<break/>(4.2e-18)</td>
                <td align="center" rowspan="1" colspan="1">0.337<break/>(2.6e-19)</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">DIAMONDScore</td>
                <td align="center" rowspan="1" colspan="1">0.592<break/>(2.4e-08)</td>
                <td align="center" rowspan="1" colspan="1">0.391<break/>(1.6e-11)</td>
                <td align="center" rowspan="1" colspan="1">0.511<break/>(1.6e-15)</td>
                <td align="center" rowspan="1" colspan="1">0.272<break/>(2.3e-18)</td>
                <td align="center" rowspan="1" colspan="1">0.209<break/>(4.5e-18)</td>
                <td align="center" rowspan="1" colspan="1">0.239<break/>(1.4e-20)</td>
                <td align="center" rowspan="1" colspan="1">0.80</td>
                <td align="center" rowspan="1" colspan="1">0.81</td>
                <td align="center" rowspan="1" colspan="1">0.78</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">TALE</td>
                <td align="center" rowspan="1" colspan="1">0.393<break/>(1.8e-16)</td>
                <td align="center" rowspan="1" colspan="1">0.315<break/>(7.7e-17)</td>
                <td align="center" rowspan="1" colspan="1">0.516<break/>(2.7e-15)</td>
                <td align="center" rowspan="1" colspan="1">0.344<break/>(2.4e-17)</td>
                <td align="center" rowspan="1" colspan="1">0.236<break/>(3.0e-17)</td>
                <td align="center" rowspan="1" colspan="1">0.496<break/>(1.6e-15)</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">ATGO</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.627</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.425</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.623</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.603</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.361</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.600</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
              </tr>
              <tr>
                <td align="center" rowspan="3" colspan="1">Composite algorithms</td>
                <td align="center" rowspan="1" colspan="1">DeepGOPlus</td>
                <td align="center" rowspan="1" colspan="1">0.603<break/>(3.4e-10)</td>
                <td align="center" rowspan="1" colspan="1">0.409<break/>(3.7e-11)</td>
                <td align="center" rowspan="1" colspan="1">0.533<break/>(6.8e-17)</td>
                <td align="center" rowspan="1" colspan="1">0.528<break/>(8.7e-14)</td>
                <td align="center" rowspan="1" colspan="1">0.323<break/>(2.2e-15)</td>
                <td align="center" rowspan="1" colspan="1">0.486<break/>(8.8e-18)</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">TALE+</td>
                <td align="center" rowspan="1" colspan="1">0.602<break/>(3.3e-10)</td>
                <td align="center" rowspan="1" colspan="1">0.420<break/>(8.4e-09)</td>
                <td align="center" rowspan="1" colspan="1">0.586<break/>(2.2e-13)</td>
                <td align="center" rowspan="1" colspan="1">0.542<break/>(5.6e-13)</td>
                <td align="center" rowspan="1" colspan="1">0.332<break/>(2.2e-14)</td>
                <td align="center" rowspan="1" colspan="1">0.569<break/>(3.5e-12)</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">ATGO+</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.631</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.438</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.624</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.611</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.368</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.600</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>After combining ATGO with SAGP, the composite AGTO+ achieves a small but consistent improvement over all three categories of GO aspects in terms of F<sub>max</sub> and AUPR, where the <italic toggle="yes">p</italic>-values in Student’s t-test between them are both below 4.3e-02, showing the difference is statistically significant at the entire dataset level. ATGO+ also significantly outperforms the two composite control methods, DeepGOPlus and TALE+, with <italic toggle="yes">p</italic>-values below 8.4e-09 in all the comparisons, although these two control methods clearly outperform their corresponding single-based methods (DeepGOCNN and TALE) respectively. It cannot escape our notice that the magnitude of performance difference between ATGO and ATGO+ is smaller than that between TALE and TALE+ (or between DeepGOCNN and DeepGOPlus) in each GO aspect as shown in <xref rid="pcbi.1010793.t001" ref-type="table">Table 1</xref>. Part of the reason is that ATGO by itself is a much more accurate predictor compared to other deep learning predictors such as DeepGO and TALE. Therefore, adding another component of sequence homology can bring a relatively smaller increase in the overall F<sub>max</sub> and AUPR scores. In fact, ATGO alone already outperforms both DeepGOPlus and TALE+, which provides a solid base for the better performance of AGTO+. Nevertheless, a consistent improvement has been seen in all datasets and approaches when adding the homology transferal component, demonstrating the advantage of combining different sources of information for improving protein function predictions.</p>
      <p>It is noted that the Student’s t-test <italic toggle="yes">p</italic>-values in <xref rid="pcbi.1010793.t001" ref-type="table">Table 1</xref> are calculated by comparing 10 independent implementations of ATGO/ATGO+ to a single implementation of the competing methods. To examine the statistical significance between all individual methods, most of which have only single implementation, we perform Friedman test [<xref rid="pcbi.1010793.ref035" ref-type="bibr">35</xref>], one of the most used approaches in analysis of variance, for the 12 GO prediction methods at individual protein level (see details in <xref rid="pcbi.1010793.s024" ref-type="supplementary-material">S4 Text</xref>). A significant performance difference with <italic toggle="yes">p</italic>-values ≤4.6e-106 is found among all prediction methods in three GO aspects. Thus, a Nemenyi post-hoc test [<xref rid="pcbi.1010793.ref036" ref-type="bibr">36</xref>] is performed to calculate the <italic toggle="yes">p</italic>-value of performance difference between each pair of prediction methods, with result listed in <xref rid="pcbi.1010793.s006" ref-type="supplementary-material">S1 Table</xref>. It can be found that the <italic toggle="yes">p</italic>-value between the propose ATGO/ATGO+ and each of competing methods is below 0.05 in each GO aspect at the individual protein level, except for (ATGO, SAGP), (ATGO, DeepGOPlus), and (AGTO+, SAGP) in MF with the <italic toggle="yes">p</italic>-values of 2.5e-01, 3.0e-01, and 1.5e-01, respectively. However, the <italic toggle="yes">p</italic>-values of F<sub>max</sub> values for the above-mentioned three pairwise methods in MF are 1.1e-07, 1.5e-06, and 4.8e-11, respectively, under Student’s t-test at the entire dataset level, while the corresponding <italic toggle="yes">p</italic>-values for AUPR values are 3.1e-17, 1.1e-11, and 1.6e-18, respectively, as shown in <xref rid="pcbi.1010793.t001" ref-type="table">Table 1</xref>. Meanwhile, although the difference between ATGO and ATGO+ is not significant at the individual protein level with the Nemenyi post-hoc test <italic toggle="yes">p</italic>-value&gt; = 0.90 for three GO aspects, the difference is statistically significant at the entire dataset level, with the Student’s t-test <italic toggle="yes">p</italic>-value = 1.3e-03/1.2e-12/3.2e-05 for F<sub>max</sub> and 1.8e-09/3.9e-12/4.3e-02 for AUPR on MF/BP/CC terms respectively. Part of the reason for the <italic toggle="yes">p</italic>-value difference in the two calculations is that at the individual protein level ATGO/ATGO+ are measured by a set of F<sub>1</sub>-scores on hundreds of proteins with significant functional differences while at the entire dataset level the ATGO/ATGO+ are run at a fixed index obtained through the average of confidences scores of 10 models, where each ATGO+ model consistently shows higher evaluation index than the corresponding ATGO model. Another reason is that the Student’s t-test is mathematically different from that in the Nemenyi post-hoc test (see explanation in <xref rid="pcbi.1010793.s022" ref-type="supplementary-material">S2 Table</xref> and <xref rid="pcbi.1010793.s025" ref-type="supplementary-material">S5 Text</xref>). Given that the Student’s t-test can be approximated in a more precise range and not be affected by the performances of other prediction methods in the same group (see details in <xref rid="pcbi.1010793.s025" ref-type="supplementary-material">S5 Text</xref>), we present the results mainly on the t-test in this work.</p>
      <p>It is noted that the F<sub>max</sub> values of TALE listed in <xref rid="pcbi.1010793.t001" ref-type="table">Table 1</xref> are considerably lower than those reported by the TALE paper [<xref rid="pcbi.1010793.ref022" ref-type="bibr">22</xref>]. The major reason for this discrepancy is that in the TALE paper, F<sub>max</sub> calculation includes the root GO terms of three GO aspects for both ground truth and prediction, while in this study, we followed the standard practice of CAFA assessment [<xref rid="pcbi.1010793.ref033" ref-type="bibr">33</xref>] and excluded the root terms. We further re-calculated the F<sub>max</sub> and AUPR values with including root GO terms for ATGO and TALE, as shown in <xref rid="pcbi.1010793.s008" ref-type="supplementary-material">S3 Table</xref>, where ATGO achieves significantly higher F<sub>max</sub> and AUPR than TALE for all GO aspects regardless of whether root terms are included.</p>
      <p><xref rid="pcbi.1010793.t001" ref-type="table">Table 1</xref> also shows that the four methods (i.e., SAGP, PPIGP, FunFams, and DIAMONDScore) are associated with lower coverage scores; this is because these methods fail to search available templates or interaction partners for some of the test proteins and therefore cannot provide the predictions for them. Since the lack of prediction may have impact on their overall prediction performances on the whole test dataset, we further benchmarked the 12 methods on a subset of 562 test proteins, for which predictions can be generated by all methods. As shown in <xref rid="pcbi.1010793.s009" ref-type="supplementary-material">S4 Table</xref>, a similar trend is observed in which ATGO/ATGO+ outperform the control methods with a significant marge. Meanwhile, it is observed that SAGP and DIAMONDScore share an obviously higher prediction accuracy than PPIGP and FunFams for both tests, showing that sequence homology is more effective than PPI and family similarity for protein functional references.</p>
      <p>We further assess the modeling results using an information theory-based evaluation metric, i.e., information content-weighted maximum F<sub>1</sub>-score (ICW-F<sub>max</sub>), which is defined by Eq S7 in <xref rid="pcbi.1010793.s026" ref-type="supplementary-material">S6 Text</xref>. The results for 12 GO prediction methods on the 1068 test proteins are listed in <xref rid="pcbi.1010793.s010" ref-type="supplementary-material">S5 Table</xref>, which shows again that the proposed ATGO and ATGO+ outperform the 10 competing methods using this new metric score.</p>
    </sec>
    <sec id="sec004">
      <title>ATGO shows great generality to new species and rare GO terms</title>
      <p>Despite the power of modeling, many of the machine-learning based methods can have reduced performance on the proteins from new species not included in their training dataset. To examine the generalizability of ATGO to new species, we mapped every protein in our dataset to the corresponding species and collected 160 test proteins from 104 new species which are never seen in training and validation datasets. In <xref rid="pcbi.1010793.g002" ref-type="fig">Fig 2</xref>, we listed the performance of ATGO/ATGO+ in control with other 9 GO prediction methods on the 160 proteins. Here, PPIGP was excluded because it cannot make any prediction for the above-mentioned 160 proteins as they have no available PPI information in STRING database [<xref rid="pcbi.1010793.ref037" ref-type="bibr">37</xref>].</p>
      <fig position="float" id="pcbi.1010793.g002">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010793.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <p>Comparison of ATGO/ATGO+ with 9 competing methods on the 160 test proteins from 104 new species for <bold>(a)</bold> F<sub>max</sub> and <bold>(b)</bold> AUPR.</p>
        </caption>
        <graphic xlink:href="pcbi.1010793.g002" position="float"/>
      </fig>
      <p>From the view of F<sub>max</sub>, ATGO and ATGO+ are top-two performers in MF and CC. As for BP, ATGO outperforms all single-based methods but slightly underperforms two composite methods (i.e., DeepGOPlus and TALE+), but ATGO+ achieves a better performance with F<sub>max</sub> 4.3% and 3.8% higher than the two composite methods. In terms of AUPR values, ATGO/ATGO+ is ranked 3/1, 2/1, and 1/2 for MF, BP, and CC, respectively, among all 11 methods. Meanwhile, the F<sub>max</sub>/AUPR values of ATGO/ATGO+ for the 160 proteins in <xref rid="pcbi.1010793.g002" ref-type="fig">Fig 2</xref> are largely comparable to that for the whole dataset in <xref rid="pcbi.1010793.t001" ref-type="table">Table 1</xref>. These observations show that the performance of ATGO and ATGO+ does not degrade when modeling new species proteins, demonstrating the generalizability of the approaches.</p>
      <p>Due to the nature of knowledge-based training, another challenge to the machine learning approaches is the modeling of rarely seen function terms. To examine ability of ATGO/ATGO+ on the GO terms of different popularities, we grouped GO terms in the test dataset into four groups: 5–10, 10–30, 30–50 and &gt;50, in terms of the number of annotated proteins per GO term; these ranges correspond to the numbers of terms of 425, 328, 64 and 105, respectively. In <xref rid="pcbi.1010793.g003" ref-type="fig">Fig 3</xref>, we present the AUROC scores for the rare GO terms in ranges 5–10 and 10–30, respectively, where the mean and median AUROC scores are listed in <xref rid="pcbi.1010793.s001" ref-type="supplementary-material">S1</xref> and <xref rid="pcbi.1010793.s002" ref-type="supplementary-material">S2</xref> Figs. It is found that ATGO/ATGO+ are ranked as the top two methods. Taking range 5–10 as an example, ATGO+ shows better performance than all other 10 methods for each GO aspect. Specifically, ATGO+ gains 2.5% and 3.9% average improvements on three aspects for mean and median AUROC values, respectively, compared to the best of the control method (TALE+). For the range 10–30, ATGO and ATGO+ share the highest mean and median AUROC values both in BP and CC. As for MF aspect, ATGO+ and ATGO are ranked as 1/3 and 1/2 in terms of mean and median AUROC values, respectively.</p>
      <fig position="float" id="pcbi.1010793.g003">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010793.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>AUROC values by different methods for the rare GO terms in terms of the number of associated proteins, where the median line in the box is the median AUROC value.</title>
          <p><bold>(a)</bold> range 5–10. <bold>(b)</bold> range 10–30.</p>
        </caption>
        <graphic xlink:href="pcbi.1010793.g003" position="float"/>
      </fig>
      <p>In <xref rid="pcbi.1010793.s003" ref-type="supplementary-material">S3 Fig</xref>, we also list the results of the two more common GO terms with ranges of 30–50 and &gt;50. ATGO and AGTO+ again outperform other control methods. These results demonstrate a balanced performance for all type of functional terms.</p>
    </sec>
    <sec id="sec005">
      <title>Testing on targets of the third CAFA challenge (CAFA3)</title>
      <p>As an independent test, we applied ATGO/ATGO+ to the third Critical Assessment of Protein Function Annotation (CAFA3) dataset which consists of 3328 test proteins [<xref rid="pcbi.1010793.ref033" ref-type="bibr">33</xref>]. To make a fair comparison, we re-trained the ATGO pipeline on a subset of the 66,841 training proteins that were released by the CAFA3 organizers. To remove homology contamination, we have filtered out the homologous proteins from the training dataset which have more than <italic toggle="yes">t</italic><sub>1</sub> sequence identity to the test proteins. Here, we have randomly selected 95% of the filtered training proteins to re-train the ATGO model and used the remaining 5% proteins as the validation set to optimize the ATGO parameters. For the in-house SAGP, PPIGP, and NGP programs, these non-homology training proteins were used to construct the template databases and prior probabilities of GO terms.</p>
      <p><xref rid="pcbi.1010793.t002" ref-type="table">Table 2</xref> summarizes the performance of the 10 GO prediction methods on all of 3328 CAFA3 test proteins under the cut-off <italic toggle="yes">t</italic><sub>1</sub> = 30%. Here, we excluded TALE and TALE+ from the method list because the models trained on the CAFA3 dataset are unavailable for TALE. It is noted that for other third-party methods, the homologous proteins were not removed from the training dataset and run with the default setting. The result shows that ATGO and ATGO+ outperform other single and composite methods, respectively, for every GO aspect. Taking ATGO as an example, it achieves 8.2%, 6.5%, and 14.6% improvements of F<sub>max</sub> values for MF, BP, and CC, respectively, compared to the best single method (SAGP), all with <italic toggle="yes">p</italic>-values below 2.2e-07 under Student’s t-test.</p>
      <table-wrap position="float" id="pcbi.1010793.t002">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010793.t002</object-id>
        <label>Table 2</label>
        <caption>
          <title>Comparison of ATGO/ATGO+ with other 8 competing methods on 3328 CAFA3 targets where a sequence identity cut-off <italic toggle="yes">t</italic><sub>1</sub> = 30% between the training and testing proteins was applied to the five in-house methods (ATGO, ATGO+, SAGP, PPIGP, and NGP).</title>
          <p><italic toggle="yes">p</italic>-values in parenthesis are calculated between ATGO and other single-based methods and between ATGO+ and other composite methods by two-sided Student’s t-test. Specifically, the proposed ATGO and ATGO+ are repeatedly implemented with 10 times on the benchmark dataset to generate the corresponding performance evaluation indices, which are compared with the fixed evaluation index generated by the competing method to calculate <italic toggle="yes">p</italic>-value using two-sided Student’s t-test. Bold fonts highlight the best performer in each category. Coverage is the ratio of the number of proteins with available prediction scores divided by the total number of test proteins.</p>
        </caption>
        <alternatives>
          <graphic xlink:href="pcbi.1010793.t002" id="pcbi.1010793.t002g" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" colspan="2" rowspan="2">Methods</th>
                <th align="center" colspan="3" rowspan="1">F<sub>max</sub></th>
                <th align="center" colspan="3" rowspan="1">AUPR</th>
                <th align="center" colspan="3" rowspan="1">Coverage</th>
              </tr>
              <tr>
                <th align="center" rowspan="1" colspan="1">MF</th>
                <th align="center" rowspan="1" colspan="1">BP</th>
                <th align="center" rowspan="1" colspan="1">CC</th>
                <th align="center" rowspan="1" colspan="1">MF</th>
                <th align="center" rowspan="1" colspan="1">BP</th>
                <th align="center" rowspan="1" colspan="1">CC</th>
                <th align="center" rowspan="1" colspan="1">MF</th>
                <th align="center" rowspan="1" colspan="1">BP</th>
                <th align="center" rowspan="1" colspan="1">CC</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" rowspan="15" colspan="1">Single algorithms</td>
                <td align="center" rowspan="2" colspan="1">SAGP</td>
                <td align="center" rowspan="1" colspan="1">0.463</td>
                <td align="center" rowspan="1" colspan="1">0.465</td>
                <td align="center" rowspan="1" colspan="1">0.473</td>
                <td align="center" rowspan="1" colspan="1">0.244</td>
                <td align="center" rowspan="1" colspan="1">0.302</td>
                <td align="center" rowspan="1" colspan="1">0.298</td>
                <td align="center" rowspan="2" colspan="1">0.82</td>
                <td align="center" rowspan="2" colspan="1">0.90</td>
                <td align="center" rowspan="2" colspan="1">0.85</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">(4.3e-10)</td>
                <td align="center" rowspan="1" colspan="1">(2.2e-07)</td>
                <td align="center" rowspan="1" colspan="1">(3.2e-11)</td>
                <td align="center" rowspan="1" colspan="1">(9.8e-19)</td>
                <td align="center" rowspan="1" colspan="1">(2.2e-14)</td>
                <td align="center" rowspan="1" colspan="1">(6.4e-19)</td>
              </tr>
              <tr>
                <td align="center" rowspan="2" colspan="1">PPIGP</td>
                <td align="center" rowspan="1" colspan="1">0.248</td>
                <td align="center" rowspan="1" colspan="1">0.377</td>
                <td align="center" rowspan="1" colspan="1">0.453</td>
                <td align="center" rowspan="1" colspan="1">0.153</td>
                <td align="center" rowspan="1" colspan="1">0.296</td>
                <td align="center" rowspan="1" colspan="1">0.421</td>
                <td align="center" rowspan="2" colspan="1">0.89</td>
                <td align="center" rowspan="2" colspan="1">0.88</td>
                <td align="center" rowspan="2" colspan="1">0.84</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">(5.6e-18)</td>
                <td align="center" rowspan="1" colspan="1">(3.2e-13)</td>
                <td align="center" rowspan="1" colspan="1">(3.0e-12)</td>
                <td align="center" rowspan="1" colspan="1">(4.5e-20)</td>
                <td align="center" rowspan="1" colspan="1">(1.2e-14)</td>
                <td align="center" rowspan="1" colspan="1">(3.9e-16)</td>
              </tr>
              <tr>
                <td align="center" rowspan="2" colspan="1">NGP</td>
                <td align="center" rowspan="1" colspan="1">0.159</td>
                <td align="center" rowspan="1" colspan="1">0.302</td>
                <td align="center" rowspan="1" colspan="1">0.445</td>
                <td align="center" rowspan="1" colspan="1">0.066</td>
                <td align="center" rowspan="1" colspan="1">0.170</td>
                <td align="center" rowspan="1" colspan="1">0.366</td>
                <td align="center" rowspan="2" colspan="1">1.00</td>
                <td align="center" rowspan="2" colspan="1">1.00</td>
                <td align="center" rowspan="2" colspan="1">1.00</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">(3.6e-19)</td>
                <td align="center" rowspan="1" colspan="1">(3.4e-15)</td>
                <td align="center" rowspan="1" colspan="1">(1.4e-12)</td>
                <td align="center" rowspan="1" colspan="1">(4.9e-21)</td>
                <td align="center" rowspan="1" colspan="1">(7.3e-18)</td>
                <td align="center" rowspan="1" colspan="1">(1.3e-17)</td>
              </tr>
              <tr>
                <td align="center" rowspan="2" colspan="1">DeepGO</td>
                <td align="center" rowspan="1" colspan="1">0.275</td>
                <td align="center" rowspan="1" colspan="1">0.386</td>
                <td align="center" rowspan="1" colspan="1">0.487</td>
                <td align="center" rowspan="1" colspan="1">0.198</td>
                <td align="center" rowspan="1" colspan="1">0.291</td>
                <td align="center" rowspan="1" colspan="1">0.487</td>
                <td align="center" rowspan="2" colspan="1">1.00</td>
                <td align="center" rowspan="2" colspan="1">1.00</td>
                <td align="center" rowspan="2" colspan="1">1.00</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">(1.6e-17)</td>
                <td align="center" rowspan="1" colspan="1">(6.8e-13)</td>
                <td align="center" rowspan="1" colspan="1">(2.8e-10)</td>
                <td align="center" rowspan="1" colspan="1">(1.8e-19)</td>
                <td align="center" rowspan="1" colspan="1">(7.8e-15)</td>
                <td align="center" rowspan="1" colspan="1">(6.3e-13)</td>
              </tr>
              <tr>
                <td align="center" rowspan="2" colspan="1">FunFams</td>
                <td align="center" rowspan="1" colspan="1">0.470</td>
                <td align="center" rowspan="1" colspan="1">0.428</td>
                <td align="center" rowspan="1" colspan="1">0.464</td>
                <td align="center" rowspan="1" colspan="1">0.304</td>
                <td align="center" rowspan="1" colspan="1">0.228</td>
                <td align="center" rowspan="1" colspan="1">0.284</td>
                <td align="center" rowspan="2" colspan="1">0.65</td>
                <td align="center" rowspan="2" colspan="1">0.71</td>
                <td align="center" rowspan="2" colspan="1">0.66</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">(4.7e-09)</td>
                <td align="center" rowspan="1" colspan="1">(6.4e-11)</td>
                <td align="center" rowspan="1" colspan="1">(1.0e-11)</td>
                <td align="center" rowspan="1" colspan="1">(1.6e-17)</td>
                <td align="center" rowspan="1" colspan="1">(1.1e-16)</td>
                <td align="center" rowspan="1" colspan="1">(3.8e-19)</td>
              </tr>
              <tr>
                <td align="center" rowspan="2" colspan="1">DeepGOCNN</td>
                <td align="center" rowspan="1" colspan="1">0.311</td>
                <td align="center" rowspan="1" colspan="1">0.291</td>
                <td align="center" rowspan="1" colspan="1">0.413</td>
                <td align="center" rowspan="1" colspan="1">0.231</td>
                <td align="center" rowspan="1" colspan="1">0.191</td>
                <td align="center" rowspan="1" colspan="1">0.288</td>
                <td align="center" rowspan="2" colspan="1">1.00</td>
                <td align="center" rowspan="2" colspan="1">1.00</td>
                <td align="center" rowspan="2" colspan="1">1.00</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">(8.0e-17)</td>
                <td align="center" rowspan="1" colspan="1">(2.0e-15)</td>
                <td align="center" rowspan="1" colspan="1">(9.8e-14)</td>
                <td align="center" rowspan="1" colspan="1">(5.9e-19)</td>
                <td align="center" rowspan="1" colspan="1">(1.8e-17)</td>
                <td align="center" rowspan="1" colspan="1">(4.4e-19)</td>
              </tr>
              <tr>
                <td align="center" rowspan="2" colspan="1">DIAMONDScore</td>
                <td align="center" rowspan="1" colspan="1">0.456</td>
                <td align="center" rowspan="1" colspan="1">0.450</td>
                <td align="center" rowspan="1" colspan="1">0.464</td>
                <td align="center" rowspan="1" colspan="1">0.199</td>
                <td align="center" rowspan="1" colspan="1">0.268</td>
                <td align="center" rowspan="1" colspan="1">0.238</td>
                <td align="center" rowspan="2" colspan="1">0.76</td>
                <td align="center" rowspan="2" colspan="1">0.85</td>
                <td align="center" rowspan="2" colspan="1">0.80</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">(8.8e-11)</td>
                <td align="center" rowspan="1" colspan="1">(3.2e-09)</td>
                <td align="center" rowspan="1" colspan="1">(1.1e-11)</td>
                <td align="center" rowspan="1" colspan="1">(1.9e-19)</td>
                <td align="center" rowspan="1" colspan="1">(1.3e-15)</td>
                <td align="center" rowspan="1" colspan="1">(8.6e-20)</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">ATGO</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.501</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.495</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.542</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.469</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.397</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.546</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
              </tr>
              <tr>
                <td align="center" rowspan="3" colspan="1">Composite algorithms</td>
                <td align="center" rowspan="2" colspan="1">DeepGOPlus</td>
                <td align="center" rowspan="1" colspan="1">0.459</td>
                <td align="center" rowspan="1" colspan="1">0.460</td>
                <td align="center" rowspan="1" colspan="1">0.474</td>
                <td align="center" rowspan="1" colspan="1">0.392</td>
                <td align="center" rowspan="1" colspan="1">0.342</td>
                <td align="center" rowspan="1" colspan="1">0.470</td>
                <td align="center" rowspan="2" colspan="1">1.00</td>
                <td align="center" rowspan="2" colspan="1">1.00</td>
                <td align="center" rowspan="2" colspan="1">1.00</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">(9.2e-12)</td>
                <td align="center" rowspan="1" colspan="1">(4.5e-13)</td>
                <td align="center" rowspan="1" colspan="1">(4.0e-12)</td>
                <td align="center" rowspan="1" colspan="1">(2.3e-15)</td>
                <td align="center" rowspan="1" colspan="1">(3.4e-15)</td>
                <td align="center" rowspan="1" colspan="1">(3.8e-14)</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">ATGO+</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.511</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.502</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.543</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.477</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.412</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.546</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
                <td align="center" rowspan="1" colspan="1">1.00</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>To examine the pairwise significance, we performed Friedman test [<xref rid="pcbi.1010793.ref035" ref-type="bibr">35</xref>] for the 10 GO prediction methods at the individual protein level and found that there exists a significant performance difference with <italic toggle="yes">p</italic>-value ≤6.0e-168 in each GO aspect. Next, the Nemenyi post-hoc test [<xref rid="pcbi.1010793.ref036" ref-type="bibr">36</xref>] is further performed to calculate the <italic toggle="yes">p</italic>-values of performance difference between all method pairs. As shown in <xref rid="pcbi.1010793.s011" ref-type="supplementary-material">S6 Table</xref>, the <italic toggle="yes">p</italic>-values between ATGO/ATGO+ and all the competing methods are both below 0.05 in three aspects at the individual protein level. In addition, we list in <xref rid="pcbi.1010793.s012" ref-type="supplementary-material">S7 Table</xref> the ICW-F<sub>max</sub> scores performed by all 10 GO prediction methods, where ATGO/ATGO+ show again a better performance than other competing methods in all three GO aspects.</p>
      <p>Among the 3328 test proteins, 1177 have no knowledge on any of the GO aspects before the CAFA3 experiment, where other 2151 proteins have limited knowledge on some GO terms and with other GO terms determined during the CAFA3 experiment. In <xref rid="pcbi.1010793.s013" ref-type="supplementary-material">S8 Table</xref>, we list the GO prediction results on the no-knowledge (NK) and limited-knowledge (LK) datasets separately. The ATGO/ATGO+ outperform again other existing methods in all GO aspects for both datasets, showing that the superiority of the pipeline does not depend on specific protein datasets.</p>
      <p>Finally, we examine our methods for different species in CAFA3 test dataset. According to statistics, the 3338 test proteins are originated from 20 species, as shown in <xref rid="pcbi.1010793.s014" ref-type="supplementary-material">S9 Table</xref>, where Human, Arabidopsis, Fission Yeast, and Mouse are the top-four species in terms of the sample number. <xref rid="pcbi.1010793.g004" ref-type="fig">Fig 4</xref> lists the F<sub>max</sub> values of the 10 GO prediction methods for the four species under the cut-off <italic toggle="yes">t</italic><sub>1</sub> = 30%, while the corresponding AUPR values are given in <xref rid="pcbi.1010793.s004" ref-type="supplementary-material">S4 Fig</xref>. For Human, Arabidopsis, and Mouse, ATGO and ATGO+ achieve the highest F<sub>max</sub> values among 10 methods for all three GO aspects. Taking Human species as an example, the F<sub>max</sub> values of ATGO+ are 2.1%, 2.1%, and 12.2% higher than that of the third-best performer in MF, BP, and CC, respectively. In Arabidopsis and Mouse, the improvements between ATGO/ATGO+ and the control methods are more significant. As for Fission Yeast, although ATGO/ATGO+ show a slightly lower F<sub>max</sub> in comparison with DeepGOPlus and DIAMONDScore for MF, they achieve the highest F<sub>max</sub> for both BP and CC aspects.</p>
      <fig position="float" id="pcbi.1010793.g004">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010793.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>The F<sub>max</sub> values of 10 GO prediction methods under the homology cut-off <italic toggle="yes">t</italic><sub>1</sub> = 30% between training and testing proteins for four individual species in CAFA3 test dataset.</title>
          <p><bold>(a)</bold> Human; <bold>(b)</bold> Arabidopsis; <bold>(c)</bold> Fission Yeast; <bold>(d)</bold> Mouse.</p>
        </caption>
        <graphic xlink:href="pcbi.1010793.g004" position="float"/>
      </fig>
      <p>In addition, the prediction performance of the ATGO/ATGO+ under the cut-off <italic toggle="yes">t</italic><sub>1</sub> = 100% on CAFA3 test dataset is summarized in <xref rid="pcbi.1010793.s015" ref-type="supplementary-material">S10 Table</xref>, where <italic toggle="yes">t</italic><sub>1</sub> = 100% indicates that we did not filter out any homologs from the training dataset. Since the third-party programs did not remove homologs, we only listed the in-house programs. Again, the ATGO/ATGO+ show superior performance in comparison with the competing methods for each GO aspect. It is also observed that ATGO has a lower sensitivity on homologous training proteins than the sequence-based method of SAGP. Specifically, after reducing cut-off <italic toggle="yes">t</italic><sub>1</sub> from 100% to 30%, the F<sub>max</sub> and AUPR values of ATGO are separately decreased by 5.2% and 6.2% on average of three GO aspects, while the corresponding decreases for SAGP are 8.9% and 19.3%. This is not surprising because the sequence alignment-based approach can obtain closer homology templates and therefore higher accuracy predictions when given a higher sequence identity cut-off.</p>
      <p>Interestingly, SAGP outperforms most of other control methods. Although both are built on BLAST alignments, SAGP is different from the BLAST baseline method used in CAFA challenge [<xref rid="pcbi.1010793.ref033" ref-type="bibr">33</xref>]; the main difference between these two is that the former deduces consensus functional patterns from multiple homology templates (see Eq S1 in <xref rid="pcbi.1010793.s021" ref-type="supplementary-material">S1 Text</xref>) while the latter only uses a single template for function deduction (see Eq S10 in <xref rid="pcbi.1010793.s027" ref-type="supplementary-material">S7 Text</xref>). In <xref rid="pcbi.1010793.s016" ref-type="supplementary-material">S11 Table</xref>, we further compare SAGP with the CAFA BLAST baseline in our constructed test dataset (1068 non-redundant proteins) and CAFA3 test dataset, respectively, with details explained in <xref rid="pcbi.1010793.s027" ref-type="supplementary-material">S7 Text</xref>. It is shown that SAGP significantly outperforms the CAFA BLAST baseline, suggesting the importance of combining multiple templates in the homology-based protein function inference. Consistently with the CAFA experiments [<xref rid="pcbi.1010793.ref033" ref-type="bibr">33</xref>], the BLAST baseline also underperforms other competing methods, such as FunFams and DeepGOPlus, as shown in Tables <xref rid="pcbi.1010793.t001" ref-type="table">1</xref>, <xref rid="pcbi.1010793.t002" ref-type="table">2</xref> and <xref rid="pcbi.1010793.s016" ref-type="supplementary-material">S11</xref>.</p>
    </sec>
    <sec id="sec006">
      <title>Ablation study</title>
      <p>To analyze the contributions of algorithmic innovations in ATGO to its improved performance, we design an ablation study, in which we start from a baseline model (M0) and incrementally add algorithmic components of ATGO to build three advanced models (M1, M2 and M3, with M3 = ATGO). The pipelines of the four models are designed as follows (see <xref rid="pcbi.1010793.s005" ref-type="supplementary-material">S5 Fig</xref> for the architectures):</p>
      <list list-type="bullet">
        <list-item>
          <p><bold>M0</bold>: Model is trained with the standard CNNs with a one-hot coding matrix extracted from the input sequence, followed by a fully connected network, in which an output layer with sigmoid activation function [<xref rid="pcbi.1010793.ref038" ref-type="bibr">38</xref>] is added to generate the confidence scores of predicted GO terms. In the training stage, the cross-entropy loss [<xref rid="pcbi.1010793.ref039" ref-type="bibr">39</xref>] is used as the loss function, as described in <xref rid="pcbi.1010793.e005" ref-type="disp-formula">Eq 5</xref>.</p>
        </list-item>
        <list-item>
          <p><bold>M1:</bold> We replace the CNN by the ESM-1b transformer and extract the feature embeddings from the last layer for input sequence, which is further fed to a fully connected network with sigmoid activation function to output the confidence scores of predicted GO terms, where the cross-entropy loss is used as loss function in the training stage.</p>
        </list-item>
        <list-item>
          <p><bold>M2:</bold> We add the triplet network-based guilt-by-association (GBA) strategy in M1, where the loss function is the combination of triplet loss [<xref rid="pcbi.1010793.ref030" ref-type="bibr">30</xref>] and cross-entropy loss, as shown in <xref rid="pcbi.1010793.e004" ref-type="disp-formula">Eq 4</xref>. The final outputs are the combination of confidence scores from triplet network-based GBA strategy and those from the output layer with sigmoid activation function, as described in <xref rid="pcbi.1010793.e001" ref-type="disp-formula">Eq 1</xref>.</p>
        </list-item>
        <list-item>
          <p><bold>M3:</bold> We add the multi-view feature fusion strategy in M2 to build the final model, where the embedding features are derived from the last three layers of ESM-1b rather than only the last layer (see “<xref rid="sec009" ref-type="sec">Methods and materials</xref>”).</p>
        </list-item>
      </list>
      <p><xref rid="pcbi.1010793.g005" ref-type="fig">Fig 5</xref> illustrates the F<sub>max</sub> and AUPR values of four models for three GO aspects on all of 1068 test proteins, where we run each model for 10 times and then used the average of all predictions as the final result. Compared with M0, M1 achieves a significant gain with the average F<sub>max</sub> and AUPR increased by 32.4% and 41.8%, respectively, demonstrating that ESM-1b transformer is critical to improve function prediction of the ATGO pipeline. Compared to other function categories, M1 achieves the highest performance improvement in MF prediction, indicating that the embeddings from ESM-1b contain sequence signals which have a closer relationship with MF than with BP and CC. This is probably because MF is mainly associated with the molecular action that a protein can perform by itself and therefore directly related to the internal feature of the sequence that can be captured by the sequence embedding. On the other hand, BP describes the biological pathways which often involve a series of intermolecular actions. Rather than a single protein, a group of interacting proteins are required to complete a BP. Since the sequence embedding is primarily derived from the sequence itself and not designed for capturing the information of protein-protein interactions or co-expression regulations, it is less effective for BP prediction. Similarly, the CC of a protein such as subcellular localization is dependent on other molecules in the cell and cannot be fully captured by the embedding.</p>
      <fig position="float" id="pcbi.1010793.g005">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010793.g005</object-id>
        <label>Fig 5</label>
        <caption>
          <title>Ablation study on the ATGO pipeline based on 1068 test proteins.</title>
          <p>M0 denotes the baseline model with one-hot embedding, where M1-3 are the models with transformer embedding, triplet network, and multi-view feature added respectively. <bold>(a)</bold> The F<sub>max</sub> values of four models. <bold>(b)</bold> The AUPR values of four models.</p>
        </caption>
        <graphic xlink:href="pcbi.1010793.g005" position="float"/>
      </fig>
      <p>After separately adding triplet network-based GBA strategy to M1 and multi-view feature fusion strategy to M2, the corresponding F<sub>max</sub> values are increased on average by 2.2% and 1.6% on three GO aspects. The AUPR values of M2 and M3 are slightly decreased for CC aspect in comparison with M1, but the corresponding values are sustainably increased in other two aspects with <italic toggle="yes">p</italic>-values≤9.1e-18 after adding the two strategies. Although M2 and M3 use the same ESM-1b model for feature extraction and the same loss function for training, M3 still slightly outperforms M2 by using the last three rather than just the last one layer of ESM-1b for feature extraction. This is partly because that different layers of a transformer model such as ESM-1b capture different levels of information and the layers closer to the end of the transformer tend to extract more abstract information. Therefore, M3 captures more fine-grained information from ESM-1b than M2. These observations indicate that the additional two strategies are helpful for enhancing the overall performance of function prediction, although less significant than the ESM-1b transformer.</p>
      <p>In addition, to examine the impact of different metric learning methods on the ATGO model (see “<xref rid="sec009" ref-type="sec">Methods and materials</xref>”), we used four metrics of F<sub>1</sub>-score, Jaccard similarity [<xref rid="pcbi.1010793.ref040" ref-type="bibr">40</xref>], weighted F<sub>1</sub>-score, and weighted Jaccard similarity to assess the functional similarity in the triplet loss separately, where the weights of GO terms are measured by information content [<xref rid="pcbi.1010793.ref041" ref-type="bibr">41</xref>]. The performance of the ATGO models via the four metric learning methods on our test datasets is summarized in <xref rid="pcbi.1010793.s017" ref-type="supplementary-material">S12 Table</xref> and discussed in <xref rid="pcbi.1010793.s028" ref-type="supplementary-material">S8 Text</xref>. It is found that although the performance of individual models varies, there is no obvious difference on the overall performance of the models for each GO aspect, suggesting that the effectiveness of the proposed ATGO framework is not sensitive to the choices of different metric learning methods.</p>
    </sec>
    <sec id="sec007">
      <title>Case studies</title>
      <p>To further examine the effects of different GO prediction methods, we selected three representative proteins from our test dataset (<bold>A6XMY0</bold>, <bold>E7CIP7</bold>, and <bold>F4I082</bold>) as illustrations. These proteins are associated with 18, 14, and 13 GO terms, respectively, for the BP aspect in the experimental annotation, where the root GO term (GO: GO:0008150, biological process) is excluded. <xref rid="pcbi.1010793.t003" ref-type="table">Table 3</xref> summarizes the numbers of correctly predicted GO terms (i.e., true positives) and mistakenly predicted terms (i.e., false positives), and the F<sub>1</sub>-scores between predicted and native GO terms (see Eq S32 in <xref rid="pcbi.1010793.s030" ref-type="supplementary-material">S10 Text</xref>) in the BP prediction for three proteins by 12 GO prediction methods.</p>
      <table-wrap position="float" id="pcbi.1010793.t003">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010793.t003</object-id>
        <label>Table 3</label>
        <caption>
          <title>The modeling results of the ATGO/ATGO+ in control with other 10 competing GO prediction methods on three representative examples in biological process (BP) prediction, where bold fonts highlight the best performer in each category.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="pcbi.1010793.t003" id="pcbi.1010793.t003g" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" colspan="2" rowspan="2">Methods</th>
                <th align="center" colspan="3" rowspan="1">A6XMY0</th>
                <th align="center" colspan="3" rowspan="1">E7CIP7</th>
                <th align="center" colspan="3" rowspan="1">F4I082</th>
              </tr>
              <tr>
                <th align="center" rowspan="1" colspan="1">TP</th>
                <th align="center" rowspan="1" colspan="1">FP</th>
                <th align="center" rowspan="1" colspan="1">F<sub>1</sub>-score</th>
                <th align="center" rowspan="1" colspan="1">TP</th>
                <th align="center" rowspan="1" colspan="1">FP</th>
                <th align="center" rowspan="1" colspan="1">F<sub>1</sub>-score</th>
                <th align="center" rowspan="1" colspan="1">TP</th>
                <th align="center" rowspan="1" colspan="1">FP</th>
                <th align="center" rowspan="1" colspan="1">F<sub>1</sub>-score</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" rowspan="9" colspan="1">Single algorithms</td>
                <td align="center" rowspan="1" colspan="1">SAGP</td>
                <td align="center" rowspan="1" colspan="1">0</td>
                <td align="center" rowspan="1" colspan="1">0</td>
                <td align="center" rowspan="1" colspan="1">0.000</td>
                <td align="center" rowspan="1" colspan="1">8</td>
                <td align="center" rowspan="1" colspan="1">9</td>
                <td align="center" rowspan="1" colspan="1">0.516</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>13</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">8</td>
                <td align="center" rowspan="1" colspan="1">0.765</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">PPIGP</td>
                <td align="center" rowspan="1" colspan="1">0</td>
                <td align="center" rowspan="1" colspan="1">0</td>
                <td align="center" rowspan="1" colspan="1">0.000</td>
                <td align="center" rowspan="1" colspan="1">0</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">0.000</td>
                <td align="center" rowspan="1" colspan="1">3</td>
                <td align="center" rowspan="1" colspan="1">16</td>
                <td align="center" rowspan="1" colspan="1">0.187</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">NGP</td>
                <td align="center" rowspan="1" colspan="1">2</td>
                <td align="center" rowspan="1" colspan="1">16</td>
                <td align="center" rowspan="1" colspan="1">0.111</td>
                <td align="center" rowspan="1" colspan="1">4</td>
                <td align="center" rowspan="1" colspan="1">14</td>
                <td align="center" rowspan="1" colspan="1">0.250</td>
                <td align="center" rowspan="1" colspan="1">1</td>
                <td align="center" rowspan="1" colspan="1">17</td>
                <td align="center" rowspan="1" colspan="1">0.065</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">DeepGO</td>
                <td align="center" rowspan="1" colspan="1">14</td>
                <td align="center" rowspan="1" colspan="1">29</td>
                <td align="center" rowspan="1" colspan="1">0.459</td>
                <td align="center" rowspan="1" colspan="1">0</td>
                <td align="center" rowspan="1" colspan="1">20</td>
                <td align="center" rowspan="1" colspan="1">0.000</td>
                <td align="center" rowspan="1" colspan="1">2</td>
                <td align="center" rowspan="1" colspan="1">57</td>
                <td align="center" rowspan="1" colspan="1">0.056</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">FunFams</td>
                <td align="center" rowspan="1" colspan="1">0</td>
                <td align="center" rowspan="1" colspan="1">0</td>
                <td align="center" rowspan="1" colspan="1">0.000</td>
                <td align="center" rowspan="1" colspan="1">0</td>
                <td align="center" rowspan="1" colspan="1">14</td>
                <td align="center" rowspan="1" colspan="1">0.000</td>
                <td align="center" rowspan="1" colspan="1">0</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">0.000</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">DeepGOCNN</td>
                <td align="center" rowspan="1" colspan="1">14</td>
                <td align="center" rowspan="1" colspan="1">26</td>
                <td align="center" rowspan="1" colspan="1">0.483</td>
                <td align="center" rowspan="1" colspan="1">3</td>
                <td align="center" rowspan="1" colspan="1">9</td>
                <td align="center" rowspan="1" colspan="1">0.231</td>
                <td align="center" rowspan="1" colspan="1">1</td>
                <td align="center" rowspan="1" colspan="1">11</td>
                <td align="center" rowspan="1" colspan="1">0.080</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">DIAMONDScore</td>
                <td align="center" rowspan="1" colspan="1">0</td>
                <td align="center" rowspan="1" colspan="1">0</td>
                <td align="center" rowspan="1" colspan="1">0.000</td>
                <td align="center" rowspan="1" colspan="1">8</td>
                <td align="center" rowspan="1" colspan="1">9</td>
                <td align="center" rowspan="1" colspan="1">0.516</td>
                <td align="center" rowspan="1" colspan="1">0</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">0.000</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">TALE</td>
                <td align="center" rowspan="1" colspan="1">14</td>
                <td align="center" rowspan="1" colspan="1">5</td>
                <td align="center" rowspan="1" colspan="1">0.757</td>
                <td align="center" rowspan="1" colspan="1">10</td>
                <td align="center" rowspan="1" colspan="1">2</td>
                <td align="center" rowspan="1" colspan="1">0.769</td>
                <td align="center" rowspan="1" colspan="1">1</td>
                <td align="center" rowspan="1" colspan="1">9</td>
                <td align="center" rowspan="1" colspan="1">0.087</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">ATGO</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>15</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">0</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.909</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>14</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">2</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.933</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">9</td>
                <td align="center" rowspan="1" colspan="1">7</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.621</bold>
                </td>
              </tr>
              <tr>
                <td align="center" rowspan="3" colspan="1">Composite algorithms</td>
                <td align="center" rowspan="1" colspan="1">DeepGOPlus</td>
                <td align="center" rowspan="1" colspan="1">1</td>
                <td align="center" rowspan="1" colspan="1">2</td>
                <td align="center" rowspan="1" colspan="1">0.095</td>
                <td align="center" rowspan="1" colspan="1">8</td>
                <td align="center" rowspan="1" colspan="1">10</td>
                <td align="center" rowspan="1" colspan="1">0.500</td>
                <td align="center" rowspan="1" colspan="1">0</td>
                <td align="center" rowspan="1" colspan="1">1</td>
                <td align="center" rowspan="1" colspan="1">0.000</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">TALE+</td>
                <td align="center" rowspan="1" colspan="1">1</td>
                <td align="center" rowspan="1" colspan="1">0</td>
                <td align="center" rowspan="1" colspan="1">0.105</td>
                <td align="center" rowspan="1" colspan="1">8</td>
                <td align="center" rowspan="1" colspan="1">9</td>
                <td align="center" rowspan="1" colspan="1">0.516</td>
                <td align="center" rowspan="1" colspan="1">0</td>
                <td align="center" rowspan="1" colspan="1">1</td>
                <td align="center" rowspan="1" colspan="1">0.000</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">ATGO+</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>11</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">0</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.759</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>14</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">2</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.933</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>13</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">3</td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>0.897</bold>
                </td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>In <xref rid="pcbi.1010793.g006" ref-type="fig">Fig 6</xref>, we plot the directed acyclic graph of GO terms in the experimental annotation and the correctly predicted GO terms of 12 methods for three proteins. Moreover, the false positives of each method are listed for all three examples in <xref rid="pcbi.1010793.s018" ref-type="supplementary-material">S13 Table</xref>. It should be noted that the predicted GO terms of different methods are determined by their own cut-off setting to achieve the highest F<sub>max</sub> value.</p>
      <fig position="float" id="pcbi.1010793.g006">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010793.g006</object-id>
        <label>Fig 6</label>
        <caption>
          <title>The directed acyclic graph with GO terms for three representative examples of A6XMY0, E7CIP7, and F4I082 in biological process.</title>
          <p>The circles above each GO term refer to the prediction methods, where a circle filled with “X” on GO term “Y” indicates that method “X” can correctly predict term “Y”.</p>
        </caption>
        <graphic xlink:href="pcbi.1010793.g006" position="float"/>
      </fig>
      <p>Several interesting observations can be made from the data. First, for two of three proteins (i.e., <bold>A6XMY0</bold> and <bold>E7CIP7</bold>), the proposed ATGO and ATGO+ are top-two performers with the highest F<sub>1</sub>-scores. In <bold>A6XMY0</bold>, for example, the four template-based or biology network-based methods, including SAGP, PPIGP, FunFams, and DIAMONDScore, cannot make any predictions, because they fail to hit the available sequence templates or PPIs. At the same time, ATGO correctly recognizes the most (15) GO terms with no false positives, indicating that the deep learning-based ATGO has low dependency on the availability of templates and interaction partners.</p>
      <p>For <bold>E7CIP7</bold>, all true positive predictions by the 10 control methods can be effectively identified by ATGO. More importantly, ATGO can correctly recognize two additional GO terms, i.e., GO: 0000272 and GO:0045490, which are missed by all the 10 control methods. This observation shows that ATGO can predict functions in a more precise level, due to the fact that it successfully identifies several children GO terms, in which other methods fail. However, because SAGP cannot provide additional true positives for ATGO, the performance of composite ATGO+ cannot be further improved and even be slightly degraded in <bold>A6XMY0</bold>. This example shows that although homology-based transferals can help improve the overall performance of ATGO+, it could negatively impact the modeling accuracy when the template quality is poor. It might be helpful to introduce additional filters to the component of homology-based models, e.g., on the confidence score of sequence alignments.</p>
      <p>Occasionally, ATGO shows the worse performance with a lower F<sub>1</sub>-score in comparison with SAGP, as illustrated in <bold>F4I082</bold>. For this case, SAGP generates the highest number of true positives among all 9 single-based methods, where four GO terms (i.e., GO:0006955, GO:0006952, GO:0098542, and GO:0050832) are missed by ATGO. Due to the valuable information inherited from SAGP, the composite ATGO+ obtains the highest F<sub>1</sub>-score across all single and composite methods, which demonstrates the advantage of ATGO+ by combining composite homology and deep learning-based results.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec008">
    <title>Discussion</title>
    <p>We developed a new deep learning-based method, named ATGO, to predict functions of proteins from the primary sequence. The algorithm was built on transformer embedding and triplet network decoding. The large-scale tests on a set of 1068 non-redundant benchmark proteins and 3328 targets from the community-wide CAFA3 experiments demonstrated that ATGO consistently outperforms other state-of-the-art approaches in the accuracy of gene-ontology (GO) predictions. The improvement of ATGO can be attributed to several advancements. First and most importantly, the ESM-1b transformer can effectively extract the discriminative feature embeddings for the input sequence from the different views of evolution diversity. Second, the multi-view feature fusion helps reduce the negative impact caused by information loss in feature extraction. Third, the triplet network-based GBA strategy is important to enhance the training efficiency by maximizing the feature distance between positive and negative samples. Finally, combining ATGO with complementary information from sequence homologous inference can further improve the prediction accuracy.</p>
    <p>Despite the demonstrated effectiveness of the transformer, it is important to note that the currently used ESM-1b transformer is only one of the several existing language models built on a single query sequence. The use of other embedding language models such as ProtTrans and a newly released extended version ESM-2 [<xref rid="pcbi.1010793.ref042" ref-type="bibr">42</xref>], both of which demonstrated superiority to ESM-1b, for GO prediction is worthy of exploration in future work. Moreover, given that multiple sequence alignment (MSA) contains significantly more enriched evolutionary information labelled with conserved protein sites [<xref rid="pcbi.1010793.ref043" ref-type="bibr">43</xref>], which are critical to protein functional annotations, we will construct a new unsupervised protein language model by the use of the MSAs created from DeepMSA [<xref rid="pcbi.1010793.ref044" ref-type="bibr">44</xref>] through self-attention networks. The embedding from MSA transformers should help further improve the sensitivity and accuracy of the GO prediction models [<xref rid="pcbi.1010793.ref045" ref-type="bibr">45</xref>]. Studies along these lines are under progress.</p>
  </sec>
  <sec sec-type="materials|methods" id="sec009">
    <title>Methods and materials</title>
    <sec id="sec010">
      <title>Dataset construction</title>
      <p>To construct the datasets, we downloaded all protein entries from the Gene Ontology Annotation (GOA) database [<xref rid="pcbi.1010793.ref046" ref-type="bibr">46</xref>], which provides functional annotations for the sequences in the UniProt database [<xref rid="pcbi.1010793.ref004" ref-type="bibr">4</xref>]. To ensure the reliability, we only considered the 123,774 proteins whose functions are annotated by at least one of the eight experimental evidence codes (i.e., EXP, IDA, IPI, IMP, IGI, IEP, TAS, and IC) [<xref rid="pcbi.1010793.ref047" ref-type="bibr">47</xref>,<xref rid="pcbi.1010793.ref048" ref-type="bibr">48</xref>].</p>
      <p>Among the 123,774 proteins, we selected 1068 non-redundant proteins as the test dataset, which have the function annotations deposited in the manually reviewed Swiss-Prot library [<xref rid="pcbi.1010793.ref049" ref-type="bibr">49</xref>] after 2019-1-1; and 1089 non-redundant proteins as the validation dataset, which were deposited in Swiss-Prot from 2017-1-1 to 2018-12-31. The remaining 109,132 proteins were used as the training dataset of ATGO. Here, a sequence identity cut-off 30% has been used to filter out the redundant proteins within each dataset and between different datasets using the CD-HIT [<xref rid="pcbi.1010793.ref050" ref-type="bibr">50</xref>] program. The number of entries in each dataset on different GO categories is summarized in <xref rid="pcbi.1010793.s019" ref-type="supplementary-material">S14 Table</xref>. While the training and validation datasets were separately used to train the ATGO models and optimize the parameters, the test dataset was used to evaluate the performance of the models.</p>
    </sec>
    <sec id="sec011">
      <title>The framework of ATGO</title>
      <p>ATGO is a deep learning-based protein function prediction method, with input being a query amino acid sequence and output including confidence scores of predicted GO terms. As showed in <xref rid="pcbi.1010793.g001" ref-type="fig">Fig 1A</xref>, ATGO consists of three procedures of multiple-view feature extraction using transformer, neural network-based feature fusion, and triplet network-based function prediction, where the first and second procedures are jointly defined as feature generation model (FGM).</p>
      <sec id="sec012">
        <title>Procedure I: Multiple-view feature extraction using transformer</title>
        <p>The input sequence is fed to ESM-1b transformer with 33 attention layers to extract the feature embeddings. Specifically, each layer outputs a feature embedding from an individual evolutionary view, which is further used as the input of next layer to generate a new embedding from a more complex view. Considering that the feature embedding from a single view (layer) cannot fully represent the evolutionary information for sequence, we extract feature embeddings from multiple views (i.e., the last three layers) to relieve information loss. Each embedding is represented as a <italic toggle="yes">L</italic>×<italic toggle="yes">D</italic> matrix, where <italic toggle="yes">L</italic> is the length of query, and <italic toggle="yes">D</italic> = 1280 is a preset hyper-parameter in ESM-1b.</p>
      </sec>
      <sec id="sec013">
        <title>Procedure II: Neural network-based feature fusion</title>
        <p>We calculate the average value for each column in the embedding matrix to generate an embedding vector with <italic toggle="yes">D</italic> dimension, as the input of fully connected layer with <italic toggle="yes">N</italic><sub>1</sub> neurons. Then, the outputs of three fully connected layers are concatenated as a new vector with 3<italic toggle="yes">N</italic><sub>1</sub> dimension, which is further fed to another fully connected layer (i.e., <italic toggle="yes">FCL</italic><sub><italic toggle="yes">a</italic></sub>) with <italic toggle="yes">N</italic><sub>2</sub> neurons. Here, we set <italic toggle="yes">N</italic><sub>1</sub> = <italic toggle="yes">N</italic><sub>2</sub> = 1024.</p>
      </sec>
      <sec id="sec014">
        <title>Procedure III: Triplet network-based function prediction</title>
        <p>The triplet network-based GBA strategy is performed on the output of <italic toggle="yes">FCL</italic><sub><italic toggle="yes">a</italic></sub> to generate a confidence score vector <italic toggle="yes">s</italic><sub><italic toggle="yes">gba</italic></sub> for predicted GO terms. At the same time, the output layer <italic toggle="yes">FCL</italic><sub><italic toggle="yes">O</italic></sub> with sigmoid activation function [<xref rid="pcbi.1010793.ref038" ref-type="bibr">38</xref>] is fully connected to <italic toggle="yes">FCL</italic><sub><italic toggle="yes">a</italic></sub> to output another confidence score vector <italic toggle="yes">s</italic><sub><italic toggle="yes">saf</italic></sub>. The final confidence score vector is the weight combination:
<disp-formula id="pcbi.1010793.e001"><alternatives><graphic xlink:href="pcbi.1010793.e001.jpg" id="pcbi.1010793.e001g" position="anchor"/><mml:math id="M1" display="block" overflow="scroll"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math></alternatives><label>(1)</label></disp-formula>
where <italic toggle="yes">w</italic> is the weight and ranges from 0 to 1.</p>
      </sec>
    </sec>
    <sec id="sec015">
      <title>ESM-1b transformer</title>
      <p>The architecture of ESM-1b transformer [<xref rid="pcbi.1010793.ref026" ref-type="bibr">26</xref>,<xref rid="pcbi.1010793.ref051" ref-type="bibr">51</xref>] is illustrated in <xref rid="pcbi.1010793.g001" ref-type="fig">Fig 1B</xref>. For an input sequence, the masking strategy [<xref rid="pcbi.1010793.ref052" ref-type="bibr">52</xref>] is performed on the corresponding tokens (i.e., amino acids). Specifically, we randomly sample 15% tokens, each of which is changed as a special “masking” token with 80% probability, a randomly-chosen alternate amino acid with 10% probability, and the original input token (i.e., no change) with 10% probability. Then, the masked sequence is represented as a <italic toggle="yes">L</italic>×28 matrix using one-hot encoding [<xref rid="pcbi.1010793.ref053" ref-type="bibr">53</xref>], where 28 is the types of tokens, including 20 common amino acids, 6 non-common amino acids (B, J, O, U, X and Z), 1 gap token, and 1 “masking” token.</p>
      <p>The one-hot matrix is firstly embedded with positions and then fed to a self-attention block [<xref rid="pcbi.1010793.ref054" ref-type="bibr">54</xref>] with <italic toggle="yes">n</italic> layers, each of which consists of <italic toggle="yes">m</italic> attention heads, a linear unit, and a feed-forward network (FFN). In each head, the scale dot-product attention is performed on three matrices, including <italic toggle="yes">M</italic><sub><italic toggle="yes">Q</italic></sub> (Query), <italic toggle="yes">M</italic><sub><italic toggle="yes">K</italic></sub> (Key), and <italic toggle="yes">M</italic><sub><italic toggle="yes">V</italic></sub> (Value), as follows. First, the dot-product between <italic toggle="yes">M</italic><sub><italic toggle="yes">Q</italic></sub> and <italic toggle="yes">M</italic><sub><italic toggle="yes">K</italic></sub> is performed to generate an <italic toggle="yes">L</italic>×<italic toggle="yes">L</italic> weight matrix, which measures the similarity for each amino acid pair in sequence. Then, we use the scale parameter and SoftMax function to normalize the weight matrix. Finally, the attention matrix is generated by multiplying the normalized weight matrix with <italic toggle="yes">M</italic><sub><italic toggle="yes">V</italic></sub>.</p>
      <p>In each layer, all attention matrices are concatenated as a new matrix, which is further fed to the subsequent linear unit and FFN with shortcut connections to output feature embedding. The output of the last attention layer is fed to a fully connect layer with SoftMax function to generate a <italic toggle="yes">L</italic>×28 probability matrix <italic toggle="yes">P</italic>, where <italic toggle="yes">P</italic><sub><italic toggle="yes">lc</italic></sub> indicates the probability that the <italic toggle="yes">l</italic>-th token in the masked sequence is predicted as the <italic toggle="yes">c</italic>-th type of amino acid.</p>
      <p>The loss function is designed as a negative log likelihood function between inputted one-hot and outputted probability matrices, to ensure that the prediction model correctly predicts the true amino acids in the masked position as much as possible. The mathematics formulas of the above-mentioned procedures are listed in <xref rid="pcbi.1010793.s029" ref-type="supplementary-material">S9 Text</xref>.</p>
      <p>The ESM-1b transformer is optimized by minimizing the loss function via Adam optimization algorithm [<xref rid="pcbi.1010793.ref055" ref-type="bibr">55</xref>]. Then, the output of each attention layer is a <italic toggle="yes">L</italic>×<italic toggle="yes">D</italic> feature embedding, where <italic toggle="yes">D</italic> is the number of neurons of FFN. The current ESM-1b model was trained on 27.1 million proteins from UniRef50 database and can be download at <ext-link xlink:href="https://github.com/facebookresearch/esm" ext-link-type="uri">https://github.com/facebookresearch/esm</ext-link>, where <italic toggle="yes">n</italic> = 33, <italic toggle="yes">m</italic> = 20, and <italic toggle="yes">D</italic> = 1280.</p>
    </sec>
    <sec id="sec016">
      <title>Triplet network-based guilt-by-association for GO prediction</title>
      <p>In GBA strategy, we select the templates with the most similar feature embeddings for a query from the training dataset to annotate the query, where the similarity of feature embeddings is measured by a supervised triplet network [<xref rid="pcbi.1010793.ref030" ref-type="bibr">30</xref>], as shown in <xref rid="pcbi.1010793.g001" ref-type="fig">Fig 1C</xref>.</p>
      <p>The input is a triplet variable (<italic toggle="yes">anc</italic>, <italic toggle="yes">pos</italic>, <italic toggle="yes">neg</italic>), where <italic toggle="yes">anc</italic> is an anchor (baseline) protein, and <italic toggle="yes">pos</italic> (or <italic toggle="yes">neg</italic>) is a positive (or negative) protein with the same (or different) function of <italic toggle="yes">anc</italic>. Each sequence is fed into feature generation model (FGM, see <xref rid="pcbi.1010793.g001" ref-type="fig">Fig 1A</xref>) to generate a feature representation vector, as the input of <italic toggle="yes">FCL</italic><sub><italic toggle="yes">a</italic></sub>, to output a new embedding vector. Then, the feature dissimilarity between two proteins is measured by Euclidean distance [<xref rid="pcbi.1010793.ref056" ref-type="bibr">56</xref>] of embedding vectors. Finally, a triplet loss is designed to associate feature similarity with functional similarity:
<disp-formula id="pcbi.1010793.e002"><alternatives><graphic xlink:href="pcbi.1010793.e002.jpg" id="pcbi.1010793.e002g" position="anchor"/><mml:math id="M2" display="block" overflow="scroll"><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:math></alternatives><label>(2)</label></disp-formula>
where <italic toggle="yes">d</italic>(<italic toggle="yes">anc</italic>, <italic toggle="yes">pos</italic>) (or <italic toggle="yes">d</italic>(<italic toggle="yes">anc</italic>, <italic toggle="yes">neg</italic>)) is the Euclidean distance of feature embeddings between anchor and positive (or negative), and <italic toggle="yes">margin</italic> is a pre-set positive value. Here, the minimization of triplet loss requests for the maximization of <italic toggle="yes">d</italic>(<italic toggle="yes">anc</italic>, <italic toggle="yes">neg</italic>)−<italic toggle="yes">d</italic>(<italic toggle="yes">anc</italic>, <italic toggle="yes">pos</italic>). In the ideal case, <italic toggle="yes">Tripet loss</italic> = 0 when <italic toggle="yes">d</italic>(<italic toggle="yes">anc</italic>, <italic toggle="yes">neg</italic>)≥<italic toggle="yes">d</italic>(<italic toggle="yes">anc</italic>, <italic toggle="yes">pos</italic>)+<italic toggle="yes">margin</italic>, which indicates substantially higher feature similarity of anchor proteins to positives than to negatives.</p>
      <p>We use the “batch on hard” strategy [<xref rid="pcbi.1010793.ref057" ref-type="bibr">57</xref>,<xref rid="pcbi.1010793.ref058" ref-type="bibr">58</xref>] to calculate the triplet loss for a training dataset:
<disp-formula id="pcbi.1010793.e003"><alternatives><graphic xlink:href="pcbi.1010793.e003.jpg" id="pcbi.1010793.e003g" position="anchor"/><mml:math id="M3" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives><label>(3)</label></disp-formula>
where <italic toggle="yes">x</italic> is a sequence in training set <italic toggle="yes">X</italic>, and <italic toggle="yes">d</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">pos</italic>)<sub><italic toggle="yes">max</italic></sub> (or <italic toggle="yes">d</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">neg</italic>)<sub><italic toggle="yes">min</italic></sub>) is the maximum (or minimum) value of Euclidean distances between <italic toggle="yes">x</italic> and all positive (or negative) proteins with same (or different) function of <italic toggle="yes">x</italic>. Moreover, two proteins are considered to have the same function if their functional similarity is larger than a cut-off value <italic toggle="yes">c</italic><sub><italic toggle="yes">f</italic></sub>. The functional similarity of two proteins is measured by F<sub>1</sub>-score between their GO terms, as shown in <xref rid="pcbi.1010793.s030" ref-type="supplementary-material">S10 Text</xref>.</p>
      <p>It has been demonstrated that the cross-entropy loss [<xref rid="pcbi.1010793.ref039" ref-type="bibr">39</xref>] helps to improve the performance of triplet network [<xref rid="pcbi.1010793.ref059" ref-type="bibr">59</xref>–<xref rid="pcbi.1010793.ref061" ref-type="bibr">61</xref>]. Therefore, we further designed a composite loss function for ATGO through combining triplet loss with cross-entropy loss:
<disp-formula id="pcbi.1010793.e004"><alternatives><graphic xlink:href="pcbi.1010793.e004.jpg" id="pcbi.1010793.e004g" position="anchor"/><mml:math id="M4" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>T</mml:mi><mml:mi>G</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo>∙</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></alternatives><label>(4)</label></disp-formula>
<disp-formula id="pcbi.1010793.e005"><alternatives><graphic xlink:href="pcbi.1010793.e005.jpg" id="pcbi.1010793.e005g" position="anchor"/><mml:math id="M5" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>∼</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:msub><mml:mi>y</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo><mml:mo>∙</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></alternatives><label>(5)</label></disp-formula>
where <italic toggle="yes">α</italic> is a balanced parameter, <italic toggle="yes">q</italic> is an element in candidate GO term set <italic toggle="yes">Q</italic>, <italic toggle="yes">s</italic><sub><italic toggle="yes">saf</italic></sub>(<italic toggle="yes">x</italic>, <italic toggle="yes">q</italic>) is the confidence score of term <italic toggle="yes">q</italic> for <italic toggle="yes">x</italic> in the <italic toggle="yes">FCL</italic><sub><italic toggle="yes">O</italic></sub> of ATGO (see <xref rid="pcbi.1010793.g001" ref-type="fig">Fig 1A</xref>); <italic toggle="yes">y</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">q</italic>) = 1 if <italic toggle="yes">x</italic> is associated with <italic toggle="yes">q</italic> in the experimental function annotation; otherwise, <italic toggle="yes">y</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">q</italic>) = 0. We minimize loss function to optimize ATGO using Adam optimization algorithm [<xref rid="pcbi.1010793.ref055" ref-type="bibr">55</xref>].</p>
      <p>After training ATGO, the GBA strategy is used to generate the confidence score vector <bold><italic toggle="yes">s</italic></bold><sub><italic toggle="yes">gba</italic></sub> of predicted GO terms for the query. Specifically, we select <italic toggle="yes">K</italic> training proteins, which have the highest feature similarity with query, as templates, where the feature dissimilarity between two proteins is defined as the Euclidean distance of corresponding embedding vectors outputted by <italic toggle="yes">FCL</italic><sub><italic toggle="yes">a</italic></sub> in trained ATGO. Then, the confidence score that the query is associated with GO term <italic toggle="yes">q</italic> is calculated:
<disp-formula id="pcbi.1010793.e006"><alternatives><graphic xlink:href="pcbi.1010793.e006.jpg" id="pcbi.1010793.e006g" position="anchor"/><mml:math id="M6" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>∙</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mi>K</mml:mi></mml:math></alternatives><label>(6)</label></disp-formula>
where <italic toggle="yes">r</italic><sub><italic toggle="yes">k</italic></sub> is the rank of the <italic toggle="yes">k</italic>-th template in <italic toggle="yes">K</italic> templates according to the feature similarity with query; <italic toggle="yes">I</italic><sub><italic toggle="yes">k</italic></sub>(<italic toggle="yes">q</italic>) = 1, if the <italic toggle="yes">k</italic>-th template is associated with <italic toggle="yes">q</italic> in the experimental annotation; otherwise, <italic toggle="yes">I</italic><sub><italic toggle="yes">k</italic></sub>(<italic toggle="yes">q</italic>) = 0. The values of <italic toggle="yes">margin</italic>, <italic toggle="yes">c</italic><sub><italic toggle="yes">f</italic></sub>, <italic toggle="yes">α</italic>, and <italic toggle="yes">K</italic> are listed in <xref rid="pcbi.1010793.s020" ref-type="supplementary-material">S15 Table</xref>, which are determined by maximizing the F<sub>max</sub> values of ATGO in the validation dataset.</p>
    </sec>
    <sec id="sec017">
      <title>Composite model of ATGO+</title>
      <p>Inspired by previous works [<xref rid="pcbi.1010793.ref021" ref-type="bibr">21</xref>,<xref rid="pcbi.1010793.ref022" ref-type="bibr">22</xref>], we implemented a composite model, ATGO+, by combining neural network-based model (AGTO) with sequence homology inference (SAGP), to further improve prediction accuracy:
<disp-formula id="pcbi.1010793.e007"><alternatives><graphic xlink:href="pcbi.1010793.e007.jpg" id="pcbi.1010793.e007g" position="anchor"/><mml:math id="M7" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>T</mml:mi><mml:mi>G</mml:mi><mml:mi>O</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo>∙</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>T</mml:mi><mml:mi>G</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>G</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:math></alternatives><label>(7)</label></disp-formula>
where <italic toggle="yes">s</italic><sub><italic toggle="yes">ATGO</italic>+</sub>(<italic toggle="yes">q</italic>) is the confidence score of ATGO+ for GO term <italic toggle="yes">q</italic>, <italic toggle="yes">s</italic><sub><italic toggle="yes">ATGO</italic></sub>(<italic toggle="yes">q</italic>) and <italic toggle="yes">s</italic><sub><italic toggle="yes">SAGP</italic></sub>(<italic toggle="yes">q</italic>) are confidence scores for term <italic toggle="yes">q</italic> by ATGO and SAGP, respectively. The values of the weight parameter <italic toggle="yes">β</italic> are set to be 0.57, 0.60, and 0.67 for MF, BP, and CC, respectively, based on the validation dataset.</p>
    </sec>
    <sec id="sec018">
      <title>Hierarchy of GO annotations</title>
      <p>The GO annotation is hierarchical [<xref rid="pcbi.1010793.ref033" ref-type="bibr">33</xref>]. Specifically, for both the ground truth and the prediction, if a protein is annotated with a GO term <italic toggle="yes">q</italic>, it should be annotated with the direct parent and all ancestors of <italic toggle="yes">q</italic>. To enforce this hierarchical relation, we follow CAFA’s rule and use a common post-processing procedure [<xref rid="pcbi.1010793.ref009" ref-type="bibr">9</xref>] for the confidence score of term <italic toggle="yes">q</italic> in all GO prediction methods:
<disp-formula id="pcbi.1010793.e008"><alternatives><graphic xlink:href="pcbi.1010793.e008.jpg" id="pcbi.1010793.e008g" position="anchor"/><mml:math id="M8" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></alternatives><label>(8)</label></disp-formula>
where <italic toggle="yes">s</italic>(<italic toggle="yes">q</italic>) and <italic toggle="yes">s</italic>(<italic toggle="yes">q</italic>)<sub><italic toggle="yes">post</italic></sub> are the confidence scores of <italic toggle="yes">q</italic> before and after post-processing, <inline-formula id="pcbi.1010793.e009"><alternatives><graphic xlink:href="pcbi.1010793.e009.jpg" id="pcbi.1010793.e009g" position="anchor"/><mml:math id="M9" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> are the confidence scores of all direct child terms of <italic toggle="yes">q</italic> after post-processing. This post-processing procedure enforces that the confidence score of <italic toggle="yes">q</italic> is larger than or equal to the scores of all children.</p>
    </sec>
    <sec id="sec019">
      <title>Evaluation metrices</title>
      <p>F<sub>max</sub> and AUPR are widely used to evaluate the performance of proposed methods. F<sub>max</sub> is a major evaluation score in CAFA [<xref rid="pcbi.1010793.ref032" ref-type="bibr">32</xref>,<xref rid="pcbi.1010793.ref033" ref-type="bibr">33</xref>] and defined as:
<disp-formula id="pcbi.1010793.e010"><alternatives><graphic xlink:href="pcbi.1010793.e010.jpg" id="pcbi.1010793.e010g" position="anchor"/><mml:math id="M10" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>∙</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>∙</mml:mo><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives><label>(9)</label></disp-formula>
where <italic toggle="yes">t</italic> is a cut-off value of confidence score; <italic toggle="yes">pr</italic>(<italic toggle="yes">t</italic>) and <italic toggle="yes">rc</italic>(<italic toggle="yes">t</italic>) are precision and recall, respectively, with confidence score ≥<italic toggle="yes">t</italic>:
<disp-formula id="pcbi.1010793.e011"><alternatives><graphic xlink:href="pcbi.1010793.e011.jpg" id="pcbi.1010793.e011g" position="anchor"/><mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives><label>(10)</label></disp-formula>
where <italic toggle="yes">tp</italic>(<italic toggle="yes">t</italic>) is the number of correctly predicted GO terms, <italic toggle="yes">tp</italic>(<italic toggle="yes">t</italic>)+<italic toggle="yes">fp</italic>(<italic toggle="yes">t</italic>) is the number of all predicted GO terms, and <italic toggle="yes">tp</italic>(<italic toggle="yes">t</italic>)+<italic toggle="yes">fn</italic>(<italic toggle="yes">t</italic>) is the number of GO terms in experimental function annotation.</p>
      <p>AUPR is the area under the precision-recall curve and ranges from 0 to 1. In addition, AUROC is the area under the receiver operating characteristic curve and also ranges from 0 to 1.</p>
    </sec>
  </sec>
  <sec id="sec020" sec-type="supplementary-material">
    <title>Supporting information</title>
    <supplementary-material id="pcbi.1010793.s001" position="float" content-type="local-data">
      <label>S1 Fig</label>
      <caption>
        <title>The mean AUROC values of GO terms versus 12 GO prediction methods in four ranges.</title>
        <p><bold>(a)</bold> range 5–10. <bold>(b)</bold> range 10–30. <bold>(c)</bold> range 30–50. <bold>(d)</bold> range &gt;50.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s001.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s002" position="float" content-type="local-data">
      <label>S2 Fig</label>
      <caption>
        <title>The median AUROC values of GO terms versus 12 GO prediction methods in four ranges.</title>
        <p><bold>(a)</bold> range 5–10. <bold>(b)</bold> range 10–30. <bold>(c)</bold> range 30–50. <bold>(d)</bold> range &gt;50.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s002.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s003" position="float" content-type="local-data">
      <label>S3 Fig</label>
      <caption>
        <title>The distributions of AUROC values for GO terms in two ranges versus 12 GO prediction methods, where the median line in the box is the median AUROC value.</title>
        <p><bold>(a)</bold> range 30–50. <bold>(b)</bold> range &gt;50.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s003.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s004" position="float" content-type="local-data">
      <label>S4 Fig</label>
      <caption>
        <title>The AUPR values of 10 GO prediction methods under the sequence identity cut-off <italic toggle="yes">t</italic><sub>1</sub> = 30% for three GO aspects on four individual species in CAFA3 test dataset.</title>
        <p><bold>(a)</bold> Human <bold>(b)</bold> Arabidopsis <bold>(c)</bold> Fission Yeast <bold>(d)</bold> Mouse.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s004.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s005" position="float" content-type="local-data">
      <label>S5 Fig</label>
      <caption>
        <title>The architectures of different models in ablation study.</title>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s005.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s006" position="float" content-type="local-data">
      <label>S1 Table</label>
      <caption>
        <title>The <italic toggle="yes">p</italic>-values of performance difference between 12 GO prediction methods on 1068 individual test proteins under post-hoc Nemenyi test at the individual protein level, where the performance of each prediction method is measured by a group of F<sub>1</sub>-scores, each of which is calculated from the predicted GO terms and native GO annotation in a single test protein.</title>
        <p>Because the <italic toggle="yes">p</italic>-values can be only approximated in the range from 1.0e-03 to 9.0e-01 under post-hoc Nemenyi test using Python package, the numerical value of 1.0e-03 (or 9.0e-01) means that the <italic toggle="yes">p</italic>-value is below to 1.0e-03 (or upon to 9.0e-01).</p>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s006.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s007" position="float" content-type="local-data">
      <label>S2 Table</label>
      <caption>
        <title>The statistic values between SAGP and ATGO in Group A under Nemenyi post-hoc test on 1068 test proteins for MF aspect versus the increase of <italic toggle="yes">K</italic>.</title>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s007.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s008" position="float" content-type="local-data">
      <label>S3 Table</label>
      <caption>
        <title>The prediction performance with including root GO terms for ATGO and TALE on all 1068 test proteins.</title>
        <p><italic toggle="yes">p</italic>-values in parenthesis are calculated between ATGO and TALE by two-sided Student’s t-test. Specifically, the proposed ATGO is repeatedly implemented with 10 times on the benchmark dataset to generate the corresponding performance evaluation indices, which are compared with the fixed evaluation index generated by TALE to calculate <italic toggle="yes">p</italic>-value using two-sided Student’s t-test. Bold fonts highlight the best performer in each category.</p>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s008.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s009" position="float" content-type="local-data">
      <label>S4 Table</label>
      <caption>
        <title>The summary of the proposed ATGO/ATGO+ and other 10 competing GO prediction methods on a subset of 562 test proteins which have available templates or interaction partners in all of SAGP, PPIGP, FunFams, and DIAMONDScore.</title>
        <p><italic toggle="yes">p</italic>-values in parenthesis are calculated between ATGO and other single-based methods and between ATGO+ and other composite methods by two-sided Student’s t-test. Specifically, the proposed ATGO and ATGO+ are repeatedly implemented with 10 times on the benchmark dataset to generate the corresponding performance evaluation indices, which are compared with the fixed evaluation index generated by the competing method to calculate <italic toggle="yes">p</italic>-value using two-sided Student’s t-test. Bold fonts highlight the best performer in each category.</p>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s009.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s010" position="float" content-type="local-data">
      <label>S5 Table</label>
      <caption>
        <title>The ICW-F<sub>max</sub> values of 12 GO prediction methods on the 1068 benchmark proteins.</title>
        <p>Bold fonts highlight the best performer in each category.</p>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s010.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s011" position="float" content-type="local-data">
      <label>S6 Table</label>
      <caption>
        <title>The <italic toggle="yes">p</italic>-values of performance difference between 10 GO prediction methods on 3328 individual CAFA3 targets under post-hoc Nemenyi test at the individual protein level, where the performance of each prediction method is measured by a group of F<sub>1</sub>-scores, each of which is calculated from the predicted GO terms and native GO annotation in a single test protein.</title>
        <p>Because the <italic toggle="yes">p</italic>-values can be only approximated in the range from 1.0e-03 to 9.0e-01 under post-hoc Nemenyi test using Python package, the numerical value of 1.0e-03 (or 9.0e-01) means that the <italic toggle="yes">p</italic>-value is below to 1.0e-03 (or upon to 9.0e-01).</p>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s011.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s012" position="float" content-type="local-data">
      <label>S7 Table</label>
      <caption>
        <title>The ICW-F<sub>max</sub> values of 10 GO prediction methods on 3328 CAFA3 targets where a sequence identity cut-off <italic toggle="yes">t</italic><sub>1</sub> = 30% between the training and testing proteins was applied to the five in-house methods (ATGO, ATGO+, SAGP, PPIGP and NGP).</title>
        <p>Bold fonts highlight the best performer in each category.</p>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s012.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s013" position="float" content-type="local-data">
      <label>S8 Table</label>
      <caption>
        <title>The performance of 10 GO prediction methods under the cut-off <italic toggle="yes">t</italic><sub>1</sub> = 30% on 1177 no-knowledge (NK) and 2151 limited-knowledge (LK) CAFA3 proteins.</title>
        <p>Bold fonts highlight the best performer in each category.</p>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s013.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s014" position="float" content-type="local-data">
      <label>S9 Table</label>
      <caption>
        <title>The numbers of proteins for 20 species in CAFA3 test dataset.</title>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s014.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s015" position="float" content-type="local-data">
      <label>S10 Table</label>
      <caption>
        <title>The prediction performance of five GO prediction methods under the cut-off <italic toggle="yes">t</italic><sub>1</sub> = 100% on CAFA3 test proteins.</title>
        <p>Bold fonts highlight the best performer in each category.</p>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s015.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s016" position="float" content-type="local-data">
      <label>S11 Table</label>
      <caption>
        <title>The prediction performance of SAGP and BLAST baseline on our constructed test dataset and CAFA3 test dataset with different cut-off values of sequence identity.</title>
        <p>Bold fonts highlight the best performer in each category.</p>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s016.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s017" position="float" content-type="local-data">
      <label>S12 Table</label>
      <caption>
        <title>The prediction performance of ATGO models via four metric learning methods on two test datasets.</title>
        <p>Bold fonts highlight the best performer in each category.</p>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s017.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s018" position="float" content-type="local-data">
      <label>S13 Table</label>
      <caption>
        <title>The incorrectly predicted GO terms for 12 methods on three proteins in BP aspect.</title>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s018.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s019" position="float" content-type="local-data">
      <label>S14 Table</label>
      <caption>
        <title>The numbers of proteins and GO terms in benchmark dataset.</title>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s019.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s020" position="float" content-type="local-data">
      <label>S15 Table</label>
      <caption>
        <title>The values of <italic toggle="yes">margin</italic>, <italic toggle="yes">c</italic><sub><italic toggle="yes">f</italic></sub>, <italic toggle="yes">α</italic>, and <italic toggle="yes">K</italic> for three GO aspects.</title>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s020.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s021" position="float" content-type="local-data">
      <label>S1 Text</label>
      <caption>
        <title>Sequence alignment-based GO prediction.</title>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s021.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s022" position="float" content-type="local-data">
      <label>S2 Text</label>
      <caption>
        <title>Protein-protein interaction-based GO prediction.</title>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s022.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s023" position="float" content-type="local-data">
      <label>S3 Text</label>
      <caption>
        <title>Naïve-based GO prediction.</title>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s023.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s024" position="float" content-type="local-data">
      <label>S4 Text</label>
      <caption>
        <title>Friedman and Nemenyi post-hoc tests at the individual protein level.</title>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s024.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s025" position="float" content-type="local-data">
      <label>S5 Text</label>
      <caption>
        <title>An explanation for the difference of <italic toggle="yes">p</italic>-value calculations between Student t-test and Nemenyi post-hoc test.</title>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s025.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s026" position="float" content-type="local-data">
      <label>S6 Text</label>
      <caption>
        <title>Information content-weighted maximum F<sub>1</sub>-score.</title>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s026.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s027" position="float" content-type="local-data">
      <label>S7 Text</label>
      <caption>
        <title>Performance comparison between SAGP and BLAST baseline used in CAFA challenge.</title>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s027.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s028" position="float" content-type="local-data">
      <label>S8 Text</label>
      <caption>
        <title>Performance comparison between four metric learning methods.</title>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s028.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s029" position="float" content-type="local-data">
      <label>S9 Text</label>
      <caption>
        <title>The mathematics formulas for ESM-1b transformer.</title>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s029.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010793.s030" position="float" content-type="local-data">
      <label>S10 Text</label>
      <caption>
        <title>The functional similarity between two proteins.</title>
        <p>(DOCX)</p>
      </caption>
      <media xlink:href="pcbi.1010793.s030.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="pcbi.1010793.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Eisenberg</surname><given-names>D</given-names></name>, <name><surname>Marcotte</surname><given-names>EM</given-names></name>, <name><surname>Xenarios</surname><given-names>I</given-names></name>, <name><surname>Yeates</surname><given-names>TO</given-names></name>. <article-title>Protein function in the post-genomic era</article-title>. <source>Nature</source>. <year>2000</year>;<volume>405</volume>(<issue>6788</issue>):<fpage>823</fpage>–<lpage>6</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/35015694</pub-id><?supplied-pmid 10866208?><pub-id pub-id-type="pmid">10866208</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Ashburner</surname><given-names>M</given-names></name>, <name><surname>Ball</surname><given-names>CA</given-names></name>, <name><surname>Blake</surname><given-names>JA</given-names></name>, <name><surname>Botstein</surname><given-names>D</given-names></name>, <name><surname>Butler</surname><given-names>H</given-names></name>, <name><surname>Cherry</surname><given-names>JM</given-names></name>, <etal>et al</etal>. <article-title>Gene ontology: tool for the unification of biology.</article-title><source>Nature genetics.</source><year>2000</year>;<volume>25</volume>(<issue>1</issue>):<fpage>25</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">10802651</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Peng</surname><given-names>J</given-names></name>, <name><surname>Xue</surname><given-names>H</given-names></name>, <name><surname>Wei</surname><given-names>Z</given-names></name>, <name><surname>Tuncali</surname><given-names>I</given-names></name>, <name><surname>Hao</surname><given-names>J</given-names></name>, <name><surname>Shang</surname><given-names>X</given-names></name>. <article-title>Integrating multi-network topology for gene function prediction using deep neural networks</article-title>. <source>Briefings in bioinformatics</source>. <year>2020</year>:<fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Consortium</surname><given-names>U.</given-names></name><article-title>UniProt: a hub for protein information</article-title>. <source>Nucleic acids research</source>. <year>2015</year>;<volume>43</volume>(<issue>D1</issue>):<fpage>D204</fpage>–<lpage>D12</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/nar/gku989</pub-id><?supplied-pmid 25348405?><pub-id pub-id-type="pmid">25348405</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Franz</surname><given-names>M</given-names></name>, <name><surname>Rodriguez</surname><given-names>H</given-names></name>, <name><surname>Lopes</surname><given-names>C</given-names></name>, <name><surname>Zuberi</surname><given-names>K</given-names></name>, <name><surname>Montojo</surname><given-names>J</given-names></name>, <name><surname>Bader</surname><given-names>GD</given-names></name>, <etal>et al</etal>. <article-title>GeneMANIA update 2018</article-title>. <source>Nucleic acids research</source>. <year>2018</year>;<volume>46</volume>(<issue>W1</issue>):<fpage>W60</fpage>–<lpage>W4</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/nar/gky311</pub-id><?supplied-pmid 29912392?><pub-id pub-id-type="pmid">29912392</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Urzúa-Traslaviña</surname><given-names>CG</given-names></name>, <name><surname>Leeuwenburgh</surname><given-names>VC</given-names></name>, <name><surname>Bhattacharya</surname><given-names>A</given-names></name>, <name><surname>Loipfinger</surname><given-names>S</given-names></name>, <name><surname>van Vugt</surname><given-names>MA</given-names></name>, <name><surname>de Vries</surname><given-names>EG</given-names></name>, <etal>et al</etal>. <article-title>Improving gene function predictions using independent transcriptional components</article-title>. <source>Nature communications</source>. <year>2021</year>;<volume>12</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>14</lpage>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Conesa</surname><given-names>A</given-names></name>, <name><surname>Götz</surname><given-names>S</given-names></name>, <name><surname>García-Gómez</surname><given-names>JM</given-names></name>, <name><surname>Terol</surname><given-names>J</given-names></name>, <name><surname>Talón</surname><given-names>M</given-names></name>, <name><surname>Robles</surname><given-names>M</given-names></name>. <article-title>Blast2GO: a universal tool for annotation, visualization and analysis in functional genomics research</article-title>. <source>Bioinformatics</source>. <year>2005</year>;<volume>21</volume>(<issue>18</issue>):<fpage>3674</fpage>–<lpage>6</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/bti610</pub-id><?supplied-pmid 16081474?><pub-id pub-id-type="pmid">16081474</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Yao</surname><given-names>S</given-names></name>, <name><surname>You</surname><given-names>R</given-names></name>, <name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Xiong</surname><given-names>Y</given-names></name>, <name><surname>Huang</surname><given-names>X</given-names></name>, <name><surname>Zhu</surname><given-names>S</given-names></name>. <article-title>NetGO 2.0: improving large-scale protein function prediction with massive sequence, text, domain, family and network information</article-title>. <source>Nucleic acids research</source>. <year>2021</year>;<volume>49</volume>(<issue>W1</issue>):<fpage>W469</fpage>–<lpage>W75</lpage>. Epub 2021/05/27. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/nar/gkab398</pub-id> ; PubMed Central PMCID: PMC8262706.<?supplied-pmid 34038555?><pub-id pub-id-type="pmid">34038555</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Kulmanov</surname><given-names>M</given-names></name>, <name><surname>Khan</surname><given-names>MA</given-names></name>, <name><surname>Hoehndorf</surname><given-names>R</given-names></name>. <article-title>DeepGO: predicting protein functions from sequence and interactions using a deep ontology-aware classifier</article-title>. <source>Bioinformatics</source>. <year>2018</year>;<volume>34</volume>(<issue>4</issue>):<fpage>660</fpage>–<lpage>8</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btx624</pub-id><?supplied-pmid 29028931?><pub-id pub-id-type="pmid">29028931</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Martin</surname><given-names>D</given-names></name>, <name><surname>Berriman</surname><given-names>M</given-names></name>, <name><surname>Barton</surname><given-names>GJ</given-names></name>. <article-title>GOtcha: a new method for prediction of protein function assessed by the annotation of seven genomes.</article-title><source>BMC bioinformatics</source>. <year>2004</year>;<volume>5</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>17</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1186/1471-2105-5-178</pub-id><?supplied-pmid 15550167?><pub-id pub-id-type="pmid">14706121</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Gong</surname><given-names>Q</given-names></name>, <name><surname>Ning</surname><given-names>W</given-names></name>, <name><surname>Tian</surname><given-names>W</given-names></name>. <article-title>GoFDR: a sequence alignment based method for predicting protein functions.</article-title><source>Methods</source>. <year>2016</year>;<volume>93</volume>:<fpage>3</fpage>–<lpage>14</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.ymeth.2015.08.009</pub-id><?supplied-pmid 26277418?><pub-id pub-id-type="pmid">26277418</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Altschul</surname><given-names>SF</given-names></name>, <name><surname>Madden</surname><given-names>TL</given-names></name>, <name><surname>Schäffer</surname><given-names>AA</given-names></name>, <name><surname>Zhang</surname><given-names>J</given-names></name>, <name><surname>Zhang</surname><given-names>Z</given-names></name>, <name><surname>Miller</surname><given-names>W</given-names></name>, <etal>et al</etal>. <article-title>Gapped BLAST and PSI-BLAST: a new generation of protein database search programs</article-title>. <source>Nucleic acids research</source>. <year>1997</year>;<volume>25</volume>(<issue>17</issue>):<fpage>3389</fpage>–<lpage>402</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/nar/25.17.3389</pub-id><?supplied-pmid 9254694?><pub-id pub-id-type="pmid">9254694</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Roy</surname><given-names>A</given-names></name>, <name><surname>Yang</surname><given-names>J</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>. <article-title>COFACTOR: an accurate comparative algorithm for structure-based protein function annotation</article-title>. <source>Nucleic acids research</source>. <year>2012</year>;<volume>40</volume>(<issue>W1</issue>):<fpage>W471</fpage>–<lpage>W7</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/nar/gks372</pub-id><?supplied-pmid 22570420?><pub-id pub-id-type="pmid">22570420</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Laskowski</surname><given-names>RA</given-names></name>, <name><surname>Watson</surname><given-names>JD</given-names></name>, <name><surname>Thornton</surname><given-names>JM</given-names></name>. <article-title>ProFunc: a server for predicting protein function from 3D structure</article-title>. <source>Nucleic acids research</source>. <year>2005</year>;<volume>33</volume>(<issue>suppl_2</issue>):<fpage>W89</fpage>–<lpage>W93</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/nar/gki414</pub-id><?supplied-pmid 15980588?><pub-id pub-id-type="pmid">15980588</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Skolnick</surname><given-names>J</given-names></name>. <article-title>TM-align: a protein structure alignment algorithm based on the TM-score</article-title>. <source>Nucleic acids research</source>. <year>2005</year>;<volume>33</volume>(<issue>7</issue>):<fpage>2302</fpage>–<lpage>9</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/nar/gki524</pub-id><?supplied-pmid 15849316?><pub-id pub-id-type="pmid">15849316</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Lan</surname><given-names>L</given-names></name>, <name><surname>Djuric</surname><given-names>N</given-names></name>, <name><surname>Guo</surname><given-names>Y</given-names></name>, <name><surname>Vucetic</surname><given-names>S</given-names></name>. <article-title>MS-k NN: protein function prediction by integrating multiple data sources.</article-title><source>BMC bioinformatics</source>. <year>2013</year>;<volume>14</volume>(<issue>3</issue>):<fpage>1</fpage>–<lpage>10</lpage>.<pub-id pub-id-type="pmid">23323762</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Zhu</surname><given-names>Y</given-names></name>, <name><surname>Zhang</surname><given-names>C</given-names></name>, <name><surname>Liu</surname><given-names>Y</given-names></name>, <name><surname>Omenn</surname><given-names>GS</given-names></name>, <name><surname>Freddolino</surname><given-names>PL</given-names></name>, <name><surname>Yu</surname><given-names>D</given-names></name>, <etal>et al</etal>. <article-title>Integrating transcript expression profiles with protein homology inferences for gene function prediction</article-title>. <source>Genomics, Proteomics &amp; Bioinformatics.</source><year>2022</year>: <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.gpb.2022.03.001</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Cozzetto</surname><given-names>D</given-names></name>, <name><surname>Minneci</surname><given-names>F</given-names></name>, <name><surname>Currant</surname><given-names>H</given-names></name>, <name><surname>Jones</surname><given-names>DT</given-names></name>. <article-title>FFPred 3: feature-based function prediction for all Gene Ontology domains.</article-title><source>Scientific reports</source>. <year>2016</year>;<volume>6</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>11</lpage>.<pub-id pub-id-type="pmid">28442746</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>You</surname><given-names>R</given-names></name>, <name><surname>Zhang</surname><given-names>Z</given-names></name>, <name><surname>Xiong</surname><given-names>Y</given-names></name>, <name><surname>Sun</surname><given-names>F</given-names></name>, <name><surname>Mamitsuka</surname><given-names>H</given-names></name>, <name><surname>Zhu</surname><given-names>S</given-names></name>. <article-title>GOLabeler: improving sequence-based large-scale protein function prediction by learning to rank</article-title>. <source>Bioinformatics</source>. <year>2018</year>;<volume>34</volume>(<issue>14</issue>):<fpage>2465</fpage>–<lpage>73</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/bty130</pub-id><?supplied-pmid 29522145?><pub-id pub-id-type="pmid">29522145</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Lobley</surname><given-names>AE</given-names></name>, <name><surname>Nugent</surname><given-names>T</given-names></name>, <name><surname>Orengo</surname><given-names>CA</given-names></name>, <name><surname>Jones</surname><given-names>DT</given-names></name>. <article-title>FFPred: an integrated feature-based function prediction server for vertebrate proteomes</article-title>. <source>Nucleic acids research</source>. <year>2008</year>;<volume>36</volume>(<issue>suppl_2</issue>):<fpage>W297</fpage>–<lpage>W302</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/nar/gkn193</pub-id><?supplied-pmid 18463141?><pub-id pub-id-type="pmid">18463141</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Kulmanov</surname><given-names>M</given-names></name>, <name><surname>Hoehndorf</surname><given-names>R</given-names></name>. <article-title>DeepGOPlus: improved protein function prediction from sequence</article-title>. <source>Bioinformatics</source>. <year>2020</year>;<volume>36</volume>(<issue>2</issue>):<fpage>422</fpage>–<lpage>9</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btz595</pub-id><?supplied-pmid 31350877?><pub-id pub-id-type="pmid">31350877</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref022">
      <label>22</label>
      <mixed-citation publication-type="journal"><name><surname>Cao</surname><given-names>Y</given-names></name>, <name><surname>Shen</surname><given-names>Y</given-names></name>. <article-title>TALE: Transformer-based protein function Annotation with joint sequence–Label Embedding</article-title>. <source>Bioinformatics</source>. <year>2021</year>;<volume>37</volume>(<issue>18</issue>):<fpage>2825</fpage>–<lpage>33</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btab198</pub-id><?supplied-pmid 33755048?><pub-id pub-id-type="pmid">33755048</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref023">
      <label>23</label>
      <mixed-citation publication-type="journal"><name><surname>Elnaggar</surname><given-names>A</given-names></name>, <name><surname>Heinzinger</surname><given-names>M</given-names></name>, <name><surname>Dallago</surname><given-names>C</given-names></name>, <name><surname>Rehawi</surname><given-names>G</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>Jones</surname><given-names>L</given-names></name>, <etal>et al</etal>. <article-title>ProtTrans: towards cracking the language of lifes code through self-supervised deep learning and high performance computing</article-title>. <source>IEEE transactions on pattern analysis and machine intelligence</source>. <year>2021</year>:<fpage>1</fpage>–<lpage>10</lpage>.<pub-id pub-id-type="pmid">31331880</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Rao</surname><given-names>R</given-names></name>, <name><surname>Bhattacharya</surname><given-names>N</given-names></name>, <name><surname>Thomas</surname><given-names>N</given-names></name>, <name><surname>Duan</surname><given-names>Y</given-names></name>, <name><surname>Chen</surname><given-names>X</given-names></name>, <name><surname>Canny</surname><given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Evaluating Protein Transfer Learning with TAPE.</article-title><source>Adv Neural Inf Process Syst.</source><year>2019</year>;<volume>32</volume>:<fpage>9689</fpage>–<lpage>701</lpage>. ; PubMed Central PMCID: PMC7774645.<?supplied-pmid 33390682?><pub-id pub-id-type="pmid">33390682</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref025">
      <label>25</label>
      <mixed-citation publication-type="journal"><name><surname>Bepler</surname><given-names>T</given-names></name>, <name><surname>Berger</surname><given-names>B</given-names></name>. <article-title>Learning protein sequence embeddings using information from structure</article-title>. <source>arXiv preprint arXiv:190208661.</source><year>2019</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref026">
      <label>26</label>
      <mixed-citation publication-type="journal"><name><surname>Rives</surname><given-names>A</given-names></name>, <name><surname>Meier</surname><given-names>J</given-names></name>, <name><surname>Sercu</surname><given-names>T</given-names></name>, <name><surname>Goyal</surname><given-names>S</given-names></name>, <name><surname>Lin</surname><given-names>Z</given-names></name>, <name><surname>Liu</surname><given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2021</year>;<volume>118</volume>(<issue>15</issue>):<fpage>1</fpage>–<lpage>12</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.2016239118</pub-id><?supplied-pmid 33876751?><pub-id pub-id-type="pmid">33876751</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Villegas-Morcillo</surname><given-names>A</given-names></name>, <name><surname>Makrodimitris</surname><given-names>S</given-names></name>, <name><surname>van Ham</surname><given-names>RC</given-names></name>, <name><surname>Gomez</surname><given-names>AM</given-names></name>, <name><surname>Sanchez</surname><given-names>V</given-names></name>, <name><surname>Reinders</surname><given-names>MJ</given-names></name>. <article-title>Unsupervised protein embeddings outperform hand-crafted sequence and structure features at predicting molecular function</article-title>. <source>Bioinformatics</source>. <year>2021</year>;<volume>37</volume>(<issue>2</issue>):<fpage>162</fpage>–<lpage>70</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btaa701</pub-id><?supplied-pmid 32797179?><pub-id pub-id-type="pmid">32797179</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref028">
      <label>28</label>
      <mixed-citation publication-type="journal"><name><surname>Littmann</surname><given-names>M</given-names></name>, <name><surname>Heinzinger</surname><given-names>M</given-names></name>, <name><surname>Dallago</surname><given-names>C</given-names></name>, <name><surname>Olenyi</surname><given-names>T</given-names></name>, <name><surname>Rost</surname><given-names>B</given-names></name>. <article-title>Embeddings from deep learning transfer GO annotations beyond homology.</article-title><source>Sci Rep.</source><year>2021</year>;<volume>11</volume>(<issue>1</issue>):<fpage>1160</fpage>. Epub 20210113. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41598-020-80786-0</pub-id> ; PubMed Central PMCID: PMC7806674.<?supplied-pmid 33441905?><pub-id pub-id-type="pmid">33441905</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref029">
      <label>29</label>
      <mixed-citation publication-type="journal"><name><surname>Littmann</surname><given-names>M</given-names></name>, <name><surname>Heinzinger</surname><given-names>M</given-names></name>, <name><surname>Dallago</surname><given-names>C</given-names></name>, <name><surname>Weissenow</surname><given-names>K</given-names></name>, <name><surname>Rost</surname><given-names>B</given-names></name>. <article-title>Protein embeddings and deep learning predict binding residues for various ligand classes.</article-title><source>Sci Rep.</source><year>2021</year>;<volume>11</volume>(<issue>1</issue>):<fpage>23916</fpage>. Epub 20211213. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41598-021-03431-4</pub-id> ; PubMed Central PMCID: PMC8668950.<?supplied-pmid 34903827?><pub-id pub-id-type="pmid">34903827</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref030">
      <label>30</label>
      <mixed-citation publication-type="journal"><name><surname>Schroff</surname><given-names>F</given-names></name>, <name><surname>Kalenichenko</surname><given-names>D</given-names></name>, <name><surname>Philbin</surname><given-names>J</given-names></name>. <article-title>Facenet: A unified embedding for face recognition and clustering</article-title>. <source>Proceedings of the 28th IEEE conference on computer vision and pattern recognition; Boston</source><year>2015</year>. p. <fpage>815</fpage>–<lpage>23</lpage>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref031">
      <label>31</label>
      <mixed-citation publication-type="journal"><name><surname>Das</surname><given-names>S</given-names></name>, <name><surname>Lee</surname><given-names>D</given-names></name>, <name><surname>Sillitoe</surname><given-names>I</given-names></name>, <name><surname>Dawson</surname><given-names>NL</given-names></name>, <name><surname>Lees</surname><given-names>JG</given-names></name>, <name><surname>Orengo</surname><given-names>CA</given-names></name>. <article-title>Functional classification of CATH superfamilies: a domain-based approach for protein function annotation</article-title>. <source>Bioinformatics</source>. <year>2015</year>;<volume>31</volume>(<issue>21</issue>):<fpage>3460</fpage>–<lpage>7</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btv398</pub-id><?supplied-pmid 26139634?><pub-id pub-id-type="pmid">26139634</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref032">
      <label>32</label>
      <mixed-citation publication-type="journal"><name><surname>Gillis</surname><given-names>J</given-names></name>, <name><surname>Pavlidis</surname><given-names>P</given-names></name>. <article-title>Characterizing the state of the art in the computational assignment of gene function: lessons from the first critical assessment of functional annotation (CAFA).</article-title><source>BMC bioinformatics.</source><year>2013</year>;<volume>14</volume>(<issue>3</issue>):<fpage>1</fpage>–<lpage>12</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1186/1471-2105-14-s3-s15</pub-id><?supplied-pmid 23630983?><pub-id pub-id-type="pmid">23323762</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref033">
      <label>33</label>
      <mixed-citation publication-type="journal"><name><surname>Zhou</surname><given-names>N</given-names></name>, <name><surname>Jiang</surname><given-names>Y</given-names></name>, <name><surname>Bergquist</surname><given-names>TR</given-names></name>, <name><surname>Lee</surname><given-names>AJ</given-names></name>, <name><surname>Kacsoh</surname><given-names>BZ</given-names></name>, <name><surname>Crocker</surname><given-names>AW</given-names></name>, <etal>et al</etal>. <article-title>The CAFA challenge reports improved protein function prediction and new functional annotations for hundreds of genes through experimental screens</article-title>. <source>Genome Biology</source>. <year>2019</year>;<volume>20</volume>(<issue>1</issue>):<fpage>244</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1186/s13059-019-1835-8</pub-id><?supplied-pmid 31744546?><pub-id pub-id-type="pmid">31744546</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref034">
      <label>34</label>
      <mixed-citation publication-type="journal"><name><surname>Boyd</surname><given-names>K</given-names></name>, <name><surname>Eng</surname><given-names>KH</given-names></name>, <name><surname>Page</surname><given-names>CD</given-names></name>. <article-title>Area under the precision-recall curve: point estimates and confidence intervals</article-title>. <source>Joint European conference on machine learning and knowledge discovery in databases</source>. <year>2013</year>:<fpage>451</fpage>–<lpage>66</lpage>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref035">
      <label>35</label>
      <mixed-citation publication-type="journal"><name><surname>Sheldon</surname><given-names>MR</given-names></name>, <name><surname>Fillyaw</surname><given-names>MJ</given-names></name>, <name><surname>Thompson</surname><given-names>WD</given-names></name>. <article-title>The use and interpretation of the Friedman test in the analysis of ordinal-scale data in repeated measures designs.</article-title><source>Physiotherapy Research International.</source><year>1996</year>;<volume>1</volume>(<issue>4</issue>):<fpage>221</fpage>–<lpage>8</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1002/pri.66</pub-id><?supplied-pmid 9238739?><pub-id pub-id-type="pmid">9238739</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref036">
      <label>36</label>
      <mixed-citation publication-type="journal"><name><surname>Hilton</surname><given-names>A</given-names></name>, <name><surname>Armstrong</surname><given-names>RA</given-names></name>. <article-title>Statnote 6: post-hoc ANOVA tests.</article-title><source>Microbiologist</source>. <year>2006</year>;<volume>2006</volume>:<fpage>34</fpage>–<lpage>6</lpage>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref037">
      <label>37</label>
      <mixed-citation publication-type="journal"><name><surname>Cv</surname><given-names>Mering</given-names></name>, <name><surname>Huynen</surname><given-names>M</given-names></name>, <name><surname>Jaeggi</surname><given-names>D</given-names></name>, <name><surname>Schmidt</surname><given-names>S</given-names></name>, <name><surname>Bork</surname><given-names>P</given-names></name>, <name><surname>Snel</surname><given-names>B</given-names></name>. <article-title>STRING: a database of predicted functional associations between proteins</article-title>. <source>Nucleic acids research</source>. <year>2003</year>;<volume>31</volume>(<issue>1</issue>):<fpage>258</fpage>–<lpage>61</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/nar/gkg034</pub-id><?supplied-pmid 12519996?><pub-id pub-id-type="pmid">12519996</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref038">
      <label>38</label>
      <mixed-citation publication-type="other">Han J, Moraga C, editors. The influence of the sigmoid function parameters on the speed of backpropagation learning. International Workshop on Artificial Neural Networks; 1995: Springer.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref039">
      <label>39</label>
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Z</given-names></name>, <name><surname>Sabuncu</surname><given-names>MR</given-names></name>. <article-title>Generalized cross entropy loss for training deep neural networks with noisy labels.</article-title><source>arXiv preprint arXiv:180507836.</source><year>2018</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref040">
      <label>40</label>
      <mixed-citation publication-type="journal"><name><surname>Bag</surname><given-names>S</given-names></name>, <name><surname>Kumar</surname><given-names>SK</given-names></name>, <name><surname>Tiwari</surname><given-names>MK</given-names></name>. <article-title>An efficient recommendation generation using relevant Jaccard similarity.</article-title><source>Information Sciences</source>. <year>2019</year>;<volume>483</volume>:<fpage>53</fpage>–<lpage>64</lpage>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref041">
      <label>41</label>
      <mixed-citation publication-type="journal"><name><surname>Hayn</surname><given-names>C.</given-names></name><article-title>The information content of losses</article-title>. <source>Journal of accounting and economics</source>. <year>1995</year>;<volume>20</volume>(<issue>2</issue>):<fpage>125</fpage>–<lpage>53</lpage>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref042">
      <label>42</label>
      <mixed-citation publication-type="journal"><name><surname>Lin</surname><given-names>Z</given-names></name>, <name><surname>Akin</surname><given-names>H</given-names></name>, <name><surname>Rao</surname><given-names>R</given-names></name>, <name><surname>Hie</surname><given-names>B</given-names></name>, <name><surname>Zhu</surname><given-names>Z</given-names></name>, <name><surname>Lu</surname><given-names>W</given-names></name>, <etal>et al</etal>. <article-title>Language models of protein sequences at the scale of evolution enable accurate structure prediction</article-title>. <source>BioRxiv</source>. <year>2022</year>:<pub-id pub-id-type="doi">10.1101/2022.07.20.500902</pub-id>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref043">
      <label>43</label>
      <mixed-citation publication-type="journal"><name><surname>Edgar</surname><given-names>RC</given-names></name>, <name><surname>Batzoglou</surname><given-names>S</given-names></name>. <article-title>Multiple sequence alignment</article-title>. <source>Current Opinion in Structural Biology</source>. <year>2006</year>;<volume>16</volume>(<issue>3</issue>):<fpage>368</fpage>–<lpage>73</lpage>. Epub 20060505. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.sbi.2006.04.004</pub-id> .<?supplied-pmid 16679011?><pub-id pub-id-type="pmid">16679011</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref044">
      <label>44</label>
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>C</given-names></name>, <name><surname>Zheng</surname><given-names>W</given-names></name>, <name><surname>Mortuza</surname><given-names>S</given-names></name>, <name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>. <article-title>DeepMSA: constructing deep multiple sequence alignment to improve contact prediction and fold-recognition for distant-homology proteins</article-title>. <source>Bioinformatics</source>. <year>2020</year>;<volume>36</volume>(<issue>7</issue>):<fpage>2105</fpage>–<lpage>12</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btz863</pub-id><?supplied-pmid 31738385?><pub-id pub-id-type="pmid">31738385</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref045">
      <label>45</label>
      <mixed-citation publication-type="journal"><name><surname>Rao</surname><given-names>R</given-names></name>, <name><surname>Liu</surname><given-names>J</given-names></name>, <name><surname>Verkuil</surname><given-names>R</given-names></name>, <name><surname>Meier</surname><given-names>J</given-names></name>, <name><surname>Canny</surname><given-names>JF</given-names></name>, <name><surname>Abbeel</surname><given-names>P</given-names></name>, <etal>et al</etal>. <article-title>MSA Transformer.</article-title><source>bioRxiv</source>. <year>2021</year>:2021.02.12.430858.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref046">
      <label>46</label>
      <mixed-citation publication-type="journal"><name><surname>Camon</surname><given-names>E</given-names></name>, <name><surname>Magrane</surname><given-names>M</given-names></name>, <name><surname>Barrell</surname><given-names>D</given-names></name>, <name><surname>Lee</surname><given-names>V</given-names></name>, <name><surname>Dimmer</surname><given-names>E</given-names></name>, <name><surname>Maslen</surname><given-names>J</given-names></name>, <etal>et al</etal>. <article-title>The gene ontology annotation (goa) database: sharing knowledge in uniprot with gene ontology.</article-title><source>Nucleic acids research</source>. <year>2004</year>;<volume>32</volume>(<issue>suppl_1</issue>):<fpage>D262</fpage>–<lpage>D6</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/nar/gkh021</pub-id><?supplied-pmid 14681408?><pub-id pub-id-type="pmid">14681408</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref047">
      <label>47</label>
      <mixed-citation publication-type="journal"><name><surname>Radivojac</surname><given-names>P</given-names></name>, <name><surname>Clark</surname><given-names>WT</given-names></name>, <name><surname>Oron</surname><given-names>TR</given-names></name>, <name><surname>Schnoes</surname><given-names>AM</given-names></name>, <name><surname>Wittkop</surname><given-names>T</given-names></name>, <name><surname>Sokolov</surname><given-names>A</given-names></name>, <etal>et al</etal>. <article-title>A large-scale evaluation of computational protein function prediction</article-title>. <source>Nature methods</source>. <year>2013</year>;<volume>10</volume>(<issue>3</issue>):<fpage>221</fpage>–<lpage>7</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nmeth.2340</pub-id><?supplied-pmid 23353650?><pub-id pub-id-type="pmid">23353650</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref048">
      <label>48</label>
      <mixed-citation publication-type="journal"><name><surname>Jiang</surname><given-names>Y</given-names></name>, <name><surname>Oron</surname><given-names>TR</given-names></name>, <name><surname>Clark</surname><given-names>WT</given-names></name>, <name><surname>Bankapur</surname><given-names>AR</given-names></name>, <name><surname>D’Andrea</surname><given-names>D</given-names></name>, <name><surname>Lepore</surname><given-names>R</given-names></name>, <etal>et al</etal>. <article-title>An expanded evaluation of protein function prediction methods shows an improvement in accuracy</article-title>. <source>Genome biology</source>. <year>2016</year>;<volume>17</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>19</lpage>.<pub-id pub-id-type="pmid">26753840</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref049">
      <label>49</label>
      <mixed-citation publication-type="journal"><name><surname>Bairoch</surname><given-names>A</given-names></name>, <name><surname>Apweiler</surname><given-names>R</given-names></name>. <article-title>The SWISS-PROT protein sequence database and its supplement TrEMBL in 2000</article-title>. <source>Nucleic acids research</source>. <year>2000</year>;<volume>28</volume>(<issue>1</issue>):<fpage>45</fpage>–<lpage>8</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/nar/28.1.45</pub-id><?supplied-pmid 10592178?><pub-id pub-id-type="pmid">10592178</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref050">
      <label>50</label>
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>W</given-names></name>, <name><surname>Godzik</surname><given-names>A</given-names></name>. <article-title>Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences</article-title>. <source>Bioinformatics</source>. <year>2006</year>;<volume>22</volume>(<issue>13</issue>):<fpage>1658</fpage>–<lpage>9</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btl158</pub-id><?supplied-pmid 16731699?><pub-id pub-id-type="pmid">16731699</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref051">
      <label>51</label>
      <mixed-citation publication-type="journal"><name><surname>Rao</surname><given-names>R</given-names></name>, <name><surname>Meier</surname><given-names>J</given-names></name>, <name><surname>Sercu</surname><given-names>T</given-names></name>, <name><surname>Ovchinnikov</surname><given-names>S</given-names></name>, <name><surname>Rives</surname><given-names>A</given-names></name>, <article-title>editors. Transformer protein language models are unsupervised structure learners</article-title>. <source>International Conference on Learning Representations</source>; <year>2020</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref052">
      <label>52</label>
      <mixed-citation publication-type="journal"><name><surname>Devlin</surname><given-names>J</given-names></name>, <name><surname>Chang</surname><given-names>M-W</given-names></name>, <name><surname>Lee</surname><given-names>K</given-names></name>, <name><surname>Toutanova</surname><given-names>K</given-names></name>. <article-title>Bert: Pre-training of deep bidirectional transformers for language understanding.</article-title><source>arXiv preprint arXiv:181004805.</source><year>2018</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref053">
      <label>53</label>
      <mixed-citation publication-type="journal"><name><surname>Buckman</surname><given-names>J</given-names></name>, <name><surname>Roy</surname><given-names>A</given-names></name>, <name><surname>Raffel</surname><given-names>C</given-names></name>, <name><surname>Goodfellow</surname><given-names>I</given-names></name>, editors. <article-title>Thermometer encoding: One hot way to resist adversarial examples.</article-title><source>International Conference on Learning Representations</source>; <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref054">
      <label>54</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>F</given-names></name>, <name><surname>Jiang</surname><given-names>M</given-names></name>, <name><surname>Qian</surname><given-names>C</given-names></name>, <name><surname>Yang</surname><given-names>S</given-names></name>, <name><surname>Li</surname><given-names>C</given-names></name>, <name><surname>Zhang</surname><given-names>H</given-names></name>, <etal>et al</etal>., <article-title>editors. Residual attention network for image classification</article-title>. <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>; <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref055">
      <label>55</label>
      <mixed-citation publication-type="journal"><name><surname>Kingma</surname><given-names>DP</given-names></name>, <name><surname>Ba</surname><given-names>J</given-names></name>. <article-title>Adam: A method for stochastic optimization.</article-title><source>arXiv preprint arXiv:14126980.</source><year>2014</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref056">
      <label>56</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>L</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Feng</surname><given-names>J</given-names></name>. <article-title>On the Euclidean distance of images</article-title>. <source>IEEE transactions on pattern analysis and machine intelligence</source>. <year>2005</year>;<volume>27</volume>(<issue>8</issue>):<fpage>1334</fpage>–<lpage>9</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TPAMI.2005.165</pub-id><?supplied-pmid 16119271?><pub-id pub-id-type="pmid">16119271</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref057">
      <label>57</label>
      <mixed-citation publication-type="journal"><name><surname>Hermans</surname><given-names>A</given-names></name>, <name><surname>Beyer</surname><given-names>L</given-names></name>, <name><surname>Leibe</surname><given-names>B</given-names></name>. <article-title>In defense of the triplet loss for person re-identification.</article-title><source>arXiv preprint arXiv:170307737.</source><year>2017</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref058">
      <label>58</label>
      <mixed-citation publication-type="journal"><name><surname>Hoffer</surname><given-names>E</given-names></name>, <name><surname>Ailon</surname><given-names>N</given-names></name>. <article-title>Deep metric learning using triplet network.</article-title><source>International workshop on similarity-based pattern recognition.</source><year>2015</year>:<fpage>84</fpage>–<lpage>92</lpage>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref059">
      <label>59</label>
      <mixed-citation publication-type="journal"><name><surname>Taha</surname><given-names>A</given-names></name>, <name><surname>Chen</surname><given-names>Y-T</given-names></name>, <name><surname>Misu</surname><given-names>T</given-names></name>, <name><surname>Shrivastava</surname><given-names>A</given-names></name>, <name><surname>Davis</surname><given-names>L</given-names></name>. <article-title>Boosting standard classification architectures through a ranking regularizer</article-title>. <source>the IEEE/CVF Winter Conference on Applications of Computer Vision</source><year>2020</year>. p. <fpage>758</fpage>–<lpage>66</lpage>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref060">
      <label>60</label>
      <mixed-citation publication-type="journal"><name><surname>Zhou</surname><given-names>Q</given-names></name>, <name><surname>Zhong</surname><given-names>B</given-names></name>, <name><surname>Lan</surname><given-names>X</given-names></name>, <name><surname>Sun</surname><given-names>G</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Zhang</surname><given-names>B</given-names></name>, <etal>et al</etal>. <article-title>Fine-grained spatial alignment model for person re-identification with focal triplet loss</article-title>. <source>IEEE Transactions on Image Processing</source>. <year>2020</year>;<volume>29</volume>:<fpage>7578</fpage>–<lpage>89</lpage>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010793.ref061">
      <label>61</label>
      <mixed-citation publication-type="journal"><name><surname>Memon</surname><given-names>SA</given-names></name>, <name><surname>Khan</surname><given-names>KA</given-names></name>, <name><surname>Naveed</surname><given-names>H</given-names></name>. <article-title>HECNet: a hierarchical approach to enzyme function classification using a Siamese Triplet Network</article-title>. <source>Bioinformatics</source>. <year>2020</year>;<volume>36</volume>(<issue>17</issue>):<fpage>4583</fpage>–<lpage>9</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btaa536</pub-id> .<?supplied-pmid 32449765?><pub-id pub-id-type="pmid">32449765</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<sub-article article-type="aggregated-review-documents" id="pcbi.1010793.r001" specific-use="decision-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1010793.r001</article-id>
    <title-group>
      <article-title>Decision Letter 0</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Friedberg</surname>
          <given-names>Iddo</given-names>
        </name>
        <role>Guest Editor</role>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ben-Tal</surname>
          <given-names>Nir</given-names>
        </name>
        <role>Section Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2022 Friedberg, Ben-Tal</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Friedberg, Ben-Tal</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1010793" id="rel-obj001" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>0</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">7 Sep 2022</named-content>
    </p>
    <p>Dear Dr. Zhang,</p>
    <p>Thank you very much for submitting your manuscript "Integrating Self-Attention Transformer with Triplet Neural Networks for Protein Gene Ontology Prediction" for consideration at PLOS Computational Biology.</p>
    <p>As with all papers reviewed by the journal, your manuscript was reviewed by members of the editorial board and by several independent reviewers. In light of the reviews (below this email), we would like to invite the resubmission of a significantly-revised version that takes into account the reviewers' comments.</p>
    <p>Please address the following comments from the reviewers:</p>
    <p>Reviewer #1: please address all comments, note that the second comment suggests retraining using different embedding methods. However, that is out of scope of this work, in my opinion, so no need to do that.</p>
    <p>Reviewer #3:</p>
    <p>Comment 1: no need to address directly, however, if appropriate please cite the citations suggested by the reviewer.</p>
    <p>Comment 2: Please address this comment regarding the suitability of F1 as a learning metric.</p>
    <p>Comment 3-5: Please address</p>
    <p>Minor comment 1: no need to address that.</p>
    <p>We cannot make any decision about publication until we have seen the revised manuscript and your response to the reviewers' comments. Your revised manuscript is also likely to be sent to reviewers for further evaluation.</p>
    <p>When you are ready to resubmit, please upload the following:</p>
    <p>[1] A letter containing a detailed list of your responses to the review comments and a description of the changes you have made in the manuscript. Please note while forming your response, if your article is accepted, you may have the opportunity to make the peer review history publicly available. The record will include editor decision letters (with reviews) and your responses to reviewer comments. If eligible, we will contact you to opt in or out.</p>
    <p>[2] Two versions of the revised manuscript: one with either highlights or tracked changes denoting where the text has been changed; the other a clean version (uploaded as the manuscript file).</p>
    <p>Important additional instructions are given below your reviewer comments.</p>
    <p>Please prepare and submit your revised manuscript within 60 days. If you anticipate any delay, please let us know the expected resubmission date by replying to this email. Please note that revised manuscripts received after the 60-day due date may require evaluation and peer review similar to newly submitted manuscripts.</p>
    <p>Thank you again for your submission. We hope that our editorial process has been constructive so far, and we welcome your feedback at any time. Please don't hesitate to contact us if you have any questions or comments.</p>
    <p>Sincerely,</p>
    <p>Iddo Friedberg, Ph.D.</p>
    <p>Guest Editor</p>
    <p>PLOS Computational Biology</p>
    <p>Nir Ben-Tal</p>
    <p>Section Editor</p>
    <p>PLOS Computational Biology</p>
    <p>***********************</p>
    <p>Please address the following comments from the reviewers:</p>
    <p>Reviewer #1: please address all comments, note that the second comment suggests retraining using different embedding methods. However, that is out of scope of this work, in my opinion, so no need to do that.</p>
    <p>Reviewer #3:</p>
    <p>Comment 1: no need to address directly, however, if appropriate please cite the citations suggested by the reviewer.</p>
    <p>Comment 2: Please address this comment regarding the suitability of F1 as a learning metric.</p>
    <p>Comment 3-5: Please address</p>
    <p>Minor comment 1: no need to address that.</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <bold>Comments to the Authors:</bold>
    </p>
    <p>
      <bold>Please note here if the review is uploaded as an attachment.</bold>
    </p>
    <p>Reviewer #1: This work presents a new protein function prediction approach and software. The main idea is to exploit pre-trained language models on protein sequences and use them as a basis for downstream tasks of GO term prediction. The paper is clear and well written. The architecture makes sense, the loss function for training is interesting, and the results were thorough and favor their method. Ultimately, the authors find a need to specifically combine homology-based models with network-based models.</p>
    <p>I have some questions and concerns, mostly minor:</p>
    <p>* The paper works off of the ESM model, however, there are other embedding systems including ProtTrans (<ext-link xlink:href="https://arxiv.org/abs/2007.06225" ext-link-type="uri">https://arxiv.org/abs/2007.06225</ext-link>), TAPE (<ext-link xlink:href="https://arxiv.org/abs/1906.08230" ext-link-type="uri">https://arxiv.org/abs/1906.08230</ext-link>), and Bepler &amp; Berger's approach (<ext-link xlink:href="https://arxiv.org/abs/1902.08661" ext-link-type="uri">https://arxiv.org/abs/1902.08661</ext-link>). Some mention or comments on the value of those systems would be important to make. The ProtTrans paper find their models to be superior to ESM-1b. If so, this could propagate to this paper and further improvements might be possible.</p>
    <p>* The ablative models showed interesting performance in that M1 obtained the highest improvement in accuracy and the addition of other components only had marginal effect, particularly in MF though the trend is the same in BO and CC (Figure 5). Would other embedding methods show similar performance gain or there is something special with ESM-1b?</p>
    <p>* Is there some interpretation in that most gain comes from the embeddings for MF. This suggest that embeddings contain some sequence signal. Have the authors tried to understand this aspect?</p>
    <p>* The paper is written by mixing past and present tense. The authors can decide to use one. For example, "...AGTO+ achieved a further improvement overall three categories of GO terms. It also significantly outperforms..." is an example of mixing tenses. Also, "overall" -&gt; "over all"</p>
    <p>* life mechanism -&gt; life mechanisms (2 places)</p>
    <p>Reviewer #2: This manuscript reports a new deep learning method to predict the function (GO terms) of proteins. The MSA embedding techniques based on self-attend are first applied to this problem. The method performs better than both control methods and other state-of-the-art methods on the CAFA3 benchmark. Therefore, the work makes valuable contributions to the field. There is some minor issue regarding the deep learning architecture. It is not clear why three embedding components (triplet networks) are used. Some rationale should be provided to justify the design of the architecture.</p>
    <p>Reviewer #3: The authors propose deep learning model (ATGO) for automatically assigning GO terms to protein sequences. The model uses a pre-trained language model to extract three different protein embedding vectors which are further combined into one vector using a neural network. A triplet loss is used to enforce that proteins with similar functions are close in the embedding space.</p>
    <p>The model and a combination of the model with homology searching are compared to several other well-known function prediction methods on two datasets.</p>
    <p>###################################################################################</p>
    <p>Major comments</p>
    <p>1) A main conclusion of the study is that the use of a pre-trained protein language model is by far the main source of performance for this model. However, the superiority of these language models with respect to supervised learners such as CNNs has already been extensively demonstrated (also for the CAFA3 dataset) in the past. See Rao et al. Adv Neural Inf Process Syst. 2019, Littmann et al. Scientific Reports, 2021, Villegas-Morcillo et al., Bioinformatics (2021).</p>
    <p>2) The idea of using metric learning to learn an embedding space that reflects structural similarity is interesting (although it has also been done before for molecular function prediction for enzymes – Aman Memon et al., Bioinformatics, 2020). However, the authors here calculate the F1 score between the annotations of two proteins and pairs with F1 larger than a threshold are considered positive. But the F1 score is not a proper distance metric (e.g. doesn’t satisfy triangle inequality) and this might hamper the performance of the triplet loss. See Powers Technical report KIT-14-001 for more information on the F1 score. Perhaps a Jaccard similarity (even weighted by term information content?) might be a more suitable metric to use for metric learning.</p>
    <p>3) The conclusion that combining ATGO with sequence homology boosts performance is not sufficiently supported. First, by looking at Tables 1 and 2, the performance improvement is in most cases &lt;0.01 and the coverage of ATGO is already 100%. Figure 6 does show that for one protein there is a BLAST hit that helps detect some functions that were missed by the network, but looking at the numbers, this seems to be a rare occasion. So given the very small gain and that for some proteins ATGO+ can do worse than ATGO (top of Fig 6), the conclusion that information fusion leads to better performance for ATGO is not valid I think.</p>
    <p>4) It is a bit strange that the second best method in the CAFA experiment is the blast-like baseline, although several methods that participated in CAFA3 and that were developed later (e.g. DeepGOPLus) perform considerably better than the BLAST baseline in this dataset.</p>
    <p>5) The p-value calculations are a little problematic. First, it is not clear how the authors controlled for multiple comparisons. Additionally they don’t compare all against all, but rather ATGO against all and ATGO+ against all, but not ATGO against ATGO+ (this is related to comment 2 as well). Other comparisons such as between SAGP and PPIGP are done implicitly in the manuscript without the proper statistical test. Ideally the authors should use analysis of variants to identify whether one or multiple methods is significantly better than the rest and then use appropriate post-hoc tests to perform the pairwise comparisons.</p>
    <p>###################################################################################</p>
    <p>Minor comments</p>
    <p>1) It would be nice to include an information theory-based evaluation metric such as semantic distance or IC-weighted Fmax. The number of positive examples does not always mean that the term is difficult to predict, depending on how many ‘sibling’ terms exist</p>
    <p>2) I like the visualization of the different predictions showed in Fig 6.</p>
    <p>**********</p>
    <p>
      <bold>Have the authors made all data and (if applicable) computational code underlying the findings in their manuscript fully available?</bold>
    </p>
    <p>The <ext-link xlink:href="https://journals.plos.org/ploscompbiol/s/materials-and-software-sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data and code underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data and code should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data or code —e.g. participant privacy or use of data from a third party—those must be specified.</p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>Reviewer #3: Yes</p>
    <p>**********</p>
    <p>PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/ploscompbiol/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.</p>
    <p>Reviewer #1: No</p>
    <p>Reviewer #2: No</p>
    <p>Reviewer #3: No</p>
    <p>
      <underline>Figure Files:</underline>
    </p>
    <p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <underline><ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com</ext-link></underline>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email us at <underline><email>figures@plos.org</email></underline>.</p>
    <p>
      <underline>Data Requirements:</underline>
    </p>
    <p>Please note that, as a condition of publication, PLOS' data policy requires that you make available all data used to draw the conclusions outlined in your manuscript. Data must be deposited in an appropriate repository, included within the body of the manuscript, or uploaded as supporting information. This includes all numerical values that were used to generate graphs, histograms etc.. For an example in PLOS Biology see here: <ext-link xlink:href="http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5" ext-link-type="uri">http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5</ext-link>.</p>
    <p>
      <underline>Reproducibility:</underline>
    </p>
    <p>To enhance the reproducibility of your results, we recommend that you deposit your laboratory protocols in protocols.io, where a protocol can be assigned its own identifier (DOI) such that it can be cited independently in the future. Additionally, PLOS ONE offers an option to publish peer-reviewed clinical study protocols. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</ext-link></p>
  </body>
</sub-article>
<sub-article article-type="author-comment" id="pcbi.1010793.r002">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1010793.r002</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 0</article-title>
    </title-group>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1010793" id="rel-obj002" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">26 Oct 2022</named-content>
    </p>
    <supplementary-material id="pcbi.1010793.s031" position="float" content-type="local-data">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">response_to_reviewers.pdf</named-content></p>
      </caption>
      <media xlink:href="pcbi.1010793.s031.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article article-type="aggregated-review-documents" id="pcbi.1010793.r003" specific-use="decision-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1010793.r003</article-id>
    <title-group>
      <article-title>Decision Letter 1</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Friedberg</surname>
          <given-names>Iddo</given-names>
        </name>
        <role>Guest Editor</role>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ben-Tal</surname>
          <given-names>Nir</given-names>
        </name>
        <role>Section Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2022 Friedberg, Ben-Tal</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Friedberg, Ben-Tal</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1010793" id="rel-obj003" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">5 Dec 2022</named-content>
    </p>
    <p>Dear Dr. Zhang,</p>
    <p>We are pleased to inform you that your manuscript 'Integrating Unsupervised Language Model with Triplet Neural Networks for Protein Gene Ontology Prediction' has been provisionally accepted for publication in PLOS Computational Biology.</p>
    <p>Before your manuscript can be formally accepted you will need to complete some formatting changes, which you will receive in a follow up email. A member of our team will be in touch with a set of requests. At this point, please also pay special attention to the typographical errors as pointed out by the reviewers.</p>
    <p>Please note that your manuscript will not be scheduled for publication until you have made the required changes, so a swift response is appreciated.</p>
    <p>IMPORTANT: The editorial review process is now complete. PLOS will only permit corrections to spelling, formatting or significant scientific errors from this point onwards. Requests for major changes, or any which affect the scientific understanding of your work, will cause delays to the publication date of your manuscript.</p>
    <p>Should you, your institution's press office or the journal office choose to press release your paper, you will automatically be opted out of early publication. We ask that you notify us now if you or your institution is planning to press release the article. All press must be co-ordinated with PLOS.</p>
    <p>Thank you again for supporting Open Access publishing; we are looking forward to publishing your work in PLOS Computational Biology. </p>
    <p>Best regards,</p>
    <p>Iddo Friedberg, Ph.D.</p>
    <p>Guest Editor</p>
    <p>PLOS Computational Biology</p>
    <p>Nir Ben-Tal</p>
    <p>Section Editor</p>
    <p>PLOS Computational Biology</p>
    <p>***********************************************************</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <bold>Comments to the Authors:</bold>
    </p>
    <p>
      <bold>Please note here if the review is uploaded as an attachment.</bold>
    </p>
    <p>Reviewer #1: The authors have responded well to the criticism. I notice a few more lingering typos and the authors could use my suggestions below to further clean up the paper.</p>
    <p>Summary:</p>
    <p>"all gene and gene products" -&gt; "all genes and gene products"</p>
    <p>Introduction:</p>
    <p>"we utilized a unsupervised" -&gt; "we utilized an unsupervised"</p>
    <p>Results:</p>
    <p>"Fmax" -&gt; max should be in the subscript (potentially also 1 for F1)</p>
    <p>"we perform Friedman test" and then "Thus, a Nemenyi post-hoc test" shortly after. As far as I understand, this should be the same test, sometimes referred to as Friedman-Nemenyi.</p>
    <p>"did not remove homologous" -&gt; "did not remove homologs"</p>
    <p>"designed as following" -&gt; "designed as follows"</p>
    <p>"with Sigmoid activation function" -&gt; "with sigmoid activation function"</p>
    <p>Reviewer #2: My comments have been addressed.</p>
    <p>Reviewer #3: The authors have addressed all my comments.</p>
    <p>Small typo at the top of page 6, 'analysis of variants' should be 'analysis of variance'</p>
    <p>**********</p>
    <p>
      <bold>Have the authors made all data and (if applicable) computational code underlying the findings in their manuscript fully available?</bold>
    </p>
    <p>The <ext-link xlink:href="https://journals.plos.org/ploscompbiol/s/materials-and-software-sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data and code underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data and code should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data or code —e.g. participant privacy or use of data from a third party—those must be specified.</p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>Reviewer #3: Yes</p>
    <p>**********</p>
    <p>PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/ploscompbiol/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.</p>
    <p>Reviewer #1: No</p>
    <p>Reviewer #2: No</p>
    <p>Reviewer #3: No</p>
  </body>
</sub-article>
<sub-article article-type="editor-report" id="pcbi.1010793.r004" specific-use="acceptance-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1010793.r004</article-id>
    <title-group>
      <article-title>Acceptance letter</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Friedberg</surname>
          <given-names>Iddo</given-names>
        </name>
        <role>Guest Editor</role>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ben-Tal</surname>
          <given-names>Nir</given-names>
        </name>
        <role>Section Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2022 Friedberg, Ben-Tal</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Friedberg, Ben-Tal</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1010793" id="rel-obj004" related-article-type="reviewed-article"/>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">16 Dec 2022</named-content>
    </p>
    <p>PCOMPBIOL-D-22-01039R1 </p>
    <p>Integrating Unsupervised Language Model with Triplet Neural Networks for Protein Gene Ontology Prediction</p>
    <p>Dear Dr Zhang,</p>
    <p>I am pleased to inform you that your manuscript has been formally accepted for publication in PLOS Computational Biology. Your manuscript is now with our production department and you will be notified of the publication date in due course.</p>
    <p>The corresponding author will soon be receiving a typeset proof for review, to ensure errors have not been introduced during production. Please review the PDF proof of your manuscript carefully, as this is the last chance to correct any errors. Please note that major changes, or those which affect the scientific understanding of the work, will likely cause delays to the publication date of your manuscript. </p>
    <p>Soon after your final files are uploaded, unless you have opted out, the early version of your manuscript will be published online. The date of the early version will be your article's publication date. The final article will be published to the same URL, and all versions of the paper will be accessible to readers.</p>
    <p>Thank you again for supporting PLOS Computational Biology and open-access publishing. We are looking forward to publishing your work! </p>
    <p>With kind regards,</p>
    <p>Anita Estes</p>
    <p>PLOS Computational Biology | Carlyle House, Carlyle Road, Cambridge CB4 3DN | United Kingdom <email>ploscompbiol@plos.org</email> | Phone +44 (0) 1223-442824 | <ext-link xlink:href="http://ploscompbiol.org" ext-link-type="uri">ploscompbiol.org</ext-link> | @PLOSCompBiol</p>
  </body>
</sub-article>
