<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6853675</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btz293</article-id>
    <article-id pub-id-type="publisher-id">btz293</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Structural Bioinformatics</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>A decision-theoretic approach to the evaluation of machine learning algorithms in computational drug discovery</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Watson</surname>
          <given-names>Oliver P</given-names>
        </name>
        <xref ref-type="aff" rid="btz293-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cortes-Ciriano</surname>
          <given-names>Isidro</given-names>
        </name>
        <xref ref-type="aff" rid="btz293-aff1">1</xref>
        <xref ref-type="aff" rid="btz293-aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Taylor</surname>
          <given-names>Aimee R</given-names>
        </name>
        <xref ref-type="aff" rid="btz293-aff3">3</xref>
        <xref ref-type="aff" rid="btz293-aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-5524-0325</contrib-id>
        <name>
          <surname>Watson</surname>
          <given-names>James A</given-names>
        </name>
        <xref ref-type="aff" rid="btz293-aff5">5</xref>
        <xref ref-type="aff" rid="btz293-aff6">6</xref>
        <xref ref-type="corresp" rid="btz293-cor1"/>
        <!--<email>james@tropmedres.ac</email>-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Valencia</surname>
          <given-names>Alfonso</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <aff id="btz293-aff1"><label>1</label><institution>Goring on Thames, Evariste Technologies Ltd.</institution>, RG8 9AL UK</aff>
    <aff id="btz293-aff2"><label>2</label><institution>Department of Chemistry, Centre for Molecular Science Informatics, University of Cambridge</institution>, Lensfield Road, Cambridge CB2 1EW, UK</aff>
    <aff id="btz293-aff3"><label>3</label><institution>Department of Epidemiology, Center for Communicable Disease Dynamics, Harvard T.H. Chan School of Public Health</institution>, Boston, MA 02115 USA</aff>
    <aff id="btz293-aff4"><label>4</label><institution>Infectious Disease Microbiome Program, Broad Institute</institution>, Cambridge, MA 02142 USA</aff>
    <aff id="btz293-aff5"><label>5</label><institution>Nuffield Department of Medicine, Centre for Tropical Medicine and Global Health, University of Oxford</institution>, Oxford OX3, 7LF UK</aff>
    <aff id="btz293-aff6"><label>6</label><institution>Mahidol-Oxford Tropical Medicine Research Unit, Faculty of Tropical Medicine, Mahidol University</institution>, Bangkok 10400, Thailand</aff>
    <author-notes>
      <corresp id="btz293-cor1">To whom correspondence should be addressed. E-mail: <email>james@tropmedres.ac</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <day>15</day>
      <month>11</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2019-05-09">
      <day>09</day>
      <month>5</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>09</day>
      <month>5</month>
      <year>2019</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>35</volume>
    <issue>22</issue>
    <fpage>4656</fpage>
    <lpage>4663</lpage>
    <history>
      <date date-type="received">
        <day>10</day>
        <month>12</month>
        <year>2018</year>
      </date>
      <date date-type="rev-recd">
        <day>22</day>
        <month>3</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>17</day>
        <month>4</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="cc-by" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btz293.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Artificial intelligence, trained via machine learning (e.g. neural nets, random forests) or computational statistical algorithms (e.g. support vector machines, ridge regression), holds much promise for the improvement of small-molecule drug discovery. However, small-molecule structure-activity data are high dimensional with low signal-to-noise ratios and proper validation of predictive methods is difficult. It is poorly understood which, if any, of the currently available machine learning algorithms will best predict new candidate drugs.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>The quantile-activity bootstrap is proposed as a new model validation framework using quantile splits on the activity distribution function to construct training and testing sets. In addition, we propose two novel rank-based loss functions which penalize only the out-of-sample predicted ranks of high-activity molecules. The combination of these methods was used to assess the performance of neural nets, random forests, support vector machines (regression) and ridge regression applied to 25 diverse high-quality structure-activity datasets publicly available on ChEMBL. Model validation based on random partitioning of available data favours models that overfit and ‘memorize’ the training set, namely random forests and deep neural nets. Partitioning based on quantiles of the activity distribution correctly penalizes extrapolation of models onto structurally different molecules outside of the training data. Simpler, traditional statistical methods such as ridge regression can outperform state-of-the-art machine learning methods in this setting. In addition, our new rank-based loss functions give considerably different results from mean squared error highlighting the necessity to define model optimality with respect to the decision task at hand.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>All software and data are available as Jupyter notebooks found at <ext-link ext-link-type="uri" xlink:href="https://github.com/owatson/QuantileBootstrap">https://github.com/owatson/QuantileBootstrap</ext-link>.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Empirical methodologies guide a significant proportion of early-stage small-molecule drug discovery (<xref rid="btz293-B9" ref-type="bibr">Cumming <italic>et al.</italic>, 2013</xref>; <xref rid="btz293-B21" ref-type="bibr">Keiser <italic>et al.</italic>, 2007</xref>; <xref rid="btz293-B24" ref-type="bibr">Lipinski, 2004</xref>). These range from simple rule-based methods (Lipinski’s rule of 5), to searching over molecules ‘similar’ to those already known, to using more complex regression models. This work concerns the objective evaluation of the predictive ability of the latter, namely statistical and machine learning regression models trained on molecular structure-activity data. The goal of these models is to characterize the relationship between a high-dimensional binary vector representation of small molecules (known as a molecular fingerprint) and the corresponding target specific <italic>in vitro</italic> activities. In this context, use of regression modelling is often known as quantitative structure-activity relationship modelling (QSAR) (<xref rid="btz293-B33" ref-type="bibr">Sliwoski <italic>et al.</italic>, 2014</xref>; <xref rid="btz293-B37" ref-type="bibr">Van De Waterbeemd and Gifford, 2003</xref>), and many different model classes have been used: support vector machines (<xref rid="btz293-B5" ref-type="bibr">Burbidge <italic>et al.</italic>, 2001</xref>), ridge regression (<xref rid="btz293-B28" ref-type="bibr">Nandi <italic>et al.</italic>, 2007</xref>), neural nets <xref rid="btz293-B1" ref-type="bibr">(Ajay <italic>et al.</italic>, 1998</xref>; <xref rid="btz293-B23" ref-type="bibr">Lenselink <italic>et al.</italic>, 2017</xref>; <xref rid="btz293-B28" ref-type="bibr">Nandi <italic>et al.</italic>, 2007</xref>; <xref rid="btz293-B30" ref-type="bibr">Sadowski and Kubinyi, 1998</xref>) and random forests (<xref rid="btz293-B36" ref-type="bibr">Svetnik <italic>et al.</italic>, 2003</xref>), to name but a few. The success of these models is in part due to high-throughput screening experiments which produce large structure-activity datasets (order of magnitude 10<sup>2</sup>–10<sup>6</sup> datapoints).</p>
    <p>Regression with high-dimensional bioinformatic data is known to be difficult. Problems include <italic>the curse of dimensionality</italic>, optimization bias, reporting bias and low signal-to-noise ratios (<xref rid="btz293-B2" ref-type="bibr">Boulesteix and Strobl, 2009</xref>; <xref rid="btz293-B6" ref-type="bibr">Castaldi <italic>et al.</italic>, 2011</xref>; <xref rid="btz293-B17" ref-type="bibr">Ioannidis, 2005</xref>; <xref rid="btz293-B18" ref-type="bibr">Jelizarow <italic>et al.</italic>, 2010</xref>; <xref rid="btz293-B26" ref-type="bibr">Matveeva <italic>et al.</italic>, 2016</xref>; <xref rid="btz293-B34" ref-type="bibr">Somorjai <italic>et al.</italic>, 2003</xref>; <xref rid="btz293-B41" ref-type="bibr">Zervakis <italic>et al.</italic>, 2009</xref>). A major theoretical framework underpinning the use and interpretation of computational methods for complex data modalities is cross-validation (<xref rid="btz293-B15" ref-type="bibr">Geisser, 1975</xref>; <xref rid="btz293-B35" ref-type="bibr">Stone, 1974</xref>), which provides an estimate of the predictive error rate (<xref rid="btz293-B6" ref-type="bibr">Castaldi <italic>et al.</italic>, 2011</xref>; <xref rid="btz293-B27" ref-type="bibr">Molinaro <italic>et al.</italic>, 2005</xref>). However, validation strategies based on random partitioning of datasets, either by <italic>K</italic>-fold cross-validation or the bootstrap, are known to be optimistic for structure-activity modelling (<xref rid="btz293-B32" ref-type="bibr">Sheridan, 2013</xref>; <xref rid="btz293-B38" ref-type="bibr">Wallach and Heifets, 2018</xref>; <xref rid="btz293-B40" ref-type="bibr">Wu <italic>et al.</italic>, 2018</xref>). Multiple alternative strategies have been proposed, for example, splitting by date of assay (<xref rid="btz293-B32" ref-type="bibr">Sheridan, 2013</xref>), constructing local neighbourhoods based on similarity scores or scaffold splitting (<xref rid="btz293-B32" ref-type="bibr">Sheridan, 2013</xref>; <xref rid="btz293-B40" ref-type="bibr">Wu <italic>et al.</italic>, 2018</xref>), or stratified sampling whereby equal distributions of the activity levels are assured across training and testing sets (<xref rid="btz293-B40" ref-type="bibr">Wu <italic>et al.</italic>, 2018</xref>). These alternative strategies can suffer from the same issues as standard cross-validation or rely on strong data assumptions. A better general approach is needed.</p>
  </sec>
  <sec>
    <title>2 Approach</title>
    <p>This work reiterates that standard validation approaches—<italic>K</italic>-fold cross-validation and the bootstrap—based on random partitioning of available data, will not target the true predictive model error in the context of small-molecule drug discovery. We give a theoretical justification for this claim and show it empirically using 25 publicly available datasets. We propose a simple alternative partitioning method—the quantile-activity bootstrap—which splits datasets on quantiles of the activity distribution function. This univariate parametrization of the training set construction allows for inference on the predictive ability of different regression methods in the limit: as information in the training set is reduced to almost zero. In addition, we argue that out-of-sample model performance should be evaluated from a decision-theoretic perspective (<xref rid="btz293-B31" ref-type="bibr">Savage, 1954</xref>) using loss functions, which reflect as best possible the process of drug discovery. Tailor-made loss functions will better determine truly optimal model classes compared with standard goodness-of-fit metrics. We propose simple rank-based loss functions to evaluate out-of-sample model prediction accuracy. We show that in these low signal-to-noise settings (<xref rid="btz293-B7" ref-type="bibr">Cortés-Ciriano and Bender, 2016</xref>; <xref rid="btz293-B19" ref-type="bibr">Kalliokoski <italic>et al.</italic>, 2013a</xref>, <xref rid="btz293-B20" ref-type="bibr">b</xref>; <xref rid="btz293-B22" ref-type="bibr">Kramer <italic>et al.</italic>, 2012</xref>), models with greater structural constraints (ridge regression and linear kernel support vector regression) outperform less constrained machine learning algorithms (neural nets and random forests).</p>
  </sec>
  <sec>
    <title>3 Materials and Methods</title>
    <sec>
      <title>3.1 Cross-validation with biased data</title>
      <sec>
        <title>3.1.1 Problem setting</title>
        <p>This section outlines the formal framework and notation we use throughout the article. We consider the general problem of comparing the performance of multiple predictive models (statistical and machine learning) with respect to a given dataset. ‘Optimality’ of these predictive models is evaluated with respect to a subjective loss function.</p>
        <p>The context investigated here is finding ‘active’ molecules within molecular space. ‘Active’ is defined as having activity level above a given threshold. This activity is target specific. Conditional on a given initial dataset, the overall loss (negative utility of the model) is defined as a function of the number of new molecules needed to be tested until an active molecule is reached.</p>
        <p>Each molecule is represented by its ‘molecular fingerprint’, a <italic>P</italic>-dimensional binary vector. We denote this as <inline-formula id="IE1"><mml:math id="IM1"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, where <italic>i</italic> indexes the molecule and <italic>j</italic> indexes the feature (as referred to in the machine learning literature) or covariate (statistics literature). We have <italic>P </italic>=<italic> </italic>128 for the fingerprint representation used in this analysis. Each molecule <italic>x<sub>i</sub></italic> has a target specific activity <italic>y<sub>i</sub></italic> which corresponds to the negative logarithmic <italic>in vitro</italic> half-inhibitory concentration (p-IC<sub>50</sub>: higher values correspond to increased activity). In this section, we ignore the target specificity as each dataset has an associated target and the datasets are analysed independently. We do not consider multi-objective regression models here. We denote the (unknown) functional relationship between the fingerprint and the outcome (activity) as <inline-formula id="IE2"><mml:math id="IM2"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>ϵ</mml:mo></mml:mrow></mml:math></inline-formula>, where <italic>ϵ</italic> is experimental error (general regression framework).</p>
        <p>Given a choice of models <inline-formula id="IE3"><mml:math id="IM3"><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, respective performances are commonly evaluated using <italic>K</italic>-fold cross-validation (<xref rid="btz293-B27" ref-type="bibr">Molinaro <italic>et al.</italic>, 2005</xref>; <xref rid="btz293-B35" ref-type="bibr">Stone, 1974</xref>) [detailed description given in <xref rid="btz293-B14" ref-type="bibr">Friedman <italic>et al.</italic> (2001)</xref>, Chapter 7], or the bootstrap (<xref rid="btz293-B11" ref-type="bibr">Efron, 1983</xref>) which is closely related. Standard <italic>K</italic>-fold cross-validation proceeds by dividing the data <inline-formula id="IE4"><mml:math id="IM4"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> into a partition of <italic>K </italic>&gt;<italic> </italic>1 equally sized subsets <inline-formula id="IE5"><mml:math id="IM5"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. For the <italic>k</italic>th subset, we train (fit) our model <italic>M<sub>t</sub></italic> using the data <inline-formula id="IE6"><mml:math id="IM6"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mtext>train</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∪</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>≠</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The out-of-sample expected loss is then estimated by testing on elements in <italic>S<sub>k</sub></italic>: <inline-formula id="IE7"><mml:math id="IM7"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mtext>train</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The overall expected loss estimate is <inline-formula id="IE8"><mml:math id="IM8"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The notation for the expected loss over each test set is deliberately not summed over the indices of the testing data as this article considers non-additive loss functions, e.g. aggregate functions of the testing data. The choice of the number of folds <italic>K</italic> is context dependent and relates to a bias-variance trade-off: smaller <italic>K</italic> implies a smaller training set and thus increased positive bias in the error rate estimate, however, smaller <italic>K</italic> also forces greater dissimilarity between the training sets and thus lowers variance in the overall error estimate. The bootstrap is similar to 3-fold cross-validation, whereby approximately two-thirds of the data are used in the training set taken as a bootstrap sample of size <italic>N</italic>, constructed by bootstrapping (sampling with replacement). Predictive error estimation is then done by averaging the out-of-bag errors. The bootstrap generally improves on standard <italic>K</italic>-fold cross-validation as it smooths the predictive error when using discontinuous loss functions.</p>
        <p><italic>K</italic>-fold cross-validation and the bootstrap provide nearly unbiased estimators of the conditional expected loss if the empirical distribution <inline-formula id="IE9"><mml:math id="IM9"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> (in this context <italic>x</italic> denotes a molecule) is an i.i.d. draw from the true underlying data-generating distribution (<xref rid="btz293-B10" ref-type="bibr">Devroye <italic>et al.</italic>, 1996</xref>). In applications where the goal is to accurately predict the outcome of new data drawn at random with respect to a given data-generating process, these are the correct methods for selecting an optimal predictive model. However, drug (lead candidate) discovery is better thought of as a complex optimization problem rather than a passive data prediction problem. The goal here is to generalize (extrapolate) from a model trained on a relatively small dataset to find active molecules in a high-dimensional space (<inline-formula id="IE10"><mml:math id="IM10"><mml:mrow><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> possibilities in total).</p>
        <p>The data-generating distribution (e.g. the underlying processes which gave rise to the data at hand: this can be thought of as the experimental protocols which lead to the data-generating assays) will be substantially different from the uniform distribution over the subset of feasible molecules within the <inline-formula id="IE11"><mml:math id="IM11"><mml:mrow><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> possibilities (<xref rid="btz293-B38" ref-type="bibr">Wallach and Heifets, 2018</xref>). This subset of feasible molecules is unknown and extra modelling procedures are needed to approximate it (<xref rid="btz293-B12" ref-type="bibr">Firth <italic>et al.</italic>, 2015</xref>). Validation methods based on random partitioning of available data give biased estimates of the true out-of-sample loss (<xref rid="btz293-B3" ref-type="bibr">Braga-Neto <italic>et al.</italic>, 2014</xref>; <xref rid="btz293-B39" ref-type="bibr">Wood <italic>et al.</italic>, 2007</xref>). For example, the data might be clustered together (with respect to Manhattan distance over the space of fingerprints) and therefore the out-of-sample estimate may in fact be highly skewed towards the in-sample estimate, leading to overconfidence. Therefore, it is necessary to partition the data in such a way that the out-of-sample testing subset is truly distinct from the in-sample data. We argue here that ‘distinct’ may not exactly map onto chemical dissimilarity measures but should be defined with respect to the outcome of interest. In this way, the partition should reflect the decision problem at hand and give reliable expected loss estimates which do not favour models that overfit to the training data. We next describe non-random data partitions which create ‘distinct’ training and testing sets motivated from a decision-theoretic perspective.</p>
      </sec>
      <sec>
        <title>3.1.2 Activity dependent model validation</title>
        <p>In theory, it would be possible to determine whether a given training set is ‘close’ to a test set using a similarity metric on the molecular fingerprint space. In this context, ‘close’ is relative to the metric of choice. Metrics such as the Manhattan distance may be a poor proxy of this true (target specific) distance between subsets of data. Instead, we propose using the observed outcome (activity) <italic>y</italic> as the discriminant measure between molecules. Data partitions based on the activity function <italic>G</italic> (function relating the molecular fingerprint to the p-IC<sub>50</sub>) instead of random partitions force dis-similarities between subsets in the partition. If <inline-formula id="IE12"><mml:math id="IM12"><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>≫</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, we assume that <italic>x</italic><sub>1</sub> is experimentally significantly different from <italic>x</italic><sub>2</sub>.</p>
        <p>The following validation design is proposed. Let <inline-formula id="IE13"><mml:math id="IM13"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> be the empirical distribution over the activities <inline-formula id="IE14"><mml:math id="IM14"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. Let <inline-formula id="IE15"><mml:math id="IM15"><mml:mrow><mml:mi>q</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> be a fixed fraction of the data used to determine the training set. With respect to the empirical distribution <inline-formula id="IE16"><mml:math id="IM16"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, this maps onto an activity threshold <italic>Y<sub>q</sub></italic> (the <italic>q</italic>th quantile of <inline-formula id="IE17"><mml:math id="IM17"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>). The training set is then constructed by bootstrapping the molecules with activity less than <italic>Y<sub>q</sub></italic>. The testing set contains all the molecules with activity greater than <italic>Y<sub>q</sub></italic>. This is the opposite of standard balanced or stratified cross-validation where one assures equal distributions of outcomes across the testing folds (<xref rid="btz293-B4" ref-type="bibr">Breiman <italic>et al.</italic>, 1984</xref>) and is not a ‘cross-validation’ design as the test data are never used as training data.</p>
        <p>Multiple bootstrapped iterations are then computed in order to construct confidence intervals around the out-of-sample expected loss estimate. This can be thought of as a stabilizing process within the validation procedure (<xref rid="btz293-B11" ref-type="bibr">Efron, 1983</xref>).</p>
        <p>In the following, we assume that the molecule index corresponds to the rank of the activities: <inline-formula id="IE18"><mml:math id="IM18"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>≤</mml:mo><mml:mo>…</mml:mo><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Let <inline-formula id="IE19"><mml:math id="IM19"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>q</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>⌊</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo>⌋</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> be the number of elements in the training set based on the <italic>q</italic>th quantile.</p>
        <p>For each model <italic>M<sub>t</sub></italic>, evaluate for <inline-formula id="IE20"><mml:math id="IM20"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:math></inline-formula> independent iterations:
<list list-type="bullet"><list-item><p>Sample with replacement <italic>N<sub>q</sub></italic> elements from <inline-formula id="IE21"><mml:math id="IM21"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> to get a bootstrapped training dataset <inline-formula id="IE22"><mml:math id="IM22"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>a</mml:mi><mml:mi>q</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Compute <inline-formula id="IE23"><mml:math id="IM23"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>q</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>q</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>a</mml:mi><mml:mi>q</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where two proposals for the loss function <italic>L</italic> are given in the next section.</p></list-item></list></p>
        <p>The set <inline-formula id="IE24"><mml:math id="IM24"><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> is then used to estimate the mean expected loss, <inline-formula id="IE25"><mml:math id="IM25"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>A</mml:mi></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>A</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and the 95% confidence intervals.</p>
      </sec>
      <sec>
        <title>3.1.3 ‘Active-rank’ loss function</title>
        <p>In the context of using statistical or machine learning methods for novel compound drug discovery, out-of-sample performance should not directly map onto standard goodness-of-fit measures (e.g. <italic>R</italic><sup>2</sup> or mean squared error), but has a simpler decision-theoretic interpretation. If these models are to be used in a real setting then a prediction of high activity for a given feature vector (fingerprint) would lead to a physical experiment confirming or refuting this prediction. As stated above, the goal is to find molecules with an activity above a certain threshold (this will be target specific) and therefore each bad prediction (whereby the true activity is less than the threshold) incurs a fixed loss (opportunity-cost and cost of experiment). In reality, experimental costs will not be constant (some molecules are more expensive to make than others); however, we simplify the situation to one where each experiment is considered to have a fixed cost. In the out-of-sample predictions, minimizing the loss corresponds to ranking the active molecules highest. When evaluating the performance of multiple models fitted to a given dataset, if there is one active molecule and a large number of inactive molecules, the expected loss is insensitive to the ranking of all the inactives below the rank of the active(s). The model’s accuracy within the region of the inactives is of no importance. This contrasts with standard measures of predictive accuracy and loss previously used in this context, such as <italic>R</italic><sup>2</sup>, mean squared error or receiver operating characteristics (AUC) (<xref rid="btz293-B9" ref-type="bibr">Cumming <italic>et al.</italic>, 2013</xref>; <xref rid="btz293-B16" ref-type="bibr">Giguere <italic>et al.</italic>, 2013</xref>; <xref rid="btz293-B32" ref-type="bibr">Sheridan, 2013</xref>; <xref rid="btz293-B36" ref-type="bibr">Svetnik <italic>et al.</italic>, 2003</xref>; <xref rid="btz293-B38" ref-type="bibr">Wallach and Heifets, 2018</xref>; <xref rid="btz293-B40" ref-type="bibr">Wu <italic>et al.</italic>, 2018</xref>).</p>
        <p>We define our ‘active-rank’ loss function as follows. We choose a quantile <inline-formula id="IE26"><mml:math id="IM26"><mml:mrow><mml:mo>γ</mml:mo><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>γ</mml:mo><mml:mo>&gt;</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:math></inline-formula>, corresponding to a threshold activity <inline-formula id="IE27"><mml:math id="IM27"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> with respect to the empirical distribution function <inline-formula id="IE28"><mml:math id="IM28"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mi>Y</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. In practice, <italic>γ</italic> would be close to 1 (e.g. in the range 0.9–0.99) to simulate scenarios where actives molecules are rare and inactives common. The subset of molecules <inline-formula id="IE29"><mml:math id="IM29"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are then defined as ‘actives’. We define <inline-formula id="IE30"><mml:math id="IM30"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>⌊</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo>γ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⌋</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (the total number of actives), and <inline-formula id="IE31"><mml:math id="IM31"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mtext>test</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (the size of the test set).</p>
        <p>For the model <inline-formula id="IE32"><mml:math id="IM32"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> fit to the training data <inline-formula id="IE33"><mml:math id="IM33"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, the out-of-sample loss is defined with respect to the ranks assigned to the out-of-sample active molecules. We take as convention that the ranks assigned to the test data go from 0 (molecule with highest predicted activity) to <inline-formula id="IE34"><mml:math id="IM34"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mtext>test</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> (molecule with least predicted activity). The loss which only depends on the rank of the highest ranked active is defined as:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">test</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msub></mml:mrow></mml:mfrac><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mrow><mml:mtext>Rank</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>The minimum active rank will vary from 0 (an active molecule is ranked top in the test data), to <inline-formula id="IE35"><mml:math id="IM35"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mtext>test</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> (all the <inline-formula id="IE36"><mml:math id="IM36"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> active molecules are ranked last). We normalize to obtain a loss function defined over the interval [0, 1]. An alternative version of this loss, whereby all the ranks of the active molecules are taken into account, thus penalizing sub-optimal ranking for all active molecules, is given by:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>sum</mml:mtext></mml:mrow><mml:mo>γ</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mtext>Rank</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi mathvariant="bold">j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">test</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
        <p>The sum of the active ranks will vary from <inline-formula id="IE37"><mml:math id="IM37"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> (all actives are ranked in the top <inline-formula id="IE38"><mml:math id="IM38"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> molecules) to <inline-formula id="IE39"><mml:math id="IM39"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mtext>test</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> (all actives are ranked last).</p>
        <p>We note that when <inline-formula id="IE40"><mml:math id="IM40"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>γ</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, e.g. there is only one active molecule, <inline-formula id="IE41"><mml:math id="IM41"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow><mml:mo>γ</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>sum</mml:mtext></mml:mrow><mml:mo>γ</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>.</p>
        <p>As mentioned above, both these loss functions are non-additive with respect to the testing data.</p>
      </sec>
      <sec>
        <title>3.1.4 Assessing similarity of molecules</title>
        <p>In order to characterize better how splitting by activity corresponds to selecting molecules that are more or less ‘similar’ to each other, we assess similarity within training and testing sets using the Tanimoto distance (also known as the Jaccard metric). Under our notation, this is defined as:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mi>j</mml:mi></mml:msubsup></mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>2</mml:mn><mml:mi>j</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mi>j</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>2</mml:mn><mml:mi>j</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
        <p>This is the number of substructures shared between <italic>x</italic><sub>1</sub> and <italic>x</italic><sub>2</sub> over all the substructures present in either one of the molecules.</p>
      </sec>
    </sec>
    <sec>
      <title>3.2 Statistical analysis</title>
      <p>All statistical analyses were done in Python version 2.7. The entire analysis is fully reproducible via a publicly available Python Jupyter notebook found at <ext-link ext-link-type="uri" xlink:href="https://github.com/owatson/QuantileBootstrap">https://github.com/owatson/QuantileBootstrap</ext-link>.</p>
      <sec>
        <title>3.2.1 Regression models</title>
        <p>We evaluated the performance of four model classes:
<list list-type="bullet"><list-item><p>Support vector regression (Python module: <italic>sklearn</italic>, function <italic>SVR</italic>)</p></list-item><list-item><p>Random forests (Python module: <italic>sklearn</italic>, function <italic>RandomForestRegressor</italic>)</p></list-item><list-item><p>Linear ridge regression (Python module: <italic>sklearn</italic>, function <italic>Ridge</italic>)</p></list-item><list-item><p>Deep neural networks (Python module: <italic>sklearn</italic>, functions <italic>Pipline</italic> and <italic>StandardScalar</italic>, and Python module: <italic>keras</italic>, function <italic>KerasRegressor</italic>).</p></list-item></list></p>
        <p>For support vector regression, we used a linear kernel. For random forests, we used the default parameter settings, growing 100 trees each with a maximum tree depth of 10 splits. For linear ridge regression, we used a penalty term of <inline-formula id="IE42"><mml:math id="IM42"><mml:mrow><mml:mo>α</mml:mo><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>. For deep neural networks, we first standardized the data, then used two dense layers, the first of dimension 128 (to match the input feature dimension) and then dimension 16, both with relu activation.</p>
        <p>These correspond to standard default choices in the literature. These four model classes are all somewhat insensitive to tuning parameters. In order to minimize any optimization bias, we did not attempt to tune any of the parameters to the set of datasets at hand.</p>
      </sec>
      <sec>
        <title>3.2.2 Model comparison</title>
        <p>We first compared model performances using 5-fold cross-validation (this uses 80% of data chosen at random to predict the remaining 20%) and bootstrapping (this uses approximately two-thirds of the data to predict the remaining third). With discontinuous loss functions, bootstrapping smooths the out-of-sample error predictions <xref rid="btz293-B11" ref-type="bibr">Efron (1983)</xref>. The out-of-sample predictions we evaluated using mean squared error, and both active-rank loss functions <inline-formula id="IE43"><mml:math id="IM43"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow><mml:mo>γ</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE44"><mml:math id="IM44"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>sum</mml:mtext></mml:mrow><mml:mo>γ</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>. For the active-rank loss functions, we evaluated out-of-sample loss using three separate <italic>γ</italic> thresholds corresponding, respectively, to labelling 10%, 5% and 1% of the test data as active.</p>
        <p>We then ran our activity dependent validation procedure using progressively lower fractional thresholds for the training data: <inline-formula id="IE45"><mml:math id="IM45"><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn><mml:mo>;</mml:mo><mml:mn>0.8</mml:mn><mml:mo>;</mml:mo><mml:mn>0.6</mml:mn><mml:mo>;</mml:mo><mml:mn>0.4</mml:mn></mml:mrow></mml:math></inline-formula>. The same three <italic>γ</italic> thresholds were used to evaluate the out-of-sample expected losses for the active-rank loss functions. All predictions were evaluated with mean squared error and both active-rank loss functions.</p>
        <p>Overall performance was evaluated by assuming independence between the 25 datasets. The total model score assigned to each model <italic>M<sub>t</sub></italic> is defined as the sum over all datasets of the probabilities that the <italic>M<sub>t</sub></italic> had lowest expected loss (probability of optimality). As the number of bootstrap iterations is much lower than the total number of possible iterations (<inline-formula id="IE46"><mml:math id="IM46"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>), we use the jackknife to calculate the standard error on the mean out-of-sample prediction <xref rid="btz293-B11" ref-type="bibr">Efron (1983)</xref>. 400 bootstrap iterations were used for each model and set of problem definition parameters, i.e. the pair of parameters <inline-formula id="IE47"><mml:math id="IM47"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mo>γ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
      </sec>
    </sec>
    <sec>
      <title>3.3 Data</title>
      <sec>
        <title>3.3.1 Data curation</title>
        <p>We extracted IC<sub>50</sub> data from ChEMBL database version 23 for 25 diverse protein targets and receptors. In order to assemble high-quality datasets, we only considered IC<sub>50</sub> values for compounds that satisfied the following filtering criteria: (i) an activity unit equal to ‘nM’, (ii) activity relationship equal to ‘=’, (iii) target type equal to ‘SINGLE PROTEIN’ and (iv) organism equal to <italic>Homo sapiens</italic>. Bioactivity values were modelled in a logarithmic scale (i.e. pIC<sub>50</sub><inline-formula id="IE48"><mml:math id="IM48"><mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mtext>IC</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mn>50</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>). The average pIC<sub>50</sub> value was calculated for protein-compound pairs with multiple IC<sub>50</sub> measurements available.</p>
        <p>Further details about the datasets are provided in <xref rid="btz293-T1" ref-type="table">Table 1</xref>. A comparative analysis of these datasets was performed previously in the context of iterative model fitting (<xref rid="btz293-B8" ref-type="bibr">Cortes-Ciriano <italic>et al.</italic>, 2018</xref>). All data used in this article (activity levels, 128-bit fingerprints and smiles) are available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/owatson/QuantileBootstrap">https://github.com/owatson/QuantileBootstrap</ext-link>.
</p>
        <table-wrap id="btz293-T1" orientation="portrait" position="float">
          <label>Table 1.</label>
          <caption>
            <p>Twenty-five publicly available datasets extracted from ChEMBL and analysed in this article</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Target preferred name</th>
                <th rowspan="1" colspan="1">Target abbreviation</th>
                <th rowspan="1" colspan="1">Uniprot ID</th>
                <th rowspan="1" colspan="1">ChEMBL ID</th>
                <th rowspan="1" colspan="1">#Bioactive molecules</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Alpha-2a adrenergic receptor</td>
                <td rowspan="1" colspan="1">A2a</td>
                <td rowspan="1" colspan="1">P08913</td>
                <td rowspan="1" colspan="1">1867</td>
                <td rowspan="1" colspan="1">203</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Tyrosine-protein kinase ABL</td>
                <td rowspan="1" colspan="1">ABL1</td>
                <td rowspan="1" colspan="1">P00519</td>
                <td rowspan="1" colspan="1">1862</td>
                <td rowspan="1" colspan="1">773</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Acetylcholinesterase</td>
                <td rowspan="1" colspan="1">Acetylcholin</td>
                <td rowspan="1" colspan="1">P22303</td>
                <td rowspan="1" colspan="1">220</td>
                <td rowspan="1" colspan="1">3159</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Androgen Receptor</td>
                <td rowspan="1" colspan="1">Androgen</td>
                <td rowspan="1" colspan="1">P10275</td>
                <td rowspan="1" colspan="1">1871</td>
                <td rowspan="1" colspan="1">1290</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Serine/threonine-protein kinase Aurora-A</td>
                <td rowspan="1" colspan="1">Aurora-A</td>
                <td rowspan="1" colspan="1">O14965</td>
                <td rowspan="1" colspan="1">4722</td>
                <td rowspan="1" colspan="1">2125</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Serine/threonine-protein kinase B-raf</td>
                <td rowspan="1" colspan="1">B-raf</td>
                <td rowspan="1" colspan="1">P15056</td>
                <td rowspan="1" colspan="1">5145</td>
                <td rowspan="1" colspan="1">1730</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Cannabinoid CB1 receptor</td>
                <td rowspan="1" colspan="1">Cannabinoid</td>
                <td rowspan="1" colspan="1">P21554</td>
                <td rowspan="1" colspan="1">218</td>
                <td rowspan="1" colspan="1">1116</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Carbonic anhydrase II</td>
                <td rowspan="1" colspan="1">Carbonic</td>
                <td rowspan="1" colspan="1">P00918</td>
                <td rowspan="1" colspan="1">205</td>
                <td rowspan="1" colspan="1">603</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Caspase-3</td>
                <td rowspan="1" colspan="1">Caspase</td>
                <td rowspan="1" colspan="1">P42574</td>
                <td rowspan="1" colspan="1">2334</td>
                <td rowspan="1" colspan="1">1606</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Thrombin</td>
                <td rowspan="1" colspan="1">Coagulation</td>
                <td rowspan="1" colspan="1">P00734</td>
                <td rowspan="1" colspan="1">204</td>
                <td rowspan="1" colspan="1">1700</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Cyclooxygenase-1</td>
                <td rowspan="1" colspan="1">COX-1</td>
                <td rowspan="1" colspan="1">P23219</td>
                <td rowspan="1" colspan="1">221</td>
                <td rowspan="1" colspan="1">1343</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Cyclooxygenase-2</td>
                <td rowspan="1" colspan="1">COX-2</td>
                <td rowspan="1" colspan="1">P35354</td>
                <td rowspan="1" colspan="1">230</td>
                <td rowspan="1" colspan="1">2855</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Dihydrofolate reductase</td>
                <td rowspan="1" colspan="1">Dihydrofolate</td>
                <td rowspan="1" colspan="1">P00374</td>
                <td rowspan="1" colspan="1">202</td>
                <td rowspan="1" colspan="1">584</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Dopamine D2 receptor</td>
                <td rowspan="1" colspan="1">Dopamine</td>
                <td rowspan="1" colspan="1">P14416</td>
                <td rowspan="1" colspan="1">217</td>
                <td rowspan="1" colspan="1">479</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Norepinephrine transporter</td>
                <td rowspan="1" colspan="1">Ephrin</td>
                <td rowspan="1" colspan="1">P23975</td>
                <td rowspan="1" colspan="1">222</td>
                <td rowspan="1" colspan="1">1740</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Epidermal growth factor receptor erbB1</td>
                <td rowspan="1" colspan="1">erbB1</td>
                <td rowspan="1" colspan="1">P00533</td>
                <td rowspan="1" colspan="1">203</td>
                <td rowspan="1" colspan="1">4 868</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Estrogen receptor alpha</td>
                <td rowspan="1" colspan="1">Estrogen</td>
                <td rowspan="1" colspan="1">P03372</td>
                <td rowspan="1" colspan="1">206</td>
                <td rowspan="1" colspan="1">1705</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Glucocorticoid receptor</td>
                <td rowspan="1" colspan="1">Glucocorticoid</td>
                <td rowspan="1" colspan="1">P04150</td>
                <td rowspan="1" colspan="1">2034</td>
                <td rowspan="1" colspan="1">1447</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Glycogen synthase kinase-3 beta</td>
                <td rowspan="1" colspan="1">Glycogen</td>
                <td rowspan="1" colspan="1">P49841</td>
                <td rowspan="1" colspan="1">262</td>
                <td rowspan="1" colspan="1">1757</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">HERG</td>
                <td rowspan="1" colspan="1">HERG</td>
                <td rowspan="1" colspan="1">Q12809</td>
                <td rowspan="1" colspan="1">240</td>
                <td rowspan="1" colspan="1">5207</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Tyrosine-protein kinase JAK2</td>
                <td rowspan="1" colspan="1">JAK2</td>
                <td rowspan="1" colspan="1">O60674</td>
                <td rowspan="1" colspan="1">2971</td>
                <td rowspan="1" colspan="1">2655</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Tyrosine-protein kinase LCK</td>
                <td rowspan="1" colspan="1">LCK</td>
                <td rowspan="1" colspan="1">P06239</td>
                <td rowspan="1" colspan="1">258</td>
                <td rowspan="1" colspan="1">1352</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Monoamine oxidase A</td>
                <td rowspan="1" colspan="1">Monoamine</td>
                <td rowspan="1" colspan="1">P21397</td>
                <td rowspan="1" colspan="1">1951</td>
                <td rowspan="1" colspan="1">1379</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Mu opioid receptor</td>
                <td rowspan="1" colspan="1">Opioid</td>
                <td rowspan="1" colspan="1">P35372</td>
                <td rowspan="1" colspan="1">233</td>
                <td rowspan="1" colspan="1">840</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Vanilloid receptor</td>
                <td rowspan="1" colspan="1">Vanilloid</td>
                <td rowspan="1" colspan="1">Q8NER1</td>
                <td rowspan="1" colspan="1">4794</td>
                <td rowspan="1" colspan="1">1923</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec>
        <title>3.3.2 Molecular representation</title>
        <p>The python module <italic>Standardizer</italic> was used to standardize all chemical structures. Inorganic molecules were removed, and the largest fragment was kept in order to filter out counterions.</p>
        <p>We computed circular Morgan fingerprints 52 using RDkit (release version 2013.03.02). The radius was set to 2 and the fingerprint length to 128.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>4 Results</title>
    <sec>
      <title>4.1 Model performance evaluated using random data partitioning</title>
      <p>Random partitioning of the data, either using 5-fold cross-validation (training set contains 80% of the data) or bootstrapping (training set contains two-thirds of the data) resulted in random forests and deep learning having the best out-of-sample performance (e.g. <xref ref-type="fig" rid="btz293-F1">Fig. 1</xref>; for full results see github Jupyter noteboook). <xref ref-type="fig" rid="btz293-F1">Figure 1</xref> shows the bootstrap out-of-bag performance as evaluated by mean squared error for the four models over the 25 datasets, with datasets ordered from smallest to largest. Ridge regression, in the majority of cases, has the largest out-of-bag error, followed by support vector regression and then deep-learning and random forests.
</p>
      <fig id="btz293-F1" orientation="portrait" position="float">
        <label>Fig. 1.</label>
        <caption>
          <p>Model comparison using the standard bootstrap. Expected model out-of-sample mean squared error shown for each dataset, ordered from left to right by increasing size of dataset. Error bars correspond to ±2 standard errors around the expected loss estimate, computed using the jackknife estimator. For each dataset, the optimal model is the one with least expected loss, with random forests scoring best for every single dataset. Datasets are ordered from left to right by increasing size</p>
        </caption>
        <graphic xlink:href="btz293f1"/>
      </fig>
      <p>Overall model performance using random data partitioning is shown in <xref ref-type="fig" rid="btz293-F2">Figure 2</xref>, corresponding to the point on the <italic>x</italic>-axis at 100%. When scored using mean squared error (<xref ref-type="fig" rid="btz293-F2">Fig. 2</xref>, top left panel), and performs on average as well as deep learning when scored with the active-rank loss functions (<xref ref-type="fig" rid="btz293-F2">Fig. 2</xref>, last three panels). Ridge regression and support vector regression are never optimal in this setting, irrespective of the loss function.
</p>
      <fig id="btz293-F2" orientation="portrait" position="float">
        <label>Fig. 2.</label>
        <caption>
          <p>Comparison of overall model performance for the standard bootstrap and the restricted activity bootstrap. All four panels show the overall model score (sum of the probabilities of model optimality over the 25 datasets) as a function of the restriction on the activity levels in the training data. About 100% corresponds to standard cross-validation (random partitioning). The first three panels show the results for the active-rank loss functions (<inline-formula id="IE49"><mml:math id="IM49"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:mrow><mml:mo>γ</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> shown by thick lines; <inline-formula id="IE50"><mml:math id="IM50"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>sum</mml:mtext></mml:mrow><mml:mo>γ</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> shown by dashed lines) with values of <italic>γ</italic> going from 0.9 (top left) to 0.99 (bottom left). The bottom right panel shows the results when models are scored using mean squared error (dot-dashed lines). Red: deep learning; blue: support vector regression; orange: random forests; green: ridge regression</p>
        </caption>
        <graphic xlink:href="btz293f2"/>
      </fig>
      <p>These out-of-sample performances closely reflect the in-sample error. Both deep learning and random forests can almost ‘memorize’ the data with in-sample losses close to zero (see Jupyter notebook).</p>
    </sec>
    <sec>
      <title>4.2 Quantile bootstrap</title>
      <p>Decreasing the quantile-activity threshold for the training data from 1 (random partitioning described above) to 0.4 (only 40% of the data ordered by activity are used in the bootstrap construction of the training set) results in a complete reversal of optimality amongst the four predictive models. When scoring models by out-of-sample mean squared error, support vector regression becomes optimal for quantiles below 0.75 (<xref ref-type="fig" rid="btz293-F2">Fig. 2</xref>, bottom right panel).</p>
      <p>For the active-rank loss functions, lowering the activity training threshold also induces a reversal of model optimality (change-point for <inline-formula id="IE51"><mml:math id="IM51"><mml:mrow><mml:mi>q</mml:mi><mml:mo>≈</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>). In the most extreme setting (<italic>q </italic>=<italic> </italic>0.4), support vector regression and ridge regression perform approximately equally well, with total scores corresponding to optimality on half of the datasets (shown in detail in <xref ref-type="fig" rid="btz293-F3">Fig. 3</xref>). There is some heterogeneity between the datasets for model optimality, but the overall trends are clearly in favour of both ridge regression and support vector regression.
</p>
      <fig id="btz293-F3" orientation="portrait" position="float">
        <label>Fig. 3.</label>
        <caption>
          <p>Model comparison using the restricted activity bootstrap with <inline-formula id="IE52"><mml:math id="IM52"><mml:mrow><mml:mo>γ</mml:mo><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:mrow></mml:math></inline-formula>. Model expected out-of-sample <inline-formula id="IE53"><mml:math id="IM53"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow><mml:mrow><mml:mo>γ</mml:mo><mml:mo>=</mml:mo><mml:mn>0.99</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> loss shown for each dataset, ordered from left to right by increasing size of dataset. Error bars correspond to ±2 standard errors around the expected loss estimate, computed using the jackknife estimator. For each dataset, the optimal model is the one with least expected loss. Datasets are ordered from left to right by increasing size</p>
        </caption>
        <graphic xlink:href="btz293f3"/>
      </fig>
      <p>By averaging over the 25 datasets, we can see that these trends are robust with respective the target used as the outcome measure in the regression models.</p>
    </sec>
    <sec>
      <title>4.3 Comparison with similarity-based unsupervised clustering</title>
      <p>We explored whether unsupervised clustering could be used to construct training and testing sets that maximize similarity within clusters and dissimilarity across clusters. For this, we used a two-mediods clustering algorithm (<xref rid="btz293-B29" ref-type="bibr">Park and Jun, 2009</xref>) with similarity defined by the Tanimoto distance metric. For the 25 datasets presented here, unsupervised clustering did not achieve good reductions in dissimilarity: the average pairwise distance within each cluster was only approximately 2% lower than the global average pairwise distance (see Supplementary Materials). In addition, when we compared the results to using an activity dependent split of the data (e.g. at the 90th quantile of activity) this achieved greater reductions in dissimilarity within the testing set (the molecules of high activity). In summary, this empirically shows that it is difficult to cluster molecular data based on a similarity metric. However, in many of the datasets analysed here, the high-activity molecules are highly similar to one another. This is likely due to bias in the experimental workflows. This reinforces the use of activity splitting to assess model extrapolation performance.</p>
    </sec>
    <sec>
      <title>4.4 Importance of the loss function</title>
      <p>There are clear disparities between model evaluations for the different loss functions. Mean squared error favours random forests in the standard setting (<italic>q </italic>=<italic> </italic>1), and support vector regression in the restricted activity setting (<italic>q </italic>&lt;<italic> </italic>0.6). However, the active-rank loss functions favour equally deep-learning and random forests in the standard setting, and support vector regression and ridge regression in the restricted activity setting. Moreover, the results differ between <inline-formula id="IE54"><mml:math id="IM54"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow><mml:mo>γ</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE55"><mml:math id="IM55"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>sum</mml:mtext></mml:mrow><mml:mo>γ</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>. The out-of-sample performance of ridge regression is consistently better when evaluated using <inline-formula id="IE56"><mml:math id="IM56"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow><mml:mo>γ</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>, and that of support vector regression is consistently better when evaluated using <inline-formula id="IE57"><mml:math id="IM57"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>sum</mml:mtext></mml:mrow><mml:mo>γ</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="btz293-F2">Fig. 2</xref>). These results show that the evaluation of model performance is highly dependent on the loss function used. This directly reflects how the different loss functions penalize predictive performance, with <inline-formula id="IE58"><mml:math id="IM58"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow><mml:mo>γ</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> only penalizing the rank of the first active molecule. We note that in this setting, for the 25 datasets, the four model families and three loss functions used here, the sample size of the training sets does not affect the overall relative performance on the testing sets (<xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S2</xref>).</p>
      <p>We also note that using mean squared error to evaluate the performance of random forests unfairly penalizes the model fit. For quantile bootstraps with <italic>q </italic>&lt;<italic> </italic>1, random forests cannot predict activities greater than the maximum activity in the training set. Therefore the contribution to the mean squared error from high-activity molecules will all be from bias rather than variance in the prediction (predictions will be systematically lower). This is in contrast to using rank-based loss functions which do not suffer from this bias issue.</p>
      <p>From a subjective Bayesian perspective (<xref rid="btz293-B31" ref-type="bibr">Savage, 1954</xref>), the choice of loss function reflects the decision task at hand. This should be specified separately from the regression model. The two active-rank loss functions are examples of possible choices of loss functions. However, other, more standard choices, are also possible. For example, the Spearman rank correlation coefficient, or the F beta score on thresholded predictions. It is important to note that assessment of models may be sensitive to the choice of loss and careful consideration of the decision goals is needed.</p>
    </sec>
  </sec>
  <sec>
    <title>5 Discussion</title>
    <p>There is considerable hype around the use of artificial intelligence and machine learning to find novel drug candidates and to optimize early-stage drug discovery (<xref rid="btz293-B13" ref-type="bibr">Fleming, 2018</xref>). Deep learning via the use of deep neural networks is a highly active research area with a wide range of applications and proven success stories. However, neural networks are known to be extremely ‘data-hungry’ and work best in high signal-to-noise settings (<xref rid="btz293-B25" ref-type="bibr">Marcus, 2018</xref>). For regression modelling using molecular structure-activity data, we do not believe deep-learning models will perform well in predicting novel areas of molecular space of high activity, contrary to recent claims (<xref rid="btz293-B23" ref-type="bibr">Lenselink <italic>et al.</italic>, 2017</xref>). This modelling exercise empirically shows that partitioning on quantiles of the activity distribution, and thereby mimicking the process of extrapolating onto previously unseen areas of molecular space, removes the predictive advantage from the deep-learning models. This approach can be contrasted with ‘temporal splitting’ whereby datasets are partitioned by assay date, the first section used to train the model, the second to test. Temporal splitting is easy to understand and could be argued to mimic real-life settings, but it does not provide any rigorous guarantees. It does not guarantee that highly similar molecules—both in structure and activity—will not be found across both testing and training data. Time of assay will not always correspond to time of conception and therefore ‘worse’ molecules could have been tested at later dates. Drug discovery does not follow a linear process nor does it directly test the capability of a statistical or machine learning model to detect signal predicting activity gradients, resulting in good predictions of molecules with high activity. Splitting on activity quantiles deals with these issues, and provides a simple and interpretable univariate parametrization of the information content used to train the model. We note that a methodological limitation of the quantile-activity bootstrap method is the inability for the regression algorithm to learn about potential ‘activity cliffs’. If there was a large activity cliff, then the low-activity molecules would be in the training data, and the proximal high-activity molecules in the testing data. However, the impact of the limitation is dependent on the decision task at hand. If the overall goal is to assess the extrapolation properties of a model then there is ‘no free lunch’: it is necessary to put aside data for testing and these data cannot also be used for training.</p>
    <p>Evaluation of the predictive performance of regression models when applied to small-molecule structure-activity datasets necessitates different approaches than in the standard bioinformatic and high-dimensional settings. Online prediction problems (e.g. image classification, spam filtering, recommender systems, etc.) and statistical inference problems (e.g. genome-wide studies, biomarker discovery, micro-array analysis) have different goals. In the drug discovery context, we start with a small training set (<inline-formula id="IE59"><mml:math id="IM59"><mml:mrow><mml:mi>N</mml:mi><mml:mo>≪</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>) and attempt to extrapolate outside of these data in order to find molecules which are inherently ‘different’ from those in the training data. In the machine learning and computational statistics literature, this is most similar to an optimization problem or gradient ascent problem. This search procedure is done in a relatively resource constrained setting (cost of experimentation, time cost) and therefore model evaluation should be decision theoretic with a subjective loss (<xref rid="btz293-B31" ref-type="bibr">Savage, 1954</xref>).</p>
    <p>We expect our active-rank loss functions to differ in performance from standard machine learning type losses (most commonly this would be mean squared error). The active-rank loss functions <inline-formula id="IE60"><mml:math id="IM60"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow><mml:mo>γ</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE61"><mml:math id="IM61"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>sum</mml:mtext></mml:mrow><mml:mo>γ</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> do not penalize bad predictions outside of the subspace of interest, i.e. high-activity areas of molecular space. In addition, these loss functions are non-additive and therefore one limitation is that they cannot be used to penalize model fitting in the training phase. However, the use of non-additive loss functions fits our proposed conceptual workflow for computational drug discovery (<xref rid="btz293-B8" ref-type="bibr">Cortes-Ciriano <italic>et al.</italic>, 2018</xref>). In the first stage, existing software such as <xref rid="btz293-B12" ref-type="bibr">Firth <italic>et al.</italic> (2015)</xref> can be used to construct sets of viable molecules similar to existing molecules with reasonable potency. In the second stage, computational algorithms are then trained to existing structure-activity datasets. Finally, fitted models are then used to rank molecules constructed in stage 1 and the highest ranked can then be tested <italic>in vitro</italic>. Other limitations of the work are that we have done little to no internal model parameter tuning, except for deep neural nets to assess structures most appropriate for these types of data. However, we do not expect parameter tuning to considerably change the results nor the conclusions of the study. Furthermore, all the analyses are easily reproducible with our openly available Jupyter notebook, thus easily extended to new computational algorithms, different parameter settings or new datasets. Lastly, the loss functions used to evaluate model performance on these benchmark datasets will not estimate the true out-of-sample expected loss in experimental settings. In reality, true <italic>γ</italic> thresholds (percentage of feasible molecules above a certain activity level) could be multiple orders of magnitude larger than those used in our study (e.g. the top <inline-formula id="IE62"><mml:math id="IM62"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>% of the testing data).</p>
    <p><italic>Conflict of Interest</italic>: For ART and JAW. OPW and ICC have shares in Evariste Technologies.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>btz293_Supplementary_Materials</label>
      <media xlink:href="btz293_supplementary_materials.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btz293-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ajay</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>1998</year>) 
<article-title>Can we learn to distinguish between ‘drug-like’ and ‘nondrug-like’ molecules?</article-title><source>J. Med. Chem</source>., <volume>41</volume>, <fpage>3314</fpage>–<lpage>3324</lpage>.<pub-id pub-id-type="pmid">9719583</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Boulesteix</surname><given-names>A.-L.</given-names></name>, <name name-style="western"><surname>Strobl</surname><given-names>C.</given-names></name></person-group> (<year>2009</year>) 
<article-title>Optimal classifier selection and negative bias in error rate estimation: an empirical study on high-dimensional prediction</article-title>. <source>BMC Med. Res. Methodol</source>., <volume>9</volume>, <fpage>85.</fpage><pub-id pub-id-type="pmid">20025773</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Braga-Neto</surname><given-names>U.M.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Cross-validation under separate sampling: strong bias and how to correct it</article-title>. <source>Bioinformatics</source>, <volume>30</volume>, <fpage>3349</fpage>–<lpage>3355</lpage>.<pub-id pub-id-type="pmid">25123902</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Breiman</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>1984</year>) <source>Classification and Regression Trees</source>. 
<publisher-name>Chapman and Hall/CRC, Boca Raton</publisher-name>. </mixed-citation>
    </ref>
    <ref id="btz293-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Burbidge</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2001</year>) 
<article-title>Drug design by machine learning: support vector machines for pharmaceutical data analysis</article-title>. <source>Comput. Chem</source>., <volume>26</volume>, <fpage>5</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">11765851</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Castaldi</surname><given-names>P.J.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>An empirical assessment of validation practices for molecular classifiers</article-title>. <source>Briefings Bioinf</source>., <volume>12</volume>, <fpage>189</fpage>–<lpage>202</lpage>.</mixed-citation>
    </ref>
    <ref id="btz293-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cortés-Ciriano</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Bender</surname><given-names>A.</given-names></name></person-group> (<year>2016</year>) 
<article-title>How consistent are publicly reported cytotoxicity data? large-scale statistical analysis of the concordance of public independent cytotoxicity measurements</article-title>. <source>ChemMedChem</source>, <volume>11</volume>, <fpage>57</fpage>–<lpage>71</lpage>.<pub-id pub-id-type="pmid">26541361</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cortes-Ciriano</surname><given-names>I.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Discovering highly potent molecules from an inital set of inactives using iterative screening</article-title>. <source>J. Chem. Inf. Model</source>., <volume>58</volume>, <fpage>2000</fpage>–<lpage>2014</lpage>.<pub-id pub-id-type="pmid">30130102</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cumming</surname><given-names>J.G.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Chemical predictive modelling to improve compound quality</article-title>. <source>Nat. Rev. Drug Discov</source>., <volume>12</volume>, <fpage>948.</fpage><pub-id pub-id-type="pmid">24287782</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B10">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Devroye</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>1996</year>) <source>A Probabilistic Theory of Pattern Recognition</source>. 
<publisher-name>Springer</publisher-name>, New York City.</mixed-citation>
    </ref>
    <ref id="btz293-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Efron</surname><given-names>B.</given-names></name></person-group> (<year>1983</year>) 
<article-title>Estimating the error rate of a prediction rule: improvement on cross-validation</article-title>. <source>J. Am. Stat. Assoc</source>., <volume>78</volume>, <fpage>316</fpage>–<lpage>331</lpage>.</mixed-citation>
    </ref>
    <ref id="btz293-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Firth</surname><given-names>N.C.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Moarf, an integrated workflow for multiobjective optimization: implementation, synthesis, and biological evaluation</article-title>. <source>J. Chem. Inf. Model</source>., <volume>55</volume>, <fpage>1169</fpage>–<lpage>1180</lpage>.<pub-id pub-id-type="pmid">26054755</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fleming</surname><given-names>N.</given-names></name></person-group> (<year>2018</year>) 
<article-title>How artificial intelligence is changing drug discovery</article-title>. <source>Nature</source>, <volume>557</volume>, <fpage>S55</fpage>–<lpage>S57</lpage>.<pub-id pub-id-type="pmid">29849160</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B14">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Friedman</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2001</year>) <source>The Elements of Statistical Learning, Volume 1</source>. 
<publisher-name>Springer series in statistics</publisher-name>, 
<publisher-loc>New York, NY</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btz293-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Geisser</surname><given-names>S.</given-names></name></person-group> (<year>1975</year>) 
<article-title>The predictive sample reuse method with applications</article-title>. <source>J. Am. Stat. Assoc</source>., <volume>70</volume>, <fpage>320</fpage>–<lpage>328</lpage>.</mixed-citation>
    </ref>
    <ref id="btz293-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Giguere</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Learning a peptide-protein binding affinity predictor with kernel ridge regression</article-title>. <source>BMC Bioinf</source>., <volume>14</volume>, <fpage>82.</fpage></mixed-citation>
    </ref>
    <ref id="btz293-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ioannidis</surname><given-names>J.P.</given-names></name></person-group> (<year>2005</year>) 
<article-title>Microarrays and molecular research: noise discovery?</article-title><source>Lancet</source>, <volume>365</volume>, <fpage>454</fpage>–<lpage>455</lpage>.<pub-id pub-id-type="pmid">15705441</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jelizarow</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>Over-optimism in bioinformatics: an illustration</article-title>. <source>Bioinformatics</source>, <volume>26</volume>, <fpage>1990</fpage>–<lpage>1998</lpage>.<pub-id pub-id-type="pmid">20581402</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kalliokoski</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2013a</year>) 
<article-title>Comparability of mixed ic50 data–a statistical analysis</article-title>. <source>PLoS One</source>, <volume>8</volume>, <fpage>e61007.</fpage><pub-id pub-id-type="pmid">23613770</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kalliokoski</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2013b</year>) 
<article-title>Quality issues with public domain chemogenomics data</article-title>. <source>Mol. Inf</source>., <volume>32</volume>, <fpage>898</fpage>–<lpage>905</lpage>.</mixed-citation>
    </ref>
    <ref id="btz293-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Keiser</surname><given-names>M.J.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) 
<article-title>Relating protein pharmacology by ligand chemistry</article-title>. <source>Nat. Biotechnol</source>., <volume>25</volume>, <fpage>197.</fpage><pub-id pub-id-type="pmid">17287757</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kramer</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>The experimental uncertainty of heterogeneous public k(i) data</article-title>. <source>J. Med. Chem</source>., <volume>55</volume>, <fpage>5165</fpage>–<lpage>5173</lpage>.<pub-id pub-id-type="pmid">22643060</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lenselink</surname><given-names>E.B.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Beyond the hype: deep neural networks outperform established methods using a chembl bioactivity benchmark set</article-title>. <source>J. Cheminf</source>., <volume>9</volume>, <fpage>45.</fpage></mixed-citation>
    </ref>
    <ref id="btz293-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lipinski</surname><given-names>C.A.</given-names></name></person-group> (<year>2004</year>) 
<article-title>Lead- and drug-like compounds: the rule-of-five revolution</article-title>. <source>Drug Discov. Today Technol</source>., <volume>1</volume>, <fpage>337</fpage>–<lpage>341</lpage>.<pub-id pub-id-type="pmid">24981612</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B25">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Marcus</surname><given-names>G.</given-names></name></person-group> (<year>2018</year>). Deep learning: a critical appraisal. <italic>arXiv preprint arXiv: 1801.00631.</italic></mixed-citation>
    </ref>
    <ref id="btz293-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Matveeva</surname><given-names>O.V.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Optimization of signal-to-noise ratio for efficient microarray probe design</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>i552</fpage>–<lpage>i558</lpage>.<pub-id pub-id-type="pmid">27587674</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Molinaro</surname><given-names>A.M.</given-names></name></person-group><etal>et al</etal> (<year>2005</year>) 
<article-title>Prediction error estimation: a comparison of resampling methods</article-title>. <source>Bioinformatics</source>, <volume>21</volume>, <fpage>3301</fpage>–<lpage>3307</lpage>.<pub-id pub-id-type="pmid">15905277</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nandi</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) 
<article-title>Anticancer activity of selected phenolic compounds: QSAR studies using ridge regression and neural networks</article-title>. <source>Chem. Biol. Drug Des</source>., <volume>70</volume>, <fpage>424</fpage>–<lpage>436</lpage>.<pub-id pub-id-type="pmid">17949360</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Park</surname><given-names>H.-S.</given-names></name>, <name name-style="western"><surname>Jun</surname><given-names>C.-H.</given-names></name></person-group> (<year>2009</year>) 
<article-title>A simple and fast algorithm for k-medoids clustering</article-title>. <source>Expert Syst. Appl</source>., <volume>36</volume>, <fpage>3336</fpage>–<lpage>3341</lpage>.</mixed-citation>
    </ref>
    <ref id="btz293-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sadowski</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Kubinyi</surname><given-names>H.</given-names></name></person-group> (<year>1998</year>) 
<article-title>A scoring scheme for discriminating between drugs and nondrugs</article-title>. <source>J. Med. Chem</source>., <volume>41</volume>, <fpage>3325</fpage>–<lpage>3329</lpage>.<pub-id pub-id-type="pmid">9719584</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B31">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Savage</surname><given-names>L.J.</given-names></name></person-group> (<year>1954</year>) <source>The Foundations of Statistics</source>. 
<publisher-name>John Wiley &amp; Sons</publisher-name>, Hoboken.</mixed-citation>
    </ref>
    <ref id="btz293-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sheridan</surname><given-names>R.P.</given-names></name></person-group> (<year>2013</year>) 
<article-title>Time-split cross-validation as a method for estimating the goodness of prospective prediction</article-title>. <source>J. Chem. Inf. Model</source>., <volume>53</volume>, <fpage>783</fpage>–<lpage>790</lpage>.<pub-id pub-id-type="pmid">23521722</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sliwoski</surname><given-names>G.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Computational methods in drug discovery</article-title>. <source>Pharmacol. Rev</source>., <volume>66</volume>, <fpage>334</fpage>–<lpage>395</lpage>.<pub-id pub-id-type="pmid">24381236</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Somorjai</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2003</year>) 
<article-title>Class prediction and discovery using gene microarray and proteomics mass spectroscopy data: curses, caveats, cautions</article-title>. <source>Bioinformatics</source>, <volume>19</volume>, <fpage>1484</fpage>–<lpage>1491</lpage>.<pub-id pub-id-type="pmid">12912828</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stone</surname><given-names>M.</given-names></name></person-group> (<year>1974</year>) 
<article-title>Cross-validatory choice and assessment of statistical predictions</article-title>. <source>J. R. Stat. Soc. B (Methodol.)</source>, <volume>36</volume>, <fpage>111</fpage>–<lpage>147</lpage>.</mixed-citation>
    </ref>
    <ref id="btz293-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Svetnik</surname><given-names>V.</given-names></name></person-group><etal>et al</etal> (<year>2003</year>) 
<article-title>Random forest: a classification and regression tool for compound classification and QSAR modeling</article-title>. <source>J. Chem. Inf. Comput. Sci</source>., <volume>43</volume>, <fpage>1947</fpage>–<lpage>1958</lpage>.<pub-id pub-id-type="pmid">14632445</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Van De Waterbeemd</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Gifford</surname><given-names>E.</given-names></name></person-group> (<year>2003</year>) 
<article-title>Admet in silico modelling: towards prediction paradise?</article-title><source>Nat. Rev. Drug Discov</source>., <volume>2</volume>, <fpage>192</fpage>–<lpage>204</lpage>.<pub-id pub-id-type="pmid">12612645</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wallach</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Heifets</surname><given-names>A.</given-names></name></person-group> (<year>2018</year>) 
<article-title>Most ligand-based classification benchmarks reward memorization rather than generalization</article-title>. <source>J. Chem. Inf. Model</source>., <volume>58</volume>, <fpage>916</fpage>–<lpage>932</lpage>.<pub-id pub-id-type="pmid">29698607</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wood</surname><given-names>I.A.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) 
<article-title>Classification based upon gene expression data: bias and precision of error rates</article-title>. <source>Bioinformatics</source>, <volume>23</volume>, <fpage>1363</fpage>–<lpage>1370</lpage>.<pub-id pub-id-type="pmid">17392326</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Moleculenet: a benchmark for molecular machine learning</article-title>. <source>Chem. Sci</source>., <volume>9</volume>, <fpage>513</fpage>–<lpage>530</lpage>.<pub-id pub-id-type="pmid">29629118</pub-id></mixed-citation>
    </ref>
    <ref id="btz293-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zervakis</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>Outcome prediction based on microarray analysis: a critical perspective on methods</article-title>. <source>BMC Bioinf</source>., <volume>10</volume>, <fpage>53</fpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
