<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Biomed Eng Online</journal-id>
    <journal-id journal-id-type="iso-abbrev">Biomed Eng Online</journal-id>
    <journal-title-group>
      <journal-title>BioMedical Engineering OnLine</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1475-925X</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8323223</article-id>
    <article-id pub-id-type="publisher-id">909</article-id>
    <article-id pub-id-type="doi">10.1186/s12938-021-00909-0</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ABrainVis: an android brain image visualization tool</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Osorio</surname>
          <given-names>Ignacio</given-names>
        </name>
        <address>
          <email>iosorio@udec.cl</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Guevara</surname>
          <given-names>Miguel</given-names>
        </name>
        <address>
          <email>mguevara.bme@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bonometti</surname>
          <given-names>Danilo</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Carrasco</surname>
          <given-names>Diego</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Descoteaux</surname>
          <given-names>Maxime</given-names>
        </name>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Poupon</surname>
          <given-names>Cyril</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mangin</surname>
          <given-names>Jean-François</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hernández</surname>
          <given-names>Cecilia</given-names>
        </name>
        <address>
          <email>cecihernandez@udec.cl</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9988-400X</contrib-id>
        <name>
          <surname>Guevara</surname>
          <given-names>Pamela</given-names>
        </name>
        <address>
          <email>pguevara@udec.cl</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.5380.e</institution-id><institution-id institution-id-type="ISNI">0000 0001 2298 9663</institution-id><institution>Department of Computer Sciences, </institution><institution>Universidad de Concepción, </institution></institution-wrap>Concepción, Chile </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.5380.e</institution-id><institution-id institution-id-type="ISNI">0000 0001 2298 9663</institution-id><institution>Department of Electrical Engineering, </institution><institution>Universidad de Concepción, </institution></institution-wrap>Concepción, Chile </aff>
      <aff id="Aff3"><label>3</label>Center for Biotechnology and Bioengineering (CeBiB), Santiago, Chile </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.457334.2</institution-id><institution>Université Paris-Saclay, CEA, CNRS, Neurospin, BAOBAB, </institution></institution-wrap>Gif-sur-Yvette, France </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.86715.3d</institution-id><institution-id institution-id-type="ISNI">0000 0000 9064 6198</institution-id><institution>Sherbrooke Connectivity Imaging Lab (SCIL), </institution><institution>Computer Science Department, Université de Sherbrooke, </institution></institution-wrap>Sherbrooke, Canada </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>29</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>29</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>20</volume>
    <elocation-id>72</elocation-id>
    <history>
      <date date-type="received">
        <day>14</day>
        <month>1</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>15</day>
        <month>7</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">The visualization and analysis of brain data such as white matter diffusion tractography and magnetic resonance imaging (MRI) volumes is commonly used by neuro-specialist and researchers to help the understanding of brain structure, functionality and connectivity. As mobile devices are widely used among users and their technology shows a continuous improvement in performance, different types of applications have been designed to help users in different work areas.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We present, ABrainVis, an Android mobile tool that allows users to visualize different types of brain images, such as white matter diffusion tractographies, represented as fibers in 3D, segmented fiber bundles, MRI 3D images as rendered volumes and slices, and meshes. The tool enables users to choose and combine different types of brain imaging data to provide visual anatomical context for specific visualization needs. ABrainVis provides high performance over a wide range of Android devices, including tablets and cell phones using medium and large tractography datasets. Interesting visualizations including brain tumors and arteries, along with fiber, are given as examples of case studies using ABrainVis.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">The functionality, flexibility and performance of ABrainVis tool introduce an improvement in user experience enabling neurophysicians and neuroscientists fast visualization of large tractography datasets, as well as the ability to incorporate other brain imaging data such as MRI volumes and meshes, adding anatomical contextual information.</p>
      </sec>
      <sec>
        <title>Supplementary Information</title>
        <p>The online version contains supplementary material available at 10.1186/s12938-021-00909-0.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Mobile visualization</kwd>
      <kwd>3D rendering</kwd>
      <kwd>Brain imaging</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>ANID</institution>
        </funding-source>
        <award-id>FONDECYT 1190701</award-id>
        <award-id>Basal Project FB0008</award-id>
        <principal-award-recipient>
          <name>
            <surname>Guevara</surname>
            <given-names>Pamela</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>ANID</institution>
        </funding-source>
        <award-id>Basal Project FB0001</award-id>
        <principal-award-recipient>
          <name>
            <surname>Hernández</surname>
            <given-names>Cecilia</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100007601</institution-id>
            <institution>Horizon 2020</institution>
          </institution-wrap>
        </funding-source>
        <award-id>945539 (HBP SGA3)</award-id>
        <award-id>785907 (HBP SGA2)</award-id>
        <principal-award-recipient>
          <name>
            <surname>Mangin</surname>
            <given-names>Jean-François</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100007601</institution-id>
            <institution>Horizon 2020</institution>
          </institution-wrap>
        </funding-source>
        <award-id>604102 (HBP SGA1).</award-id>
        <principal-award-recipient>
          <name>
            <surname>Mangin</surname>
            <given-names>Jean-François</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par31">Neuroscientists and neurophysicians typically use visual inspection of brain imaging as a way of understanding the structure and extracting useful information of the brain, as well as to perform quality control. However, high-quality brain images usually require large high dimension datasets and developing fast, interactive and flexible visualization tools is a challenging task. For instance, tractography datasets are constructed based on the diffusion local models from diffusion magnetic resonance imaging (dMRI) [<xref ref-type="bibr" rid="CR1">1</xref>], that measures the 3D motion of white matter molecules in the brain. Tractography data, representing the main 3D white matter fiber pathways, are commonly composed of a set of fibers or streamlines, where each fiber is a discrete 3D line formed by a set of 3D data points, also known as 3D polyline. See Fig. <xref rid="Fig1" ref-type="fig">1</xref>A for a schematics of tractography data representation. With the development of different diffusion models [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>], currently tractography datasets can have over one hundred thousand of fibers. In addition, there exist software tools to cluster fibers with similar shapes and lengths [<xref ref-type="bibr" rid="CR4">4</xref>–<xref ref-type="bibr" rid="CR6">6</xref>] and segment bundles based on a fiber bundles atlas [<xref ref-type="bibr" rid="CR7">7</xref>–<xref ref-type="bibr" rid="CR9">9</xref>]. These tools enable the visualization of such structures and can be of interest for analyzing their potential anatomical meaning.</p>
    <p id="Par32">Other types of brain imaging, such as MRI images and meshes are also of great interest for visualization (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>B, C for a schematics of mesh and MRI volume representation). These types of data are relevant for providing a more complete view of the anatomical context. For instance, visualizing different fiber bundles connecting different brain regions can be better understood when visualized in combination with MRI volumes and slices, to see clearly which other organs can be close. The flexibility of a mobile visualization tool, able to display tractography datasets, supporting the inclusion of other brain images can be of special interest. For example, a neurophysician might need to visualize a brain tumor in the context of MRI volume and fiber bundles in the neighbor region, to extract more information. Being able to visualize different brain images in a combined way is also challenging because in addition to data size, the method must deal with different formats and provide different visualization operations and interactions, smoothly and efficiently.</p>
    <p id="Par33">There exist several applications that enable image visualization and analysis for brain research studies. Most of these tools work on general purpose computer systems [<xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR11">11</xref>] or are web-based [<xref ref-type="bibr" rid="CR12">12</xref>–<xref ref-type="bibr" rid="CR15">15</xref>]. As the mobile industry, including tablets and cell phones devices, keeps growing in the number of users and improving in performance, its use in different working areas is becoming increasingly common. Several mobile visualization tools for neurophysicians and researchers have been proposed. Some of these applications have general educational purposes or seek to instruct their users regarding more specific areas. These applications allow the user to navigate through different preset data, for instance the free mobile version of BrainTutor [<xref ref-type="bibr" rid="CR16">16</xref>], that includes brain cortex information from a 3D object; Atlas of MRI Brain Anatomy [<xref ref-type="bibr" rid="CR17">17</xref>] and Brain MRI atlas [<xref ref-type="bibr" rid="CR18">18</xref>], both inform about the brain structures from 2D MRI slices; NeuroNavigator [<xref ref-type="bibr" rid="CR19">19</xref>] that offers the 3D visualization of structures from the brain cortex, blood vessels, functional activation and fiber pathways; MRI Viewer [<xref ref-type="bibr" rid="CR20">20</xref>] that includes MRI images of neck, chest and pelvis; CT Scan Cross Sectional Anatomy for Imaging Pros [<xref ref-type="bibr" rid="CR21">21</xref>] that shows 2D slices of computer tomography (CT) images from the body, complemented with educational drawings. Other applications target more specific areas for instructing medical trainees, going from a general learning as the radiology of the whole body (Radiological Anatomy For FRCR1 [<xref ref-type="bibr" rid="CR22">22</xref>]), or the typical artifacts and variants in different imaging modalities (Imaging Brain, Skull and Craniocervical Vasculature [<xref ref-type="bibr" rid="CR23">23</xref>]), to more specific ones as brain MRI atlases (NeuroSlice, [<xref ref-type="bibr" rid="CR24">24</xref>]) or the myelination changes in the brain (Myelination Brain, [<xref ref-type="bibr" rid="CR25">25</xref>]). Other applications of this type aim to more practical objectives as the aid in surgical scenarios. For instance the work of Dogan et al. [<xref ref-type="bibr" rid="CR26">26</xref>] proposes an application that allows the user to input their own data where brain lesions are identified, and superpose it over the actual patient’s head by using the device camera. Another work, presented by Rojas et al. [<xref ref-type="bibr" rid="CR27">27</xref>], proposes a tool for the visualization of functional connectivity networks and the relative positions of EEG electrodes from preset data, in order to facilitate this kind of exams.</p>
    <p id="Par34">Other available applications offer the user the possibility of visualizing their own data as mRay [<xref ref-type="bibr" rid="CR28">28</xref>] (MRI data) or IMAIOS Dicom Viewer [<xref ref-type="bibr" rid="CR29">29</xref>] (ultrasound, scanner, MRI, PET, etc.). However, these tools only provide 2D visualizations of the 3D volume data. A summary of all these applications can be seen in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S1.</p>
    <p id="Par35">Furthermore, there exist some high-performance 3D objects viewers available for mobile devices (3D Model Viewer-OBJ/STL/DAE [<xref ref-type="bibr" rid="CR30">30</xref>], 3D Model Viewer [<xref ref-type="bibr" rid="CR31">31</xref>]), but these are not focused on medical imaging data. Moreover they do not offer a direct 3D rendering from images.</p>
    <p id="Par36">As mentioned, most existing applications allow users only to visualize their own preset data, which limits their uses. Although some of the applications allow the user to load their own data, their functionalities are reduced to few imaging modalities or visualization modes. Taking this into account, in this work we propose a visualization tool for Android mobile devices that enables the visualization of user’s different types of brain imaging data. This work is an extension of our previously presented application  [<xref ref-type="bibr" rid="CR32">32</xref>], which proposed an efficient Android tool for large tractography datasets, enabling fast visualization of large number of fibers, and includes interactive operations using the graphic OpenGL pipeline framework. In this new release we considered the visualization of a wider range of data types, including the visualization of 3D medical images and meshes, that can be displayed independently or combining different imaging data formats. The supported types are generics, allowing the users to include different data, such as brain tumors and arteries, in the form of 3D images or meshes, not being exclusive for brain imaging, which allows researches to improve the analysis using different case studies.</p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <p id="Par37">ABrainVis supports three main data formats: tractography datasets, meshes and MRI volumes. Figure <xref rid="Fig1" ref-type="fig">1</xref> presents the main representation of these data formats. Both tractography datasets and meshes have associated only one display object each, while MRI volumes can be displayed by using two different display objects. The first is a <italic>slice</italic> object, which is a 2D image of the volume that is aligned with the volume axes. The second is a 3D volume rendering object, which performs a direct rendering of the three-dimensional image data. Tractography datasets are composed of fibers or streamlines, where each fiber is represented by a 3D polyline consisting of a set of 3D points. Depending on the specific file format, additional information may be included, such as fiber bundle names (<italic>.bundles</italic> format) or a spatial transformation (<italic>.trk</italic> format). Mesh files are more generic data types. These are composed of a list of 3D vertices (3D points), and a list of polygons, commonly triangles, where each triangle is defined by the indices of its vertices. Another type of information, such as the vertex normals, can be included. An MRI volume is represented with a 3D matrix of volume elements (voxels) distributed on a 3D regular grid, where each voxel is associated with an intensity. Additional information, such as voxel dimensions, can be included in the input file header.<fig id="Fig1"><label>Fig. 1</label><caption><p>Main data formats supported by ABrainVis. <bold>A</bold> Tractography dataset, composed of fibers or streamlines. Each fiber is represented by a 3D polyline consisting of a set of 3D points. Other information may be included, such as fiber bundle names or a spatial transformation, not represented here. <bold>B</bold> Mesh file, composed of a list of 3D vertices (3D points), and a list of triangles, where each triangle is defined by the indices of the three vertices that compose it. Other information can be included, such as vertex normals (not represented here). <bold>C</bold> MRI volume, composed of a 3D matrix of voxels (volume elements), where each voxel is associated with an intensity. The file also contains other information, such as voxel dimensions, included on a header (not represented here). Both tractography datasets and meshes have associated only one display object each, while MRI volumes can be displayed as slices or volume rendering</p></caption><graphic xlink:href="12938_2021_909_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par38">ABrainVis provides different functionalities aiming to provide a fast visualization and an intuitive user interface. In order to provide a fast 3D rendering, the tool uses the OpenGL 3D graphic engine. The main components of the visualization tool are the Graphic User Interface (GUI) and the Rendering Engine, which are displayed in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. As shown, the user interface component consists of a Menu and an Event Manager module. This component allows users to load files, define visualization settings and interactively adjust object views.<fig id="Fig2"><label>Fig. 2</label><caption><p>ABrainVis software components and interactions. The user’s interface component (GUI) consists of a Menu and an Event Manager module. This component connects with the rendering engine which contains modules to manipulate the camera, visualization, and the supported visualization objects (tractography datasets, meshes and MRI volumes)</p></caption><graphic xlink:href="12938_2021_909_Fig2_HTML" id="MO2"/></fig></p>
    <p id="Par39">The GUI component communicates with the rendering engine which uses OpenGL ES (OpenGL for Embedded Systems) [<xref ref-type="bibr" rid="CR33">33</xref>] pipeline visualization framework. The rendering engine has modules to manipulate the camera, visualization, and the supported visualization objects (tractography datasets, meshes and MRI volumes). The ABrainVis graphic component uses extensible OpenGL shaders, which exploit parallelism for processing graphic operations using OpenGL data specialized objects, such as VBO (Vertex Buffer Object) and EBO (Element Buffer Object), for vertex and index high-performance processing. A VBO enables the processing of methods to upload the vertex data properties, such as positions, normal vectors, and color, while an EBO stores indices that OpenGL uses to decide which vertices to draw. The main features of the rendering engine modules are described below.</p>
    <sec id="Sec3">
      <title>Fiber tract manipulation</title>
      <p id="Par40">This module processes all tasks related to fiber tractography operations, such as loading fiber bundles from files, choosing fiber visual representation, bundle selection and fiber sampling. Tractography files contain the 3D coordinates for each point of each fiber in the dataset. The number of points can be different for each fiber. The application supports two formats: TrackVis format (files with extension <italic>.trk</italic>) [<xref ref-type="bibr" rid="CR34">34</xref>] and Bundles format (two files with extensions <italic>.bundles</italic> and <italic>.bundlesdata</italic>) [<xref ref-type="bibr" rid="CR35">35</xref>]. Bundles format supports the use of labels, hence tractography datasets can contain segmented bundles or clusters, identified with a name.</p>
      <sec id="Sec4">
        <title>Data objects</title>
        <p id="Par41">Tractography datasets are loaded and represented in VBO buffers. These buffers are used to perform required processing for the tractography dataset, without the need of operating with immediate rendering. In addition, this module uses the OpenGL EBO buffers to store the vertex indices, which are used for selecting specific vertices to render.</p>
        <p id="Par42">The tractography visualization also allows a user to represent fibers visually as lines or cylinders. Both visualization modes support illumination. In the case of line display, the normal is emulated using the local fiber direction, where for each point it is calculated as the direction of the next fiber segment. The cylinder representation provides a more realistic appearance, improving the quality of the visualization, but decreases the rendering performance.</p>
      </sec>
      <sec id="Sec5">
        <title>Data sampling</title>
        <p id="Par43">The visualization of complete large tractography datasets might be demanding in terms of memory usage and rendering time. The current version of ABrainVis stores the complete tractography dataset in VBO buffers and, since visualizing all fibers can consume too much memory and processing time for the device, the tool supports fiber sampling. Note that the resource usage requirement is also true for general purpose computers or laptops. The sampling can be selected as a percentage of fibers, from 1% to 100%.</p>
      </sec>
      <sec id="Sec6">
        <title>Fiber bundle selection</title>
        <p id="Par44">This operation allows a user to select fiber bundles from a label list given with the input data for Bundles format. The bundle labels are not part of ABrainVis processing, they need to be produced by a previous step such as applying a clustering or segmentation method. In order the keep the visualization processing fast, ABrainVis creates a new EBO for each bundle to be rendered. As the number of bundles, as well as some bundles in a file can be large, sampling is also an alternative that the user can choose to improve visualization time and reduce data occlusion. In addition, different colors are randomly selected for each bundle.</p>
      </sec>
    </sec>
    <sec id="Sec7">
      <title>MRI manipulation</title>
      <p id="Par45">The volume and slice rendering are based on 3D images in NIfTI data format [<xref ref-type="bibr" rid="CR36">36</xref>], including the file extensions <italic>.nii</italic> or <italic>.nii.gz</italic>. This module reads the data into a 3D matrix and, if provided, a transformation matrix. The 3D matrices are stored as textures in OpenGL objects, which can be processed using specialized methods for each rendering mode. The base technique used for both renderings is described by Hadwiger et al. [<xref ref-type="bibr" rid="CR37">37</xref>], and are briefly outlined below.</p>
      <sec id="Sec8">
        <title>Slice rendering</title>
        <p id="Par46">The slice manipulation allows a user to visualize 2D cross-sections of the volume. The tool enables the visualization of one slice for each one of the three planes, according to the “<italic>X</italic>”, “<italic>Y</italic>”, and “<italic>Z</italic>” coordinates of the image. These slices will correspond to the axial, sagittal and coronal planes of the head, depending on the orientation of the image. A slider allows the user to modify the variables describing the slice number for each plane. Then, a volume slice is calculated on the GPU using the OpenGL pipeline. The implementation enables to extend the module for creating slices with planes that are not perpendicular to the main three axes. The rendering is performed, without interpolation over the plane, by fetching the intensity values from the 3D texture. The slice is colored according to a simple injective linear function that maps the values <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f:{\mathrm{3DTexture}} \rightarrow \mathcal {R}$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mi mathvariant="normal">DTexture</mml:mi></mml:mrow><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="script">R</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq1.gif"/></alternatives></inline-formula>.</p>
      </sec>
      <sec id="Sec9">
        <title>3D volume rendering</title>
        <p id="Par47">The volume rendering uses the same techniques as the slice rendering explained above. The rendering process draws a group of slices, covering the whole volume for the plane perpendicular to the viewing vector. The plane is drawn using a number of slices (<inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n_{\mathrm{s}}$$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mi>n</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq2.gif"/></alternatives></inline-formula>) over the volume, moving along the viewing direction by a uniform spacing. The number of slices to be drawn is determined in terms of the number of slices of the volume in each axis (<inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n_x$$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mi>n</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq3.gif"/></alternatives></inline-formula>, <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n_y$$\end{document}</tex-math><mml:math id="M8"><mml:msub><mml:mi>n</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq4.gif"/></alternatives></inline-formula> and <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n_z$$\end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mi>n</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq5.gif"/></alternatives></inline-formula>) and a sampling factor (sf). By default sf is set to 0.2 (see Eq. <xref rid="Equ1" ref-type="">1</xref>):<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} n_{\mathrm{s}} = {\mathrm{sf}} \times \sqrt{n_x^2 + n_y^2 + n_z^2}. \end{aligned}$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">sf</mml:mi><mml:mo>×</mml:mo><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12938_2021_909_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>To define the intensities to be considered, the Otsu thresholding algorithm [<xref ref-type="bibr" rid="CR38">38</xref>] is used to select a proper threshold for the volume. The lighting effect is calculated using the gradient estimation and Phong illumination algorithm [<xref ref-type="bibr" rid="CR37">37</xref>, <xref ref-type="bibr" rid="CR39">39</xref>]. A good performance is achieved thanks to the modern OpenGL functionalities, such as <italic>instancing drawing</italic>, for fast rendering of multiples planes. Also, <italic>shader processing</italic> is used for the lighting algorithm, which is executed on the GPU using the OpenGL framework. All important values are programmed to be easily modified and added to the GUI if necessary. The tool allows the user to modify different parameters of the visualization of slice and volume rendering through its graphical user interface.</p>
      </sec>
    </sec>
    <sec id="Sec10">
      <title>Mesh manipulation</title>
      <p id="Par48">The mesh processing supports GIfTI (files with extension <italic>.gii</italic>) [<xref ref-type="bibr" rid="CR40">40</xref>], and Mesh (two files with extensions <italic>.mesh</italic> and <italic>.mesh.minf</italic>) [<xref ref-type="bibr" rid="CR41">41</xref>] formats. All meshes contain graphical surface-based data including 3D vertex coordinates, triangle vertex indices (geometry information), and optionally, the normal vector for each vertex. If normal vectors are not included in the input data, the tool computes them. To produce proper transparent objects, the mesh is drawn from back to front, using a sorting algorithm and the user viewing vector (<italic>eye</italic> perspective). The wireframe can also easily be activated through the GUI. Different colors can be selected for triangles and wireframe. The effects are obtained by modifying the OpenGL options.</p>
    </sec>
    <sec id="Sec11">
      <title>Camera</title>
      <p id="Par49">The camera module manages the object position, focus and orientation. The user interface interacts with this module through the event manager, by providing the operation required by the user such as orbit, rotate, pan and zoom. These operations modify the axis angles, camera origin coordinates and radius of the visualization matrix. To support 3D objects, the camera module uses spherical coordinates as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. As the users are able to modify the objects view by touching the device screen, this module supports the following interaction operations. <italic>Orbiting</italic> by swiping the screen with one finger, <italic>rotating</italic> by pinching the screen with two fingers, <italic>zooming</italic> by pinching the screen with two fingers, and <italic>panning</italic>, by swiping the screen with two or more fingers.</p>
      <p id="Par50">The camera module communicates with the visualization module, which is responsible for processing and displaying the actual graphic objects on the screen.<fig id="Fig3"><label>Fig. 3</label><caption><p>Camera coordinates internal representation. The center of the sphere, located at the origin, is the point where the camera looks, <italic>r</italic> is the position of the camera. The angles <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta$$\end{document}</tex-math><mml:math id="M14"><mml:mi>θ</mml:mi></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq6.gif"/></alternatives></inline-formula> and <inline-formula id="IEq7"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varphi$$\end{document}</tex-math><mml:math id="M16"><mml:mi>φ</mml:mi></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq7.gif"/></alternatives></inline-formula> are stored as a rotation quaternion to facilitate camera manipulation. The three vectors are spherical coordinate representations of the plane of view</p></caption><graphic xlink:href="12938_2021_909_Fig3_HTML" id="MO4"/></fig></p>
    </sec>
    <sec id="Sec12">
      <title>Visualization</title>
      <p id="Par51">This module performs the object rendering. It contains the programmable shaders, the visualization matrix and illumination constants.</p>
      <p id="Par52">Shaders represent small programs that run on the GPU, controlling small parts of the OpenGL pipeline. They normally take an input and transforms it into an output to the next stage. There are three main types of shaders, the Vertex Shader (VS), the Fragment Shader (FS) and the Geometry Shader (GS). The VS allows to deal with the geometry information of the vertices, the FS generates the rendered pixels, and the GS is optional and enables the extension or reduction of the geometry and is executed after the VS. As shaders are the parallel processing functions executed on the GPU of the device, they are defined according to the object type required to display. All shaders use the VBO and EBO buffers, where the EBO buffer indicates to OpenGL which objects, residing as vertices in VBO buffers, to render. The vertex shader is responsible for drawing vertices, while the line shader and cylinder (extension) shader draw lines and cylinders, correspondingly. For fast tract manipulation, ABrainVis uses OpenGL ES mode called <italic>primitive restart</italic> to draw all fibers in only one function call.</p>
      <p id="Par53">The visualization module also implements the illumination 3D model using the Phong algorithm [<xref ref-type="bibr" rid="CR39">39</xref>]. The color displayed at each pixel depends on the color of the object, given by the material reflection constants, and the effect of the shading, defined by the lighting parameters. The algorithm computes the illumination (<italic>I</italic>) at each surface point as a function of the ambient (<inline-formula id="IEq8"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{\mathrm{a}}$$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mi>L</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq8.gif"/></alternatives></inline-formula>), diffuse (<inline-formula id="IEq9"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{\mathrm{d}}$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mi>L</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq9.gif"/></alternatives></inline-formula>), and specular (<inline-formula id="IEq10"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{\mathrm{s}}$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mi>L</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq10.gif"/></alternatives></inline-formula>) light components, material reflection constants (ambient <inline-formula id="IEq11"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K_{\mathrm{a}}$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>K</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq11.gif"/></alternatives></inline-formula>, diffuse <inline-formula id="IEq12"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K_{\mathrm{d}}$$\end{document}</tex-math><mml:math id="M26"><mml:msub><mml:mi>K</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq12.gif"/></alternatives></inline-formula> and specular <inline-formula id="IEq13"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K_{\mathrm{s}}$$\end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mi>K</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq13.gif"/></alternatives></inline-formula>), the view vector (<inline-formula id="IEq14"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{\varvec{v}}}$$\end{document}</tex-math><mml:math id="M30"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq14.gif"/></alternatives></inline-formula>), the light reflection vector (<inline-formula id="IEq15"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{\varvec{r}}}$$\end{document}</tex-math><mml:math id="M32"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq15.gif"/></alternatives></inline-formula>), the plane normal vector (<inline-formula id="IEq16"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{\varvec{n}}}$$\end{document}</tex-math><mml:math id="M34"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq16.gif"/></alternatives></inline-formula>), the direction vector from the point toward the light source (<inline-formula id="IEq17"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{\varvec{l}}}$$\end{document}</tex-math><mml:math id="M36"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq17.gif"/></alternatives></inline-formula>), and the shininess coefficient (<italic>f</italic>). The illumination computation is shown in Eq. (<xref rid="Equ2" ref-type="">2</xref>), where all the constants (<inline-formula id="IEq18"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{\mathrm{a}},L_{\mathrm{d}},L_{\mathrm{s}},K_{\mathrm{a}},K_{\mathrm{d}},K_{\mathrm{s}}$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq18.gif"/></alternatives></inline-formula>) are in the range [0, 1] and the vectors are normalized. This algorithm is used in the vertex shader.<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} I = L_{\mathrm{a}}K_{\mathrm{a}} + L_{\mathrm{d}}K_{\mathrm{d}}({\hat{\varvec{s}}}\cdot {\hat{\varvec{n}}}) + L_{\mathrm{s}}K_{\mathrm{s}}({\hat{\varvec{r}}}\cdot {\hat{\varvec{v}}})^f. \end{aligned}$$\end{document}</tex-math><mml:math id="M40" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>·</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>·</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>f</mml:mi></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12938_2021_909_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>The rendering effects are generated using the combination of these shaders, described in Table <xref rid="Tab1" ref-type="table">1</xref>. For each type of data, different shaders are needed, which are summarized in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. For the <italic>Tract visualization</italic>, we use a special vertex shader for adding lighting to the fibers. When using the cylinder render option, the vertex shader passes the vertex information to the next pipeline stage, where a <italic>geometry shader</italic> is used to extend lines to cylinders, while adding the lighting.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Shaders used for visualization</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Type</th><th align="left">Name</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left" rowspan="6">Vertex shader</td><td align="left">Bounding box</td><td align="left">Model and view matrix applied to lines without lighting effect</td></tr><tr><td align="left">Bundle</td><td align="left">Model and view matrix applied to tract 3D data with Phong lighting algorithm</td></tr><tr><td align="left">Coordinate system</td><td align="left">Model and view applied to volumetrical arrows without lighting effect</td></tr><tr><td align="left">Cylinder</td><td align="left">Pass through shader for tract data</td></tr><tr><td align="left">Mesh</td><td align="left">Model and view matrix applied to mesh 3D data with Phong lighting algorithm and opacity</td></tr><tr><td align="left">Volume-slice</td><td align="left">Model and view matrix applied to vertex from bounding box collided with plane</td></tr><tr><td align="left" rowspan="2">Geometry shader</td><td align="left">Cylinder</td><td align="left">Model and view matrix applied to a line-to-cylinder algorithm</td></tr><tr><td align="left">Quaternion</td><td align="left">Functions describing quaternion mathematics</td></tr><tr><td align="left" rowspan="3">Fragment shader</td><td align="left">Standard fragment shader</td><td align="left">Pass through shader for color data</td></tr><tr><td align="left">MRI slide</td><td align="left">Evaluation of pixel color for 3D texture (MRI data)</td></tr><tr><td align="left">MRI volume</td><td align="left">Evaluation of pixel color for 3D texture (MRI data) and Phong lighting algorithm using gradient estimation as normal</td></tr></tbody></table></table-wrap></p>
      <p id="Par54">
        <fig id="Fig4">
          <label>Fig. 4</label>
          <caption>
            <p>A diagram showing the different types of data that the application can display, along with the supported visualization modes and the shaders used to render them.<italic> VS</italic> vertex shader,<italic> FS</italic> fragment shader,<italic> GS</italic> geometry shader</p>
          </caption>
          <graphic xlink:href="12938_2021_909_Fig4_HTML" id="MO6"/>
        </fig>
      </p>
      <p id="Par55">When rendering slices and volumes, the vertex shader transforms a plane equation into a slice inside the 3D volume. The 3D texture is applied to the resulting slice on the fragment shader, resulting in the slice’s rendering. The volume rendering requires more work to provide a proper perception of depth, achieved by the use of a threshold and a lighting algorithm running on the fragment shader.</p>
      <p id="Par56">The <italic>mesh</italic> has a special vertex shader, which allows the user to modify some variables of interest, such as transparency and color. Similarly, the <italic>tools</italic> object has some very simple dedicated vertex shaders to display the <italic>coordinate arrows</italic> and the <italic>bounding boxes</italic> of the objects.</p>
    </sec>
  </sec>
  <sec id="Sec13">
    <title>Results</title>
    <p id="Par57">This section contains examples with the main functionalities of ABrainVis, describes and discusses the experimental evaluation and results, as well as the datasets used for this purpose.</p>
    <p id="Par58">First, a comparison of the most similar state-of-the-art tools with ABrainVis is shown in Table <xref rid="Tab2" ref-type="table">2</xref>. There is no mobile application with similar features to ABrainVis. The most similar one is <italic>FiberWeb</italic>, which is a web application that allows the user to load tractography data and MRI volumes as slices, but not meshes. It also does not allow color differentiation of different tracts. The most similar mobile applications are <italic>BrainTutor</italic> and <italic>Neuronavigator</italic>, which support viewing meshes, 3D images and fibers, with additional structural information, but they do not allow users to include external data, i.e., only predefined data can be viewed. In addition, neither of these applications supports volume rendering for 3D volumes, nor do they allow users to customize illumination parameters. Although, in some cases, they do offer other functionalities. We include an example of visualization for each one of the three applications in the Additional file <xref rid="MOESM2" ref-type="media">2</xref>.</p>
    <p id="Par59">The experimental evaluation considers performance metrics such as frames per seconds (fps) for the tractography datasets using the mobile devices listed in Table <xref rid="Tab3" ref-type="table">3</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Comparison between most similar applications</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature/app</th><th align="left">FiberWeb</th><th align="left">BrainTutor</th><th align="left">NeuroNavigator</th><th align="left">ABrainVis</th></tr></thead><tbody><tr><td align="left">External data</td><td align="left">✓</td><td align="left">✗</td><td align="left">✗</td><td align="left">✓</td></tr><tr><td align="left">Web/Android/iOS</td><td align="left">✓/✗/✗</td><td align="left">✗/✓/✓</td><td align="left">✗/✓/✗</td><td align="left">✗/✓/✗</td></tr><tr><td align="left">MRI volume</td><td align="left">✓</td><td align="left">✓</td><td align="left">✓</td><td align="left">✓</td></tr><tr><td align="left">Mesh</td><td align="left">✗</td><td align="left">✓</td><td align="left">✓</td><td align="left">✓</td></tr><tr><td align="left">Tractography</td><td align="left">✓</td><td align="left">✓</td><td align="left">✓</td><td align="left">✓</td></tr><tr><td align="left">Functional connectivity</td><td align="left">✗</td><td align="left">✓</td><td align="left">✓</td><td align="left">✗</td></tr><tr><td align="left">Zooming</td><td align="left">✓</td><td align="left">✓</td><td align="left">✓</td><td align="left">✓</td></tr><tr><td align="left">Panning</td><td align="left">✓</td><td align="left">✓</td><td align="left">✗</td><td align="left">✓</td></tr><tr><td align="left">Rotating</td><td align="left">✓</td><td align="left">✓</td><td align="left">✓</td><td align="left">✓</td></tr><tr><td align="left">Slice navigation</td><td align="left">✗</td><td align="left">✓</td><td align="left">✓</td><td align="left">✓</td></tr><tr><td align="left">Transparency</td><td align="left">✗</td><td align="left">✗</td><td align="left">✓</td><td align="left">✓</td></tr><tr><td align="left">Superimposing</td><td align="left">✗</td><td align="left">✓</td><td align="left">✓</td><td align="left">✓</td></tr><tr><td align="left">3D rendering</td><td align="left">✗</td><td align="left">✗</td><td align="left">✗</td><td align="left">✓</td></tr><tr><td align="left">Fiber color</td><td align="left">✗</td><td align="left">✗</td><td align="left">✓</td><td align="left">✓(random)</td></tr><tr><td align="left">Illumination configuration</td><td align="left">✗</td><td align="left">✗</td><td align="left">✗</td><td align="left">✓</td></tr><tr><td align="left">Fiber virtual dissect</td><td align="left">✓</td><td align="left">✗</td><td align="left">✓</td><td align="left">✗</td></tr><tr><td align="left">Fiber tractography</td><td align="left">✓</td><td align="left">✗</td><td align="left">✗</td><td align="left">✗</td></tr><tr><td align="left">Fiber sampling</td><td align="left">✗</td><td align="left">✗</td><td align="left">✓</td><td align="left">✓</td></tr><tr><td align="left">Fiber length filter</td><td align="left">✗</td><td align="left">✗</td><td align="left">✓</td><td align="left">✗</td></tr><tr><td align="left">Structure info</td><td align="left">✗</td><td align="left">✓</td><td align="left">✓</td><td align="left">✓</td></tr></tbody></table><table-wrap-foot><p>A comparison table between ABrainVis and the most similar applications in the literature. The presence (✓) or absence (✗) of different visualization features is evaluated.</p></table-wrap-foot></table-wrap></p>
    <sec id="Sec14">
      <title>Datasets</title>
      <p id="Par60">Four datasets were used to display all the different types of data supported by ABrainVis. The same data is used to carry out the performance tests.</p>
      <sec id="Sec15">
        <title>Dataset I</title>
        <p id="Par61">This is a tractography dataset (<italic>.bundles</italic>) that represents a superficial white matter bundle atlas [<xref ref-type="bibr" rid="CR42">42</xref>]. It is composed of 100 short association bundles, with a total of 10,622 fibers.</p>
      </sec>
      <sec id="Sec16">
        <title>Dataset II</title>
        <p id="Par62">This dataset is composed of data of a subject from the ARCHI database [<xref ref-type="bibr" rid="CR43">43</xref>]. It includes a T1 MRI volume (<italic>.nii.gz</italic>), a head mesh (<italic>.mesh</italic>) and a whole-brain tractography (<italic>.bundles</italic>). The MRI image contains <inline-formula id="IEq19"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$240 \times 256 \times 160$$\end{document}</tex-math><mml:math id="M42"><mml:mrow><mml:mn>240</mml:mn><mml:mo>×</mml:mo><mml:mn>256</mml:mn><mml:mo>×</mml:mo><mml:mn>160</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq19.gif"/></alternatives></inline-formula> slices, with a <inline-formula id="IEq20"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.0 \times 1.0 \times 1.1$$\end{document}</tex-math><mml:math id="M44"><mml:mrow><mml:mn>1.0</mml:mn><mml:mo>×</mml:mo><mml:mn>1.0</mml:mn><mml:mo>×</mml:mo><mml:mn>1.1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq20.gif"/></alternatives></inline-formula> mm resolution. The head mesh is composed of 27,347 vertices conforming 54,722 triangles. The tractography file consists of 36 bundles segmented from a deep white matter bundle atlas [<xref ref-type="bibr" rid="CR7">7</xref>], using an automatic bundle identification algorithm [<xref ref-type="bibr" rid="CR8">8</xref>], with a total of 204,052 fibers, resampled with 21 equidistant points.</p>
      </sec>
      <sec id="Sec17">
        <title>Dataset III</title>
        <p id="Par63">This dataset consists of the data of a patient with a tumor. It has two MRI volumes, a T1 image with <inline-formula id="IEq21"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$256 \times 256 \times 180$$\end{document}</tex-math><mml:math id="M46"><mml:mrow><mml:mn>256</mml:mn><mml:mo>×</mml:mo><mml:mn>256</mml:mn><mml:mo>×</mml:mo><mml:mn>180</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq21.gif"/></alternatives></inline-formula> slices and a resolution of <inline-formula id="IEq22"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.0 \times 1.0 \times 1.0$$\end{document}</tex-math><mml:math id="M48"><mml:mrow><mml:mn>1.0</mml:mn><mml:mo>×</mml:mo><mml:mn>1.0</mml:mn><mml:mo>×</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq22.gif"/></alternatives></inline-formula> mm, and a segmented tumor mask, containing <inline-formula id="IEq23"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$128 \times 128 \times 74$$\end{document}</tex-math><mml:math id="M50"><mml:mrow><mml:mn>128</mml:mn><mml:mo>×</mml:mo><mml:mn>128</mml:mn><mml:mo>×</mml:mo><mml:mn>74</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq23.gif"/></alternatives></inline-formula> slices, with a resolution of <inline-formula id="IEq24"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.8 \times 1.8 \times 1.8$$\end{document}</tex-math><mml:math id="M52"><mml:mrow><mml:mn>1.8</mml:mn><mml:mo>×</mml:mo><mml:mn>1.8</mml:mn><mml:mo>×</mml:mo><mml:mn>1.8</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq24.gif"/></alternatives></inline-formula> mm (both in <italic>.nii.gz</italic> format). Also, a whole-brain tractography dataset containing 71,361 fibers is included (<italic>.bundles</italic>). This file is a resampled version of the original file, with only a 4% of the fibers. For this database, we used other tractography dataset, composed of clusters calculated from the original tractography dataset, using the FFClust algorithm [<xref ref-type="bibr" rid="CR6">6</xref>]. It is composed of all the clusters with more than 200 fibers, consisting of 314 clusters and a total of 117,519 fibers. Neuroscientists usually apply a clustering algorithm on a tractography dataset to perform an exploratory analysis to obtain a general overview of the main structures found in the tractography.</p>
      </sec>
      <sec id="Sec18">
        <title>Dataset IV</title>
        <p id="Par64">This dataset includes the data of MRI volumes, an arteries mesh, and segmented bundles of a subject. The MRI volume dimensions are <inline-formula id="IEq25"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$275 \times 332 \times 206$$\end{document}</tex-math><mml:math id="M54"><mml:mrow><mml:mn>275</mml:mn><mml:mo>×</mml:mo><mml:mn>332</mml:mn><mml:mo>×</mml:mo><mml:mn>206</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq25.gif"/></alternatives></inline-formula> with a <inline-formula id="IEq26"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0.625 \times 0.625 \times 0.625$$\end{document}</tex-math><mml:math id="M56"><mml:mrow><mml:mn>0.625</mml:mn><mml:mo>×</mml:mo><mml:mn>0.625</mml:mn><mml:mo>×</mml:mo><mml:mn>0.625</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq26.gif"/></alternatives></inline-formula> mm resolution. The images used are a skull stripped T1 image, and a mask of the segmented arteries from an MRI angiography. Also, an arteries mesh (<italic>.gii</italic>) was calculated from the mask, containing 101,123 vertices and 201,038 triangles. Tractography data are composed of 20 deep white matter bundles in separated <italic>.trk</italic> files that has been segmented using [<xref ref-type="bibr" rid="CR9">9</xref>], representing a total of 370,613 fibers.</p>
      </sec>
    </sec>
    <sec id="Sec19">
      <title>Main interface</title>
      <p id="Par65">The main interface is composed of a main display window and a “Settings Bar”, available at the left side of the screen, as shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>A. It has controls to show the “Display Settings Bar”, load files, hide all the settings bars, reset the camera configuration, show the object settings bar (see Fig. <xref rid="Fig5" ref-type="fig">5</xref>B), and toggle the bonding boxes of the objects. Also, a “Display Settings Bar” is available at the bottom of the screen, as shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. It allows a user to set the global illumination constants (see Eq. <xref rid="Equ2" ref-type="">2</xref>), as well as the material reflection parameters for each type of object (Bundle or Tractography, Mesh, MRI).<fig id="Fig5"><label>Fig. 5</label><caption><p>Main components of the application interface. <bold>A</bold> Main screen with the “Settings Bar” visible. When this bar is displayed, the main visualization window and the other setting bars are disabled and obscured. In the main visualization screen a tractography is displayed, as well as the “Display Settings Bar”. <bold>B</bold> Main screen with the “Object Settings Bar” visible, listing the three objects loaded: a tractography dataset, a head mesh and a slice from an MRI volume. Multiple objects can be selected for removing them. Only one object must be selected for displaying its settings options on the bar. In the main visualization screen, the three objects are displayed with their bounding boxes</p></caption><graphic xlink:href="12938_2021_909_Fig5_HTML" id="MO7"/></fig><fig id="Fig6"><label>Fig. 6</label><caption><p>Tractography dataset with a group of labeled bundles representing an atlas of short association bundles (Dataset I), displayed using lines, with different illumination settings. The “Display Settings Bar” is also visible at the bottom of the screen. <bold>A</bold> Default parameters for line display. <bold>B</bold> Changes in the material reflection constants (ambient <inline-formula id="IEq27"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K_{\rm a}$$\end{document}</tex-math><mml:math id="M58"><mml:msub><mml:mi>K</mml:mi><mml:mi mathvariant="normal">a</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq27.gif"/></alternatives></inline-formula>, diffuse <inline-formula id="IEq28"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K_{\mathrm{d}}$$\end{document}</tex-math><mml:math id="M60"><mml:msub><mml:mi>K</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq28.gif"/></alternatives></inline-formula> and specular <inline-formula id="IEq29"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K_{\mathrm{s}}$$\end{document}</tex-math><mml:math id="M62"><mml:msub><mml:mi>K</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq29.gif"/></alternatives></inline-formula>). <bold>C</bold> Additional change in the background color</p></caption><graphic xlink:href="12938_2021_909_Fig6_HTML" id="MO8"/></fig></p>
    </sec>
    <sec id="Sec20">
      <title>Object visualization</title>
      <p id="Par66">Once an object is loaded, it can be selected in the “Object Settings Bar” (see Fig. <xref rid="Fig5" ref-type="fig">5</xref>B), to manipulate its display settings. The settings are specific to the three object types: Mesh, MRI and Tractography. Figure <xref rid="Fig7" ref-type="fig">7</xref> shows different display options for a Mesh object, including different colors for the triangles and wireframe, and the setting of alpha value (transparency). Figure <xref rid="Fig8" ref-type="fig">8</xref> shows examples of a visualization for MRI data using volume rendering and slices. This kind of object can be any 3D volume in NIfTI format. Figure <xref rid="Fig9" ref-type="fig">9</xref> shows different display options for Tractography type, showing the effect of only ambient light component, compared to all the illumination components, and using lines or cylinder rendering. Additionally, Fig. <xref rid="Fig10" ref-type="fig">10</xref> shows the option of fiber sampling and bundle selection.<fig id="Fig7"><label>Fig. 7</label><caption><p>A mesh object of a head displayed with different visualization options (Dataset II). <bold>A</bold> Triangles and wireframe visible, with default colors, and high alpha for triangles (almost opaque). <bold>B</bold> Only triangles visible in copper color, with alpha=1.0 (totally opaque). <bold>C</bold> Triangles (in copper) and wireframe (in white) visible, with alpha=0.7 (triangles semi-transparent)</p></caption><graphic xlink:href="12938_2021_909_Fig7_HTML" id="MO9"/></fig><fig id="Fig8"><label>Fig. 8</label><caption><p>A brain MRI volume (without skull) displayed with different visualization options (Dataset IV). <bold>A</bold> Two slices, in axes “<italic>X</italic>” and “<italic>Y</italic>” of the volume, and a volume rendering of the image with maximum alpha, visualizing the cortical surface. <bold>B</bold> Only “<italic>Z</italic>” axis slice, and the volume rendering with alpha = 0.3 (semi-transparent). <bold>C</bold> Only two slices displayed, in axes “<italic>Y</italic>” and “<italic>Z</italic>”</p></caption><graphic xlink:href="12938_2021_909_Fig8_HTML" id="MO10"/></fig><fig id="Fig9"><label>Fig. 9</label><caption><p>A tractography dataset with a group of segmented bundles (Dataset II), displayed with different visualization and illumination settings. The “Display Settings Bar” is also displayed at the bottom of the screen. <bold>A</bold> Line display with only ambient illumination (<inline-formula id="IEq30"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A=1$$\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq30.gif"/></alternatives></inline-formula>, <inline-formula id="IEq31"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D=0$$\end{document}</tex-math><mml:math id="M66"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq31.gif"/></alternatives></inline-formula>, <inline-formula id="IEq32"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S=0$$\end{document}</tex-math><mml:math id="M68"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq32.gif"/></alternatives></inline-formula>). <bold>B</bold> Line rendering with high ambient, diffuse and specular illumination (<inline-formula id="IEq33"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A=0.8$$\end{document}</tex-math><mml:math id="M70"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq33.gif"/></alternatives></inline-formula>, <inline-formula id="IEq34"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D=0.9$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq34.gif"/></alternatives></inline-formula>, <inline-formula id="IEq35"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S=14$$\end{document}</tex-math><mml:math id="M74"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mn>14</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq35.gif"/></alternatives></inline-formula>). <bold>C</bold> Cylinder rendering with a non-zero material reflection coefficient for ambient illumination (<inline-formula id="IEq36"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Ka=1$$\end{document}</tex-math><mml:math id="M76"><mml:mrow><mml:mi>K</mml:mi><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq36.gif"/></alternatives></inline-formula>, <inline-formula id="IEq37"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Kd=0$$\end{document}</tex-math><mml:math id="M78"><mml:mrow><mml:mi>K</mml:mi><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq37.gif"/></alternatives></inline-formula>, <inline-formula id="IEq38"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Ks=0$$\end{document}</tex-math><mml:math id="M80"><mml:mrow><mml:mi>K</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq38.gif"/></alternatives></inline-formula>), resulting in only an ambient light component. <bold>D</bold> Cylinder display with high ambient, diffuse and specular illumination (<inline-formula id="IEq39"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A=1$$\end{document}</tex-math><mml:math id="M82"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq39.gif"/></alternatives></inline-formula>, <inline-formula id="IEq40"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D=0.9$$\end{document}</tex-math><mml:math id="M84"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq40.gif"/></alternatives></inline-formula>, <inline-formula id="IEq41"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S=0.95$$\end{document}</tex-math><mml:math id="M86"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12938_2021_909_Article_IEq41.gif"/></alternatives></inline-formula>)</p></caption><graphic xlink:href="12938_2021_909_Fig9_HTML" id="MO11"/></fig><fig id="Fig10"><label>Fig. 10</label><caption><p>The tractography dataset with a group of segmented bundles (Dataset II), displayed with different percentages of fibers. <bold>A</bold> 100%, 204,052 fibers (all the bundles selected). <bold>B</bold> 10%, only 20,405 fibers selected. <bold>C, D</bold> 100% of the fibers for selected bundles of the left hemisphere (arcuate fasciculus, corticospinal tract, inferior fronto-occipital fasciculus and cingulum), sagittal lateral (<bold>C</bold>) and internal (<bold>D</bold>) views</p></caption><graphic xlink:href="12938_2021_909_Fig10_HTML" id="MO12"/></fig></p>
      <p id="Par67">Finally, Figs. <xref rid="Fig11" ref-type="fig">11</xref>, <xref rid="Fig12" ref-type="fig">12</xref> and <xref rid="Fig13" ref-type="fig">13</xref> illustrate three case studies providing different types of visualizations supported by ABrainVis, for Datasets II, III and IV, respectively.</p>
      <p id="Par68">For all the figures, we provide dataset information and display options details in the caption of each figure.<fig id="Fig11"><label>Fig. 11</label><caption><p>Example of brain data visualization of a healthy subject, consisting of a head mesh (semi-transparent), a brain MRI and a tractography dataset with labeled bundles (1% of the fibers displayed) (Dataset II). <bold>A</bold> An axial slice of the volume, all the bundles with cylinders, and the head mesh. <bold>B</bold> A sagittal slice, all the bundles with lines, and the head mesh. <bold>C</bold> An axial slice, some selected bundles with cylinders, and the head mesh</p></caption><graphic xlink:href="12938_2021_909_Fig11_HTML" id="MO13"/></fig><fig id="Fig12"><label>Fig. 12</label><caption><p>Example of brain data of a patient with a tumor, consisting of a brain MRI and a whole-brain tractography dataset with 71,361 fibers (Dataset III). Also, the dataset contains a brain mask of the segmented tumor which is displayed using volume rendering (VR), with maximum alpha, and a tractography dataset containing the 314 larger fiber clusters. <bold>A</bold> A coronal slice of the brain image, the whole-brain tractography displayed in purple, and the VR of the tumor. <bold>B</bold> A coronal slice of the brain, the 314 clusters displayed with random colors, and the VR of the tumor. <bold>C, D</bold> The same as <bold>A</bold> and <bold>B</bold>, respectively, but with a sagittal brain slice (sagittal oblique view)</p></caption><graphic xlink:href="12938_2021_909_Fig12_HTML" id="MO14"/></fig><fig id="Fig13"><label>Fig. 13</label><caption><p>Example of brain data, including a brain MRI (skull stripped), a tractography dataset with labeled bundles, a mask of segmented arteries from MRI angiography and an arteries mesh (Dataset IV). <bold>A</bold> Coronal and axial slices of the brain image, the tractography displayed with random colors, and the arteries mesh in red. <bold>B</bold> The same slice and tractography display as in <bold>A</bold>, but with a volume rendering of the arteries mask, instead of the mesh (with maximum alpha). <bold>C, D</bold> The same as <bold>A</bold> and <bold>B</bold>, respectively, but from a sagittal oblique view</p></caption><graphic xlink:href="12938_2021_909_Fig13_HTML" id="MO15"/></fig></p>
    </sec>
    <sec id="Sec21">
      <title>Performance results</title>
      <p id="Par69">This section describes the experiments performed to evaluate the visualization time, using the cell phones and tablet devices listed in Table <xref rid="Tab3" ref-type="table">3</xref>, for the tractography of Dataset II (204,052 fibers).</p>
      <p id="Par70">The first experiment evaluates the time required for the creation of the fiber sampling data (EBO creation), for two different fiber sampling percentages (10% and 90%). Figure <xref rid="Fig14" ref-type="fig">14</xref> shows such results. As expected the creation and EBO binding time increases when the percentage of fibers considered grows, but it is still reasonably low, with a mean of about 20 seconds for 90 % of the fibers (184,068 fibers) for the medium- to high-quality devices (all the devices, excepting Tablet Samsung Tab S2, which is quite old).</p>
      <p id="Par71">
        <table-wrap id="Tab3">
          <label>Table 3</label>
          <caption>
            <p>Mobile devices main features</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left"/>
                <th align="left"/>
                <th align="left" colspan="4">Features</th>
              </tr>
              <tr>
                <th align="left"/>
                <th align="left">ID</th>
                <th align="left">Model</th>
                <th align="left">CPU</th>
                <th align="left">GPU</th>
                <th align="left">RAM</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="4">Cell phones</td>
                <td align="left">C1</td>
                <td align="left">Samsung S9+</td>
                <td align="left">Snapdragon 845</td>
                <td align="left">Adreno 630</td>
                <td align="left">6 GB</td>
              </tr>
              <tr>
                <td align="left">C2</td>
                <td align="left">Samsung S10+</td>
                <td align="left">Exynos 9820</td>
                <td align="left">Adreno 640</td>
                <td align="left">8 GB</td>
              </tr>
              <tr>
                <td align="left">C3</td>
                <td align="left">Huawei P20</td>
                <td align="left">Kirin 970</td>
                <td align="left">Mali-G72</td>
                <td align="left">4 GB</td>
              </tr>
              <tr>
                <td align="left">C4</td>
                <td align="left">Samsung A30</td>
                <td align="left">Exynos 7904</td>
                <td align="left">Mali-G71</td>
                <td align="left">3 GB</td>
              </tr>
              <tr>
                <td align="left" rowspan="2">Tablets</td>
                <td align="left">T1</td>
                <td align="left">Samsung Tab S2</td>
                <td align="left">Exynos 5433</td>
                <td align="left">Adreno 510</td>
                <td align="left">3 GB</td>
              </tr>
              <tr>
                <td align="left">T2</td>
                <td align="left">Samsung Tab A (2016)</td>
                <td align="left">Exynos 7870 Octa</td>
                <td align="left">Mali-T830</td>
                <td align="left">3 GB</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
      <p id="Par72">
        <fig id="Fig14">
          <label>Fig. 14</label>
          <caption>
            <p>Time required for the creation of the fiber sampling data (EBO creation), for two different fiber sampling percentages (10% and 90%), measured in the six devices listed in Table <xref rid="Tab3" ref-type="table">3</xref>. The tractography used has 204,052 fibers (Dataset II)</p>
          </caption>
          <graphic xlink:href="12938_2021_909_Fig14_HTML" id="MO16"/>
        </fig>
        <fig id="Fig15">
          <label>Fig. 15</label>
          <caption>
            <p>Frames per second (fps) obtained using different percentage of fibers for line and cylinder rendering, when displaying the tractography of Dataset II (204,052 fibers), measured in the six devices listed in Table <xref rid="Tab3" ref-type="table">3</xref></p>
          </caption>
          <graphic xlink:href="12938_2021_909_Fig15_HTML" id="MO17"/>
        </fig>
      </p>
      <p id="Par73">The second experiment evaluates the frames per second (fps) achieved, using different percentage of fibers for line and cylinder rendering (see Fig. <xref rid="Fig15" ref-type="fig">15</xref>). The fps decreases with the number of fibers displayed, where 60 fps is the optimal and 15 fps is the minimum to have an interactive display. As can be seen for line rendering, about 145,000 fibers can be displayed with the device of higher performance, and a mean of about 70,000 fibers can be displayed, considering all the devices, excepting the Tablet Samsung Tab S2. In the case of cylinders, the quantity of fibers that can be displayed decreases to a maximum of 13,000 fibers and a mean of about 5000 fibers. In any case, note that cylinder display is more useful when displaying few fibers.</p>
    </sec>
  </sec>
  <sec id="Sec22">
    <title>Conclusions</title>
    <p id="Par74">We present a mobile application for Android devices for the visualization of imaging data. This tool is very versatile since it supports three types of data, namely, labeled tractography datasets, 3D images and meshes. It offers numerous display options, uncommon for applications of this type. Given the massive use of mobile devices, we believe it may be of great interest to researchers, clinicians, and educators. As far as we know, this is the most complete application of this type that exists. We offer it as open source, so that the community can contribute to its development and improvement.</p>
    <p id="Par75">Future work will be focused on the addition of more visualization options. For example, the support of an affine transformation data type, that could be applied to any object, or the addition of different color maps for 3D volumes. Also, it would be useful to support a mesh label data type, for applying different colors to the mesh vertices, and be able to color different regions over a mesh. Regarding the volume rendering tool, we plan to add the support of a transfer function, for associating a different alpha value to each voxel intensity. Finally, the support of a configuration file would provide the capability to store and load display options for a group of objects, enabling a fastest display of a dataset.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Information</title>
    <sec id="Sec23">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12938_2021_909_MOESM1_ESM.pdf">
            <caption>
              <p><bold>Additional file 1: Table S1.</bold> Neuroimaging visualization tools.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="12938_2021_909_MOESM2_ESM.mp4">
            <caption>
              <p><bold>Additional file 2.</bold> Demo video file.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>CPU</term>
        <def>
          <p id="Par4">Central processing unit</p>
        </def>
      </def-item>
      <def-item>
        <term>CT</term>
        <def>
          <p id="Par5">Computer tomography</p>
        </def>
      </def-item>
      <def-item>
        <term>DAE</term>
        <def>
          <p id="Par6">Digital asset exchange file format</p>
        </def>
      </def-item>
      <def-item>
        <term>dMRI</term>
        <def>
          <p id="Par7">Diffusion magnetic resonance imaging</p>
        </def>
      </def-item>
      <def-item>
        <term>EBO</term>
        <def>
          <p id="Par8">Element buffer object</p>
        </def>
      </def-item>
      <def-item>
        <term>EEG</term>
        <def>
          <p id="Par9">Electroencephalography</p>
        </def>
      </def-item>
      <def-item>
        <term>FFClust</term>
        <def>
          <p id="Par10">Fast fiber clustering</p>
        </def>
      </def-item>
      <def-item>
        <term>fps</term>
        <def>
          <p id="Par11">Frames per second</p>
        </def>
      </def-item>
      <def-item>
        <term>FS</term>
        <def>
          <p id="Par12">Fragment Shader</p>
        </def>
      </def-item>
      <def-item>
        <term>GIfTI</term>
        <def>
          <p id="Par13">Geometry informatics technology initiative</p>
        </def>
      </def-item>
      <def-item>
        <term>GUI</term>
        <def>
          <p id="Par14">Graphic user interface</p>
        </def>
      </def-item>
      <def-item>
        <term>GPU</term>
        <def>
          <p id="Par15">Graphics processing unit</p>
        </def>
      </def-item>
      <def-item>
        <term>GS</term>
        <def>
          <p id="Par16">Geometry shader</p>
        </def>
      </def-item>
      <def-item>
        <term>iOS</term>
        <def>
          <p id="Par17">iPhone operating system</p>
        </def>
      </def-item>
      <def-item>
        <term>MRI</term>
        <def>
          <p id="Par18">Magnetic resonance imaging</p>
        </def>
      </def-item>
      <def-item>
        <term>NIfTI</term>
        <def>
          <p id="Par19">Neuroimaging informatics technology initiative</p>
        </def>
      </def-item>
      <def-item>
        <term>OBJ</term>
        <def>
          <p id="Par20">Wavefront 3D object file</p>
        </def>
      </def-item>
      <def-item>
        <term>OpenGL</term>
        <def>
          <p id="Par21">Open graphics library</p>
        </def>
      </def-item>
      <def-item>
        <term>OpenGL ES</term>
        <def>
          <p id="Par22">OpenGL for embedded systems</p>
        </def>
      </def-item>
      <def-item>
        <term>PET</term>
        <def>
          <p id="Par23">Positron emission tomography</p>
        </def>
      </def-item>
      <def-item>
        <term>RAM</term>
        <def>
          <p id="Par24">Random access memory</p>
        </def>
      </def-item>
      <def-item>
        <term>STL</term>
        <def>
          <p id="Par25">Stereolithography file</p>
        </def>
      </def-item>
      <def-item>
        <term>VBO</term>
        <def>
          <p id="Par26">Vertex buffer object</p>
        </def>
      </def-item>
      <def-item>
        <term>VR</term>
        <def>
          <p id="Par27">Volume rendering</p>
        </def>
      </def-item>
      <def-item>
        <term>VS</term>
        <def>
          <p id="Par28">Vertex shader</p>
        </def>
      </def-item>
      <def-item>
        <term>2D</term>
        <def>
          <p id="Par29">2-Dimensional</p>
        </def>
      </def-item>
      <def-item>
        <term>3D</term>
        <def>
          <p id="Par30">3-Dimensional</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors thank Claudio Román and Liset González for pre-processing the data.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author’s contributions</title>
    <p>IO developed the main visualization modules of the application, some parts of the graphical interface and executed the performance experiments. MG developed the main graphical interface and contributed to the manuscript writing. DB and DC contributed on the development of the visualization modules. MD, CP and JFM provided pre-processed data and manuscript revision. CH performed the main design and writing of the manuscript. PG directed the project, provided funding and collaborated in the writing of the manuscript. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work has received funding by ANID FONDECYT 1190701, ANID-Basal Project FB0008 (AC3E) and ANID-Basal Project FB0001 (CeBiB). This work was also partially funded by the Human Brain Project, funded from the European Union’s Horizon 2020 Framework Programme for Research and Innovation under the Specific Grant Agreements No: 945539 (HBP SGA3), No. 785907 (HBP SGA2) and No: 604102 (HBP SGA1).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The source code of the ABrainVis is available on a repository of github. Project name: ABrainVis. Project home page: <ext-link ext-link-type="uri" xlink:href="https://github.com/Cocobio/aBrainVis">https://github.com/Cocobio/aBrainVis</ext-link>. Operating system for development: Windows 10. Programming language: Java, GL Shader Language (GLSL). Other requirements for development: Android Studio 3.5 or higher, OpenGL ES 3.2, JRE 1.8.0 or higher. Mobile operating system: Android 6.0 or higher. License: GNU General Public License v3.0 (non-commercial use).</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar2">
      <title>Ethics approval and consent to participate</title>
      <p id="Par76">All the data used in this manuscript for visualization purposes were acquired in accordance with the principles stated in the Declaration of Helsinki. Prior to starting the study, ethical approval was obtained for all protocols from the local ethics committee.</p>
    </notes>
    <notes id="FPar3">
      <title>Consent for publication</title>
      <p id="Par77">Not applicable.</p>
    </notes>
    <notes id="FPar4" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par78">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Le Bihan</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Iima</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Diffusion magnetic resonance imaging: what water tells us about biological tissues</article-title>
        <source>PLOS Biol.</source>
        <year>2015</year>
        <volume>13</volume>
        <issue>7</issue>
        <fpage>1002203</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pbio.1002203</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Descoteaux</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Angelino</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Fitzgibbons</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Deriche</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Regularized, fast, and robust analytical Q-ball imaging</article-title>
        <source>Mag Reson Med.</source>
        <year>2007</year>
        <volume>58</volume>
        <issue>3</issue>
        <fpage>497</fpage>
        <lpage>510</lpage>
        <pub-id pub-id-type="doi">10.1002/mrm.21277</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yeh</surname>
            <given-names>F-C</given-names>
          </name>
          <name>
            <surname>Wedeen</surname>
            <given-names>VJ</given-names>
          </name>
          <name>
            <surname>Tseng</surname>
            <given-names>W-YI</given-names>
          </name>
        </person-group>
        <article-title>Generalized q-sampling imaging</article-title>
        <source>IEEE Trans Med Imag.</source>
        <year>2010</year>
        <volume>29</volume>
        <issue>9</issue>
        <fpage>1626</fpage>
        <lpage>1635</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2010.2045126</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guevara</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Poupon</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Rivière</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Cointepas</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Descoteaux</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Thirion</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Mangin</surname>
            <given-names>J-F</given-names>
          </name>
        </person-group>
        <article-title>Robust clustering of massive tractography datasets</article-title>
        <source>NeuroImage</source>
        <year>2011</year>
        <volume>54</volume>
        <issue>3</issue>
        <fpage>1975</fpage>
        <lpage>1993</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.10.028</pub-id>
        <pub-id pub-id-type="pmid">20965259</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Garyfallidis</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Brett</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Correia</surname>
            <given-names>MM</given-names>
          </name>
          <name>
            <surname>Williams</surname>
            <given-names>GB</given-names>
          </name>
          <name>
            <surname>Nimmo-Smith</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Quickbundles, a method for tractography simplification</article-title>
        <source>Front Neurosci.</source>
        <year>2012</year>
        <volume>6</volume>
        <fpage>175</fpage>
        <pub-id pub-id-type="doi">10.3389/fnins.2012.00175</pub-id>
        <pub-id pub-id-type="pmid">23248578</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vázquez</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>López-López</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Sánchez</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Houenou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Poupon</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Mangin</surname>
            <given-names>J-F</given-names>
          </name>
          <name>
            <surname>Hernández</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Guevara</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>FFClust: Fast fiber clustering for large tractography datasets for a detailed study of brain connectivity</article-title>
        <source>NeuroImage</source>
        <year>2020</year>
        <volume>220</volume>
        <fpage>117070</fpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117070</pub-id>
        <pub-id pub-id-type="pmid">32599269</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guevara</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Duclap</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Poupon</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Marrakchi-Kacem</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Fillard</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Le Bihan</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Leboyer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Houenou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Mangin</surname>
            <given-names>J-F</given-names>
          </name>
        </person-group>
        <article-title>Automatic fiber bundle segmentation in massive tractography datasets using a multi-subject bundle atlas</article-title>
        <source>NeuroImage</source>
        <year>2012</year>
        <volume>61</volume>
        <issue>4</issue>
        <fpage>1083</fpage>
        <lpage>1099</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.071</pub-id>
        <pub-id pub-id-type="pmid">22414992</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Labra</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Guevara</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Duclap</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Houenou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Poupon</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Mangin</surname>
            <given-names>J-F</given-names>
          </name>
          <name>
            <surname>Figueroa</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Fast automatic segmentation of white matter streamlines based on a multi-subject bundle atlas</article-title>
        <source>Neuroinformatics</source>
        <year>2017</year>
        <volume>15</volume>
        <issue>1</issue>
        <fpage>71</fpage>
        <lpage>86</lpage>
        <pub-id pub-id-type="doi">10.1007/s12021-016-9316-7</pub-id>
        <pub-id pub-id-type="pmid">27722821</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Garyfallidis</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Côté</surname>
            <given-names>M-A</given-names>
          </name>
          <name>
            <surname>Rheault</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Sidhu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hau</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Petit</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Fortin</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Cunanne</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Descoteaux</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Recognition of white matter bundles using local and global streamline-based registration and clustering</article-title>
        <source>NeuroImage</source>
        <year>2018</year>
        <volume>170</volume>
        <fpage>283</fpage>
        <lpage>295</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.07.015</pub-id>
        <pub-id pub-id-type="pmid">28712994</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Norton</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Essayed</surname>
            <given-names>WI</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Pujol</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Yarmarkovich</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Golby</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Kindlmann</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Wassermann</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Estepar</surname>
            <given-names>RSJ</given-names>
          </name>
          <name>
            <surname>Rathi</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SlicerDMRI: open source diffusion MRI software for brain cancer research</article-title>
        <source>Cancer Res.</source>
        <year>2017</year>
        <volume>77</volume>
        <issue>21</issue>
        <fpage>101</fpage>
        <lpage>103</lpage>
        <pub-id pub-id-type="doi">10.1158/0008-5472.CAN-17-0332</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>A fast 3D brain extraction and visualization framework using active contour and modern OpenGL pipelines</article-title>
        <source>IEEE Access</source>
        <year>2019</year>
        <volume>7</volume>
        <fpage>156097</fpage>
        <lpage>156109</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2019.2948621</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Muschelli</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Sweeney</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Crainiceanu</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>BrainR: interactive 3 and 4D images of high resolution neuroimage data</article-title>
        <source>The R J.</source>
        <year>2014</year>
        <volume>6</volume>
        <issue>1</issue>
        <fpage>41</fpage>
        <lpage>48</lpage>
        <pub-id pub-id-type="doi">10.32614/RJ-2014-004</pub-id>
        <pub-id pub-id-type="pmid">27330829</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Heuer</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Ghosh</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sterling</surname>
            <given-names>AR</given-names>
          </name>
          <name>
            <surname>Toro</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Open neuroimaging laboratory</article-title>
        <source>Res Ideas Outcomes.</source>
        <year>2016</year>
        <volume>2</volume>
        <fpage>9113</fpage>
        <pub-id pub-id-type="doi">10.3897/rio.2.e9113</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ledoux</surname>
            <given-names>L-P</given-names>
          </name>
          <name>
            <surname>Morency</surname>
            <given-names>FC</given-names>
          </name>
          <name>
            <surname>Cousineau</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Houde</surname>
            <given-names>J-C</given-names>
          </name>
          <name>
            <surname>Whittingstall</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Descoteaux</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Fiberweb: diffusion visualization and processing in the browser</article-title>
        <source>Front Neuroinformat</source>
        <year>2017</year>
        <volume>11</volume>
        <fpage>54</fpage>
        <pub-id pub-id-type="doi">10.3389/fninf.2017.00054</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>MK</given-names>
          </name>
          <name>
            <surname>Nicolini</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Waxenegger</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Galloway</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Ullmann</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Janke</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Interpretation of medical imaging data with a mobile application: a mobile digital imaging processing environment</article-title>
        <source>Front Neurol.</source>
        <year>2013</year>
        <volume>4</volume>
        <fpage>85</fpage>
        <pub-id pub-id-type="doi">10.3389/fneur.2013.00085</pub-id>
        <pub-id pub-id-type="pmid">23847587</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Brain Tutor: Android. <ext-link ext-link-type="uri" xlink:href="https://www.brainvoyager.com/Mobile/BrainTutor3D_Android.html">https://www.brainvoyager.com/Mobile/BrainTutor3D_Android.html</ext-link>. [Online; Accessed 28 Dec 2020] (2020).</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Atlas of MRI Brain Anatomy. <ext-link ext-link-type="uri" xlink:href="https://play.google.com/store/apps/details?id=com.appsclinical.atlasofmribrainanatomydraft">https://play.google.com/store/apps/details?id=com.appsclinical.atlasofmribrainanatomydraft</ext-link>. [Online; Accessed 28 Dec 2020] (2020).</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Minkowitz</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Review of “brain mri atlas” app for the ipad</article-title>
        <source>J Dig Imaging.</source>
        <year>2015</year>
        <volume>28</volume>
        <issue>6</issue>
        <fpage>633</fpage>
        <lpage>635</lpage>
        <pub-id pub-id-type="doi">10.1007/s10278-015-9827-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">NeuroNavigator. <ext-link ext-link-type="uri" xlink:href="https://play.google.com/store/apps/details?id=com.russ.fiber_visualizer&amp;hl=es_CL&amp;gl=US">https://play.google.com/store/apps/details?id=com.russ.fiber_visualizer&amp;hl=es_CL&amp;gl=US</ext-link>. [Online; Accessed 28 Dec 2020] (2020).</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">MRI Viewer. <ext-link ext-link-type="uri" xlink:href="https://play.google.com/store/apps/details?id=mdtoolkit.mriviewer">https://play.google.com/store/apps/details?id=mdtoolkit.mriviewer</ext-link>. [Online; Accessed 28 Dec 2020] (2020).</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">CT Scan Cross Sectional Anatomy for Imaging Pros. <ext-link ext-link-type="uri" xlink:href="https://play.google.com/store/apps/details?id=com.andromo.dev658544.app1004140">https://play.google.com/store/apps/details?id=com.andromo.dev658544.app1004140</ext-link>. [Online; Accessed 28 Dec 2020] (2020).</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Radiological Anatomy For FRCR1. <ext-link ext-link-type="uri" xlink:href="https://play.google.com/store/apps/details?id=com.radrevision.frcr1anatomyrevisionapp">https://play.google.com/store/apps/details?id=com.radrevision.frcr1anatomyrevisionapp</ext-link>. [Online; Accessed 28 Dec 2020] (2020).</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Imaging Brain, Skull &amp; Craniocervical Vasculature. <ext-link ext-link-type="uri" xlink:href="https://play.google.com/store/apps/details?id=com.andromo.dev658544.app1004162">https://play.google.com/store/apps/details?id=com.andromo.dev658544.app1004162</ext-link>. [Online; Accessed 28 Dec 2020] (2020).</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">NeuroSlice. <ext-link ext-link-type="uri" xlink:href="https://play.google.com/store/apps/details?id=org.homphysiology.neuroslice">https://play.google.com/store/apps/details?id=org.homphysiology.neuroslice</ext-link>. [Online; Accessed 28 Dec 2020] (2020).</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Myelination Brain. <ext-link ext-link-type="uri" xlink:href="https://play.google.com/store/apps/details?id=com.drb.brains&amp;hl=es_CL&amp;gl=US">https://play.google.com/store/apps/details?id=com.drb.brains&amp;hl=es_CL&amp;gl=US</ext-link>. [Online; Accessed 28 Dec 2020] (2020).</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dogan</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Eroglu</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Ozgural</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Al-Beyati</surname>
            <given-names>ES</given-names>
          </name>
          <name>
            <surname>Kilinc</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Comert</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bozkurt</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Visualization of superficial cerebral lesions using a smartphone application</article-title>
        <source>Turk Neurosurg</source>
        <year>2018</year>
        <volume>28</volume>
        <issue>3</issue>
        <fpage>349</fpage>
        <lpage>355</lpage>
        <pub-id pub-id-type="pmid">29105725</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rojas</surname>
            <given-names>GM</given-names>
          </name>
          <name>
            <surname>Fuentes</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Gálvez</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Mobile device applications for the visualization of functional connectivity networks and EEG electrodes: iBraiN and iBraiNEEG</article-title>
        <source>Front Neuroinformat.</source>
        <year>2016</year>
        <volume>10</volume>
        <fpage>40</fpage>
        <pub-id pub-id-type="doi">10.3389/fninf.2016.00040</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">mRay. <ext-link ext-link-type="uri" xlink:href="https://play.google.com/store/apps/details?id=org.mes">https://play.google.com/store/apps/details?id=org.mes</ext-link>. [Online; Accessed 28 Dec 2020] (2020).</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">IMAIOS Dicom Viewer. <ext-link ext-link-type="uri" xlink:href="https://play.google.com/store/apps/details?id=com.imaios.imaiosdicomviewer">https://play.google.com/store/apps/details?id=com.imaios.imaiosdicomviewer</ext-link>. [Online; Accessed 28 Dec 2020] (2020).</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">3D Model Viewer - OBJ/STL/DAE. <ext-link ext-link-type="uri" xlink:href="https://play.google.com/store/apps/details?id=com.shyambarange.viewer3d">https://play.google.com/store/apps/details?id=com.shyambarange.viewer3d</ext-link>. [Online; Accessed 28 Dec 2020] (2020).</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">3D Model Viewer. <ext-link ext-link-type="uri" xlink:href="https://play.google.com/store/apps/details?id=com.dmitrybrant.modelviewer">https://play.google.com/store/apps/details?id=com.dmitrybrant.modelviewer</ext-link>. [Online; Accessed 28 Dec 2020] (2020).</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Guevara M, Osorio I, Bonometti D, Duclap D, Poupon C, Mangin J, Guevara P. iFiber: A brain tract visualizer for android devices. In: 2015 CHILEAN Conference on Electrical, Electronics Engineering, Information and Communication Technologies (CHILECON), 2015; 245–250. IEEE.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Shreiner D, Sellers G, Kessenich J, Licea-kane B. OpenGL Programming Guide: The Official Guide to Learning OpenGL, Versions 4.3. Addison-Wesley Professional, USA, 2013.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">TrackVis. <ext-link ext-link-type="uri" xlink:href="http://trackvis.org/docs/?subsect=fileformat">http://trackvis.org/docs/?subsect=fileformat</ext-link>. [Online; Accessed 27 Dec 2020] (2020).</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">BrainVisa: bundles format. <ext-link ext-link-type="uri" xlink:href="http://brainvisa.info/anatomist-4.6/user_doc/anatomist_manual1.html?highlight=bundles">http://brainvisa.info/anatomist-4.6/user_doc/anatomist_manual1.html?highlight=bundles</ext-link>. [Online; Accessed 27 Dec 2020] (2020).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">NIfTI. <ext-link ext-link-type="uri" xlink:href="https://nifti.nimh.nih.gov/">https://nifti.nimh.nih.gov/</ext-link>. [Online; Accessed 27 Dec 2020] (2020).</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Hadwiger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kniss</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Rezk-salama</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Weiskopf</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Engel</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <source>Real-Time Volume Graphics</source>
        <year>2006</year>
        <publisher-loc>USA</publisher-loc>
        <publisher-name>A. K. Peters Ltd</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Otsu</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>A threshold selection method from gray-level histograms</article-title>
        <source>IEEE Trans Syst Man Cybernet.</source>
        <year>1979</year>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>62</fpage>
        <lpage>66</lpage>
        <pub-id pub-id-type="doi">10.1109/TSMC.1979.4310076</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Phong</surname>
            <given-names>BT</given-names>
          </name>
        </person-group>
        <article-title>Illumination for computer generated pictures</article-title>
        <source>Commun ACM.</source>
        <year>1975</year>
        <volume>18</volume>
        <issue>6</issue>
        <fpage>311</fpage>
        <lpage>317</lpage>
        <pub-id pub-id-type="doi">10.1145/360825.360839</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">GIfTI. <ext-link ext-link-type="uri" xlink:href="https://surfer.nmr.mgh.harvard.edu/fswiki/GIfTI">https://surfer.nmr.mgh.harvard.edu/fswiki/GIfTI</ext-link>. [Online; Accessed 27 Dec 2020] (2020).</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">BrainVisa: mesh format. <ext-link ext-link-type="uri" xlink:href="http://brainvisa.info/aimsdata-4.6/user_doc/formats.html">http://brainvisa.info/aimsdata-4.6/user_doc/formats.html</ext-link>. [Online; Accessed 27 Dec 2020] (2020).</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guevara</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Román</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Houenou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Duclap</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Poupon</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Mangin</surname>
            <given-names>JF</given-names>
          </name>
          <name>
            <surname>Guevara</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Reproducibility of superficial white matter tracts using diffusion-weighted imaging tractography</article-title>
        <source>NeuroImage</source>
        <year>2017</year>
        <volume>147</volume>
        <fpage>703</fpage>
        <lpage>725</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.11.066</pub-id>
        <pub-id pub-id-type="pmid">28034765</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schmitt</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Lebois</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Duclap</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Guevara</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Poupon</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Rivière</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Cointepas</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>LeBihan</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Mangin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Poupon</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>CONNECT/ARCHI: an open database to infer atlases of the human brain connectivity</article-title>
        <source>ESMRMB</source>
        <year>2012</year>
        <volume>272</volume>
        <fpage>2012</fpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
