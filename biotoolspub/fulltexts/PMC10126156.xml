<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10126156</article-id>
    <article-id pub-id-type="pmid">37095168</article-id>
    <article-id pub-id-type="publisher-id">33781</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-023-33781-0</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>A ready-to-use machine learning tool for symmetric multi-modality registration of brain MRI</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Iglesias</surname>
          <given-names>Juan Eugenio</given-names>
        </name>
        <address>
          <email>jiglesiasgonzalez@mgh.harvard.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.32224.35</institution-id><institution-id institution-id-type="ISNI">0000 0004 0386 9924</institution-id><institution>Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital and Harvard Medical School, </institution></institution-wrap>Boston, 02129 USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.83440.3b</institution-id><institution-id institution-id-type="ISNI">0000000121901201</institution-id><institution>Department of Medical Physics and Biomedical Engineering, </institution><institution>University College London, </institution></institution-wrap>London, WC1V 6LJ UK </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.116068.8</institution-id><institution-id institution-id-type="ISNI">0000 0001 2341 2786</institution-id><institution>Computer Science and Artificial Intelligence Laboratory, </institution><institution>Massachusetts Institute of Technology, </institution></institution-wrap>Boston, 02139 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>24</day>
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>24</day>
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>13</volume>
    <elocation-id>6657</elocation-id>
    <history>
      <date date-type="received">
        <day>5</day>
        <month>1</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>19</day>
        <month>4</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Volumetric registration of brain MRI is routinely used in human neuroimaging, e.g., to align different MRI modalities, to measure change in longitudinal analysis, to map an individual to a template, or in registration-based segmentation. Classical registration techniques based on numerical optimization have been very successful in this domain, and are implemented in widespread software suites like ANTs, Elastix, NiftyReg, or DARTEL. Over the last 7–8 years, learning-based techniques have emerged, which have a number of advantages like high computational efficiency, potential for higher accuracy, easy integration of supervision, and the ability to be part of a meta-architectures. However, their adoption in neuroimaging pipelines has so far been almost inexistent. Reasons include: lack of robustness to changes in MRI modality and resolution; lack of robust affine registration modules; lack of (guaranteed) symmetry; and, at a more practical level, the requirement of deep learning expertise that may be lacking at neuroimaging research sites. Here, we present <italic>EasyReg</italic>, an open-source, learning-based registration tool that can be easily used from the command line without any deep learning expertise or specific hardware. <italic>EasyReg</italic> combines the features of classical registration tools, the capabilities of modern deep learning methods, and the robustness to changes in MRI modality and resolution provided by our recent work in domain randomization. As a result, <italic>EasyReg</italic> is: fast; symmetric; diffeomorphic (and thus invertible); agnostic to MRI modality and resolution; compatible with affine and nonlinear registration; and does not require any preprocessing or parameter tuning. We present results on challenging registration tasks, showing that <italic>EasyReg</italic> is as accurate as classical methods when registering 1 mm isotropic scans within MRI modality, but much more accurate across modalities and resolutions. <italic>EasyReg</italic> is publicly available as part of FreeSurfer; see <ext-link ext-link-type="uri" xlink:href="https://surfer.nmr.mgh.harvard.edu/fswiki/EasyReg">https://surfer.nmr.mgh.harvard.edu/fswiki/EasyReg</ext-link>.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Software</kwd>
      <kwd>Biomedical engineering</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id>
            <institution>National Institutes of Health</institution>
          </institution-wrap>
        </funding-source>
        <award-id>1RF1-MH123195</award-id>
        <principal-award-recipient>
          <name>
            <surname>Iglesias</surname>
            <given-names>Juan Eugenio</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002283</institution-id>
            <institution>Alzheimer’s Research UK</institution>
          </institution-wrap>
        </funding-source>
        <award-id>ARUK-IRG2019A-003</award-id>
        <principal-award-recipient>
          <name>
            <surname>Iglesias</surname>
            <given-names>Juan Eugenio</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2023</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <sec id="Sec2">
      <title>Background</title>
      <p id="Par2">Image registration is the problem of finding a spatial correspondence between two or more images, i.e., finding a spatial transform that aligns them. Registration of 2D images was initially driven by the computer vision community, with the objective to align photographs of the same scene acquired at different times, viewpoints, or sensors<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Soon after, the medical imaging community started building on these tools to develop a huge body of methods that adapted to the peculiarities of medical images, such as the anisotropic resolution, modality-specific artifacts, or the higher dimensionality of the images. The great majority of these techniques were based on numerical optimization of an objective function estimating the similarity between the reference and deformed images—where the latter is a function of the deformation parameters that one optimizes<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Throughout the rest of this article, we refer to these methods as “classical methods”—as opposed to modern learning-based methods.</p>
      <p id="Par3">In human neuroimaging with brain MRI, medical image registration has been widely used. Barring large age gaps or presence of pathology that greatly distorts brain anatomy (e.g., tumors, very strong atrophy), volumetric registration methods generally succeed at aligning subcortical brain structures; we note that surface methods are instead used for the cortex, due to its convoluted geometry<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. There is an abundance of publicly available methods that can accurately register brain MRI scans of the same contrast at isotropic resolution, such as those implemented in packages like ANTs<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, NiftyReg<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>, Elastix<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>, or DARTEL<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. These methods are used routinely in neuroimaging, e.g., in radiation therapy planning<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, registration-based segmentation<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, longitudinal analysis<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, estimating change across timepoints<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, or to bring subjects to a common coordinate frame for statistical analysis<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>.</p>
      <p id="Par4">Classical registration methods build on well-established mathematical tools like numerical optimization<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR14">14</xref></sup> or Lie algebra<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR15">15</xref></sup>, which equip them with properties that are desirable when spatially mapping brain images. For example, smooth and invertible deformation fields that preserve topology can be obtained with diffeomorphic models like the Log-Euclidean framework<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> or large deformation diffeomorphic metric mapping (LDDMM<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>). Symmetric frameworks measure image similarity in both registration directions or in a mid space while enforcing inverse consistency<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, thus ensuring that switching the order of the inputs produces the inverse transform as a result. General-purpose objective functions can deal with artifacts or differences in intensity profiles; for example, the local normalized cross correlation function is robust against bias fields<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, and information theory metrics like mutual information can map intensities across modalities<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>.</p>
      <p id="Par5">Learning-based registration methods emerged in the mid 2010s as part of the deep learning revolution. Early learning-based methods used direct voxel-wise supervision on the deformation fields, with ground truth obtained synthetically<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> or with classical methods<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. These methods were superseded by unsupervised approaches using objective functions similar to those used by classical approaches<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR22">22</xref></sup>. These unsupervised methods yield higher performance because they avoid overfitting to ground truth fields in flat image regions. Learning-based methods are not only much faster than classical approaches (one or two orders of magnitude faster), but also compatible with features that classical methods do not support. For example, they can be supervised with landmarks or volumetric segmentations, to achieve higher accuracy than classical methods<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. This supervision can also be used to train neural networks that register images of different contrast much more accurately than mutual information<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. Another advantage of learning-based registration is that it can be used as a building block for more complex-meta architectures. For example, we used it to guide contrast synthesis between MRI and histology<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>.</p>
      <p id="Par6">Despite all its potential advantages, the adoption of learning-based registration in neuroimaging pipelines has so far been nearly inexistent. We believe that there are several reasons contributing to this lack of adoption. First, the lack of robustness to changes in contrast and resolution. Convolutional neural networks (CNNs) used in registration are generally fragile to changes in MRI modality, even with aggressive augmentation<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. Another reason is the lack of robust affine modules in registration architectures. While affine registration layers do exist<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>, their robustness has not been widely demonstrated in publicly available software. Other minor reasons are the lack of symmetry and inverse consistency in most learning-based approaches, as well as the lack of publicly available implementations that do not require expertise in deep learning or complex pipelines—which precludes routine use at many neuroimaging research sites.</p>
    </sec>
    <sec id="Sec3">
      <title>Contribution</title>
      <p id="Par7">In this article, we present <italic>EasyReg</italic>, a deep learning registration tool for brain MRI scans that combines the features of classical registration methods with the speed and other advantages of learning-based methods, while being straightforward to use from the command line. Specifically our new method:<list list-type="bullet"><list-item><p id="Par8">Capitalizes on our recent work on domain randomization (SynthMorph<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>) to be agnostic to MRI modality and resolution, i.e., it can register two images with different orientations (coronal, axial, sagittal), slice spacings, and pulse sequences; the deformation fields are always computed at 1 mm isotropic resolution, independently of the resolutions of the input images. We note that <italic>EasyReg</italic> is, along with SynthMorph, the only deep learning method that can register brain scans of any MRI contrast “out of the box”, without retraining—since CNNs trained with mutual information on real scans (e.g., T1- and T2-weighted) will not generalize to other modalities (e.g., proton density and FLAIR) without retraining.</p></list-item><list-item><p id="Par9">Combines with our recent domain-agnostic segmentation and parcellation CNN<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> to analytically compute an affine transform to atlas space using the centroids of the parcels, while skull stripping the scans.</p></list-item><list-item><p id="Par10">Parameterizes the deformation as a stationary velocity field (SVF<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>), which provides diffeomorphic fields that preserve topology and can be easily inverted.</p></list-item><list-item><p id="Par11">Guarantees symmetry and inverse consistency by construction.</p></list-item><list-item><p id="Par12">Has no parameters to tune.</p></list-item><list-item><p id="Par13">Runs in about a minute on a modern CPU.</p></list-item><list-item><p id="Par14">Once FreeSurfer has been installed, <italic>EasyReg</italic> can be used directly from the command line, without the need to set up virtual environments, install dependencies, or preprocess the scans (e.g., skull stripping, or linear registration to a reference space are not needed).</p></list-item></list></p>
    </sec>
    <sec id="Sec4">
      <title>Further related work</title>
      <p id="Par15">Even though they only capture a limited spectrum of diffeomorphisms, SVFs<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> have been very popular in classical image registration<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup> thanks to their computational efficiency: an SVF can be quickly integrated into a (diffeomorphic) deformation field with the scale and square algorithm<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. The SVF is typically non-parametric, but can also be parameterized, e.g., with B-splines<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. While the inverse field can be easily computed simply by integrating the negated SVF, symmetry is not guaranteed by the sole use of SVFs. Symmetric algorithms can be obtained by computing the similarity metric in the space of both input images or at the midpoint of the flow between them<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. An example of the latter is the widespread ANTs package<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>.</p>
      <p id="Par16">Inter-modality registration of brain MRI relies almost exclusively on information theory metrics like mutual information<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>—even though correlation metrics can sometimes be used if the relationship of the intensities of the two modalities is approximately linear. While mutual information has been very successful in rigid and affine registration, its excessive flexibility makes its application to inter-modality registration ill posed<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. Moreover, possible differences in resolution between the two input scans are (to the best of our knowledge) generally disregarded.</p>
      <p id="Par17">Learning-based registration methods have successfully implemented diffeomorphic models by integrating SVFs with vector field exponentiation layers<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup>—which can be achieved by unrolling the scale and square algorithm. Differences in MRI modality and resolution can be effectively handled with supervision, leading to higher accuracy than classical methods<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. This is achieved by training neural networks with losses that exploit additional high-resolution images or anatomical labels that are available during training but <italic>not</italic> as test time. This way, the network can effectively learn the features that best align the underlying labels (or high-resolution images) without having access to them<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>.</p>
      <p id="Par18">Since symmetry can reduce systematic bias caused by the order of input images and increase robustness, learning-based registration approaches have sought to incorporate this features. A number of works have attempted to promote symmetry by adding inverse-consistency losses<sup><xref ref-type="bibr" rid="CR34">34</xref>–<xref ref-type="bibr" rid="CR41">41</xref></sup>. These losses are generally based on computing two deformation fields (one with the order of the images switched), composing them, and penalizing the deviation from identity. Some works also deform the images twice with both fields and penalize deviations from the original images, to further encourage cycle consistency<sup><xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR43">43</xref></sup>. However, these approaches do not guarantee inverse consistency at test time, when a new pair of images is fed to the neural network.</p>
      <p id="Par19">Affine registration can be implemented as a regression layer in learning-based approaches, but often fails when the inputs are not already roughly aligned<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. An alternative to regression is to pose affine registration as a keypoint matching problem. This approach is common in the classical computer vision literature (best represented by the SIFT algorithm<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>), which has evolved from matching handcrafted features to capitalizing on features obtained with deep learning approaches<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>. A first application of this keypoint matching to brain MRI was proposed in<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>. Rather than keypoints, our proposed approach builds on the idea of aligning segmentations, which is and old idea in the classical registration literature (see, e.g.,<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>) and also been explored in the deep learning registration literature (e.g.,<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>). To the best of our knowledge, no affine registration method for brain MRI is publicly available.<fig id="Fig1"><label>Figure 1</label><caption><p>Overview of the training procedure used by <italic>EasyReg</italic>. The gray arrows follow the generator; the blue arrows follow the layers of the neural network; and the green blocks represent the different terms of the loss. We emphasize that the segmentations are used to compute the loss, but are not given as input to the CNN.</p></caption><graphic xlink:href="41598_2023_33781_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec5">
    <title>Methods</title>
    <sec id="Sec6">
      <title>Preliminaries</title>
      <p id="Par20">Pair-wise registration involves two images: fixed (also known as reference or target) and moving (also know as floating or source). The goal is to find a geometric transform that maximizes the similarity of the fixed image and the deformed moving image. The transform is usually parameterized by a vector of parameters <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{p}$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq1.gif"/></alternatives></inline-formula>, and registration posed as an optimization problem:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\hat{p}} = \mathop {\text{ argmax }}\limits _{\varvec{p}} {\mathscr {S}}{[}F(\varvec{x}), M(\varvec{T}(\varvec{x}; \varvec{p})){]} + {\mathscr {R}} {[}\varvec{T}(\varvec{x}; \varvec{p}){]}, \end{aligned}$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>argmax</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:munder><mml:mi mathvariant="script">S</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="script">R</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2023_33781_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{x}$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq2.gif"/></alternatives></inline-formula> is the spatial location, <italic>F</italic> is the fixed image, <italic>M</italic> is the moving image, <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{T}$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq3.gif"/></alternatives></inline-formula> is the transform, <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {S}}$$\end{document}</tex-math><mml:math id="M10"><mml:mi mathvariant="script">S</mml:mi></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq4.gif"/></alternatives></inline-formula> is a function that measures the similarity between the fixed and deformed images, and <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {R}}(\varvec{T})$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mi mathvariant="script">R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq5.gif"/></alternatives></inline-formula> is a regularizer that encourages the regularity of the transform. Equation (<xref rid="Equ1" ref-type="disp-formula">1</xref>) is typically solved with a numerical optimizer, e.g., gradient ascent.</p>
      <p id="Par21">In learning-based registration, ones instead predicts the transform directly with a neural network parameterized by its own vector of parameters <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\theta }$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq6.gif"/></alternatives></inline-formula> (the weights of the network):<disp-formula id="Equ3"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \hat{\varvec{T}}(\varvec{x}) = \varvec{h}[F(\varvec{x}), M(\varvec{x}); \varvec{\theta }]. \end{aligned}$$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2023_33781_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par22">The parameters of the network <inline-formula id="IEq7"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\theta }$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq7.gif"/></alternatives></inline-formula> are optimized during training, and are fixed at test time. The loss <inline-formula id="IEq8"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {L}}$$\end{document}</tex-math><mml:math id="M20"><mml:mi mathvariant="script">L</mml:mi></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq8.gif"/></alternatives></inline-formula> used during training depends on the type of approach. In unsupervised learning paradigms, losses are similar to those used classical registration, i.e., Eq. (<xref rid="Equ1" ref-type="disp-formula">1</xref>):<disp-formula id="Equ4"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\mathscr {L}} = \ {\mathscr {S}}(F(\varvec{x}), M(\varvec{x}) \circ \varvec{h}[F(\varvec{x}), M(\varvec{x}); \varvec{\theta }] ) + {\mathscr {R}} (\varvec{h}[F(\varvec{x}), M(\varvec{x}); \varvec{\theta }]), \end{aligned}$$\end{document}</tex-math><mml:math id="M22" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mo>=</mml:mo><mml:mspace width="4pt"/><mml:mi mathvariant="script">S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>∘</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="script">R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2023_33781_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>which can be optimized with respect to <inline-formula id="IEq9"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\theta }$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq9.gif"/></alternatives></inline-formula> using stochastic gradient descent approaches on an unlabeled training dataset: at every mini-batch, one just samples two random scans from the training data, computes the loss, and backpropagates through the neural network to update its weights <inline-formula id="IEq10"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\theta }$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq10.gif"/></alternatives></inline-formula>.</p>
    </sec>
    <sec id="Sec7">
      <title>Overview of algorithms</title>
      <p id="Par23">Our method is summarized in Figs. <xref rid="Fig1" ref-type="fig">1</xref> and <xref rid="Fig2" ref-type="fig">2</xref>, which display the pipelines used for training the CNN and testing on a pair of input scans, respectively.</p>
      <p id="Par24">Training relies on a large dataset of 3D segmentations of brain structures, which are affinely pre-aligned to a reference space with fixed orientation and voxel dimensions; we used the MNI template<sup><xref ref-type="bibr" rid="CR49">49</xref></sup> with identity voxel-to-world coordinate transform matrix. Working in MNI space has two advantages. First, affine registration (for which robust learning methods do not exist) does not need to be modeled by the CNN; instead, we handle it with a geometric module based on centroids of brain structures (see below). And second, the CNN only needs to learn to register images with a consistent orientation, which makes the learning procedure easier as the CNN does not need to learn to estimate pose.</p>
      <p id="Par25">At each training iteration, a mini-batch consisting of two segmentations is drawn from the training dataset. These segmentations are used to synthesize two skull-stripped scans with random contrast and resolution. The two synthetic images are fed to a CNN that estimates a stationary velocity field in a symmetric fashion (i.e., switching the orders returns the negated field). This SVF and its negated version are integrated in order to obtain the forward and backward deformation fields, which are used to warp the labels. The training loss consists of a similarity term and a regularizer. The former is the Dice overlap between the original and deformed labels, in both fixed an moving image space; the latter penalizes the irregularity of the deformation field—specifically, the square of the magnitude of its spatial gradient.</p>
      <p id="Par26">At test time, the input scans (Fig. <xref rid="Fig2" ref-type="fig">2</xref>a) go through a segmentation and parcellation layer that is also trained with synthetic data to be agnostic to MRI modality and resolution<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> (Fig. <xref rid="Fig2" ref-type="fig">2</xref>b). The centroids of the segmented structures are then used to compute an affine registration to MNI space, which is estimated with a simple least squares fit; the centroids for the MNI atlas are precomputed (Fig. <xref rid="Fig2" ref-type="fig">2</xref>c). We note that this step is differentiable, so it enables the use of the whole architecture in training of more complex meta-architectures. The input scans are then skull stripped with the union of the segmentation labels and linearly deformed to our reference space (Fig. <xref rid="Fig2" ref-type="fig">2</xref>d). This deformation resamples the images to a 1 mm isotropic grid, irrespective of their original resolution. At that point, the resampled images can be fed to the trained CNN, which expects 1 mm isotropic inputs, and produces the forward and backward nonlinear fields (again, at 1 mm resolution). These fields are finally concatenated with the affine transforms to obtain the final forward and backward transforms between the two native spaces.<fig id="Fig2"><label>Figure 2</label><caption><p>Overview of the inference procedure used by <italic>EasyReg</italic>, using a 5 mm axial FLAIR scan and a 1 mm isotropic MPRAGE as sample inputs. (<bold>a</bold>) Sagittal slices of the input scans. (<bold>b</bold>) Output of segmentation layers from<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, which include subcortical structures and cortical parcels. (<bold>c</bold>) The centroids of the different labels are used to compute an affine transform to MNI space using least squares regression; the centroids of the MNI atlas are precomputed. (<bold>d</bold>) The transforms are used to bring the two (skull-stripped) scans into the voxel space of the MNI reference, which is the space where the CNN was trained. (<bold>e</bold>) The CNN is used to compute the forward and backward deformation fields, which are composed with the affine transforms in order to obtain the final fields and finally deform the original images.</p></caption><graphic xlink:href="41598_2023_33781_Fig2_HTML" id="MO5"/></fig></p>
    </sec>
    <sec id="Sec8">
      <title>Training</title>
      <sec id="Sec9">
        <title>Training dataset</title>
        <p id="Par27">To train <italic>EasyReg</italic>, we need a large pool of 3D segmentations affinely registered to a reference space. Crucially, these segmentations do not need to be manual: since synthetic images will be generated from them, the alignment between images and labels will be perfect by construction. In practice, we obtained the training data as follows.</p>
        <p id="Par28">First, we defined the reference space. We used the symmetric version of 2009 MNI template<sup><xref ref-type="bibr" rid="CR49">49</xref></sup> at 1 mm resolution, which we reoriented and shifted such that the voxel-to-world transform matrix was the identity matrix. We processed this template with SynthSeg<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> in order to obtain segmentations for 97 regions of interest (ROIs): 68 cortical and 29 subcortical. The centroids of these ROIs were computed and saved for later use during inference. SynthSeg also provided an intracranial mask, which was used to skull strip the template.</p>
        <p id="Par29">Next, we compiled a set of 1000 1 mm isotropic MPRAGE scans from two publicly available datasets: 500 randomly selected from ADNI<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> and 500 randomly selected from HCP<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. These two datasets complement each other well; ADNI mostly comprises elderly subjects (including controls and cases with strong Alzheimer-related atrophy and white matter lesions), whereas HCP consists of younger controls. These scans were skull stripped with SynthStrip<sup><xref ref-type="bibr" rid="CR52">52</xref></sup> and affinely registered to the stripped MNI template with a block matching approach implemented in NiftyReg<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. Finally, the registered scans were segmented with SynthSeg, without subdividing the cortex into parcels - since parcels are not needed to synthesize the gray matter and are very difficult to register volumetrically.</p>
        <p id="Par30">We note that:<list list-type="bullet"><list-item><p id="Par31">We register and segment, in that order (i.e., as opposed to segmenting in native space and deforming the segmentation), in order to avoid resampling segmentations with nearest neighbors.</p></list-item><list-item><p id="Par32">The described preprocessing is only needed for the training dataset; running SynthStrip or registering with NiftyReg is not required at test time (see Section “<xref rid="Sec15" ref-type="sec">Registration</xref>” below).</p></list-item><list-item><p id="Par33">Our pool of 1000 cases yields almost half a million unique pairs of 3D segmentations for training.</p></list-item></list></p>
      </sec>
      <sec id="Sec10">
        <title>Synthetic data generator</title>
        <p id="Par34">In order to make the neural network agnostic to the orientation (axial, coronal, sagittal), MRI modality, and resolution of the inputs (slice thickness, slice spacing), we adopt a domain randomization approach that we have successfully used in other brain MRI analysis problems (e.g.,<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR54">54</xref></sup>). The key idea is that, by randomizing the orientation, resolution, and MR contrast at every iteration during training, the CNN learns features that are agnostic to MR contrast and resolution. The fine details of the synthetic data generator can be found in<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>, but we summarize them here for completeness.</p>
        <p id="Par35">Given a 3D segmentation at 1 mm isotropic resolution, we obtain a corresponding image of random contrast and resolution as follows. First, we group the labels with similar tissue types. For example, the different cerebrospinal fluid (CSF) structures are grouped into a single CSF label. Next, we sample a mean and variance for each label from uniform distributions. These means and variances are used to produce a first “Gaussian image” at 1 mm isotropic resolution. This Gaussian image is a sample of a Gaussian mixture model conditioned on the underlying segmentation. The Gaussian image is further corrupted by a random, synthetic, multiplicative bias field. This field is obtained by sampling a small, zero-mean Gaussian field (4 × 4 × 4 voxels), taking voxel-wise exponentials, and upscaling to the full voxel size of the Gaussian image.</p>
        <p id="Par36">After randomly choosing an orientation (axial, sagittal, coronal), a slice thickness (sampled from a uniform distribution) is simulated with a one-dimensional Gaussian kernel in the perpendicular direction of the orientation. Next, a slice spacing (also sampled from a uniform distribution) is simulated by downsampling the (bias-corrupted) Gaussian image in the direction perpendicular to the orientation, by a factor equal to the ratio between the sampled spacing and 1 mm. Finally, this image is min-max normalized and scaled back up to the original resolution, i.e., 1 mm isotropic. This resampling operation (which will also happen at test time, see Section “<xref rid="Sec15" ref-type="sec">Registration</xref>”), ensures that that the size of the input to the CNN is constant and equal to the size of the underlying segmentation. This yields deformation fields that are always 1 mm isotropic and enables computation of the loss on a 1 mm grid, independently of the resolution of the input scans – which can be different for the fixed and moving images.</p>
      </sec>
      <sec id="Sec11">
        <title>Symmetric estimation of nonlinear deformation with a CNN</title>
        <p id="Par37">In order to estimate the nonlinear deformation, we train a CNN that takes as input two scans in the voxel space of the MNI reference, and estimates a diffeomorphic deformation field and its inverse in a symmetric fashion. Let <inline-formula id="IEq13"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{g}[F(\varvec{x}), M(\varvec{x}); \varvec{\theta }]$$\end{document}</tex-math><mml:math id="M28"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq13.gif"/></alternatives></inline-formula> be a set of convolutional layers with weights <inline-formula id="IEq14"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{\theta }$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq14.gif"/></alternatives></inline-formula> that estimate (regress) an SVF from two input images (which are simply concatenated in the feature dimension), and let <inline-formula id="IEq15"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{V}[\cdot ]$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq15.gif"/></alternatives></inline-formula> be a vector field exponentiation (integration) layer with no trainable parameters. Our method produces a symmetric deformation field by construction, by explicitly symmetrizing the estimate:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \hat{\varvec{v}}(\varvec{x})&amp;= \varvec{g}[F(\varvec{x}), M(\varvec{x}); \varvec{\theta }] - \varvec{g}[M(\varvec{x}), F(\varvec{x}); \varvec{\theta }], \nonumber \\ \hat{\varvec{T}}(\varvec{x})&amp;= \varvec{V}[\hat{\varvec{v}}(\varvec{x})], \nonumber \\ \hat{\varvec{T}^{-1}}(\varvec{x})&amp;= \varvec{V}[-\hat{\varvec{v}}(\varvec{x})], \end{aligned}$$\end{document}</tex-math><mml:math id="M34" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow/><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow/><mml:mover accent="true"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2023_33781_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where switching the order returns the inverse fields, up to the numerical precision of the vector field exponentiation<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>.</p>
        <p id="Par38">We note that:<list list-type="bullet"><list-item><p id="Par39"><inline-formula id="IEq16"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{g}(F,M)$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq16.gif"/></alternatives></inline-formula> approximates the SVF of the deformation taking <italic>F</italic> to the mid-space of <italic>F</italic> and <italic>M</italic>.</p></list-item><list-item><p id="Par40">Conversely, <inline-formula id="IEq17"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{g}(M,F)$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq17.gif"/></alternatives></inline-formula> approximates the SVF of the deformation taking <italic>M</italic> to the mid-space.</p></list-item><list-item><p id="Par41">Adding two SVFs is an approximation to the composition of the fields they parameterize (it is equivalent to truncating the Lie bracket in the Baker–Campbell–Hausdorff series, see<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>). Therefore, Eq. (<xref rid="Equ2" ref-type="disp-formula">2</xref>) approximates the SVF of the field taking <italic>F</italic> to <italic>M</italic> (i.e., <italic>F</italic> to mid-space, composed with mid-space to <italic>M</italic>).</p></list-item></list></p>
        <p id="Par42">These properties enable us to easily initialize our CNN with existing nonlinear registration CNNs based on SVFs, simply by dividing the weights and biases of the final layer by two. This is of course only an approximation due to the BCH truncation and the fact that dividing the SVF by two is not equivalent to integrating to 0.5. However, finetuning from this point is in practive much faster than training from scratch—particularly when training with domain randomized data, which is typically much slower than training with real data.</p>
      </sec>
      <sec id="Sec12">
        <title>Training loss</title>
        <p id="Par43">Our training loss comprises a similarity term and a regularizer. As similarity term, we use the soft Dice overlap of the deformed labels<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>, computed in both fixed and moving space, for symmetry:<disp-formula id="Equ5"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\mathscr {L}}_S = 2 - \text {Dice}[F_{labs}, M_{labs} \circ {\varvec{T}}] - \text {Dice} [M_{labs}, F_{labs} \circ {\varvec{T}}^{-1}], \end{aligned}$$\end{document}</tex-math><mml:math id="M40" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>-</mml:mo><mml:mtext>Dice</mml:mtext><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi mathvariant="italic">labs</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">labs</mml:mi></mml:mrow></mml:msub><mml:mo>∘</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mtext>Dice</mml:mtext><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">labs</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi mathvariant="italic">labs</mml:mi></mml:mrow></mml:msub><mml:mo>∘</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2023_33781_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq18"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F_{labs}$$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi mathvariant="italic">labs</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq18.gif"/></alternatives></inline-formula> is the labels (3D segmentation) of the fixed image, <inline-formula id="IEq19"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$M_{labs}$$\end{document}</tex-math><mml:math id="M44"><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">labs</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq19.gif"/></alternatives></inline-formula> is the labels of the moving image, and <inline-formula id="IEq20"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {Dice}$$\end{document}</tex-math><mml:math id="M46"><mml:mtext>Dice</mml:mtext></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq20.gif"/></alternatives></inline-formula> is the Dice overlap.</p>
        <p id="Par44">The regularizer is also symmetric and discourages irregularities in the deformation fields, by penalizing the magnitude of their gradients:<disp-formula id="Equ6"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\mathscr {L}}_R = \frac{1}{|\Omega |} \sum _{\varvec{x}\in \Omega } \left( \Vert \nabla {\varvec{T}}(\varvec{x}) \Vert ^2 + \Vert \nabla {\varvec{T}}^{-1}(\varvec{x}) \Vert ^2 \right) , \end{aligned}$$\end{document}</tex-math><mml:math id="M48" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:munder><mml:mfenced close=")" open="("><mml:msup><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">∇</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2023_33781_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq21"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Omega$$\end{document}</tex-math><mml:math id="M50"><mml:mi mathvariant="normal">Ω</mml:mi></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq21.gif"/></alternatives></inline-formula> is the image domain (i.e., <inline-formula id="IEq22"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$|\Omega |$$\end{document}</tex-math><mml:math id="M52"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq22.gif"/></alternatives></inline-formula> is the number of voxels). The final loss is simply the linear combination:<disp-formula id="Equ7"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\mathscr {L}} = {\mathscr {L}}_S + \lambda {\mathscr {L}}_R, \end{aligned}$$\end{document}</tex-math><mml:math id="M54" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2023_33781_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq23"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda$$\end{document}</tex-math><mml:math id="M56"><mml:mi>λ</mml:mi></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq23.gif"/></alternatives></inline-formula> is a trade-off parameter.</p>
      </sec>
    </sec>
    <sec id="Sec13">
      <title>Registration</title>
      <sec id="Sec14">
        <title>Robust and symmetric affine registration with centroids</title>
        <p id="Par45">The CNN described above symmetrically predicts deformation fields for a pair of skull-stripped images in the voxel space of the reference (the MNI atlas). Therefore, an affine registration method is needed to bring the input scans into this reference space. Rather than relying on affine registration networks, we build on our previous CNN for segmentation and parcellation of brain scans of any MRI contrast and resolution<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup> (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>b), which has three advantages: it already is agnostic to MRI modality and resolution like the rest of the method; provides the brain mask that is required by the nonlinear registration CNN; and it segments 97 ROIs that are useful not only for affine registration (see below), but potentially for other analyses as well (e.g., volumetry).</p>
        <p id="Par46">Given the soft segmentation, it is straightforward to compute the centroids of the 97 ROIs using a weighted sum, which is differentiable (Fig. <xref rid="Fig2" ref-type="fig">2</xref>c). Given these centroids, along with the centroids of the reference (which are precomputed and constant, as explained in Section “<xref rid="Sec9" ref-type="sec">Training</xref>”), it is straightforward to compute the affine transform that minimizes the least squares error. If <italic>C</italic> is the 4 × 97 matrix with the coordinates of the centroids of the scan in homogeneous coordinates (i.e., the last row consists of ones), and <inline-formula id="IEq25"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_{ref}$$\end{document}</tex-math><mml:math id="M58"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="italic">ref</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq25.gif"/></alternatives></inline-formula> is the matrix with the centroids of the reference, the 4 × 4 affine transform matrix in homogeneous coordinates is given by:<disp-formula id="Equ8"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\hat{A}} = C_{ref} C^t (C C^t)^{-1}, \end{aligned}$$\end{document}</tex-math><mml:math id="M60" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="italic">ref</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>C</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:msup><mml:mi>C</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2023_33781_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>which, again, is differentiable. In practice, we remove from <italic>C</italic> and <inline-formula id="IEq27"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_{ref}$$\end{document}</tex-math><mml:math id="M62"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="italic">ref</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq27.gif"/></alternatives></inline-formula> the rows corresponding to ROIs that have less than 100 voxels in the segmentation of either input scan, in order to make the fit robust against ROIs that are partially or totally outside the field of view of the scans.</p>
        <p id="Par47">This affine registration approach is symmetric by construction, since the two scans to be registered are independently registered to target space. The affine registration is also robust, for two reasons. First, the large number of ROIs provides 3 × 97 = 291 equations to fit 12 unknowns. And second, thanks to the cortical parcellation, many of the centroids are in the periphery of the brain, which avoids extrapolation errors that could occur in these areas if the fit was restricted to subcortical ROIs.</p>
      </sec>
      <sec id="Sec15">
        <title>CNN inference</title>
        <p id="Par48">Once the affine transform has been computed, the input scans are resampled to the 1 mm isotropic voxel space of the MNI reference, skull stripped with the deformed segmentation, and min-max normalized (Fig. <xref rid="Fig2" ref-type="fig">2</xref>d). These images are then fed to the CNN that has been trained for nonlinear registration, which produces estimates of the forward and backward fields <inline-formula id="IEq29"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\varvec{T}}$$\end{document}</tex-math><mml:math id="M64"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq29.gif"/></alternatives></inline-formula> and <inline-formula id="IEq30"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\varvec{T}}^{-1}$$\end{document}</tex-math><mml:math id="M66"><mml:msup><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq30.gif"/></alternatives></inline-formula> on a 1 mm isotropic grid, irrespective of the original resolution of the inputs. At that point, one can estimate the final transforms simply by concatenating the affine and nonlinear transforms—which requires interpolation of <inline-formula id="IEq31"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\varvec{T}}$$\end{document}</tex-math><mml:math id="M68"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq31.gif"/></alternatives></inline-formula> and <inline-formula id="IEq32"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\varvec{T}}^{-1}$$\end{document}</tex-math><mml:math id="M70"><mml:msup><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq32.gif"/></alternatives></inline-formula>, which we accomplish with a trilinear model. Finally, these transforms are used to warp the images (Fig. <xref rid="Fig2" ref-type="fig">2</xref>e).</p>
        <p id="Par49">In practice, we store the two final transforms in Nifti files with the same header and voxel size of the inputs (which may be different from each other), using three frames that store the real world coordinates of the location where each voxel is mapped. This enables fast and simple computation of the deformed image for any scan that lives in the same real-world coordinates as the corresponding input.</p>
      </sec>
    </sec>
    <sec id="Sec16">
      <title>Implementation details</title>
      <p id="Par50">The generative model has a number of parameters, which we take directly from<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. We note that this choice of parameters generates MRI contrasts, resolutions, and bias fields that go well beyond what one realistically encounters in real data; such aggressive simulations enables the CNN to generalize better in practice.</p>
      <p id="Par51">The architecture of the registration CNN is the same as the in the model distributed with SynthMorph<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, which is, to the best of our knowledge, the only publicly available neural network for nonlinear registration across non-predefined contrasts. In short, this CNN is a 3D regression U-net<sup><xref ref-type="bibr" rid="CR58">58</xref></sup> with four resolution levels in the encoder and three in the decoder. Each encoder block has a stride-2 convolution and a LeakyReLU layer (parameter 0.2). Each decoder block has a stride-1 convolution, an upsampling layer, and a skip connection to the corresponding block in the encoder. The last layer of the decoder is followed by three further convolutional blocks and an upsampling layer that brings the size back to the original resolution. All convolutions have size 3 × 3 × 3. The number of features is 256 for all layers.</p>
      <p id="Par52">For training, we finetuned the SynthMorph model (with the weights of the last layer divided by 2, as explained in Section “<xref rid="Sec11" ref-type="sec">Training</xref>”) for 100,000 iterations using the Adam optimizer<sup><xref ref-type="bibr" rid="CR59">59</xref></sup> and a learning rate of <inline-formula id="IEq35"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{-4}$$\end{document}</tex-math><mml:math id="M72"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq35.gif"/></alternatives></inline-formula>. We used the same trade-off parameter as in the original paper: <inline-formula id="IEq36"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda =1.0$$\end{document}</tex-math><mml:math id="M74"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq36.gif"/></alternatives></inline-formula>. The networks are implemented in TensorFlow/Keras<sup><xref ref-type="bibr" rid="CR60">60</xref>,<xref ref-type="bibr" rid="CR61">61</xref></sup>. Crucially, the inference code is implemented in a version of Python that is distributed with FreeSurfer, such that users do not need to create virtual environments and install dependencies.</p>
      <p id="Par53">We also emphasize that all the components of the pipeline are differentiable: the segmentation, the computation of centroids, the affine alignment, and the nonlinear registration. Therefore, <italic>EasyReg</italic> could be used as a building block in meta-architectures that include registration components.</p>
    </sec>
  </sec>
  <sec id="Sec17">
    <title>Experiments and results</title>
    <sec id="Sec18">
      <title>MRI data</title>
      <p id="Par54">We performed experiments using seven different datasets:<list list-type="bullet"><list-item><p id="Par55"><bold>T1</bold>: 100 1 mm isotropic T1 scans, randomly selected from the publicly available IXI dataset (<ext-link ext-link-type="uri" xlink:href="http://www.brain-development.org/ixi-dataset/">www.brain-development.org/ixi-dataset/</ext-link>).</p></list-item><list-item><p id="Par56"><bold>T1b</bold>: to test the registrations across T1 scans acquired with different sequences, we also used 100 1 mm isotropic T1 scans from the publicly available MindBoggle dataset<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>.</p></list-item><list-item><p id="Par57"><bold>T2</bold>: 100 1 mm isotropic T2 scans randomly selected from IXI (leaving out the subjects corresponding to the 100 T1 scans).</p></list-item><list-item><p id="Par58"><bold>FA</bold>: 100 fractional anisotropy (FA) volumes derived from the diffusion MRI data from 100 subjects randomly selected from the HCP dataset (leaving out the subjects whose data were used in training). These FA volumes have 1.25 mm isotropic resolution.</p></list-item><list-item><p id="Par59"><bold>AxFLAIR</bold>: 100 FLAIR scans with 1<inline-formula id="IEq37"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M76"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq37.gif"/></alternatives></inline-formula>1<inline-formula id="IEq38"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M78"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq38.gif"/></alternatives></inline-formula>5 mm axial resolution (5 mm slice thickness), from 100 randomly selected subjects from the ADNI dataset (leaving out the subjects whose data were used in training).</p></list-item><list-item><p id="Par60"><bold>SagT1</bold>: We downsampled the 100 T1 scans to 1<inline-formula id="IEq39"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M80"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq39.gif"/></alternatives></inline-formula>1<inline-formula id="IEq40"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M82"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq40.gif"/></alternatives></inline-formula>5 mm sagittal resolution (5 mm slice thickness).</p></list-item><list-item><p id="Par61"><bold>AxT2</bold>: We downsampled the 100 T2 scans to 1<inline-formula id="IEq41"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M84"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq41.gif"/></alternatives></inline-formula>1<inline-formula id="IEq42"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M86"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq42.gif"/></alternatives></inline-formula>5 mm axial resolution (5 mm slice thickness).</p></list-item></list></p>
      <p id="Par62">Ground truth segmentations at 1 mm isotropic resolution were obtained for every dataset using the method implemented in FreeSurfer 7.3.0<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>. For T1 and T1b, FreeSurfer was run directly on the scans. For the rest of datasets, FreeSurfer was run on the companion 1 mm T1 scans that are available for IXI, HCP, and ADNI, after registration to the T2/FLAIR/diffusion scans. We note that running FreeSurfer after registration introduces a slight amount of blurring but avoids nearest neighbor interpolation artifacts associated with deforming segmentations.</p>
    </sec>
    <sec id="Sec19">
      <title>Registration experiments</title>
      <p id="Par63">In order to evaluate different aspects of the registration method, we used eight different experimental setups, each comprising 100 registrations:<list list-type="bullet"><list-item><p id="Par64"><bold>T1–T1</bold>: We first registered each 1 mm T1 scan to another 1 mm T1 scan from the same dataset. This is a frequent scenario in neuroimaging—and also the setup in which classical techniques best perform.</p></list-item><list-item><p id="Par65"><bold>T1–T1b</bold>: We register across the two different T1 datasets with 1 mm resolution, in order to assess the impact of differences in acquisition platform and parameters.</p></list-item><list-item><p id="Par66"><bold>T1–T2</bold>: Registering T1 and T2 scans at 1 mm resolution enables us to assess the impact of registering across modalities, while keeping the resolution high and isotropic.</p></list-item><list-item><p id="Par67"><bold>T1–FA</bold>: This setup enables us to assess the accuracy when registering across modalities with inherently different mechanisms to generate image contrast (and slightly different resolution).</p></list-item><list-item><p id="Par68"><bold>T1–AxFLAIR</bold>: We now consider registration between 1 mm T1s and clinical scans with large slice spacing (5 mm) and different contrast (FLAIR).</p></list-item><list-item><p id="Par69"><bold>SagT1–SagT1</bold>: We use this setup to assess the impact of large slice spacing, while sharing the same MRI modality and resolution.</p></list-item><list-item><p id="Par70"><bold>AxT2–AxFLAIR</bold>: This is similar to the previous setup, but using images with different MRI modality (T2 and FLAIR).</p></list-item><list-item><p id="Par71"><bold>SagT1–AxFLAIR</bold>: The most general case, where the images have different MRI modality (T1 vs FLAIR), large slice spacing, and different orientation (sagittal vs axial).</p></list-item></list></p>
    </sec>
    <sec id="Sec20">
      <title>Competing methods</title>
      <p id="Par72">We used the following competing methods in our experiments:<list list-type="bullet"><list-item><p id="Par73"><bold>NiftyRegAffine</bold>: the block matching method implemented in NiftyReg<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>, with default parameters.</p></list-item><list-item><p id="Par74"><bold>MI-Affine</bold>: a linear registration method based on mutual information<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>, which is widely used in neuroimaging (e.g., it is implemented in SPM and FreeSurfer). We used the implementation in FreeSurfer (“mri_coreg”).</p></list-item><list-item><p id="Par75"><bold>EasyRegAffine</bold>: the affine component of our proposed method, used in isolation.</p></list-item><list-item><p id="Par76"><bold>NiftyReg</bold>: the full NiftyReg pipeline, with its linear and nonlinear modules. For the nonlinear algorithm, we used the diffeomorphic mode (based on SVFs) with control point spacing of 5 mm when both input images are isotropic, and 20 mm if at least one of them has large slice spacing. As cost functions, we used local normalized cross correlation within modalities (radius: 4 mm) and mutual information across modalities. These parameters were set based on pilot experiments on withheld scans from the different datasets.</p></list-item><list-item><p id="Par77"><bold>ANTs</bold>: the full ANTs pipeline with affine and nonlinear “SyN” models. For isotropic inputs, we used a smoothing factor <inline-formula id="IEq43"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma =3$$\end{document}</tex-math><mml:math id="M88"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq43.gif"/></alternatives></inline-formula>; for large slice spacing, we use <inline-formula id="IEq44"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma =6$$\end{document}</tex-math><mml:math id="M90"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq44.gif"/></alternatives></inline-formula>. Within modalities, we used the local normalized cross correlation (semi-radius: 4 mm, as for NiftyReg); across modalities we used mutual information. As for NiftyReg, these parameters were set based on pilot experiments on withheld scans from the different datasets.</p></list-item><list-item><p id="Par78"><bold>SynthMorph</bold>: we used the publicly available implementation of SynthMorph, which requires affine registration to a reference space; we used “mri_coreg” for this purpose.</p></list-item><list-item><p id="Par79"><bold><italic>EasyReg</italic></bold>: the proposed method.</p></list-item></list></p>
      <p id="Par80">We note that SynthMorph is, to the best of our knowledge, the only deep learning registration method that can align two scans without knowing their contrast a priori. This is the reason why it is the only learning-based competing method in our experiments. We also note that the classical methods benefit from skull stripping; we used SynthStrip<sup><xref ref-type="bibr" rid="CR52">52</xref></sup> for this purpose. Likewise, SynthMorph requires skull stripping, for which we also use SynthStrip. We note than our proposed method does not require skull stripped inputs.</p>
    </sec>
    <sec id="Sec21">
      <title>Accuracy metrics</title>
      <p id="Par81">As is commonplace in the registration literature, we use segmentation quality metrics as a proxy for registration accuracy. Evaluating registration accuracy directly would require ground truth correspondences that are very difficult to obtain, particularly across subjects. Instead, we register a scan to another, use the resulting transform to deform the labels, and compare these deformed labels with the labels of the fixed image in native space. We use three different metrics to quantify the agreement of label maps: Dice overlap<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>, the mean distance between the surfaces of the two segmentations, and the 95th percentile of the distribution of the surface-to-surface distance (henceforth, the “95% Hausdorff distance”). This 95th percentile is a robust version of the Hausdorff distance, i.e., the maximum distance between the surfaces. All the metrics were averaged across 29 ROIs defined by FreeSurfer: the 27 subcortical ROIs, the left cerebral cortex, and the right cerebral cortex.</p>
      <p id="Par82">Even though all the competing methods are diffeomorphic in their continuous formulation, their discrete implementations lead to negative Jacobian determinants in practice. Therefore, we also count the number of voxels for which the Jacobian determinant is negative; i.e., the voxels where the deformation field folds onto itself and the one-to-one mapping is lost. This quantity enables us to compare the regularity and invertibility of the deformation fields produced by different methods.</p>
      <p id="Par83">Finally, we also report the inverse consistency error for the different methods:<disp-formula id="Equ9"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} ICE = \frac{1}{2|\Omega |} \sum _{\varvec{x}\in \Omega } \Vert \varvec{x} - \hat{\varvec{T}^{-1}}(\hat{\varvec{T}}(\varvec{x})) \Vert + \frac{1}{2|\Omega |} \sum _{\varvec{x}\in \Omega } \Vert \varvec{x} - \hat{\varvec{T}}(\hat{\varvec{T}^{-1}}(\varvec{x})) \Vert . \end{aligned}$$\end{document}</tex-math><mml:math id="M92" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mrow><mml:mover accent="true"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mo>+</mml:mo></mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2023_33781_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par84">We note that this error will be greater than zero for the symmetric methods, due to the discrete integration. Importantly, we also note than SynthMorph is <italic>not</italic> inverse consistent, so the error is expected to be much higher than for the other approaches.<fig id="Fig3"><label>Figure 3</label><caption><p>Box plot for the registration accuracy metrics achieved by the different methods in the different registrations tasks: Dice scores (top), mean surface-to-surface distances (center), and 95% Hausdorff distance (bottom). On each box, the central mark is the median, the edges of the box are the 25th and 75th percentiles, and the whiskers extend to the most extreme datapoints that are not outliers (which are plotted as crosses). The green brackets indicate <italic>lack</italic> of statistically significant differences between two methods (<inline-formula id="IEq45"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p&gt;0.05$$\end{document}</tex-math><mml:math id="M94"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq45.gif"/></alternatives></inline-formula>), according to a non-parametric Wilcoxon test; note that we mark the lack (rather than existence) of differences not to clutter the figures, since most differences are significant (due to the large sample size and to the fact that the values have little noise, as they are averages over brain structures).</p></caption><graphic xlink:href="41598_2023_33781_Fig3_HTML" id="MO12"/></fig><fig id="Fig4"><label>Figure 4</label><caption><p>Sample registration from T1–T1b setup, i.e., MindBoggle to IXI Top left: sagittal slices of fixed and moving images. Rest: corresponding registered slice and deformation field (represented as a deformed grid) for the seven competing methods.</p></caption><graphic xlink:href="41598_2023_33781_Fig4_HTML" id="MO13"/></fig><fig id="Fig5"><label>Figure 5</label><caption><p>Sample registration from SagT1–AxFLAIR setup, i.e., 5 mm axial FLAIR from ADNI, to 5 mm sagittal T1-weighted scan from IXI. Top left: axial slice of the fixed image and corresponding isotropic slice; the later was <italic>not</italic> used in registration; we display it for easier qualitative assessment of registration accuracy. Rest: corresponding registered slice and deformation field (represented as a deformed grid) for the seven competing methods.</p></caption><graphic xlink:href="41598_2023_33781_Fig5_HTML" id="MO14"/></fig></p>
    </sec>
    <sec id="Sec22">
      <title>Results</title>
      <p id="Par85">Figure <xref rid="Fig3" ref-type="fig">3</xref> shows box plots for the Dice overlap, mean surface distance and Hausdorff distance, for all the registration experiments and competing methods—as well as the results of non-parametric Wilcoxon tests comparing the medians of the different methods. Tables <xref rid="Tab1" ref-type="table">1</xref> and <xref rid="Tab2" ref-type="table">2</xref> show the number of voxels with negative Jacobian determinants and the average inverse consistence error, respectively. Figures <xref rid="Fig4" ref-type="fig">4</xref> and <xref rid="Fig5" ref-type="fig">5</xref> show registration examples for an intra-modality (T1-T1b) and an inter-modality setup (SagT1-AxFLAIR).<table-wrap id="Tab1"><label>Table 1</label><caption><p>Number of voxels with negative Jacobian determinants, for the different methods and registration tasks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Task</th><th align="left">NiftyReg</th><th align="left">ANTs</th><th align="left">SynthMorph</th><th align="left">EasyReg</th></tr></thead><tbody><tr><td align="left">T1–T1</td><td align="left">0.09</td><td align="left">0.02</td><td align="left">0.91</td><td align="left">&lt; 0.01</td></tr><tr><td align="left">T1–T1b</td><td align="left">0.12</td><td align="left">0.04</td><td align="left">1.12</td><td align="left">&lt; 0.01</td></tr><tr><td align="left">T1–T2</td><td align="left">0.10</td><td align="left">0.03</td><td align="left">1.10</td><td align="left">&lt; 0.01</td></tr><tr><td align="left">T1–FA</td><td align="left">0.14</td><td align="left">0.09</td><td align="left">1.21</td><td align="left">&lt; 0.01</td></tr><tr><td align="left">T1–AxFLAIR</td><td align="left">0.23</td><td align="left">0.11</td><td align="left">0.97</td><td align="left">&lt; 0.01</td></tr><tr><td align="left">SagT1–SagT1</td><td align="left">0.27</td><td align="left">0.08</td><td align="left">0.26</td><td align="left">&lt; 0.01</td></tr><tr><td align="left">AxT2–AxFLAIR</td><td align="left">1.45</td><td align="left">0.53</td><td align="left">88.58</td><td align="left">46.22</td></tr><tr><td align="left">SagT1–AxFLAIR</td><td align="left">0.54</td><td align="left">0.26</td><td align="left">0.25</td><td align="left">&lt; 0.01</td></tr></tbody></table><table-wrap-foot><p>Only voxels inside the brain are considered.</p></table-wrap-foot></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>Average inverse consistency errors (in mm), for the different methods and registration tasks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Task</th><th align="left">NiftyReg</th><th align="left">ANTs</th><th align="left">SynthMorph</th><th align="left">EasyReg</th></tr></thead><tbody><tr><td align="left">T1–T1</td><td align="left">0.02</td><td align="left">0.01</td><td align="left">1.60</td><td align="left">0.08</td></tr><tr><td align="left">T1–T1b</td><td align="left">0.02</td><td align="left">0.02</td><td align="left">1.69</td><td align="left">0.08</td></tr><tr><td align="left">T1–T2</td><td align="left">0.02</td><td align="left">0.01</td><td align="left">1.53</td><td align="left">0.06</td></tr><tr><td align="left">T1–FA</td><td align="left">0.03</td><td align="left">0.02</td><td align="left">1.61</td><td align="left">0.07</td></tr><tr><td align="left">T1–AxFLAIR</td><td align="left">0.04</td><td align="left">0.03</td><td align="left">1.73</td><td align="left">0.08</td></tr><tr><td align="left">SagT1–SagT1</td><td align="left">0.04</td><td align="left">0.02</td><td align="left">1.69</td><td align="left">0.08</td></tr><tr><td align="left">AxT2–AxFLAIR</td><td align="left">0.06</td><td align="left">0.04</td><td align="left">1.80</td><td align="left">0.07</td></tr><tr><td align="left">SagT1–AxFLAIR</td><td align="left">0.04</td><td align="left">0.02</td><td align="left">1.66</td><td align="left">0.08</td></tr></tbody></table><table-wrap-foot><p>Errors are averaged over voxels inside the brain.</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec23">
      <title>Affine registration</title>
      <p id="Par86">The results for NiftyReg (affine) and the mutual information method are very similar to each other. Moreover, they remain fairly constant across metrics and registration experiments. For example, the median Dice scores are all between 0.62 and 0.68—which is very little variation, considering the range of MRI modalities, orientations, and resolutions involved in the experiments. This is because, given skull-stripped scans, both methods are quite robust (even though NiftyReg is often 1–2 Dice points and 0.1–0.2 mm better than the mutual information method).</p>
      <p id="Par87">Our proposed affine algorithm produces approximately the same results as the other two affine approaches: the boxes corresponding to our method in Fig. <xref rid="Fig3" ref-type="fig">3</xref> are most often between those of the other two affine techniques. Moreover, there are not noticeably more outliers for any of the three affine registration techniques. We can thus conclude that the proposed affine module, which does not require any preprocessing, is as accurate and robust as widespread affine registration techniques.</p>
    </sec>
    <sec id="Sec24">
      <title>Nonlinear registration</title>
      <p id="Par88">The box plots in Fig. <xref rid="Fig3" ref-type="fig">3</xref> and the example in Fig. <xref rid="Fig4" ref-type="fig">4</xref> show that, when registering 1 mm isotropic T1 scans to each other (even across datasets), all methods (classical and learning-based) are very accurate: Dice scores range between 0.79 and 0.81, mean surfaces distances between 1.3 and 1.4 mm, and 95% Hausdorff distances between 3.2 and 3.5 mm. However, when the modalities or resolutions of the input scans differ, the performance of the classical methods quickly drops, whereas the learning-based techniques yield Dice scores over 0.75 across the board, including the most challenging setups (e.g., when registering scans of different MRI modalities with large slice spacing). The same trend is observed for the surface distances. Qualitatevely speaking, the example in Fig. <xref rid="Fig5" ref-type="fig">5</xref> shows how classical techniques fail to align even high-contrast regions, like the lateral ventricles—which are well registered by the learning-based methods.</p>
      <p id="Par89">Compared with SynthMorph, our proposed approach loses 1–2 Dice points and 0.1–0.2 mm in some of the setups; this is the price to pay for the smoother, symmetric fields. For example, the field produced by SynthMorph in the intra-modality registration example in Fig. <xref rid="Fig4" ref-type="fig">4</xref> is highly convoluted, with plenty of deformation in flat regions (not penalized by segmentation metrics) that lead to a cartoon-like deformed image. <italic>EasyReg</italic>, on the other hand, yields a smooth field, which is very similar to those produced by NiftyReg and ANTs (which are remarkably similar to each other). The same effect is observed across modalities in Fig. <xref rid="Fig5" ref-type="fig">5</xref>, where SynthMorph creates strong local variations in the deformation field that are not realistic, compared with the much smoother fields produced by <italic>EasyReg</italic>—which, in the setup of the example (SagT1–AxFLAIR), yields the same median Dice score and surface distances (Fig. <xref rid="Fig3" ref-type="fig">3</xref>).</p>
      <p id="Par90">The regularity of the deformation fields is also reflected in the number of voxels with negative Jacobians (Table <xref rid="Tab1" ref-type="table">1</xref>). All the approaches produce highly regular fields, with less than one “flipped” voxel in almost every setup. The only exception occurs when the aial T2s and axial FLAIRs are registered with the deep learning approaches, which leads to 88 (SynthMorph) and 46 (<italic>EasyReg</italic>) flipped voxels per scan; we note that this is a tiny fraction (&lt;0.01%) of the total number of voxels. Outside that registration task, <italic>EasyReg</italic> yields less than 0.01 flipped voxels per scan all across the board.</p>
      <p id="Par91">Finally, Table <xref rid="Tab2" ref-type="table">2</xref> shows the average inverse consistency errors for the different methods and registration tasks. We note that SynthMorph is not an inverse consistent approach, so it yields fairly large consistency errors across the boards, almost 2 mm on average. NiftyReg, ANTs, and <italic>EasyReg</italic>, on the other hand, yield very small averages (below 0.1 mm) for all registration tasks. We note that EasyReg produces slightly larger errors than the classical approaches, despite using the same integration method as NiftyReg. This is for two reasons: the fact that our CNN uses single (rather than double) floating-point precision, and the fact that NiftyReg uses an adaptive strategy for the number of integration steps—which is constant in our CNN.</p>
    </sec>
  </sec>
  <sec id="Sec25">
    <title>Discussion and conclusion</title>
    <p id="Par92">In this article, we have presented <italic>EasyReg</italic>, a learning-based registration method for brain MRI that is: symmetric; modality- and resolution-agnostic; diffeomorphic; affine and nonlinear; and parameter-free. <italic>EasyReg</italic> requires no preprocessing and can be used with unpreprocessed data direct from the scanner. <italic>EasyReg</italic> is publicly available as part of FreeSurfer and can be used out of the box, without dedicated hardware or machine learning expertise.</p>
    <p id="Par93"><italic>EasyReg</italic>’s run time on a modern desktop without a GPU is approximately 2 mins—or just one minute, if the input scans have already been registered to other images and the computation of the centroids can be bypassed. This is in contrast with classical techniques: ANTs ran on approximately 35 mins on the same computer, while NiftyReg ran in 20 mins. SynthMorph runs in approximately 30 s, since it only does one forward pass through the network (<italic>EasyReg</italic> does two passes; see Eq. (<xref rid="Equ2" ref-type="disp-formula">2</xref>)).</p>
    <p id="Par94">Our experiments covered a wide range of setups, demonstrating <italic>EasyReg</italic>’s ability to cope with large variations in scan orientation, resolution, and pulse sequence. This is in contrast with classical techniques, which cannot handle such dissimilitudes. Compared with SynthMorph, on which it builds, <italic>EasyReg</italic> is symmetric (up to numerical precision of the SVF integration), produces smoother deformation fields, and does not require preprocessing (skull stripping, registration to a template). <italic>EasyReg</italic>’s symmetry comes at the expense of a minimal loss of accuracy around label boundaries. Therefore, SynthMorph may be more appropriate for inherently asymmetric tasks where field irregularity is irrelevant (e.g., segmentation), while <italic>EasyReg</italic> is a better choice when robustness or minimization of directional bias are the priority.</p>
    <p id="Par95"><italic>EasyReg</italic> is independent of resolution, as the deformations fields are always computed on a 1 mm isotropic grid—which is the resolution to which the inputs of the CNN are resampled. Therefore, <italic>EasyReg</italic> implicitly learns to super-resolve scans of lower resolution to 1 mm isotropic. However, if the resolution is higher than that, <italic>EasyReg</italic> still operates at 1 mm internally and cannot exploit the higher resolution. Capitalizing on such smaller voxel sizes requires a training dataset with higher resolution and remains as future work.</p>
    <p id="Par96">The proposed framework is also independent of MR contrast (i.e., pulse sequence), but relies on an image formation model that is fairly specific to MRI, including artifacts like bias field. While this approach may cope with other modalities that may be modeled by a Gaussian mixture (e.g., computerized tomography<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>), a different image formation model would be required to handle other modalities like ultrasound or positron emission tomography (PET).</p>
    <p id="Par97">A limitation of our experimental setup is that we did not exhaustively sweep all the parameters of the classical methods using the full datasets, which would have been computationally very expensive. While the default settings produce state-of-the-art results in the intra-modality setups, it is very possible that exhaustive finetuning of the parameters for every scenario (or even every case separately) may yield better results. For example, the NiftyReg parameters we used across modalities worked well for T1–AxFLAIR, but poorly for T1–FA.</p>
    <p id="Par98"><italic>EasyReg</italic>, on the other hand, has no parameters. While this is an advantage in terms of simplicity and reproducibility, it is also possible that the ability to tune the trade-off parameter <inline-formula id="IEq46"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda$$\end{document}</tex-math><mml:math id="M96"><mml:mi>λ</mml:mi></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq46.gif"/></alternatives></inline-formula> may enable the user to improve the results, e.g., by increasing <inline-formula id="IEq47"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda$$\end{document}</tex-math><mml:math id="M98"><mml:mi>λ</mml:mi></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq47.gif"/></alternatives></inline-formula> in inter-modality scenarios. While frequent retraining for different values of <inline-formula id="IEq48"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda$$\end{document}</tex-math><mml:math id="M100"><mml:mi>λ</mml:mi></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq48.gif"/></alternatives></inline-formula> is impractical, recent advances could enable training of a single <inline-formula id="IEq49"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda$$\end{document}</tex-math><mml:math id="M102"><mml:mi>λ</mml:mi></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq49.gif"/></alternatives></inline-formula>-adaptive network, such that the user can specify the value of this parameter for each scan at test time. Examples of such approaches include Hypermorph<sup><xref ref-type="bibr" rid="CR66">66</xref></sup>, where the weights of the CNN are given by a separate network that takes <inline-formula id="IEq50"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda$$\end{document}</tex-math><mml:math id="M104"><mml:mi>λ</mml:mi></mml:math><inline-graphic xlink:href="41598_2023_33781_Article_IEq50.gif"/></alternatives></inline-formula> as input, or LapIRN<sup><xref ref-type="bibr" rid="CR67">67</xref></sup>, which modulates the feature statistics of the deeper layers instead (and is thus much less memory intensive). Exploring this direction, along with other architectural improvements that may have a positive impact on <italic>EasyReg</italic> (e.g., progressive deformations<sup><xref ref-type="bibr" rid="CR68">68</xref></sup>), remains as future work.</p>
    <p id="Par99">By enabling fast and symmetric registration of unpreprocessed brain MRI scans, <italic>EasyReg</italic> holds great promise to ease the adoption of learning-based registration method by neuroimaging pipelines.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>This project has been primarily funded by the NIH (1RF1-MH123195, 1R01-AG070988, 1R01-EB031114, 1UM1-MH130981) and Alzheimers Research UK (Interdisciplinary Grant ARUK-IRG2019A-003). The collection and sharing of the ADNI data used in this study was funded by the Alzheimer’s Disease Neuroimaging Initiative (National Institutes of Health Grant U01-AG024904) and DOD ADNI (Department of Defence Award Number W81XWH-12-2-0012). ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the following: Alzheimer’s Association; Alzheimer’s Drug Discovery Foundation; BioClinica, Inc.; Biogen Idec Inc.; Bristol-Myers Squibb Company; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; F. Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc.; GE Healthcare; Innogenetics, N.V.; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research &amp; Development, LLC.; Johnson &amp; Johnson Pharmaceutical Research &amp; Development LLC.; Medpace, Inc.; Merck &amp; Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Synarc Inc.; and Takeda Pharmaceutical Company. The Canadian Institutes of Health Research is providing funds to support ADNI clinical sites in Canada. Private sector contributions are facilitated by the Foundation for the National Institutes of Health (<ext-link ext-link-type="uri" xlink:href="http://www.fnih.org">www.fnih.org</ext-link>). The grantee organisation is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer’s Disease Cooperative Study at the University of California, San Diego. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>J.E.I. designed this study, implemented the methods, ran the experiments, analyzed the results, and wrote the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>All the brain MRI scans used in this article were obtained from publicly available datasets: HCP, ADNI, IXI, and MindBoggle: HCP: <ext-link ext-link-type="uri" xlink:href="https://www.humanconnectome.org/study/hcp-young-adult/data-releases">https://www.humanconnectome.org/study/hcp-young-adult/data-releases</ext-link>. ADNI: <ext-link ext-link-type="uri" xlink:href="https://adni.loni.usc.edu/">https://adni.loni.usc.edu/</ext-link>. IXI: <ext-link ext-link-type="uri" xlink:href="https://brain-development.org/ixi-dataset/">https://brain-development.org/ixi-dataset/</ext-link>. MindBoggle: <ext-link ext-link-type="uri" xlink:href="https://mindboggle.info/">https://mindboggle.info/</ext-link>. We also note that the ADNI Data Use Agreement (available at <ext-link ext-link-type="uri" xlink:href="https://ida.loni.usc.edu/collaboration/access/appLicense.jsp">https://ida.loni.usc.edu/collaboration/access/appLicense.jsp</ext-link>) requires that we note that: “Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (<ext-link ext-link-type="uri" xlink:href="http://adni.loni.usc.edu">http://adni.loni.usc.edu</ext-link>). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf”. The ADNI Data Use Agreement also requires us to note that: “The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer’s disease”.</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par100">The author declares no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zitova</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Flusser</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Image registration methods: A survey</article-title>
        <source>Image Vis. Comput.</source>
        <year>2003</year>
        <volume>21</volume>
        <fpage>977</fpage>
        <lpage>1000</lpage>
        <pub-id pub-id-type="doi">10.1016/S0262-8856(03)00137-9</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sotiras</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Davatzikos</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Paragios</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Deformable medical image registration: A survey</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2013</year>
        <volume>32</volume>
        <fpage>1153</fpage>
        <lpage>1190</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2013.2265603</pub-id>
        <?supplied-pmid 23739795?>
        <pub-id pub-id-type="pmid">23739795</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fischl</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sereno</surname>
            <given-names>MI</given-names>
          </name>
          <name>
            <surname>Dale</surname>
            <given-names>AM</given-names>
          </name>
        </person-group>
        <article-title>Cortical surface-based analysis: II: Inflation, flattening, and a surface-based coordinate system</article-title>
        <source>Neuroimage</source>
        <year>1999</year>
        <volume>9</volume>
        <fpage>195</fpage>
        <lpage>207</lpage>
        <pub-id pub-id-type="doi">10.1006/nimg.1998.0396</pub-id>
        <?supplied-pmid 9931269?>
        <pub-id pub-id-type="pmid">9931269</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Avants</surname>
            <given-names>BB</given-names>
          </name>
          <name>
            <surname>Epstein</surname>
            <given-names>CL</given-names>
          </name>
          <name>
            <surname>Grossman</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gee</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>Symmetric diffeomorphic image registration with cross-correlation: Evaluating automated labeling of elderly and neurodegenerative brain</article-title>
        <source>Med. Image Anal.</source>
        <year>2008</year>
        <volume>12</volume>
        <fpage>26</fpage>
        <lpage>41</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2007.06.004</pub-id>
        <?supplied-pmid 17659998?>
        <pub-id pub-id-type="pmid">17659998</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Modat</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Fast free-form deformation using graphics processing units</article-title>
        <source>Comput. Methods Programs Biomed.</source>
        <year>2010</year>
        <volume>98</volume>
        <fpage>278</fpage>
        <lpage>284</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cmpb.2009.09.002</pub-id>
        <?supplied-pmid 19818524?>
        <pub-id pub-id-type="pmid">19818524</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Klein</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Staring</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Murphy</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Viergever</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Pluim</surname>
            <given-names>JP</given-names>
          </name>
        </person-group>
        <article-title>Elastix: A toolbox for intensity-based medical image registration</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2009</year>
        <volume>29</volume>
        <fpage>196</fpage>
        <lpage>205</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2009.2035616</pub-id>
        <?supplied-pmid 19923044?>
        <pub-id pub-id-type="pmid">19923044</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ashburner</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A fast diffeomorphic image registration algorithm</article-title>
        <source>Neuroimage</source>
        <year>2007</year>
        <volume>38</volume>
        <fpage>95</fpage>
        <lpage>113</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.07.007</pub-id>
        <?supplied-pmid 17761438?>
        <pub-id pub-id-type="pmid">17761438</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brock</surname>
            <given-names>KK</given-names>
          </name>
          <name>
            <surname>Mutic</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>McNutt</surname>
            <given-names>TR</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Kessler</surname>
            <given-names>ML</given-names>
          </name>
        </person-group>
        <article-title>Use of image registration and fusion algorithms and techniques in radiotherapy: Report of the AAPM Radiation Therapy Committee Task Group No 132</article-title>
        <source>Med. Phys.</source>
        <year>2017</year>
        <volume>44</volume>
        <fpage>e43</fpage>
        <lpage>e76</lpage>
        <pub-id pub-id-type="doi">10.1002/mp.12256</pub-id>
        <?supplied-pmid 28376237?>
        <pub-id pub-id-type="pmid">28376237</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Iglesias</surname>
            <given-names>JE</given-names>
          </name>
          <name>
            <surname>Sabuncu</surname>
            <given-names>MR</given-names>
          </name>
        </person-group>
        <article-title>Multi-atlas segmentation of biomedical images: A survey</article-title>
        <source>Med. Image Anal.</source>
        <year>2015</year>
        <volume>24</volume>
        <fpage>205</fpage>
        <lpage>219</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2015.06.012</pub-id>
        <?supplied-pmid 26201875?>
        <pub-id pub-id-type="pmid">26201875</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reuter</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rosas</surname>
            <given-names>HD</given-names>
          </name>
          <name>
            <surname>Fischl</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Highly accurate inverse consistent registration: A robust approach</article-title>
        <source>Neuroimage</source>
        <year>2010</year>
        <volume>53</volume>
        <fpage>1181</fpage>
        <lpage>1196</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.020</pub-id>
        <?supplied-pmid 20637289?>
        <pub-id pub-id-type="pmid">20637289</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Freeborough</surname>
            <given-names>PA</given-names>
          </name>
          <name>
            <surname>Fox</surname>
            <given-names>NC</given-names>
          </name>
        </person-group>
        <article-title>The boundary shift integral: An accurate and robust measure of cerebral volume changes from registered repeat MRI</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>1997</year>
        <volume>16</volume>
        <fpage>623</fpage>
        <lpage>629</lpage>
        <pub-id pub-id-type="doi">10.1109/42.640753</pub-id>
        <?supplied-pmid 9368118?>
        <pub-id pub-id-type="pmid">9368118</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ashburner</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Friston</surname>
            <given-names>KJ</given-names>
          </name>
        </person-group>
        <article-title>Voxel-based morphometry-the methods</article-title>
        <source>Neuroimage</source>
        <year>2000</year>
        <volume>11</volume>
        <fpage>805</fpage>
        <lpage>821</lpage>
        <pub-id pub-id-type="doi">10.1006/nimg.2000.0582</pub-id>
        <?supplied-pmid 10860804?>
        <pub-id pub-id-type="pmid">10860804</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Nocedal</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wright</surname>
            <given-names>SJ</given-names>
          </name>
        </person-group>
        <source>Numerical Optimization</source>
        <year>1999</year>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Boyd</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Boyd</surname>
            <given-names>SP</given-names>
          </name>
          <name>
            <surname>Vandenberghe</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <source>Convex Optimization</source>
        <year>2004</year>
        <publisher-name>Cambridge University Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Arsigny, V., Commowick, O., Pennec, X. &amp; Ayache, N. A log-euclidean framework for statistics on diffeomorphisms. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>, 924–931 (Springer, 2006).</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Beg</surname>
            <given-names>MF</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>MI</given-names>
          </name>
          <name>
            <surname>Trouvé</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Younes</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Computing large deformation metric mappings via geodesic flows of diffeomorphisms</article-title>
        <source>Int. J. Comput. Vision</source>
        <year>2005</year>
        <volume>61</volume>
        <fpage>139</fpage>
        <lpage>157</lpage>
        <pub-id pub-id-type="doi">10.1023/B:VISI.0000043755.93987.aa</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Christensen</surname>
            <given-names>GE</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>HJ</given-names>
          </name>
        </person-group>
        <article-title>Consistent image registration</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2001</year>
        <volume>20</volume>
        <fpage>568</fpage>
        <lpage>582</lpage>
        <pub-id pub-id-type="doi">10.1109/42.932742</pub-id>
        <?supplied-pmid 11465464?>
        <pub-id pub-id-type="pmid">11465464</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pluim</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Maintz</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Viergever</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Mutual-information-based registration of medical images: A survey</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2003</year>
        <volume>22</volume>
        <fpage>986</fpage>
        <lpage>1004</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2003.815867</pub-id>
        <?supplied-pmid 12906253?>
        <pub-id pub-id-type="pmid">12906253</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Sokooti, H. <italic>et al.</italic> Nonrigid image registration using multi-scale 3D convolutional neural networks. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>, 232–239 (Springer, 2017).</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Kwitt</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Styner</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Niethammer</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Quicksilver: Fast predictive image registration: A deep learning approach</article-title>
        <source>Neuroimage</source>
        <year>2017</year>
        <volume>158</volume>
        <fpage>378</fpage>
        <lpage>396</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.07.008</pub-id>
        <?supplied-pmid 28705497?>
        <pub-id pub-id-type="pmid">28705497</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>De Vos</surname>
            <given-names>BD</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A deep learning framework for unsupervised affine and deformable image registration</article-title>
        <source>Med. Image Anal.</source>
        <year>2019</year>
        <volume>52</volume>
        <fpage>128</fpage>
        <lpage>143</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2018.11.010</pub-id>
        <?supplied-pmid 30579222?>
        <pub-id pub-id-type="pmid">30579222</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Balakrishnan</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sabuncu</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Guttag</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Dalca</surname>
            <given-names>AV</given-names>
          </name>
        </person-group>
        <article-title>Voxelmorph: A learning framework for deformable medical image registration</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2019</year>
        <volume>38</volume>
        <fpage>1788</fpage>
        <lpage>1800</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2019.2897538</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Weakly-supervised convolutional neural networks for multimodal image registration</article-title>
        <source>Med. Image Anal.</source>
        <year>2018</year>
        <volume>49</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2018.07.002</pub-id>
        <?supplied-pmid 30007253?>
        <pub-id pub-id-type="pmid">30007253</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hoffmann</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SynthMorph: Learning contrast-invariant registration without acquired images</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2021</year>
        <volume>41</volume>
        <fpage>543</fpage>
        <lpage>558</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2021.3116879</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Casamitjana, A., Mancini, M. &amp; Iglesias, J. E. Synth-by-reg (SbR): Contrastive learning for synthesis-based registration of paired images. In <italic>International Workshop on Simulation and Synthesis in Medical Imaging</italic>, 44–54 (Springer, 2021).</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Billot</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SynthSeg: Segmentation of brain MRI scans of any contrast and resolution without retraining</article-title>
        <source>Med. Image Anal.</source>
        <year>2023</year>
        <volume>1</volume>
        <fpage>102789</fpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2023.102789</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Billot</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Robust machine learning segmentation for large-scale analysis of heterogeneous clinical brain MRI datasets</article-title>
        <source>Proc. Natl. Acad. Sci.</source>
        <year>2023</year>
        <volume>120</volume>
        <fpage>e2216399120</fpage>
        <pub-id pub-id-type="doi">10.1073/pnas.2216399120</pub-id>
        <?supplied-pmid 36802420?>
        <pub-id pub-id-type="pmid">36802420</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vercauteren</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Pennec</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Perchant</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ayache</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Diffeomorphic demons: Efficient non-parametric image registration</article-title>
        <source>Neuroimage</source>
        <year>2009</year>
        <volume>45</volume>
        <fpage>S61</fpage>
        <lpage>S72</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.10.040</pub-id>
        <?supplied-pmid 19041946?>
        <pub-id pub-id-type="pmid">19041946</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Moler</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Van Loan</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Nineteen dubious ways to compute the exponential of a matrix, twenty-five years later</article-title>
        <source>SIAM Rev.</source>
        <year>2003</year>
        <volume>45</volume>
        <fpage>3</fpage>
        <lpage>49</lpage>
        <pub-id pub-id-type="doi">10.1137/S00361445024180</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Modat, M. <italic>et al.</italic> Parametric non-rigid registration using a stationary velocity field. In <italic>2012 IEEE Workshop on Mathematical Methods in Biomedical Image Analysis</italic>, 145–150 (IEEE, 2012).</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Beg</surname>
            <given-names>MF</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Symmetric data attachment terms for large deformation image registration</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2007</year>
        <volume>26</volume>
        <fpage>1179</fpage>
        <lpage>1189</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2007.898813</pub-id>
        <?supplied-pmid 17896591?>
        <pub-id pub-id-type="pmid">17896591</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Iglesias, J. E. <italic>et al.</italic> Is synthesizing MRI contrast useful for inter-modality analysis? In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>, 631–638 (Springer, 2013).</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Krebs</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Delingette</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Mailhé</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Ayache</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Mansi</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Learning a probabilistic model for diffeomorphic registration</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2019</year>
        <volume>38</volume>
        <fpage>2165</fpage>
        <lpage>2176</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2019.2897112</pub-id>
        <?supplied-pmid 30716033?>
        <pub-id pub-id-type="pmid">30716033</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Greer, H., Kwitt, R., Vialard, F.-X. &amp; Niethammer, M. ICON: Learning regular maps through inverse consistency. In <italic>Proceedings of the IEEE/CVF International Conference on Computer Vision</italic>, 3396–3405 (2021).</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Gu, D. <italic>et al.</italic> Pair-wise and group-wise deformation consistency in deep registration network. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>, 171–180 (Springer, 2020).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Zhu, Y. &amp; Lu, S. Swin-voxelmorph: A symmetric unsupervised learning model for deformable medical image registration using swin transformer. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>, 78–87 (Springer, 2022).</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Estienne, T. <italic>et al.</italic> MICS: Multi-steps, inverse consistency and symmetric deep learning registration network. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2111.12123">http://arxiv.org/abs/2111.12123</ext-link> (2021).</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Tian, L. <italic>et al.</italic> GradICON: Approximate diffeomorphisms via gradient inverse consistency. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2206.05897">http://arxiv.org/abs/2206.05897</ext-link> (2022).</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Lee, D. <italic>et al.</italic> Seq2Morph: A deep learning deformable image registration algorithm for longitudinal imaging studies and adaptive radiotherapy. <italic>Med. Phys.</italic> (2022).</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>W</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A coarse-to-fine deformable transformation framework for unsupervised multi-contrast MR image registration with dual consistency constraint</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2021</year>
        <volume>40</volume>
        <fpage>2589</fpage>
        <lpage>2599</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2021.3059282</pub-id>
        <?supplied-pmid 33577451?>
        <pub-id pub-id-type="pmid">33577451</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Zhang, J. Inverse-consistent deep networks for unsupervised deformable image registration. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1809.03443">http://arxiv.org/abs/1809.03443</ext-link> (2018).</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Gao, X., Tao, R. &amp; Zheng, G. BIDMIR: Bi-directional medical image registration with symmetric attention and cyclic consistency regularization. In <italic>2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)</italic>, 1–5 (IEEE, 2022).</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Kim, B. <italic>et al.</italic> Unsupervised deformable image registration using cycle-consistent cnn. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>, 166–174 (Springer, 2019).</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Lowe, D. G. Object recognition from local scale-invariant features. In <italic>Proceedings of the Seventh IEEE International Conference on Computer Vision</italic>, vol. 2, 1150–1157 (IEEE, 1999).</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ma</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Fan</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Image matching from handcrafted to deep features: A survey</article-title>
        <source>Int. J. Comput. Vision</source>
        <year>2021</year>
        <volume>129</volume>
        <fpage>23</fpage>
        <lpage>79</lpage>
        <pub-id pub-id-type="doi">10.1007/s11263-020-01359-2</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Yu, E., Wang, A. Q., Dalca, A. V. &amp; Sabuncu, M. R. KeyMorph: Robust multi-modal affine registration via unsupervised keypoint detection. In <italic>Medical Imaging with Deep Learning</italic> (2021).</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Camara</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Delso</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Colliot</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Moreno-Ingelmo</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bloch</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Explicit incorporation of prior anatomical information into a nonrigid registration of thoracic and abdominal CT and 18-FDG whole-body emission PET images</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2007</year>
        <volume>26</volume>
        <fpage>164</fpage>
        <lpage>178</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2006.889712</pub-id>
        <?supplied-pmid 17304731?>
        <pub-id pub-id-type="pmid">17304731</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other">Pan, W. Submission to Learn2Reg challenge (method “Winter”). <ext-link ext-link-type="uri" xlink:href="https://github.com/WinterPan2017/ADLReg">https://github.com/WinterPan2017/ADLReg</ext-link> (2022).</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fonov</surname>
            <given-names>VS</given-names>
          </name>
          <name>
            <surname>Evans</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>McKinstry</surname>
            <given-names>RC</given-names>
          </name>
          <name>
            <surname>Almli</surname>
            <given-names>CR</given-names>
          </name>
          <name>
            <surname>Collins</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Unbiased nonlinear average age-appropriate brain templates from birth to adulthood</article-title>
        <source>NeuroImage</source>
        <year>2009</year>
        <volume>1</volume>
        <fpage>S102</fpage>
        <pub-id pub-id-type="doi">10.1016/S1053-8119(09)70884-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jack</surname>
            <given-names>CR</given-names>
            <suffix>Jr</suffix>
          </name>
          <etal/>
        </person-group>
        <article-title>The Alzheimer’s disease neuroimaging initiative (ADNI): MRI methods</article-title>
        <source>J. Magn. Reson. Imaging</source>
        <year>2008</year>
        <volume>27</volume>
        <fpage>685</fpage>
        <lpage>691</lpage>
        <pub-id pub-id-type="doi">10.1002/jmri.21049</pub-id>
        <?supplied-pmid 18302232?>
        <pub-id pub-id-type="pmid">18302232</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Glasser</surname>
            <given-names>MF</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The minimal preprocessing pipelines for the Human Connectome Project</article-title>
        <source>Neuroimage</source>
        <year>2013</year>
        <volume>80</volume>
        <fpage>105</fpage>
        <lpage>124</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.04.127</pub-id>
        <?supplied-pmid 23668970?>
        <pub-id pub-id-type="pmid">23668970</pub-id>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hoopes</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mora</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Dalca</surname>
            <given-names>AV</given-names>
          </name>
          <name>
            <surname>Fischl</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Hoffmann</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>SynthStrip: Skull-stripping for any brain image</article-title>
        <source>Neuroimage</source>
        <year>2022</year>
        <volume>260</volume>
        <fpage>119474</fpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119474</pub-id>
        <?supplied-pmid 35842095?>
        <pub-id pub-id-type="pmid">35842095</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Modat</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Global image registration using a symmetric block-matching approach</article-title>
        <source>J. Med. Imaging</source>
        <year>2014</year>
        <volume>1</volume>
        <fpage>024003</fpage>
        <pub-id pub-id-type="doi">10.1117/1.JMI.1.2.024003</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Iglesias</surname>
            <given-names>JE</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SynthSR: A public AI tool to turn heterogeneous clinical brain scans into high-resolution T1-weighted images for 3D morphometry</article-title>
        <source>Sci. Adv.</source>
        <year>2023</year>
        <volume>9</volume>
        <fpage>3607</fpage>
        <pub-id pub-id-type="doi">10.1126/sciadv.add3607</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Iglesias</surname>
            <given-names>JE</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Joint super-resolution and synthesis of 1 mm isotropic MP-RAGE volumes from clinical MRI exams with scans of different orientation, resolution and contrast</article-title>
        <source>Neuroimage</source>
        <year>2021</year>
        <volume>237</volume>
        <fpage>118206</fpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118206</pub-id>
        <?supplied-pmid 34048902?>
        <pub-id pub-id-type="pmid">34048902</pub-id>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Darkner</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Pai</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Liptrot</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Sporring</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Collocation for diffeomorphic deformations in medical image registration</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2017</year>
        <volume>40</volume>
        <fpage>1570</fpage>
        <lpage>1583</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2017.2730205</pub-id>
        <?supplied-pmid 28742029?>
        <pub-id pub-id-type="pmid">28742029</pub-id>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <mixed-citation publication-type="other">Milletari, F., Navab, N. &amp; Ahmadi, S.-A. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In <italic>2016 fourth international conference on 3D vision (3DV)</italic>, 565–571 (IEEE, 2016).</mixed-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &amp; Brox, T. U-net: Convolutional networks for biomedical image segmentation. In <italic>International Conference on Medical image computing and computer-assisted intervention</italic>, 234–241 (Springer, 2015).</mixed-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <mixed-citation publication-type="other">Kingma, D. P. &amp; Ba, J. Adam: A method for stochastic optimization. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</ext-link> (2014).</mixed-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <mixed-citation publication-type="other">Abadi, M. <italic>et al.</italic> TensorFlow: A system for large-scale machine learning. In <italic>12th USENIX symposium on operating systems design and implementation (OSDI 16)</italic>, 265–283 (2016).</mixed-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Chollet</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <source>Deep Learning with Python</source>
        <year>2021</year>
        <publisher-name>Simon and Schuster</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Klein</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Mindboggling morphometry of human brains</article-title>
        <source>PLoS Comput. Biol.</source>
        <year>2017</year>
        <volume>13</volume>
        <fpage>e1005350</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1005350</pub-id>
        <?supplied-pmid 28231282?>
        <pub-id pub-id-type="pmid">28231282</pub-id>
      </element-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fischl</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Whole brain segmentation: Automated labeling of neuroanatomical structures in the human brain</article-title>
        <source>Neuron</source>
        <year>2002</year>
        <volume>33</volume>
        <fpage>341</fpage>
        <lpage>355</lpage>
        <pub-id pub-id-type="doi">10.1016/S0896-6273(02)00569-X</pub-id>
        <?supplied-pmid 11832223?>
        <pub-id pub-id-type="pmid">11832223</pub-id>
      </element-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <mixed-citation publication-type="other">Collignon, A. <italic>et al.</italic> Automated multi-modality image registration based on information theory. In <italic>Information processing in medical imaging</italic>, vol. 3, 263–274 (Citeseer, 1995).</mixed-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sorensen</surname>
            <given-names>TA</given-names>
          </name>
        </person-group>
        <article-title>A method of establishing groups of equal amplitude in plant sociology based on similarity of species content and its application to analyses of the vegetation on Danish commons</article-title>
        <source>Biol. Skar.</source>
        <year>1948</year>
        <volume>5</volume>
        <fpage>1</fpage>
        <lpage>34</lpage>
      </element-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <mixed-citation publication-type="other">Hoopes, A., Hoffmann, M., Fischl, B., Guttag, J. &amp; Dalca, A. V. Hypermorph: Amortized hyperparameter learning for image registration. In <italic>International Conference on Information Processing in Medical Imaging</italic>, 3–17 (Springer, 2021).</mixed-citation>
    </ref>
    <ref id="CR67">
      <label>67.</label>
      <mixed-citation publication-type="other">Mok, T. C. &amp; Chung, A. C. Conditional deformable image registration with convolutional neural network. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>, 35–45 (Springer, 2021).</mixed-citation>
    </ref>
    <ref id="CR68">
      <label>68.</label>
      <mixed-citation publication-type="other">Lv, J. <italic>et al.</italic> Joint progressive and coarse-to-fine registration of brain MRI via deformation field integration and non-rigid feature fusion. <italic>IEEE Trans. Med. Imaging</italic> (2022).</mixed-citation>
    </ref>
  </ref-list>
</back>
