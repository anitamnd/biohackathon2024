<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-archivearticle1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Research (Wash D C)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Research (Wash D C)</journal-id>
    <journal-id journal-id-type="publisher-id">RESEARCH</journal-id>
    <journal-title-group>
      <journal-title>Research</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2639-5274</issn>
    <publisher>
      <publisher-name>AAAS</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10232324</article-id>
    <article-id pub-id-type="doi">10.34133/research.0153</article-id>
    <article-id pub-id-type="publisher-id">0153</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Enzyme Commission Number Prediction and Benchmarking with Hierarchical Dual-core Multitask Learning Framework</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Shi</surname>
          <given-names>Zhenkun</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Deng</surname>
          <given-names>Rui</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yuan</surname>
          <given-names>Qianqian</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mao</surname>
          <given-names>Zhitao</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Ruoyu</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Haoran</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Liao</surname>
          <given-names>Xiaoping</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
        <xref rid="corr1" ref-type="corresp">
          <sup>*</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Ma</surname>
          <given-names>Hongwu</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="corr1" ref-type="corresp">
          <sup>*</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff1"><label><sup>1</sup></label><addr-line>Biodesign Center, Key Laboratory of Engineering Biology for Low-carbon Manufacturing, Tianjin Institute of Industrial Biotechnology</addr-line>, 
<institution>Chinese Academy of Sciences</institution>, 300308, Tianjin, <country>China</country>.</aff>
    <aff id="aff2"><label><sup>2</sup></label><institution>National Center of Technology Innovation for Synthetic Biology</institution>, 300308, Tianjin, <country>China</country>.</aff>
    <aff id="aff3"><label><sup>3</sup></label><addr-line>College of Biotechnology</addr-line>, 
<institution>Tianjin University of Science &amp; Technology</institution>, Tianjin, <country>China</country>.</aff>
    <aff id="aff4"><label><sup>4</sup></label><institution>Haihe Laboratory of Synthetic Biology</institution>, 300308, Tianjin, <country>China</country>.</aff>
    <author-notes>
      <corresp id="corr1"><label>*</label>Address correspondence to: <email xlink:href="mailto:liao_xp@tib.cas.cn">liao_xp@tib.cas.cn</email> (X.L.); <email xlink:href="mailto:ma_hw@tib.cas.cn">ma_hw@tib.cas.cn</email> (H.M.)</corresp>
    </author-notes>
    <pub-date publication-format="electronic" date-type="pub">
      <day>31</day>
      <month>5</month>
      <year>2023</year>
    </pub-date>
    <pub-date publication-format="electronic" date-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>6</volume>
    <elocation-id>0153</elocation-id>
    <history>
      <date date-type="received">
        <day>07</day>
        <month>3</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>28</day>
        <month>4</month>
        <year>2023</year>
      </date>
      <date date-type="pub">
        <day>31</day>
        <month>5</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2023 Zhenkun Shi et al.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Zhenkun Shi et al.</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>Exclusive licensee Science and Technology Review Publishing House. No claim to original U.S. Government Works. Distributed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License (CC BY 4.0)</ext-link>.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="research.0153.pdf"/>
    <abstract>
      <p>Enzyme commission (EC) numbers, which associate a protein sequence with the biochemical reactions it catalyzes, are essential for the accurate understanding of enzyme functions and cellular metabolism. Many ab initio computational approaches were proposed to predict EC numbers for given input protein sequences. However, the prediction performance (accuracy, recall, and precision), usability, and efficiency of existing methods decreased seriously when dealing with recently discovered proteins, thus still having much room to be improved. Here, we report HDMLF, a hierarchical dual-core multitask learning framework for accurately predicting EC numbers based on novel deep learning techniques. HDMLF is composed of an embedding core and a learning core; the embedding core adopts the latest protein language model for protein sequence embedding, and the learning core conducts the EC number prediction. Specifically, HDMLF is designed on the basis of a gated recurrent unit framework to perform EC number prediction in the multi-objective hierarchy, multitasking manner. Additionally, we introduced an attention layer to optimize the EC prediction and employed a greedy strategy to integrate and fine-tune the final model. Comparative analyses against 4 representative methods demonstrate that HDMLF stably delivers the highest performance, which improves accuracy and F1 score by 60% and 40% over the state of the art, respectively. An additional case study of tyrB predicted to compensate for the loss of aspartate aminotransferase aspC, as reported in a previous experimental study, shows that our model can also be used to uncover the enzyme promiscuity. Finally, we established a web platform, namely, ECRECer (<ext-link xlink:href="https://ecrecer.biodesign.ac.cn" ext-link-type="uri">https://ecrecer.biodesign.ac.cn</ext-link>), using an entirely could-based serverless architecture and provided an offline bundle to improve usability.</p>
    </abstract>
    <counts>
      <fig-count count="6"/>
      <table-count count="1"/>
      <ref-count count="36"/>
      <page-count count="0"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <title>Introduction</title>
    <p>With the widespread adoption of high-throughput methods and high-quality infrastructure, the speed of new protein discovery has increased dramatically. However, this was not followed by a concomitant increase in the speed of protein annotation. For example, 801,118 sequences were added to TrEMBL in the UniProt database [<xref rid="B1" ref-type="bibr">1</xref>] in the single month of December 2022, while only 388 sequences were reviewed and added to Swiss-Prot in the same period (Fig. <xref rid="supplementary-material-1" ref-type="sec">S2</xref>). Such a slow speed of protein annotation considerably restricts related research and industrial applications.</p>
    <p>Among the multiple and complex protein annotation tasks, one of the crucial steps is enzyme function annotation [<xref rid="B2" ref-type="bibr">2</xref>,<xref rid="B3" ref-type="bibr">3</xref>]. Annotations of enzyme function provide critical starting points for generating and testing biological hypotheses [<xref rid="B3" ref-type="bibr">3</xref>]. Current functional annotations of enzymes describe the biochemistry or process by assigning an enzyme commission (EC) number. An EC number is a 4-part code associated with a recommended name for the corresponding enzyme-catalyzed reaction that describes the enzyme class, the chemical bond acted on, the reaction, and the substrates [<xref rid="B4" ref-type="bibr">4</xref>]. Thus, the primary task of enzyme annotation is to assign an EC number to a given protein sequence. However, as the uncertainty of the assignments for uncharacterized protein sequences is high and biochemical data are relatively sparse, both the speed and the quality of enzyme annotation are considerably restricted.</p>
    <p>To achieve improved, rapid, and intelligent functional annotation, computational methods were introduced to assign or predict EC numbers. The simplest and most commonly used method is based on sequence homology [<xref rid="B5" ref-type="bibr">5</xref>]; researchers have developed a variety of EC databases and profile-based methods for the functional annotation of enzymes [<xref rid="B6" ref-type="bibr">6</xref>–<xref rid="B8" ref-type="bibr">8</xref>]. However, these methods cannot perform annotations for novel proteins without similar sequences, which is generally the case for newly discovered enzymes. To overcome this restriction, researchers introduced machine learning (ML) methods, such as hidden Markov model [<xref rid="B9" ref-type="bibr">9</xref>], K-nearest neighbor (KNN) [<xref rid="B10" ref-type="bibr">10</xref>], and SVM [<xref rid="B11" ref-type="bibr">11</xref>] for annotating enzymes. Although these methods can predict EC numbers for proteins without similar references, the prediction speed and precision are not ideal. Because deep learning has delivered powerful results in many areas [<xref rid="B12" ref-type="bibr">12</xref>–<xref rid="B15" ref-type="bibr">15</xref>], researchers use deep learning methods to predict EC numbers and continually improve the precision of functional annotation [<xref rid="B2" ref-type="bibr">2</xref>]. However, deep learning methods are prone to overfitting because of an unbalanced distribution of training datasets [<xref rid="B16" ref-type="bibr">16</xref>]. Specific to the EC number prediction task, this would lead to predictions with high precision, medium recall, and low accuracy.</p>
    <p>Overall, there has been a steady improvement in computational methods for enzyme annotation [<xref rid="B2" ref-type="bibr">2</xref>,<xref rid="B7" ref-type="bibr">7</xref>,<xref rid="B17" ref-type="bibr">17</xref>,<xref rid="B18" ref-type="bibr">18</xref>], but several obstacles still exist that have slowed the progress of computational enzyme function annotation. The first is a lack of publicly available benchmark datasets to evaluate the existing and newly proposed models, making it troublesome for the end user to choose the best method in their production scenario. Another hindrance is the lack of an explicitly designed method with stable prediction performance to deal with newly discovered proteins. Especially, the lacking of an efficient and universal protein sequence embedding method made researchers have to spend large amounts of time on handcrafted feature engineering to encode the sequence, such as functional domain encoding [<xref rid="B19" ref-type="bibr">19</xref>] and position-specific scoring matrix (PSSM) encoding [<xref rid="B20" ref-type="bibr">20</xref>], as encoding quality dramatically affects the performance of downstream applications [<xref rid="B21" ref-type="bibr">21</xref>]. The third one is the usability of existing tools, as some tools are only available offline and are not easy to install, while some online tools are no longer available.</p>
    <p>To overcome these obstacles, we proposed a hierarchical dual-core multitask learning framework (HDMLF) in this work. The main contributions are as follows:<list list-type="simple"><list-item><label>•</label><p>We constructed 3 standard datasets for benchmarking and evaluation. The datasets contain more than 470,000 distinct labeled protein sequences from Swiss-Prot.</p></list-item><list-item><label>•</label><p>We proposed a novel framework on the basis of the latest protein language embedding methods and gated recurrent unit (GRU) with the attention mechanism. In HDMLF, we formulate the EC number prediction as a multitask multilabel classification problem. The first task predicts whether a given protein sequence is an enzyme. The second task predicts how many functions the enzyme can perform, i.e., multifunctional enzyme prediction. The last task predicts the exact EC number for each enzyme function. To achieve cutting-edge performance for EC number prediction, we first introduced and evaluated the state-of-the-art deep learning language embedding methods for universal protein sequence embedding [<xref rid="B22" ref-type="bibr">22</xref>,<xref rid="B23" ref-type="bibr">23</xref>]. Then, a novel prediction method based on GRU with an attention mechanism was proposed to solve the 3 tasks in a multitasking manner. Finally, a feedback mechanism is adopted to choose the most suitable embedding, and a greedy strategy is introduced to integrate these tasks to maximize the EC prediction performance.</p></list-item><list-item><label>•</label><p>A webserver was published for easy usability. We published a web platform based on a serverless architecture so that anyone can annotate EC numbers smoothly in high throughput, whether they have coding experience or not. Our webserver is publicly accessed via <ext-link xlink:href="http://ecrecer.biodesign.ac.cn" ext-link-type="uri">http://ecrecer.biodesign.ac.cn</ext-link>.</p></list-item></list></p>
  </sec>
  <sec id="sec2">
    <title>Results</title>
    <sec id="sec3">
      <title>A dataset for benchmarking</title>
      <p>Because of the lack of a public benchmark for EC number prediction, we constructed a standard dataset from Swiss-Prot for model development and evaluation. To simulate real application scenarios as closely as possible, we organized data chronologically. Specifically, we used a snapshot from February 2018 as the training dataset, consisting of 469,134 records (4,854 distinct EC numbers). We construct 2 testing sets to simulate the real protein discovery and annotation processes and validate the EC prediction performance of effectiveness and stability with time variance. Testing set 1 (testset_20) is from the June 2020 snapshot, consisting of 7,101 records (937 distinct EC numbers). Testing set 2 (testset_22) is from February 2022, consisting of 10,614 records (1,355 distinct EC numbers). All testing sets filtered the sequences that appeared in the training set. The details are listed in Tables <xref rid="supplementary-material-1" ref-type="sec">S2</xref> to <xref rid="supplementary-material-1" ref-type="sec">S5</xref>.</p>
    </sec>
    <sec id="sec4">
      <title>Suitable embedding methods do help in improving the prediction performance</title>
      <p>ML models trained on protein sequences and their measured functions can infer unseen sequences’ biological properties without understanding the underlying physical or biological mechanisms. However, ML models require vectors as input other than amino acid sequences, and converting from a protein sequence to a vector representation extremely affects the model’s ability to learn [<xref rid="B21" ref-type="bibr">21</xref>].</p>
      <p>To evaluate and choose the best embedding methods for our downstream prediction tasks, we evaluated 3 different protein embedding methods, one-hot embedding, UniRep embedding, and evolutionary scale modeling embedding method (ESM) embedding with different layers (from 1 to 33) in 2 different testing datasets. The evaluation process can be categorized into 2: (a) We use an enzyme or non-enzyme prediction task and 6 ML baselines to evaluate the embedding performance in binary classifications, and (b) we use the EC prediction task and our proposed method to evaluate the embedding performance in multiclass classifications. The 6 ML baselines are KNN, logistic regression (LR), XGBoost, decision tree (DT), random forest (RF), and gradient boosting decision tree (GBDT).<list list-type="simple"><list-item><label>•</label><p>Protein embedding methods can learn semantic information from sequences directly, remarkably improving downstream tasks’ performance.</p></list-item></list></p>
      <p>As shown in Fig. <xref rid="F1" ref-type="fig">1</xref>, compared with the traditional one-hot protein representation, methods UniRep and ESM improve the downstream task by more than 20% in all testing tasks, in terms of F1 score (details can be found in Tables <xref rid="supplementary-material-1" ref-type="sec">S6</xref> and <xref rid="supplementary-material-1" ref-type="sec">S7</xref>). For embedding, ESM-32 exhibited the best overall performance among all 6 baselines regarding all evaluation metrics for embedding. As shown in Fig. <xref rid="F1" ref-type="fig">1</xref>C and D, in the EC prediction task, ESM-32 achieved 21.67% and 6.03% improvements over one-hot and UniRep in terms of accuracy, as well as 27.20% and 7.32% in terms of mF1, respectively. This experiment suggests that better embedding can lead to better learning performance, and deep latent representation can comprehensively represent the protein sequence. Apart from protein sequences, protein structure can also be helpful in conducting protein embedding, and existing studies have demonstrated this [<xref rid="B24" ref-type="bibr">24</xref>,<xref rid="B25" ref-type="bibr">25</xref>]. In our future work, we plan to incorporate these techniques to further enhance the prediction performance.<list list-type="simple"><list-item><label>•</label><p>For embedding layers, not the deeper the better.</p></list-item></list></p>
      <fig position="float" id="F1">
        <label>Fig. 1.</label>
        <caption>
          <p>Performance comparison of different embedding methods. (A and B) Embedding performance (F1 score) comparison using ML baselines on task 1. (A) Results on testset_20 and (B) on testset_22. (C and D) Embedding performance comparison using our proposed framework HDMLF on task 3; mACC, mPR, mRecall, and mF1 are different evaluation metrics defined in the “Evaluation metrics” section. (C) Results on testset_20 and (D) on testset_22.</p>
        </caption>
        <graphic xlink:href="research.0153.fig.001" position="float"/>
      </fig>
      <p>To validate the effectiveness within different depths of embedding layers, we evaluated embedding layers 1 to 33 from ESM under 4 classification baseline methods. Interestingly, we found that, when layers increase from 1 to 32, the performance increases, while when layers reach 32, the performance began to decrease (Fig. <xref rid="F1" ref-type="fig">1</xref>); this is mainly due to the overfitting issue and suggests that, for selection of embedding layers, not the deeper, the better.</p>
    </sec>
    <sec id="sec5">
      <title>HDMLF versus existing EC prediction methods</title>
      <p>To achieve the best performance, HDMLF is proposed with 3 objectives: (a) classify the enzyme and non-enzyme with high accuracy, (b) provide the ability to predict multifunctional enzymes, and (c) achieve state-of-the-art EC prediction performance, and we treat these objectives as 3 learning tasks. To meet these objectives, our proposed framework is designed with mutitask learning techniques and is organized in a hierarchical order.</p>
      <p>To evaluate the performance of HDMLF and validate whether these objectives do help our final goal, we made a comprehensive comparison experiment with existing EC prediction methods among all these tasks.</p>
      <sec id="sec6">
        <title>EC number prediction performance comparison</title>
        <p>EC prediction is the main task and final goal in HDMLF. As shown in Fig. <xref rid="F2" ref-type="fig">2</xref>C, our proposed methods achieved the best overall performance in testset_20. PRIAM [<xref rid="B7" ref-type="bibr">7</xref>] is mainly designed to include more sequences, so the mRecall is high (78.48%, 75.26%), while the mPR (20.80%, 25.03%) is very low. DeepEC, ECPred, and CatFam pursue high precision; these methods are very likely to miss many new functions, which, in turn, wound underperform in terms of accuracy and F1 score. The F1 score should be a better evaluation metric for the EC assignment of real-world proteins. In terms of mF1 score, our model reached over 150% performance than the second best method, which achieved 86.91% accuracy with 69% mPR and 63.88% mRecall. In other words, if 100 protein sequences were uploaded for annotation, then we can obtain approximately 87 correct annotations. Interestingly, existing methods show much poorer performance on testset_22 while our method can maintain the prediction performance. We conducted further validation of the HDMLF’s performance in predicting EC numbers at first to third levels compared to other baselines. The results are shown in Tables <xref rid="supplementary-material-1" ref-type="sec">S11</xref> to <xref rid="supplementary-material-1" ref-type="sec">S13</xref>. Our findings demonstrate that the HDMLF outperforms the second-best method by more than 20%, 28%, and 30% in terms of mF1 at the first, second, and third levels of EC, respectively. However, when we simplify the evaluation criteria and only consider the first-digit EC, the mF1 score remains below 70%. Note that these methods were specifically designed to predict the fully 4-digit EC number, which is not optimal for first-digit EC prediction.</p>
        <fig position="float" id="F2">
          <label>Fig. 2.</label>
          <caption>
            <p>Performance comparison of different methods for different prediction tasks. Bars with <inline-graphic xlink:href="research.0153.inline.001.jpg"/> represent prediction results on testset_20, bars with <inline-graphic xlink:href="research.0153.inline.002.jpg"/> represent prediction results on testset_22 (A) Performance comparison on EC number prediction. (B) Performance comparison on enzyme and non-enzyme prediction. (C) Performance comparison on task 2 multifunctional enzyme prediction.</p>
          </caption>
          <graphic xlink:href="research.0153.fig.002" position="float"/>
        </fig>
        <p>Overall, in testset_22, for these relatively new proteins, our method much improved the mF1 score over existing methods; this is mainly due to the EC numbers and enzyme samples being more inclusive in testset_22. All the above results show that our method shows a clear advantage in terms of EC number assignment.</p>
      </sec>
      <sec id="sec7">
        <title>Enzyme or non-enzyme prediction performance comparison</title>
        <p>In enzyme or non-enzyme prediction task, as shown in Fig. <xref rid="F2" ref-type="fig">2</xref>B, our method can achieve scores of 92.01% and 93.61% in terms of F1 score in the above-constructed testing sets, respectively. Compared with other state-of-the-art tools, the overall accuracy was greatly improved. Many previous methods were designed to obtain high precision while neglecting accuracy, negative predictive value, and recall. For example, DeepEC can reach 94.68% precision while recall is only 20.83% in testset_20, which means that many enzymes would be missed by DeepEC prediction.</p>
      </sec>
      <sec id="sec8">
        <title>Multifunctional enzyme prediction</title>
        <p>The multifunctional enzyme prediction task is designed to predict the number of ECs assigned to a given protein. As shown in Fig. <xref rid="F2" ref-type="fig">2</xref>C, we can see that the performance of our proposed method is stable and superior to existing baselines. In testset_20, our method achieved 91.71% accuracy with 58.37% mPR and 55.20% mRecall recall. In testset_22, our method achieved 92.45% accuracy with 70.68% mPR and 59.56% mRecall recall. The low mRecall and mPR are mainly due to the data sparseness of 3 to 8 functional enzymes , which results in the classifier being more preferred to predict an enzyme as a single-function enzyme. Although our proposed method achieved the best performance among existing methods, this is still inadequate in a productive scenario, especially for enzymes assigned with more than 3 EC numbers, so it should be further improved in future work.</p>
      </sec>
    </sec>
    <sec id="sec9">
      <title>Assessing the stability of EC prediction performance over time</title>
      <p>To assess the efficacy and predictability of our proposed framework over time, we simulated an EC prediction experiment using Swiss-Prot data in a realistic setting. We first collected the snapshot of Swiss-Prot from 2019 to 2022. Then, we excluded the data that appeared in the training set and used the trained model to predict the new data added from 2019 to 2022. Comparative results with existing EC prediction methods are shown in Fig. <xref rid="F3" ref-type="fig">3</xref>. From Fig. <xref rid="F3" ref-type="fig">3</xref>, we can observe that all methods’ prediction performance will decrease over time. In terms of baselines, DeepEC and CatFam are relatively more stable than the others. For DeepEC, the performance in terms of F1 score dropped by 3.20%, 11.34%, and 21.20% from 2020 to 2022, compared to 2019, and decreased by 9.86%, 19.94%, and 29.48% in terms of accuracy. The reason for this is that DeepEC uses one-hot embedding and 3 different models to predict the different levels of EC, and it integrates the output using the sequences alignment method. While the first level of EC prediction is much more precise, one-hot embedding cannot preserve sufficient information to represent protein sequences. Moreover, it is weak in the fourth level of EC prediction, and the sequence alignment method cannot handle new sequences with low similarities. On the other hand, CatFam’s performance dropped by −0.4%, 3.28%, and 9.95% from 2020 to 2022, compared to 2019, in terms of F1 score and decreased by 9.02%, 8.99%, and 28.25% in terms of accuracy. This is because CatFam is based on homologous similarity and PSSM. It is more stable for protein sequences with existing homologous sequences than DeepEC. However, the performance will decrease significantly for newly added proteins without homologous sequences. Remarkably, the prediction performance of our proposed method is more stable, decreasing by less than 1% from 2019 to 2022, both in terms of F1 score and accuracy. This mainly benefits from the language model, which is powerful in sequence embedding and can reserve sufficient information for downstream applications, and we treat 4-level EC as a tuple that reduces the integration error from different predictive models. Thus, our proposed method is considerably more statable than the existing ones.</p>
      <fig position="float" id="F3">
        <label>Fig. 3.</label>
        <caption>
          <p>Task 1 comparison of EC prediction performance over time.</p>
        </caption>
        <graphic xlink:href="research.0153.fig.003" position="float"/>
      </fig>
    </sec>
    <sec id="sec10">
      <title>Case study: HDMLF can annotate protein with incomplete EC number</title>
      <p>In the databases, many enzymes with EC numbers exist in an uncompleted 3-level, 2-level, or even 1-level state. These proteins with incomplete EC numbers might not directly be utilized for retrieving enzymatic reactions. For instance, an enzyme iron/alpha-ketoglutarate-dependent dioxygenase AusU (UniProt ID: A0A0U5GJ41) has a 2-level EC number in the database (1.14.-.-), while our method HDMLF can assign this protein with the fourth-level EC number 1.14.11.38. After blasting it against the UniProt database, we find that the top 5 reviewed proteins with the highest identities include 3 verruculogen synthases (Fig. <xref rid="F4" ref-type="fig">4</xref>A). Because only Q4WAW9 has a crystal structure, we take protein Q4WAW9 [<xref rid="B26" ref-type="bibr">26</xref>] as an example and find that both genes belong to exactly the same protein families with the same domains (Fig. <xref rid="F4" ref-type="fig">4</xref>B). To further validate the results, we compare the structure of A0A0U5GJ41 (alphfold2 predicted) and Q4WAW9 (alphfold2 predicted and crystal structure). The results show that these 2 proteins have a highly similar structure (Figs. <xref rid="supplementary-material-1" ref-type="sec">S4</xref> to <xref rid="supplementary-material-1" ref-type="sec">S7</xref>) with a small root mean square deviation (1.104). Because Q4WAW9 has a 4-level EC number 1.14.11.38, the protein A0A0U5GJ41 could be potentially annotated as EC 1.14.11.38 as well, which supports our 4-level prediction.</p>
      <fig position="float" id="F4">
        <label>Fig. 4.</label>
        <caption>
          <p>(A to C) Comparison of sequence similarity and structural similarity.</p>
        </caption>
        <graphic xlink:href="research.0153.fig.004" position="float"/>
      </fig>
    </sec>
    <sec id="sec11">
      <title>Case study: HDMLF can uncover enzyme promiscuity</title>
      <p>Enzyme promiscuity toward substrates has been discussed in evolutionary terms as providing the flexibility to adapt to novel environments. Moreover, it has been demonstrated that many enzymes exhibit flexibility, or promiscuity, in regard to what substrates their catalytic pockets recognize. A previous study using model-driven approaches found that even in <italic toggle="yes">Escherichia coli</italic>, a well-studied model species, many underground reactions still occur [<xref rid="B27" ref-type="bibr">27</xref>]. For example, gene essentiality analysis and gene knockout experiment revealed that gene tyrB, which is annotated as tyrosine aminotransferase (EC number 2.6.1.57; 2.6.1.107 in UniProt), also has aspartate aminotransferase (encoded by aspC; EC number 2.6.1.1 in UniProt) activity and thus can compensate for aspC gene deletion [<xref rid="B28" ref-type="bibr">28</xref>]. The knockout of the gene encoding the potential isozyme revealed that tyrosine aminotransferase, which is encoded by tyrB (EC number 2.6.1.57; 2.6.1.107 in UniProt), can compensate for the loss of aspartate aminotransferase, which is encoded by aspC (EC number 2.6.1.1 in UniProt).</p>
      <p>Although the specific functional annotation and substrate are different, our method can uncover the underground reaction 2.6.1.1 for tyrB (Fig. <xref rid="F5" ref-type="fig">5</xref>C) . In addition, our method can also assign EC numbers 2.6.1.57 and 2.6.1.107 to tryB. However, with DeepEC, only 2.6.1.57 can be assigned. This can be further validated by the EcoCyc [<xref rid="B29" ref-type="bibr">29</xref>], a well-annotated database for <italic toggle="yes">E. coli</italic> MG1655, as tryB was assigned with EC number 2.6.1.1 in EcoCyc. In addition, there are 852 proteins with more than 90% similarity with tyrB in the UniProt database based on a homology search. Only one protein(UniProt ID: P74861) was reviewed and the assigned EC number is 2.6.1.57. All the other proteins were assigned with EC number 2.6.1.-. Because these proteins all have very high similarity with tryB, they should all have the EC number 2.6.1.1. As expected, our model can assign these proteins with EC number 2.6.1.1. All these cases show that our model can be used to uncover enzyme promiscuity.</p>
      <fig position="float" id="F5">
        <label>Fig. 5.</label>
        <caption>
          <p>(A) TryB is assigned with EC numbers 2.6.1.57 and 2.6.1.107 in UniProt. (B) Our model can assign TyrB with additional EC number 2.6.1.1, and this is consistent with a previous experimental study in <italic toggle="yes">Escherichia coli</italic>, which finds that tyrB can compensate for the loss of aspC (EC number 2.6.1.1 in UniProt), while DeepEC can only assign tyrB with EC number 2.6.1.57.</p>
        </caption>
        <graphic xlink:href="research.0153.fig.005" position="float"/>
      </fig>
    </sec>
    <sec id="sec12">
      <title>Case study: HDMLF can uncover unknown enzyme for a not well-annotated genome</title>
      <p>To demonstrate the inclusiveness and predictive ability of our proposed method, we conducted EC number prediction on unreviewed proteins. <italic toggle="yes">Corynebacterium glutamicum</italic>, the famous industrial workhorse for amino acid production with a current output of over 6 million tons per year [<xref rid="B30" ref-type="bibr">30</xref>], is increasingly being adopted as a promising chassis for the biosynthesis of other compounds. However, unlike <italic toggle="yes">E. coli</italic> (1,652 protein sequences with EC numbers out of 4,322 proteins, 38.2%), the protein sequences of <italic toggle="yes">C. glutamicum</italic> were not well annotated. Of 3,305 protein sequences, only 537 were reviewed and included in the Swiss-Prot database (357 proteins have assigned EC numbers). We used the other 2,768 protein sequences to compare our tool with DeepEC. Our approach was able to assign 1,056 proteins with EC numbers, while DeepEC only assigned 157 EC numbers (123 same EC numbers between DeepEC and HDMLF). We found many interesting cases, for example, pyrimidine reductase, an enzyme involved in riboflavin biosynthesis (UniProt ID: Q8NPB8); DeepEC and TrEMBL both predict it to be non-enzyme, which is not correct, while our model predicts it with EC number 1.1.1.302 with high confidence. Another case is deoxynucleoside monophosphate kinase (UniProt ID:Q8NPP0); DeepEC and TrEMBL cannot assign an EC number, while our model predicts it with EC number 2.7.4.13. As shown in the Rhea database (RHEA:11216), the corresponding reaction and enzyme class are all consistent with protein annotation. These results again show that our method can better annotate new proteins than existing annotation tools.</p>
    </sec>
    <sec id="sec13">
      <title>ECRECer: A web platform for EC prediction based on HDMLF</title>
      <p>To enhance the usability of our proposed framework (Fig. <xref rid="supplementary-material-1" ref-type="sec">S8</xref>) so that the end user can use them smoothly even with no coding experience, we built a web application using a cloud-based architecture, offering high reliability, robustness, and scalability. The static files such as the HTML pages and JavaScript codes are hosted on AWS S3 and distributed through AWS CloudFront for speed of content delivery to users worldwide. End users can simply upload sequences to our platform and then click the submit button to trigger the prediction workflow. AWS API Gateway routes HTTP requests from the front end to the backend. We used AWS Step Functions to coordinate the components of our applications, process messages passed from AWS API Gateway, and invoke the workflows asynchronously. We used AWS S3 to exchange data between jobs and store the result files. In general, the whole workflow can be completed in a few seconds. We use Amazon DynamoDB to store job information, and users can track their previous submission records and corresponding status information. Once the analysis is finished, the user can view or download the corresponding results.</p>
    </sec>
  </sec>
  <sec id="sec24">
    <title>Conclusion</title>
    <p>In this work, we proposed a novel HDMLF to complete 3 benchmarking tasks: (a) enzyme or non-enzyme annotation, (b) quantity of EC numbers prediction, and (c) EC number prediction. The method developed in this work has 2 cores, an embedding core and a learning core. The embedding core is responsible for selecting the best available embedding method among one-hot, UniRep, and ESM to calculate sequence embeddings. The learning core is responsible for completing the specific benchmarking tasks using the best-calculated protein sequence embedding as input in a hierarchical multitask way.</p>
    <p>We were guided by 2 principles in the design of HDMLF. The first principle is providing state-of-the-art EC number prediction performance. The second principle is high usability (both can be accessed via the world wide web and offer a standalone suit for high-throughput prediction). To implement the first principle, we proposed HDMLF, which integrates the protein language model with a hierarchical BGRU with an attention mechanism. To implement the second principle, we provided a web server (ECRECer) and a standalone package. We opened all the source codes, including data preprocessing, dataset buildup, model training, and model testing/evaluation.</p>
    <p>Comprehensive comparisons with existing state-of-the-art methods demonstrated that our method is highly competitive and has the best performance with high usability. In addition, our method can be used to uncover enzyme promiscuity. Although our method exhibited the best performance, it still needs improvement. For example, the performance of multifunctional enzyme annotation is relatively low, while the accuracy and recall of EC number annotation are less than 90%.
<table-wrap position="anchor" id="T1"><table frame="hsides" rules="groups"><tbody><tr><td align="left" rowspan="1" colspan="1">Key points:<break/>
<p><list list-type="simple"><list-item><label>•</label><p>An HDMLF framework is proposed to predict EC numbers by using protein sequence data.</p></list-item><list-item><label>•</label><p>A protein language model and an extreme multilabel classifier are adopted to reduce the heavy head-crafted feature engineering and elevate the prediction performance.</p></list-item><list-item><label>•</label><p>The proposed framework remarkably outperforms the existing state-of-the-art method in terms of accuracy and mF1 score by 70% and 20%, respectively.</p></list-item><list-item><label>•</label><p>An online service and an offline bundle are provided for end users to annotate EC numbers in high throughput easily and efficiently.</p></list-item></list></p></td></tr></tbody></table></table-wrap></p>
  </sec>
  <sec id="sec14">
    <title>Materials and Methods</title>
    <sec id="sec15">
      <title>Problem formulation</title>
      <p>To annotate the enzyme function of a new protein sequence, the initial and basic task is to define whether the given protein is an enzyme. Because there are numerous multifunctional enzymes, the next task is to determine the quantity of EC numbers. After completing the above 2 tasks, it is necessary to assign an EC number to each function. On the basis of these considerations, we proposed 3 basic tasks for the functional annotation of enzymes, as shown below.</p>
      <sec id="sec16">
        <title>Enzyme or non-enzyme annotation</title>
        <p>The enzyme or non-enzyme annotation task is formulated as a binary classification problem:<disp-formula id="EQ1"><mml:math id="M1" display="block" overflow="scroll"><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:mi>X</mml:mi><mml:mo>→</mml:mo><mml:mfenced open="{" close="}" separators=","><mml:mn>0</mml:mn><mml:mn>1</mml:mn></mml:mfenced></mml:math><label>(1)</label></disp-formula>where <italic toggle="yes">X</italic> = {<italic toggle="yes">x</italic><sub>1</sub>, <italic toggle="yes">x</italic><sub>2</sub>, ⋯, <italic toggle="yes">x<sub>n</sub></italic>}, <italic toggle="yes">n</italic> ≥ 1 represents a group of protein sequences, and {0, 1} is the label indicating whether a given protein is an enzyme.</p>
      </sec>
      <sec id="sec17">
        <title>Multifunctional enzyme annotation</title>
        <p>Multifunctional enzyme annotation is formulated as a multiclassification problem:<disp-formula id="EQ2"><mml:math id="M2" display="block" overflow="scroll"><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:mi>X</mml:mi><mml:mo>→</mml:mo><mml:mfenced open="{" close="}" separators=",,,"><mml:mn>1</mml:mn><mml:mn>2</mml:mn><mml:mo>⋯</mml:mo><mml:mi>k</mml:mi></mml:mfenced><mml:mo>,</mml:mo></mml:math><label>(2)</label></disp-formula></p>
        <p>where <italic toggle="yes">k</italic> represents the maximum number of EC numbers for a given protein.</p>
      </sec>
      <sec id="sec18">
        <title>EC number assignment</title>
        <p>The EC number assignment task is also formulated as a multiclassification problem as defined in <xref rid="EQ3" ref-type="disp-formula">Eq. 3</xref>.<disp-formula id="EQ3"><mml:math id="M3" display="block" overflow="scroll"><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:mi>X</mml:mi><mml:mo>→</mml:mo><mml:mfenced open="{" close="}" separators=",,"><mml:mn>1.1.1.1</mml:mn><mml:mn>1.1.1.2</mml:mn><mml:mo>⋯</mml:mo></mml:mfenced><mml:mo>,</mml:mo></mml:math><label>(3)</label></disp-formula></p>
      </sec>
    </sec>
    <sec id="sec19">
      <title>Dataset description</title>
      <p>To address the first challenge, we constructed 3 standard datasets (Supplementary Materials). Similar to previous work [<xref rid="B21" ref-type="bibr">21</xref>,<xref rid="B25" ref-type="bibr">25</xref>], these datasets are extracted from the Swiss-Prot database. To simulate real application scenarios as closely as possible, we did not shuffle data randomly. Instead, after data preprocessing (Supplementary Materials), we organized data in chronological order. Specifically, we used a snapshot from February 2018 as the training dataset. To simulate the real protein discovery and annotation processes and validate the EC prediction performance of effectiveness and stability with time variance, we construct 2 testing sets per task; testing set 1 is from June 2020 snapshot, and testing set 2 is from February 2022, all testing sets filtered the sequences that appeared in the training set. The details are listed in Table <xref rid="supplementary-material-1" ref-type="sec">S7</xref>.<list list-type="simple"><list-item><label>•</label><p>Dataset 1: Enzyme and non-enzyme dataset</p></list-item></list></p>
      <p>The training set in total has 469,134 records, 222,567 of which are enzymes and 246,567 are non-enzymes (Table <xref rid="supplementary-material-1" ref-type="sec">S2</xref>). To make the data more inclusive, we did not filter any sequence in terms of length and homology, which is different from previous studies. An enzyme is labeled as 1 and a non-enzyme is labeled as 0.<list list-type="simple"><list-item><label>•</label><p>Dataset 2: Multifunctional enzyme dataset</p></list-item></list></p>
      <p>The multifunctional enzyme dataset only contains enzyme data. The number of EC categories ranges from 1 to 8 (Table <xref rid="supplementary-material-1" ref-type="sec">S3</xref>).<list list-type="simple"><list-item><label>•</label><p>Dataset 3: EC number dataset</p></list-item></list></p>
      <p>Similar to the multifunctional enzyme dataset, the EC number dataset contains only enzyme records, 222,567 of which constitute the training dataset (covering 5,111 EC numbers). The test data include newly added EC numbers compared with the training data (Fig. <xref rid="supplementary-material-1" ref-type="sec">S3</xref>), which means that these EC numbers do not appear in the training process, so predictive methods cannot handle this part of the EC numbers. Thus, we exclude the sequences with these EC numbers in the evaluation process.</p>
    </sec>
    <sec id="sec20">
      <title>Proposed framework</title>
      <p>To develop a novel EC prediction method with cutting-edge performance, we proposed HDMLF, which is composed of an embedding core and a learning core. These 2 cores operate relatively independently. The embedding core is responsible for embedding protein sequences into a machine-readable matrix. The learning core is responsible for solving specific downstream biological tasks (e.g., enzyme and non-enzyme prediction, multifunctional enzyme prediction, and EC number prediction). The overall scheme of HDMLF is illustrated in Fig. <xref rid="F6" ref-type="fig">6</xref>.<list list-type="simple"><list-item><label>•</label><p>Core 1: Embedding</p></list-item></list></p>
      <fig position="float" id="F6">
        <label>Fig. 6.</label>
        <caption>
          <p>HDMLF is an explicitly designed dual-core driven framework for EC number prediction. It consists of 2 independent operation units—an embedding core and a learning core. The embedding core is tasked with converting protein sequences into features. The learning core is designed to address the specific biological tasks defined in the problem formulation section.</p>
        </caption>
        <graphic xlink:href="research.0153.fig.006" position="float"/>
      </fig>
      <p>The objective of this core is to calculate the embedding representations for protein sequences. For protein sequence encoding/embedding, recent studies have shown the superior performance of deep learning-based methods compared to traditional methods [<xref rid="B31" ref-type="bibr">31</xref>,<xref rid="B32" ref-type="bibr">32</xref>]. Accordingly, we only compared one-hot encoding to show the difference between these 2 kinds of embedding in this study. Here, we adopted 3 different embedding methods to calculate the sequence embedding patterns that adequately represent protein sequences. The first one is the commonly used one-hot encoding [<xref rid="B33" ref-type="bibr">33</xref>]. The second is UniRep [<xref rid="B22" ref-type="bibr">22</xref>], an mLSTM “babbler” deep representation learner for proteins. We used the last layer for protein representation. The third is the ESM [<xref rid="B23" ref-type="bibr">23</xref>], a pretrained transformer language model for protein representation. We used representations from the 1st, 32nd, and 33rd layers as protein embeddings.<list list-type="simple"><list-item><label>•</label><p>Core 2: Learning</p></list-item></list></p>
      <p>The learning core is specialized to perform specific biological tasks using a multitask learning (MTL) framework, which is implemented by a bidirectional GRU (BGRU) network with an attention mechanism. As shown in Fig. <xref rid="F6" ref-type="fig">6</xref>, the learning core uses embedding results as a unified input and uses BGRU to learn enzyme and non-enzyme prediction task (task 1), multifunction enzyme prediction task (task 2), and EC number prediction task (task 3) together. We use 3 multihead attention layers to learn and highlight interactive information among different tasks:<disp-formula id="UEQ1"><mml:math id="M4" display="block" overflow="scroll"><mml:msub><mml:mi>O</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>softmax</mml:mtext><mml:mfenced open="(" close=")"><mml:mfrac><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mi>K</mml:mi><mml:mi>h</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mfenced><mml:msub><mml:mi>V</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mfenced open="{" close="}"><mml:mrow><mml:mtext>task</mml:mtext><mml:mspace width="3pt"/><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>task</mml:mtext><mml:mspace width="3pt"/><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mtext>task</mml:mtext><mml:mspace width="3pt"/><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:math></disp-formula></p>
      <p>where <inline-formula><mml:math id="M5" display="inline" overflow="scroll"><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:msqrt></mml:math></inline-formula> is a scaling factor and <italic toggle="yes">O<sub>h</sub></italic> is a one-head output of the attention layer, <italic toggle="yes">Q<sub>ht<sub>i</sub></sub></italic> is the weight learning weight from task 1 to task 3, and <inline-formula><mml:math id="M6" display="inline" overflow="scroll"><mml:msubsup><mml:mi>K</mml:mi><mml:mi>h</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:math></inline-formula> represents the multitask learning hidden state vector for each learning layers.</p>
      <p>The advantage of MTL is that multiple related learning tasks are solved simultaneously by exploiting commonalities and differences across relevant tasks [<xref rid="B34" ref-type="bibr">34</xref>], which considerably fits the current scenario. Compared with solving the 3 tasks separately or predicting EC number directly, MTL can improve the generalization performances of all the tasks because useful information contained in multiple related tasks is shared in the learning procedure. However, obtaining optimized weight parameters for all tasks will lead to a negative transfer problem that will hurt the learning performance [<xref rid="B35" ref-type="bibr">35</xref>]. To overcome this problem, here, we introduced a penalty parameter Ω to enforce a clustering of the task parameter vectors <italic toggle="yes">a</italic><sub><italic toggle="yes">t</italic>1</sub>, <italic toggle="yes">a</italic><sub><italic toggle="yes">t</italic>2</sub>, <italic toggle="yes">a</italic><sub><italic toggle="yes">t</italic>3</sub> toward their mean that is controlled by a hyperparameter <italic toggle="yes">λ</italic>. Ω is defined as follows:<disp-formula id="UEQ2"><mml:math id="M7" display="block" overflow="scroll"><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">||</mml:mo><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo stretchy="true">||</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:munderover><mml:mo>‍</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true">||</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="true">||</mml:mo></mml:math></disp-formula>where <inline-formula><mml:math id="M8" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mo>‍</mml:mo><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:math></inline-formula> is the mean parameter vector, <italic toggle="yes">T</italic> represents the number of tasks; in this work, we set <italic toggle="yes">λ</italic><sub>1</sub> = 0.5, <italic toggle="yes">λ</italic><sub>2</sub> = 0.1, and <italic toggle="yes">λ</italic><sub>3</sub> = 0.4. The details of implementation and parameter settings can be found in the Supplementary Materials.<list list-type="simple"><list-item><label>•</label><p>Integration, fine-tuning, and output</p></list-item></list></p>
      <p>As illustrated in Fig. <xref rid="F6" ref-type="fig">6</xref>, the final EC number prediction output is an integrated process. As shown in <xref rid="EQ4" ref-type="disp-formula">Eq. 4</xref>, we formulated this integrated process as an optimization problem:<disp-formula id="EQ4"><mml:math id="M9" display="block" overflow="scroll"><mml:munder><mml:mtext>MAX</mml:mtext><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:mfenced open="{" close="}"><mml:mrow><mml:mi>f</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="italic">ob</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="italic">ob</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="italic">ob</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="italic">sa</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math><label>(4)</label></disp-formula></p>
      <p>where <italic toggle="yes">obj</italic><sub>1</sub>, <italic toggle="yes">obj</italic><sub>2</sub>, and <italic toggle="yes">obj</italic><sub>3</sub> are the prediction results from task 1, task 2, and task 3, respectively, while <italic toggle="yes">sa</italic> is the predicted result from multiple sequence alignment. The integration and fine-tuning process aim to maximize the optimizing objective. In this work, the objective is the performance of prediction tasks in terms of the F1 score. We used a greedy strategy to perform this optimization.</p>
    </sec>
    <sec id="sec21">
      <title>Compared baselines</title>
      <p>To evaluate our proposed method comprehensively, we compared our proposed method with 4 existing state-of-the-art techniques with ‘GOOD’ usability (Supplementary Materials) and traditional sequence alignment method, which is provided by Diamond software. Four state-of-the-art techniques are CatFam, PRIAM (version 2), ECPred, and DeepEC.</p>
    </sec>
    <sec id="sec22">
      <title>Evaluation metrics</title>
      <p>To comprehensively evaluate the proposed method and existing baselines, we use 5 metrics to evaluate binary classification problems and 4 metrics to evaluate multiple classification problems. For the binary classification task, the evaluation criteria include ACC (accuracy), Precision, NPV (negative predictive value), Recall, and F1 value:<disp-formula id="EQ5"><mml:math id="M10" display="block" overflow="scroll"><mml:mtext>ACC</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>UP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>UN</mml:mtext></mml:mrow></mml:mfrac></mml:math><label>(5)</label></disp-formula><disp-formula id="EQ6"><mml:math id="M11" display="block" overflow="scroll"><mml:mtext>Precision</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow></mml:mfrac></mml:math><label>(6)</label></disp-formula><disp-formula id="EQ7"><mml:math id="M12" display="block" overflow="scroll"><mml:mtext>NPV</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mtext>TN</mml:mtext><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:math><label>(7)</label></disp-formula><disp-formula id="EQ8"><mml:math id="M13" display="block" overflow="scroll"><mml:mtext>Recall</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>UP</mml:mtext></mml:mrow></mml:mfrac></mml:math><label>(8)</label></disp-formula><disp-formula id="EQ9"><mml:math id="M14" display="block" overflow="scroll"><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mtext>Precision</mml:mtext><mml:mo>×</mml:mo><mml:mtext>Recall</mml:mtext></mml:mrow><mml:mrow><mml:mtext>Precision</mml:mtext><mml:mo>+</mml:mo><mml:mtext>Recall</mml:mtext></mml:mrow></mml:mfrac></mml:math><label>(9)</label></disp-formula></p>
      <p>where TP is the true-positive value, FP is the false-positive value, TN is the true-negative value, FN is the false-negative value, UP is unclassified-positive samples, and UN is unclassified-negative samples.</p>
      <p>For multiple classification problems, the evaluation criteria included mACC (macro-average accuracy), mPR (macro-average precision), mRecall (macro-average recall), and mF1 (macro-average F1 value):<disp-formula id="EQ10"><mml:math id="M15" display="block" overflow="scroll"><mml:mtext>mACC</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mo>‍</mml:mo><mml:msub><mml:mtext>ACC</mml:mtext><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:math><label>(10)</label></disp-formula><disp-formula id="EQ11"><mml:math id="M16" display="block" overflow="scroll"><mml:mtext>mPR</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mo>‍</mml:mo><mml:msub><mml:mtext>PPV</mml:mtext><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:math><label>(11)</label></disp-formula><disp-formula id="EQ12"><mml:math id="M17" display="block" overflow="scroll"><mml:mtext>mRecall</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mo>‍</mml:mo><mml:msub><mml:mtext>Recall</mml:mtext><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:math><label>(12)</label></disp-formula><disp-formula id="EQ13"><mml:math id="M18" display="block" overflow="scroll"><mml:mtext>mF</mml:mtext><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mtext>mPR</mml:mtext><mml:mo>×</mml:mo><mml:mtext>mRecall</mml:mtext></mml:mrow><mml:mrow><mml:mtext>mPR</mml:mtext><mml:mo>+</mml:mo><mml:mtext>mRecall</mml:mtext></mml:mrow></mml:mfrac></mml:math><label>(13)</label></disp-formula></p>
      <p>where <italic toggle="yes">N</italic> represents the total number of classes, while ACC<italic toggle="yes"><sub>i</sub></italic>, PPV<italic toggle="yes"><sub>i</sub></italic>, and Recall<italic toggle="yes"><sub>i</sub></italic> represent the accuracy, precision, and recall of the <italic toggle="yes">i</italic>th class in a one-versus-all mode [<xref rid="B36" ref-type="bibr">36</xref>], respectively.</p>
    </sec>
    <sec id="sec23">
      <title>Web platform implementation</title>
      <p>As shown in Fig. <xref rid="supplementary-material-1" ref-type="sec">S8</xref> the web platform uses Amazon ECR to store Docker images, which packages a set of bioinformatics software, such as Diamond and in-house Python scripts. We built a scalable, elastic, and easily maintainable batch engine using AWS Batch. This solution took care of dynamically scaling our computer resources in response to the number of runnable jobs in our job queue. Finally, we used AWS step functions to coordinate the components of our applications easily, process messages passed from AWS API Gateway, and invoke the workflows asynchronously. AWS API Gateway was used as the API server to handle the HTTP requests and route traffic to the correct backends. The static website was hosted by AWS S3 and sped up using AWS CloudFront.</p>
    </sec>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgments</title>
    <p><bold>Funding:</bold> This work was supported by the National Key Research and Development Program of China (2020YFA0908300), the National Natural Science Foundation of China (32201242), the Youth Innovation Promotion Association CAS, Innovation fund of Haihe Laboratory of Synthetic Biology (22HHSWSS00021), Tianjin Synthetic Biotechnology Innovation Capacity Improvement Project (TSBICIP-PTJS-001, TSBICIP-CXRC-018, and TSBICIP-PTJJ-007), and the China Postdoctoral Science Foundation (2022M713328). <bold>Author contributions:</bold> Z.S. and X.L. designed and implemented the model, analyzed the results, and wrote the manuscript. Z.S. and R.D. conducted the experiments. H.M., Z.M., and Q.Y. reviewed the manuscript. R.W. and H.L. designed the website. <bold>Competing interests:</bold> The authors declare that they have no competing interests regarding the publication of this article.</p>
  </ack>
  <sec sec-type="data-availability">
    <title>Data Availability</title>
    <p>The code of HDMLF, the training data, and the prediction results are available at <ext-link xlink:href="https://github.com/kingstdio/ECRECer" ext-link-type="uri">https://github.com/kingstdio/ECRECer</ext-link>
<ext-link xlink:href="https://github.com/tibbdc/ECRECer" ext-link-type="uri">https://github.com/tibbdc/ECRECer</ext-link>. </p>
  </sec>
  <sec sec-type="supplementary-material" id="supplementary-material-1">
    <title>Supplementary Materials</title>
    <supplementary-material id="supp-1" position="float" content-type="local-data">
      <label>Supplementary 1</label>
      <caption>
        <p>Tables S1 to S13</p>
        <p>Figs. S1 to S8</p>
      </caption>
      <media xlink:href="research.0153.f1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><collab>UniProt Consortium</collab>. 
<article-title>Uniprot: The universal protein knowledgebase in 2021</article-title>. <source>Nucleic Acids Res</source>. <year>2021</year>;<volume>49</volume>(<issue>D1</issue>):<fpage>D480</fpage>–<lpage>D489</lpage>.<pub-id pub-id-type="pmid">33237286</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ryu</surname><given-names>JY</given-names></string-name>, <string-name><surname>Kim</surname><given-names>HU</given-names></string-name>, <string-name><surname>Lee</surname><given-names>SY</given-names></string-name></person-group>. 
<article-title>Deep learning enables high-quality and high-throughput prediction of enzyme commission numbers</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2019</year>;<volume>116</volume>(<issue>28</issue>):<fpage>13996</fpage>–<lpage>14001</lpage>.<pub-id pub-id-type="pmid">31221760</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Furnham</surname><given-names>N</given-names></string-name>, <string-name><surname>Garavelli</surname><given-names>JS</given-names></string-name>, <string-name><surname>Apweiler</surname><given-names>R</given-names></string-name>, <string-name><surname>Thornton</surname><given-names>JM</given-names></string-name></person-group>. 
<article-title>Missing in action: Enzyme functional annotations in biological databases</article-title>. <source>Nat Chem Biol</source>. <year>2009</year>;<volume>5</volume>(<issue>8</issue>):<fpage>521</fpage>–<lpage>525</lpage>.<pub-id pub-id-type="pmid">19620987</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McDonald</surname><given-names>AG</given-names></string-name>, <string-name><surname>Tipton</surname><given-names>KF</given-names></string-name></person-group>. 
<article-title>Enzyme nomenclature and classification: The state of the art</article-title>. <source>FEBS J</source>. <year>2023</year>;<volume>290</volume>(<issue>9</issue>):<fpage>2214</fpage>–<lpage>2231</lpage>.<pub-id pub-id-type="pmid">34773359</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hung</surname><given-names>J-H</given-names></string-name>, <string-name><surname>Weng</surname><given-names>Z</given-names></string-name></person-group>. 
<article-title>Sequence alignment and homology search with BLAST and Clustalw</article-title>. <source>Cold Spring Harb Protoc</source>. <year>2016</year>;<volume>2016</volume>(<issue>11</issue>): <comment>10.1101/pdb.prot093088.</comment></mixed-citation>
    </ref>
    <ref id="B6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname><given-names>C</given-names></string-name>, <string-name><surname>Zavaljevski</surname><given-names>N</given-names></string-name>, <string-name><surname>Desai</surname><given-names>V</given-names></string-name>, <string-name><surname>Reifman</surname><given-names>J</given-names></string-name></person-group>. 
<article-title>Genome-wide enzyme annotation with precision control: Catalytic families (catfam) databases</article-title>. <source>Proteins</source>. <year>2009</year>;<volume>74</volume>(<issue>2</issue>):<fpage>449</fpage>–<lpage>460</lpage>.<pub-id pub-id-type="pmid">18636476</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Claudel-Renard</surname><given-names>C</given-names></string-name>, <string-name><surname>Chevalet</surname><given-names>C</given-names></string-name>, <string-name><surname>Faraut</surname><given-names>T</given-names></string-name>, <string-name><surname>Kahn</surname><given-names>D</given-names></string-name></person-group>. 
<article-title>Enzyme-specific profiles for genome annotation: PRIAM</article-title>. <source>Nucleic Acids Res</source>. <year>2003</year>;<volume>31</volume>(<issue>22</issue>):<fpage>6633</fpage>–<lpage>6639</lpage>.<pub-id pub-id-type="pmid">14602924</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nursimulu</surname><given-names>N</given-names></string-name>, <string-name><surname>Xu</surname><given-names>LL</given-names></string-name>, <string-name><surname>Wasmuth</surname><given-names>JD</given-names></string-name>, <string-name><surname>Krukov</surname><given-names>I</given-names></string-name>, <string-name><surname>Parkinson</surname><given-names>J</given-names></string-name></person-group>. 
<article-title>Improved enzyme annotation with ec-specific cutoffs using detect v2</article-title>. <source>Bioinformatics</source>. <year>2018</year>;<volume>34</volume>(<issue>19</issue>):<fpage>3393</fpage>–<lpage>3395</lpage>.<pub-id pub-id-type="pmid">29722785</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arakaki</surname><given-names>AK</given-names></string-name>, <string-name><surname>Huang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Skolnick</surname><given-names>J</given-names></string-name></person-group>. 
<article-title>Eficaz<sup>2</sup>: Enzyme function inference by a combined approach enhanced by machine learning</article-title>. <source>BMC Bioinformatics</source>. <year>2009</year>;<volume>10</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>15</lpage>.<pub-id pub-id-type="pmid">19118496</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dalkiran</surname><given-names>A</given-names></string-name>, <string-name><surname>Rifaioglu</surname><given-names>AS</given-names></string-name>, <string-name><surname>Martin</surname><given-names>MJ</given-names></string-name>, <string-name><surname>Cetin-Atalay</surname><given-names>R</given-names></string-name>, <string-name><surname>Atalay</surname><given-names>V</given-names></string-name>, <string-name><surname>Doğan</surname><given-names>T</given-names></string-name></person-group>. 
<article-title>ECPred: A tool for the prediction of the enzymatic functions of protein sequences based on the EC nomenclature</article-title>. <source>BMC bioinformatics</source>. <year>2018</year>;<volume>19</volume>(<issue>1</issue>):<fpage>334</fpage>.<pub-id pub-id-type="pmid">30241466</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>YH</given-names></string-name>, <string-name><surname>Xu</surname><given-names>JY</given-names></string-name>, <string-name><surname>Tao</surname><given-names>L</given-names></string-name>, <string-name><surname>Li</surname><given-names>XF</given-names></string-name>, <string-name><surname>Li</surname><given-names>S</given-names></string-name>, <string-name><surname>Zeng</surname><given-names>X</given-names></string-name>, <string-name><surname>Chen</surname><given-names>SY</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>P</given-names></string-name>, <string-name><surname>Qin</surname><given-names>C</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>C</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Svm-prot 2016: A web-server for machine learning prediction of protein functional families from sequence irrespective of similarity</article-title>. <source>PLOS ONE</source>. <year>2016</year>;<volume>11</volume>(<issue>8</issue>):
<elocation-id>e0155290</elocation-id>.<pub-id pub-id-type="pmid">27525735</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Akinosho</surname><given-names>TD</given-names></string-name>, <string-name><surname>Oyedele</surname><given-names>LO</given-names></string-name>, <string-name><surname>Bilal</surname><given-names>M</given-names></string-name>, <string-name><surname>Ajayi</surname><given-names>AO</given-names></string-name>, <string-name><surname>Delgado</surname><given-names>MD</given-names></string-name>, <string-name><surname>Akinade</surname><given-names>OO</given-names></string-name>, <string-name><surname>Ahmed</surname><given-names>AA</given-names></string-name></person-group>. 
<article-title>Deep learning in the construction industry: A review of present status and future innovations</article-title>. <source>J Build Eng</source>. <year>2020</year>;<volume>32</volume>:
<elocation-id>101827</elocation-id>.</mixed-citation>
    </ref>
    <ref id="B13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>H</given-names></string-name>, <string-name><surname>Tian</surname><given-names>S</given-names></string-name>, <string-name><surname>Li</surname><given-names>Y</given-names></string-name>, <string-name><surname>Fang</surname><given-names>Q</given-names></string-name>, <string-name><surname>Tan</surname><given-names>R</given-names></string-name>, <string-name><surname>Pan</surname><given-names>Y</given-names></string-name>, <string-name><surname>Huang</surname><given-names>C</given-names></string-name>, <string-name><surname>Xu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Gao</surname><given-names>X</given-names></string-name></person-group>. 
<article-title>Modern deep learning in bioinformatics</article-title>. <source>J Mol Cell Biol</source>. <year>2020</year>;<volume>12</volume>(<issue>11</issue>):<fpage>823</fpage>–<lpage>827</lpage>.<pub-id pub-id-type="pmid">32573721</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>Y</given-names></string-name>, <string-name><surname>Zuo</surname><given-names>Y</given-names></string-name>, <string-name><surname>Song</surname><given-names>H</given-names></string-name>, <string-name><surname>Lv</surname><given-names>Z</given-names></string-name></person-group>. 
<article-title>Deep learning in security of internet of things</article-title>. <source>IEEE Internet Things J</source>. <year>2021</year>;<volume>9</volume>(<issue>22</issue>):<fpage>22133</fpage>–<lpage>22146</lpage>.</mixed-citation>
    </ref>
    <ref id="B15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shi</surname><given-names>Z</given-names></string-name>, <string-name><surname>Wang</surname><given-names>S</given-names></string-name>, <string-name><surname>Yue</surname><given-names>L</given-names></string-name>, <string-name><surname>Pang</surname><given-names>L</given-names></string-name>, <string-name><surname>Zuo</surname><given-names>X</given-names></string-name>, <string-name><surname>Zuo</surname><given-names>W</given-names></string-name>, <string-name><surname>Li</surname><given-names>X</given-names></string-name></person-group>. 
<article-title>Deep dynamic imputation of clinical time series for mortality prediction</article-title>. <source>Inf Sci</source>. <year>2021</year>;<volume>579</volume>:<fpage>607</fpage>–<lpage>622</lpage>.</mixed-citation>
    </ref>
    <ref id="B16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Santos</surname><given-names>CFGD</given-names></string-name>, <string-name><surname>Papa</surname><given-names>JP</given-names></string-name></person-group>. 
<article-title>Avoiding overfitting: A survey on regularization methods for convolutional neural networks</article-title>. <source>ACM Comput Surv</source>. <year>2022</year>;<volume>54</volume>(<issue>10s</issue>):<fpage>1</fpage>–<lpage>25</lpage>.</mixed-citation>
    </ref>
    <ref id="B17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>C</given-names></string-name>, <string-name><surname>Freddolino</surname><given-names>PL</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Y</given-names></string-name></person-group>. 
<article-title>Cofactor: Improved protein function prediction by combining structure, sequence and protein–protein interaction information</article-title>. <source>Nucleic Acids Res</source>. <year>2017</year>;<volume>45</volume>(<issue>W1</issue>):<fpage>W291</fpage>–<lpage>W299</lpage>.<pub-id pub-id-type="pmid">28472402</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname><given-names>H-B</given-names></string-name>, <string-name><surname>Chou</surname><given-names>K-C</given-names></string-name></person-group>. 
<article-title>Ezypred: A top–down approach for predicting enzyme functional classes and subclasses</article-title>. <source>Biochem Biophys Res Commun</source>. <year>2007</year>;<volume>364</volume>(<issue>1</issue>):<fpage>53</fpage>–<lpage>59</lpage>.<pub-id pub-id-type="pmid">17931599</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname><given-names>S</given-names></string-name>, <string-name><surname>Umarov</surname><given-names>R</given-names></string-name>, <string-name><surname>Xie</surname><given-names>B</given-names></string-name>, <string-name><surname>Fan</surname><given-names>M</given-names></string-name>, <string-name><surname>Li</surname><given-names>L</given-names></string-name>, <string-name><surname>Gao</surname><given-names>X</given-names></string-name></person-group>. 
<article-title>Deepre: Sequence-based enzyme ec number prediction by deep learning</article-title>. <source>Bioinformatics</source>. <year>2018</year>;<volume>34</volume>(<issue>5</issue>):<fpage>760</fpage>–<lpage>769</lpage>.<pub-id pub-id-type="pmid">29069344</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>An</surname><given-names>JY</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>Y</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>YJ</given-names></string-name>, <string-name><surname>Yan</surname><given-names>ZJ</given-names></string-name></person-group>. 
<article-title>An efficient feature extraction technique based on local coding PSSM and multifeatures fusion for predicting protein-protein interactions</article-title>. <source>Evol Bioinforma</source>. <year>2019</year>;<volume>15</volume>:<fpage>1176934319879920</fpage>.</mixed-citation>
    </ref>
    <ref id="B21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>KK</given-names></string-name>, <string-name><surname>Wu</surname><given-names>Z</given-names></string-name>, <string-name><surname>Bedbrook</surname><given-names>CN</given-names></string-name>, <string-name><surname>Arnold</surname><given-names>FH</given-names></string-name></person-group>. 
<article-title>Learned protein embeddings for machine learning</article-title>. <source>Bioinformatics</source>. <year>2018</year>;<volume>34</volume>(<issue>15</issue>):<fpage>2642</fpage>–<lpage>2648</lpage>.<pub-id pub-id-type="pmid">29584811</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alley</surname><given-names>EC</given-names></string-name>, <string-name><surname>Khimulya</surname><given-names>G</given-names></string-name>, <string-name><surname>Biswas</surname><given-names>S</given-names></string-name>, <string-name><surname>AlQuraishi</surname><given-names>M</given-names></string-name>, <string-name><surname>Church</surname><given-names>GM</given-names></string-name></person-group>. 
<article-title>Unified rational protein engineering with sequence-based deep representation learning</article-title>. <source>Nat Methods</source>. <year>2019</year>;<volume>16</volume>(<issue>12</issue>):<fpage>1315</fpage>–<lpage>1322</lpage>.<pub-id pub-id-type="pmid">31636460</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <label>23.</label>
      <mixed-citation publication-type="other">Rao R, Meier J, Sercu T, Ovchinnikov S, Rives A. Transformer protein language models are unsupervised structure learners. Paper presented at: ICLR 2021. Proceedings of the International Conference on Learning Representations; 2020 May 3–7; Vienna, Austria.</mixed-citation>
    </ref>
    <ref id="B24">
      <label>24.</label>
      <mixed-citation publication-type="other">Zhang Z, Xu M, Jamasb AR, Chenthamarakshan V, Lozano A, Das P, Tang J. Protein representation learning by geometric structure pretraining. Paper presented at: ICLR 2023. Proceedings of the International Conference on Learning Representations; 2023 May 1–5; Kigali, Rwanda.</mixed-citation>
    </ref>
    <ref id="B25">
      <label>25.</label>
      <mixed-citation publication-type="other">Zhang Z, Xu M, Chenthamarakshan V, Lozano A, Das P, Tang J. Enhancing protein language model with structure-based encoder and pre-training. Paper presented at: MLDD 2023. Proceedings of the International Conference on Learning Representations Machine Learning for Drug Discovery Workshop; 2023 May 5; virtual.</mixed-citation>
    </ref>
    <ref id="B26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grundmann</surname><given-names>A</given-names></string-name>, <string-name><surname>Li</surname><given-names>S-M</given-names></string-name></person-group>. 
<article-title>Overproduction, purification and characterization of ftmpt1, a brevianamide f prenyltransferase from aspergillus fumigatus</article-title>. <source>Microbiology</source>. <year>2005</year>;<volume>151</volume>(<issue>7</issue>):<fpage>2199</fpage>–<lpage>2207</lpage>.<pub-id pub-id-type="pmid">16000710</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khersonsky</surname><given-names>O</given-names></string-name>, <string-name><surname>Tawfik</surname><given-names>DS</given-names></string-name></person-group>. 
<article-title>Enzyme promiscuity: A mechanistic and evolutionary perspective</article-title>. <source>Annu Rev Biochem</source>. <year>2010</year>;<volume>79</volume>:<fpage>471</fpage>–<lpage>505</lpage>.<pub-id pub-id-type="pmid">20235827</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guzmán</surname><given-names>GI</given-names></string-name>, <string-name><surname>Utrilla</surname><given-names>J</given-names></string-name>, <string-name><surname>Nurk</surname><given-names>S</given-names></string-name>, <string-name><surname>Brunk</surname><given-names>E</given-names></string-name>, <string-name><surname>Monk</surname><given-names>JM</given-names></string-name>, <string-name><surname>Ebrahim</surname><given-names>A</given-names></string-name>, <string-name><surname>Palsson</surname><given-names>BO</given-names></string-name>, <string-name><surname>Feist</surname><given-names>AM</given-names></string-name></person-group>. 
<article-title>Model-driven discovery of underground metabolic functions in escherichia coli</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2015</year>;<volume>112</volume>(<issue>3</issue>):<fpage>929</fpage>–<lpage>934</lpage>.<pub-id pub-id-type="pmid">25564669</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keseler</surname><given-names>IM</given-names></string-name>, <string-name><surname>Gama-Castro</surname><given-names>S</given-names></string-name>, <string-name><surname>Mackie</surname><given-names>A</given-names></string-name>, <string-name><surname>Billington</surname><given-names>R</given-names></string-name>, <string-name><surname>Bonavides-Martínez</surname><given-names>C</given-names></string-name>, <string-name><surname>Caspi</surname><given-names>R</given-names></string-name>, <string-name><surname>Kothari</surname><given-names>A</given-names></string-name>, <string-name><surname>Krummenacker</surname><given-names>M</given-names></string-name>, <string-name><surname>Midford</surname><given-names>PE</given-names></string-name>, <string-name><surname>Muñiz-Rascado</surname><given-names>L</given-names></string-name>, <etal>et al.</etal></person-group><article-title>The ecocyc database in 2021</article-title>. <source>Front Microbiol</source>. <year>2021</year>;<volume>12</volume>:
<elocation-id>711077</elocation-id>.<pub-id pub-id-type="pmid">34394059</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname><given-names>J-Y</given-names></string-name>, <string-name><surname>Na</surname><given-names>Y-A</given-names></string-name>, <string-name><surname>Kim</surname><given-names>E</given-names></string-name>, <string-name><surname>Lee</surname><given-names>H-S</given-names></string-name>, <string-name><surname>Kim</surname><given-names>P</given-names></string-name></person-group>. 
<article-title>The actinobacterium corynebacterium glutamicum, an industrial workhorse</article-title>. <source>J Microbiol Biotechnol</source>. <year>2016</year>;<volume>26</volume>(<issue>5</issue>):<fpage>807</fpage>–<lpage>822</lpage>.<pub-id pub-id-type="pmid">26838341</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anteghini</surname><given-names>M</given-names></string-name>, <string-name><surname>Martins dos Santos</surname><given-names>V</given-names></string-name>, <string-name><surname>Saccenti</surname><given-names>E</given-names></string-name></person-group>. 
<article-title>In-pero: Exploiting deep learning embeddings of protein sequences to predict the localisation of peroxisomal proteins</article-title>. <source>Int J Mol Sci</source>. <year>2021</year>;<volume>22</volume>(<issue>12</issue>):<fpage>6409</fpage>.<pub-id pub-id-type="pmid">34203866</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <label>32.</label>
      <mixed-citation publication-type="other">Martiny H-M, Armenteros JJA, Johansen AR, Salomon J, Nielsen H. Deep protein representations enable recombinant protein expression prediction. bioRxiv. 2021. <pub-id pub-id-type="doi">10.1101/2021.05.13.443426</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>ElAbd</surname><given-names>H</given-names></string-name>, <string-name><surname>Bromberg</surname><given-names>Y</given-names></string-name>, <string-name><surname>Hoarfrost</surname><given-names>A</given-names></string-name>, <string-name><surname>Lenz</surname><given-names>T</given-names></string-name>, <string-name><surname>Franke</surname><given-names>A</given-names></string-name>, <string-name><surname>Wendorff</surname><given-names>M</given-names></string-name></person-group>. 
<article-title>Amino acid encoding for deep learning applications</article-title>. <source>BMC Bioinformatics</source>. <year>2020</year>;<volume>21</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">31898485</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname><given-names>X</given-names></string-name>, <string-name><surname>Lu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>P</given-names></string-name></person-group>. 
<article-title>Multi-task learning on nuclear masses and separation energies with the kernel ridge regression</article-title>. <source>Phys Lett B</source>. <year>2022</year>;<volume>834</volume>:
<elocation-id>137394</elocation-id>.</mixed-citation>
    </ref>
    <ref id="B35">
      <label>35.</label>
      <mixed-citation publication-type="other">Ruder S. An overview of multi-task learning in deep neural networks. arXiv. 2017. https://doi.org/10.48550/arXiv.1706.05098</mixed-citation>
    </ref>
    <ref id="B36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rifkin</surname><given-names>R</given-names></string-name>, <string-name><surname>Klautau</surname><given-names>A</given-names></string-name></person-group>. 
<article-title>In defense of one-vs-all classification</article-title>. <source>J Mach Learn Res</source>. <year>2004</year>;<volume>5</volume>:<fpage>101</fpage>–<lpage>141</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-archivearticle1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Research (Wash D C)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Research (Wash D C)</journal-id>
    <journal-id journal-id-type="publisher-id">RESEARCH</journal-id>
    <journal-title-group>
      <journal-title>Research</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2639-5274</issn>
    <publisher>
      <publisher-name>AAAS</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10232324</article-id>
    <article-id pub-id-type="doi">10.34133/research.0153</article-id>
    <article-id pub-id-type="publisher-id">0153</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Enzyme Commission Number Prediction and Benchmarking with Hierarchical Dual-core Multitask Learning Framework</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Shi</surname>
          <given-names>Zhenkun</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Deng</surname>
          <given-names>Rui</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yuan</surname>
          <given-names>Qianqian</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mao</surname>
          <given-names>Zhitao</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Ruoyu</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Haoran</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Liao</surname>
          <given-names>Xiaoping</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
        <xref rid="corr1" ref-type="corresp">
          <sup>*</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Ma</surname>
          <given-names>Hongwu</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="corr1" ref-type="corresp">
          <sup>*</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff1"><label><sup>1</sup></label><addr-line>Biodesign Center, Key Laboratory of Engineering Biology for Low-carbon Manufacturing, Tianjin Institute of Industrial Biotechnology</addr-line>, 
<institution>Chinese Academy of Sciences</institution>, 300308, Tianjin, <country>China</country>.</aff>
    <aff id="aff2"><label><sup>2</sup></label><institution>National Center of Technology Innovation for Synthetic Biology</institution>, 300308, Tianjin, <country>China</country>.</aff>
    <aff id="aff3"><label><sup>3</sup></label><addr-line>College of Biotechnology</addr-line>, 
<institution>Tianjin University of Science &amp; Technology</institution>, Tianjin, <country>China</country>.</aff>
    <aff id="aff4"><label><sup>4</sup></label><institution>Haihe Laboratory of Synthetic Biology</institution>, 300308, Tianjin, <country>China</country>.</aff>
    <author-notes>
      <corresp id="corr1"><label>*</label>Address correspondence to: <email xlink:href="mailto:liao_xp@tib.cas.cn">liao_xp@tib.cas.cn</email> (X.L.); <email xlink:href="mailto:ma_hw@tib.cas.cn">ma_hw@tib.cas.cn</email> (H.M.)</corresp>
    </author-notes>
    <pub-date publication-format="electronic" date-type="pub">
      <day>31</day>
      <month>5</month>
      <year>2023</year>
    </pub-date>
    <pub-date publication-format="electronic" date-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>6</volume>
    <elocation-id>0153</elocation-id>
    <history>
      <date date-type="received">
        <day>07</day>
        <month>3</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>28</day>
        <month>4</month>
        <year>2023</year>
      </date>
      <date date-type="pub">
        <day>31</day>
        <month>5</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2023 Zhenkun Shi et al.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Zhenkun Shi et al.</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>Exclusive licensee Science and Technology Review Publishing House. No claim to original U.S. Government Works. Distributed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License (CC BY 4.0)</ext-link>.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="research.0153.pdf"/>
    <abstract>
      <p>Enzyme commission (EC) numbers, which associate a protein sequence with the biochemical reactions it catalyzes, are essential for the accurate understanding of enzyme functions and cellular metabolism. Many ab initio computational approaches were proposed to predict EC numbers for given input protein sequences. However, the prediction performance (accuracy, recall, and precision), usability, and efficiency of existing methods decreased seriously when dealing with recently discovered proteins, thus still having much room to be improved. Here, we report HDMLF, a hierarchical dual-core multitask learning framework for accurately predicting EC numbers based on novel deep learning techniques. HDMLF is composed of an embedding core and a learning core; the embedding core adopts the latest protein language model for protein sequence embedding, and the learning core conducts the EC number prediction. Specifically, HDMLF is designed on the basis of a gated recurrent unit framework to perform EC number prediction in the multi-objective hierarchy, multitasking manner. Additionally, we introduced an attention layer to optimize the EC prediction and employed a greedy strategy to integrate and fine-tune the final model. Comparative analyses against 4 representative methods demonstrate that HDMLF stably delivers the highest performance, which improves accuracy and F1 score by 60% and 40% over the state of the art, respectively. An additional case study of tyrB predicted to compensate for the loss of aspartate aminotransferase aspC, as reported in a previous experimental study, shows that our model can also be used to uncover the enzyme promiscuity. Finally, we established a web platform, namely, ECRECer (<ext-link xlink:href="https://ecrecer.biodesign.ac.cn" ext-link-type="uri">https://ecrecer.biodesign.ac.cn</ext-link>), using an entirely could-based serverless architecture and provided an offline bundle to improve usability.</p>
    </abstract>
    <counts>
      <fig-count count="6"/>
      <table-count count="1"/>
      <ref-count count="36"/>
      <page-count count="0"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <title>Introduction</title>
    <p>With the widespread adoption of high-throughput methods and high-quality infrastructure, the speed of new protein discovery has increased dramatically. However, this was not followed by a concomitant increase in the speed of protein annotation. For example, 801,118 sequences were added to TrEMBL in the UniProt database [<xref rid="B1" ref-type="bibr">1</xref>] in the single month of December 2022, while only 388 sequences were reviewed and added to Swiss-Prot in the same period (Fig. <xref rid="supplementary-material-1" ref-type="sec">S2</xref>). Such a slow speed of protein annotation considerably restricts related research and industrial applications.</p>
    <p>Among the multiple and complex protein annotation tasks, one of the crucial steps is enzyme function annotation [<xref rid="B2" ref-type="bibr">2</xref>,<xref rid="B3" ref-type="bibr">3</xref>]. Annotations of enzyme function provide critical starting points for generating and testing biological hypotheses [<xref rid="B3" ref-type="bibr">3</xref>]. Current functional annotations of enzymes describe the biochemistry or process by assigning an enzyme commission (EC) number. An EC number is a 4-part code associated with a recommended name for the corresponding enzyme-catalyzed reaction that describes the enzyme class, the chemical bond acted on, the reaction, and the substrates [<xref rid="B4" ref-type="bibr">4</xref>]. Thus, the primary task of enzyme annotation is to assign an EC number to a given protein sequence. However, as the uncertainty of the assignments for uncharacterized protein sequences is high and biochemical data are relatively sparse, both the speed and the quality of enzyme annotation are considerably restricted.</p>
    <p>To achieve improved, rapid, and intelligent functional annotation, computational methods were introduced to assign or predict EC numbers. The simplest and most commonly used method is based on sequence homology [<xref rid="B5" ref-type="bibr">5</xref>]; researchers have developed a variety of EC databases and profile-based methods for the functional annotation of enzymes [<xref rid="B6" ref-type="bibr">6</xref>–<xref rid="B8" ref-type="bibr">8</xref>]. However, these methods cannot perform annotations for novel proteins without similar sequences, which is generally the case for newly discovered enzymes. To overcome this restriction, researchers introduced machine learning (ML) methods, such as hidden Markov model [<xref rid="B9" ref-type="bibr">9</xref>], K-nearest neighbor (KNN) [<xref rid="B10" ref-type="bibr">10</xref>], and SVM [<xref rid="B11" ref-type="bibr">11</xref>] for annotating enzymes. Although these methods can predict EC numbers for proteins without similar references, the prediction speed and precision are not ideal. Because deep learning has delivered powerful results in many areas [<xref rid="B12" ref-type="bibr">12</xref>–<xref rid="B15" ref-type="bibr">15</xref>], researchers use deep learning methods to predict EC numbers and continually improve the precision of functional annotation [<xref rid="B2" ref-type="bibr">2</xref>]. However, deep learning methods are prone to overfitting because of an unbalanced distribution of training datasets [<xref rid="B16" ref-type="bibr">16</xref>]. Specific to the EC number prediction task, this would lead to predictions with high precision, medium recall, and low accuracy.</p>
    <p>Overall, there has been a steady improvement in computational methods for enzyme annotation [<xref rid="B2" ref-type="bibr">2</xref>,<xref rid="B7" ref-type="bibr">7</xref>,<xref rid="B17" ref-type="bibr">17</xref>,<xref rid="B18" ref-type="bibr">18</xref>], but several obstacles still exist that have slowed the progress of computational enzyme function annotation. The first is a lack of publicly available benchmark datasets to evaluate the existing and newly proposed models, making it troublesome for the end user to choose the best method in their production scenario. Another hindrance is the lack of an explicitly designed method with stable prediction performance to deal with newly discovered proteins. Especially, the lacking of an efficient and universal protein sequence embedding method made researchers have to spend large amounts of time on handcrafted feature engineering to encode the sequence, such as functional domain encoding [<xref rid="B19" ref-type="bibr">19</xref>] and position-specific scoring matrix (PSSM) encoding [<xref rid="B20" ref-type="bibr">20</xref>], as encoding quality dramatically affects the performance of downstream applications [<xref rid="B21" ref-type="bibr">21</xref>]. The third one is the usability of existing tools, as some tools are only available offline and are not easy to install, while some online tools are no longer available.</p>
    <p>To overcome these obstacles, we proposed a hierarchical dual-core multitask learning framework (HDMLF) in this work. The main contributions are as follows:<list list-type="simple"><list-item><label>•</label><p>We constructed 3 standard datasets for benchmarking and evaluation. The datasets contain more than 470,000 distinct labeled protein sequences from Swiss-Prot.</p></list-item><list-item><label>•</label><p>We proposed a novel framework on the basis of the latest protein language embedding methods and gated recurrent unit (GRU) with the attention mechanism. In HDMLF, we formulate the EC number prediction as a multitask multilabel classification problem. The first task predicts whether a given protein sequence is an enzyme. The second task predicts how many functions the enzyme can perform, i.e., multifunctional enzyme prediction. The last task predicts the exact EC number for each enzyme function. To achieve cutting-edge performance for EC number prediction, we first introduced and evaluated the state-of-the-art deep learning language embedding methods for universal protein sequence embedding [<xref rid="B22" ref-type="bibr">22</xref>,<xref rid="B23" ref-type="bibr">23</xref>]. Then, a novel prediction method based on GRU with an attention mechanism was proposed to solve the 3 tasks in a multitasking manner. Finally, a feedback mechanism is adopted to choose the most suitable embedding, and a greedy strategy is introduced to integrate these tasks to maximize the EC prediction performance.</p></list-item><list-item><label>•</label><p>A webserver was published for easy usability. We published a web platform based on a serverless architecture so that anyone can annotate EC numbers smoothly in high throughput, whether they have coding experience or not. Our webserver is publicly accessed via <ext-link xlink:href="http://ecrecer.biodesign.ac.cn" ext-link-type="uri">http://ecrecer.biodesign.ac.cn</ext-link>.</p></list-item></list></p>
  </sec>
  <sec id="sec2">
    <title>Results</title>
    <sec id="sec3">
      <title>A dataset for benchmarking</title>
      <p>Because of the lack of a public benchmark for EC number prediction, we constructed a standard dataset from Swiss-Prot for model development and evaluation. To simulate real application scenarios as closely as possible, we organized data chronologically. Specifically, we used a snapshot from February 2018 as the training dataset, consisting of 469,134 records (4,854 distinct EC numbers). We construct 2 testing sets to simulate the real protein discovery and annotation processes and validate the EC prediction performance of effectiveness and stability with time variance. Testing set 1 (testset_20) is from the June 2020 snapshot, consisting of 7,101 records (937 distinct EC numbers). Testing set 2 (testset_22) is from February 2022, consisting of 10,614 records (1,355 distinct EC numbers). All testing sets filtered the sequences that appeared in the training set. The details are listed in Tables <xref rid="supplementary-material-1" ref-type="sec">S2</xref> to <xref rid="supplementary-material-1" ref-type="sec">S5</xref>.</p>
    </sec>
    <sec id="sec4">
      <title>Suitable embedding methods do help in improving the prediction performance</title>
      <p>ML models trained on protein sequences and their measured functions can infer unseen sequences’ biological properties without understanding the underlying physical or biological mechanisms. However, ML models require vectors as input other than amino acid sequences, and converting from a protein sequence to a vector representation extremely affects the model’s ability to learn [<xref rid="B21" ref-type="bibr">21</xref>].</p>
      <p>To evaluate and choose the best embedding methods for our downstream prediction tasks, we evaluated 3 different protein embedding methods, one-hot embedding, UniRep embedding, and evolutionary scale modeling embedding method (ESM) embedding with different layers (from 1 to 33) in 2 different testing datasets. The evaluation process can be categorized into 2: (a) We use an enzyme or non-enzyme prediction task and 6 ML baselines to evaluate the embedding performance in binary classifications, and (b) we use the EC prediction task and our proposed method to evaluate the embedding performance in multiclass classifications. The 6 ML baselines are KNN, logistic regression (LR), XGBoost, decision tree (DT), random forest (RF), and gradient boosting decision tree (GBDT).<list list-type="simple"><list-item><label>•</label><p>Protein embedding methods can learn semantic information from sequences directly, remarkably improving downstream tasks’ performance.</p></list-item></list></p>
      <p>As shown in Fig. <xref rid="F1" ref-type="fig">1</xref>, compared with the traditional one-hot protein representation, methods UniRep and ESM improve the downstream task by more than 20% in all testing tasks, in terms of F1 score (details can be found in Tables <xref rid="supplementary-material-1" ref-type="sec">S6</xref> and <xref rid="supplementary-material-1" ref-type="sec">S7</xref>). For embedding, ESM-32 exhibited the best overall performance among all 6 baselines regarding all evaluation metrics for embedding. As shown in Fig. <xref rid="F1" ref-type="fig">1</xref>C and D, in the EC prediction task, ESM-32 achieved 21.67% and 6.03% improvements over one-hot and UniRep in terms of accuracy, as well as 27.20% and 7.32% in terms of mF1, respectively. This experiment suggests that better embedding can lead to better learning performance, and deep latent representation can comprehensively represent the protein sequence. Apart from protein sequences, protein structure can also be helpful in conducting protein embedding, and existing studies have demonstrated this [<xref rid="B24" ref-type="bibr">24</xref>,<xref rid="B25" ref-type="bibr">25</xref>]. In our future work, we plan to incorporate these techniques to further enhance the prediction performance.<list list-type="simple"><list-item><label>•</label><p>For embedding layers, not the deeper the better.</p></list-item></list></p>
      <fig position="float" id="F1">
        <label>Fig. 1.</label>
        <caption>
          <p>Performance comparison of different embedding methods. (A and B) Embedding performance (F1 score) comparison using ML baselines on task 1. (A) Results on testset_20 and (B) on testset_22. (C and D) Embedding performance comparison using our proposed framework HDMLF on task 3; mACC, mPR, mRecall, and mF1 are different evaluation metrics defined in the “Evaluation metrics” section. (C) Results on testset_20 and (D) on testset_22.</p>
        </caption>
        <graphic xlink:href="research.0153.fig.001" position="float"/>
      </fig>
      <p>To validate the effectiveness within different depths of embedding layers, we evaluated embedding layers 1 to 33 from ESM under 4 classification baseline methods. Interestingly, we found that, when layers increase from 1 to 32, the performance increases, while when layers reach 32, the performance began to decrease (Fig. <xref rid="F1" ref-type="fig">1</xref>); this is mainly due to the overfitting issue and suggests that, for selection of embedding layers, not the deeper, the better.</p>
    </sec>
    <sec id="sec5">
      <title>HDMLF versus existing EC prediction methods</title>
      <p>To achieve the best performance, HDMLF is proposed with 3 objectives: (a) classify the enzyme and non-enzyme with high accuracy, (b) provide the ability to predict multifunctional enzymes, and (c) achieve state-of-the-art EC prediction performance, and we treat these objectives as 3 learning tasks. To meet these objectives, our proposed framework is designed with mutitask learning techniques and is organized in a hierarchical order.</p>
      <p>To evaluate the performance of HDMLF and validate whether these objectives do help our final goal, we made a comprehensive comparison experiment with existing EC prediction methods among all these tasks.</p>
      <sec id="sec6">
        <title>EC number prediction performance comparison</title>
        <p>EC prediction is the main task and final goal in HDMLF. As shown in Fig. <xref rid="F2" ref-type="fig">2</xref>C, our proposed methods achieved the best overall performance in testset_20. PRIAM [<xref rid="B7" ref-type="bibr">7</xref>] is mainly designed to include more sequences, so the mRecall is high (78.48%, 75.26%), while the mPR (20.80%, 25.03%) is very low. DeepEC, ECPred, and CatFam pursue high precision; these methods are very likely to miss many new functions, which, in turn, wound underperform in terms of accuracy and F1 score. The F1 score should be a better evaluation metric for the EC assignment of real-world proteins. In terms of mF1 score, our model reached over 150% performance than the second best method, which achieved 86.91% accuracy with 69% mPR and 63.88% mRecall. In other words, if 100 protein sequences were uploaded for annotation, then we can obtain approximately 87 correct annotations. Interestingly, existing methods show much poorer performance on testset_22 while our method can maintain the prediction performance. We conducted further validation of the HDMLF’s performance in predicting EC numbers at first to third levels compared to other baselines. The results are shown in Tables <xref rid="supplementary-material-1" ref-type="sec">S11</xref> to <xref rid="supplementary-material-1" ref-type="sec">S13</xref>. Our findings demonstrate that the HDMLF outperforms the second-best method by more than 20%, 28%, and 30% in terms of mF1 at the first, second, and third levels of EC, respectively. However, when we simplify the evaluation criteria and only consider the first-digit EC, the mF1 score remains below 70%. Note that these methods were specifically designed to predict the fully 4-digit EC number, which is not optimal for first-digit EC prediction.</p>
        <fig position="float" id="F2">
          <label>Fig. 2.</label>
          <caption>
            <p>Performance comparison of different methods for different prediction tasks. Bars with <inline-graphic xlink:href="research.0153.inline.001.jpg"/> represent prediction results on testset_20, bars with <inline-graphic xlink:href="research.0153.inline.002.jpg"/> represent prediction results on testset_22 (A) Performance comparison on EC number prediction. (B) Performance comparison on enzyme and non-enzyme prediction. (C) Performance comparison on task 2 multifunctional enzyme prediction.</p>
          </caption>
          <graphic xlink:href="research.0153.fig.002" position="float"/>
        </fig>
        <p>Overall, in testset_22, for these relatively new proteins, our method much improved the mF1 score over existing methods; this is mainly due to the EC numbers and enzyme samples being more inclusive in testset_22. All the above results show that our method shows a clear advantage in terms of EC number assignment.</p>
      </sec>
      <sec id="sec7">
        <title>Enzyme or non-enzyme prediction performance comparison</title>
        <p>In enzyme or non-enzyme prediction task, as shown in Fig. <xref rid="F2" ref-type="fig">2</xref>B, our method can achieve scores of 92.01% and 93.61% in terms of F1 score in the above-constructed testing sets, respectively. Compared with other state-of-the-art tools, the overall accuracy was greatly improved. Many previous methods were designed to obtain high precision while neglecting accuracy, negative predictive value, and recall. For example, DeepEC can reach 94.68% precision while recall is only 20.83% in testset_20, which means that many enzymes would be missed by DeepEC prediction.</p>
      </sec>
      <sec id="sec8">
        <title>Multifunctional enzyme prediction</title>
        <p>The multifunctional enzyme prediction task is designed to predict the number of ECs assigned to a given protein. As shown in Fig. <xref rid="F2" ref-type="fig">2</xref>C, we can see that the performance of our proposed method is stable and superior to existing baselines. In testset_20, our method achieved 91.71% accuracy with 58.37% mPR and 55.20% mRecall recall. In testset_22, our method achieved 92.45% accuracy with 70.68% mPR and 59.56% mRecall recall. The low mRecall and mPR are mainly due to the data sparseness of 3 to 8 functional enzymes , which results in the classifier being more preferred to predict an enzyme as a single-function enzyme. Although our proposed method achieved the best performance among existing methods, this is still inadequate in a productive scenario, especially for enzymes assigned with more than 3 EC numbers, so it should be further improved in future work.</p>
      </sec>
    </sec>
    <sec id="sec9">
      <title>Assessing the stability of EC prediction performance over time</title>
      <p>To assess the efficacy and predictability of our proposed framework over time, we simulated an EC prediction experiment using Swiss-Prot data in a realistic setting. We first collected the snapshot of Swiss-Prot from 2019 to 2022. Then, we excluded the data that appeared in the training set and used the trained model to predict the new data added from 2019 to 2022. Comparative results with existing EC prediction methods are shown in Fig. <xref rid="F3" ref-type="fig">3</xref>. From Fig. <xref rid="F3" ref-type="fig">3</xref>, we can observe that all methods’ prediction performance will decrease over time. In terms of baselines, DeepEC and CatFam are relatively more stable than the others. For DeepEC, the performance in terms of F1 score dropped by 3.20%, 11.34%, and 21.20% from 2020 to 2022, compared to 2019, and decreased by 9.86%, 19.94%, and 29.48% in terms of accuracy. The reason for this is that DeepEC uses one-hot embedding and 3 different models to predict the different levels of EC, and it integrates the output using the sequences alignment method. While the first level of EC prediction is much more precise, one-hot embedding cannot preserve sufficient information to represent protein sequences. Moreover, it is weak in the fourth level of EC prediction, and the sequence alignment method cannot handle new sequences with low similarities. On the other hand, CatFam’s performance dropped by −0.4%, 3.28%, and 9.95% from 2020 to 2022, compared to 2019, in terms of F1 score and decreased by 9.02%, 8.99%, and 28.25% in terms of accuracy. This is because CatFam is based on homologous similarity and PSSM. It is more stable for protein sequences with existing homologous sequences than DeepEC. However, the performance will decrease significantly for newly added proteins without homologous sequences. Remarkably, the prediction performance of our proposed method is more stable, decreasing by less than 1% from 2019 to 2022, both in terms of F1 score and accuracy. This mainly benefits from the language model, which is powerful in sequence embedding and can reserve sufficient information for downstream applications, and we treat 4-level EC as a tuple that reduces the integration error from different predictive models. Thus, our proposed method is considerably more statable than the existing ones.</p>
      <fig position="float" id="F3">
        <label>Fig. 3.</label>
        <caption>
          <p>Task 1 comparison of EC prediction performance over time.</p>
        </caption>
        <graphic xlink:href="research.0153.fig.003" position="float"/>
      </fig>
    </sec>
    <sec id="sec10">
      <title>Case study: HDMLF can annotate protein with incomplete EC number</title>
      <p>In the databases, many enzymes with EC numbers exist in an uncompleted 3-level, 2-level, or even 1-level state. These proteins with incomplete EC numbers might not directly be utilized for retrieving enzymatic reactions. For instance, an enzyme iron/alpha-ketoglutarate-dependent dioxygenase AusU (UniProt ID: A0A0U5GJ41) has a 2-level EC number in the database (1.14.-.-), while our method HDMLF can assign this protein with the fourth-level EC number 1.14.11.38. After blasting it against the UniProt database, we find that the top 5 reviewed proteins with the highest identities include 3 verruculogen synthases (Fig. <xref rid="F4" ref-type="fig">4</xref>A). Because only Q4WAW9 has a crystal structure, we take protein Q4WAW9 [<xref rid="B26" ref-type="bibr">26</xref>] as an example and find that both genes belong to exactly the same protein families with the same domains (Fig. <xref rid="F4" ref-type="fig">4</xref>B). To further validate the results, we compare the structure of A0A0U5GJ41 (alphfold2 predicted) and Q4WAW9 (alphfold2 predicted and crystal structure). The results show that these 2 proteins have a highly similar structure (Figs. <xref rid="supplementary-material-1" ref-type="sec">S4</xref> to <xref rid="supplementary-material-1" ref-type="sec">S7</xref>) with a small root mean square deviation (1.104). Because Q4WAW9 has a 4-level EC number 1.14.11.38, the protein A0A0U5GJ41 could be potentially annotated as EC 1.14.11.38 as well, which supports our 4-level prediction.</p>
      <fig position="float" id="F4">
        <label>Fig. 4.</label>
        <caption>
          <p>(A to C) Comparison of sequence similarity and structural similarity.</p>
        </caption>
        <graphic xlink:href="research.0153.fig.004" position="float"/>
      </fig>
    </sec>
    <sec id="sec11">
      <title>Case study: HDMLF can uncover enzyme promiscuity</title>
      <p>Enzyme promiscuity toward substrates has been discussed in evolutionary terms as providing the flexibility to adapt to novel environments. Moreover, it has been demonstrated that many enzymes exhibit flexibility, or promiscuity, in regard to what substrates their catalytic pockets recognize. A previous study using model-driven approaches found that even in <italic toggle="yes">Escherichia coli</italic>, a well-studied model species, many underground reactions still occur [<xref rid="B27" ref-type="bibr">27</xref>]. For example, gene essentiality analysis and gene knockout experiment revealed that gene tyrB, which is annotated as tyrosine aminotransferase (EC number 2.6.1.57; 2.6.1.107 in UniProt), also has aspartate aminotransferase (encoded by aspC; EC number 2.6.1.1 in UniProt) activity and thus can compensate for aspC gene deletion [<xref rid="B28" ref-type="bibr">28</xref>]. The knockout of the gene encoding the potential isozyme revealed that tyrosine aminotransferase, which is encoded by tyrB (EC number 2.6.1.57; 2.6.1.107 in UniProt), can compensate for the loss of aspartate aminotransferase, which is encoded by aspC (EC number 2.6.1.1 in UniProt).</p>
      <p>Although the specific functional annotation and substrate are different, our method can uncover the underground reaction 2.6.1.1 for tyrB (Fig. <xref rid="F5" ref-type="fig">5</xref>C) . In addition, our method can also assign EC numbers 2.6.1.57 and 2.6.1.107 to tryB. However, with DeepEC, only 2.6.1.57 can be assigned. This can be further validated by the EcoCyc [<xref rid="B29" ref-type="bibr">29</xref>], a well-annotated database for <italic toggle="yes">E. coli</italic> MG1655, as tryB was assigned with EC number 2.6.1.1 in EcoCyc. In addition, there are 852 proteins with more than 90% similarity with tyrB in the UniProt database based on a homology search. Only one protein(UniProt ID: P74861) was reviewed and the assigned EC number is 2.6.1.57. All the other proteins were assigned with EC number 2.6.1.-. Because these proteins all have very high similarity with tryB, they should all have the EC number 2.6.1.1. As expected, our model can assign these proteins with EC number 2.6.1.1. All these cases show that our model can be used to uncover enzyme promiscuity.</p>
      <fig position="float" id="F5">
        <label>Fig. 5.</label>
        <caption>
          <p>(A) TryB is assigned with EC numbers 2.6.1.57 and 2.6.1.107 in UniProt. (B) Our model can assign TyrB with additional EC number 2.6.1.1, and this is consistent with a previous experimental study in <italic toggle="yes">Escherichia coli</italic>, which finds that tyrB can compensate for the loss of aspC (EC number 2.6.1.1 in UniProt), while DeepEC can only assign tyrB with EC number 2.6.1.57.</p>
        </caption>
        <graphic xlink:href="research.0153.fig.005" position="float"/>
      </fig>
    </sec>
    <sec id="sec12">
      <title>Case study: HDMLF can uncover unknown enzyme for a not well-annotated genome</title>
      <p>To demonstrate the inclusiveness and predictive ability of our proposed method, we conducted EC number prediction on unreviewed proteins. <italic toggle="yes">Corynebacterium glutamicum</italic>, the famous industrial workhorse for amino acid production with a current output of over 6 million tons per year [<xref rid="B30" ref-type="bibr">30</xref>], is increasingly being adopted as a promising chassis for the biosynthesis of other compounds. However, unlike <italic toggle="yes">E. coli</italic> (1,652 protein sequences with EC numbers out of 4,322 proteins, 38.2%), the protein sequences of <italic toggle="yes">C. glutamicum</italic> were not well annotated. Of 3,305 protein sequences, only 537 were reviewed and included in the Swiss-Prot database (357 proteins have assigned EC numbers). We used the other 2,768 protein sequences to compare our tool with DeepEC. Our approach was able to assign 1,056 proteins with EC numbers, while DeepEC only assigned 157 EC numbers (123 same EC numbers between DeepEC and HDMLF). We found many interesting cases, for example, pyrimidine reductase, an enzyme involved in riboflavin biosynthesis (UniProt ID: Q8NPB8); DeepEC and TrEMBL both predict it to be non-enzyme, which is not correct, while our model predicts it with EC number 1.1.1.302 with high confidence. Another case is deoxynucleoside monophosphate kinase (UniProt ID:Q8NPP0); DeepEC and TrEMBL cannot assign an EC number, while our model predicts it with EC number 2.7.4.13. As shown in the Rhea database (RHEA:11216), the corresponding reaction and enzyme class are all consistent with protein annotation. These results again show that our method can better annotate new proteins than existing annotation tools.</p>
    </sec>
    <sec id="sec13">
      <title>ECRECer: A web platform for EC prediction based on HDMLF</title>
      <p>To enhance the usability of our proposed framework (Fig. <xref rid="supplementary-material-1" ref-type="sec">S8</xref>) so that the end user can use them smoothly even with no coding experience, we built a web application using a cloud-based architecture, offering high reliability, robustness, and scalability. The static files such as the HTML pages and JavaScript codes are hosted on AWS S3 and distributed through AWS CloudFront for speed of content delivery to users worldwide. End users can simply upload sequences to our platform and then click the submit button to trigger the prediction workflow. AWS API Gateway routes HTTP requests from the front end to the backend. We used AWS Step Functions to coordinate the components of our applications, process messages passed from AWS API Gateway, and invoke the workflows asynchronously. We used AWS S3 to exchange data between jobs and store the result files. In general, the whole workflow can be completed in a few seconds. We use Amazon DynamoDB to store job information, and users can track their previous submission records and corresponding status information. Once the analysis is finished, the user can view or download the corresponding results.</p>
    </sec>
  </sec>
  <sec id="sec24">
    <title>Conclusion</title>
    <p>In this work, we proposed a novel HDMLF to complete 3 benchmarking tasks: (a) enzyme or non-enzyme annotation, (b) quantity of EC numbers prediction, and (c) EC number prediction. The method developed in this work has 2 cores, an embedding core and a learning core. The embedding core is responsible for selecting the best available embedding method among one-hot, UniRep, and ESM to calculate sequence embeddings. The learning core is responsible for completing the specific benchmarking tasks using the best-calculated protein sequence embedding as input in a hierarchical multitask way.</p>
    <p>We were guided by 2 principles in the design of HDMLF. The first principle is providing state-of-the-art EC number prediction performance. The second principle is high usability (both can be accessed via the world wide web and offer a standalone suit for high-throughput prediction). To implement the first principle, we proposed HDMLF, which integrates the protein language model with a hierarchical BGRU with an attention mechanism. To implement the second principle, we provided a web server (ECRECer) and a standalone package. We opened all the source codes, including data preprocessing, dataset buildup, model training, and model testing/evaluation.</p>
    <p>Comprehensive comparisons with existing state-of-the-art methods demonstrated that our method is highly competitive and has the best performance with high usability. In addition, our method can be used to uncover enzyme promiscuity. Although our method exhibited the best performance, it still needs improvement. For example, the performance of multifunctional enzyme annotation is relatively low, while the accuracy and recall of EC number annotation are less than 90%.
<table-wrap position="anchor" id="T1"><table frame="hsides" rules="groups"><tbody><tr><td align="left" rowspan="1" colspan="1">Key points:<break/>
<p><list list-type="simple"><list-item><label>•</label><p>An HDMLF framework is proposed to predict EC numbers by using protein sequence data.</p></list-item><list-item><label>•</label><p>A protein language model and an extreme multilabel classifier are adopted to reduce the heavy head-crafted feature engineering and elevate the prediction performance.</p></list-item><list-item><label>•</label><p>The proposed framework remarkably outperforms the existing state-of-the-art method in terms of accuracy and mF1 score by 70% and 20%, respectively.</p></list-item><list-item><label>•</label><p>An online service and an offline bundle are provided for end users to annotate EC numbers in high throughput easily and efficiently.</p></list-item></list></p></td></tr></tbody></table></table-wrap></p>
  </sec>
  <sec id="sec14">
    <title>Materials and Methods</title>
    <sec id="sec15">
      <title>Problem formulation</title>
      <p>To annotate the enzyme function of a new protein sequence, the initial and basic task is to define whether the given protein is an enzyme. Because there are numerous multifunctional enzymes, the next task is to determine the quantity of EC numbers. After completing the above 2 tasks, it is necessary to assign an EC number to each function. On the basis of these considerations, we proposed 3 basic tasks for the functional annotation of enzymes, as shown below.</p>
      <sec id="sec16">
        <title>Enzyme or non-enzyme annotation</title>
        <p>The enzyme or non-enzyme annotation task is formulated as a binary classification problem:<disp-formula id="EQ1"><mml:math id="M1" display="block" overflow="scroll"><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:mi>X</mml:mi><mml:mo>→</mml:mo><mml:mfenced open="{" close="}" separators=","><mml:mn>0</mml:mn><mml:mn>1</mml:mn></mml:mfenced></mml:math><label>(1)</label></disp-formula>where <italic toggle="yes">X</italic> = {<italic toggle="yes">x</italic><sub>1</sub>, <italic toggle="yes">x</italic><sub>2</sub>, ⋯, <italic toggle="yes">x<sub>n</sub></italic>}, <italic toggle="yes">n</italic> ≥ 1 represents a group of protein sequences, and {0, 1} is the label indicating whether a given protein is an enzyme.</p>
      </sec>
      <sec id="sec17">
        <title>Multifunctional enzyme annotation</title>
        <p>Multifunctional enzyme annotation is formulated as a multiclassification problem:<disp-formula id="EQ2"><mml:math id="M2" display="block" overflow="scroll"><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:mi>X</mml:mi><mml:mo>→</mml:mo><mml:mfenced open="{" close="}" separators=",,,"><mml:mn>1</mml:mn><mml:mn>2</mml:mn><mml:mo>⋯</mml:mo><mml:mi>k</mml:mi></mml:mfenced><mml:mo>,</mml:mo></mml:math><label>(2)</label></disp-formula></p>
        <p>where <italic toggle="yes">k</italic> represents the maximum number of EC numbers for a given protein.</p>
      </sec>
      <sec id="sec18">
        <title>EC number assignment</title>
        <p>The EC number assignment task is also formulated as a multiclassification problem as defined in <xref rid="EQ3" ref-type="disp-formula">Eq. 3</xref>.<disp-formula id="EQ3"><mml:math id="M3" display="block" overflow="scroll"><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:mi>X</mml:mi><mml:mo>→</mml:mo><mml:mfenced open="{" close="}" separators=",,"><mml:mn>1.1.1.1</mml:mn><mml:mn>1.1.1.2</mml:mn><mml:mo>⋯</mml:mo></mml:mfenced><mml:mo>,</mml:mo></mml:math><label>(3)</label></disp-formula></p>
      </sec>
    </sec>
    <sec id="sec19">
      <title>Dataset description</title>
      <p>To address the first challenge, we constructed 3 standard datasets (Supplementary Materials). Similar to previous work [<xref rid="B21" ref-type="bibr">21</xref>,<xref rid="B25" ref-type="bibr">25</xref>], these datasets are extracted from the Swiss-Prot database. To simulate real application scenarios as closely as possible, we did not shuffle data randomly. Instead, after data preprocessing (Supplementary Materials), we organized data in chronological order. Specifically, we used a snapshot from February 2018 as the training dataset. To simulate the real protein discovery and annotation processes and validate the EC prediction performance of effectiveness and stability with time variance, we construct 2 testing sets per task; testing set 1 is from June 2020 snapshot, and testing set 2 is from February 2022, all testing sets filtered the sequences that appeared in the training set. The details are listed in Table <xref rid="supplementary-material-1" ref-type="sec">S7</xref>.<list list-type="simple"><list-item><label>•</label><p>Dataset 1: Enzyme and non-enzyme dataset</p></list-item></list></p>
      <p>The training set in total has 469,134 records, 222,567 of which are enzymes and 246,567 are non-enzymes (Table <xref rid="supplementary-material-1" ref-type="sec">S2</xref>). To make the data more inclusive, we did not filter any sequence in terms of length and homology, which is different from previous studies. An enzyme is labeled as 1 and a non-enzyme is labeled as 0.<list list-type="simple"><list-item><label>•</label><p>Dataset 2: Multifunctional enzyme dataset</p></list-item></list></p>
      <p>The multifunctional enzyme dataset only contains enzyme data. The number of EC categories ranges from 1 to 8 (Table <xref rid="supplementary-material-1" ref-type="sec">S3</xref>).<list list-type="simple"><list-item><label>•</label><p>Dataset 3: EC number dataset</p></list-item></list></p>
      <p>Similar to the multifunctional enzyme dataset, the EC number dataset contains only enzyme records, 222,567 of which constitute the training dataset (covering 5,111 EC numbers). The test data include newly added EC numbers compared with the training data (Fig. <xref rid="supplementary-material-1" ref-type="sec">S3</xref>), which means that these EC numbers do not appear in the training process, so predictive methods cannot handle this part of the EC numbers. Thus, we exclude the sequences with these EC numbers in the evaluation process.</p>
    </sec>
    <sec id="sec20">
      <title>Proposed framework</title>
      <p>To develop a novel EC prediction method with cutting-edge performance, we proposed HDMLF, which is composed of an embedding core and a learning core. These 2 cores operate relatively independently. The embedding core is responsible for embedding protein sequences into a machine-readable matrix. The learning core is responsible for solving specific downstream biological tasks (e.g., enzyme and non-enzyme prediction, multifunctional enzyme prediction, and EC number prediction). The overall scheme of HDMLF is illustrated in Fig. <xref rid="F6" ref-type="fig">6</xref>.<list list-type="simple"><list-item><label>•</label><p>Core 1: Embedding</p></list-item></list></p>
      <fig position="float" id="F6">
        <label>Fig. 6.</label>
        <caption>
          <p>HDMLF is an explicitly designed dual-core driven framework for EC number prediction. It consists of 2 independent operation units—an embedding core and a learning core. The embedding core is tasked with converting protein sequences into features. The learning core is designed to address the specific biological tasks defined in the problem formulation section.</p>
        </caption>
        <graphic xlink:href="research.0153.fig.006" position="float"/>
      </fig>
      <p>The objective of this core is to calculate the embedding representations for protein sequences. For protein sequence encoding/embedding, recent studies have shown the superior performance of deep learning-based methods compared to traditional methods [<xref rid="B31" ref-type="bibr">31</xref>,<xref rid="B32" ref-type="bibr">32</xref>]. Accordingly, we only compared one-hot encoding to show the difference between these 2 kinds of embedding in this study. Here, we adopted 3 different embedding methods to calculate the sequence embedding patterns that adequately represent protein sequences. The first one is the commonly used one-hot encoding [<xref rid="B33" ref-type="bibr">33</xref>]. The second is UniRep [<xref rid="B22" ref-type="bibr">22</xref>], an mLSTM “babbler” deep representation learner for proteins. We used the last layer for protein representation. The third is the ESM [<xref rid="B23" ref-type="bibr">23</xref>], a pretrained transformer language model for protein representation. We used representations from the 1st, 32nd, and 33rd layers as protein embeddings.<list list-type="simple"><list-item><label>•</label><p>Core 2: Learning</p></list-item></list></p>
      <p>The learning core is specialized to perform specific biological tasks using a multitask learning (MTL) framework, which is implemented by a bidirectional GRU (BGRU) network with an attention mechanism. As shown in Fig. <xref rid="F6" ref-type="fig">6</xref>, the learning core uses embedding results as a unified input and uses BGRU to learn enzyme and non-enzyme prediction task (task 1), multifunction enzyme prediction task (task 2), and EC number prediction task (task 3) together. We use 3 multihead attention layers to learn and highlight interactive information among different tasks:<disp-formula id="UEQ1"><mml:math id="M4" display="block" overflow="scroll"><mml:msub><mml:mi>O</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>softmax</mml:mtext><mml:mfenced open="(" close=")"><mml:mfrac><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mi>K</mml:mi><mml:mi>h</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mfenced><mml:msub><mml:mi>V</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mfenced open="{" close="}"><mml:mrow><mml:mtext>task</mml:mtext><mml:mspace width="3pt"/><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>task</mml:mtext><mml:mspace width="3pt"/><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mtext>task</mml:mtext><mml:mspace width="3pt"/><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:math></disp-formula></p>
      <p>where <inline-formula><mml:math id="M5" display="inline" overflow="scroll"><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:msqrt></mml:math></inline-formula> is a scaling factor and <italic toggle="yes">O<sub>h</sub></italic> is a one-head output of the attention layer, <italic toggle="yes">Q<sub>ht<sub>i</sub></sub></italic> is the weight learning weight from task 1 to task 3, and <inline-formula><mml:math id="M6" display="inline" overflow="scroll"><mml:msubsup><mml:mi>K</mml:mi><mml:mi>h</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:math></inline-formula> represents the multitask learning hidden state vector for each learning layers.</p>
      <p>The advantage of MTL is that multiple related learning tasks are solved simultaneously by exploiting commonalities and differences across relevant tasks [<xref rid="B34" ref-type="bibr">34</xref>], which considerably fits the current scenario. Compared with solving the 3 tasks separately or predicting EC number directly, MTL can improve the generalization performances of all the tasks because useful information contained in multiple related tasks is shared in the learning procedure. However, obtaining optimized weight parameters for all tasks will lead to a negative transfer problem that will hurt the learning performance [<xref rid="B35" ref-type="bibr">35</xref>]. To overcome this problem, here, we introduced a penalty parameter Ω to enforce a clustering of the task parameter vectors <italic toggle="yes">a</italic><sub><italic toggle="yes">t</italic>1</sub>, <italic toggle="yes">a</italic><sub><italic toggle="yes">t</italic>2</sub>, <italic toggle="yes">a</italic><sub><italic toggle="yes">t</italic>3</sub> toward their mean that is controlled by a hyperparameter <italic toggle="yes">λ</italic>. Ω is defined as follows:<disp-formula id="UEQ2"><mml:math id="M7" display="block" overflow="scroll"><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">||</mml:mo><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo stretchy="true">||</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:munderover><mml:mo>‍</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true">||</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="true">||</mml:mo></mml:math></disp-formula>where <inline-formula><mml:math id="M8" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mo>‍</mml:mo><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:math></inline-formula> is the mean parameter vector, <italic toggle="yes">T</italic> represents the number of tasks; in this work, we set <italic toggle="yes">λ</italic><sub>1</sub> = 0.5, <italic toggle="yes">λ</italic><sub>2</sub> = 0.1, and <italic toggle="yes">λ</italic><sub>3</sub> = 0.4. The details of implementation and parameter settings can be found in the Supplementary Materials.<list list-type="simple"><list-item><label>•</label><p>Integration, fine-tuning, and output</p></list-item></list></p>
      <p>As illustrated in Fig. <xref rid="F6" ref-type="fig">6</xref>, the final EC number prediction output is an integrated process. As shown in <xref rid="EQ4" ref-type="disp-formula">Eq. 4</xref>, we formulated this integrated process as an optimization problem:<disp-formula id="EQ4"><mml:math id="M9" display="block" overflow="scroll"><mml:munder><mml:mtext>MAX</mml:mtext><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:mfenced open="{" close="}"><mml:mrow><mml:mi>f</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="italic">ob</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="italic">ob</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="italic">ob</mml:mi><mml:msub><mml:mi>j</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="italic">sa</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math><label>(4)</label></disp-formula></p>
      <p>where <italic toggle="yes">obj</italic><sub>1</sub>, <italic toggle="yes">obj</italic><sub>2</sub>, and <italic toggle="yes">obj</italic><sub>3</sub> are the prediction results from task 1, task 2, and task 3, respectively, while <italic toggle="yes">sa</italic> is the predicted result from multiple sequence alignment. The integration and fine-tuning process aim to maximize the optimizing objective. In this work, the objective is the performance of prediction tasks in terms of the F1 score. We used a greedy strategy to perform this optimization.</p>
    </sec>
    <sec id="sec21">
      <title>Compared baselines</title>
      <p>To evaluate our proposed method comprehensively, we compared our proposed method with 4 existing state-of-the-art techniques with ‘GOOD’ usability (Supplementary Materials) and traditional sequence alignment method, which is provided by Diamond software. Four state-of-the-art techniques are CatFam, PRIAM (version 2), ECPred, and DeepEC.</p>
    </sec>
    <sec id="sec22">
      <title>Evaluation metrics</title>
      <p>To comprehensively evaluate the proposed method and existing baselines, we use 5 metrics to evaluate binary classification problems and 4 metrics to evaluate multiple classification problems. For the binary classification task, the evaluation criteria include ACC (accuracy), Precision, NPV (negative predictive value), Recall, and F1 value:<disp-formula id="EQ5"><mml:math id="M10" display="block" overflow="scroll"><mml:mtext>ACC</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>UP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>UN</mml:mtext></mml:mrow></mml:mfrac></mml:math><label>(5)</label></disp-formula><disp-formula id="EQ6"><mml:math id="M11" display="block" overflow="scroll"><mml:mtext>Precision</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow></mml:mfrac></mml:math><label>(6)</label></disp-formula><disp-formula id="EQ7"><mml:math id="M12" display="block" overflow="scroll"><mml:mtext>NPV</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mtext>TN</mml:mtext><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:math><label>(7)</label></disp-formula><disp-formula id="EQ8"><mml:math id="M13" display="block" overflow="scroll"><mml:mtext>Recall</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>UP</mml:mtext></mml:mrow></mml:mfrac></mml:math><label>(8)</label></disp-formula><disp-formula id="EQ9"><mml:math id="M14" display="block" overflow="scroll"><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mtext>Precision</mml:mtext><mml:mo>×</mml:mo><mml:mtext>Recall</mml:mtext></mml:mrow><mml:mrow><mml:mtext>Precision</mml:mtext><mml:mo>+</mml:mo><mml:mtext>Recall</mml:mtext></mml:mrow></mml:mfrac></mml:math><label>(9)</label></disp-formula></p>
      <p>where TP is the true-positive value, FP is the false-positive value, TN is the true-negative value, FN is the false-negative value, UP is unclassified-positive samples, and UN is unclassified-negative samples.</p>
      <p>For multiple classification problems, the evaluation criteria included mACC (macro-average accuracy), mPR (macro-average precision), mRecall (macro-average recall), and mF1 (macro-average F1 value):<disp-formula id="EQ10"><mml:math id="M15" display="block" overflow="scroll"><mml:mtext>mACC</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mo>‍</mml:mo><mml:msub><mml:mtext>ACC</mml:mtext><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:math><label>(10)</label></disp-formula><disp-formula id="EQ11"><mml:math id="M16" display="block" overflow="scroll"><mml:mtext>mPR</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mo>‍</mml:mo><mml:msub><mml:mtext>PPV</mml:mtext><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:math><label>(11)</label></disp-formula><disp-formula id="EQ12"><mml:math id="M17" display="block" overflow="scroll"><mml:mtext>mRecall</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mo>‍</mml:mo><mml:msub><mml:mtext>Recall</mml:mtext><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:math><label>(12)</label></disp-formula><disp-formula id="EQ13"><mml:math id="M18" display="block" overflow="scroll"><mml:mtext>mF</mml:mtext><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mtext>mPR</mml:mtext><mml:mo>×</mml:mo><mml:mtext>mRecall</mml:mtext></mml:mrow><mml:mrow><mml:mtext>mPR</mml:mtext><mml:mo>+</mml:mo><mml:mtext>mRecall</mml:mtext></mml:mrow></mml:mfrac></mml:math><label>(13)</label></disp-formula></p>
      <p>where <italic toggle="yes">N</italic> represents the total number of classes, while ACC<italic toggle="yes"><sub>i</sub></italic>, PPV<italic toggle="yes"><sub>i</sub></italic>, and Recall<italic toggle="yes"><sub>i</sub></italic> represent the accuracy, precision, and recall of the <italic toggle="yes">i</italic>th class in a one-versus-all mode [<xref rid="B36" ref-type="bibr">36</xref>], respectively.</p>
    </sec>
    <sec id="sec23">
      <title>Web platform implementation</title>
      <p>As shown in Fig. <xref rid="supplementary-material-1" ref-type="sec">S8</xref> the web platform uses Amazon ECR to store Docker images, which packages a set of bioinformatics software, such as Diamond and in-house Python scripts. We built a scalable, elastic, and easily maintainable batch engine using AWS Batch. This solution took care of dynamically scaling our computer resources in response to the number of runnable jobs in our job queue. Finally, we used AWS step functions to coordinate the components of our applications easily, process messages passed from AWS API Gateway, and invoke the workflows asynchronously. AWS API Gateway was used as the API server to handle the HTTP requests and route traffic to the correct backends. The static website was hosted by AWS S3 and sped up using AWS CloudFront.</p>
    </sec>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgments</title>
    <p><bold>Funding:</bold> This work was supported by the National Key Research and Development Program of China (2020YFA0908300), the National Natural Science Foundation of China (32201242), the Youth Innovation Promotion Association CAS, Innovation fund of Haihe Laboratory of Synthetic Biology (22HHSWSS00021), Tianjin Synthetic Biotechnology Innovation Capacity Improvement Project (TSBICIP-PTJS-001, TSBICIP-CXRC-018, and TSBICIP-PTJJ-007), and the China Postdoctoral Science Foundation (2022M713328). <bold>Author contributions:</bold> Z.S. and X.L. designed and implemented the model, analyzed the results, and wrote the manuscript. Z.S. and R.D. conducted the experiments. H.M., Z.M., and Q.Y. reviewed the manuscript. R.W. and H.L. designed the website. <bold>Competing interests:</bold> The authors declare that they have no competing interests regarding the publication of this article.</p>
  </ack>
  <sec sec-type="data-availability">
    <title>Data Availability</title>
    <p>The code of HDMLF, the training data, and the prediction results are available at <ext-link xlink:href="https://github.com/kingstdio/ECRECer" ext-link-type="uri">https://github.com/kingstdio/ECRECer</ext-link>
<ext-link xlink:href="https://github.com/tibbdc/ECRECer" ext-link-type="uri">https://github.com/tibbdc/ECRECer</ext-link>. </p>
  </sec>
  <sec sec-type="supplementary-material" id="supplementary-material-1">
    <title>Supplementary Materials</title>
    <supplementary-material id="supp-1" position="float" content-type="local-data">
      <label>Supplementary 1</label>
      <caption>
        <p>Tables S1 to S13</p>
        <p>Figs. S1 to S8</p>
      </caption>
      <media xlink:href="research.0153.f1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><collab>UniProt Consortium</collab>. 
<article-title>Uniprot: The universal protein knowledgebase in 2021</article-title>. <source>Nucleic Acids Res</source>. <year>2021</year>;<volume>49</volume>(<issue>D1</issue>):<fpage>D480</fpage>–<lpage>D489</lpage>.<pub-id pub-id-type="pmid">33237286</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ryu</surname><given-names>JY</given-names></string-name>, <string-name><surname>Kim</surname><given-names>HU</given-names></string-name>, <string-name><surname>Lee</surname><given-names>SY</given-names></string-name></person-group>. 
<article-title>Deep learning enables high-quality and high-throughput prediction of enzyme commission numbers</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2019</year>;<volume>116</volume>(<issue>28</issue>):<fpage>13996</fpage>–<lpage>14001</lpage>.<pub-id pub-id-type="pmid">31221760</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Furnham</surname><given-names>N</given-names></string-name>, <string-name><surname>Garavelli</surname><given-names>JS</given-names></string-name>, <string-name><surname>Apweiler</surname><given-names>R</given-names></string-name>, <string-name><surname>Thornton</surname><given-names>JM</given-names></string-name></person-group>. 
<article-title>Missing in action: Enzyme functional annotations in biological databases</article-title>. <source>Nat Chem Biol</source>. <year>2009</year>;<volume>5</volume>(<issue>8</issue>):<fpage>521</fpage>–<lpage>525</lpage>.<pub-id pub-id-type="pmid">19620987</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McDonald</surname><given-names>AG</given-names></string-name>, <string-name><surname>Tipton</surname><given-names>KF</given-names></string-name></person-group>. 
<article-title>Enzyme nomenclature and classification: The state of the art</article-title>. <source>FEBS J</source>. <year>2023</year>;<volume>290</volume>(<issue>9</issue>):<fpage>2214</fpage>–<lpage>2231</lpage>.<pub-id pub-id-type="pmid">34773359</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hung</surname><given-names>J-H</given-names></string-name>, <string-name><surname>Weng</surname><given-names>Z</given-names></string-name></person-group>. 
<article-title>Sequence alignment and homology search with BLAST and Clustalw</article-title>. <source>Cold Spring Harb Protoc</source>. <year>2016</year>;<volume>2016</volume>(<issue>11</issue>): <comment>10.1101/pdb.prot093088.</comment></mixed-citation>
    </ref>
    <ref id="B6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname><given-names>C</given-names></string-name>, <string-name><surname>Zavaljevski</surname><given-names>N</given-names></string-name>, <string-name><surname>Desai</surname><given-names>V</given-names></string-name>, <string-name><surname>Reifman</surname><given-names>J</given-names></string-name></person-group>. 
<article-title>Genome-wide enzyme annotation with precision control: Catalytic families (catfam) databases</article-title>. <source>Proteins</source>. <year>2009</year>;<volume>74</volume>(<issue>2</issue>):<fpage>449</fpage>–<lpage>460</lpage>.<pub-id pub-id-type="pmid">18636476</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Claudel-Renard</surname><given-names>C</given-names></string-name>, <string-name><surname>Chevalet</surname><given-names>C</given-names></string-name>, <string-name><surname>Faraut</surname><given-names>T</given-names></string-name>, <string-name><surname>Kahn</surname><given-names>D</given-names></string-name></person-group>. 
<article-title>Enzyme-specific profiles for genome annotation: PRIAM</article-title>. <source>Nucleic Acids Res</source>. <year>2003</year>;<volume>31</volume>(<issue>22</issue>):<fpage>6633</fpage>–<lpage>6639</lpage>.<pub-id pub-id-type="pmid">14602924</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nursimulu</surname><given-names>N</given-names></string-name>, <string-name><surname>Xu</surname><given-names>LL</given-names></string-name>, <string-name><surname>Wasmuth</surname><given-names>JD</given-names></string-name>, <string-name><surname>Krukov</surname><given-names>I</given-names></string-name>, <string-name><surname>Parkinson</surname><given-names>J</given-names></string-name></person-group>. 
<article-title>Improved enzyme annotation with ec-specific cutoffs using detect v2</article-title>. <source>Bioinformatics</source>. <year>2018</year>;<volume>34</volume>(<issue>19</issue>):<fpage>3393</fpage>–<lpage>3395</lpage>.<pub-id pub-id-type="pmid">29722785</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arakaki</surname><given-names>AK</given-names></string-name>, <string-name><surname>Huang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Skolnick</surname><given-names>J</given-names></string-name></person-group>. 
<article-title>Eficaz<sup>2</sup>: Enzyme function inference by a combined approach enhanced by machine learning</article-title>. <source>BMC Bioinformatics</source>. <year>2009</year>;<volume>10</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>15</lpage>.<pub-id pub-id-type="pmid">19118496</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dalkiran</surname><given-names>A</given-names></string-name>, <string-name><surname>Rifaioglu</surname><given-names>AS</given-names></string-name>, <string-name><surname>Martin</surname><given-names>MJ</given-names></string-name>, <string-name><surname>Cetin-Atalay</surname><given-names>R</given-names></string-name>, <string-name><surname>Atalay</surname><given-names>V</given-names></string-name>, <string-name><surname>Doğan</surname><given-names>T</given-names></string-name></person-group>. 
<article-title>ECPred: A tool for the prediction of the enzymatic functions of protein sequences based on the EC nomenclature</article-title>. <source>BMC bioinformatics</source>. <year>2018</year>;<volume>19</volume>(<issue>1</issue>):<fpage>334</fpage>.<pub-id pub-id-type="pmid">30241466</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>YH</given-names></string-name>, <string-name><surname>Xu</surname><given-names>JY</given-names></string-name>, <string-name><surname>Tao</surname><given-names>L</given-names></string-name>, <string-name><surname>Li</surname><given-names>XF</given-names></string-name>, <string-name><surname>Li</surname><given-names>S</given-names></string-name>, <string-name><surname>Zeng</surname><given-names>X</given-names></string-name>, <string-name><surname>Chen</surname><given-names>SY</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>P</given-names></string-name>, <string-name><surname>Qin</surname><given-names>C</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>C</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Svm-prot 2016: A web-server for machine learning prediction of protein functional families from sequence irrespective of similarity</article-title>. <source>PLOS ONE</source>. <year>2016</year>;<volume>11</volume>(<issue>8</issue>):
<elocation-id>e0155290</elocation-id>.<pub-id pub-id-type="pmid">27525735</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Akinosho</surname><given-names>TD</given-names></string-name>, <string-name><surname>Oyedele</surname><given-names>LO</given-names></string-name>, <string-name><surname>Bilal</surname><given-names>M</given-names></string-name>, <string-name><surname>Ajayi</surname><given-names>AO</given-names></string-name>, <string-name><surname>Delgado</surname><given-names>MD</given-names></string-name>, <string-name><surname>Akinade</surname><given-names>OO</given-names></string-name>, <string-name><surname>Ahmed</surname><given-names>AA</given-names></string-name></person-group>. 
<article-title>Deep learning in the construction industry: A review of present status and future innovations</article-title>. <source>J Build Eng</source>. <year>2020</year>;<volume>32</volume>:
<elocation-id>101827</elocation-id>.</mixed-citation>
    </ref>
    <ref id="B13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>H</given-names></string-name>, <string-name><surname>Tian</surname><given-names>S</given-names></string-name>, <string-name><surname>Li</surname><given-names>Y</given-names></string-name>, <string-name><surname>Fang</surname><given-names>Q</given-names></string-name>, <string-name><surname>Tan</surname><given-names>R</given-names></string-name>, <string-name><surname>Pan</surname><given-names>Y</given-names></string-name>, <string-name><surname>Huang</surname><given-names>C</given-names></string-name>, <string-name><surname>Xu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Gao</surname><given-names>X</given-names></string-name></person-group>. 
<article-title>Modern deep learning in bioinformatics</article-title>. <source>J Mol Cell Biol</source>. <year>2020</year>;<volume>12</volume>(<issue>11</issue>):<fpage>823</fpage>–<lpage>827</lpage>.<pub-id pub-id-type="pmid">32573721</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>Y</given-names></string-name>, <string-name><surname>Zuo</surname><given-names>Y</given-names></string-name>, <string-name><surname>Song</surname><given-names>H</given-names></string-name>, <string-name><surname>Lv</surname><given-names>Z</given-names></string-name></person-group>. 
<article-title>Deep learning in security of internet of things</article-title>. <source>IEEE Internet Things J</source>. <year>2021</year>;<volume>9</volume>(<issue>22</issue>):<fpage>22133</fpage>–<lpage>22146</lpage>.</mixed-citation>
    </ref>
    <ref id="B15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shi</surname><given-names>Z</given-names></string-name>, <string-name><surname>Wang</surname><given-names>S</given-names></string-name>, <string-name><surname>Yue</surname><given-names>L</given-names></string-name>, <string-name><surname>Pang</surname><given-names>L</given-names></string-name>, <string-name><surname>Zuo</surname><given-names>X</given-names></string-name>, <string-name><surname>Zuo</surname><given-names>W</given-names></string-name>, <string-name><surname>Li</surname><given-names>X</given-names></string-name></person-group>. 
<article-title>Deep dynamic imputation of clinical time series for mortality prediction</article-title>. <source>Inf Sci</source>. <year>2021</year>;<volume>579</volume>:<fpage>607</fpage>–<lpage>622</lpage>.</mixed-citation>
    </ref>
    <ref id="B16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Santos</surname><given-names>CFGD</given-names></string-name>, <string-name><surname>Papa</surname><given-names>JP</given-names></string-name></person-group>. 
<article-title>Avoiding overfitting: A survey on regularization methods for convolutional neural networks</article-title>. <source>ACM Comput Surv</source>. <year>2022</year>;<volume>54</volume>(<issue>10s</issue>):<fpage>1</fpage>–<lpage>25</lpage>.</mixed-citation>
    </ref>
    <ref id="B17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>C</given-names></string-name>, <string-name><surname>Freddolino</surname><given-names>PL</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Y</given-names></string-name></person-group>. 
<article-title>Cofactor: Improved protein function prediction by combining structure, sequence and protein–protein interaction information</article-title>. <source>Nucleic Acids Res</source>. <year>2017</year>;<volume>45</volume>(<issue>W1</issue>):<fpage>W291</fpage>–<lpage>W299</lpage>.<pub-id pub-id-type="pmid">28472402</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname><given-names>H-B</given-names></string-name>, <string-name><surname>Chou</surname><given-names>K-C</given-names></string-name></person-group>. 
<article-title>Ezypred: A top–down approach for predicting enzyme functional classes and subclasses</article-title>. <source>Biochem Biophys Res Commun</source>. <year>2007</year>;<volume>364</volume>(<issue>1</issue>):<fpage>53</fpage>–<lpage>59</lpage>.<pub-id pub-id-type="pmid">17931599</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname><given-names>S</given-names></string-name>, <string-name><surname>Umarov</surname><given-names>R</given-names></string-name>, <string-name><surname>Xie</surname><given-names>B</given-names></string-name>, <string-name><surname>Fan</surname><given-names>M</given-names></string-name>, <string-name><surname>Li</surname><given-names>L</given-names></string-name>, <string-name><surname>Gao</surname><given-names>X</given-names></string-name></person-group>. 
<article-title>Deepre: Sequence-based enzyme ec number prediction by deep learning</article-title>. <source>Bioinformatics</source>. <year>2018</year>;<volume>34</volume>(<issue>5</issue>):<fpage>760</fpage>–<lpage>769</lpage>.<pub-id pub-id-type="pmid">29069344</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>An</surname><given-names>JY</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>Y</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>YJ</given-names></string-name>, <string-name><surname>Yan</surname><given-names>ZJ</given-names></string-name></person-group>. 
<article-title>An efficient feature extraction technique based on local coding PSSM and multifeatures fusion for predicting protein-protein interactions</article-title>. <source>Evol Bioinforma</source>. <year>2019</year>;<volume>15</volume>:<fpage>1176934319879920</fpage>.</mixed-citation>
    </ref>
    <ref id="B21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>KK</given-names></string-name>, <string-name><surname>Wu</surname><given-names>Z</given-names></string-name>, <string-name><surname>Bedbrook</surname><given-names>CN</given-names></string-name>, <string-name><surname>Arnold</surname><given-names>FH</given-names></string-name></person-group>. 
<article-title>Learned protein embeddings for machine learning</article-title>. <source>Bioinformatics</source>. <year>2018</year>;<volume>34</volume>(<issue>15</issue>):<fpage>2642</fpage>–<lpage>2648</lpage>.<pub-id pub-id-type="pmid">29584811</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alley</surname><given-names>EC</given-names></string-name>, <string-name><surname>Khimulya</surname><given-names>G</given-names></string-name>, <string-name><surname>Biswas</surname><given-names>S</given-names></string-name>, <string-name><surname>AlQuraishi</surname><given-names>M</given-names></string-name>, <string-name><surname>Church</surname><given-names>GM</given-names></string-name></person-group>. 
<article-title>Unified rational protein engineering with sequence-based deep representation learning</article-title>. <source>Nat Methods</source>. <year>2019</year>;<volume>16</volume>(<issue>12</issue>):<fpage>1315</fpage>–<lpage>1322</lpage>.<pub-id pub-id-type="pmid">31636460</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <label>23.</label>
      <mixed-citation publication-type="other">Rao R, Meier J, Sercu T, Ovchinnikov S, Rives A. Transformer protein language models are unsupervised structure learners. Paper presented at: ICLR 2021. Proceedings of the International Conference on Learning Representations; 2020 May 3–7; Vienna, Austria.</mixed-citation>
    </ref>
    <ref id="B24">
      <label>24.</label>
      <mixed-citation publication-type="other">Zhang Z, Xu M, Jamasb AR, Chenthamarakshan V, Lozano A, Das P, Tang J. Protein representation learning by geometric structure pretraining. Paper presented at: ICLR 2023. Proceedings of the International Conference on Learning Representations; 2023 May 1–5; Kigali, Rwanda.</mixed-citation>
    </ref>
    <ref id="B25">
      <label>25.</label>
      <mixed-citation publication-type="other">Zhang Z, Xu M, Chenthamarakshan V, Lozano A, Das P, Tang J. Enhancing protein language model with structure-based encoder and pre-training. Paper presented at: MLDD 2023. Proceedings of the International Conference on Learning Representations Machine Learning for Drug Discovery Workshop; 2023 May 5; virtual.</mixed-citation>
    </ref>
    <ref id="B26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grundmann</surname><given-names>A</given-names></string-name>, <string-name><surname>Li</surname><given-names>S-M</given-names></string-name></person-group>. 
<article-title>Overproduction, purification and characterization of ftmpt1, a brevianamide f prenyltransferase from aspergillus fumigatus</article-title>. <source>Microbiology</source>. <year>2005</year>;<volume>151</volume>(<issue>7</issue>):<fpage>2199</fpage>–<lpage>2207</lpage>.<pub-id pub-id-type="pmid">16000710</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khersonsky</surname><given-names>O</given-names></string-name>, <string-name><surname>Tawfik</surname><given-names>DS</given-names></string-name></person-group>. 
<article-title>Enzyme promiscuity: A mechanistic and evolutionary perspective</article-title>. <source>Annu Rev Biochem</source>. <year>2010</year>;<volume>79</volume>:<fpage>471</fpage>–<lpage>505</lpage>.<pub-id pub-id-type="pmid">20235827</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guzmán</surname><given-names>GI</given-names></string-name>, <string-name><surname>Utrilla</surname><given-names>J</given-names></string-name>, <string-name><surname>Nurk</surname><given-names>S</given-names></string-name>, <string-name><surname>Brunk</surname><given-names>E</given-names></string-name>, <string-name><surname>Monk</surname><given-names>JM</given-names></string-name>, <string-name><surname>Ebrahim</surname><given-names>A</given-names></string-name>, <string-name><surname>Palsson</surname><given-names>BO</given-names></string-name>, <string-name><surname>Feist</surname><given-names>AM</given-names></string-name></person-group>. 
<article-title>Model-driven discovery of underground metabolic functions in escherichia coli</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2015</year>;<volume>112</volume>(<issue>3</issue>):<fpage>929</fpage>–<lpage>934</lpage>.<pub-id pub-id-type="pmid">25564669</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Keseler</surname><given-names>IM</given-names></string-name>, <string-name><surname>Gama-Castro</surname><given-names>S</given-names></string-name>, <string-name><surname>Mackie</surname><given-names>A</given-names></string-name>, <string-name><surname>Billington</surname><given-names>R</given-names></string-name>, <string-name><surname>Bonavides-Martínez</surname><given-names>C</given-names></string-name>, <string-name><surname>Caspi</surname><given-names>R</given-names></string-name>, <string-name><surname>Kothari</surname><given-names>A</given-names></string-name>, <string-name><surname>Krummenacker</surname><given-names>M</given-names></string-name>, <string-name><surname>Midford</surname><given-names>PE</given-names></string-name>, <string-name><surname>Muñiz-Rascado</surname><given-names>L</given-names></string-name>, <etal>et al.</etal></person-group><article-title>The ecocyc database in 2021</article-title>. <source>Front Microbiol</source>. <year>2021</year>;<volume>12</volume>:
<elocation-id>711077</elocation-id>.<pub-id pub-id-type="pmid">34394059</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname><given-names>J-Y</given-names></string-name>, <string-name><surname>Na</surname><given-names>Y-A</given-names></string-name>, <string-name><surname>Kim</surname><given-names>E</given-names></string-name>, <string-name><surname>Lee</surname><given-names>H-S</given-names></string-name>, <string-name><surname>Kim</surname><given-names>P</given-names></string-name></person-group>. 
<article-title>The actinobacterium corynebacterium glutamicum, an industrial workhorse</article-title>. <source>J Microbiol Biotechnol</source>. <year>2016</year>;<volume>26</volume>(<issue>5</issue>):<fpage>807</fpage>–<lpage>822</lpage>.<pub-id pub-id-type="pmid">26838341</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anteghini</surname><given-names>M</given-names></string-name>, <string-name><surname>Martins dos Santos</surname><given-names>V</given-names></string-name>, <string-name><surname>Saccenti</surname><given-names>E</given-names></string-name></person-group>. 
<article-title>In-pero: Exploiting deep learning embeddings of protein sequences to predict the localisation of peroxisomal proteins</article-title>. <source>Int J Mol Sci</source>. <year>2021</year>;<volume>22</volume>(<issue>12</issue>):<fpage>6409</fpage>.<pub-id pub-id-type="pmid">34203866</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <label>32.</label>
      <mixed-citation publication-type="other">Martiny H-M, Armenteros JJA, Johansen AR, Salomon J, Nielsen H. Deep protein representations enable recombinant protein expression prediction. bioRxiv. 2021. <pub-id pub-id-type="doi">10.1101/2021.05.13.443426</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>ElAbd</surname><given-names>H</given-names></string-name>, <string-name><surname>Bromberg</surname><given-names>Y</given-names></string-name>, <string-name><surname>Hoarfrost</surname><given-names>A</given-names></string-name>, <string-name><surname>Lenz</surname><given-names>T</given-names></string-name>, <string-name><surname>Franke</surname><given-names>A</given-names></string-name>, <string-name><surname>Wendorff</surname><given-names>M</given-names></string-name></person-group>. 
<article-title>Amino acid encoding for deep learning applications</article-title>. <source>BMC Bioinformatics</source>. <year>2020</year>;<volume>21</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">31898485</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname><given-names>X</given-names></string-name>, <string-name><surname>Lu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>P</given-names></string-name></person-group>. 
<article-title>Multi-task learning on nuclear masses and separation energies with the kernel ridge regression</article-title>. <source>Phys Lett B</source>. <year>2022</year>;<volume>834</volume>:
<elocation-id>137394</elocation-id>.</mixed-citation>
    </ref>
    <ref id="B35">
      <label>35.</label>
      <mixed-citation publication-type="other">Ruder S. An overview of multi-task learning in deep neural networks. arXiv. 2017. https://doi.org/10.48550/arXiv.1706.05098</mixed-citation>
    </ref>
    <ref id="B36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rifkin</surname><given-names>R</given-names></string-name>, <string-name><surname>Klautau</surname><given-names>A</given-names></string-name></person-group>. 
<article-title>In defense of one-vs-all classification</article-title>. <source>J Mach Learn Res</source>. <year>2004</year>;<volume>5</volume>:<fpage>101</fpage>–<lpage>141</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
