<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6697996</article-id>
    <article-id pub-id-type="publisher-id">3000</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-019-3000-5</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Multitask learning for biomedical named entity recognition with cross-sharing structure</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Xi</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lyu</surname>
          <given-names>Jiagao</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dong</surname>
          <given-names>Li</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Xu</surname>
          <given-names>Ke</given-names>
        </name>
        <address>
          <email>kexu@nlsde.buaa.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9999 1211</institution-id><institution-id institution-id-type="GRID">grid.64939.31</institution-id><institution>State Key Laboratory of Software Development Environment, Beihang University, </institution></institution-wrap>Beijing, 100191 China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>16</day>
      <month>8</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>16</day>
      <month>8</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <elocation-id>427</elocation-id>
    <history>
      <date date-type="received">
        <day>6</day>
        <month>12</month>
        <year>2018</year>
      </date>
      <date date-type="accepted">
        <day>18</day>
        <month>7</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p>Biomedical named entity recognition (BioNER) is a fundamental and essential task for biomedical literature mining, which affects the performance of downstream tasks. Most BioNER models rely on domain-specific features or hand-crafted rules, but extracting features from massive data requires much time and human efforts. To solve this, neural network models are used to automatically learn features. Recently, multi-task learning has been applied successfully to neural network models of biomedical literature mining. For BioNER models, using multi-task learning makes use of features from multiple datasets and improves the performance of models.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>In experiments, we compared our proposed model with other multi-task models and found our model outperformed the others on datasets of gene, protein, disease categories. We also tested the performance of different dataset pairs to find out the best partners of datasets. Besides, we explored and analyzed the influence of different entity types by using sub-datasets. When dataset size was reduced, our model still produced positive results.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p>We propose a novel multi-task model for BioNER with the cross-sharing structure to improve the performance of multi-task models. The cross-sharing structure in our model makes use of features from both datasets in the training procedure. Detailed analysis about best partners of datasets and influence between entity categories can provide guidance of choosing proper dataset pairs for multi-task training. Our implementation is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/JogleLew/bioner-cross-sharing">https://github.com/JogleLew/bioner-cross-sharing</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Multi-task learning</kwd>
      <kwd>Named entity recognition</kwd>
      <kwd>Cross-sharing structure</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id>
            <institution>National Natural Science Foundation of China</institution>
          </institution-wrap>
        </funding-source>
        <award-id>61421003</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100011347</institution-id>
            <institution>State Key Laboratory of Software Development Environment</institution>
          </institution-wrap>
        </funding-source>
        <award-id>SKLSDE- 2017ZX-05</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Biomedical named entity recognition (BioNER) aims at annotating named entity mentions with their entity types (e.g., genes, proteins [<xref ref-type="bibr" rid="CR1">1</xref>], and diseases [<xref ref-type="bibr" rid="CR2">2</xref>]) in the input biomedical text. The outputs of model indicate not only the locations of entity mentions but also their types. BioNER models provide useful information for downstream tasks of biomedical literature mining, such as entity relation extraction [<xref ref-type="bibr" rid="CR3">3</xref>–<xref ref-type="bibr" rid="CR5">5</xref>], and biomedical network construction [<xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR8">8</xref>].</p>
    <p>BioNER task requires to detect boundaries of biomedical entities and predict their entity types. Most previous systems treat the task as a sequence labeling problem. Traditional neural network models for BioNER rely on features designed for each task. These BioNER models use hand-crafted rules [<xref ref-type="bibr" rid="CR9">9</xref>] and domain-specific features [<xref ref-type="bibr" rid="CR10">10</xref>], such as orthographic features, morphological features [<xref ref-type="bibr" rid="CR11">11</xref>–<xref ref-type="bibr" rid="CR14">14</xref>]. The drawback of these neural network models is that features are specially designed for each dataset or each entity type in order to achieve good performance; thus, features used in one BioNER model may not work well in another. Recent studies showed that the neural network model is capable of feature generation work without manual choosing. Some of these models use bi-directional Long Short-Term Memory with Conditional Random Field (BiLSTM-CRF) [<xref ref-type="bibr" rid="CR15">15</xref>], and other models have extra character-level CNN [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR17">17</xref>] or character-level LSTM [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR19">19</xref>] to capture character features of entities.</p>
    <p>Recently, multi-task learning (MTL) [<xref ref-type="bibr" rid="CR20">20</xref>] has been adopted successfully to applications of biomedical literature mining, such as drug discovery [<xref ref-type="bibr" rid="CR21">21</xref>], entity linking [<xref ref-type="bibr" rid="CR22">22</xref>]. The multi-task model trains several datasets at the same time, and transfers domain information between datasets. By sharing representations between the main task and auxiliary task, the multi-task model improves the performance on the main task. For MTL BioNER models, the number of successful examples is growing. Crichton et al. [<xref ref-type="bibr" rid="CR23">23</xref>] uses convolution layer as the shared part and fully connected layer as task-specific part. Wang et al. [<xref ref-type="bibr" rid="CR19">19</xref>] experiments shared character Bi-LSTM, shared word Bi-LSTM, and shared both. Although the multi-task model can optimize the performance of the main dataset, using different combinations of training datasets may have discrepancy performances. Some other models use special methods to improve performance, such as adversarial loss [<xref ref-type="bibr" rid="CR24">24</xref>], label-aware MMD [<xref ref-type="bibr" rid="CR25">25</xref>], Learn What to Share Structure [<xref ref-type="bibr" rid="CR26">26</xref>].</p>
    <p>In this paper, we compare some different multi-task models and propose our new model with the cross-sharing structure for BioNER. No hand-crafted feature is required in our model. The proposed model is based on the BiLSTM-CNN-CRF model [<xref ref-type="bibr" rid="CR16">16</xref>] which is a single-task neural network model. In our model, shared Bi-LSTM unit is used to learn the shared features, and private Bi-LSTM units are for the task-specific features. Besides, a cross-sharing structure helps to share information between private units. We compare the proposed model with other multi-task models [<xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR24">24</xref>] on four main datasets of different domains. We also discover the influence of dataset pairs and dataset size to the performance of our proposed model. Results demonstrate that the proposed model achieves good results. Our method provides a novel structure of multi-task sharing in BioNER task and improves the overall performance on BioNER datasets.</p>
  </sec>
  <sec id="Sec2">
    <title>Preliminaries</title>
    <p>In this section, some basic concepts related to our multi-task neural network are introduced.</p>
    <sec id="Sec3">
      <title>Bi-directional long short-Term memory (Bi-LSTM)</title>
      <p>Long Short-Term Memory (LSTM) [<xref ref-type="bibr" rid="CR27">27</xref>] is a special edition of Recurrent neural network (RNN), and LSTM avoids the gradient vanishing or exploding problems appearing in RNN. A normal LSTM cell contains a input gate, a output gate and a forget gate, and there are connections between these gates. We denote <bold><italic>X</italic></bold>={<bold><italic>x</italic></bold><sub>1</sub>,<bold><italic>x</italic></bold><sub>2</sub>,...,<bold><italic>x</italic></bold><sub><italic>T</italic></sub>} as the series input of LSTM, where <italic>T</italic> is the sequence length of input vector. The output of LSTM is a sequence of vector <bold><italic>H</italic></bold>={<bold><italic>h</italic></bold><sub>1</sub>,<bold><italic>h</italic></bold><sub>2</sub>,...,<bold><italic>h</italic></bold><sub><italic>T</italic></sub>}. The LSTM cell calculates <bold><italic>h</italic></bold><sub><italic>t</italic></sub> via the following calculation: 
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} \boldsymbol{f}_{t} &amp;= \sigma (\boldsymbol{W}_{f} [\boldsymbol{h}_{t-1}, \boldsymbol{x}_{t}] + \boldsymbol{b}_{f}) \end{array} $$ \end{document}</tex-math><mml:math id="M2"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd class="align-2"><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mspace width="2em"/></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_3000_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>
        <disp-formula id="Equ2">
          <label>2</label>
          <alternatives>
            <tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} \boldsymbol{i}_{t} &amp;= \sigma (\boldsymbol{W}_{i} [\boldsymbol{h}_{t-1}, \boldsymbol{x}_{t}] + \boldsymbol{b}_{i}) \end{array} $$ \end{document}</tex-math>
            <mml:math id="M4">
              <mml:mtable class="align" columnalign="left">
                <mml:mtr>
                  <mml:mtd class="align-1">
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">i</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mtd>
                  <mml:mtd class="align-2">
                    <mml:mo>=</mml:mo>
                    <mml:mi>σ</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">W</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>[</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">h</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                        <mml:mo>−</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">x</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>]</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">b</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>)</mml:mo>
                    <mml:mspace width="2em"/>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
            <graphic xlink:href="12859_2019_3000_Article_Equ2.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>
        <disp-formula id="Equ3">
          <label>3</label>
          <alternatives>
            <tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} \tilde{\boldsymbol{C}_{t}} &amp;= tanh (\boldsymbol{W}_{C} [\boldsymbol{h}_{t-1}, \boldsymbol{x}_{t}] + \boldsymbol{b}_{C}) \end{array} $$ \end{document}</tex-math>
            <mml:math id="M6">
              <mml:mtable class="align" columnalign="left">
                <mml:mtr>
                  <mml:mtd class="align-1">
                    <mml:mover accent="true">
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi mathvariant="bold-italic">C</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>t</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mo>~</mml:mo>
                    </mml:mover>
                  </mml:mtd>
                  <mml:mtd class="align-2">
                    <mml:mo>=</mml:mo>
                    <mml:mtext mathvariant="italic">tanh</mml:mtext>
                    <mml:mo>(</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">W</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>C</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>[</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">h</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                        <mml:mo>−</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">x</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>]</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">b</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>C</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>)</mml:mo>
                    <mml:mspace width="2em"/>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
            <graphic xlink:href="12859_2019_3000_Article_Equ3.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>
        <disp-formula id="Equ4">
          <label>4</label>
          <alternatives>
            <tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} \boldsymbol{C}_{t} &amp;= \boldsymbol{f}_{t} \odot \boldsymbol{C}_{t-1} + \boldsymbol{i}_{t} \odot \tilde{\boldsymbol{C}_{t}} \end{array} $$ \end{document}</tex-math>
            <mml:math id="M8">
              <mml:mtable class="align" columnalign="left">
                <mml:mtr>
                  <mml:mtd class="align-1">
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">C</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mtd>
                  <mml:mtd class="align-2">
                    <mml:mo>=</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">f</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>⊙</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">C</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                        <mml:mo>−</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>+</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">i</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>⊙</mml:mo>
                    <mml:mover accent="true">
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi mathvariant="bold-italic">C</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>t</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mo>~</mml:mo>
                    </mml:mover>
                    <mml:mspace width="2em"/>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
            <graphic xlink:href="12859_2019_3000_Article_Equ4.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>
        <disp-formula id="Equ5">
          <label>5</label>
          <alternatives>
            <tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} \boldsymbol{o}_{t} &amp;= \sigma (\boldsymbol{W}_{o} [\boldsymbol{h}_{t-1}, \boldsymbol{x}_{t}] + \boldsymbol{b}_{o}) \end{array} $$ \end{document}</tex-math>
            <mml:math id="M10">
              <mml:mtable class="align" columnalign="left">
                <mml:mtr>
                  <mml:mtd class="align-1">
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">o</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mtd>
                  <mml:mtd class="align-2">
                    <mml:mo>=</mml:mo>
                    <mml:mi>σ</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">W</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>o</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>[</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">h</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                        <mml:mo>−</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">x</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>]</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">b</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>o</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>)</mml:mo>
                    <mml:mspace width="2em"/>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
            <graphic xlink:href="12859_2019_3000_Article_Equ5.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>
        <disp-formula id="Equ6">
          <label>6</label>
          <alternatives>
            <tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} \boldsymbol{h}_{t} &amp;= \boldsymbol{o}_{t} \odot tanh (\boldsymbol{C}_{t}) \end{array} $$ \end{document}</tex-math>
            <mml:math id="M12">
              <mml:mtable class="align" columnalign="left">
                <mml:mtr>
                  <mml:mtd class="align-1">
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">h</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mtd>
                  <mml:mtd class="align-2">
                    <mml:mo>=</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">o</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>⊙</mml:mo>
                    <mml:mtext mathvariant="italic">tanh</mml:mtext>
                    <mml:mo>(</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">C</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>)</mml:mo>
                    <mml:mspace width="2em"/>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
            <graphic xlink:href="12859_2019_3000_Article_Equ6.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>In these equations, ⊙ denotes element-wise multiplication. <italic>σ</italic> and <italic>t</italic><italic>a</italic><italic>n</italic><italic>h</italic> are element-wise sigmoid function and tanh function, respectively. <bold><italic>f</italic></bold><sub><italic>t</italic></sub>,<bold><italic>i</italic></bold><sub><italic>t</italic></sub>,<bold><italic>o</italic></bold><sub><italic>t</italic></sub> are the forget gate, the input gate, and the output gate, respectively. <inline-formula id="IEq1"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$ \tilde {\boldsymbol {C}_{t}} $\end{document}</tex-math><mml:math id="M14"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2019_3000_Article_IEq1.gif"/></alternatives></inline-formula> indicates some information from current input applied to cell state. <bold><italic>h</italic></bold><sub><italic>t</italic></sub> calculates the cell output by the input and current cell state. <bold><italic>W</italic></bold><sub><italic>j</italic></sub>,<bold><italic>b</italic></bold><sub><italic>j</italic></sub>(<italic>j</italic>=<italic>f</italic>,<italic>i</italic>,<italic>C</italic>,<italic>o</italic>) are the trainable parameters. The LSTM cell is designed to avoid the long-term dependency problem, and it is capable of capturing information for long periods.</p>
      <p>Bi-LSTM is the two-direction version of LSTM. For original LSTM, the cells take input in one direction, so <bold><italic>h</italic></bold><sub><italic>t</italic></sub> will capture some information only from previous LSTM cells. In order to capture the information from the following cells, another set of LSTM cells is used in Bi-LSTM. As shown in Figure <xref rid="Fig1" ref-type="fig">1</xref>, the bi-directional long short-term memory (Bi-LSTM) model contains two directions of LSTM network, original direction and reversed direction.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Bi-LSTM Structure. The figure displays a part of Bi-LSTM network. Input vectors are fed to two directions of LSTM, and the output of two directions of LSTM is concatenated as the whole output</p></caption><graphic xlink:href="12859_2019_3000_Fig1_HTML" id="MO1"/></fig></p>
      <p>
        <disp-formula id="Equ7">
          <label>7</label>
          <alternatives>
            <tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \overrightarrow{\textbf{h}}_{t} = LSTM\left(\overrightarrow{\textbf{h}}_{t - 1}, {\textbf{x}}_{t}\right)  $$ \end{document}</tex-math>
            <mml:math id="M16">
              <mml:msub>
                <mml:mrow>
                  <mml:mover class="overrightarrow">
                    <mml:mrow>
                      <mml:mtext mathvariant="bold">h</mml:mtext>
                    </mml:mrow>
                    <mml:mo>⃗</mml:mo>
                  </mml:mover>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>=</mml:mo>
              <mml:mtext mathvariant="italic">LSTM</mml:mtext>
              <mml:mfenced close=")" open="(" separators="">
                <mml:mrow>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mover class="overrightarrow">
                        <mml:mrow>
                          <mml:mtext mathvariant="bold">h</mml:mtext>
                        </mml:mrow>
                        <mml:mo>⃗</mml:mo>
                      </mml:mover>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                      <mml:mo>−</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>,</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mtext mathvariant="bold">x</mml:mtext>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
              </mml:mfenced>
            </mml:math>
            <graphic xlink:href="12859_2019_3000_Article_Equ7.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>
        <disp-formula id="Equ8">
          <label>8</label>
          <alternatives>
            <tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \overleftarrow{\textbf{h}}_{t} = LSTM\left(\overleftarrow{\textbf{h}}_{t + 1}, {\textbf{x}}_{t}\right)  $$ \end{document}</tex-math>
            <mml:math id="M18">
              <mml:msub>
                <mml:mrow>
                  <mml:mover class="overleftarrow">
                    <mml:mrow>
                      <mml:mtext mathvariant="bold">h</mml:mtext>
                    </mml:mrow>
                    <mml:mo>⃖</mml:mo>
                  </mml:mover>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>=</mml:mo>
              <mml:mtext mathvariant="italic">LSTM</mml:mtext>
              <mml:mfenced close=")" open="(" separators="">
                <mml:mrow>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mover class="overleftarrow">
                        <mml:mrow>
                          <mml:mtext mathvariant="bold">h</mml:mtext>
                        </mml:mrow>
                        <mml:mo>⃖</mml:mo>
                      </mml:mover>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                      <mml:mo>+</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>,</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mtext mathvariant="bold">x</mml:mtext>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
              </mml:mfenced>
            </mml:math>
            <graphic xlink:href="12859_2019_3000_Article_Equ8.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>
        <disp-formula id="Equ9">
          <label>9</label>
          <alternatives>
            <tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \textbf{o}_{t} = \overrightarrow{\textbf{h}}_{t} \oplus \overleftarrow{\textbf{h}}_{t}  $$ \end{document}</tex-math>
            <mml:math id="M20">
              <mml:msub>
                <mml:mrow>
                  <mml:mtext mathvariant="bold">o</mml:mtext>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>=</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mover class="overrightarrow">
                    <mml:mrow>
                      <mml:mtext mathvariant="bold">h</mml:mtext>
                    </mml:mrow>
                    <mml:mo>⃗</mml:mo>
                  </mml:mover>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>⊕</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mover class="overleftarrow">
                    <mml:mrow>
                      <mml:mtext mathvariant="bold">h</mml:mtext>
                    </mml:mrow>
                    <mml:mo>⃖</mml:mo>
                  </mml:mover>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
              </mml:msub>
            </mml:math>
            <graphic xlink:href="12859_2019_3000_Article_Equ9.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>In these equations, <inline-formula id="IEq2"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$ \overrightarrow {\boldsymbol {h}}_{t} $\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mrow><mml:mover class="overrightarrow"><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mo>⃗</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_3000_Article_IEq2.gif"/></alternatives></inline-formula> and <inline-formula id="IEq3"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$ \overleftarrow {\boldsymbol {h}}_{t} $\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mrow><mml:mover class="overleftarrow"><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mo>⃖</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_3000_Article_IEq3.gif"/></alternatives></inline-formula> are the cells output of two directions. ⊕ denotes vector concatenation. The vectors, <inline-formula id="IEq4"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$ \overrightarrow {\boldsymbol {h}}_{t} $\end{document}</tex-math><mml:math id="M26"><mml:msub><mml:mrow><mml:mover class="overrightarrow"><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mo>⃗</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_3000_Article_IEq4.gif"/></alternatives></inline-formula> and <inline-formula id="IEq5"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$ \overleftarrow {\boldsymbol {h}}_{t} $\end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mrow><mml:mover class="overleftarrow"><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mo>⃖</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_3000_Article_IEq5.gif"/></alternatives></inline-formula>, are concatenated as the final output. In this way, <bold><italic>o</italic></bold><sub><italic>t</italic></sub> keeps the information from previous and following LSTM cells.</p>
    </sec>
    <sec id="Sec4">
      <title>Conditional random field (CRF)</title>
      <p>Conditional Random Field (CRF) [<xref ref-type="bibr" rid="CR28">28</xref>] is a conditional probability distribution model and widely used in sequence labeling tasks to generate new tag based on recent tags. When a set of random variables are given as input, CRF outputs another set of random variables according to some rules. For example, in biomedical NER task with IOB annotation, the tag after B-Gene can be I-Gene rather than I-Disease. If the previous tag is B-Gene, CRF would output I-Disease in a low probability to avoid the error of mixing different types of tags. CRF has been adopted in many state-of-art models to help to generate meaningful and legal annotations.</p>
      <p>Let the input of CRF is vector <bold><italic>Z</italic></bold>=(<bold><italic>z</italic></bold><sub>1</sub>,<bold><italic>z</italic></bold><sub>2</sub>,...,<bold><italic>z</italic></bold><sub><italic>n</italic></sub>), and the generated output sequence is <inline-formula id="IEq6"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$ \boldsymbol {\hat {Y}} = (\hat {y}_{1}, \hat {y}_{2},..., \hat {y}_{n}) $\end{document}</tex-math><mml:math id="M30"><mml:mi mathvariant="bold-italic">Ŷ</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>ŷ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>ŷ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>...</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>ŷ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12859_2019_3000_Article_IEq6.gif"/></alternatives></inline-formula>. For BioNER task, the input <bold><italic>z</italic></bold><sub><italic>i</italic></sub> can be a feature vector representing the <italic>i</italic>th word. CRF model describes the probability of generating the whole label sequence based on <bold><italic>Z</italic></bold>, shown as below: 
<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ p(\boldsymbol{\hat{Y}}|\boldsymbol{Z}; \boldsymbol{W}, \boldsymbol{b})=\frac{\prod_{i=1}^{n} f_{i}(\hat{y}_{i-1}, \hat{y}_{i}, \boldsymbol{Z})}{\sum_{y' \in\phi(\boldsymbol{Z})} \prod_{i=1}^{n} f_{i}(y'_{i-1}, y'_{i}, \boldsymbol{Z})}  $$ \end{document}</tex-math><mml:math id="M32"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Ŷ</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mrow><mml:mo>∏</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>ŷ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>ŷ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:munder><mml:munderover><mml:mrow><mml:mo>∏</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="12859_2019_3000_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>In this equation, <italic>ϕ</italic>(<bold><italic>Z</italic></bold>) represents all of the possible label sequences for <italic>Z</italic>. The function <inline-formula id="IEq7"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$ f_{i}(y_{j}, y_{k}, \boldsymbol {Z}) = exp(\boldsymbol {W}_{y_{j},y_{k}}\boldsymbol {z_{i}} + \boldsymbol {b}_{y_{j},y_{k}}) \phantom {\dot {i}\!}$\end{document}</tex-math><mml:math id="M34"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">exp</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mstyle mathvariant="bold-italic"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12859_2019_3000_Article_IEq7.gif"/></alternatives></inline-formula>, where the weight <inline-formula id="IEq8"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\phantom {\dot {i}\!} \boldsymbol {W}_{y_{j},y_{k}} $\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_3000_Article_IEq8.gif"/></alternatives></inline-formula> and the bias <inline-formula id="IEq9"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$ \boldsymbol {b}_{y_{j},y_{k}} \phantom {\dot {i}\!}$\end{document}</tex-math><mml:math id="M38"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_3000_Article_IEq9.gif"/></alternatives></inline-formula> are the trainable parameters corresponding to the pair of labels (<italic>y</italic><sub><italic>j</italic></sub>,<italic>y</italic><sub><italic>k</italic></sub>).</p>
      <p>In the training procedure, we use the negative log-likelihood function to calculate the loss function <italic>J</italic> and find the optimal sequence <italic>y</italic><sup>∗</sup> by minimum the loss function. The Viterbi algorithm is used to calculate the loss and the optimal sequence. 
<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} J(\boldsymbol{W}, \boldsymbol{b}) &amp;= -\sum_{i} \log(p(\boldsymbol{\hat{Y}}|\boldsymbol{Z}; \boldsymbol{W}, \boldsymbol{b})) \end{array} $$ \end{document}</tex-math><mml:math id="M40"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:mi>J</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>)</mml:mo></mml:mtd><mml:mtd class="align-2"><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munder><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mo>log</mml:mo><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Ŷ</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mspace width="2em"/></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_3000_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>
        <disp-formula id="Equ12">
          <label>12</label>
          <alternatives>
            <tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} y^{*} &amp;= \underset{y \in \phi(\boldsymbol{Z})}{\arg\min} \ \ J(\boldsymbol{W}, \boldsymbol{b}) \end{array} $$ \end{document}</tex-math>
            <mml:math id="M42">
              <mml:mtable class="align" columnalign="left">
                <mml:mtr>
                  <mml:mtd class="align-1">
                    <mml:msup>
                      <mml:mrow>
                        <mml:mi>y</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>∗</mml:mo>
                      </mml:mrow>
                    </mml:msup>
                  </mml:mtd>
                  <mml:mtd class="align-2">
                    <mml:mo>=</mml:mo>
                    <mml:munder>
                      <mml:mrow>
                        <mml:mo>arg</mml:mo>
                        <mml:mo>min</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>y</mml:mi>
                        <mml:mo>∈</mml:mo>
                        <mml:mi>ϕ</mml:mi>
                        <mml:mo>(</mml:mo>
                        <mml:mi mathvariant="bold-italic">Z</mml:mi>
                        <mml:mo>)</mml:mo>
                      </mml:mrow>
                    </mml:munder>
                    <mml:mspace width="1em"/>
                    <mml:mspace width="1em"/>
                    <mml:mi>J</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mi mathvariant="bold-italic">W</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi mathvariant="bold-italic">b</mml:mi>
                    <mml:mo>)</mml:mo>
                    <mml:mspace width="2em"/>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
            <graphic xlink:href="12859_2019_3000_Article_Equ12.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
    </sec>
  </sec>
  <sec id="Sec5">
    <title>Methods</title>
    <p>In this section, we introduce our baseline single-task model and some multi-task models for BioNER tasks.</p>
    <sec id="Sec6">
      <title>Baseline single-task model (STM)</title>
      <p>We choose the model from Ma and Hovy [<xref ref-type="bibr" rid="CR16">16</xref>] as our baseline single-task model. Unlike the vanilla BiLSTM-CRF model, this model uses an extra CNN layer to capture character-level features. All the multi-task models in the paper are implemented based on this single-task model; thus, we choose it as our baseline model. The model structure is shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Single-task Model (STM). The input is a sentence from the BioNER dataset. The dotted rectangles represent words in a sentence, and the solid rectangles represent Bi-LSTM cells. The circles represent CNN units, and the double circles represent CRF units. The tags in the double circles, e.g., “O”, “B-GENE”, are the output of the CRF layer</p></caption><graphic xlink:href="12859_2019_3000_Fig2_HTML" id="MO2"/></fig></p>
      <p>For simplicity, <bold><italic>w</italic></bold><sub><italic>t</italic></sub> denotes word embedding of word <italic>t</italic> and the <bold><italic>c</italic></bold><sub><italic>t</italic></sub> denotes character embeddings of word <italic>t</italic>. The shape of <bold><italic>c</italic></bold><sub><italic>t</italic></sub> is <italic>d</italic><sub><italic>c</italic></sub> by <italic>l</italic><sub><italic>c</italic></sub>, where <italic>d</italic><sub><italic>c</italic></sub> is the dimension of character embedding and <italic>l</italic><sub><italic>c</italic></sub> is the count of characters in the word.</p>
      <p>In the embedding layer, the character representation <bold><italic>r</italic></bold><sub><italic>t</italic></sub> is calculated based on character embedding <bold><italic>c</italic></bold><sub><italic>t</italic></sub> by CNN to extract morphological information. The CNN scheme we use is the same as Ma and Hovy [<xref ref-type="bibr" rid="CR16">16</xref>]. The convolution has the filter size of <italic>d</italic><sub><italic>c</italic></sub> by <italic>l</italic><sub><italic>f</italic></sub> and padding length of <italic>l</italic><sub><italic>f</italic></sub>−1, where <italic>l</italic><sub><italic>f</italic></sub> is a hyperparameter. After the convolution calculation, the output is a new vector of shape <italic>d</italic><sub><italic>c</italic></sub> by (<italic>l</italic><sub><italic>c</italic></sub>+<italic>l</italic><sub><italic>f</italic></sub>−1). Then max pooling is used to produce a vector of size <italic>d</italic><sub><italic>c</italic></sub> as the final char representation <bold><italic>r</italic></bold><sub><italic>t</italic></sub>. A dropout layer is adopted at the input of CNN. Finally, word embedding <bold><italic>w</italic></bold><sub><italic>t</italic></sub> and character representation <bold><italic>r</italic></bold><sub><italic>t</italic></sub> are concatenated as <bold><italic>x</italic></bold><sub><italic>t</italic></sub>.</p>
      <p>After the embedding layer, resulting sequence of embeddings <bold><italic>X</italic></bold>={<bold><italic>x</italic></bold><sub>1</sub>,<bold><italic>x</italic></bold><sub>2</sub>,...,<bold><italic>x</italic></bold><sub><italic>n</italic></sub>} are fed into Bi-LSTM layer to get <bold><italic>O</italic></bold>={<bold><italic>o</italic></bold><sub>1</sub>,<bold><italic>o</italic></bold><sub>2</sub>,...,<bold><italic>o</italic></bold><sub><italic>n</italic></sub>}. Two dropout layers are applied at the input and output of the Bi-LSTM layer. The Bi-LSTM layer is used to extract information from the word representation <bold><italic>x</italic></bold><sub><italic>t</italic></sub>.</p>
      <p>The top layer of the model is the CRF layer. This layer takes output vectors <bold><italic>O</italic></bold> to predict label sequences. As shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, the word “28S” and the word “rRNA” are predicted as B-Gene and I-Gene, respectively, which suggests that the model recognizes the entity “28S rRNA”.</p>
    </sec>
    <sec id="Sec7">
      <title>Fully-shared multi-task model (FS-MTM)</title>
      <p>Our fully-shared multi-task model is based on MTM-CW from Crichton et al. [<xref ref-type="bibr" rid="CR23">23</xref>]. All the multi-task models in this paper are designed for two datasets. If modifications applied, these models are suitable for three or more datasets. The embedding layer, Bi-LSTM layer and CRF layer in the multi-task models are the same as those in the baseline single-task model.</p>
      <p>In the fully-shared multi-task model, we use an embedding layer and a Bi-LSTM layer as shared parts, and two CRF layers for two datasets, as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. When training and testing, word embeddings and character embeddings are first fed to the embedding layer, and then the Bi-LSTM layer takes the output of embedding layer. In the end, the output of Bi-LSTM is fed to one of the CRF layers. If source data is from dataset 1, CRF layer for dataset 1 is activated with another CRF layer ignored, and vice versa. In this model, Bi-LSTM captures all the features of dataset 1 and 2, and CRF layer produces different tags according to the input dataset.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Fully-shared Multi-task Model (FS-MTM). The embedding layer and the Bi-LSTM layer are shared by two datasets, and two CRF layer are used for two datasets</p></caption><graphic xlink:href="12859_2019_3000_Fig3_HTML" id="MO3"/></fig></p>
    </sec>
    <sec id="Sec8">
      <title>Shared-private multi-task model (SP-MTM)</title>
      <p>Our shared-private multi-task model is based on SP-MTL from Liu et al. [<xref ref-type="bibr" rid="CR24">24</xref>]. As shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>, there are two private Bi-LSTMs for two tasks and one shared Bi-LSTM. Word embeddings and character embeddings are first fed to the embedding layer. Then the output of the embedding layer is replicated and fed into shared Bi-LSTM and corresponding private Bi-LSTM, according to the source dataset. Finally, the output of shared and private Bi-LSTMs are concatenated and fed into corresponding CRF layer. In this model, shared Bi-LSTM and private Bi-LSTM captures shared and task-independent features, respectively. CRF layer produces different tags based on task-related feature representations.
<fig id="Fig4"><label>Fig. 4</label><caption><p>Shared-private Multi-task Model (SP-MTM). The embedding layer and shared Bi-LSTM are shared by two datasets. Two CRF layer and two private Bi-LSTMs are used for two datasets</p></caption><graphic xlink:href="12859_2019_3000_Fig4_HTML" id="MO4"/></fig></p>
    </sec>
    <sec id="Sec9">
      <title>Adversarial multi-task model (ADV-MTM)</title>
      <p>As shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>, our adversarial multi-task model is based on the adversarial shared-private model from Liu et al. [<xref ref-type="bibr" rid="CR24">24</xref>]. The basic network structure of the adversarial multi-task model is the same as the shared-private multi-task model, but the calculation of loss is different.
<fig id="Fig5"><label>Fig. 5</label><caption><p>Adversarial Multi-task Model (ADV-MTM). The embedding layer and shared Bi-LSTM are shared by two datasets. Two CRF layer and two private Bi-LSTMs are used for two datasets. Three kinds of losses are marked on the figure</p></caption><graphic xlink:href="12859_2019_3000_Fig5_HTML" id="MO5"/></fig></p>
      <p>We deem the current data as <italic>d</italic>, and source datasets are <inline-formula id="IEq10"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$ \mathcal {D}_{1}, \mathcal {D}_{2} $\end{document}</tex-math><mml:math id="M44"><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_3000_Article_IEq10.gif"/></alternatives></inline-formula>. <italic>L</italic><sub><italic>task</italic></sub> is the task loss calculated by CRF layer. shown as Eq. <xref rid="Equ13" ref-type="">13</xref>. 
<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  L_{task} = \left\{ \begin{array}{lr} L_{task1}, &amp; d \in \mathcal{D}_{1}; \\ L_{task2}, &amp; d \in \mathcal{D}_{2}. \end{array} \right.  $$ \end{document}</tex-math><mml:math id="M46"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">task</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="" open="{" separators=""><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">task</mml:mtext><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">task</mml:mtext><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2019_3000_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula></p>
      <p><italic>L</italic><sub><italic>diff</italic></sub> is calculated by the output of shared Bi-LSTM and private Bi-LSTM. <italic>L</italic><sub><italic>diff</italic></sub> describes the similarity of these two output vectors. Minimizing <italic>L</italic><sub><italic>diff</italic></sub> encourages shared and private Bi-LSTM to extract different features of input. <italic>L</italic><sub><italic>diff</italic></sub> is calculated as Eq. <xref rid="Equ14" ref-type="">14</xref>: 
<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  L_{diff} = \sum_{k=1, 2}||{\boldsymbol{S}}^{\top}\boldsymbol{P}^{k}||_{F}^{2}  $$ \end{document}</tex-math><mml:math id="M48"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">diff</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><graphic xlink:href="12859_2019_3000_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <bold><italic>S</italic></bold> is the output of shared Bi-LSTM and <bold><italic>P</italic></bold><sup><italic>k</italic></sup> is the output of private Bi-LSTM of dataset <italic>k</italic>. <inline-formula id="IEq11"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$ ||\cdot ||_{F}^{2} $\end{document}</tex-math><mml:math id="M50"><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mo>·</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_3000_Article_IEq11.gif"/></alternatives></inline-formula> is the squared Frobenius norm.</p>
      <p><italic>L</italic><sub><italic>adv</italic></sub> is task adversarial loss. The shared Bi-LSTM can be regarded as generative model G which produce vector to hide the information of source dataset, and we use a discriminative model D to identify the source dataset against generative model G. Discriminative model D is shown as Eq. <xref rid="Equ15" ref-type="">15</xref>: 
<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  D(\boldsymbol{s}_{T}^{k}, \theta_{D}) = softmax\left(\boldsymbol{W}\boldsymbol{s}_{T}^{k} + \boldsymbol{b}\right)  $$ \end{document}</tex-math><mml:math id="M52"><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">softmax</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2019_3000_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <inline-formula id="IEq12"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$ \boldsymbol {s}_{T}^{k} $\end{document}</tex-math><mml:math id="M54"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_3000_Article_IEq12.gif"/></alternatives></inline-formula> is the output of shared Bi-LSTM of dataset <italic>k</italic> at time <italic>T</italic>. <bold><italic>W</italic></bold> and <bold><italic>b</italic></bold> are trainable parameters. And the adversarial loss function is: 
<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  L_{adv} = -\max_{\theta_{G}}\left(\min_{\theta_{D}}\left(\sum_{k=1}^{K} \boldsymbol{d}_{i}^{k} \log\left[D\left(E\left(\boldsymbol{x}^{k}\right)\right)\right]\right)\right)  $$ \end{document}</tex-math><mml:math id="M56"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">adv</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munder><mml:mrow><mml:mo>max</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:munder><mml:mrow><mml:mo>min</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>log</mml:mo><mml:mfenced close="]" open="[" separators=""><mml:mrow><mml:mi>D</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>E</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2019_3000_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>Discriminative model D is able to recognize source dataset by task-dependent features, and generative model G tends to keep common features to confuse discriminative model D; therefore, minimizing <italic>L</italic><sub><italic>adv</italic></sub> encourages shared Bi-LSTM to keep more shared features of two datasets.</p>
      <p>The final loss is the weighted sum of these three kinds of losses. 
<disp-formula id="Equ17"><label>17</label><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ L = L_{task} + \alpha L_{adv} + \beta L_{diff}  $$ \end{document}</tex-math><mml:math id="M58"><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">task</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">adv</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">diff</mml:mtext></mml:mrow></mml:msub></mml:math><graphic xlink:href="12859_2019_3000_Article_Equ17.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <italic>α</italic> and <italic>β</italic> are hyperparameters.</p>
      <p>Grid search can be used to find the optimized hyperparameters <italic>α</italic> and <italic>β</italic>. By using the gradient reversal layer [<xref ref-type="bibr" rid="CR29">29</xref>] before the discriminative model, the whole network can be trained with backpropagation.</p>
    </sec>
    <sec id="Sec10">
      <title>Multi-task model with cross-sharing structure (CS-MTM)</title>
      <p>In this section, we introduce our multi-task model with cross-sharing structure. This model captures features from both datasets and takes advantage of all the feature representations.</p>
      <p>As shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>, the word embeddings and character embeddings of the input sentence are first fed to the embedding layer. The structure of the embedding layer is the same as that in the baseline single-task model. The embedding layer captures the information in word embeddings and character embeddings. The output of the embedding layer is the word representations, which can be used in the Bi-LSTM layers.
<fig id="Fig6"><label>Fig. 6</label><caption><p>Cross-sharing Multi-task Model (CS-MTM). The embedding layer and shared Bi-LSTM are shared by two datasets. Gated interaction unit is used to adjust the output of private Bi-LSTMs. <italic>P</italic><sub>1</sub>,<italic>P</italic><sub>2</sub>: Output of private Bi-LSTMs. <italic>S</italic>: Output of the shared Bi-LSTM. <italic>G</italic><sub>1</sub>,<italic>G</italic><sub>2</sub>: Output of the gated interaction unit</p></caption><graphic xlink:href="12859_2019_3000_Fig6_HTML" id="MO6"/></fig></p>
      <p>After the embedding layer, the word representations are replicated as the input of shared Bi-LSTM and <bold>both</bold> private Bi-LSTMs. <bold><italic>P</italic></bold><sub>1</sub>,<bold><italic>P</italic></bold><sub>2</sub> denote the output of two private Bi-LSTMs. <bold><italic>S</italic></bold> denotes the output of shared Bi-LSTM. Intuitively, the private Bi-LSTMs are used to capture task-independent features; thus, <bold><italic>P</italic></bold><sub>1</sub>,<bold><italic>P</italic></bold><sub>2</sub> are the feature representations of dataset 1 and 2. The shared Bi-LSTM captures the common features from both datasets and <bold><italic>S</italic></bold> is the representation of common features.</p>
      <p>In our previous SP-MTM and ADV-MTM, either <bold><italic>P</italic></bold><sub>1</sub> or <bold><italic>P</italic></bold><sub>2</sub> is calculated depending on source dataset. In this way, only feature representation of source dataset is calculated, but the other feature representation which may still be useful is not calculated. In multi-task learning, using information from other datasets to improve the performance of origin dataset is the main idea, so both <bold><italic>P</italic></bold><sub>1</sub> and <bold><italic>P</italic></bold><sub>2</sub> are used in this model.</p>
      <p>The gated interaction unit then takes <bold><italic>P</italic></bold><sub>1</sub>,<bold><italic>P</italic></bold><sub>2</sub> as input and produces a mixed feature representation. <bold><italic>G</italic></bold><sub>1</sub>,<bold><italic>G</italic></bold><sub>2</sub> denote the output of gated interaction unit for two datasets. Eq. <xref rid="Equ18" ref-type="">18</xref> and (<xref rid="Equ19" ref-type="">19</xref>) show how gated interaction unit works. 
<disp-formula id="Equ18"><label>18</label><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l}  \boldsymbol{G}_{1} &amp;= \boldsymbol{P}_{1} \odot \sigma(\boldsymbol{W}_{2 \rightarrow 1}\boldsymbol{P}_{2} + \boldsymbol{b}_{2 \rightarrow 1}) \end{array} $$ \end{document}</tex-math><mml:math id="M60"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd class="align-2"><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>→</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>→</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mspace width="2em"/></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_3000_Article_Equ18.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>
        <disp-formula id="Equ19">
          <label>19</label>
          <alternatives>
            <tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} \boldsymbol{G}_{2} &amp;= \boldsymbol{P}_{2} \odot \sigma(\boldsymbol{W}_{1 \rightarrow 2}\boldsymbol{P}_{1} + \boldsymbol{b}_{1 \rightarrow 2}) \end{array} $$ \end{document}</tex-math>
            <mml:math id="M62">
              <mml:mtable class="align" columnalign="left">
                <mml:mtr>
                  <mml:mtd class="align-1">
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">G</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mtd>
                  <mml:mtd class="align-2">
                    <mml:mo>=</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>⊙</mml:mo>
                    <mml:mi>σ</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">W</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>1</mml:mn>
                        <mml:mo>→</mml:mo>
                        <mml:mn>2</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>+</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold-italic">b</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>1</mml:mn>
                        <mml:mo>→</mml:mo>
                        <mml:mn>2</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>)</mml:mo>
                    <mml:mspace width="2em"/>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
            <graphic xlink:href="12859_2019_3000_Article_Equ19.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>where ⊙ is element-wise multiplication, <italic>σ</italic> is a sigmoidal function, and <bold><italic>W</italic></bold><sub>1→2</sub>,<bold><italic>W</italic></bold><sub>2→1</sub>,<bold><italic>b</italic></bold><sub>1→2</sub>,<bold><italic>b</italic></bold><sub>2→1</sub> are trainable parameters.</p>
      <p>We deem the current data as <italic>d</italic>, and source datasets are <inline-formula id="IEq13"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$ \mathcal {D}_{1}, \mathcal {D}_{2} $\end{document}</tex-math><mml:math id="M64"><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_3000_Article_IEq13.gif"/></alternatives></inline-formula>. The final output of gated interaction unit <italic>G</italic> is determined by the source dataset, shown as Eq. <xref rid="Equ20" ref-type="">20</xref>. 
<disp-formula id="Equ20"><label>20</label><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  \boldsymbol{G}=\left\{ \begin{array}{lr} \boldsymbol{G}_{1}, &amp; d \in \mathcal{D}_{1}; \\ \boldsymbol{G}_{2}, &amp; d \in \mathcal{D}_{2}. \end{array} \right.  $$ \end{document}</tex-math><mml:math id="M66"><mml:mi mathvariant="bold-italic">G</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="" open="{" separators=""><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2019_3000_Article_Equ20.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>In the gated interaction unit, two private feature representations <bold><italic>P</italic></bold><sub>1</sub>,<bold><italic>P</italic></bold><sub>2</sub> share feature information with each other. When training, four trainable parameters are adjusting to learning what to share between two representations. For dataset 1, <bold><italic>P</italic></bold><sub>2</sub> contains the information of features from dataset 2, and these features are task-independent and cannot be used directly to improve the performance of dataset 1; otherwise, these features should be captured by shared Bi-LSTM. The operation in gated interaction unit provides an indirect way to make use of the information in <bold><italic>P</italic></bold><sub>2</sub>. In this way, both feature representations <bold><italic>P</italic></bold><sub>1</sub>,<bold><italic>P</italic></bold><sub>2</sub> are used to produce a new mixed feature representation.</p>
      <p>Before the CRF layer, output vectors of gated interaction unit and shared Bi-LSTM are concatenated, shown as Eq. <xref rid="Equ21" ref-type="">21</xref>. 
<disp-formula id="Equ21"><label>21</label><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  \boldsymbol{V} = \boldsymbol{G} \oplus \boldsymbol{S}  $$ \end{document}</tex-math><mml:math id="M68"><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">G</mml:mi><mml:mo>⊕</mml:mo><mml:mi mathvariant="bold-italic">S</mml:mi></mml:math><graphic xlink:href="12859_2019_3000_Article_Equ21.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>In this way, information of shared feature representation and private feature representation is combined and fed to the CRF layer. CRF layer produces predicted tags based on <bold><italic>V</italic></bold>.</p>
    </sec>
  </sec>
  <sec id="Sec11">
    <title>Experiment settings</title>
    <p>In this section, we introduce our datasets, evaluation metrics, and training details.</p>
    <sec id="Sec12">
      <title>Datasets</title>
      <p>We conduct experiments on several BioNER datasets from Crichton et al. [<xref ref-type="bibr" rid="CR23">23</xref>]. The detailed information about the datasets used in our experiments is listed in Table <xref rid="Tab1" ref-type="table">1</xref>. We use datasets with IOB format. These datasets are available to the open, and you can access <ext-link ext-link-type="uri" xlink:href="https://github.com/cambridgeltl/MTL-Bioinformatics-2016">https://github.com/cambridgeltl/MTL-Bioinformatics-2016</ext-link> to get these datasets.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Biomedical NER datasets used in the experiments</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Size</th><th align="left">Entity types &amp; counts</th></tr></thead><tbody><tr><td align="left">BC2GM</td><td align="left">20,131 sentences</td><td align="justify">Gene (24,583)</td></tr><tr><td align="left">Ex-PTM</td><td align="left">3,653 sentences</td><td align="justify">Protein (4,698)</td></tr><tr><td align="left">NCBI-disease</td><td align="left">7,287 sentences</td><td align="justify">Disease (6,881)</td></tr><tr><td align="left">Linnaeus</td><td align="left">23,155 sentences</td><td align="justify">Species (4,263)</td></tr><tr><td align="left">JNLPBA</td><td align="left">24,806 sentences</td><td align="justify">Cell (12,969), Gene (10,589), Protein (35,336)</td></tr><tr><td align="left">BC5CDR</td><td align="left">13,938 sentences</td><td align="justify">Chemical (15,935), Disease (12,852)</td></tr><tr><td align="left">BioNLP09</td><td align="left">11,356 sentences</td><td align="justify">Protein (14,963)</td></tr><tr><td align="left">BioNLP11ID</td><td align="left">5,178 sentences</td><td align="justify">Chemical (973), Protein (6,551), Species (3,471)</td></tr><tr><td align="left">BioNLP13PC</td><td align="left">5,051 sentences</td><td align="justify">Cell (1,013), Chemical (3,989), Gene (10,891)</td></tr></tbody></table></table-wrap></p>
      <p>As these datasets use various BioNER tags to mark out entities, we divide them into six categories: Cell, Chemical, Disease, Gene, Protein and Species. For the entity types column in Table <xref rid="Tab1" ref-type="table">1</xref>, BioNER tags are counted according to these six categories. In different datasets, BioNER tags belonging to the same category may vary. For example, in Gene categories, B-GENE/I-GENE tags are used in BC2GM dataset, while B-DNA/I-DNA are in JNLPBA dataset. In our experiments, tags are kept as they are rather than changed to be the same.</p>
      <p>In our multi-task models, two datasets are used in the training procedure. We focus on one dataset and try to optimize the performance on it. This dataset is called the main dataset, and the other is called the auxiliary dataset. By observation, we find that some datasets contain entities from just one category, while some others from multiple categories. In order to diminish the influence between different entity categories, we prefer datasets which contain entities from one category to be main datasets. In our experiments, BC2GM, Ex-PTM, NCBI-disease, and Linnaeus are chosen as main datasets, and auxiliary datasets are picked from JNLPBA, BioNLP09, BioNLP11ID, BioNLP13PC, and BC5CDR. The performance of the main datasets is recorded in experimental results.</p>
    </sec>
    <sec id="Sec13">
      <title>Evaluation metrics</title>
      <p>We use the training set and the development set to train the model, and report the performance on the test set. We deem each predicted tag is correct only if it is the same as the ground-truth tag. We calculate macro-averaged precision, recall, F1 scores of main dataset, and these scores are recorded as final dataset scores.</p>
    </sec>
    <sec id="Sec14">
      <title>Training details</title>
      <p><bold>Word embeddings</bold> We use pre-trained word vectors of GloVe model, and the pre-trained corpus is Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab). The dimension of word vectors is 100.</p>
      <p><bold>Character embeddings</bold> The dimension of character embeddings <italic>d</italic><sub><italic>c</italic></sub> is 30. Number of filters in CNN is 30. <italic>l</italic><sub><italic>f</italic></sub> in the CNN is set to 3.</p>
      <p><bold>Bi-LSTM layers</bold> Bi-LSTM in our model uses the same hyperparameters, including Bi-LSTM in baseline single-task model, and shared/private Bi-LSTM in multi-task models. We set the dimension of hidden vectors to 256. For Bi-LSTM layers in all of our models, we use a linear unit to reshape hidden vectors to 128-dimensional vector as output. The dropout rate of all the dropout layers is 0.5.</p>
      <p><bold>CRF layers</bold> We use Linear-chain CRF to train and test. The Viterbi algorithm is used in the training procedure.</p>
      <p><bold>Training settings</bold> Our training procedure contains 80 epochs. Parameter optimization is performed with RMSprop. The decay rate of RMSProp is set to 0.95, and momentum is set to 0. Batch size is set to 16. Learning rate is 0.001 at initial, and decay at the end of every epoch at the rate of 3%. Besides, We use gradient clipping to limit max L2 norm of the gradients to 5.0 in order to avoid gradient exploding.</p>
      <p><bold>MTM training</bold> When performing multi-task training, batches of data from 2 datasets train in turns. To be specific, a batch of data from dataset 1 is used to train, then a batch of data from dataset 2 is used to train, this procedure is regarded as a turn. In one turn, two batches of data are randomly picked from their source datasets. In an epoch, the number of turns is set to the number of batches in the main dataset. In this case, we ensure the main dataset to be fully trained.</p>
      <p><bold>Grid search</bold> For the adversarial multi-task model, grid search is used to find the hyperparameters <italic>α</italic> and <italic>β</italic>. We try out <italic>α</italic> from {0, 0.1, 0.01}, and <italic>β</italic> from {0, 0.01, 0.001}. An extra gradient reverse layer is applied before the task discriminator unit in order to train the model with gradient descent.</p>
    </sec>
  </sec>
  <sec id="Sec15" sec-type="results">
    <title>Results</title>
    <p>In this section, we display and analyze the experiment results, and compare our proposed model with related ones.</p>
    <sec id="Sec16">
      <title>Performance comparison</title>
      <p>We compare the baseline single-task model (STM) and other multi-task models (MTM). The results are shown in Table <xref rid="Tab2" ref-type="table">2</xref>. It shows the performance (precision, recall, F1) of different models on four target datasets. The bold number in one row indicates the best F1 score for the dataset.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Model Performance Comparison</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left"/><th align="left">Baseline Single-task Model (STM)</th><th align="left">Fully-shared Multi-task Model (FS-MTM)</th><th align="left">Shared-private Multi-task Model (SP-MTM)</th><th align="left">Adversarial Multi-task Model (ADV-MTM)</th><th align="left">Cross-sharing Multi-task Model (CS-MTM)</th></tr></thead><tbody><tr><td align="justify">BC2GM</td><td align="justify">Precision</td><td align="justify">84.00</td><td align="justify">83.34</td><td align="justify">84.51</td><td align="justify">83.66</td><td align="justify">83.12</td></tr><tr><td align="justify"/><td align="justify">Recall</td><td align="justify">83.82</td><td align="justify">84.75</td><td align="justify">84.17</td><td align="justify">84.05</td><td align="justify">85.74</td></tr><tr><td align="justify"/><td align="justify">F1</td><td align="justify">83.91</td><td align="justify">84.04</td><td align="justify">84.34</td><td align="justify">83.85</td><td align="justify"><bold>84.41</bold></td></tr><tr><td align="justify">Ex-PTM</td><td align="justify">Precision</td><td align="justify">70.83</td><td align="justify">72.56</td><td align="justify">70.45</td><td align="justify">76.60</td><td align="justify">74.73</td></tr><tr><td align="justify"/><td align="justify">Recall</td><td align="justify">64.12</td><td align="justify">70.46</td><td align="justify">70.03</td><td align="justify">67.43</td><td align="justify">69.56</td></tr><tr><td align="justify"/><td align="justify">F1</td><td align="justify">67.31</td><td align="justify">71.49</td><td align="justify">70.24</td><td align="justify">71.72</td><td align="justify"><bold>72.05</bold></td></tr><tr><td align="justify">NCBI-disease</td><td align="justify">Precision</td><td align="justify">88.45</td><td align="justify">84.39</td><td align="justify">87.11</td><td align="justify">86.02</td><td align="justify">86.59</td></tr><tr><td align="justify"/><td align="justify">Recall</td><td align="justify">83.78</td><td align="justify">86.61</td><td align="justify">85.49</td><td align="justify">86.86</td><td align="justify">86.42</td></tr><tr><td align="justify"/><td align="justify">F1</td><td align="justify">86.05</td><td align="justify">85.49</td><td align="justify">86.29</td><td align="justify">86.44</td><td align="justify"><bold>86.50</bold></td></tr><tr><td align="justify">Linnaeus</td><td align="justify">Precision</td><td align="justify">92.86</td><td align="justify">92.66</td><td align="justify">93.00</td><td align="justify">93.74</td><td align="justify">89.81</td></tr><tr><td align="justify"/><td align="justify">Recall</td><td align="justify">67.62</td><td align="justify">66.76</td><td align="justify">73.86</td><td align="justify">73.81</td><td align="justify">76.12</td></tr><tr><td align="justify"/><td align="justify">F1</td><td align="justify">78.25</td><td align="justify">77.60</td><td align="justify">82.33</td><td align="justify"><bold>82.59</bold></td><td align="justify">82.40</td></tr></tbody></table><table-wrap-foot><p>Bold: the best F1 score for the dataset</p></table-wrap-foot></table-wrap></p>
      <p>FS-MTM achieves better performance than STM on BC2GM and Ex-PTM datasets but degrades on other two datasets. FS-MTM uses the most basic multi-task structure, and the only shared Bi-LSTM may not separate task-specific features for each task.</p>
      <p>SP-MTM improves the performance comparing to FS-MTM and STM, also achieves higher F1 score than baseline STM on all of four main datasets. Intuitively, the private Bi-LSTMs are added and capable of capturing task-specific features.</p>
      <p>We observe that both the ADV-MTM and CS-MTM improve the performance of STM, and especially CS-MTM achieves higher F1 score than baseline STM on all of four datasets. On BC2GM dataset, improvements of ADV-MTM are marginal compared with STM. Besides, CS-MTM outperforms ADV-MTM in F1 score on BC2GM, Ex-PTM, and NCBI-disease datasets. Comparing the structure of ADV-MTM and CS-MTM to SP-MTM, it indicates that the adversarial loss calculation and cross-sharing structure could help to improve the performance.</p>
      <p>According to the precision and recall score of datasets, CS-MTM tends to produce a higher recall score, and ADV-MTM tends to improve the precision score. Intuitively, minimizing the adversarial loss in ADV-MTM helps to separate shared features and task-specific features and reduce the number of false positives. Unlike ADV-MTM, gated interaction unit in CS-MTM makes use of both feature representations, resulting in less number of false negatives.</p>
      <p>When training, we find that the performance of ADV-MTM is not very stable, and the adversarial model uses more epochs to converge. This model has limited performance improvement comparing to SP-MTM and exposes the weakness of GAN.</p>
      <p>We list the trainable parameter number of each model in Table <xref rid="Tab3" ref-type="table">3</xref>. In the table, the parameter numbers of STM and FS-MTM are close, and SP-MTM, ADV-MTM, CS-MTM have more parameters. We can conclude that the gated interaction unit in CS-MTM has only a few parameters but improves the overall performance. It suggests that our performance improvement is not just based on the increase in the huge amount of parameters.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Parameter numbers of all models</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Number</th></tr></thead><tbody><tr><td align="left">STM</td><td align="left">3.68M</td></tr><tr><td align="left">FS-MTM</td><td align="left">3.68M</td></tr><tr><td align="left">SP-MTM</td><td align="left">5.41M</td></tr><tr><td align="left">ADV-MTM</td><td align="left">5.41M</td></tr><tr><td align="left">CS-MTM</td><td align="left">5.44M</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec17">
      <title>Performance with different auxiliary datasets</title>
      <p>Different dataset pairs could produce different results in multi-task learning. We try out all the combinations of one main dataset and one auxiliary dataset. The results are shown in Table <xref rid="Tab4" ref-type="table">4</xref>. The numbers in the table are the F1 scores of dataset pairs. BC2GM, Ex-PTM, NCBI-disease, and Linnaeus are the main dataset that we focus on. The bold number in one row indicates the best F1 score for the dataset. The <italic>↑</italic> / <italic>↓</italic> indicates the positive/negative improvement comparing to STM.
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Performance with different auxiliary datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">JNLPBA</th><th align="left">BC5CDR</th><th align="left">BioNLP 09</th><th align="left">BioNLP 11ID</th><th align="left">BioNLP 13PC</th></tr></thead><tbody><tr><td align="left">BC2GM</td><td align="left"><bold>84.41</bold><italic>↑</italic></td><td align="left">84.11 <italic>↑</italic></td><td align="left">83.85</td><td align="left">84.15 <italic>↑</italic></td><td align="left">83.90</td></tr><tr><td align="left">Ex-PTM</td><td align="left">68.81 <italic>↑</italic></td><td align="left">67.51 <italic>↑</italic></td><td align="left"><bold>72.05</bold><italic>↑</italic></td><td align="left">68.89 <italic>↑</italic></td><td align="left">70.87 <italic>↑</italic></td></tr><tr><td align="left">NCBI-disease</td><td align="left">86.17 <italic>↑</italic></td><td align="left">85.74 <italic>↓</italic></td><td align="left"><bold>86.50</bold><italic>↑</italic></td><td align="left">84.90 <italic>↓</italic></td><td align="left">85.63 <italic>↓</italic></td></tr><tr><td align="left">Linnaeus</td><td align="left">78.07 <italic>↓</italic></td><td align="left"><bold>82.40</bold><italic>↑</italic></td><td align="left">81.93 <italic>↑</italic></td><td align="left">78.46 <italic>↑</italic></td><td align="left">78.37 <italic>↓</italic></td></tr></tbody></table><table-wrap-foot><p>Bold: the best F1 score for the dataset. <italic>↑</italic> / <italic>↓</italic>: positive / negative improvement comparing to STM</p></table-wrap-foot></table-wrap></p>
      <p>From experiment results, JNLPBA is the best partner for BC2GM, and BC5CDR, BioNLP09 are helpful to BC2GM. All these five auxiliary datasets are helpful to Ex-PTM, but the best partner of Ex-PTM is BioNLP09. As for NCBI-disease, BioNLP09 is the best partner, and JNLPBA is also helpful. Auxiliary datasets except JNLPBA and BioNLP13PC are helpful to Linnaeus, and BC5CDR improves its performance significantly.</p>
      <p>In auxiliary datasets, JNLPBA is of the biggest size, and BioNLP13PC is the smallest. Using JNLPBA as the auxiliary dataset still degrades on Linnaeus dataset, while using BioNLP13PC as the auxiliary dataset in this experiment improves the performance on Ex-PTM. For these five auxiliary datasets, we cannot observe a tendency of performance increasing or decreasing with the size of dataset changing. This phenomenon indicates that the size of the dataset is not the major factor of performance. If auxiliary dataset lacks beneficial information for the main dataset, the performance of multi-task model would be unfavorable.</p>
      <p>BC2GM contains gene tags, and its best partner JNLPBA also contains gene tags. The situation is similar for Ex-PTM and BioNLP09. It could indicate that the dataset pair could work if the auxiliary dataset contains the categories of tags that main dataset also has. But for Linnaeus and its best partner BC5CDR, although they share no same categories of tags, BC5CDR can still provide biomedical information of other categories which is helpful to Linnaeus.</p>
      <p>In conclusion, there is no simple rule to find the best partner, the most accurate way is to try out all the combinations.</p>
    </sec>
    <sec id="Sec18">
      <title>Performance with different entity types in auxiliary datasets</title>
      <p>In our five auxiliary datasets, some of them contain multiple categories of tags. In order to discover which category of tags is the major factor of performance, we use sub-datasets to perform the experiments. The BC5CDR, BioNLP11ID, BioNLP13PC datasets provide sub-datasets that contain the single category of tags. In this experiments, We choose our four main datasets and BioNLP11ID-chem (Chemical), BioNLP11ID-ggp (Protein), BioNLP11ID-species (Species) as auxiliary datasets. This experiment aims to check which category of tags is the most important for main datasets in CS-MTM. The results are shown in Table <xref rid="Tab5" ref-type="table">5</xref>. The <italic>↑</italic> / <italic>↓</italic> indicates the positive/negative improvement comparing to STM.
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Performance with different entity types in BioNLP11ID</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">BioNLP11 ID</th><th align="left">BioNLP11 ID-chem</th><th align="left">BioNLP11 ID-ggp</th><th align="left">BioNLP11 ID-species</th></tr></thead><tbody><tr><td align="left">BC2GM</td><td align="justify">84.15 <italic>↑</italic></td><td align="justify"><bold>84.39</bold><italic>↑</italic></td><td align="justify">84.01</td><td align="justify">83.45 <italic>↓</italic></td></tr><tr><td align="left">Ex-PTM</td><td align="justify">68.89 <italic>↑</italic></td><td align="justify">67.51 <italic>↑</italic></td><td align="justify"><bold>68.80</bold><italic>↑</italic></td><td align="justify">67.58 <italic>↑</italic></td></tr><tr><td align="left">NCBI-disease</td><td align="justify">84.90 <italic>↓</italic></td><td align="justify"><bold>85.44</bold><italic>↓</italic></td><td align="justify">85.26 <italic>↓</italic></td><td align="justify">85.24 <italic>↓</italic></td></tr><tr><td align="left">Linnaeus</td><td align="justify">78.46 <italic>↑</italic></td><td align="justify">72.09 <italic>↓</italic></td><td align="justify">73.21 <italic>↓</italic></td><td align="justify"><bold>76.88</bold><italic>↓</italic></td></tr></tbody></table><table-wrap-foot><p>Bold: the best F1 score between sub-datasets. <italic>↑</italic>/ <italic>↓</italic>: positive / negative improvement comparing to STM</p></table-wrap-foot></table-wrap></p>
      <p>Ex-PTM dataset contains tags of protein category, and its best partner BioNLP11ID-ggp also contains that category of tags. Besides, as for Linnaeus and BioNLP11ID-species, these two datasets are the best pair and both contain tags of species category. It indicates that protein tags and species tags are the major factors for Ex-PTM and Linnaeus datasets, respectively, when BioNLP11ID as the auxiliary dataset. As for other tags, chemical and species tags in the BioNLP11ID dataset are hardly helpful to Ex-PTM dataset, while chemical and protein tags would make the performance of Linnaeus ever worse.</p>
      <p>BC2GM and NCBI-disease datasets contain no tags of chemical, protein and species categories. In experiment results, we could observe that chemical and protein tags in BioNLP11ID dataset are helpful to BC2GM while species tags are harmful. For NCBI-disease dataset, all categories of tags make performance worse.</p>
      <p>When a dataset contains multiple categories of tags, mutual influences may exist between them. For BC2GM datasets, chemical tags improve performance and species tags reduce performance, but the result of all tags is still positive. It indicates that categories of tags with the opposite effect would neutralize each other. For Ex-PTM dataset, all the categories of tags improve performance, and the performance of all tags is better than a single category of tags. Similarly, for NCBI-disease dataset, the merged result is worse. It demonstrates that the categories of tags with the same effect could cooperate and accumulate their effects. Exceptionally, for Linnaeus dataset, categories of tags are all negative, but the result of all tags is positive. We don’t have an analysis to explain this phenomenon if just base on the current experiment result, but it suggests that the influence of different categories of tags is not a simple linear calculation.</p>
    </sec>
    <sec id="Sec19">
      <title>Impact of dataset size</title>
      <p>In this part, we discover the performance of CS-MTM on smaller datasets. Using reduced-size main datasets, we record the performance (precision, recall, F1) of different situations. The results of CS-MTM in this experiment are produced using the best pairs in “<xref rid="Sec17" ref-type="sec">Performance with different auxiliary datasets</xref>” section. The reduced-size datasets are produced by removing sentences in training sets randomly, and the development sets and test sets are not modified. To compare with, we also use the reduced-size dataset on baseline single-task model. The results are shown in Table <xref rid="Tab6" ref-type="table">6</xref>. The better F1 scores for each training set size are bold.
<table-wrap id="Tab6"><label>Table 6</label><caption><p>Impact of dataset size</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left"/><th align="left">Full-size STM</th><th align="left">Full-size CS-MTM</th><th align="left">50%-size STM</th><th align="left">50%-size CS-MTM</th><th align="left">25%-size STM</th><th align="left">25%-size CS-MTM</th><th align="left">10%-size STM</th><th align="left">10%-size CS-MTM</th></tr></thead><tbody><tr><td align="justify">BC2GM</td><td align="left">Precision</td><td align="justify">84.00</td><td align="justify">83.12</td><td align="justify">82.37</td><td align="justify">79.37</td><td align="justify">77.82</td><td align="justify">79.44</td><td align="justify">73.19</td><td align="justify">72.95</td></tr><tr><td align="justify"/><td align="left">Recall</td><td align="justify">83.82</td><td align="justify">85.74</td><td align="justify">80.77</td><td align="justify">85.05</td><td align="justify">79.57</td><td align="justify">78.98</td><td align="justify">73.59</td><td align="justify">75.39</td></tr><tr><td align="justify"/><td align="left">F1</td><td align="justify">83.91</td><td align="justify"><bold>84.41</bold></td><td align="justify">81.56</td><td align="justify"><bold>82.12</bold></td><td align="justify">78.69</td><td align="justify"><bold>79.21</bold></td><td align="justify">73.39</td><td align="justify"><bold>74.15</bold></td></tr><tr><td align="justify">Ex-PTM</td><td align="left">Precision</td><td align="justify">70.83</td><td align="justify">74.73</td><td align="justify">67.74</td><td align="justify">68.18</td><td align="justify">57.46</td><td align="justify">54.00</td><td align="justify">42.47</td><td align="justify">50.69</td></tr><tr><td align="justify"/><td align="left">Recall</td><td align="justify">64.12</td><td align="justify">69.56</td><td align="justify">58.62</td><td align="justify">67.48</td><td align="justify">53.69</td><td align="justify">63.97</td><td align="justify">50.27</td><td align="justify">41.68</td></tr><tr><td align="justify"/><td align="left">F1</td><td align="justify">67.31</td><td align="justify"><bold>72.05</bold></td><td align="justify">62.85</td><td align="justify"><bold>67.83</bold></td><td align="justify">55.51</td><td align="justify"><bold>58.56</bold></td><td align="justify"><bold>46.04</bold></td><td align="justify">45.75</td></tr><tr><td align="justify">NCBI-disease</td><td align="left">Precision</td><td align="justify">88.45</td><td align="justify">86.59</td><td align="justify">84.03</td><td align="justify">84.72</td><td align="justify">81.52</td><td align="justify">81.00</td><td align="justify">81.02</td><td align="justify">79.32</td></tr><tr><td align="justify"/><td align="left">Recall</td><td align="justify">83.78</td><td align="justify">86.42</td><td align="justify">84.56</td><td align="justify">84.76</td><td align="justify">76.50</td><td align="justify">81.00</td><td align="justify">68.59</td><td align="justify">74.40</td></tr><tr><td align="justify"/><td align="left">F1</td><td align="justify">86.05</td><td align="justify"><bold>86.50</bold></td><td align="justify">84.30</td><td align="justify"><bold>84.74</bold></td><td align="justify">78.93</td><td align="justify"><bold>81.00</bold></td><td align="justify">74.29</td><td align="justify"><bold>76.78</bold></td></tr><tr><td align="justify">Linnaeus</td><td align="left">Precision</td><td align="justify">92.86</td><td align="justify">89.81</td><td align="justify">91.77</td><td align="justify">88.92</td><td align="justify">89.90</td><td align="justify">90.20</td><td align="justify">90.80</td><td align="justify">85.98</td></tr><tr><td align="justify"/><td align="left">Recall</td><td align="justify">67.62</td><td align="justify">76.12</td><td align="justify">68.11</td><td align="justify">72.95</td><td align="justify">67.62</td><td align="justify">68.29</td><td align="justify">52.65</td><td align="justify">51.33</td></tr><tr><td align="justify"/><td align="left">F1</td><td align="justify">78.25</td><td align="justify"><bold>82.40</bold></td><td align="justify">78.19</td><td align="justify"><bold>80.15</bold></td><td align="justify">77.18</td><td align="justify"><bold>77.73</bold></td><td align="justify"><bold>66.65</bold></td><td align="justify">64.29</td></tr></tbody></table><table-wrap-foot><p>Bold: the better F1 scores between STM and CS-MTM for each dataset size</p></table-wrap-foot></table-wrap></p>
      <p>For STM and CS-MTM, the F1 score decreases when the size of training data is limited. When the training set is reduced and the test set is kept, the missing of information in removed sentences makes the model produce worse results. In CS-MTM, the missing information could be found in auxiliary datasets, so CS-MTM could improve the performance back if a suitable auxiliary dataset is chosen.</p>
      <p>For 50%-size and 25%-size datasets, CS-MTM outperforms STM on F1 score by providing a higher recall score. But for 10%-size datasets, CS-MTM outperforms STM on BC2GM and NCBI-disease datasets and degrades on Ex-PTM and Linnaeus datasets. In this case, our CS-MTM may not learn missing information from auxiliary dataset well.</p>
    </sec>
    <sec id="Sec20">
      <title>Performance with different word embeddings</title>
      <p>In this part, we discover the performance of STM and CS-MTM by using different pre-trained word embeddings. In our previous experiments, we just use the pre-trained GloVe to produce our word embeddings. Our CS-MTM model may have better performance when using other word embeddings. In this experiment, we obtain the performance with several different pre-trained Word2Vec and compare them with the performance with the original pre-trained GloVe. The results are shown in Table <xref rid="Tab7" ref-type="table">7</xref>. The best F1 scores for the model on each dataset are bold.
<table-wrap id="Tab7"><label>Table 7</label><caption><p>Performance with different word embeddings</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left" colspan="4">STM</th><th align="left" colspan="4">CS-MTM</th></tr><tr><th align="left"/><th align="left">BC2GM</th><th align="left">Ex-PTM</th><th align="left">NCBI-disease</th><th align="left">Linnaeus</th><th align="left">BC2GM</th><th align="left">Ex-PTM</th><th align="left">NCBI-disease</th><th align="left">Linnaeus</th></tr></thead><tbody><tr><td align="left">PMC</td><td align="left">84.22</td><td align="left">66.09</td><td align="left">85.24</td><td align="left">76.87</td><td align="left">85.07</td><td align="left">70.61</td><td align="left">84.32</td><td align="left">80.00</td></tr><tr><td align="left">PubMed</td><td align="left">84.15</td><td align="left">66.86</td><td align="left">85.21</td><td align="left">71.23</td><td align="left">83.84</td><td align="left">70.66</td><td align="left">84.99</td><td align="left">74.63</td></tr><tr><td align="left">PMC+PubMed</td><td align="left">84.35</td><td align="left">66.57</td><td align="left">84.39</td><td align="left">75.07</td><td align="left"><bold>85.18</bold></td><td align="left">72.03</td><td align="left">85.34</td><td align="left">76.71</td></tr><tr><td align="left">PMC+PubMed +Wikipedia</td><td align="left"><bold>84.71</bold></td><td align="left">65.71</td><td align="left">84.46</td><td align="left">76.87</td><td align="left">84.10</td><td align="left">71.79</td><td align="left">85.27</td><td align="left">78.99</td></tr><tr><td align="left">Our GloVe</td><td align="left">83.91</td><td align="left"><bold>67.31</bold></td><td align="left"><bold>86.05</bold></td><td align="left"><bold>78.25</bold></td><td align="left">84.41</td><td align="left"><bold>72.05</bold></td><td align="left"><bold>86.50</bold></td><td align="left"><bold>82.40</bold></td></tr></tbody></table><table-wrap-foot><p>Bold: the best F1 scores for the model on each dataset</p></table-wrap-foot></table-wrap></p>
      <p>Four pre-trained Word2Vec word embeddings are used in this experiment. One trains with PMC corpus, one trains with PubMed corpus, one trains with PMC + PubMed corpora, one trains with PMC + PubMed + Wikipedia corpora. These pre-trained Word2Vec word embeddings are available at <ext-link ext-link-type="uri" xlink:href="http://bio.nlplab.org/">http://bio.nlplab.org/</ext-link>. They report that Word2Vec was run using the skip-gram model with a window size of 5, hierarchical softmax training, and a frequent word subsampling threshold of 0.001 to create 200-dimensional vectors.</p>
      <p>For STM, we have the best performance on BC2GM dataset when choosing PMC + PubMed + Wikipedia word embedding, and the best performance on the other three datasets is achieved by our original GloVe word embedding. For CS-MTM, PMC + PubMed on BC2GM and other three datasets on GloVe word embedding can produce the best performance. This phenomenon shows that different word embeddings can produce discrepant performance.</p>
      <p>Our GloVe word embedding achieves good performance on three datasets, but the coverage of Glove might be relatively small because it is not trained with the biomedical corpus. An important reason is that CNN in the embedding layer builds character embeddings to compensate for the missing of words. Besides, according to the overall performance, GloVe embeddings work better with our models than Word2Vec embeddings. But on certain datasets, such as BC2GM, character embeddings may not work well, and using word embedding which trains with specialized corpus can improve the performance.</p>
    </sec>
    <sec id="Sec21">
      <title>Case study</title>
      <p>In this part, we use some examples from datasets to illustrate the effect of the multi-task model. The examples are shown in Table <xref rid="Tab8" ref-type="table">8</xref>.
<table-wrap id="Tab8"><label>Table 8</label><caption><p>Case Study: Bold text: ground-truth entity; Underlined text: model prediction</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" colspan="3">Main dataset: Ex-PTM Auxiliary dataset: BioNLP09</td></tr><tr><td align="left">Case 1</td><td align="left">STM</td><td align="left">The myristoylation of <bold>Nef</bold> and its membrane localization were essential for these effects.</td></tr><tr><td align="left"/><td align="left">CS-MTM</td><td align="left">The myristoylation of <bold><underline>Nef</underline></bold> and its membrane localization were essential for these effects.</td></tr><tr><td align="left"/><td align="left">Auxiliary data</td><td align="left">Human immunodeficiency virus type 1 <bold>Nef</bold> protein inhibits NF-kappa B induction in human T cells.</td></tr><tr><td align="left">Description</td><td align="left" colspan="2">The training data of auxiliary dataset directly provides entity information about Nef protein.</td></tr><tr><td align="left" colspan="3">Main dataset: Ex-PTM Auxiliary dataset: BioNLP09</td></tr><tr><td align="left">Case 2</td><td align="left">STM</td><td align="left"><underline>Vitamin K</underline> deficiency is a relatively common condition in neonates.</td></tr><tr><td align="left"/><td align="left">CS-MTM</td><td align="left">Vitamin K deficiency is a relatively common condition in neonates.</td></tr><tr><td align="left"/><td align="left">Auxiliary data</td><td align="left">Ascorbic acid (ascorbate or vitamin C) has been shown to suppress the induction of HIV in...</td></tr><tr><td align="left"/><td align="left"/><td align="left">In conclusion, we demonstrate that the vitamin E derivative TCP succinate prevents monocytic...</td></tr><tr><td align="left">Description</td><td align="left" colspan="2">The training data of auxiliary dataset indirectly provides information that Vitamin is not protein.</td></tr><tr><td align="left" colspan="3">Main dataset: Linnaeus Auxiliary dataset: BC5CDR</td></tr><tr><td align="left">Case 3</td><td align="left">STM</td><td align="left">He <underline>slept</underline> well at night, <underline>ate</underline> more than his mother thought was good for him, and was able to...</td></tr><tr><td align="left"/><td align="left">CS-MTM</td><td align="left">He slept well at night, ate more than his mother thought was good for him, and was able to...</td></tr><tr><td align="left"/><td align="left">Auxiliary data</td><td align="left">During the night <bold>clomipramine</bold> ingestion altered the complete sleep architecture in that it suppressed REM sleep and the sleep cycles and induced increased wakefulness.</td></tr><tr><td align="left">Description</td><td align="left" colspan="2">The training data of auxiliary dataset directly provides information that sleep don’t belong to species.</td></tr></tbody></table></table-wrap></p>
      <p>Case 1 and 2 are picked from the test set of Ex-PTM. The main dataset, Ex-PTM, and the auxiliary dataset, BioNLP09, only have entity tags of protein category. In case 1, STM cannot recognize the entity Nef but CS-MTM can find it out, because the training data of auxiliary dataset directly provides entity information about Nef protein. In case 2, STM recognizes Vitamin K as a protein entity, which is incorrect. For the CS-MTM, in the training data of auxiliary dataset, there is no information about Vitamin K, but other Vitamins, such as Vitamin C and Vitamin E, appear in the dataset. The character embedding in the model can capture the morphological information; therefore, the multi-task model can recognize these Vitamins as non-protein entities.</p>
      <p>Case 3 is picked from the test set of Linnaeus. Linnaeus contains entity tags of species category, but the auxiliary dataset, BC5CDR, have no species entity tags. In case 3, STM recognizes <italic>s</italic><italic>l</italic><italic>e</italic><italic>p</italic><italic>t</italic> as a species entity. Because our model use no pre-defined feature, such as Part-of-Speech feature, STM may not learn that <italic>s</italic><italic>l</italic><italic>e</italic><italic>p</italic><italic>t</italic> is not an entity if there are few appearances of this word. For the CS-MTM, it can learn from auxiliary training data which exists the information of <italic>sleep</italic>; therefore, CS-MTM can recognize it as a non-species entity.</p>
    </sec>
  </sec>
  <sec id="Sec22" sec-type="discussion">
    <title>Discussion</title>
    <p>In this part, we compare our models with other BioNER models as well as the state-of-the-art models.</p>
    <p>For the multi-task model from Crichton et al. [<xref ref-type="bibr" rid="CR23">23</xref>], they experiment with many BioNER datasets. They report their best model achieves the F1 of 73.17% on BC2GM, 74.90% on Ex-PTM, 80.37% on NCBI-disease, and 84.04% on Linnaeus. Our model has better performance on BC2GM and NCBI-disease datasets, because both word embedding and character embedding are used as input in our model, while only word embedding is used in their model. In Crichton’s work, many more combinations of datasets are tried in the experiment, so this could be the reason why they have better performance on Ex-PTM and Linnaeus.</p>
    <p>For the multi-task model from Wang et al. [<xref ref-type="bibr" rid="CR19">19</xref>], they achieve the F1 of 83.14% on BC2GM and 86.37% on NCBI-disease. Our model outperforms their model on these two datasets, because we use shared and private Bi-LSTMs to capture different features, as well as the gated interaction unit to make use of features from the auxiliary dataset.</p>
    <p>For the BioBERT model from Lee et al. [<xref ref-type="bibr" rid="CR30">30</xref>], they report their best model achieves the F1 of 84.40% on BC2GM, 89.36% on NCBI-disease, and 89.81% on Linnaeus. Their model outperforms ours because BioBERT has much more trainable parameters than ours. In BioBERT’s paper, the authors don’t report the number of parameters, but BioBERT should be similar to the original BERT which has more than 100M parameters to train.</p>
    <p>For the CollaboNet model from Yoon et al. [<xref ref-type="bibr" rid="CR31">31</xref>], they achieve the F1 of 78.56% on BC2GM and 86.36% on NCBI-disease. This model uses a special structure to achieve good performance, but our model uses multi-task learning to achieve better performance on BC2GM dataset.</p>
    <p>As for state-of-the-art models, BioCreative II Gene Mention Tagging System [<xref ref-type="bibr" rid="CR10">10</xref>] achieves the F1 of 87.21% on BC2GM dataset, MO-MTM from Crichton et al. [<xref ref-type="bibr" rid="CR23">23</xref>] achieves the F1 of 74.90% on Ex-PTM dataset, BioBERT [<xref ref-type="bibr" rid="CR30">30</xref>] achieves the F1 of 89.36% on NCBI-disease dataset, and the original LINNAEUS system [<xref ref-type="bibr" rid="CR32">32</xref>] achieves the F1 of 95.68% on Linnaeus dataset. Although BioCreative II and LINNAEUS system have the best performance on certain datasets, they rely heavily on hand-craft features which are not used in our model. Besides, these systems can pre-process the input data or have some special process using field knowledge, which benefits the performance.</p>
  </sec>
  <sec id="Sec23" sec-type="conclusion">
    <title>Conclusion</title>
    <p>In this paper, we propose a new multi-task learning framework for BioNER. We also implement some other multi-task models and compare our new model with them. Our proposed model achieves better performance, even if the size of the training data is smaller. Detailed analysis about best partners of datasets and influence between entity categories can provide guidance of choosing proper dataset pairs for multi-task training. Furthermore, our analysis suggests that the cross-sharing structure in our model is a key point to improve performance in the way of cross-dataset feature sharing.</p>
    <p>Limitations to the work include that it is difficult to predict whether one dataset can help another before running the model. Another limitation is that the current implementation of the model may not produce promising results for all datasets, in our experiment we find the performance of the proposed model on Linnaeus dataset worse than the ADV-MTM.</p>
    <p>There are several further directions with our cross-sharing multi-task model. First, training more datasets at the same time could provide more cross-dataset information and obtain better performance. Besides, we can adjust our cross-sharing structure to improve the performance on certain datasets or combine the current multi-task model with the newly proposed structure, such as BioBERT. Finally, our work may have entity type conflict problem, we could use an entity type unifier to recognize by source datasets in order to get the performance improvement.</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>ADV-MTM</term>
        <def>
          <p>Adversarial multi-task model</p>
        </def>
      </def-item>
      <def-item>
        <term>Bi-LSTM</term>
        <def>
          <p>Bi-directional long short-term memory</p>
        </def>
      </def-item>
      <def-item>
        <term>BiLSTM-CRF</term>
        <def>
          <p>Bi-directional long short-term memory with conditional random field</p>
        </def>
      </def-item>
      <def-item>
        <term>BioNER</term>
        <def>
          <p>Biomedical named entity recognition</p>
        </def>
      </def-item>
      <def-item>
        <term>CRF</term>
        <def>
          <p>Conditional random field</p>
        </def>
      </def-item>
      <def-item>
        <term>CS-MTM</term>
        <def>
          <p>Multi-task model with cross-sharing structure</p>
        </def>
      </def-item>
      <def-item>
        <term>FS-MTM</term>
        <def>
          <p>Fully-shared multi-task model</p>
        </def>
      </def-item>
      <def-item>
        <term>LSTM</term>
        <def>
          <p>Long short-term memory</p>
        </def>
      </def-item>
      <def-item>
        <term>MTL</term>
        <def>
          <p>Multi-task learning</p>
        </def>
      </def-item>
      <def-item>
        <term>RNN</term>
        <def>
          <p>Recurrent neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>SP-MTM</term>
        <def>
          <p>Shared-private multi-task model</p>
        </def>
      </def-item>
      <def-item>
        <term>STM</term>
        <def>
          <p>Single-task model</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank the members of the lab for thoughtful discussions.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>JL contributed to the implementation of the methods, to performing the experiments and to writing the manuscript. XW contributed to the design of the methods and supervised the experiments. LD and KX supervised the whole work. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was supported in part by the National Natural Science Foundation of China (NSFC) [Grant No. 61421003] and the fund of the State Key Lab of Software Development Environment [Grant No. SKLSDE-2017ZX-05]. The funding bodies did not play any role in the design of the study, data collection and analysis, or writing the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>BioNER datasets are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/cambridgeltl/MTL-Bioinformatics-2016">https://github.com/cambridgeltl/MTL-Bioinformatics-2016</ext-link>. Our implement of cross-sharing multi-task model is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/JogleLew/bioner-cross-sharing">https://github.com/JogleLew/bioner-cross-sharing</ext-link>.</p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p>Not applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pan</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Fan</surname>
            <given-names>Y-X</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>H-B</given-names>
          </name>
        </person-group>
        <article-title>Identifying rna-binding proteins using multi-label deep learning</article-title>
        <source>Sci China Inf Sci</source>
        <year>2019</year>
        <volume>62</volume>
        <issue>1</issue>
        <fpage>19103</fpage>
        <pub-id pub-id-type="doi">10.1007/s11432-018-9558-2</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>HE</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>LIU</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>QIAN</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>ZHOU</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Disease name recognition based on syntactic and semantic features</article-title>
        <source>Sci Sin Inf</source>
        <year>2018</year>
        <volume>48</volume>
        <issue>11</issue>
        <fpage>1546</fpage>
        <lpage>57</lpage>
        <pub-id pub-id-type="doi">10.1360/N112018-00210</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>C-H</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Davis</surname>
            <given-names>AP</given-names>
          </name>
          <name>
            <surname>Mattingly</surname>
            <given-names>CJ</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wiegers</surname>
            <given-names>TC</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Assessing the state of the art in biomedical relation extraction: overview of the biocreative v chemical-disease relation (cdr) task</article-title>
        <source>Database</source>
        <year>2016</year>
        <volume>2016</volume>
        <fpage>8</fpage>
        <pub-id pub-id-type="doi">10.1093/database/baw032</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Donaldson</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>De Bruijn</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Wolting</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Lay</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Tuekam</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Baskin</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Bader</surname>
            <given-names>GD</given-names>
          </name>
          <name>
            <surname>Michalickova</surname>
            <given-names>K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Prebind and textomy–mining the biomedical literature for protein-protein interactions using a support vector machine</article-title>
        <source>BMC Bioinformatics</source>
        <year>2003</year>
        <volume>4</volume>
        <issue>1</issue>
        <fpage>11</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-4-11</pub-id>
        <pub-id pub-id-type="pmid">12689350</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rajpal</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Qu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Freudenberg</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Mining emerging biomedical literature for understanding disease associations in drug discovery</article-title>
        <source>Methods Mol Biol (Clifton, NJ)</source>
        <year>2014</year>
        <volume>1159</volume>
        <fpage>171</fpage>
        <lpage>206</lpage>
        <pub-id pub-id-type="doi">10.1007/978-1-4939-0709-0_11</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Montojo</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zuberi</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Rodriguez</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Bader</surname>
            <given-names>GD</given-names>
          </name>
          <name>
            <surname>Morris</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>Genemania: Fast gene network construction and function prediction for cytoscape</article-title>
        <source>F1000Research</source>
        <year>2014</year>
        <volume>3</volume>
        <fpage>153</fpage>
        <pub-id pub-id-type="doi">10.12688/f1000research.4572.1</pub-id>
        <pub-id pub-id-type="pmid">25254104</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>F-X</given-names>
          </name>
        </person-group>
        <article-title>Dynamic protein interaction network construction and applications</article-title>
        <source>Proteomics</source>
        <year>2014</year>
        <volume>14</volume>
        <issue>4-5</issue>
        <fpage>338</fpage>
        <lpage>52</lpage>
        <pub-id pub-id-type="doi">10.1002/pmic.201300257</pub-id>
        <pub-id pub-id-type="pmid">24339054</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ji</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Spangler</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Mining strong relevance between heterogeneous entities from unstructured biomedical data</article-title>
        <source>Data Min Knowl Disc</source>
        <year>2015</year>
        <volume>29</volume>
        <issue>4</issue>
        <fpage>976</fpage>
        <lpage>98</lpage>
        <pub-id pub-id-type="doi">10.1007/s10618-014-0396-4</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Fukuda</surname>
            <given-names>K-i</given-names>
          </name>
          <name>
            <surname>Tsunoda</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tamura</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Takagi</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Toward information extraction: identifying protein names from biological papers</article-title>
        <source>Proceedings of the 3rd Pacific Symposium on Biocomputing</source>
        <year>1998</year>
        <publisher-loc>Hawaii</publisher-loc>
        <publisher-name>PSB</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <mixed-citation publication-type="other">Ando RK. Biocreative ii gene mention tagging system at ibm watson. In: Proceedings of the Second BioCreative Challenge Evaluation Workshop. Madrid: Centro Nacional de Investigaciones Oncologicas (CNIO). 2007;23:101–3.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Campos</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Matos</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Oliveira</surname>
            <given-names>JL</given-names>
          </name>
        </person-group>
        <article-title>Biomedical named entity recognition: a survey of machine-learning tools</article-title>
        <source>Theory and Applications for Advanced Text Mining</source>
        <year>2012</year>
        <publisher-loc>London</publisher-loc>
        <publisher-name>IntechOpen</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>C-H</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>tmchem: a high performance approach for chemical named entity recognition and normalization</article-title>
        <source>J Cheminformatics</source>
        <year>2015</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>3</fpage>
        <pub-id pub-id-type="doi">10.1186/1758-2946-7-S1-S3</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Jin</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Biomedical named entity recognition based on extended recurrent neural networks</article-title>
        <source>Bioinformatics and Biomedicine (BIBM), 2015 IEEE International Conference On</source>
        <year>2015</year>
        <publisher-loc>Washington</publisher-loc>
        <publisher-name>IEEE</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Chemdner system with mixed conditional random fields and multi-scale word clustering</article-title>
        <source>J Cheminformatics</source>
        <year>2015</year>
        <volume>7</volume>
        <issue>S1</issue>
        <fpage>4</fpage>
        <pub-id pub-id-type="doi">10.1186/1758-2946-7-S1-S4</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Lample</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Ballesteros</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Subramanian</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kawakami</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Dyer</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Neural architectures for named entity recognition</article-title>
        <source>Proceedings of NAACL-HLT</source>
        <year>2016</year>
        <publisher-loc>US</publisher-loc>
        <publisher-name>Association for Computational Linguistics</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <mixed-citation publication-type="other">Ma X, Hovy E. End-to-end sequence labeling via bi-directional lstm-cnns-crf. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. US: Association for Computational Linguistics. 2016;1:1064–74.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chiu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Nichols</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Named entity recognition with bidirectional lstm-cnns</article-title>
        <source>Trans Assoc Comput Linguist</source>
        <year>2016</year>
        <volume>4</volume>
        <issue>1</issue>
        <fpage>357</fpage>
        <lpage>70</lpage>
        <pub-id pub-id-type="doi">10.1162/tacl_a_00104</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Habibi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Weber</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Neves</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wiegandt</surname>
            <given-names>DL</given-names>
          </name>
          <name>
            <surname>Leser</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <article-title>Deep learning with word embeddings improves biomedical named entity recognition</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <issue>14</issue>
        <fpage>37</fpage>
        <lpage>48</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx228</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zitnik</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Shang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Langlotz</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Cross-type biomedical named entity recognition with deep multi-task learning</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>35</volume>
        <issue>10</issue>
        <fpage>1745</fpage>
        <lpage>52</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty869</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Collobert</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Weston</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A unified architecture for natural language processing: Deep neural networks with multitask learning</article-title>
        <source>Proceedings of the 25th International Conference on Machine Learning</source>
        <year>2008</year>
        <publisher-loc>New York City</publisher-loc>
        <publisher-name>ACM</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Speck-Planche</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cordeiro</surname>
            <given-names>MNDS</given-names>
          </name>
        </person-group>
        <article-title>Multitasking models for quantitative structure–biological effect relationships: current status and future perspectives to speed up drug discovery</article-title>
        <source>Expert Opin Drug Discov</source>
        <year>2015</year>
        <volume>10</volume>
        <issue>3</issue>
        <fpage>245</fpage>
        <lpage>56</lpage>
        <pub-id pub-id-type="doi">10.1517/17460441.2015.1006195</pub-id>
        <pub-id pub-id-type="pmid">25613725</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Simm</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lam</surname>
            <given-names>CC</given-names>
          </name>
          <name>
            <surname>Zakeri</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>van Westen</surname>
            <given-names>GJ</given-names>
          </name>
          <name>
            <surname>Moreau</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Saez-Rodriguez</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Linking drug target and pathway activation for effective therapy using multi-task learning</article-title>
        <source>Sci Rep</source>
        <year>2018</year>
        <volume>8</volume>
        <fpage>8322</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-018-25947-y</pub-id>
        <pub-id pub-id-type="pmid">29844324</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Crichton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Pyysalo</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Chiu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Korhonen</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>A neural network multi-task learning approach to biomedical named entity recognition</article-title>
        <source>BMC Bioinformatics</source>
        <year>2017</year>
        <volume>18</volume>
        <issue>1</issue>
        <fpage>368</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-017-1776-8</pub-id>
        <pub-id pub-id-type="pmid">28810903</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <mixed-citation publication-type="other">Liu P, Qiu X, Huang X. Adversarial multi-task learning for text classification. In: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), vol. 1. US: Association for Computational Linguistics. 2017;1:1–10.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <mixed-citation publication-type="other">Wang Z, Qu Y, Chen L, Shen J, Zhang W, Zhang S, Gao Y, Gu G, Chen K, Yu Y. Label-aware double transfer learning for cross-specialty medical named entity recognition. In: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. US: Association for Computational Linguistics. 2018;1:1–15.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Abdou</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kulmizev</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ravishankar</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Abzianidze</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Bos</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>What can we learn from semantic tagging?</article-title>
        <source>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</source>
        <year>2018</year>
        <publisher-loc>US</publisher-loc>
        <publisher-name>SIGDAT</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Long short-term memory</article-title>
        <source>Neural Comput</source>
        <year>1997</year>
        <volume>9</volume>
        <issue>8</issue>
        <fpage>1735</fpage>
        <lpage>80</lpage>
        <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Lafferty</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>McCallum</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pereira</surname>
            <given-names>FC</given-names>
          </name>
        </person-group>
        <article-title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data</article-title>
        <source>Proceedings of the 17th International Conference on Machine Learning (ICML)</source>
        <year>2001</year>
        <publisher-loc>New York City</publisher-loc>
        <publisher-name>ACM</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29</label>
      <mixed-citation publication-type="other">Ganin Y, Lempitsky V. Unsupervised domain adaptation by backpropagation. In: Proceedings of the 32nd International Conference on International Conference on Machine Learning. New York City: ACM. 2015;37:1180–9.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30</label>
      <mixed-citation publication-type="other">Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, Kang J. Biobert: a pre-trained biomedical language representation model for biomedical text mining. 2019. arXiv preprint arXiv:1901.08746.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yoon</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>So</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Collabonet: collaboration of deep neural networks for biomedical named entity recognition</article-title>
        <source>BMC Bioinformatics</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>10</issue>
        <fpage>249</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-019-2813-6</pub-id>
        <pub-id pub-id-type="pmid">31138109</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gerner</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nenadic</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Bergman</surname>
            <given-names>CM</given-names>
          </name>
        </person-group>
        <article-title>Linnaeus: a species name identification system for biomedical literature</article-title>
        <source>BMC Bioinformatics</source>
        <year>2010</year>
        <volume>11</volume>
        <issue>1</issue>
        <fpage>85</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-11-85</pub-id>
        <pub-id pub-id-type="pmid">20149233</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
