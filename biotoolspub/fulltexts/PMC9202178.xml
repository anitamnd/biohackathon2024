<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9202178</article-id>
    <article-id pub-id-type="publisher-id">4769</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-022-04769-w</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Euclidean distance-optimized data transformation for cluster analysis in biomedical data (EDOtrans)</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Ultsch</surname>
          <given-names>Alfred</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Lötsch</surname>
          <given-names>Jörn</given-names>
        </name>
        <address>
          <email>j.loetsch@em.uni-frankfurt.de</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.10253.35</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 9756</institution-id><institution>DataBionics Research Group, </institution><institution>University of Marburg, </institution></institution-wrap>Hans - Meerwein - Straße, 35032 Marburg, Germany </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.7839.5</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 9721</institution-id><institution>Institute of Clinical Pharmacology, </institution><institution>Goethe - University, </institution></institution-wrap>Theodor Stern Kai 7, 60590 Frankfurt am Main, Germany </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.510864.e</institution-id><institution>Fraunhofer Institute for Translational Medicine and Pharmacology ITMP, </institution></institution-wrap>Theodor-Stern-Kai 7, 60596 Frankfurt am Main, Germany </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>16</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>16</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>23</volume>
    <elocation-id>233</elocation-id>
    <history>
      <date date-type="received">
        <day>21</day>
        <month>4</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>1</day>
        <month>6</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Data transformations are commonly used in bioinformatics data processing in the context of data projection and clustering. The most used Euclidean metric is not scale invariant and therefore occasionally inappropriate for complex, e.g., multimodal distributed variables and may negatively affect the results of cluster analysis. Specifically, the squaring function in the definition of the Euclidean distance as the square root of the sum of squared differences between data points has the consequence that the value 1 implicitly defines a limit for distances within clusters versus distances between (inter-) clusters.</p>
      </sec>
      <sec>
        <title>Methods</title>
        <p id="Par2">The Euclidean distances within a standard normal distribution (N(0,1)) follow a N(0,<inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sqrt{2}$$\end{document}</tex-math><mml:math id="M2"><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:math><inline-graphic xlink:href="12859_2022_4769_Article_IEq1.gif"/></alternatives></inline-formula>) distribution. The EDO-transformation of a variable X is proposed as <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$EDO= X/(\sqrt{2}\cdot s)$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mi>E</mml:mi><mml:mi>D</mml:mi><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mo>·</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4769_Article_IEq2.gif"/></alternatives></inline-formula> following modeling of the standard deviation <italic>s</italic> by a mixture of Gaussians and selecting the dominant modes via item categorization. The method was compared in artificial and biomedical datasets with clustering of untransformed data, z-transformed data, and the recently proposed pooled variable scaling.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par3">A simulation study and applications to known real data examples showed that the proposed EDO scaling method is generally useful. The clustering results in terms of cluster accuracy, adjusted Rand index and Dunn’s index outperformed the classical alternatives. Finally, the EDO transformation was applied to cluster a high-dimensional genomic dataset consisting of gene expression data for multiple samples of breast cancer tissues, and the proposed approach gave better results than classical methods and was compared with pooled variable scaling.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par4">For multivariate procedures of data analysis, it is proposed to use the EDO transformation as a better alternative to the established z-standardization, especially for nontrivially distributed data. The “EDOtrans” R package is available at <ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=EDOtrans">https://cran.r-project.org/package=EDOtrans</ext-link>.
</p>
      </sec>
      <sec>
        <title>Supplementary Information</title>
        <p>The online version contains supplementary material available at 10.1186/s12859-022-04769-w.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Data science</kwd>
      <kwd>Machine-learning</kwd>
      <kwd>Biomedical informatics</kwd>
      <kwd>Data preprocessing</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id>
            <institution>Deutsche Forschungsgemeinschaft</institution>
          </institution-wrap>
        </funding-source>
        <award-id>DFG LO 612/16-1</award-id>
        <principal-award-recipient>
          <name>
            <surname>Lötsch</surname>
            <given-names>Jörn</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Landesoffensive zur Entwicklung wissenschaftlich-ökonomischer Exzellenz (LOEWE)</institution>
        </funding-source>
        <award-id>Reproducible cleaning of biomedical laboratory data using methods of visualization, error correc-tion and transformation implemented as interactive R-notebooks</award-id>
        <principal-award-recipient>
          <name>
            <surname>Lötsch</surname>
            <given-names>Jörn</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Johann Wolfgang Goethe-Universität, Frankfurt am Main (1022)</institution>
        </funding-source>
      </award-group>
      <open-access>
        <p>Open Access funding enabled and organized by Projekt DEAL.</p>
      </open-access>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par5">Biomedical data often contain subgroup structures that are identified by data projection and clustering. For this purpose, informatic methods of data projection [<xref ref-type="bibr" rid="CR1">1</xref>] and cluster identification [<xref ref-type="bibr" rid="CR2">2</xref>] are available. Clustering is widely used in biomedical research. The quality of clustering plays a critical role in biomedical research and depends on the distance metrics used to quantify similarities and dissimilarities between data points [<xref ref-type="bibr" rid="CR3">3</xref>], which allows the integration of the different dimensions into a measure of (dis-)similarity. Two problems must be solved for a definition of a valid distance function on the data. First, the dispersions, e.g., the variances respectively standard deviations, of the different variables in the data set must be made comparable. Therefore, standard workflows include data transformation. Second, the design of this transformation must consider the specifics of the distance function used. The importance of similarity metrics on the clustering of biomedical data has been recognized [<xref ref-type="bibr" rid="CR4">4</xref>]. In many projects, Euclidean distance to z-standardized data is the default approach [<xref ref-type="bibr" rid="CR5">5</xref>].</p>
    <p id="Par6">A particular problem with Euclidean distance is that it is not scale invariant, i.e., multiplying the data by a common factor changes the distance. Recognizing this potential pitfall in clustering approaches, adapted scaling methods have been proposed that take into account the scale dependence of the Euclidean distance, such as pooled variable scaling (PVS) [<xref ref-type="bibr" rid="CR6">6</xref>]. In this report, an alternative to the standard z-transform of biomedical data is proposed as a more appropriate approach for clustering biomedical data. The "Euclidean Distance Optimized" (EDO) data transformation addresses the scale dependence of Euclidean distance, but treats each variable separately and therefore does not introduce clustering at the transformation level as alternative approaches do [<xref ref-type="bibr" rid="CR6">6</xref>]. It specifically takes into account that the squaring function in the definition of Euclidean distance results a breakpoint for distances within (inner)clusters versus distances between (inter)clusters at a value of 1. Thus, while in [<xref ref-type="bibr" rid="CR6">6</xref>] the goal is to make the scales of different variables comparable, the present approach aims at minimizing differences within classes and maximizing differences between classes. It is therefore more focused on finding clusters in the data. The present work is concerned with numerical data, in particular data variables with an interval scale level of measurement. Fusion of heterogeneous data such as strings or images is not considered. For integration of such data into a single path, see, e.g., [<xref ref-type="bibr" rid="CR7">7</xref>].</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>Distributions within and between classes in data sets</title>
      <p id="Par7">Defining a meaningful distance requires expert knowledge, which in many projects is replaced by the "usual procedure" of applying the Euclidean distance to standardized (z-standardized) data [<xref ref-type="bibr" rid="CR5">5</xref>]. The Euclidean distance is the most intuitive distance metric as it corresponds to the everyday perception of distances. The Euclidean distance <italic>d</italic> of two data cases (x<sub>1</sub>, x<sub>2</sub>) is defined as the square root of the sum of squared differences <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d\left(x,y\right)= \sqrt{\sum {\left|{x}_{i}-{y}_{i}\right|}^{2}}$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi>d</mml:mi><mml:mfenced close=")" open="("><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mo>∑</mml:mo><mml:msup><mml:mrow><mml:mfenced close="|" open="|"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4769_Article_IEq3.gif"/></alternatives></inline-formula>. The Euclidean metric is translation invariant, i.e., it does not change when a common value is added to each variable of the data; however, the Euclidian distance is not scale invariant, i.e., multiplying the data with a common factor changes the distance! That is, using the squaring function (<italic>d</italic><sup><italic>2</italic></sup>) on <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d=\left|{x}_{i}-{y}_{i}\right|$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="|" open="|"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4769_Article_IEq4.gif"/></alternatives></inline-formula> has the effect that differences &lt; 1 become smaller and differences &gt; 1 result in larger values of d<sup>2</sup>. For class or cluster problems, the squaring function in Euclidean distance (see above) has therefore the consequence of implicitly defining 1 as the limit for inner class distances and between (inter) class distances. While this may be appropriate for simple distributions of a variable, such as normal (Gaussian) distributions, it may be inappropriate for more complex variables, such as characteristics with bimodal or multimodal distributions.</p>
      <p id="Par8">In data mining and knowledge discovery in multivariate data, the first step is to analyze the distribution of the individual variables. Let <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X in {\mathbb{R}}^{d}$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mi>X</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4769_Article_IEq5.gif"/></alternatives></inline-formula> be a multivariate, i.e., d-dimensional data set representing a finite |X|= n number of outcomes (cases) of an experiment. This is typically described as drawn at random from a process that generates the data according to some probability distribution function (<italic>pdf(X)</italic>). Distributions are called simple, if the standard deviation <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s\left(x\right)=\frac{1}{n}\sqrt{\sum_{i=1}^{n}{\left(m-{x}_{i}\right)}^{2}}$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:msqrt><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mi>m</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4769_Article_IEq6.gif"/></alternatives></inline-formula>, for all <italic>x</italic><sub><italic>i</italic></sub> in <italic>X</italic> with where <italic>m</italic> denotes the arithmetic mean value of in <italic>X</italic>, is an appropriate measure of the dispersion of the variable. Normal or Gaussian distributions are the most encountered types of simple distributions and often serve as standard model. If the values x of a given variable are normally distributed, then an application of the z-standardization <italic>z</italic> = (<italic>x</italic> − <italic>m</italic>)/<italic>s</italic>, where <italic>m</italic> denotes the mean of the values in the variable and <italic>s</italic> the standard deviation, yields a standard normal distribution <italic>z</italic> ~ <italic>N</italic>(0, 1). The Euclidean distances = (<italic>z</italic><sub><italic>i</italic></sub> − <italic>z</italic><sub><italic>j</italic></sub>) in N(0, 1) follow an N(0, <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sqrt{2}$$\end{document}</tex-math><mml:math id="M14"><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:math><inline-graphic xlink:href="12859_2022_4769_Article_IEq7.gif"/></alternatives></inline-formula>) distribution [<xref ref-type="bibr" rid="CR8">8</xref>]. This implicitly defines innerclass distances as distances &lt; 1 and interclass distances as distances &gt; 1 (see the first panel in Fig. <xref rid="Fig1" ref-type="fig">1</xref>).<fig id="Fig1"><label>Fig. 1</label><caption><p>Implicit definition of instances within a class (innerclass instances) in an exemplary distribution using the Euclidean distance. Properties of the Euclidian distance relevant to innerclass and interclass distances. <bold>A</bold> The problem addressed by the EDO transformation has its origin in the behavior of the squared differences function,<inline-formula id="IEq16"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f\left(x\right)={x}^{2}$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4769_Article_IEq16.gif"/></alternatives></inline-formula>. Here x<sup>2</sup> &lt; x holds for x values in the interval [1, 1] and x<sup>2</sup> &gt; x for x values outside this interval, which affects the analogous behavior of the Euclidean distance based on the sum of the squared single differences. The value of x = 1 at which the change occurs is marked by a red solid line. The dashed dark gray lines indicate the identity x<sup>2</sup> = x. <bold>B</bold> Behavior of Euclidean distances compared to distances computed without using the square of individual distances, again indicating a break from ≤ 1 to &gt; 1 at a distance of <inline-formula id="IEq17"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d = 1$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4769_Article_IEq17.gif"/></alternatives></inline-formula> (solid red line). The identity between the two implementations of the distances is shown as a (horizontal) dashed dark gray line. <bold>C–E</bold> Limits on the assignment of a data point to the inner center of a distribution. The green lines mark the distance of one standard deviation from the mean in a normally distributed data set with distribution N(4,3). The red vertical lines mark the boundaries between which a data point has a Euclidean distance ≤ 1 from the center. Data points located within the innerclass rage are colored black, while data points located at greater distances from the center are colored gold. <bold>C</bold> For untransformed raw data, this innerclass range is much narrower than the usual mean ± standard deviation range. <bold>D</bold> When z-standardization is applied, the innerclass range becomes wider. The graph again shows the original data, but the innerclass limits were calculated for z-standardized data and transformed back to the original data range. <bold>E</bold> With the EDO transformation, the innerclass angel finally fulfills the desire to cover the usual mean +—normalization range. Again, the graph shows the original data, but the innerclass limits were calculated for EDO-transformed data and transformed back to the original data range. The figure has been created using the R software package (version 4.1.2 for Linux; <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/">https://CRAN.R-project.org/</ext-link> [<xref ref-type="bibr" rid="CR9">9</xref>]) and the R libraries “ggplot2” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=ggplot2">https://cran.r-project.org/package=ggplot2</ext-link> [<xref ref-type="bibr" rid="CR10">10</xref>]) and “ggthemes” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=ggthemes">https://cran.r-project.org/package=ggthemes</ext-link> [<xref ref-type="bibr" rid="CR11">11</xref>])</p></caption><graphic xlink:href="12859_2022_4769_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec4">
      <title>Algorithm</title>
      <p id="Par9">Considering the distribution parameters, this means that the intraclass versus interclass relevant limit for the distance of a data point from the center is 1<inline-formula id="IEq8"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$/\sqrt{2}$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mo stretchy="false">/</mml:mo><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4769_Article_IEq8.gif"/></alternatives></inline-formula>. Hence, the EDO transformation, provided that a suitable estimate of s is available, is defined as<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$EDO= \frac{X}{\sqrt{2}\cdot s}$$\end{document}</tex-math><mml:math id="M22" display="block"><mml:mrow><mml:mi>E</mml:mi><mml:mi>D</mml:mi><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>X</mml:mi><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mo>·</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2022_4769_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>This increases the innerclass range from that for raw data or z-standardized data, i.e., more data points around the arithmetic mean fall within the defined Euclidian distance of &lt; 1 (Fig. <xref rid="Fig1" ref-type="fig">1</xref>).</p>
      <p id="Par10">Distributions are called complex if the data generating process produces multimodal distributions that have two or more local maxima (modes, peaks) in their probability distribution function. The reason for such modes may be that the data-generating process operates in different states, e.g., "healthy" versus "sick." Such complex distributions are common in nature and especially in biology. A generative process underlying such multimodal distributed data can be described by a Gaussian mixture model (GMM), which generates data using a sum (i.e., mixture) of conditional probabilities:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p\left(x\right) = {\sum }_{i = 0}^{M}{w}_{i}N\left(x|{m}_{i},{s}_{i}\right) = {\sum }_{i = 1}^{M}{w}_{i}\cdot \frac{1}{\sqrt{2\pi {s}_{i}}}\cdot {e}^{-\frac{{\left(x-{m}_{i}\right)}^{2}}{{2s}_{i}^{2}}},$$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mrow><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>N</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msqrt></mml:mfrac><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mrow><mml:mn>2</mml:mn><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="12859_2022_4769_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <italic>N</italic>(<italic>x|m</italic><sub><italic>i</italic></sub>, <italic>s</italic><sub><italic>i</italic></sub>) (components) denote Gaussian probability densities with means <italic>m</italic><sub><italic>i</italic></sub> and standard deviations <italic>s</italic><sub><italic>i</italic></sub>. <italic>M</italic> is the number of components in the mixture. The weights <italic>w</italic><sub><italic>i</italic></sub> denote the relative contribution of each Gaussian component to the overall distribution and add up to a value of 1. The most important or dominant subsets or modes within the data set are those with the largest weights, i.e., the largest prior probability. Which modes belong to this category can be determined using computed ABC analysis [<xref ref-type="bibr" rid="CR12">12</xref>]. This divides the weights of the Gaussian mixture components into three non-overlapping subsets named “A”, “B”, and “C” [<xref ref-type="bibr" rid="CR13">13</xref>]. Subset “A” contains the “important few,” i.e., the weights that place the particular Gaussian mode in the “dominant” category. The combined variances within this dominant set of components provide a more adequate measure of the dispersion of the data than the z-standardization. This will be discussed in more detail later in this report. When there is no dominant mode, e.g., when all classes are equally weighted, the median of the standard deviations of the modes in ABC set “B” is used in the EDO transformation.</p>
      <p id="Par11">The standard deviation relevant for the EDO transformation is that of the dominant modes. In the case where more than one mode has been assigned to the dominant category, the combined standard deviation <inline-formula id="IEq9"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{combined}$$\end{document}</tex-math><mml:math id="M26"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="italic">combined</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4769_Article_IEq9.gif"/></alternatives></inline-formula> is calculated as<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{combined}=\sqrt{\frac{(\sum_{i=1}^{M}\left(\left({n}_{i}-1\right)\cdot {s}_{i}^{2}+{n}_{i\cdot }\cdot {{m}_{i}}^{2}\right)-n\cdot {{m}_{w}}^{2}}{n-1}}$$\end{document}</tex-math><mml:math id="M28" display="block"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="italic">combined</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mfenced close=")" open="("><mml:mfenced close=")" open="("><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mfenced><mml:mo>·</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>·</mml:mo></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfenced><mml:mo>-</mml:mo><mml:mi>n</mml:mi><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:msqrt></mml:mrow></mml:math><graphic xlink:href="12859_2022_4769_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq10"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${n}_{i}={w}_{i}\cdot n$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4769_Article_IEq10.gif"/></alternatives></inline-formula> is the relative number of data in mode <italic>i</italic> and <inline-formula id="IEq11"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${m}_{w} = \sum_{1}^{M}\left({w}_{i}\cdot {m}_{i}\right)$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mfenced close=")" open="("><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4769_Article_IEq11.gif"/></alternatives></inline-formula> the weighted mean in the GMM. The EDO transformation on multimodally distributed one-dimensional data is then defined as<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$EDO= \frac{X}{\sqrt{2}\cdot {s}_{combined}}$$\end{document}</tex-math><mml:math id="M34" display="block"><mml:mrow><mml:mi>E</mml:mi><mml:mi>D</mml:mi><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>X</mml:mi><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mo>·</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="italic">combined</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2022_4769_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par12">It is often known that the data is generated by a data generating process that operates in different states (e.g., healthy or sick), i.e., a prior classification of the data is given. The EDO transformation could then be calculated using this prior classification. The distribution of the distances of the data points can be divided into innerclass and interclass according to the previous classification: If two cases <italic>x</italic> and <italic>y</italic> are from the same prior class, the distance is an innerclass distance, otherwise <italic>x</italic> and <italic>y</italic> are from two different classes and therefore their distance is classified as interclass distance. Based on these considerations, applying the EDO transformation to a three-class data set with three variables distributed according to Gaussian mixtures with M = 3 modes resulted in the expected improvement in k-means based clustering [<xref ref-type="bibr" rid="CR14">14</xref>] (Fig. <xref rid="Fig2" ref-type="fig">2</xref>).<fig id="Fig2"><label>Fig. 2</label><caption><p>Effects of EDO transformation on innerclass and interclass distances and clustering of multivariate datasets. K-means clustering of an artificial data set that represented a three-class scenario with values generated by Gaussian mixture models with four different variables with increasing means, various standard deviations with a total of 3000 instances with class weights = [0.7, 0.2, 0.1] in each variable. The clustering was performed on untransformed (raw) data (panels <bold>A</bold>–<bold>D</bold>), on z-standardized data (panels <bold>E</bold>–<bold>H</bold>), and on EDO transformed data (panels <bold>I</bold>–<bold>L</bold>). For each kind of data transformation, four panels are shown. The left panels <bold>A</bold>, <bold>E</bold>, <bold>I</bold> show the original data that consist of three variables that are distributed according to a Gaussian mixture containing three modes. The sinaplot [<xref ref-type="bibr" rid="CR15">15</xref>] shows the individual data points of the three subgroups dithering along the x-axis to create a contour indicating the probability density of the distribution of the data points. Panels <bold>B</bold>, <bold>F</bold>, <bold>J</bold> show the distribution of innerclass and interclass distances as histograms. Panels <bold>C</bold>, <bold>G</bold>, <bold>H</bold> show factorial plots of the individual data points on a principal component analysis projection colored according to a k-means clustering. The borders of the colored areas visualize the cluster separation. The right panels <bold>D</bold>, <bold>H</bold>, <bold>L</bold> show Silhouette plots for the three clusters. Positive values indicate that the sample is within a cluster while negative values indicate that those samples might have been assigned to the wrong cluster because they are closer to neighboring than to their own cluster. The figure has been created using the R software package (version 4.1.2 for Linux; <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/">https://CRAN.R-project.org/</ext-link> [<xref ref-type="bibr" rid="CR9">9</xref>]) and the R packages “ggplot2” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=ggplot2">https://cran.r-project.org/package=ggplot2</ext-link> [<xref ref-type="bibr" rid="CR10">10</xref>]), and “FactoMineR” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=FactoMineR">https://cran.r-project.org/package=FactoMineR</ext-link> [<xref ref-type="bibr" rid="CR16">16</xref>]). The colors were selected from the “colorblind_pal” palette provided with the R library “ggthemes” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=ggthemes">https://cran.r-project.org/package=ggthemes</ext-link> [<xref ref-type="bibr" rid="CR11">11</xref>])</p></caption><graphic xlink:href="12859_2022_4769_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par13">However, the underlying assumption that the pre-classification structure is reflected in the Euclidean distance structures may not always be valid. Assume a measured variable <italic>X</italic> that after statistical testing can be assumed to be normal Gaussian distributed. Let the pre-classification into healthy versus sick be such that if <italic>x</italic> ≤ <italic>mean</italic>(<italic>X</italic>), the diagnosis is that <italic>x</italic> is healthy, if <italic>x</italic> &gt; <italic>mean</italic>(<italic>X</italic>), the diagnosis is that <italic>x</italic> is sick. In this type of pre-classification, the distance structures between healthy and sick are indistinguishable. Therefore, it is advisable to base the EDO transformation on the observed modes of the distribution of the variable rather than on the pre-classification structure.</p>
    </sec>
    <sec id="Sec5">
      <title>Experimentation</title>
      <p id="Par14">The programming work for this report was performed in the R language [<xref ref-type="bibr" rid="CR17">17</xref>] using the R software package [<xref ref-type="bibr" rid="CR9">9</xref>] (version 4.1.2 for Linux), which is available free of charge in the Comprehensive R Archive Network at <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/">https://CRAN.R-project.org/</ext-link>. Considering the goal of EDO data transformation to improve subgroup separation, e.g., clustering, the experiments were performed with artificial datasets created to have the required subgroup structure, or with biomedical data for which a subgroup structure was known. Clustering results were compared between the use of raw data, standard z-transformed data, and EDO-transformed data. The EDO transformation was performed following the analysis of each variable for the modal distribution using the automated Gaussian mixture modeling implemented in the R library “opGMMassessment” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=opGMMassessment">https://cran.r-project.org/package=opGMMassessment</ext-link>).</p>
      <p id="Par15">Partitioning based clustering was mainly implemented as k-means clustering [<xref ref-type="bibr" rid="CR14">14</xref>]; however, partitioning around medoids (PAM) was used for comparison [<xref ref-type="bibr" rid="CR18">18</xref>]. Hierarchical clustering with Ward's linkage [<xref ref-type="bibr" rid="CR19">19</xref>] was used; however, average and complete linkage were used for comparison in analogy to the choice made in [<xref ref-type="bibr" rid="CR6">6</xref>]. The Euclidean distance was used as the target of the transformation method presented here. Clustering was done using the R package “cluster” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=cluster">https://cran.r-project.org/package=cluster</ext-link> [<xref ref-type="bibr" rid="CR20">20</xref>]). Cluster quality and stability were assessed by calculating the cluster accuracy and the adjusted Rand index [<xref ref-type="bibr" rid="CR21">21</xref>] against the prior classification of the data, and as Dunn’s index [<xref ref-type="bibr" rid="CR22">22</xref>], calculated using the R packages “fossil” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=fossil">https://cran.r-project.org/package=fossil</ext-link> [<xref ref-type="bibr" rid="CR23">23</xref>]) and “clValid” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=clValid">https://cran.r-project.org/package=clValid</ext-link> [<xref ref-type="bibr" rid="CR24">24</xref>]). Clustering was compared to a modern alternative scaling approach targeting Euclidean distance boundaries, recently proposed as pooled variable scaling (PVS) [<xref ref-type="bibr" rid="CR6">6</xref>], in which, unlike the present method, scaling assumes a k-means clustering analysis of the entire dataset.</p>
    </sec>
    <sec id="Sec6">
      <title>Implementation</title>
      <p id="Par16">The EDO data transformation method proposed here has been implemented in the R package “EDOtrans”, which is available at <ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=EDOtrans">https://cran.r-project.org/package=EDOtrans</ext-link>. The transformation process of a one-dimensional variable can be called with the function "EDOtrans(Data, Cls, PlotIt = FALSE, FitAlg = "normalmixEM", Criterion = "LR", MaxModes = 8, MaxCores = getOption("mc.cores", 2L), Seed)". At least one data vector ("Data") is expected as input (1). If available, class information (2) per instance ("Cls") can be entered, which is then used as the basis for EDO transformation. The class information can be the prior classification as used in the proof-of-concept in this report, or it can be obtained in any way, e.g., through interactive Gaussian mixture modeling with the R library "AdaptGauss" (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=AdaptGauss">https://cran.r-project.org/package=AdaptGauss</ext-link> [<xref ref-type="bibr" rid="CR25">25</xref>]), which allows data analysis under visual control which may capture the entire modal structure in more complicated cases better than fully automated solutions. However, as used in the experiments in this report, the class information can be omitted and then be created internally in the “EDOtrans” library, using Gaussian mixture modeling imported from the R package “opGMMassessment” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=opGMMassessment">https://cran.r-project.org/package=opGMMassessment</ext-link>). In this case, subsequent parameters such as the fitting algorithm, "FitAlg", the criterion for determining the number of modes in the mixture, "Criterion", and the maximum number of modes, “MaxModes”, are forwarded to that library. More detailed hyperparameter settings are beyond the scope of this report and are provided via the R library help function.</p>
    </sec>
  </sec>
  <sec id="Sec7">
    <title>Results</title>
    <sec id="Sec8">
      <title>Proof of concept study</title>
      <p id="Par17">A three-dimensional data set was created with 3000 instances drawn from three normal distributions with different probabilities, resulting in three-modal data (M = 3 modes). For the three variables of which each followed a three-modal distribution, the class weights were always w<sub>i</sub> = [0.7, 0.2, 0.1] for classes 1 to 3. However, the means and standard deviations differed for the Gaussian mixtures, with parameter values for mixture no. 1 were means = [0, 5, 15] and standard deviations = [2, 2, 3], for mixture no. 2 were means = [4, 5, 6] and standard deviations = [4, 4, 3], and for mixture no. 3 were means = [3, 11, 15] and standard deviations = [0.5, 0.1, 0.4].</p>
      <p id="Par18">For this data set, standard clustering with k-means failed on the raw data, although at least variables #1 and #3 appeared to break into three groups (Fig. <xref rid="Fig2" ref-type="fig">2</xref> top row of panels). The distribution of distances separately by class membership showed that there was a large overlap between within-class and between-class distances. This was little changed by z-standardization, which improved clustering only slightly by 1% accuracy (Fig. <xref rid="Fig2" ref-type="fig">2</xref> middle row of panels). The EDO transformation was performed based on the previous classification. After the EDO transformation of each variable, the clustering solution appeared almost perfect in this sample data set (Fig. <xref rid="Fig2" ref-type="fig">2</xref> bottom row of panels). Replacing k-means with PAM clustering did not change these observations (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S1).</p>
    </sec>
    <sec id="Sec9">
      <title>Simulation study</title>
      <p id="Par19">A four-dimensional data set with 1,000 instances drawn from three normal distributions each with different probabilities resulting in a three-modal distribution (M = 3 modes) in a Gaussian mixture model (GMM). Each of the three normal distributions is characterized by its expected value <italic>m</italic><sub><italic>i</italic></sub> and standard deviation <italic>s</italic><sub><italic>i</italic></sub>. The probability that an event is drawn from a certain normal distribution is described by a weighting parameter <italic>w</italic><sub><italic>i</italic></sub> so that <inline-formula id="IEq13"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sum_{i=1}^{M}{w}_{i}=1$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4769_Article_IEq13.gif"/></alternatives></inline-formula>. Specifically, parameters <italic>m</italic><sub><italic>i</italic></sub>, <italic>s</italic><sub><italic>i</italic></sub>, and <italic>w</italic><sub><italic>i</italic></sub>, of the Gaussian mixtures M1, …, M4 were M1: m<sub>i</sub> = [− 9, − 3, 10], s<sub>i</sub> = [3, 4, 5], w<sub>i</sub> = [0.12, 0.05, 0.83], M2: m<sub>i</sub> = [− 2, 0, 5], s<sub>i</sub> = [2, 4, 2], w<sub>i</sub> = [0.39, 0.48, 0.13], M3: m<sub>i</sub> = [− 2, 0, 5], s<sub>i</sub> = [2, 4, 2], w<sub>i</sub> = [0.39, 0.48, 0.13], and M4: m<sub>i</sub> = [− 6, 0, 2], s<sub>i</sub> = [3, 1, 5], w<sub>i</sub> = [0.27, 0.06, 0.67]. This data set is available in the R library "EDOtrans" as “GMMartificialData”. Seven classes were obtained via combining the GMM decisions for all individual variables.</p>
      <p id="Par20">In a tenfold cross-validation scenario, random samples of n = 1000 instances were drawn from the original dataset with replacement by bootstrap resampling [<xref ref-type="bibr" rid="CR26">26</xref>]. Each variable (Fig. <xref rid="Fig3" ref-type="fig">3</xref> A) was the used untransformed, z-transformed, EDO-transformed after automatic detection of the number of modes using the R package "EDOtrans" described above, and PVS-transformed using the R script provided with the original publication of this method [<xref ref-type="bibr" rid="CR6">6</xref>]. Internally, this uses majority voting among several methods to determine the number of modes, was imported from the R package "NbClust" (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=NbClust">https://cran.r-project.org/package=NbClust</ext-link>), and then performs standard k-means clustering with 100 initial random seeds to determine the center of the initial clusters, and the results are then used to perform the PVS transformation.<fig id="Fig3"><label>Fig. 3</label><caption><p>Results of hierarchical cluster analysis of a four-dimensional data set with 1,000 instances created from Gaussian mixtures with M = 3 modes (“GMMartificialData”). <bold>A</bold> The original and transformed data (z-transformation, EDO transformation, and PVS transformation [<xref ref-type="bibr" rid="CR6">6</xref>]) are shown as a probability density function (PDF) estimated using the Pareto density estimation (PDE [<xref ref-type="bibr" rid="CR27">27</xref>]), which was developed as a nonparametric kernel density estimator to improve subgroup separation in mixtures. <bold>B</bold> Cluster quality and stability assessed a as cluster accuracy and adjusted Rand index [<xref ref-type="bibr" rid="CR21">21</xref>] against the prior classification of the data, and as Dunn’s index [<xref ref-type="bibr" rid="CR22">22</xref>]. The boxes were constructed using minimum, quartiles, median (solid line inside the box) and maximum. The whiskers add 1.5 times the inter-quartile range (IQR) to the 75th percentile or subtract 1.5 times the IQR from the 25th percentile. The figure has been created using the R software package (version 4.1.2 for Linux; <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/">https://CRAN.R-project.org/</ext-link> [<xref ref-type="bibr" rid="CR9">9</xref>]) and the R packages “ggplot2” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=ggplot2">https://cran.r-project.org/package=ggplot2</ext-link> [<xref ref-type="bibr" rid="CR10">10</xref>]), and “FactoMineR” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=FactoMineR">https://cran.r-project.org/package=FactoMineR</ext-link> [<xref ref-type="bibr" rid="CR16">16</xref>]). The colors were selected from the “colorblind_pal” palette provided with the R library “ggthemes” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=ggthemes">https://cran.r-project.org/package=ggthemes</ext-link> [<xref ref-type="bibr" rid="CR11">11</xref>])</p></caption><graphic xlink:href="12859_2022_4769_Fig3_HTML" id="MO3"/></fig></p>
      <p id="Par21">The results of hierarchical clustering using Ward's method showed that the clustering compared to the prior classification was best when EDO-transformed variables were used (Fig. <xref rid="Fig3" ref-type="fig">3</xref>B). For this dataset, the z-transformation and the PVS-transformation gave the poorest results. However, in the bootstrap scenario, the clustering solutions were generally modest, as indicated by the relatively low values of cluster accuracy and the Rand and Dunn’s indices. Replacing Ward's linkage with average or complete linkage did not change the results in terms of the relative impact of the data transformation methods used on the cluster quality (see Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figs. S1–S7).</p>
    </sec>
    <sec id="Sec10">
      <title>Application of EDO transformation for clustering of further artificial and real datasets</title>
      <sec id="Sec11">
        <title>Artificial data example</title>
        <p id="Par22">The “Lsun” dataset belongs to the so-called “Fundamental Clustering and Projection Suite” (FCPS), occasionally also referred to as “Fundamental Clustering Problems Suite”, of which the most comprehensive description has been published in [<xref ref-type="bibr" rid="CR28">28</xref>]. The data set consists of three well-separated data classes, but with different convex hulls: a sphere and two "bricks" of different size (insert C in Fig. <xref rid="Fig4" ref-type="fig">4</xref>). This structural property raises the problem of different variances or densities in the cluster. The original data set consists of n = 400 instances with d = 2 variables and k = 3 classes. To make the task slightly more difficult, in the present experiments the variables were included twice, one in the original version and again after random permutation.<fig id="Fig4"><label>Fig. 4</label><caption><p>Results of hierarchical cluster analysis of a modified version of the “Lsun” dataset form the “Fundamental Clustering and Projection Suite” (FCPS) [<xref ref-type="bibr" rid="CR28">28</xref>]. The data set n = 400 instances with d = 4 variables (X1–X4), of which 2 variables are original and two were the same variables but randomly permuted, and k = 3 classes (see insert <bold>C</bold>). <bold>A</bold> The original and transformed data (z-transformation, EDO transformation, and PVS transformation [<xref ref-type="bibr" rid="CR6">6</xref>]) are shown as a probability density function (PDF) estimated using the Pareto density estimation (PDE [<xref ref-type="bibr" rid="CR27">27</xref>]), which was developed as a nonparametric kernel density estimator to improve subgroup separation in mixtures. <bold>B</bold> Cluster quality and stability assessed a as cluster accuracy and adjusted Rand index [<xref ref-type="bibr" rid="CR21">21</xref>] against the prior classification of the data, and as Dunn’s index [<xref ref-type="bibr" rid="CR22">22</xref>]. The boxes were constructed using minimum, quartiles, median (solid line inside the box) and maximum. The whiskers add 1.5 times the inter-quartile range (IQR) to the 75th percentile or subtract 1.5 times the IQR from the 25th percentile. The figure has been created using the R software package (version 4.1.2 for Linux; <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/">https://CRAN.R-project.org/</ext-link> [<xref ref-type="bibr" rid="CR9">9</xref>]) and the R packages “ggplot2” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=ggplot2">https://cran.r-project.org/package=ggplot2</ext-link> [<xref ref-type="bibr" rid="CR10">10</xref>]), and “FactoMineR” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=FactoMineR">https://cran.r-project.org/package=FactoMineR</ext-link> [<xref ref-type="bibr" rid="CR16">16</xref>]). The colors were selected from the “colorblind_pal” palette provided with the R library “ggthemes” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=ggthemes">https://cran.r-project.org/package=ggthemes</ext-link> [<xref ref-type="bibr" rid="CR11">11</xref>])</p></caption><graphic xlink:href="12859_2022_4769_Fig4_HTML" id="MO4"/></fig></p>
        <p id="Par23">The results of hierarchical clustering using Ward's method showed that the best cluster accuracy and adjusted Rand index when the PVS-transformation was used, while the best Dunn’s index provided the EDO transformation (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). Both innovative methods outperformed the classical methods of z-transformation or using untransformed variables.</p>
      </sec>
      <sec id="Sec12">
        <title>Flow cytometric data example</title>
        <p id="Par24">Biomedical empirical data from flow cytometry using fluorescence-activated cell sorting (FACS) were available from a hematologic data set. For the present experiments, d = 4 variables including the value of the forward scatter (FS) and cytological makers (CD) called for nondisclosure reasons a, b and d, which were downsampled from originally n = 111,686 cells obtained from 100 patients with chronic lymphocytic leukemia and 100 healthy control subjects to n = 3,000 instances. This data set (Fig. <xref rid="Fig5" ref-type="fig">5</xref>A) is available in the R library "EDOtrans" as “FACSdata” and consist of a subsample of a larger data set published at <ext-link ext-link-type="uri" xlink:href="https://data.mendeley.com/datasets/jk4dt6wprv/1">https://data.mendeley.com/datasets/jk4dt6wprv/1</ext-link>, (accessed March 1, 2022) [<xref ref-type="bibr" rid="CR29">29</xref>]. The original study followed the Declaration of Helsinki and was approved by the Ethics Committee of Medical Faculty of the Phillips University of Marburg, Germany.<fig id="Fig5"><label>Fig. 5</label><caption><p>Results of hierarchical cluster analysis of a four-dimensional data set with 3,000 instances of flow cytometric (FACS) measurements modes (“FACSData”). <bold>A</bold> The original and transformed data (z-transformation, EDO transformation, and PVS transformation [<xref ref-type="bibr" rid="CR6">6</xref>]) are shown as a probability density function (PDF) estimated using the Pareto density estimation (PDE [<xref ref-type="bibr" rid="CR27">27</xref>]), which was developed as a nonparametric kernel density estimator to improve subgroup separation in mixtures. <bold>B</bold> Cluster quality and stability assessed a as cluster accuracy and adjusted Rand index [<xref ref-type="bibr" rid="CR21">21</xref>] against the prior classification of the data, and as Dunn’s index [<xref ref-type="bibr" rid="CR22">22</xref>]. The boxes were constructed using minimum, quartiles, median (solid line inside the box) and maximum. The whiskers add 1.5 times the inter-quartile range (IQR) to the 75th percentile or subtract 1.5 times the IQR from the 25th percentile. The figure has been created using the R software package (version 4.1.2 for Linux; <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/">https://CRAN.R-project.org/</ext-link> [<xref ref-type="bibr" rid="CR9">9</xref>]) and the R packages “ggplot2” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=ggplot2">https://cran.r-project.org/package=ggplot2</ext-link> [<xref ref-type="bibr" rid="CR10">10</xref>]), and “FactoMineR” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=FactoMineR">https://cran.r-project.org/package=FactoMineR</ext-link> [<xref ref-type="bibr" rid="CR16">16</xref>]). The colors were selected from the “colorblind_pal” palette provided with the R library “ggthemes” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=ggthemes">https://cran.r-project.org/package=ggthemes</ext-link> [<xref ref-type="bibr" rid="CR11">11</xref>])</p></caption><graphic xlink:href="12859_2022_4769_Fig5_HTML" id="MO5"/></fig></p>
        <p id="Par25">The results of hierarchical clustering using Ward's method showed that the clustering compared to the prior classification was again best when EDO-transformed variables were used (Fig. <xref rid="Fig5" ref-type="fig">5</xref>B). Z-transforming the individual variables resulted in poorer clustering results in terms of accuracy and Rand or Dunn’s indices than using the original, untransformed variables, while PVS-transforming resulted in clustering results comparable to those obtained with the untransformed dataset.</p>
      </sec>
      <sec id="Sec13">
        <title>Iris flower data example</title>
        <p id="Par26">The Iris flower data set was included for its wide use in statistics for testing of methods and because it was also used for the introductory simulation study in the report on the PVS method, which serves here as a comparative method [<xref ref-type="bibr" rid="CR6">6</xref>]. The Iris data set gives the measurements in centimeters of the four variables sepal length and width or petal length and width for 50 flowers each of the three species <italic>Iris setosa</italic>, <italic>versicolor</italic> and <italic>virginica</italic>. As there are apparently at least half a dozen different versions of this data set, it is necessary to specify that in the present analysis, the version implemented in R software package as “data(iris)” was used. Table <xref rid="Tab1" ref-type="table">1</xref> illustrates the effect of scaling with the proposed EDO transformation on this data and compares it with the SD and the range.<table-wrap id="Tab1"><label>Table 1</label><caption><p>The effect of variable scaling on the Iris flower data set</p></caption><graphic position="anchor" xlink:href="12859_2022_4769_Tab1_HTML" id="MO17"/><table-wrap-foot><p>Compare Table 1 in [<xref ref-type="bibr" rid="CR6">6</xref>]</p></table-wrap-foot></table-wrap></p>
        <p id="Par27">The results of hierarchical clustering using Ward's method (Fig. <xref rid="Fig6" ref-type="fig">6</xref>) resulted in an inverse ranking between EDO and PVS transformations as observed for the FACS dataset. The PVS method provided the best preprocessing when judged by cluster accuracy, the Rand index calculated against the prior classification and Dunn’s index. Nevertheless, both the EDO and PVS transforms outperformed the classical approaches, especially the z-transform before clustering.<fig id="Fig6"><label>Fig. 6</label><caption><p>Results of hierarchical cluster analysis of a four-dimensional data set with 150 instances of three species of Iris flower [<xref ref-type="bibr" rid="CR30">30</xref>, <xref ref-type="bibr" rid="CR31">31</xref>] (“Iris”). <bold>A</bold> The original and transformed data (z-transformation, EDO transformation, and PVS transformation [<xref ref-type="bibr" rid="CR6">6</xref>]) are shown as a probability density function (PDF) estimated using the Pareto density estimation (PDE [<xref ref-type="bibr" rid="CR27">27</xref>]), which was developed as a nonparametric kernel density estimator to improve subgroup separation in mixtures. <bold>B</bold> Cluster quality and stability assessed a as cluster accuracy, adjusted Rand index [<xref ref-type="bibr" rid="CR21">21</xref>] against the prior classification of the data, and as Dunn’s index [<xref ref-type="bibr" rid="CR22">22</xref>]. The boxes were constructed using minimum, quartiles, median (solid line inside the box) and maximum. The whiskers add 1.5 times the inter-quartile range (IQR) to the 75th percentile or subtract 1.5 times the IQR from the 25th percentile. The figure has been created using the R software package (version 4.1.2 for Linux; <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/">https://CRAN.R-project.org/</ext-link> [<xref ref-type="bibr" rid="CR9">9</xref>]) and the R packages “ggplot2” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=ggplot2">https://cran.r-project.org/package=ggplot2</ext-link> [<xref ref-type="bibr" rid="CR10">10</xref>]), and “FactoMineR” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=FactoMineR">https://cran.r-project.org/package=FactoMineR</ext-link> [<xref ref-type="bibr" rid="CR16">16</xref>]). The colors were selected from the “colorblind_pal” palette provided with the R library “ggthemes” (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=ggthemes">https://cran.r-project.org/package=ggthemes</ext-link> [<xref ref-type="bibr" rid="CR11">11</xref>])</p></caption><graphic xlink:href="12859_2022_4769_Fig6_HTML" id="MO6"/></fig></p>
      </sec>
      <sec id="Sec14">
        <title>Gene expression example</title>
        <p id="Par28">Another set of biomedical empirical data were the gene expression patterns of 65 surgical samples of human breast tumors provided in the supplementary materials of [<xref ref-type="bibr" rid="CR6">6</xref>]. The data originate from a publication of patterns in 496 intrinsic genes that showed significantly greater variation between different tumors than variation between paired samples of the same tumor, resulting in four distinct tumor types by applying hierarchical clustering, including (1) ER+/luminal-like, (2) basal-like, (3) hereditary B2+, and (4) normal breast [<xref ref-type="bibr" rid="CR32">32</xref>]. The dataset was used in a replication of the experiment conducted by Raymaekers and Zamar [<xref ref-type="bibr" rid="CR6">6</xref>], using their R script available at <ext-link ext-link-type="uri" xlink:href="https://wis.kuleuven.be/statdatascience/robust/Programs/pooledVariableScaling/pvs-r.zip">https://wis.kuleuven.be/statdatascience/robust/Programs/pooledVariableScaling/pvs-r.zip</ext-link> to compare the effects of different data transformations on the results of hierarchical clustering with average, complete, and Ward linkage. Figure <xref rid="Fig7" ref-type="fig">7</xref> shows the resulting dendrograms when applying each of these clustering algorithms to the dataset after different scaling. For average linkage, EDO scaling outperformed classical transformations or non-transformations, and its results were at the same level as PVS scaling. For complete linkage, EDO scaling misclassified three observations, PVS scaling was wrong for five observations, while range scaling yielded only two errors. This was reversed for Ward clustering, as the EDO transformation produced two more errors than the PVS, but both outperformed the other options.<fig id="Fig7"><label>Fig. 7</label><caption><p>The effect of variable scaling on the gene expression data. The data set comprised 65 surgical samples of human breast tumors in which hierarchical clustering of the expression of 496 intrinsic genes that showed significantly greater variation between different tumors than variation between paired samples of the same tumor had resulted in four distinct tumor types [<xref ref-type="bibr" rid="CR32">32</xref>]. The dendrogram colors correspond to the tumor type: basal-like in red, Erb-B2þ in green, normal-breast-like in dark blue and luminal epithelial/ERþ in cyan. The EDO transformation generally yields superior recovery of the true clusters, comparable with the PVS transformation. The experiment is a re-run of the experiment performed for Fig. 3 in [<xref ref-type="bibr" rid="CR6">6</xref>], using the R script </p><p>available at <ext-link ext-link-type="uri" xlink:href="https://wis.kuleuven.be/statdatascience/robust/Programs/pooledVariableScaling/pvs-r.zip">https://wis.kuleuven.be/statdatascience/robust/Programs/pooledVariableScaling/pvs-r.zip</ext-link> with addition of code implementing the present EDO transformation</p></caption><graphic xlink:href="12859_2022_4769_Fig7_HTML" id="MO7"/></fig></p>
        <p id="Par29">Using the clustering algorithms chosen by [<xref ref-type="bibr" rid="CR6">6</xref>] for the previous datasets (Gaussian mixture, "Lsun," FACS data, and iris flower data) also revealed a heterogeneous picture, although a tendency for clustering to improve over classical methods also prevailed for average and complete linkage when all three measures of cluster quality, i.e., cluster accuracy, Rand index, and Dunn’s index, were considered (see Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figures). It should be noted, however, that average and complete linkage seemed to benefit less from the EDO or PVS transformation than Ward’s linkage.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec15">
    <title>Discussion</title>
    <p id="Par30">The present experiments have shown that a data transformation that takes into account the inherent properties of the Euclidean distance metric by addressing the inflection point at 1 from distance decreasing to distance increasing effects, as well as the N(0,<inline-formula id="IEq14"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sqrt{2}$$\end{document}</tex-math><mml:math id="M38"><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:math><inline-graphic xlink:href="12859_2022_4769_Article_IEq14.gif"/></alternatives></inline-formula>) distribution of innerclass distances, can improve the clustering of multidimensional data. Since Euclidean distance is by far the most used distance metric for data projection and subgroup assessment, it is virtually the standard implemented by default in statistical software and rarely specifically mentioned in scientific reports. Therefore, the present proposal of an improved data transformation adapted to this distance metric is relevant for the analysis of biomedical or other data sets.</p>
    <p id="Par31">All non-trivial analyses of multivariate (high-dimensional) data require a distance function (metric) to allow the comparison of cases. There are more than a thousand different distance functions besides the Euclidean metric (for an overview, see [<xref ref-type="bibr" rid="CR33">33</xref>]). Selecting a suitable distance function is crucial for visualizing the data, i.e., projecting the high-dimensional data into two or three dimensions, and for identifying subgroups or clusters in the data (clustering). Clustering aims to group the cases into a finite number of clusters, in such a way that the objects in one cluster are more similar to each other and more dissimilar to the cases in other clusters. Similarities and dissimilarities are determined by the distance function. The selection of a meaningful and appropriate distance function is therefore the key issue in the analysis of complex multivariate data. Unfortunately, there are few theoretical properties that can be used to identify an appropriate distance function. Moreover, if too many seemingly simple properties such as "identity of indistinguishable data" and "scale invariance" are required of the metric, it can even be shown that subsequent data analysis such as clustering is impossible [<xref ref-type="bibr" rid="CR34">34</xref>].</p>
    <p id="Par32">However, some requirements for a distance function are essential. The first is translation invariance. That is, it should not matter where the origin of the high-dimensional data space lies. This is equivalent to the requirement that the addition or subtraction of a constant to any of the dimensions of the data should not change the (dis-)similarities within the data. For metric distances, three axioms must be satisfied for this to happen: Identity of indiscernibles (= non-negativity), symmetry, and the triangle inequality. The postulation of segment additivity [<xref ref-type="bibr" rid="CR35">35</xref>] reduces the admissible distance functions d(x,y) between data points <italic>x</italic> and <italic>y</italic> to Minkowski distances [<xref ref-type="bibr" rid="CR36">36</xref>]. These have the general formula:<disp-formula id="Equa"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${d\left(x,y\right)}_{m}={\left(\sum_{i=1}^{d}{\left|{x}_{i}-{y}_{i}\right|}^{m}\right)}^\frac{1}{m}$$\end{document}</tex-math><mml:math id="M40" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:mfenced close=")" open="("><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mfenced></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>d</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mfenced close="|" open="|"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mi>m</mml:mi></mml:msup></mml:mfenced></mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:mfrac></mml:msup></mml:mrow></mml:math><graphic xlink:href="12859_2022_4769_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p>
    <p id="Par33">Among these Minkowski distances, the Euclidean distance, with m = 2, is the only metric invariant to orthogonal rotations of the coordinate system. Moreover, the Euclidean distance is experienced in the everyday 3-dimensional world. However, it is often ignored that Euclidean distance has a property that is critical to the success or failure of clustering: it is not invariant to the scaling of the data. As explained above, this means that the similarity or dissimilarity of cases in the data depends on whether the Euclidean distance was applied to the data in its original form or to variables that were transformed (scaled), even if a common scaling factor was used for all variables.</p>
    <p id="Par34">Furthermore, for any comparison of different dimensions, the dispersion, also called scatter or variance, of the variables is important, i.e., a measure of how different the data are within the respective dimension. A key issue in choosing an appropriate distance function is to make a rational choice of this comparison between the different scatters. The standard choice for this is to z-standardize the data. The implicit assumption in using the z-standardization as a scaling prior to applying the Euclidean distance is that the standard deviation (variance) is an appropriate description of the dispersion of the variables. This is valid for simple distributions, i.e., Gaussian-like distributions. However, it encounters problems when the data are less simply distributed, such as multimodal data. For example, an 80/20 (weights = [0.8, 0.2]) bimodally distributed variable with means = [− 5, 5] and small standard deviations of 0.5 for both modes indicates two separate groups that are in themselves quite homogeneous. However, their joint standard division is 4.0 compared to 0.5 for the two separate modes. Therefore, the z-standardization uses a large span, resulting in low values that may become relevant in subsequent projections of the data, considering the scale sensitivity of the Euclidean distance metric. Here, the EDO transformation, by using only the dispersion of the dominant variable, i.e., a standard deviation of 0.5 multiplied with the value of <inline-formula id="IEq15"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sqrt{2}$$\end{document}</tex-math><mml:math id="M42"><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:math><inline-graphic xlink:href="12859_2022_4769_Article_IEq15.gif"/></alternatives></inline-formula>, leads to larger transformed data, which has consequences for the subsequent application of Euclidean distances in projecting and clustering procedures.</p>
    <p id="Par35">It should be noted that for empirical data with skewed distributions such as exponential or lognormal distributions, it is recommended to transform the data towards normality before applying the EDO algorithm. An example of such a transformation is the Box-Cox transformation [<xref ref-type="bibr" rid="CR37">37</xref>]. Such a transformation also often eliminates outliers in the original distributions. The EDO transformation is likely to be susceptible to outliers. Treatment of outliers should also be done before applying the EDO method or any other range transformation. To keep this paper short and sweet, we assume that skewed distributions and outliers are part of "pre-processing" before applying a transformation such as the Z or EDO transformation.</p>
    <p id="Par36">The presently proposed EDO transform can be considered as an alternative to the also recently proposed PVS transformation [<xref ref-type="bibr" rid="CR6">6</xref>], which was developed with a similar goal of adapting the data transformation during preprocessing for clustering to the scale dependence of the Euclidean distance. Neither transformation was always ranked first in the present cluster experiments, while in most cases of the present experimentation both methods were superior to using untransformed data or z-standardization as a preprocessing approach. However, the PVS and EDO transforms differ in their underlying theoretical considerations. That is, PVS assumes that the k-means algorithm yields a valid clustering of the high-dimensional data. This is equivalent to assuming that all such clusters are in the form of hyperspheres and that their decision boundaries are hyperplanes [<xref ref-type="bibr" rid="CR38">38</xref>]. K-means is a gradient descent algorithm and is therefore sensitive to the specification of points and any local minimal solutions. In practical situations, it is impossible to confirm or falsify this model assumption. EDO exploits the often-overlooked fundamental property of Euclidean distances d within (d &lt; 1) and between classes (d &gt; 1). For high-dimensional data, it is assumed that a valid classification of the data can be modeled as independent Bayesian models using Gaussian mixtures (GMM) [<xref ref-type="bibr" rid="CR39">39</xref>]. The resulting model for the decision boundaries of the high-dimensional classes are conic sections. Algorithms for fitting the GMM in each dimension, such as expectation maximization, are typically also gradient descent methods but their results can be confirmed or falsified for each variable.</p>
  </sec>
  <sec id="Sec16">
    <title>Conclusions</title>
    <p id="Par37">The Euclidean distance has a peculiarity that is well known but less considered in practice. Namely, the Euclidean distance is not invariant to the scaling of the variables of the data set. The reason for this is the squaring of the differences of the data when calculating the Euclidean distance. The squaring function has the property that it changes its behavior exactly at the value 1: Differences smaller than 1 are reduced, differences larger than 1 are overemphasized quadratically. The EDO transformation takes advantage of this particularity: The scaling of the data is adjusted to map differences of data points within the same group (mode, clustering) to a range smaller than 1. EDO itself is neither a distance nor a clustering algorithm, but is intended as a more reasonable transformation than, for example usual standardization (Z-transformation). The EDO transformation is especially recommended when the Euclidean distance is used in later steps of multivariate data analysis, such as projections, visualizations, or clustering.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Information</title>
    <sec id="Sec17">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2022_4769_MOESM1_ESM.docx">
            <caption>
              <p><bold>Additional file 1.</bold> Supplemental figures, showing the results of the proof-of-concept study using PAM clustering instead of k-means, and results of the three experiments with the data sets of Gaussian mixtures, Iris flowers and FACS data, using average or complete linkage instead of Ward’s linkage.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Not applicable.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>AU—Conceptualization of the project, mathematical implementation, data analysis and writing of the manuscript, revision of the first submission. JL—Conceptualization of the project, programming, writing of the manuscript, data analyses and creation of the figures, revision of the first submission. Both authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Open Access funding enabled and organized by Projekt DEAL. This work has been funded by the Landesoffensive zur Entwicklung wissenschaftlich-ökonomischer Exzellenz (LOEWE), LOEWE-Zentrum für Translationale Medizin und Pharmakologie (JL), in particular through the project “Reproducible cleaning of biomedical laboratory data using methods of visualization, error correction and transformation implemented as interactive R-notebooks” (JL).). JL was supported by the Deutsche Forschungsgemeinschaft (DFG LO 612/16-1).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The “EDOtrans” R package is freely available at <ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=EDOtrans">https://cran.r-project.org/package=EDOtrans</ext-link>. It contains the data sets used in this report and not elsewhere available as referenced. Results of the proof-of-concept study using PAM clustering instead of k-means, and results of the four experiments with the data sets of Gaussian mixtures, Iris flowers, FACS data and “Lsun”, using average or complete linkage instead of Ward’s linkage, are provided as Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figures.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1">
      <title>Ethics approval and consent to participate</title>
      <p id="Par38">Data have been either taken from publicly available sources or, in case of own data (FACS example), the original study followed the Declaration of Helsinki and was approved by the Ethics Committee of Medical Faculty of the Phillips University of Marburg, Germany.</p>
    </notes>
    <notes id="FPar2">
      <title>Consent for publication</title>
      <p id="Par39">Not applicable.</p>
    </notes>
    <notes id="FPar3" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par40">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lötsch</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ultsch</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Current projection methods-induced biases at subgroup detection for machine-learning based data-analysis of biomedical data</article-title>
        <source>Int J Mol Sci</source>
        <year>2019</year>
        <volume>21</volume>
        <issue>1</issue>
        <fpage>79</fpage>
        <pub-id pub-id-type="doi">10.3390/ijms21010079</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ultsch</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lötsch</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Machine-learned cluster identification in high-dimensional data</article-title>
        <source>J Biomed Inform</source>
        <year>2017</year>
        <volume>66</volume>
        <fpage>95</fpage>
        <lpage>104</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2016.12.011</pub-id>
        <pub-id pub-id-type="pmid">28040499</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Hair</surname>
            <given-names>JF</given-names>
          </name>
        </person-group>
        <source>Multivariate data analysis</source>
        <year>2019</year>
        <publisher-loc>Boston</publisher-loc>
        <publisher-name>Cengage</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>IR</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>AY</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>JYH</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Impact of similarity metrics on single-cell RNA-seq data clustering</article-title>
        <source>Brief Bioinform</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>6</issue>
        <fpage>2316</fpage>
        <lpage>2326</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/bby076</pub-id>
        <pub-id pub-id-type="pmid">30137247</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Hurewicz</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>James</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Nichols</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>James</surname>
            <given-names>HM</given-names>
          </name>
          <name>
            <surname>Nichols</surname>
            <given-names>NB</given-names>
          </name>
          <name>
            <surname>Phillips</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Phillips</surname>
            <given-names>RS</given-names>
          </name>
        </person-group>
        <article-title>Filters and servo systems with pulsed data</article-title>
        <source>Theory of servomechanisms</source>
        <year>1947</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>McGraw-Hill</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Raymaekers</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zamar</surname>
            <given-names>RH</given-names>
          </name>
        </person-group>
        <article-title>Pooled variable scaling for cluster analysis</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <issue>12</issue>
        <fpage>3849</fpage>
        <lpage>3855</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa243</pub-id>
        <pub-id pub-id-type="pmid">32282889</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cauteruccio</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Terracina</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Ursino</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Generalizing identity-based string comparison metrics: framework and techniques</article-title>
        <source>Knowl-Based Syst</source>
        <year>2020</year>
        <volume>187</volume>
        <fpage>104820</fpage>
        <pub-id pub-id-type="doi">10.1016/j.knosys.2019.06.028</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lellouche</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Souris</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Distribution of distances between elements in a compact set</article-title>
        <source>Stats</source>
        <year>2020</year>
        <volume>3</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>15</lpage>
        <pub-id pub-id-type="doi">10.3390/stats3010001</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <collab>R Core Team</collab>
        </person-group>
        <source>R: a language and environment for statistical computing</source>
        <year>2021</year>
        <publisher-loc>Vienna</publisher-loc>
        <publisher-name>R Core Team</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Wickham</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <source>ggplot2: elegant graphics for data analysis</source>
        <year>2009</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Arnold JB. ggthemes: extra themes, scales and geoms for 'ggplot2'. 2019.</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ultsch</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lötsch</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Computed ABC analysis for rational selection of most informative variables in multivariate data</article-title>
        <source>PLoS ONE</source>
        <year>2015</year>
        <volume>10</volume>
        <issue>6</issue>
        <fpage>e0129767</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0129767</pub-id>
        <pub-id pub-id-type="pmid">26061064</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Juran</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <article-title>The non-Pareto principle, Mea culpa</article-title>
        <source>Qual Prog</source>
        <year>1975</year>
        <volume>8</volume>
        <issue>5</issue>
        <fpage>8</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">MacQueen J. Some methods for classification and analysis of multivariate observations. In: Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, volume 1: statistics. Berkeley: University of California Press; 1967. p. 281–97.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sidiropoulos</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Sohi</surname>
            <given-names>SH</given-names>
          </name>
          <name>
            <surname>Pedersen</surname>
            <given-names>TL</given-names>
          </name>
          <name>
            <surname>Porse</surname>
            <given-names>BT</given-names>
          </name>
          <name>
            <surname>Winther</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Rapin</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Bagger</surname>
            <given-names>FO</given-names>
          </name>
        </person-group>
        <article-title>SinaPlot: an enhanced chart for simple and truthful representation of single observations over multiple classes</article-title>
        <source>J Comput Graph Stat</source>
        <year>2018</year>
        <volume>27</volume>
        <issue>3</issue>
        <fpage>673</fpage>
        <lpage>676</lpage>
        <pub-id pub-id-type="doi">10.1080/10618600.2017.1366914</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Le</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Josse</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Husson</surname>
            <given-names>FC</given-names>
          </name>
        </person-group>
        <article-title>FactoMineR: a package for multivariate analysis</article-title>
        <source>J Stat Softw</source>
        <year>2008</year>
        <volume>25</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>18</lpage>
        <pub-id pub-id-type="doi">10.18637/jss.v025.i01</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ihaka</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Gentleman</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>R: a language for data analysis and graphics</article-title>
        <source>J Comput Graph Stat</source>
        <year>1996</year>
        <volume>5</volume>
        <issue>3</issue>
        <fpage>299</fpage>
        <lpage>314</lpage>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Kaufman L, Rousseeuw PJ. Partitioning around medoids (program PAM). In: Finding groups in data. 1990. p. 68–125.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ward</surname>
            <given-names>JH</given-names>
            <suffix>Jr</suffix>
          </name>
        </person-group>
        <article-title>Hierarchical grouping to optimize an objective function</article-title>
        <source>J Am Stat Assoc</source>
        <year>1963</year>
        <volume>58</volume>
        <issue>301</issue>
        <fpage>236</fpage>
        <lpage>244</lpage>
        <pub-id pub-id-type="doi">10.1080/01621459.1963.10500845</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Maechler M, Rousseeuw P, Struyf A, Hubert M, Hornik K. Cluster: cluster analysis basics and extensions. 2017.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rand</surname>
            <given-names>WM</given-names>
          </name>
        </person-group>
        <article-title>Objective criteria for the evaluation of clustering methods</article-title>
        <source>J Am Stat Assoc</source>
        <year>1971</year>
        <volume>66</volume>
        <issue>336</issue>
        <fpage>846</fpage>
        <lpage>850</lpage>
        <pub-id pub-id-type="doi">10.1080/01621459.1971.10482356</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dunn</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>Well-separated clusters and optimal fuzzy partitions</article-title>
        <source>J Cybern</source>
        <year>1974</year>
        <volume>4</volume>
        <issue>1</issue>
        <fpage>95</fpage>
        <lpage>104</lpage>
        <pub-id pub-id-type="doi">10.1080/01969727408546059</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vavrek</surname>
            <given-names>MJ</given-names>
          </name>
        </person-group>
        <article-title>fossil: palaeoecological and palaeogeographical analysis tools</article-title>
        <source>Palaeontol Electron</source>
        <year>2011</year>
        <volume>14</volume>
        <issue>1</issue>
        <fpage>1T</fpage>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pihur</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Datta</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Datta</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>clValid: an R package for cluster validation</article-title>
        <source>J Stat Softw</source>
        <year>2008</year>
        <volume>25</volume>
        <issue>4</issue>
        <fpage>22</fpage>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ultsch</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Thrun</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Hansen-Goos</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Lötsch</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Identification of molecular fingerprints in human heat pain thresholds by use of an interactive mixture model R toolbox (AdaptGauss)</article-title>
        <source>Int J Mol Sci</source>
        <year>2015</year>
        <volume>16</volume>
        <issue>10</issue>
        <fpage>25897</fpage>
        <lpage>25911</lpage>
        <pub-id pub-id-type="doi">10.3390/ijms161025897</pub-id>
        <pub-id pub-id-type="pmid">26516852</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Efron</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>RJ</given-names>
          </name>
        </person-group>
        <source>An introduction to the bootstrap</source>
        <year>1995</year>
        <publisher-loc>San Francisco</publisher-loc>
        <publisher-name>Chapman and Hall</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Ultsch A. Pareto density estimation: a density estimation for knowledge discovery. In: Innovations in classification, data science, and information systems—proceedings 27th annual conference of the German classification society (GfKL). Berlin: Springer; 2003.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ultsch</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lötsch</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>The fundamental clustering and projection suite (FCPS): a dataset collection to test the performance of clustering and data projection algorithms</article-title>
        <source>Data</source>
        <year>2020</year>
        <volume>5</volume>
        <issue>1</issue>
        <fpage>13</fpage>
        <pub-id pub-id-type="doi">10.3390/data5010013</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Thrun M, Hoffmann J, Röhnert M, von Bonin M, Oelschlägel U, Brendel C, Ultsch A. Flow cytometry datasets consisting of peripheral blood and bone marrow samples for the evaluation of explainable artificial intelligence methods. In: Mendeley data. 2022.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fisher</surname>
            <given-names>RA</given-names>
          </name>
        </person-group>
        <article-title>The use of multiple measurements in taxonomic problems</article-title>
        <source>Ann Eugen</source>
        <year>1936</year>
        <volume>7</volume>
        <issue>2</issue>
        <fpage>179</fpage>
        <lpage>188</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1469-1809.1936.tb02137.x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Anderson</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>The irises of the Gaspé peninsula</article-title>
        <source>Bull Am Iris Soc</source>
        <year>1935</year>
        <volume>59</volume>
        <fpage>2</fpage>
        <lpage>5</lpage>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Perou</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Sørlie</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Eisen</surname>
            <given-names>MB</given-names>
          </name>
          <name>
            <surname>van de Rijn</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Jeffrey</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Rees</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Pollack</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Ross</surname>
            <given-names>DT</given-names>
          </name>
          <name>
            <surname>Johnsen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Akslen</surname>
            <given-names>LA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Molecular portraits of human breast tumours</article-title>
        <source>Nature</source>
        <year>2000</year>
        <volume>406</volume>
        <issue>6797</issue>
        <fpage>747</fpage>
        <lpage>752</lpage>
        <pub-id pub-id-type="doi">10.1038/35021093</pub-id>
        <pub-id pub-id-type="pmid">10963602</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ontañón</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>An overview of distance and similarity functions for structured data</article-title>
        <source>Artif Intell Rev</source>
        <year>2020</year>
        <volume>53</volume>
        <issue>7</issue>
        <fpage>5309</fpage>
        <lpage>5351</lpage>
        <pub-id pub-id-type="doi">10.1007/s10462-020-09821-w</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Kleinberg J. An impossibility theorem for clustering. In: Proceedings of the 15th international conference on neural information processing systems. MIT Press; 2002. p. 463–70.</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Borsuk K, Szmielew W, Marquit E. Foundations of geometry: Euclidean, Bolyai-Lobachevskian, and projective geometry. 2018.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Minkowski</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Die Grundgleichungen für die elektromagnetischen Vorgänge in bewegten Körpern</article-title>
        <source>Math Ann</source>
        <year>1910</year>
        <volume>68</volume>
        <issue>4</issue>
        <fpage>525</fpage>
        <pub-id pub-id-type="doi">10.1007/BF01455871</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Box</surname>
            <given-names>GE</given-names>
          </name>
          <name>
            <surname>Cox</surname>
            <given-names>DR</given-names>
          </name>
        </person-group>
        <article-title>An analysis of transformations</article-title>
        <source>J R Stat Soc Ser B (Methodol)</source>
        <year>1964</year>
        <volume>26</volume>
        <fpage>211</fpage>
        <lpage>252</lpage>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Press</surname>
            <given-names>WH</given-names>
          </name>
          <name>
            <surname>Teukolsky</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Vetterling</surname>
            <given-names>WT</given-names>
          </name>
          <name>
            <surname>Flannery</surname>
            <given-names>BP</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Press</surname>
            <given-names>WH</given-names>
          </name>
          <name>
            <surname>Teukolsky</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Vetterling</surname>
            <given-names>WT</given-names>
          </name>
          <name>
            <surname>Flannery</surname>
            <given-names>BP</given-names>
          </name>
        </person-group>
        <article-title>Gaussian mixture models and k-means clustering</article-title>
        <source>Numerical recipes: the art of scientific computing</source>
        <year>2007</year>
        <edition>3</edition>
        <publisher-loc>Cambridge</publisher-loc>
        <publisher-name>Cambridge University Press</publisher-name>
        <fpage>30</fpage>
        <lpage>31</lpage>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hand</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Idiot's Bayes: not so stupid after all?</article-title>
        <source>Int Stat Rev/Revue Internationale de Statistique</source>
        <year>2001</year>
        <volume>69</volume>
        <issue>3</issue>
        <fpage>385</fpage>
        <lpage>398</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
