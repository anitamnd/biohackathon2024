<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6690869</article-id>
    <article-id pub-id-type="publisher-id">48242</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-019-48242-w</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>NGSReadsTreatment – A Cuckoo Filter-based Tool for Removing Duplicate Reads in NGS Data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Gaia</surname>
          <given-names>Antonio Sérgio Cruz</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0398-209X</contrib-id>
        <name>
          <surname>de Sá</surname>
          <given-names>Pablo Henrique Caracciolo Gomes</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>de Oliveira</surname>
          <given-names>Mônica Silva</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7227-0590</contrib-id>
        <name>
          <surname>Veras</surname>
          <given-names>Adonney Allan de Oliveira</given-names>
        </name>
        <address>
          <email>allanveras@ufpa.br</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2171 5249</institution-id><institution-id institution-id-type="GRID">grid.271300.7</institution-id><institution>Postgraduate Program in Applied Computing, Federal University of Pará (UFPA), </institution></institution-wrap>Pará, Brazil </aff>
      <aff id="Aff2"><label>2</label>Federal Rural University of Amazonia Campus Tomé-Açu (UFRA), Pará, Brazil </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>12</day>
      <month>8</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>12</day>
      <month>8</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>9</volume>
    <elocation-id>11681</elocation-id>
    <history>
      <date date-type="received">
        <day>16</day>
        <month>10</month>
        <year>2018</year>
      </date>
      <date date-type="accepted">
        <day>1</day>
        <month>8</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">The Next-Generation Sequencing (NGS) platforms provide a major approach to obtaining millions of short reads from samples. NGS has been used in a wide range of analyses, such as for determining genome sequences, analyzing evolutionary processes, identifying gene expression and resolving metagenomic analyses. Usually, the quality of NGS data impacts the final study conclusions. Moreover, quality assessment is generally considered the first step in data analyses to ensure the use of only reliable reads for further studies. In NGS platforms, the presence of duplicated reads (redundancy) that are usually introduced during library sequencing is a major issue. These might have a serious impact on research application, as redundancies in reads can lead to difficulties in subsequent analysis (e.g., <italic>de novo</italic> genome assembly). Herein, we present NGSReadsTreatment, a computational tool for the removal of duplicated reads in paired-end or single-end datasets. NGSReadsTreatment can handle reads from any platform with the same or different sequence lengths. Using the probabilistic structure Cuckoo Filter, the redundant reads are identified and removed by comparing the reads with themselves. Thus, no prerequisite is required beyond the set of reads. NGSReadsTreatment was compared with other redundancy removal tools in analyzing different sets of reads. The results demonstrated that NGSReadsTreatment was better than the other tools in both the amount of redundancies removed and the use of computational memory for all analyses performed. Available in <ext-link ext-link-type="uri" xlink:href="https://sourceforge.net/projects/ngsreadstreatment/">https://sourceforge.net/projects/ngsreadstreatment/</ext-link>.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Computational platforms and environments</kwd>
      <kwd>Software</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par2">The advent of Next-Generation Sequencing (NGS) technologies in mid-2005 provided significant breakthroughs in the omics fields. These platforms can generate millions of reads in a short time; for instance, Illumina NextSeq is capable of generating 400 million reads per round. The genomic library must be prepared prior to the actual sequencing and one task included in this stage is polymerase chain reaction (PCR) amplification<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>.</p>
    <p id="Par3">PCR generates a super-representation of a sample fragment, giving rise to the concept of coverage, where the genetic material of an organism to be sequenced presents a several-fold multiplication of its expected size. This super-representation is important for several analyses, including frameshift curation and single nucleotide polymorphism analyses, among others<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>.</p>
    <p id="Par4">However, some analyses are impacted by this super-representation, such as <italic>de novo</italic> assembly and the final scaffolding process. Moreover, the tasks demand a high computational cost, and duplication gives rise to false positives with overlapping contigs, as well as their subsequent extension due to the high number of connections. Consequently, false negatives arise as a result of the overlapping conflicts generated by the duplications<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>.Thus, the development of computational methods that can remove sequencing read redundancies is important. Several software solutions have been developed over the years to address this situation. GPU-DupRemoval (by Removing GPU Duplicates) aims to remove duplicate reads using graphical processing units (GPUs) generated with the Illumina platform. The task is divided into two phases: the clustering of possible duplicate sequences according to their prefix, followed by comparison of the sequence suffixes in each cluster to detect and remove redundancies<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>.</p>
    <p id="Par5">The FASTX-Toolkit Collapser (<ext-link ext-link-type="uri" xlink:href="http://hannonlab.cshl.edu/fastx_toolkit">http://hannonlab.cshl.edu/fastx_toolkit</ext-link>), FastUniq<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, Fulcrum<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>, and CD-HIT<sup><xref ref-type="bibr" rid="CR6">6</xref></sup> tools employ an alignment-free strategy. FASTX-Toolkit Collapser is able to identify and remove identical sequences from single-end reads. FastUniq, on the other hand, is designed to remove identical duplicates in three steps: initially, all paired reads are loaded into the memory; subsequently, the read pairs are sorted, and finally the duplicate sequences are identified by comparing the adjacent read pairs in the sorted list.</p>
    <p id="Par6">Fulcrum is able to identify duplicates that are fully or partially identical. Reads identified as possible duplicates are kept in different files, whose maximum size is defined by the user. The read sequences within each file are compared to identify duplicates<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>.</p>
    <p id="Par7">CD-HIT has two different tools for removing duplicates of single-end and paired-end reads generated with the Illumina platform. CD-HIT-454 parses libraries generated with 454 to identify exactly identical duplicates<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>.</p>
    <p id="Par8">The majority of the existing tools are designed to serve a particular sequencing platform. Thus, we present the NGSReadsTreatment tool for the removal of read redundancies for any NGS platform, based on the probabilistic structure of Cuckoo Filter.</p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results and Discussion</title>
    <p id="Par9">The reads sets of the sixteen organisms (real datasets) were processed using the FastUniq 1.1<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, ParDRe 2.2.5<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, MarDre 1.3<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, CD-HIT-DUP 4.6.86<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>, Clumpify (<ext-link ext-link-type="uri" xlink:href="https://sourceforge.net/projects/bbmap">https://sourceforge.net/projects/bbmap</ext-link>), and NGSReadsTreatment computational tools. The percentage of redundancy removal for each organism as well as an evaluation of the total memory used per tool is shown in Tables <xref rid="Tab1" ref-type="table">1</xref> and <xref rid="Tab2" ref-type="table">2</xref>, respectively.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Percentage of read redundancy removal per tool for each organism. NP - not processed owing to errors.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Organism</th><th>FastUniq 1.1</th><th>ParDRe 2.2.5</th><th>MarDre 1.3</th><th>CD-HIT-DUP 4.6.8</th><th>Clumpify (bbmap)</th><th>NGSReadsTreatment</th></tr></thead><tbody><tr><td>SRR2014554</td><td>NP</td><td>NP</td><td>NP</td><td>NP</td><td>NP</td><td>0.29%</td></tr><tr><td>ERR007646</td><td>50.55%</td><td>50.55%</td><td>50.55%</td><td>50.55%</td><td>50.65%</td><td>50.50%</td></tr><tr><td>SRR2000272</td><td>0.82%</td><td>0.81%</td><td>NP</td><td>0.81%</td><td>0.95%</td><td>1.91%</td></tr><tr><td>SRR1424625</td><td>0%</td><td>0%</td><td>0%</td><td>0%</td><td>0.20%</td><td>0.94%</td></tr><tr><td>SRR933487</td><td>0.72%</td><td>0.72%</td><td>0.72%</td><td>0.72%</td><td>1.11%</td><td>1.90%</td></tr><tr><td>SRR6479489</td><td>0.14%</td><td>0.14%</td><td>0.13%</td><td>0.14%</td><td>0.19%</td><td>1.21%</td></tr><tr><td>SRR6479482</td><td>0.14%</td><td>0.14%</td><td>0.14%</td><td>0.14%</td><td>0.18%</td><td>1.17%</td></tr><tr><td>SRR974839</td><td>48.74%</td><td>48.74%</td><td>48.74%</td><td>48.74%</td><td>49.07%</td><td>49.08%</td></tr><tr><td>SRR1144800</td><td>0.06%</td><td>0.06%</td><td>0.06%</td><td>0.06%</td><td>0.10%</td><td>0.94%</td></tr><tr><td>SRR7587111</td><td>NP</td><td>0.37%</td><td>0.37%</td><td>0.37%</td><td>0.43%</td><td>0.87%</td></tr><tr><td>SRR7819959</td><td>NP</td><td>0.74%</td><td>0.74%</td><td>NP</td><td>0.80%</td><td>1.36%</td></tr><tr><td>ERR2375157</td><td>NP</td><td>0.07%</td><td>0.07%</td><td>NP</td><td>0.08%</td><td>1.55%</td></tr><tr><td>SRR6799098</td><td>NP</td><td>2.11%</td><td>2.11%</td><td>2.1%</td><td>2.22%</td><td>2.22%</td></tr><tr><td>SRR7905974</td><td>NP</td><td>0%</td><td>NP</td><td>NP</td><td>0%</td><td>0.13%</td></tr><tr><td>SRR7739756</td><td>NP</td><td>0%</td><td>NP</td><td>NP</td><td>0%</td><td>0.08%</td></tr><tr><td>ERR2162181</td><td>NP</td><td>0%</td><td>0%</td><td>0%</td><td>NP</td><td>0.33%</td></tr></tbody></table></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>Memory amount used by each tool in megabyte. NP - not processed owing to errors.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Organism</th><th>FastUniq 1.1</th><th>ParDRe 2.2.5</th><th>MarDre 1.3</th><th>CD-HIT-DUP 4.6.8</th><th>Clumpify (bbmap)</th><th>NGSReadsTreatment</th></tr></thead><tbody><tr><td>SRR2014554</td><td>NP</td><td>NP</td><td>NP</td><td>NP</td><td>NP</td><td>549</td></tr><tr><td>ERR007646</td><td>3987</td><td>5387</td><td>1653</td><td>5393</td><td>870</td><td>543</td></tr><tr><td>SRR2000272</td><td>1722</td><td>2278</td><td>NP</td><td>3076</td><td>423</td><td>537</td></tr><tr><td>SRR1424625</td><td>2571</td><td>3586</td><td>1676</td><td>4097</td><td>455</td><td>539</td></tr><tr><td>SRR933487</td><td>1449</td><td>1950</td><td>1411</td><td>2063</td><td>2215</td><td>538</td></tr><tr><td>SRR6479489</td><td>2629</td><td>3629</td><td>1652</td><td>4313</td><td>3454</td><td>538</td></tr><tr><td>SRR6479482</td><td>2783</td><td>3850</td><td>1647</td><td>4501</td><td>3616</td><td>538</td></tr><tr><td>SRR974839</td><td>2725</td><td>3825</td><td>1634</td><td>4499</td><td>2625</td><td>540</td></tr><tr><td>SRR1144800</td><td>2633</td><td>3730</td><td>1653</td><td>4351</td><td>3250</td><td>540</td></tr><tr><td>SRR7587111</td><td>NP</td><td>888</td><td>1118</td><td>1561</td><td>769</td><td>533</td></tr><tr><td>SRR7819959</td><td>NP</td><td>3197</td><td>1665</td><td>NP</td><td>659</td><td>537</td></tr><tr><td>ERR2375157</td><td>NP</td><td>1989</td><td>1394</td><td>NP</td><td>744</td><td>537</td></tr><tr><td>SRR6799098</td><td>NP</td><td>247</td><td>913</td><td>481</td><td>373</td><td>531</td></tr><tr><td>SRR7905974</td><td>NP</td><td>3796</td><td>NP</td><td>NP</td><td>961</td><td>526</td></tr><tr><td>SRR7739756</td><td>NP</td><td>1704</td><td>NP</td><td>NP</td><td>700</td><td>532</td></tr><tr><td>ERR2162181</td><td>NP</td><td>625</td><td>947</td><td>779</td><td>NP</td><td>535</td></tr></tbody></table></table-wrap></p>
    <p id="Par10">Table <xref rid="Tab1" ref-type="table">1</xref> shows that NGSReadsTreatment obtained a greater percentage of redundant read removals for thirteen of the sixteen organisms analyzed, being that in an organism the percentage of removal equal to that of another tool used in the test; that is, it was able to identify and remove the largest amount of redundancies. Some datasets of organisms, for example SRR2000272, SRR7905974 and SRR2014554, experienced processing problems with the other computational tools: computer crashes during execution and processing failure due to the existence of orphan sequences in the read files. The tools that presented 0% were not able to remove any redundancy in the dataset, despite processing the data normally.</p>
    <p id="Par11">For the SRR2014554 organism, only the NGSReadsTreatment was successful in processing the 4-GB dataset. All the other tested tools presented errors during read processing.</p>
    <p id="Par12">Table <xref rid="Tab2" ref-type="table">2</xref> lists the total memory used by each tool in the processing of the raw reads. Similar to the results described in Table <xref rid="Tab1" ref-type="table">1</xref>, the dataset of some organisms presented problems during the execution by the other tools. However, it was possible to use the NGSReadsTreatment software in all cases, thereby also demonstrating its efficiency in the use of memory, since it was the only tool that used the least computational memory among all the tested tools in most analyses.</p>
    <p id="Par13">The FastUniq software does not support single-end reads in its analyzes, so it was not possible to perform the processing of reads of this type with the tool. However, in all cases it was possible to use the NGSReadsTreatment, also demonstrating its efficiency in processing paired-end and single-end reads, with a reduced computational memory usage.</p>
    <p id="Par14">To improve the validation of NGSReadsTreatment the same analyzes performed with the real datasets (sixteen organisms) were performed with simulated datasets from ART tool<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. It can be observed that NGSReadsTreatment has proved to be efficient for both redundancy removal and memory usage as shown in the Tables <xref rid="Tab3" ref-type="table">3</xref> and <xref rid="Tab4" ref-type="table">4</xref>.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Percentage of read redundancy removal per tool for each simulated dataset. NP - not processed owing to errors.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Organism</th><th>FastUniq 1.1</th><th>ParDRe 2.2.5</th><th>MarDre 1.3</th><th>CD-HIT-DUP 4.6.8</th><th>Clumpify (bbmap)</th><th>NGSReadsTreatment</th></tr></thead><tbody><tr><td><italic>Mycobacterium tuberculosis variant bovis BCG</italic> str. Korea 1168P - Platform HiSeq 2500</td><td>0.08%</td><td>NP</td><td>0.08%</td><td>0.08%</td><td>0.20%</td><td>0.77%</td></tr><tr><td><italic>Mycobacterium tuberculosis KZN</italic> 4207 - Platform HiSeq 2500</td><td>0.08%</td><td>NP</td><td>0.08%</td><td>0.08%</td><td>0.20%</td><td>0.05%</td></tr><tr><td><italic>Escherichia coli</italic> O103:H2 str. 12009 - Platform HiSeq 2500</td><td>0.09%</td><td>NP</td><td>0.09%</td><td>0.09%</td><td>0.23%</td><td>1.15%</td></tr><tr><td><italic>Arcobacter halophilus</italic> strain CCUG 53805 - Platform HiSeq 2500</td><td>0.08%</td><td>NP</td><td>0.08%</td><td>0.08%</td><td>0.22%</td><td>0.10%</td></tr><tr><td><italic>Mycobacterium tuberculosis variant bovis BCG</italic> str. Korea 1168P - Platform 454</td><td>0%</td><td>NP</td><td>NP</td><td>0%</td><td>0%</td><td>1.16%</td></tr><tr><td><italic>Mycobacterium tuberculosis KZN</italic> 4207 - Platform 454</td><td>0%</td><td>NP</td><td>NP</td><td>0%</td><td>0%</td><td>0.08%</td></tr><tr><td><italic>Escherichia coli</italic> O103:H2 str. 12009 - Platform 454</td><td>0%</td><td>NP</td><td>NP</td><td>0%</td><td>0%</td><td>1.43%</td></tr><tr><td><italic>Arcobacter halophilus</italic> strain CCUG 53805 - Platform 454</td><td>0%</td><td>NP</td><td>NP</td><td>0%</td><td>0%</td><td>0.13%</td></tr></tbody></table></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>Memory amount used by each tool in megabyte for each simulated dataset. NP - not processed owing to errors.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Organism</th><th>FastUniq 1.1</th><th>ParDRe 2.2.5</th><th>MarDre 1.3</th><th>CD-HIT-DUP 4.6.8</th><th>Clumpify (bbmap)</th><th>NGSReadsTreatment</th></tr></thead><tbody><tr><td><italic>Mycobacterium tuberculosis variant bovis BCG</italic> str. Korea 1168P - Platform HiSeq2500</td><td>1272</td><td>NP</td><td>1474</td><td>2173</td><td>771</td><td>537</td></tr><tr><td><italic>Mycobacterium tuberculosis KZN</italic> 4207 - Platform HiSeq2500</td><td>1278</td><td>NP</td><td>1533</td><td>2153</td><td>538</td><td>538</td></tr><tr><td><italic>Escherichia coli</italic> O103:H2 str. 12009 - Platform HiSeq2500</td><td>1583</td><td>NP</td><td>1632</td><td>2660</td><td>558</td><td>537</td></tr><tr><td><italic>Arcobacter halophilus</italic> strain CCUG 53805 - Platform HiSeq2500</td><td>832</td><td>NP</td><td>1250</td><td>1363</td><td>569</td><td>536</td></tr><tr><td><italic>Mycobacterium tuberculosis variant bovis BCG</italic> str. Korea 1168P - Platform 454</td><td>143</td><td>NP</td><td>NP</td><td>477</td><td>337</td><td>534</td></tr><tr><td><italic>Mycobacterium tuberculosis KZN</italic> 4207 - Library 454</td><td>143</td><td>NP</td><td>NP</td><td>476</td><td>262</td><td>534</td></tr><tr><td><italic>Escherichia coli</italic> O103:H2 str. 12009 - Platform 454</td><td>177</td><td>NP</td><td>NP</td><td>598</td><td>400</td><td>533</td></tr><tr><td><italic>Arcobacter halophilus</italic> strain CCUG 53805 - Platform 454</td><td>96</td><td>NP</td><td>NP</td><td>321</td><td>264</td><td>536</td></tr></tbody></table></table-wrap></p>
    <p id="Par15">Most errors were observed during the processing of the single-end reads, all details on the errors and all processing results per organism are available in the supplementary material.</p>
    <p id="Par16">In the third validation step, after the generation of the nine datasets with different coverage, the reads were counted to determine the amount of reads, number of unique reads and the amount of redundant reads in the raw data of each dataset (last table of the section simulated data with different coverage values in the Supplementary Material).</p>
    <p id="Par17">All nine datasets were processed by all tools for redundancy removal, where the memory usage by each tool was evaluated. After this processing, the unique reads of each of the datasets were counted. This count seeks to identify whether the number of unique reads in a processed dataset (Supplementary Material) is equal to the number of unique reads of the raw dataset, thus ensuring that only redundant reads were removed in the analysis.</p>
    <p id="Par18">As can be seen in Supplementary Material, the NGSReadsTreatment and all the tools used, with the exception of the Clumpify (bbmap) tool, were able to reach the number of unique reads equal to the raw data, thus ensuring that all these tools succeeded in removing only the redundant reads of each dataset.</p>
    <p id="Par19">The Clumpify (bbmap) tool was the only one that presented a different number of unique reads in relation to the raw data, indicating that this tool may be removing more data than just redundant reads.</p>
    <p id="Par20">As there was no difference in the amount of redundant reads removed between NGSReadsTreatment and the other tools of this analysis, with exception of the Clumpify (bbmap) tool, we can validate that all are removing only redundant data from the datasets, however, it is possible to observe the disparity in the amount of memory required for the data processing between NGSReadsTreatment and the other tools, where NGSReadsTreatment used a much smaller amount of memory to process the same amount of data (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). All processing results per organism are available in the supplementary material.<fig id="Fig1"><label>Figure 1</label><caption><p>Evaluation of memory usage for each computational tool in the processing of simulated datasets.</p></caption><graphic xlink:href="41598_2019_48242_Fig1_HTML" id="d29e1635"/></fig></p>
    <p id="Par21">The analysis of the results obtained herein allowed verification of the efficiency of the adopted Cuckoo Filter probabilistic data structure, as it proved effective in removing read redundancies from the raw files, besides showing optimal memory usage for task processing. The NGSReadsTreatment tool is capable of handling single-end and paired-end files, and is available in two versions: one with a graphical interface and control of processing status through a database. Thus, in case of some kind of error or if the user wishes to interrupt processing, it can be resumed. A version without a graphical interface is also available.</p>
    <p id="Par22">The NGSReadsTreatment presented the same behavior in the analysis of both real data and simulated data. The simulated dataset results show the efficiency of the NGSReadsTreatment in the removal of the reads redundancies as listed in Table <xref rid="Tab3" ref-type="table">3</xref>.</p>
    <p id="Par23">Thus, it is concluded that NGSReadsTreatment has proven to be an efficient tool in removing redundancy from NGS reads, thus being an alternative in the execution of this task even if the user does not have high computational resources.</p>
  </sec>
  <sec id="Sec3">
    <title>Methodology</title>
    <sec id="Sec4">
      <title>Programming language and database</title>
      <p id="Par24">NGSReadsTreatment was developed in JAVA language (<ext-link ext-link-type="uri" xlink:href="http://www.oracle.com/">http://www.oracle.com/</ext-link>) and the Swing library was used to create the graphical interface (<ext-link ext-link-type="uri" xlink:href="http://www.oracle.com/">http://www.oracle.com/</ext-link>). Maven (<ext-link ext-link-type="uri" xlink:href="https://maven.apache.org/">https://maven.apache.org/</ext-link>) was used for dependency management and build automation. Its main features include the following (among others): simplified project configuration following best practices, automated dependency management, and JAR generation with all the dependencies used in the project. The project management was performed with SQLite version 3 (<ext-link ext-link-type="uri" xlink:href="https://www.sqlite.org/">https://www.sqlite.org/</ext-link>).</p>
    </sec>
    <sec id="Sec5">
      <title>Redundancy removal</title>
      <p id="Par25">Cuckoo Filter<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> was used to remove redundancies from the reads in the raw files. It is a quick and effective probabilistic data structure for cluster association queries. Developed by Fan, Andersen, Kaminsky, and Mitzenmacher, Cuckoo Filter emerged as an enhancement to Bloom Filter<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, introducing support for dynamic item deletion, improved search performance, and improved space efficiency for low false-positive applications.</p>
      <p id="Par26">The Cuckoo Filter uses cuckoo hashing<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> to resolve collisions and basically consists of a compact cuckoo hash table that stores the fingerprints of inserted items. Each fingerprint is a string of bits derived from the hash of the item to be inserted.</p>
      <p id="Par27">A cuckoo hash table consists of a two-dimensional array where the rows correspond to the associative units called buckets and their cells are called slots. A bucket can contain multiple slots and each slot is used to store a single fingerprint of predefined size<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. For example a cuckoo filter (2,4) has slots that store 2-bit fingerprints and each table bucket can hold up to 4 fingerprints.</p>
      <p id="Par28">In the process of removing redundancy is generated for each read a fingerprint and checked if it is contained in the cuckoo hash table, if the answer is false the fingerprint is inserted into the table and the read is stored in a text file, otherwise the read is discarded.</p>
      <p id="Par29">It is worth mentioning that these probabilistic structures<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> do not provide false negatives, which allows greater efficiency in the removal of duplicate reads from the raw file.</p>
    </sec>
    <sec id="Sec6">
      <title>Evaluation of computational cost</title>
      <p id="Par30">Linux’s <italic>time</italic> software (<ext-link ext-link-type="uri" xlink:href="http://man7.org/linux/man-pages/man1/time.1.html">http://man7.org/linux/man-pages/man1/time.1.html</ext-link>) was used to generate statistics for a command, shell script, or any executed program. The statistics included the time spent by the program in the user mode, the time spent by the program in the kernel mode, and the average memory usage by the program. The output was formatted using the -f option or the TIME environment variable. The string type format was interpreted in the same way as <italic>printf</italic>, where common characters were copied directly whereas special characters were copied using \t (tab) and \n (new line). The percent sign is represented by %% (otherwise, % indicates a conversion<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>).</p>
    </sec>
    <sec id="Sec7">
      <title>Raw data download</title>
      <p id="Par31">Fastq-dump version 2.9.2 (<ext-link ext-link-type="uri" xlink:href="https://edwards.sdsu.edu/research/fastq-dump/">https://edwards.sdsu.edu/research/fastq-dump/</ext-link>) was used to download the NCBI-SRA database files in fastq format.</p>
    </sec>
    <sec id="Sec8">
      <title>Tool validation with real datasets</title>
      <p id="Par32">To validate NGSReadsTreatment were used data from sixteen organisms. Two strains of <italic>Mycobacterium tuberculosis</italic>, two <italic>Kineococcus</italic>, six strains of <italic>Escherichia</italic> coli, and one strain of <italic>Rhodopirellula báltica</italic>, <italic>Arcobacter halophilus</italic>, <italic>Rathayibacter tritici</italic>, <italic>Salmonella entérica</italic>, <italic>Staphylococcus aureus</italic> and <italic>Pseudomonas aeruginosa</italic>. Each organism with its SRA number is listed in Table <xref rid="Tab5" ref-type="table">5</xref>. For paired reads the File size by Dataset and Total of Reads by Dataset represent the sum of tag1 and tag2 (Table <xref rid="Tab5" ref-type="table">5</xref>).<table-wrap id="Tab5"><label>Table 5</label><caption><p>Organisms and SRA number used to validate NGSReadsTreatment.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Organism</th><th>SRA Access number</th><th>File size by Dataset</th><th>Total of Reads by Dataset</th><th>Type Library</th><th>Platform</th></tr></thead><tbody><tr><td><italic>Escherichia coli</italic> RR1</td><td>SRR2014554</td><td>8192MB</td><td>24248885</td><td>Paired</td><td>Illumina HiSeq 2000</td></tr><tr><td><italic>Escherichia coli</italic> 042</td><td>ERR007646</td><td>2406MB</td><td>14110696</td><td>Paired</td><td>(Illumina Genome Analyzer</td></tr><tr><td><italic>Escherichia coli</italic> P12b</td><td>SRR2000272</td><td>1350MB</td><td>2990758</td><td>Paired</td><td>Illumina MiSeq</td></tr><tr><td><italic>Escherichia coli</italic> KLY</td><td>SRR1424625</td><td>1682MB</td><td>6886668</td><td>Paired</td><td>Illumina HiSeq 2000</td></tr><tr><td><italic>Escherichia coli</italic> O25b:H4-ST131</td><td>SRR933487</td><td>1070MB</td><td>3214312</td><td>Paired</td><td>Illumina Genome Analyzer IIx</td></tr><tr><td><italic>Kineococcus rhizosphaerae</italic> DSM 19711</td><td>SRR6479489</td><td>2048MB</td><td>5641334</td><td>Paired</td><td>Illumina HiSeq 2500</td></tr><tr><td><italic>Kineococcus xinjiangensis</italic> DSM 22857</td><td>SRR6479482</td><td>2168MB</td><td>5971022</td><td>Paired</td><td>Illumina HiSeq 2500</td></tr><tr><td><italic>Mycobacterium tuberculosis</italic> F11</td><td>SRR974839</td><td>1936 MB</td><td>7279254</td><td>Paired</td><td>Illumina HiSeq 2000</td></tr><tr><td><italic>Mycobacterium tuberculosis XDR KZN 4207</italic></td><td>SRR1144800</td><td>1884MB</td><td>7033428</td><td>Paired</td><td>(Illumina HiSeq 2000</td></tr><tr><td><italic>Arcobacter halophilus</italic></td><td>SRR7587111</td><td>588MB</td><td>670813</td><td>Paired</td><td>454 Titanium</td></tr><tr><td><italic>Rhodopirellula baltica</italic></td><td>SRR7819959</td><td>1984MB</td><td>3207713</td><td>Single</td><td>Ion Torrent</td></tr><tr><td><italic>Escherichia coli O157:H7 in Romania</italic></td><td>ERR2375157</td><td>1201MB</td><td>2106268</td><td>Single</td><td>Ion Torrent</td></tr><tr><td><italic>Rathayibacter tritici</italic></td><td>SRR6799098</td><td>157MB</td><td>160403</td><td>Single</td><td>454 Junior</td></tr><tr><td><italic>Salmonella enterica</italic></td><td>SRR7905974</td><td>2990MB</td><td>163468</td><td>Single</td><td>PacBio</td></tr><tr><td><italic>Staphylococcus aureus</italic></td><td>SRR7739756</td><td>1336MB</td><td>86389</td><td>Single</td><td>Oxford nanopore MinIon</td></tr><tr><td><italic>Pseudomonas aeruginosa</italic></td><td>ERR2162181</td><td>246MB</td><td>1146696</td><td>Single</td><td>SoliD 5500</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec9">
      <title>Tool validation with simulated datasets</title>
      <p id="Par33">Aiming to further validate the tool NGSReadsTreatment another approach was employed, the use of simulated NGS datasets. The idea is that the tool NGSReadsTreatment should exhibit the same behavior in both real and simulated data.</p>
      <p id="Par34">To generate the simulated datasets, the ART tool version 2.5.8<sup><xref ref-type="bibr" rid="CR9">9</xref></sup> was used, which is able to generate simulated next-generation reads from different platforms, based on a reference in the fasta format. The ART tool can simulate real sequencing read errors and quality, and it is used to test or benchmark a variety of method or tools for next-generation sequencing data analysis.</p>
      <p id="Par35">For this validation of the NGSReadsTreatment were simulated reads from sequencing on the Illumina HiSeq 2500 and Roche 454 GS FLX Titanium platforms.</p>
      <p id="Par36">The organisms used as reference to generate the simulated reads were: <italic>Mycobacterium bovis</italic> BCG str. Korea 1168P (GenBank: CP003900.2), <italic>Mycobacterium tuberculosis</italic> KZN 4207 (GenBank: CP001662.1), <italic>Arcobacter halophilus</italic> strain CCUG 53805 (GenBank: CP031218) and <italic>Escherichia coli</italic> O103:H2 str. 12009 (GenBank: AP010958.1). For each of the organisms two sets of reads were generated, one of the Illumina platform and another of the 454 platform.</p>
    </sec>
    <sec id="Sec10">
      <title>Tool validation with simulated datasets of different coverage</title>
      <p id="Par37">A third validation step was performed, this time using simulated data with different sequencing coverage. The goal was to simulate different amounts of redundant reads by mimicking the PCR process. We selected as reference the genomes <italic>Mycobacterium bovis</italic> BCG str. Korea 1168P (dataset prefix name HS25MicoKorea1168P) <italic>Mycobacterium tuberculosis</italic> KZN 4207 (dataset prefix name HS25MicoKZN_4207) <italic>and Escherichia coli</italic> O103:H2 str. 12009 (dataset prefix name HS25EcoliO103_H2).</p>
      <p id="Par38">Each of the reference genomes was used in ART tool version 2.5.8 to generate simulated datasets with 100x, 200x and 300x coverage, respectively. Thus, nine simulated datasets were generated as shown in Table <xref rid="Tab6" ref-type="table">6</xref>.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Generation of simulated data with different coverage.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Organism</th><th>Coverage</th><th>Dataset Name</th></tr></thead><tbody><tr><td><italic>Mycobacterium bovis</italic> BCG str. Korea 1168P</td><td>300x</td><td>HS25MicoKorea1168P_300</td></tr><tr><td><italic>Mycobacterium bovis</italic> BCG str. Korea 1168P</td><td>200x</td><td>HS25MicoKorea1168P_200</td></tr><tr><td><italic>Mycobacterium bovis</italic> BCG str. Korea 1168P</td><td>100x</td><td>HS25MicoKorea1168P_100</td></tr><tr><td><italic>Mycobacterium tuberculosis</italic> KZN 4207</td><td>300x</td><td>HS25MicoKZN_4207_300</td></tr><tr><td><italic>Mycobacterium tuberculosis</italic> KZN 4207</td><td>200x</td><td>HS25MicoKZN_4207_200</td></tr><tr><td><italic>Mycobacterium tuberculosis</italic> KZN 4207</td><td>100x</td><td>HS25MicoKZN_4207_100</td></tr><tr><td><italic>Escherichia coli</italic> O103:H2 str. 12009</td><td>300x</td><td>HS25EcoliO103_H2_300</td></tr><tr><td><italic>Escherichia coli</italic> O103:H2 str. 12009</td><td>200x</td><td>HS25EcoliO103_H2_200</td></tr><tr><td><italic>Escherichia coli</italic> O103:H2 str. 12009</td><td>100x</td><td>HS25EcoliO103_H2_100</td></tr></tbody></table></table-wrap></p>
      <p id="Par39">After this step we use an <italic>ad-hoc</italic> script (available in <ext-link ext-link-type="uri" xlink:href="https://sourceforge.net/projects/ngsreadstreatment/files/AnalyzeDuplicatesInFastq.pl">https://sourceforge.net/projects/ngsreadstreatment/files/AnalyzeDuplicatesInFastq.pl</ext-link>) designed to count the number of unique reads in a dataset, this is, reads that appear only once. The purpose of using this script was to determine if after processing the data, redundant reads were completely removed, thus ensuring that only unique reads would stay in each dataset. In this way, after each of the nine datasets were processed by each of the tools, the number of unique reads of each one was counted.</p>
    </sec>
    <sec id="Sec11">
      <title>Workstation</title>
      <p id="Par40">The Workstation used to carry out the analyzes has the following configuration: Intel Core i7-2620M CPU 2.70 GHz with four processing cores, 324 GB HD and 6GB memory.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec12">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41598_2019_48242_MOESM1_ESM.pdf">
            <caption>
              <p>List of all results about datasets processing</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisher’s note:</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>Antonio Sérgio Cruz Gaia and Pablo Henrique Caracciolo Gomes de Sá contributed equally.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p><bold>Supplementary information</bold> accompanies this paper at 10.1038/s41598-019-48242-w.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>This work was supported by Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq) and Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES). AAOV was supported by PRO2154-2018 from Federal University of Pará (UFPA) and PHCGS was supported by 092017-767 from Federal Rural University of Amazonia (UFRA).</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author Contributions</title>
    <p>Antonio Sérgio Cruz Gaia developed the computational tool and article writing; Pablo Henrique Caracciolo Gomes de Sá reviewed the graphical interface and the article; Mônica Silva de Oliveira reviewed the tools and approaches, the user’s manual; Adonney Allan O. Veras designed the project and review of the article.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing Interests</title>
    <p id="Par41">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reuter</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Spacek</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Snyder</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>High-Throughput Sequencing Technologies</article-title>
        <source>Molecular Cell</source>
        <year>2015</year>
        <volume>58</volume>
        <fpage>586</fpage>
        <lpage>597</lpage>
        <pub-id pub-id-type="doi">10.1016/j.molcel.2015.05.004</pub-id>
        <pub-id pub-id-type="pmid">26000844</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <mixed-citation publication-type="other">Ebbert, M. <italic>et al</italic>. Evaluating the necessity of PCR duplicate removal from next-generation sequencing data and a comparison of approaches. <italic>BMC Bioinformatics</italic><bold>17</bold> (2016).</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <mixed-citation publication-type="other">Manconi, A. <italic>et al</italic>. Removing duplicate reads using graphics processing units. <italic>BMC Bioinformatics</italic><bold>17</bold> (2016).</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>FastUniq: A Fast De Novo Duplicates Removal Tool for Paired Short Reads</article-title>
        <source>PLoS ONE</source>
        <year>2012</year>
        <volume>7</volume>
        <fpage>e52249</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0052249</pub-id>
        <pub-id pub-id-type="pmid">23284954</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Burriesci</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lehnert</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Pringle</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Fulcrum: condensing redundant reads from high-throughput sequencing studies</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>28</volume>
        <fpage>1324</fpage>
        <lpage>1327</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts123</pub-id>
        <pub-id pub-id-type="pmid">22419786</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Godzik</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences</article-title>
        <source>Bioinformatics</source>
        <year>2006</year>
        <volume>22</volume>
        <fpage>1658</fpage>
        <lpage>1659</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl158</pub-id>
        <pub-id pub-id-type="pmid">16731699</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>González-Domínguez</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Schmidt</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>ParDRe: faster parallel duplicated reads removal tool for sequencing studies: Table 1</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <fpage>1562</fpage>
        <lpage>1564</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btw038</pub-id>
        <pub-id pub-id-type="pmid">26803159</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Expósito</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Veiga</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>González-Domínguez</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Touriño</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>MarDRe: efficient MapReduce-based removal of duplicate DNA reads in the cloud</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <fpage>2762</fpage>
        <lpage>2764</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx307</pub-id>
        <pub-id pub-id-type="pmid">28475668</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Myers</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Marth</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>ART: a next-generation sequencing read simulator</article-title>
        <source>Bioinformatics</source>
        <year>2011</year>
        <volume>28</volume>
        <fpage>593</fpage>
        <lpage>594</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btr708</pub-id>
        <pub-id pub-id-type="pmid">22199392</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Fan, B., Andersen, D., Kaminsky, M. &amp; Mitzenmacher, M. Cuckoo Filter. <italic>Proceedings of the 10th ACM International on Conference on emerging Networking Experiments and Technologies - CoNEXT ’</italic>14, 10.1145/2674005.2674994 (2014).</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bloom</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Space/time trade-offs in hash coding with allowable errors</article-title>
        <source>Communications of the ACM</source>
        <year>1970</year>
        <volume>13</volume>
        <fpage>422</fpage>
        <lpage>426</lpage>
        <pub-id pub-id-type="doi">10.1145/362686.362692</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pagh</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Rodler</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Cuckoo hashing</article-title>
        <source>Journal of Algorithms</source>
        <year>2004</year>
        <volume>51</volume>
        <fpage>122</fpage>
        <lpage>144</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jalgor.2003.12.002</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Kerrisk, M. <italic>The Linux programming interface</italic>. (No Starch Press, 2010).</mixed-citation>
    </ref>
  </ref-list>
</back>
