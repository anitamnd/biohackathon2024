<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6296065</article-id>
    <article-id pub-id-type="publisher-id">2500</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-018-2500-z</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methodology Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>PASNet: pathway-associated sparse deep neural network for prognosis prediction from high-throughput data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Hao</surname>
          <given-names>Jie</given-names>
        </name>
        <address>
          <email>jhao2@students.kennesaw.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kim</surname>
          <given-names>Youngsoon</given-names>
        </name>
        <address>
          <email>ykim111@kennesaw.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kim</surname>
          <given-names>Tae-Kyung</given-names>
        </name>
        <address>
          <email>taekyung.kim@utsouthwestern.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9565-9523</contrib-id>
        <name>
          <surname>Kang</surname>
          <given-names>Mingon</given-names>
        </name>
        <address>
          <email>mkang9@kennesaw.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9620 8332</institution-id><institution-id institution-id-type="GRID">grid.258509.3</institution-id><institution>Kennesaw State University, </institution></institution-wrap>Kennesaw, USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9620 8332</institution-id><institution-id institution-id-type="GRID">grid.258509.3</institution-id><institution>Kennesaw State University, </institution></institution-wrap>Marietta, USA </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9482 7121</institution-id><institution-id institution-id-type="GRID">grid.267313.2</institution-id><institution>University of Texas Southwestern Medical Center, </institution></institution-wrap>Dallas, USA </aff>
      <aff id="Aff4"><label>4</label>Department of Life Sciences, Pohang Institute of Science and Technology (POSTECH), Dallas, USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>17</day>
      <month>12</month>
      <year>2018</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>17</day>
      <month>12</month>
      <year>2018</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2018</year>
    </pub-date>
    <volume>19</volume>
    <elocation-id>510</elocation-id>
    <history>
      <date date-type="received">
        <day>29</day>
        <month>6</month>
        <year>2018</year>
      </date>
      <date date-type="accepted">
        <day>16</day>
        <month>11</month>
        <year>2018</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2018</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p>Predicting prognosis in patients from large-scale genomic data is a fundamentally challenging problem in genomic medicine. However, the prognosis still remains poor in many diseases. The poor prognosis may be caused by high complexity of biological systems, where multiple biological components and their hierarchical relationships are involved. Moreover, it is challenging to develop robust computational solutions with high-dimension, low-sample size data.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>In this study, we propose a Pathway-Associated Sparse Deep Neural Network (PASNet) that not only predicts patients’ prognoses but also describes complex biological processes regarding biological pathways for prognosis. PASNet models a multilayered, hierarchical biological system of genes and pathways to predict clinical outcomes by leveraging deep learning. The sparse solution of PASNet provides the capability of model interpretability that most conventional fully-connected neural networks lack. We applied PASNet for long-term survival prediction in Glioblastoma multiforme (GBM), which is a primary brain cancer that shows poor prognostic performance. The predictive performance of PASNet was evaluated with multiple cross-validation experiments. PASNet showed a higher Area Under the Curve (AUC) and F1-score than previous long-term survival prediction classifiers, and the significance of PASNet’s performance was assessed by Wilcoxon signed-rank test. Furthermore, the biological pathways, found in PASNet, were referred to as significant pathways in GBM in previous biology and medicine research.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p>PASNet can describe the different biological systems of clinical outcomes for prognostic prediction as well as predicting prognosis more accurately than the current state-of-the-art methods. PASNet is the first pathway-based deep neural network that represents hierarchical representations of genes and pathways and their nonlinear effects, to the best of our knowledge. Additionally, PASNet would be promising due to its flexible model representation and interpretability, embodying the strengths of deep learning. The open-source code of PASNet is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/DataX-JieHao/PASNet">https://github.com/DataX-JieHao/PASNet</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Sparse deep neural network</kwd>
      <kwd>Prognosis prediction</kwd>
      <kwd>Long-term survival prediction</kwd>
      <kwd>Pathway-based analysis</kwd>
      <kwd>Glioblastoma multiforme</kwd>
      <kwd>TCGA</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2018</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Predicting prognosis in patients from large-scale genomic data is a fundamentally challenging problem in genomic medicine [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref>]. Along with the rapid advances of high-throughput technologies and their effectivenesses, high-dimensional genomic data provides more accurate and richer biological descriptions of clinical phenotypes of interests than ever before. Therefore, translating large-scale genomic profiles to clinical outcomes not only improves predicting patient prognosis but also helps in identifying prognostic factors and biological processes.</p>
    <p>The capabilities of high-level biological representation and interpretation of the prognosis are often more desired in biomedical research rather than merely improving predictive performance. Pathway-based analysis is an approach that a number of studies have been investigating to improve both predictive performance and biological interpretability [<xref ref-type="bibr" rid="CR4">4</xref>–<xref ref-type="bibr" rid="CR6">6</xref>]. In pathway-based analyses, the incorporation of biological pathway databases in a model takes advantage of leveraging prior biological knowledge so that potential prognostic factors of well-known biological functionality can be identified. Pathway-based analyses identify biological links between pathways and clinical outcomes and enable the interpretation of biological processes where their corresponding genes and proteins are involved. Thus, pathway-based interpretation and visualization provide an intuitive and comprehensive understanding of functionally-related molecular mechanisms.</p>
    <p>Moreover, pathway-based approaches have shown more reproducible analysis results than gene expression data analysis alone [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR7">7</xref>–<xref ref-type="bibr" rid="CR10">10</xref>]. High-level representations of gene co-expressions are considered in most pathway-based analyses; each of which represents a biological pathway while preserving the original information. Thus, pathway-based analyses remedy the limitations of gene expression data, which are intrinsically sensitive to stochastic fluctuations and are often caused by multiple potential sources, such as inherent stochasticity of biochemical processes, environmental differences, and genetic mutation [<xref ref-type="bibr" rid="CR11">11</xref>]. Pathway-based markers were proposed for classifying breast cancer metastasis and ovarian cancer survival time [<xref ref-type="bibr" rid="CR5">5</xref>]. Cancer subtypes were discovered with pathway-based markers via Restricted Boltzmann Machine (RBM) [<xref ref-type="bibr" rid="CR8">8</xref>]. A group LASSO-based approach associated genes with pathways and characterized them based on biological pathways [<xref ref-type="bibr" rid="CR10">10</xref>]. Higher-order functional representation of pathway-based metabolic features provided reproducible biomarkers for breast cancer diagnosis [<xref ref-type="bibr" rid="CR9">9</xref>].</p>
    <p>However, reliable and accurate prognosis still remains poor in many diseases due to the following challenges: high-dimension, low-sample size data and complex nonlinear effects between biological components.</p>
    <p>Genomic data are highly dimensional relative to their sample sizes. High-dimension, low-sample size (HDLSS) data often make prediction models sensitive to noise and false positive associations, which consequently make predicting accurate prognoses difficult. LASSO-based approaches have been mainly considered to estimate the effects of a gene set that are associated with various types of clinical outcomes on HDLSS data. The LASSO-based approaches embed sparse coding schemes into linear or logistic regression models for selecting few but greatly informative features among the high-dimensional data. For instance, a logistic regression with sparse regularization was applied for the prognostic model of mortality after acute myocardial infarction [<xref ref-type="bibr" rid="CR12">12</xref>]. Random LASSO was proposed to enhance the LASSO solution by applying multiple bootstrapping and was applied to predict patients’ survival times with glioblastoma gene expression data [<xref ref-type="bibr" rid="CR13">13</xref>]. LASSO-based regression models as a prediction model were validated with multiple imputed data in chronic obstructive pulmonary disease patients [<xref ref-type="bibr" rid="CR14">14</xref>].</p>
    <p>Pathway-based analysis also helps to reduce data dimensionality. The number of biological pathways is relatively smaller than the number of genes, and a set of genes in the same pathway can be represented by the pathway’s effect. Thus, pathways can be used as summary variables for the input of the predictive model instead of including all genes, which consequently reduces the model complexity.</p>
    <p>Most association studies between a gene set and various clinical outcomes have considered linear or logistic regression models for identifying prognostic factors as well as understanding a biological mechanism of the progression of disease. However, nonlinear effects of genes or pathways may fail to be identified by linear-based approaches. As a solution, kernel-based models have been proposed to capture nonlinear effects of complex pathways [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>]. Multiple kernel learning models were introduced to aggregate complex effects from multiple pathways [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR18">18</xref>]. Kernel Principle Component Analysis (KPCA) was applied to reduce the dimensionality of the feature space by using the correlation structure of the pathways [<xref ref-type="bibr" rid="CR18">18</xref>].</p>
    <p>Recently, several attempts to capture hierarchical effects of genes and pathways have been made. Inferences of multilayered hierarchical gene regulatory networks have been considered to understand how pathways regulate each other hierarchically. A bottom-up graphic Gaussian model [<xref ref-type="bibr" rid="CR19">19</xref>] and a recursive random forest algorithm [<xref ref-type="bibr" rid="CR20">20</xref>] were proposed to construct multilayered hierarchical gene regulatory networks. Moreover, complex biological networks were modeled by inferring the multiple hierarchical models (1) between gene expression and pathways and (2) within pathways [<xref ref-type="bibr" rid="CR21">21</xref>]. However, complex hierarchical relationships between pathways have not been considered for prognostic studies yet, to the best of our knowledge, although hierarchical effects of pathways are prevalent in biological systems [<xref ref-type="bibr" rid="CR22">22</xref>].</p>
    <p>In this paper, we propose a Pathway-Associated Sparse Deep Neural Network (PASNet) to achieve the goals: (1) to predict prognosis in patients accurately by incorporating biological pathways, (2) to provide a solution for hierarchical interpretation of nonlinear relationships between biological pathways of disease systematically, and (3) to handle computational problems on HDLSS data with unbalanced classes. An innovative aspect of our model is biological interpretability; we achieved this with sparse coding and by constructing hidden layers with biological pathways, which oppose the <italic>black box</italic> nature of deep learning. Our new sparse deep learning architecture represents multiple molecular biological layers, such as a gene layer and a pathway layer, along with their hierarchical relationships, which use sparse regularization.</p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <p>Pathway-Associated Sparse Deep Neural Network (PASNet) identifies a subset of genes and pathways involved in a disease as prognostic biomarkers, as well as their interactions. PASNet models a multilayered, hierarchical biological system of genes and pathways on a disease, while leveraging the strengths of deep learning for competitive predictive performance. The sparsity of PASNet allows one to interpret the model, which is what conventional fully-connected networks lack. The architecture of PASNet and the strategies for training a sparse neural network model with HDLSS and imbalanced data are described in “<xref rid="Sec8" ref-type="sec">Methods</xref>” section.</p>
    <p>We conducted experiments to evaluate PASNet’s predictive performance for long-term survival prediction in Glioblastoma multiforme (GBM). The capability of the prediction was assessed by comparing our model with the classifiers that have been used for long-term survival prediction. Furthermore, we will describe how PASNet can represent the biological system of GBM in the following section.</p>
    <sec id="Sec3">
      <title>Data</title>
      <p>GBM is a primary brain cancer that shows poor prognosis performance due to the above challenges. Comprising more than half of all brain tumors, GBM is the most prevailing and aggressive malignant type of primary astrocytomas [<xref ref-type="bibr" rid="CR23">23</xref>]. Patients with GBM have a median survival time of approximately 15 months with intensive treatments [<xref ref-type="bibr" rid="CR24">24</xref>]. Furthermore, long-term survival patients with GBM are rare as more than 90% of patients are deceased within three years of diagnosis. Although treatments in neurosurgery, chemotherapy, and radiotherapy have improved, the prognosis of GBM remains poor [<xref ref-type="bibr" rid="CR25">25</xref>]. Hence, the advancement in understanding molecular mechanisms and related biological pathways of GBM is significant to accelerating the progress for new treatments [<xref ref-type="bibr" rid="CR24">24</xref>].</p>
      <p>We used the gene expression data of GBM patients, which is available at The Cancer Genome Atlas (TCGA, <ext-link ext-link-type="uri" xlink:href="http://cancergenome.nih.gov">http://cancergenome.nih.gov</ext-link>). The dataset includes the gene expression data of 522 samples and 12,042 genes and provides survival time and status. We considered patients who survived past 24 months (regardless of survival status) as long-term survivals (LTS) and patients that deceased in less than 24 months as short-term survivals (non-LTS). Living patients with a survival time of less than 24 months were excluded in the experiments and considered censored data. Finally, we obtained 99 LTS and 376 non-LTS samples, where around 20% of the samples were LTS patients.</p>
      <p>For pathway-based analysis, we utilized a biological pathway database from the Molecular Signatures Database (MSigDB) [<xref ref-type="bibr" rid="CR26">26</xref>]. In MSigDB, we extracted the biological pathways of Reactome. Then, we excluded the pathways that include less than ten genes, because small pathways are often redundant with larger pathways. As the input features, we considered the genes that belong to at least one pathway, since pathway annotations of genes are essential to construct the mask matrix <bold>M</bold> between the gene layer and the pathway layer. Finally, we considered 574 pathways and 4359 genes in the experiments. The gene expression data were standardized to a mean of zero and a standard deviation of one.</p>
    </sec>
    <sec id="Sec4">
      <title>Experimental setting</title>
      <p>We followed a typical design of conventional deep neural networks for PASNet. A sigmoid function and cross-entropy were considered for the activation and the cost function, respectively. A softmax function was used in the output layer so that the probabilities of output nodes add up to one. For the optimal tuning of PASNet’s training, we empirically determined the hyper-parameters by random search before cross-validation experiments. The learning rate (<italic>η</italic>) was set to 1e−4, and <italic>L</italic><sup>2</sup> regularization (<italic>λ</italic>) was set to 3e−4. Adaptive Moment Estimation (Adam) was performed as the stochastic optimizer [<xref ref-type="bibr" rid="CR27">27</xref>]. The dropouts for two intermediate layers were also applied with a dropping probability of 0.8 and 0.7, respectively. PASNet was implemented by PyTorch, and the source code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/DataX-JieHao/PASNet">https://github.com/DataX-JieHao/PASNet</ext-link>.</p>
    </sec>
    <sec id="Sec5">
      <title>Comparison</title>
      <p>We evaluated PASNet by comparing the performance with classifiers that have been used for prognosis prediction: Support Vector Machine (SVM), Random LASSO [<xref ref-type="bibr" rid="CR13">13</xref>], LASSO Logistic Regression (LLR) [<xref ref-type="bibr" rid="CR1">1</xref>], and neural network with dropout (Dropout NN).</p>
      <p>Specifically, we used a SVM with a radial basis function (RBF) kernel (<italic>γ</italic>=2<sup>−16</sup> and <italic>C</italic>=2<sup>3.9</sup> by two-step grid search [<xref ref-type="bibr" rid="CR28">28</xref>]). Random LASSO was trained so that every feature could be selected 20 times on average by bootstrapping, and the <italic>L</italic><sup>1</sup> regularization parameter was determined by 10-fold cross-validation. The LASSO parameter for LLR was also selected by 10-fold cross-validation. The fully-connected Dropout NN was designed with the same numbers of intermediate layers and neurons as the proposed PASNet as well as the dropout probabilities. The learning rate was 0.01 and the <italic>L</italic><sup>2</sup> regularization was 0.005. Note that PASNet has less number of weights to be trained in each epoch because of sparse coding, compared to Dropout NN. Hence, the optimal hyper-parameters of <italic>L</italic><sup>2</sup> regularization and learning rate should be different between PASNet and Dropout NN. We empirically searched the optimal hyper-parameters for PASNet and Dropout NN separately through multiple experiments. Dropout NN was implemented by PyTorch (<ext-link ext-link-type="uri" xlink:href="https://pytorch.org/">https://pytorch.org/</ext-link>).</p>
      <p>The experiments were carried out by stratified 5-fold cross-validation for maintaining the same proportions of the imbalanced samples in the classes. The cross-validation experiments were repeated ten times for performance reproducibility. Data preprocessing, such as data normalization, was separately applied on each fold. The testing data on each fold was scaled with the mean and standard deviation of the training data of the same fold.</p>
      <p>The predictive performances of the five models were evaluated with two metrics: Area Under the Curve (AUC) and F1-scores. The Receiver Operating Characteristic (ROC) curve (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>) was traced over the thresholds of scores to examine the trade-off between True Positive Rate (<italic>T</italic><italic>P</italic><italic>R</italic>=<italic>T</italic><italic>P</italic>/(<italic>T</italic><italic>P</italic>+<italic>F</italic><italic>N</italic>)) and False Positive Rate (<italic>F</italic><italic>P</italic><italic>R</italic>=<italic>F</italic><italic>P</italic>/(<italic>F</italic><italic>P</italic>+<italic>T</italic><italic>N</italic>)), where LTS was considered positive. An AUC was computed by the area under the ROC curve. An F1-score, an average of Positive Predicted Value (<italic>P</italic><italic>P</italic><italic>V</italic>=<italic>T</italic><italic>P</italic>/(<italic>T</italic><italic>P</italic>+<italic>F</italic><italic>P</italic>)) and TPR, is calculated by 2(<italic>P</italic><italic>P</italic><italic>V</italic>×<italic>T</italic><italic>P</italic><italic>R</italic>)/(<italic>P</italic><italic>P</italic><italic>V</italic>+<italic>T</italic><italic>P</italic><italic>R</italic>). The F1-score was computed for the LTS class.
<fig id="Fig1"><label>Fig. 1</label><caption><p>ROC Curves. PASNet produces the highest AUC of 0.6622 while the AUC of Dropout NN, SVM, random LASSO, and LLR is 0.6408, 0.6337, 0.6209, and 0.5899, respectively</p></caption><graphic xlink:href="12859_2018_2500_Fig1_HTML" id="MO1"/></fig></p>
      <p>The average AUC and the average F1-score of the five methods on the test datasets are shown in Table <xref rid="Tab1" ref-type="table">1</xref>. PASNet outperformed others as both AUC and F1-score are relatively high. PASNet produced AUC of 0.6622 ±0.013 (mean ±std) and F1-score of 0.3978 ±0.016. Following PASNet, Dropout NN produced AUC of 0.6408 ±0.014, and SVM produced AUC of 0.6337 ±0.015.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Comparison of AUC and F1-score in over ten stratified 5-fold cross-validations</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">AUC</th><th align="left">F1-Score</th></tr></thead><tbody><tr><td align="left">Logistic LASSO</td><td align="left">0.5899 ±0.020</td><td align="left">0.3347 ±0.025</td></tr><tr><td align="left">Random LASSO</td><td align="left">0.6209 ±0.020</td><td align="left">0.3370 ±0.020</td></tr><tr><td align="left">SVM</td><td align="left">0.6337 ±0.015</td><td align="left">0.3446 ±0.015</td></tr><tr><td align="left">Dropout NN</td><td align="left">0.6408 ±0.014</td><td align="left">0.2957 ±0.025</td></tr><tr><td align="left">PASNet</td><td align="left">0.6622 ±0.013</td><td align="left">0.3978 ±0.016</td></tr></tbody></table></table-wrap></p>
      <p>To statistically assess the performance of PASNet (AUC) as compared to others, we conducted the Wilcoxon signed-rank test: a non-parametric paired, two sided test for the null hypothesis that states the median difference in paired samples is zero. Specifically, the null hypothesis is that the benchmark classifier has equal or better performance than our proposed algorithm. Table <xref rid="Tab2" ref-type="table">2</xref> shows the performance of PASNet is significantly better than others, where the null hypotheses are rejected at the 5% significance level (<italic>p</italic>-value &lt;0.05). Hence, the outperformance of PASNet was statistically significant compared to the benchmark classifiers.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>The Wilcoxon signed-rank tests for comparing PASNet with the Benchmark Classifiers</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">W Statistic</th><th align="left"><italic>P</italic>-value</th></tr></thead><tbody><tr><td align="left">PASNet vs. Dropout NN</td><td align="left">146.5</td><td align="left">2.13e-06</td></tr><tr><td align="left">PASNet vs. RBF-SVM</td><td align="left">137.0</td><td align="left">1.35e-06</td></tr><tr><td align="left">PASNet vs. Random LASSO</td><td align="left">45.0</td><td align="left">1.06e-08</td></tr><tr><td align="left">PASNet vs. Logistic LASSO</td><td align="left">43.0</td><td align="left">9.52e-09</td></tr></tbody></table></table-wrap></p>
      <p>SVM and Dropout NN showed a higher AUC than LASSO logistic regression and Random LASSO, probably because of their capability of capturing nonlinear effects of genes. Compared to Dropout NN, PASNet is a relatively thin network, where the connections between layers are very sparse. However, PASNet interestingly produced higher performance than Dropout NN. It shows that PASNet builds a robust network model, which is simplified to represent the biological processes for prognosis prediction by incorporating biological prior knowledge.</p>
    </sec>
  </sec>
  <sec id="Sec6" sec-type="discussion">
    <title>Discussion</title>
    <p>Although PASNet yielded competitive predictive performance in the experiments, a more promising contribution of PASNet is in the model’s interpretability. In this section, we demonstrate a plausible biological mechanism inferred by PASNet for long-term survival prediction in GBM. The graphical representations of the PASNet model are illustrated in Figs. <xref rid="Fig2" ref-type="fig">2</xref>, <xref rid="Fig3" ref-type="fig">3</xref> and <xref rid="Fig4" ref-type="fig">4</xref> in the top-down order. The heatmaps were generated by sorting the weights and node values of LTS, and positive and negative weight values are colored in red and blue, respectively.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Graphical representation of the output node values over the samples by PASNet. LTS samples obtain higher node values in LTS node than non-LTS samples. Similarly, non-LTS samples obtain higher node values in non-LTS node than LTS samples</p></caption><graphic xlink:href="12859_2018_2500_Fig2_HTML" id="MO2"/></fig>
<fig id="Fig3"><label>Fig. 3</label><caption><p>Graphical representation among the output layer, hidden layer, and pathway layer in PASNet. (<bold>a</bold>) The weights between the hidden layer and the output layer. Hidden nodes are sorted in a descending order. (<bold>b</bold>) The node values in the hidden layer. The horizontal dotted lines indicates LTS/non-LTS samples. The vertical dotted lines indicates LTS/non-LTS samples are significantly distinguished by top 16 pathways. (<bold>c</bold>) The absolute weights between the pathway layer and the hidden layer</p></caption><graphic xlink:href="12859_2018_2500_Fig3_HTML" id="MO3"/></fig>
<fig id="Fig4"><label>Fig. 4</label><caption><p>Graphical representation of the 10 top-ranked pathways by PASNet (<bold>a</bold>) The absolute weights between the 10 top-ranked pathway nodes and the hidden layer. It is a zoom-in view of Fig. <xref rid="Fig3" ref-type="fig">3</xref>c. (<bold>b</bold>) Weights between the gene layer and the 10 top-ranked pathway nodes. The connections are determined by Reactome database</p></caption><graphic xlink:href="12859_2018_2500_Fig4_HTML" id="MO4"/></fig></p>
    <p>First, Fig. <xref rid="Fig2" ref-type="fig">2</xref> manifests the posterior probability of the samples in the clinical outcomes. The dark block on the top shows the output node values (−<italic>l</italic><italic>o</italic><italic>g</italic><sub>2</sub>(node value)) of the LTS samples, while the remaining ones are non-LTS samples. The weight values of the connections from hidden nodes to the output nodes are depicted in Fig. <xref rid="Fig3" ref-type="fig">3</xref>a, where dropped connections are colored in white. The figure reveals distinct patterns of weights (opposite signs) to the two output neurons. Note that there are hidden nodes disconnected to the neurons in the output layer (colored in white) by sparse coding, which shows that the hidden nodes are insignificant.</p>
    <p>The hidden node values of the samples are shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>b. The values of the hidden nodes indicate the intensity of the group effects on the pathways, which are connected to the hidden nodes. For instance, the first 16 hidden nodes in Fig. <xref rid="Fig3" ref-type="fig">3</xref>b show distinguishable intensities on LTS and non-LTS patients. The LTS patients present significant intensities of the group effects of the 16 pathways while non-LTS patients show significant lower values.</p>
    <p>The weights between the pathway nodes and the hidden nodes are exhibited in Fig. <xref rid="Fig3" ref-type="fig">3</xref>c, and the top-10 ranked pathways among them are zoomed in Fig. <xref rid="Fig4" ref-type="fig">4</xref>a. It appears that a small number of pathways mainly contribute to the hidden nodes simultaneously, which implies that the cohort of the pathways may be candidates of prognostic biomarkers in long-term survival of GBM. The top-10 ranked pathways include signaling by GPCR, GPCR downstream signaling, innate immune system, adaptive immune system, metabolism of carbohydrates, transmembrane transport of small molecules, developmental biology, metabolism of proteins, class A/1 (rhodopsin-like receptors), and axon guidance. Most of the pathways are referred to as significant pathways in GBM in biological literature. The pathways and the references are listed in Table <xref rid="Tab3" ref-type="table">3</xref>. Since the top-10 ranked pathways are all large (gene numbers &gt;200), we further explored small pathways as well. Class B/2 (Secretin family receptors) pathway which includes 88 genes is ranked 14th. One of the subgroups in Class B/2 family is categorized as brain-specific angiogenesis inhibitors that are growth suppressors of glioblastoma cells [<xref ref-type="bibr" rid="CR29">29</xref>]. Hence, Class B/2 pathway may play an important role in inhibition of GBM.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Top-10 ranked pathways for survival prediction in GBM by PASNet</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Pathway name</th><th align="left">Pathway size</th><th align="left">Reference</th><th align="left">Top-5 ranked genes<sup>a</sup></th></tr></thead><tbody><tr><td align="left">Signaling by GPCR</td><td align="left">920</td><td align="left">[<xref ref-type="bibr" rid="CR33">33</xref>]</td><td align="left">SHH, PTGFR, GNG5, CHRM5, LHB</td></tr><tr><td align="left">GPCR downstream signaling</td><td align="left">805</td><td align="left">[<xref ref-type="bibr" rid="CR50">50</xref>]</td><td align="left">PTGFR, OR7C2, GNG5, OR10H3, MLNR</td></tr><tr><td align="left">Innate immune system</td><td align="left">933</td><td align="left">[<xref ref-type="bibr" rid="CR35">35</xref>]</td><td align="left">CD79B, INPPL1, SRC, NUP85, DNM2</td></tr><tr><td align="left">Adaptive immune system</td><td align="left">539</td><td align="left">[<xref ref-type="bibr" rid="CR51">51</xref>]</td><td align="left">CD79B, ASB6, PTEN, NCF4, FBXO2</td></tr><tr><td align="left">Metabolism of carbohydrates</td><td align="left">247</td><td align="left">-</td><td align="left">HS3ST3B1, NUP85, PFKFB3, LUM, SLC2A4</td></tr><tr><td align="left">Transmembrane transport of small molecules</td><td align="left">413</td><td align="left">[<xref ref-type="bibr" rid="CR52">52</xref>]</td><td align="left">SLC9A7, ABCA7, GNG5, AQP8, HK3</td></tr><tr><td align="left">Developmental biology</td><td align="left">396</td><td align="left">-</td><td align="left">NRP2, FES, WNT10B, MYOD1, SLC2A4</td></tr><tr><td align="left">Metabolism of proteins</td><td align="left">518</td><td align="left">-</td><td align="left">EIF3G, CCT2, TIMM22, RPL3L, GMPPA</td></tr><tr><td align="left">Class A/1 (rhodopsin-like receptors)</td><td align="left">305</td><td align="left">[<xref ref-type="bibr" rid="CR53">53</xref>]</td><td align="left">PTGFR, OPRD1, CHRM5, NPFF, NTSR2</td></tr><tr><td align="left">Axon guidance</td><td align="left">251</td><td align="left">[<xref ref-type="bibr" rid="CR54">54</xref>]</td><td align="left">NRP2, NRTN, AGRN, FES, RPS6KA4</td></tr></tbody></table><table-wrap-foot><p><sup>a</sup>The genes were ranked by absolute weights in the pathways</p></table-wrap-foot></table-wrap></p>
    <p>The genes of the pathways are illustrated by the weight values in Fig. <xref rid="Fig4" ref-type="fig">4</xref>b. Since the connections between the gene layer and the pathway layer are given by pathway databases, e.g., Reactome, they are very sparse. It also shows that multiple pathways share genes in common. The genes, which are most frequently shown in the ten pathways, include CDC42, PRKCQ, RAC1, AKT1, AKT2, AKT3, C3, CREB1, GRB2, HRAS, KRAS, NRAS, PRKACA, PRKACB, PRKACG, RAF1, and YWHAB, where CDC42, PRKCQ, and RAC1 are shown in six pathways and others are in five pathways. Among them, several genes have been reported as biomarkers in GBM. For instance, AKT1, AKT2, and AKT3, belonging to the five pathways of signaling by GPCR, GPCR downstream signaling, innate immune system, adaptive immune system, and developmental biology, are three isoforms of AKT in PI3K/AKT pathway, which is an important drug target in many cancers including GBM [<xref ref-type="bibr" rid="CR30">30</xref>]. In particular, AKT2 is a well-known proto-oncogene that promotes the growth of tumors and reduces the survival of patients in GBM [<xref ref-type="bibr" rid="CR31">31</xref>, <xref ref-type="bibr" rid="CR32">32</xref>].</p>
    <p>Finally, we demonstrate a hierarchical representation of genes and pathways in PASNet. In Fig. <xref rid="Fig5" ref-type="fig">5</xref>a, PASNet is partially visualized, where positive and negative weights are colored in red and blue respectively. The pathways are represented by the corresponding genes in the pathway layer, and then the nonlinear effects of the pathways are described in the hidden layer. The hierarchical representations can be captured in the output layer, which produces a posterior probability for prognosis prediction. Although we considered a single hidden layer to simplify the model with HDLSS data in this study, multiple hidden layers may be able to capture the biological processes and their effects more accurately if a sufficient number of samples are available. Figure <xref rid="Fig5" ref-type="fig">5</xref>b–c illustrate distinctive representations of LTS and non-LTS samples in PASNet. The color of nodes in the figures shows the values computed with LTS/non-LTS samples in average. Note that node values between the pathway layer and the output layer are between zero and one. The node with a high value may be a potential prognostic biomarker in the group. Figure <xref rid="Fig5" ref-type="fig">5</xref>b shows that pathways including aquaporin-mediated transport, signaling by BMP, and cytokine signaling in immune system are activated with LTS samples. The second node in the hidden layer is triggered by the active pathways, and the hidden node activates the LTS node in the output layer. On the other hand, Fig. <xref rid="Fig5" ref-type="fig">5</xref>c shows that additional pathways of signaling by GPCR and innate immune system are also activated for non-LTS samples. The other two hidden nodes take the active pathways into account, and they activate the non-LTS node in the output layer. Hence, the two pathways of signaling by GPCR and innate immune system may be potential prognostic biomarkers for predicting LTS/non-LTS. Pathway of signaling by GPCR has been investigated as a potential therapeutic target to inhibit the progression of glioblastomas. [<xref ref-type="bibr" rid="CR33">33</xref>]. Activating the innate immune system, i.e. immunotherapy, is a promising strategy for the treatment of GBM [<xref ref-type="bibr" rid="CR34">34</xref>]. Vascular endothelial growth factor (VEGF), a modulator of the innate immune system, is reported crucial for the tumor progression [<xref ref-type="bibr" rid="CR35">35</xref>]. Moreover, aquaporin-mediated transport, signaling by BMP, and cytokine signaling in immune system may play an important role in GBM, since they are shown in common as active in both LTS and non-LTS. Note that the activation/inactivation of a node in PASNet does not directly represent biological activation in the system, whereas it indicates different states of the biological components in the groups.
<fig id="Fig5"><label>Fig. 5</label><caption><p>Hierarchical representation of pathways in PASNet. (<bold>a</bold>) PASNet is partially visualized showing the five pathways. Distinct neural network activations between LTS (<bold>b</bold>) and non-LTS (<bold>c</bold>) are shown via PASNet. The nodes of the neural network of (b) and (c) correspond to (a). For instance, the nodes in the pathway layer of (b) and (c) represent signaling by GPCR, innate immune system, aquaporin-mediated transport, signaling by BMP, and Cytokine signaling in immune system. The pathways of signaling by GPCR and innate immune system are inactive with LTS patients, whereas the both pathways are active with non-LTS patients</p></caption><graphic xlink:href="12859_2018_2500_Fig5_HTML" id="MO5"/></fig></p>
  </sec>
  <sec id="Sec7" sec-type="conclusion">
    <title>Conclusions</title>
    <p>In this paper, we proposed pathway-associated sparse deep neural network for prognosis predictions (long-term survivals in GBM in this study). PASNet builds a network model by leveraging prior biological knowledge of pathway databases and by taking hierarchical nonlinear relationships of biological processes into account. To improve the model interpretability, PASNet introduces sparse coding. Moreover, we developed a training strategy to avoid the overfitting problem with HDLSS data and the imbalanced problem.</p>
    <p>To investigate the performance of PASNet, we used gene expression data of GBM patients in TCGA. PASNet was assessed by comparing the predictive performance with support vector machine, random LASSO, LASSO logistic Regression, and neural network with dropout that have been widely used for prognosis prediction. PASNet outperformed them with respect to both AUC and F1-score in the multiple stratified 5-fold cross-validation experiments. Furthermore, we discussed how PASNet can describe the biological system of GBM.</p>
    <p>PASNet is the first deep neural network-based model that represents hierarchical representations of genes and pathways and their nonlinear effects, to the best of our knowledge. Additionally, PASNet would be promising due to its flexible model representation and interpretability, embodying the strengths of deep learning.</p>
  </sec>
  <sec id="Sec8">
    <title>Methods</title>
    <sec id="Sec9">
      <title>The architecture of PASNet</title>
      <p>PASNet incorporates biological pathways and the concept of sparse modeling based on Deep Neural Network (DNN). The neural network architecture of PASNet consists of a gene layer (an input layer), a pathway layer that represents the biological pathways linked with input genes, a hidden layer that represents hierarchical relationships among biological pathways, and an output layer that corresponds with clinical outcomes, e.g. a binary class that has long-term survival and short-term survival, stages of cancer (see Fig. <xref rid="Fig6" ref-type="fig">6</xref>).
<fig id="Fig6"><label>Fig. 6</label><caption><p>Architecture of PASNet. The structure of PASNet is constructed by a gene layer (an input layer), a pathway layer that represents the biological pathways linked with input genes, a hidden layer that represents hierarchical relationships among biological pathways, and an output layer that corresponds with clinical outcomes, e.g. a binary class that has long-term survival and short-term survival, stages of cancer</p></caption><graphic xlink:href="12859_2018_2500_Fig6_HTML" id="MO6"/></fig></p>
      <p>In PASNet, sparse coding is considered on the connections between layers for model interpretability. Sparse coding provides a solution to capture significant components of a biological mechanism in the model, since biological processes may involve only a few biological components. On the other hand, conventional fully-connected networks lack to represent biological mechanisms.</p>
      <sec id="Sec10">
        <title>Gene layer</title>
        <p>The gene layer (as an input layer) corresponds to gene expression data. A patient sample of <italic>m</italic> gene expressions is formed as a column vector, which is denoted by <bold>x</bold>={<italic>x</italic><sub>1</sub>,<italic>x</italic><sub>2</sub>,...,<italic>x</italic><sub><italic>m</italic></sub>}. Each input node represents one gene feature.</p>
      </sec>
      <sec id="Sec11">
        <title>Pathway layer</title>
        <p>The pathway layer represents biological pathways, where each node indicates an individual pathway. The connections between the gene layer and the pathway layer are established by well-known pathway databases (e.g., Reactome and KEGG). Pathway databases contain associations between pathways and genes; each of which provides a set of gene components. Therefore, the pathway layer makes it possible to interpret the model as a pathway-based analysis.</p>
        <p>To begin with initializing the connections between the gene layer and the pathway layer, we consider a binary biadjacency matrix (<bold>A</bold>) from biological pathway databases. The biadjacency matrix can be defined as <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\textbf {A} \in \mathbb {B}^{n \times m}$\end{document}</tex-math><mml:math id="M2"><mml:mtext mathvariant="bold">A</mml:mtext><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">𝔹</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2018_2500_Article_IEq1.gif"/></alternatives></inline-formula>, where <italic>n</italic> is number of pathways and <italic>m</italic> is number of genes. Then, an element of <bold>A</bold>, i.e., <italic>a</italic><sub><italic>ij</italic></sub>, is set to one if gene <italic>j</italic> belongs to pathway <italic>i</italic>; otherwise, zero. Sparse coding is applied based on the matrix <bold>A</bold> to represent the relationships between genes and pathways in the model.</p>
      </sec>
      <sec id="Sec12">
        <title>Hidden layer</title>
        <p>Biological components may cooperate with others instead of functioning alone. A biological system involves multiple pathways which have interactions together, whereas a node in the pathway layer indicates a biological pathway. The associative interactions between pathways can be represented in the hidden layer. In PASNet, the hidden layer represents biological nonlinear associations between the pathways to outputs.</p>
        <p>Sparse coding between the pathway and the hidden layers enables one to interpret these relationships. Although we consider only a single hidden layer in this study for simplicity’s sake, multiple hidden layers can be used for deeper hierarchical representations of pathways. For example, if there are two hidden layers, the second hidden layer will represent deeper hierarchical associations of the nodes of the first hidden layer, which are association effects of pathways.</p>
      </sec>
      <sec id="Sec13">
        <title>Output layer</title>
        <p>The output layer shows clinical outcomes for which nodes compute the posterior probabilities. In this layer, sparse coding allows to distinguish hierarchical groups of pathways (which are detected from hidden layers) to predict clinical outcomes. In PASNet, more than two clinical outcomes can be easily represented with multiple nodes in the output layer.</p>
        <p>Consequently, PASNet can dissect distinguishable biological processes of hierarchical nonlinear relationships and associations of genes and pathways to predict clinical outcomes. Furthermore, this <italic>generative</italic> model-based approach would be often useful to predict prognosis accurately with complex data of HDLSS. When data is highly complex and only small sample sizes are available, model optimization may be easily biased to the training data rather than providing a general solution. On the other hand, the integration of the biological structures and prior knowledge to the model would produce a robust solution.</p>
      </sec>
    </sec>
    <sec id="Sec14">
      <title>Overall description of PASNet training</title>
      <p>The main challenge in training PASNet is to reduce both risk of overfitting and computational complexity of training on HDLSS data. The related works that have handled the HDLSS data problem are discussed in “<xref rid="Sec17" ref-type="sec">Related works in deep learning</xref>” section. To unravel the problems, PASNet optimizes a small sub-network, which involves feasible nodes and parameters to train instead of the whole network and then makes the sub-network sparse. Figure <xref rid="Fig7" ref-type="fig">7</xref> illustrates the overall training flow of PASNet.
<fig id="Fig7"><label>Fig. 7</label><caption><p>Training of PASNet. (<bold>a</bold>) Weights and biases are randomly initialized. Connections between the gene layer and the pathway layer are determined by biological pathway databases, and the remaining layers are considered as fully-connected in this step. (<bold>b</bold>) A sub-network is randomly selected using a dropout technique and trained. (<bold>c</bold>) Sparse coding optimizes the sparsity of connections in the sub-network</p></caption><graphic xlink:href="12859_2018_2500_Fig7_HTML" id="MO7"/></fig></p>
      <p>First, we initialize the connections between the gene layer and the pathway layer with prior biological knowledge of pathways (see Fig. <xref rid="Fig7" ref-type="fig">7</xref>a). Active/inactive connections are determined by the biadjacency matrix, <bold>A</bold>. The weights of active connections and biases are randomly initialized from standard normal distribution, while the weights of inactive connections are set to zero. The sparsity of the connections between the gene layer and the pathway layer is invariant over the entire training. The remaining layers are fully interconnected as the initial.</p>
      <p>In the training phase, we repeat training sub-networks and applying sparse coding on the sub-networks until convergence (Fig. <xref rid="Fig7" ref-type="fig">7</xref>b–c). A sub-network is selected by a dropout technique, where neurons are randomly dropped in the intermediate layers. In Fig. <xref rid="Fig7" ref-type="fig">7</xref>b, a small sub-network is shown with bold solid circles and lines. Then, the small sub-network is trained by feed-forward and backpropagation. Note that only weights and biases of the sub-network are trained. Upon the completion of the sub-network’s training, sparse coding is applied to the sub-network by trimming the connections that do not contribute or worsen to minimize the loss. In Fig. <xref rid="Fig7" ref-type="fig">7</xref>c, the dropped connections and nodes are marked as bold, dashed lines. The details of the training are elucidated in the following sections.</p>
    </sec>
    <sec id="Sec15">
      <title>Sparse coding</title>
      <p>Once a small sub-network is completed to train with the HDLSS data, the sub-network is imposed to be sparse for the model interpretation. The sparsity of the sub-network is determined by the mask matrix <bold>M</bold> on each layer as: 
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \textbf{h}^{(\ell +1)} = a\left(\left(\textbf{W}^{(\ell)}\star\textbf{M}^{(\ell)}\right)\textbf{h}^{(\ell)}+\textbf{b}^{(\ell)}\right),   $$ \end{document}</tex-math><mml:math id="M4"><mml:msup><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⋆</mml:mo><mml:msup><mml:mrow><mml:mtext mathvariant="bold">M</mml:mtext></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mtext mathvariant="bold">h</mml:mtext></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mtext mathvariant="bold">b</mml:mtext></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2018_2500_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where ⋆ denotes element-wise multiplication, and <italic>a</italic>(·) is an activation function. <bold>h</bold><sup>(<italic>ℓ</italic>)</sup> denotes an output vector on the <italic>ℓ</italic>-th layer, and <bold>W</bold><sup>(<italic>ℓ</italic>)</sup> and <bold>b</bold><sup>(<italic>ℓ</italic>)</sup> are a weight matrix and a bias vector, respectively. An element value of <bold>M</bold> is either one or zero, which determines whether the associated weights are dropped in the current epoch.</p>
      <p>The mask matrix <bold>M</bold> is generated with respect to a sparsity level (<italic>S</italic>) that indicates the proportion of weights to be dropped in a single layer. <italic>S</italic> is a value between 0 to 100, where zero creates a fully-connected layer while 100 causes no connection. The optimal <italic>S</italic><sup>∗</sup> is approximated on each layer individually in the sub-network, while most related methods consider a single hyper-parameter for the sparsity of all layers [<xref ref-type="bibr" rid="CR36">36</xref>, <xref ref-type="bibr" rid="CR37">37</xref>]. The individual setting of the sparsity on each layer shows different levels of biological associations on the genes and pathways.</p>
      <p>We obtain the optimal sparsity level <italic>S</italic><sup>∗</sup> that minimizes the cost score. For efficient computation, the cost scores are computed with a small number of finite sparsity levels. Then, the optimal sparsity level is estimated by applying a cubic-spline interpolation to the cost scores with the assumption that the cost function, with respect to the sparsity level, is continuous.</p>
      <p>In particular, an element of <bold>M</bold> is set to one if the absolute value of the corresponding weight is greater than threshold <italic>Q</italic>; otherwise, the element is zero, where <italic>Q</italic> is an <italic>S</italic>-th percentile of absolute values of <bold>W</bold>. Note that the mask between the gene layer and the pathway layer, i.e. <bold>M</bold><sup>(0)</sup>, is determined by the biadjacency matrix <bold>A</bold> of biological pathways. Thus, the mask matrices are formulated as 
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \textbf{M}^{(\ell)}= \begin{cases} \mathbbm{1}\left(\lvert \textbf{W}^{(\ell)} \rvert \geq Q^{(\ell)}\right), &amp; \text{if}\, \ell \neq 0 \\ \textbf{A}, &amp; \text{if}\, \ell = 0 \end{cases}   $$ \end{document}</tex-math><mml:math id="M6"><mml:msup><mml:mrow><mml:mtext mathvariant="bold">M</mml:mtext></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfenced close="" open="{" separators=""><mml:mrow><mml:mtable class="array" columnalign="left"><mml:mtr><mml:mtd><mml:mi>𝟙</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:mo>≥</mml:mo><mml:msup><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="1em"/></mml:mtd><mml:mtd><mml:mtext>if</mml:mtext><mml:mspace width="0.3em"/><mml:mi>ℓ</mml:mi><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext mathvariant="bold">A</mml:mtext><mml:mo>,</mml:mo><mml:mspace width="1em"/></mml:mtd><mml:mtd><mml:mtext>if</mml:mtext><mml:mspace width="0.3em"/><mml:mi>ℓ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2018_2500_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <italic>Q</italic><sup>(<italic>ℓ</italic>)</sup> is the <italic>S</italic>-th percentile of |<bold>W</bold><sup>(<italic>ℓ</italic>)</sup>| if <italic>ℓ</italic>≠0.</p>
    </sec>
    <sec id="Sec16">
      <title>Cost-sensitive learning for imbalanced data</title>
      <p>We refine the cost function and the backpropagation for cost-sensitive learning, since imbalanced data causes bias of the predictions towards the majority class. We adapt the Mean False Error (MFE) method [<xref ref-type="bibr" rid="CR38">38</xref>], which penalizes the errors of the majority class.</p>
      <p>Let <italic>K</italic> be the number of clinical outcomes. The normalized cost is computed separately for each class by: 
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \mathcal{L} = \sum\limits_{k=1}^{K} \mathcal{C}_{k} + \frac{1}{2}\lambda\lVert \textbf{W}\lVert_{2},   $$ \end{document}</tex-math><mml:math id="M8"><mml:mi mathvariant="script">ℒ</mml:mi><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi mathvariant="script">C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mi>λ</mml:mi><mml:mo>∥</mml:mo><mml:mtext mathvariant="bold">W</mml:mtext><mml:msub><mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2018_2500_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>
        <disp-formula id="Equ4">
          <label>4</label>
          <alternatives>
            <tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \mathcal{C}_{k} = \frac{1}{n_{k}}\sum\limits_{i=1}^{n_{k}}c\left(\textbf{y}_{i}, \tilde{\textbf{y}}_{i}\right),  $$ \end{document}</tex-math>
            <mml:math id="M10">
              <mml:msub>
                <mml:mrow>
                  <mml:mi mathvariant="script">C</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>k</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>=</mml:mo>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
                <mml:mrow>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>n</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>k</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
              </mml:mfrac>
              <mml:munderover accent="false" accentunder="false">
                <mml:mrow>
                  <mml:mo>∑</mml:mo>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>i</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
                <mml:mrow>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>n</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>k</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
              </mml:munderover>
              <mml:mi>c</mml:mi>
              <mml:mfenced close=")" open="(" separators="">
                <mml:mrow>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mtext mathvariant="bold">y</mml:mtext>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>,</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mover accent="true">
                        <mml:mrow>
                          <mml:mtext mathvariant="bold">y</mml:mtext>
                        </mml:mrow>
                        <mml:mo>~</mml:mo>
                      </mml:mover>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
              </mml:mfenced>
              <mml:mo>,</mml:mo>
            </mml:math>
            <graphic xlink:href="12859_2018_2500_Article_Equ4.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>where <inline-formula id="IEq2"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {C}_{k}$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mrow><mml:mi mathvariant="script">C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2018_2500_Article_IEq2.gif"/></alternatives></inline-formula> denotes mean error on the class <italic>k</italic>, and <italic>n</italic><sub><italic>k</italic></sub> is the number of samples in the class <italic>k</italic>. <bold>y</bold><sub><italic>i</italic></sub> is a vectorized ground truth class label of the <italic>i</italic>-th sample, and <inline-formula id="IEq3"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\tilde {\textbf {y}}_{i}$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext mathvariant="bold">y</mml:mtext></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2018_2500_Article_IEq3.gif"/></alternatives></inline-formula> is its vectorized prediction. <italic>c</italic>(·) denotes a cost function (e.g., cross-entropy loss), and <inline-formula id="IEq4"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {L}$\end{document}</tex-math><mml:math id="M16"><mml:mi mathvariant="script">ℒ</mml:mi></mml:math><inline-graphic xlink:href="12859_2018_2500_Article_IEq4.gif"/></alternatives></inline-formula> is the total cost. ∥<bold>W</bold>∥<sub>2</sub> denotes a <italic>L</italic><sup>2</sup>-norm of <bold>W</bold>, and <italic>λ</italic>&gt;0 is a regularization hyperparameter.</p>
      <p>In the backpropagation phrase, the gradient is also computed separately for each class. Hence, the weights and biases on the <italic>ℓ</italic>-th layer are updated by: 
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{@{}rcl@{}} \textbf{W}^{(\ell)} &amp;\leftarrow&amp; (1-\eta\lambda)\textbf{W}^{(\ell)} - \eta\sum\limits_{k=1}^{K} \frac{\partial \mathcal{C}_{k}}{\partial \textbf{W}^{(\ell)}},  \end{array} $$ \end{document}</tex-math><mml:math id="M18"><mml:mtable class="eqnarray" columnalign="left center right"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msup><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>←</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ηλ</mml:mi><mml:mo>)</mml:mo><mml:msup><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="script">C</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msup><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2018_2500_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>
        <disp-formula id="Equ6">
          <label>6</label>
          <alternatives>
            <tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{@{}rcl@{}} \textbf{b}^{(\ell)} &amp;\leftarrow&amp; \textbf{b}^{(\ell)} - \eta\sum\limits_{k=1}^{K} \frac{\partial \mathcal{C}_{k}}{\partial \textbf{b}^{(\ell)}},  \end{array} $$ \end{document}</tex-math>
            <mml:math id="M20">
              <mml:mtable class="eqnarray" columnalign="left center right">
                <mml:mtr>
                  <mml:mtd class="eqnarray-1">
                    <mml:msup>
                      <mml:mrow>
                        <mml:mtext mathvariant="bold">b</mml:mtext>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mi>ℓ</mml:mi>
                        <mml:mo>)</mml:mo>
                      </mml:mrow>
                    </mml:msup>
                  </mml:mtd>
                  <mml:mtd class="eqnarray-2">
                    <mml:mo>←</mml:mo>
                  </mml:mtd>
                  <mml:mtd class="eqnarray-3">
                    <mml:msup>
                      <mml:mrow>
                        <mml:mtext mathvariant="bold">b</mml:mtext>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mi>ℓ</mml:mi>
                        <mml:mo>)</mml:mo>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mo>−</mml:mo>
                    <mml:mi>η</mml:mi>
                    <mml:munderover accent="false" accentunder="false">
                      <mml:mrow>
                        <mml:mo>∑</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>k</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>K</mml:mi>
                      </mml:mrow>
                    </mml:munderover>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mi>∂</mml:mi>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi mathvariant="script">C</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>k</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>∂</mml:mi>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mtext mathvariant="bold">b</mml:mtext>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mo>(</mml:mo>
                            <mml:mi>ℓ</mml:mi>
                            <mml:mo>)</mml:mo>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                    </mml:mfrac>
                    <mml:mo>,</mml:mo>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
            <graphic xlink:href="12859_2018_2500_Article_Equ6.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>where <italic>η</italic> is a learning rate. The algorithm of PASNet is briefly described in Algorithm 1.</p>
      <p>
        <graphic position="anchor" xlink:href="12859_2018_2500_Figa_HTML" id="MO8"/>
      </p>
    </sec>
  </sec>
  <sec id="Sec17">
    <title>Related works in deep learning</title>
    <p>In recent years, deep learning has been spotlighted as the most active research field in various machine learning communities, such as image analysis, speech recognition, and natural language processing as its promising potential is being actively discussed in bioinformatics and biomedicine [<xref ref-type="bibr" rid="CR39">39</xref>]. Most deep learning-based approaches have been developed for classification and association studies in bioinformatics. For instance, D-GEX infers the expression of target genes from landmark genes, capturing the nonlinear relationships by combining gene expression, DNA methylation, and miRNA expression data [<xref ref-type="bibr" rid="CR40">40</xref>]. A convolutional neural network (CNN) was adapted to predict DNA-protein binding sites with Chromatin Immunoprecipitation sequencing (ChIP-seq) data [<xref ref-type="bibr" rid="CR41">41</xref>]. Additionally, CNN-based DeepBind was proposed to predict whether a specific DNA/RNA binding protein will bind to a specific DNA sequence [<xref ref-type="bibr" rid="CR42">42</xref>]. The functionality of non-coding variants was predicted by DeepSEA by employing a CNN model [<xref ref-type="bibr" rid="CR43">43</xref>].</p>
    <p>Although only a small subset of deep learning research has been reported in bioinformatics due to the difficulty of structure definition and interpretation, the future of deep learning in biology and medicine is promising [<xref ref-type="bibr" rid="CR44">44</xref>]. First, since a neural network is inspired by the neurons in the human brain, a neuron network architecture is applicable to modeling a mechanism for a complex biological system. Specifically, deep learning approaches take advantage of flexible representation of hierarchical structures from inputs to outputs. The representation of nonlinear effects of neurons in multiple layers in neural networks may be able to model hierarchical biological signals. DCell constructs a multi-layer neural network based on extensive prior biological knowledge to simulate the growth of a eukaryotic cell [<xref ref-type="bibr" rid="CR45">45</xref>]. However, DCell’s network architecture is entirely based on well-known prior biological knowledge, so the model was applied to relatively simple biological system of yeast. Moreover, deep learning captures nonlinear effects of variables with high-level feature representation, which allows deep learning to outperform other state-of-the-art methods.</p>
    <p>However, training deep neural networks with HDLSS data poses a computational problem. A large number of parameters are involved in deep neural networks, and it often makes the training infeasible or causes a model overfit on HDLSS data. Particularly, backpropagation gradients in neural networks are of high variance on HDLSS data, which consequently causes the model overfit [<xref ref-type="bibr" rid="CR46">46</xref>]. In order to tackle the HDLSS problem, the leave-one-out approach was used to avoid the overfitting problem in backpropagation [<xref ref-type="bibr" rid="CR47">47</xref>]. Regarding backpropagation, the risk of overfitting was examined with validation data by the leave-one-out approach and terminates the training early when overfitting occurs. For an alternative solution, an attempt to reduce the dimensionality of the input space to a feasible size has been made [<xref ref-type="bibr" rid="CR48">48</xref>]. Dimension reduction techniques, such as subsampled randomized Hadamard transform (SRHT) and Count Sketch-base construction, were utilized to reduce the dimensional size of the input data. Then, the projected data into the lower space were introduced to a neural network for training.</p>
    <p>For HDLSS data, feature selection is one of the conventional approaches. Deep Feature Selection (DFS) was developed to select a discriminative feature subset in a deep learning model [<xref ref-type="bibr" rid="CR49">49</xref>]. Although DFS is not the optimal solution to low-sample size data, DFS shows that deep learning can detect informative and discriminative features of nonlinearity effects through multiple layers with high-dimensional data. Then, Deep Neural Pursuit (DNP) improved the solution of the feature selection in deep learning, taking the HDLSS data problem into account [<xref ref-type="bibr" rid="CR46">46</xref>]. DNP iteratively augments features in the input layer by performing multiple dropouts. The multiple dropouts grant the ability to train a small-sized sub-network at a time and to compute gradients with low variance for alleviating the overfitting problem.</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>Adam</term>
        <def>
          <p>Adaptive moment estimation</p>
        </def>
      </def-item>
      <def-item>
        <term>AUC</term>
        <def>
          <p>Area under the curve</p>
        </def>
      </def-item>
      <def-item>
        <term>ChIP-seq</term>
        <def>
          <p>Chromatin Immunoprecipitation sequencing</p>
        </def>
      </def-item>
      <def-item>
        <term>CNN</term>
        <def>
          <p>Convolutional neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>DFS</term>
        <def>
          <p>Deep feature selection</p>
        </def>
      </def-item>
      <def-item>
        <term>DNN</term>
        <def>
          <p>Deep neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>DNP</term>
        <def>
          <p>Deep neural pursuit</p>
        </def>
      </def-item>
      <def-item>
        <term>Dropout NN</term>
        <def>
          <p>Neural network with dropout</p>
        </def>
      </def-item>
      <def-item>
        <term>GBM</term>
        <def>
          <p>Glioblastoma multiforme</p>
        </def>
      </def-item>
      <def-item>
        <term>HDLSS</term>
        <def>
          <p>High-dimension, low-sample size</p>
        </def>
      </def-item>
      <def-item>
        <term>KPCA</term>
        <def>
          <p>Kernel principle component analysis</p>
        </def>
      </def-item>
      <def-item>
        <term>LLR</term>
        <def>
          <p>LASSO logistic regression</p>
        </def>
      </def-item>
      <def-item>
        <term>MFE</term>
        <def>
          <p>Mean false error</p>
        </def>
      </def-item>
      <def-item>
        <term>MSigDB</term>
        <def>
          <p>Molecular signatures database</p>
        </def>
      </def-item>
      <def-item>
        <term>PASNet</term>
        <def>
          <p>Pathway-associated sparse deep neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>RBF</term>
        <def>
          <p>Radial basis function</p>
        </def>
      </def-item>
      <def-item>
        <term>RBM</term>
        <def>
          <p>Restricted Boltzmann machine</p>
        </def>
      </def-item>
      <def-item>
        <term>ROC</term>
        <def>
          <p>Receiver operating characteristic</p>
        </def>
      </def-item>
      <def-item>
        <term>SRHT</term>
        <def>
          <p>Subsampled randomized hadamard transform</p>
        </def>
      </def-item>
      <def-item>
        <term>SVM</term>
        <def>
          <p>Support vector machine</p>
        </def>
      </def-item>
      <def-item>
        <term>TCGA</term>
        <def>
          <p>The cancer genome atlas</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ack>
    <title>Acknowledgements</title>
    <p>We would like to thank Dr. Jung Hun Oh for his help and advice in this study.</p>
    <sec id="d29e2176">
      <title>Funding</title>
      <p>Not applicable.</p>
    </sec>
    <sec id="d29e2181">
      <title>Availability of data and materials</title>
      <p>The datasets are publicly available and accessible at <ext-link ext-link-type="uri" xlink:href="http://cancergenome.nih.gov">http://cancergenome.nih.gov</ext-link>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>MK designed and supervised the research project. JH developed and implemented the algorithms, and performed the data analyses. JH and MK wrote the manuscript. YK helped the implementation and performed the experiments. TK discussed and verified the biological interpretation of PASNet. All authors have read and approved the final version of the manuscript.</p>
  </notes>
  <notes notes-type="COI-statement">
    <sec>
      <title>Ethics approval and consent to participate</title>
      <p>Not applicable.</p>
    </sec>
    <sec>
      <title>Consent for publication</title>
      <p>Not applicable.</p>
    </sec>
    <sec>
      <title>Competing interests</title>
      <p>The authors declare that they have no competing interests.</p>
    </sec>
    <sec>
      <title>Publisher’s Note</title>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </sec>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <mixed-citation publication-type="other">Lu J, Cowperthwaite MC, Burnett MG, Shpak M. Molecular Predictors of Long-Term Survival in Glioblastoma Multiforme Patients. PloS ONE. 2016; 11(4):0154313. 10.1371/journal.pone.0154313.</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <mixed-citation publication-type="other">Onaitis MW, et al. Prediction of Long-Term Survival After Lung Cancer Surgery for Elderly Patients in The Society of Thoracic Surgeons General Thoracic Surgery Database. Ann Thorac Surg. 2018; 105(1):309–16. 10.1016/j.athoracsur.2017.06.071.</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <mixed-citation publication-type="other">Cao Y, et al. Prediction of long-term survival rates in patients undergoing curative resection for solitary hepatocellular carcinoma. Oncol Letters. 2018; 15(2):2574–82. 10.3892/ol.2017.7612.</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <mixed-citation publication-type="other">Jin L, et al. Pathway-based Analysis Tools for Complex Diseases: A Review. Genomics Proteomics Bioinforma. 2014; 12(5):210–20. 10.1016/j.gpb.2014.10.002.</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <mixed-citation publication-type="other">Kim S, Kon M, DeLisi C. Pathway-based classification of cancer subtypes. Biol Direct. 2012; 7:21. 10.1186/1745-6150-7-21.</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <mixed-citation publication-type="other">Cirillo E, Parnell LD, Evelo CT. A review of pathway-based analysis tools that visualize genetic variants. Front Genet. 2017; 8(174):174. 10.3389/fgene.2017.00174.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <mixed-citation publication-type="other">Drier Y, Sheffer M, Domany E. Pathway-based personalized analysis of cancer. Proc Natl Acad Sci U S A. 2013; 110(16):6388–93. 10.1073/pnas.1219651110.</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <mixed-citation publication-type="other">Mallavarapu T, Kim Y, Oh JH, Kang M. R-pathcluster: Identifying cancer subtype of glioblastoma multiforme using pathway-based restricted boltzmann machine. In: 2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM).2017. p. 1183–8. 10.1109/BIBM.2017.8217825.</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <mixed-citation publication-type="other">Huang S, et al. Novel personalized pathway-based metabolomics models reveal key metabolic pathways for breast cancer diagnosis. Genome Med. 2016; 8(1):34. 10.1186/s13073-016-0289-9.</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <mixed-citation publication-type="other">Li Y, Nan B, Zhu J. Multivariate sparse group lasso for the multivariate multiple linear regression with an arbitrary group structure. Biometrics. 2015; 71(2):354–63. 10.1111/biom.12292. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/15334406">15334406</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <mixed-citation publication-type="other">Raser JM, O’Shea EK. Noise in Gene Expression: Orgins, Consequences, and Control. Science. 2005; 309(5743):2010–3. 10.1126/science.1105891. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/NIHMS150003">NIHMS150003</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <mixed-citation publication-type="other">Steyerberg EW, Eijkemans MJC, Habbema JDF. Application of Shrinkage Techniques in Logistic Regression Analysis: A Case Study. Statistica Neerlandica. 2001; 55(1):76–88. 10.1111/1467-9574.00157.</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <mixed-citation publication-type="other">Wang S, Nan B, Rosset S, Zhu J. Random lasso. Ann Appl Stat. 2011; 5(1):468–85. 10.1214/10-AOAS377. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1104.3398">http://arxiv.org/abs/1104.3398</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <mixed-citation publication-type="other">Musoro JZ, Zwinderman AH, Puhan MA, Ter Riet G, Geskus RB. Validation of prediction models based on lasso regression with multiply imputed data. BMC Med Res Methodol. 2014;14(1). 10.1186/1471-2288-14-116.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <mixed-citation publication-type="other">Liu D, Lin X, Ghosh D. Semiparametric regression of multidimensional genetic pathway data: Least-squares kernel machines and linear mixed models. Biometrics. 2007; 63(4):1079–88. 10.1111/j.1541-0420.2007.00799.x.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <mixed-citation publication-type="other">Liu D, Ghosh D, Lin X. Estimation and testing for the effect of a genetic pathway on a disease outcome using logistic kernel machine regression via logistic mixed models. BMC Bioinformatics. 2008;9. 10.1186/1471-2105-9-292.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <mixed-citation publication-type="other">Bach FR, Lanckriet GRG, Jordan MI. Multiple kernel learning, conic duality, and the SMO algorithm. In: Twenty-first International Conference on Machine Learning - ICML ’04. 2004. p. 6. 10.1145/1015330.1015424. <ext-link ext-link-type="uri" xlink:href="http://portal.acm.org/citation.cfm?doid=1015330.1015424">http://portal.acm.org/citation.cfm?doid=1015330.1015424</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <mixed-citation publication-type="other">Sinnott JA, Cai T. Pathway aggregation for survival prediction via multiple kernel learning. Stat Med. 2018;0(0). 10.1002/sim.7681. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7681">http://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7681</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <mixed-citation publication-type="other">Kumari S, et al. Bottom-up GGM algorithm for constructing multilayered hierarchical gene regulatory networks that govern biological pathways or processes. BMC Bioinformatics. 2016;17(1). 10.1186/s12859-016-0981-1.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <mixed-citation publication-type="other">Deng W, Zhang K, Busov V, Wei H. Recursive random forest algorithm for constructing multilayered hierarchical gene regulatory networks that govern biological pathways. PLoS ONE. 2017;12(2). 10.1371/journal.pone.0171532.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <mixed-citation publication-type="other">Pham LM, Carvalho L, Schaus S, Kolaczyk ED. Perturbation Detection Through Modeling of Gene Expression on a Latent Biological Pathway Network: A Bayesian Hierarchical Approach. J Am Stat Assoc. 2016; 111(513):73–92. 10.1080/01621459.2015.1110523. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1409.0503">http://arxiv.org/abs/1409.0503</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <mixed-citation publication-type="other">Kher S, Peng J, Wurtele ES, Dickerson J. In: Pérez-Sánchez H, (ed).Hierarchical Biological Pathway Data Integration and Mining, Bioinformatics: IntechOpen; 2012. 10.5772/49974. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.intechopen.com/books/bioinformatics/hierarchical-biological-pathway-data-integration-and-mining">https://www.intechopen.com/books/bioinformatics/hierarchical-biological-pathway-data-integration-and-mining</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <mixed-citation publication-type="other">Hanif F, Muzaffar K, Perveen K, Malhi SM, Simjee SU. Glioblastoma Multiforme: A Review of its Epidemiology and Pathogenesis through Clinical Presentation and Treatment. Asian Pac J Cancer Prev. 2017; 18(1):3–9. <pub-id pub-id-type="doi">10.22034/APJCP.2017.18.1.3</pub-id>.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <mixed-citation publication-type="other">Davis ME. Glioblastoma: Overview of Disease and Treatment. Clin J Oncol Nurs. 2016; 20(5):1–14. 10.1188/16.CJON.S1.2-8.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <mixed-citation publication-type="other">Walid MS. Prognostic factors for long-term survival after glioblastoma. Permanente J. 2008; 12(4):45–8. 10.7812/TPP/08-027.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <mixed-citation publication-type="other">Liberzon A, Birger C, Thorvaldsdóttir H, Ghandi M, Mesirov J, Tamayo P. The molecular signatures database hallmark gene set collection. Cell Syst. 2015; 1(6):417–25. 10.1016/j.cels.2015.12.004.</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J. Adam: A method for stochastic optimization. CoRR. 2014;abs/1412.6980. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <mixed-citation publication-type="other">Hsu C-W, Chang C-C, Lin C-J. A Practical Guide to Support Vector Classification. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf">https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf</ext-link>. Accessed 15 June 2008.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29</label>
      <mixed-citation publication-type="other">Harmar AJ. Family-B G-protein-coupled receptors. Genome Biol. 2001; 2(12):3013–1301310. 10.1186/gb-2001-2-12-reviews3013.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30</label>
      <mixed-citation publication-type="other">Joy A, et al. The role of AKT isoforms in glioblastoma: AKT3 delays tumor progression. J Neuro-Oncol. 2016; 130(1):43–52. 10.1007/s11060-016-2220-z.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31</label>
      <mixed-citation publication-type="other">Hu B, et al. Astrocyte elevated gene-1 interacts with Akt isoform 2 to control glioma growth, survival, and pathogenesis. Cancer Res. 2014; 74(24):7321–32. 10.1158/0008-5472.CAN-13-2978.</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32</label>
      <mixed-citation publication-type="other">Hinske LC, et al. Intronic mirna-641 controls its host gene’s pathway pi3k/akt and this relationship is dysfunctional in glioblastoma multiforme. Biochem Biophys Res Commun. 2017; 489(4):477–83. 10.1016/j.bbrc.2017.05.175.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33</label>
      <mixed-citation publication-type="other">Cherry AE, Stella N. G protein-coupled receptors as oncogenic signals in glioma: Emerging therapeutic avenues. Neuroscience. 2014; 278(1):222–36. 10.1016/j.neuroscience.2014.08.015.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34</label>
      <mixed-citation publication-type="other">Lim M, Xia Y, Bettegowda C, Weller M. Current state of immunotherapy for glioblastoma. Nat Rev Clin Oncol. 2018; 15(7):422–42. <pub-id pub-id-type="doi">10.1038/s41571-018-0003-5</pub-id>.</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35</label>
      <mixed-citation publication-type="other">Turkowski K, et al. VEGF as a modulator of the innate immune response in glioblastoma. GLIA. 2018; 66(1):161–74. <pub-id pub-id-type="doi">10.1002/glia.23234</pub-id>.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36</label>
      <mixed-citation publication-type="other">Han S, et al. DSD: Dense-Sparse-Dense Training for Deep Neural Networks. Int Conf Learn Represent. 2017. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1607.04381">http://arxiv.org/abs/1607.04381</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37</label>
      <mixed-citation publication-type="other">Wang B, Klabjan D. Regularization for Unsupervised Deep Neural Nets. CoRR. 2016; 1:1–7. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1608.04426">http://arxiv.org/abs/1608.04426</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38</label>
      <mixed-citation publication-type="other">Wang S, Liu W, Wu J, Cao L, Meng Q, Kennedy PJ. Training deep neural networks on imbalanced data sets. 2016 Int Jt Conf Neural Netw. 2016;:4368–4374. <pub-id pub-id-type="doi">10.1109/IJCNN.2016.7727770</pub-id>.</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39</label>
      <mixed-citation publication-type="other">Min S, Lee B, Yoon S. Deep learning in bioinformatics. Brief Bioinforms. 2017; 18(5):851–69. <pub-id pub-id-type="doi">10.1093/bib/bbw068</pub-id>. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1603.06430">http://arxiv.org/abs/1603.06430</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40</label>
      <mixed-citation publication-type="other">Liang M, Li Z, Chen T, Zeng J. Integrative Data Analysis of Multi-Platform Cancer Data with a Multimodal Deep Learning Approach. IEEE/ACM Trans Comput Biol Bioinforma. 2015; 12(4):928–37. <pub-id pub-id-type="doi">10.1109/TCBB.2014.2377729</pub-id>.</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41</label>
      <mixed-citation publication-type="other">Zeng H, Edwards MD, Liu G, Gifford DK. Convolutional neural network architectures for predicting DNA-protein binding. Bioinformatics. 2016; 32(12):121–7. <pub-id pub-id-type="doi">10.1093/bioinformatics/btw255</pub-id>.</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42</label>
      <mixed-citation publication-type="other">Alipanahi B, Delong A, Weirauch MT, Frey BJ. Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning. Nat Biotechnol. 2015; 33(8):831–8. <pub-id pub-id-type="doi">10.1038/nbt.3300</pub-id>.</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43</label>
      <mixed-citation publication-type="other">Zhou J, Troyanskaya OG. Predicting effects of noncoding variants with deep learning-based sequence model. Nat Methods. 2015; 12(10):931–4. <pub-id pub-id-type="doi">10.1038/nmeth.3547</pub-id>. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/15334406">https://arxiv.org/abs/15334406</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44</label>
      <mixed-citation publication-type="other">Ching T, et al. Opportunities and obstacles for deep learning in biology and medicine. J R Soc Interface. 2018;15(141).<pub-id pub-id-type="doi">10.1098/rsif.2017.0387</pub-id>. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/http://rsif.royalsocietypublishing.org/content/15/141/20170387.full.pdf">http://arxiv.org/abs/http://rsif.royalsocietypublishing.org/content/15/141/20170387.full.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45</label>
      <mixed-citation publication-type="other">Ma J, Yu MK, Fong S, Ono K, Sage E, Demchak B, Sharan R, Ideker T. Using deep learning to model the hierarchical structure and function of a cell. Nat Methods. 2018; 15(4):290–8. 10.1038/nmeth.4627.</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46</label>
      <mixed-citation publication-type="other">Liu B, Wei Y, Zhang Y, Yang Q. Deep Neural Networks for High Dimension, Low Sample Size Data. In: Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17: 2017. p. 2287–93. 10.24963/ijcai.2017/318.</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47</label>
      <mixed-citation publication-type="other">Pasini A. Artificial neural networks for small dataset analysis. J Thorac Dis. 2015; 7(5):953–60. 10.3978/j.issn.2072-1439.2015.04.61.</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48</label>
      <mixed-citation publication-type="other">Wójcik PI, Kurdziel M. Training neural networks on high-dimensional data using random projection. Pattern Anal Applic. 2018. 10.1007/s10044-018-0697-0.</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49</label>
      <mixed-citation publication-type="other">Li Y, Chen C-Y, Wasserman WW. Deep feature selection: Theory and application to identify enhancers and promoters. J Comput Biol. 2016; 23(5):322–36. 10.1089/cmb.2015.0189. PMID: 26799292.</mixed-citation>
    </ref>
    <ref id="CR50">
      <label>50</label>
      <mixed-citation publication-type="other">Zhang J, Feng H, Xu S, Feng P. Hijacking GPCRs by viral pathogens and tumor. 2016. 10.1016/j.bcp.2016.03.021.</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51</label>
      <mixed-citation publication-type="other">Feng L, et al. Heterogeneity of tumor-infiltrating lymphocytes ascribed to local immune status rather than neoantigens by multi-omics analysis of glioblastoma multiforme. Sci Reports. 2017;1(7). 10.1038/s41598-017-05538-z.</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52</label>
      <mixed-citation publication-type="other">Zhou C, et al. Analysis of the gene-protein interaction network in glioma. Genet Mol Res. 2015; 14(4):14196–206. 10.4238/2015.November.13.3.</mixed-citation>
    </ref>
    <ref id="CR53">
      <label>53</label>
      <mixed-citation publication-type="other">Choi HY, et al. G protein-coupled receptors in stem cell maintenance and somatic reprogramming to pluripotent or cancer stem cells. BMB Rep. 2015; 48(2):68–80. 10.5483/BMBRep.2015.48.2.250.</mixed-citation>
    </ref>
    <ref id="CR54">
      <label>54</label>
      <mixed-citation publication-type="other">Chédotal A, Kerjan G, Moreau-Fauvarque C. The brain within the tumor: New roles for axon guidance molecules in cancers. 2005. 10.1038/sj.cdd.4401707.</mixed-citation>
    </ref>
  </ref-list>
</back>
