<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6612823</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btz353</article-id>
    <article-id pub-id-type="publisher-id">btz353</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Ismb/Eccb 2019 Conference Proceedings</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Studies of Phenotypes and Clinical Applications</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Rotation equivariant and invariant neural networks for microscopy image analysis</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Chidester</surname>
          <given-names>Benjamin</given-names>
        </name>
        <xref ref-type="aff" rid="btz353-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhou</surname>
          <given-names>Tianming</given-names>
        </name>
        <xref ref-type="aff" rid="btz353-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Do</surname>
          <given-names>Minh N</given-names>
        </name>
        <xref ref-type="aff" rid="btz353-aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ma</surname>
          <given-names>Jian</given-names>
        </name>
        <xref ref-type="aff" rid="btz353-aff1">1</xref>
        <xref ref-type="corresp" rid="btz353-cor1"/>
        <!--<email>jianma@cs.cmu.edu</email>-->
      </contrib>
    </contrib-group>
    <aff id="btz353-aff1"><label>1</label>Computational Biology Department, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA</aff>
    <aff id="btz353-aff2"><label>2</label>Department of Electrical and Computer Engineering, University of Illinois, Urbana, IL, USA</aff>
    <author-notes>
      <corresp id="btz353-cor1">To whom correspondence should be addressed. <email>jianma@cs.cmu.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2019-07-05">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>35</volume>
    <issue>14</issue>
    <fpage>i530</fpage>
    <lpage>i537</lpage>
    <permissions>
      <copyright-statement>Â© The Author(s) 2019. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btz353.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Neural networks have been widely used to analyze high-throughput microscopy images. However, the performance of neural networks can be significantly improved by encoding known invariance for particular tasks. Highly relevant to the goal of automated cell phenotyping from microscopy image data is rotation invariance. Here we consider the application of two schemes for encoding rotation equivariance and invariance in a convolutional neural network, namely, the group-equivariant CNN (G-CNN), and a new architecture with simple, efficient conic convolution, for classifying microscopy images. We additionally integrate the 2D-discrete-Fourier transform (2D-DFT) as an effective means for encoding global rotational invariance. We call our new method the <italic>Conic Convolution and DFT Network</italic> (CFNet).</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We evaluated the efficacy of CFNet and G-CNN as compared to a standard CNN for several different image classification tasks, including simulated and real microscopy images of subcellular protein localization, and demonstrated improved performance. We believe CFNet has the potential to improve many high-throughput microscopy image analysis applications.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>Source code of CFNet is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/bchidest/CFNet">https://github.com/bchidest/CFNet</ext-link>.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">National Science Foundation grant</named-content>
        </funding-source>
        <award-id>1717205</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Though the appeal of neural networks is their versatility for arbitrary classification tasks, there is still much benefit in designing them for particular problem settings. In particular, the effectiveness of neural networks can be greatly increased by encoding invariance to uniformative augmentations of the data (<xref rid="btz353-B17" ref-type="bibr">LeCun <italic>et al.</italic>, 1989</xref>). A key invariance inherent to many imaging contexts, including microscopy data, is <italic>rotation</italic> (<xref rid="btz353-B2" ref-type="bibr">Boland and Murphy, 2001</xref>). For biological imaging, since data is often scarce and difficult or expensive to acquire, improving the effectiveness and reliability of models by encoding such invariance is highly significant.</p>
    <p>Recently, convolutional neural networks (CNNs) have been applied to the highly relevant problem of cell phenotyping based on microscopy image analysis and have demonstrated much improved performance (<xref rid="btz353-B14" ref-type="bibr">Kraus <italic>et al.</italic>, 2016</xref>, <xref rid="btz353-B15" ref-type="bibr">2017</xref>). Formerly, crafted features that inherently exhibit such invariance, such as Zernike moments and Haralick texture features, were extracted and used for subsequent analysis (<xref rid="btz353-B2" ref-type="bibr">Boland and Murphy, 2001</xref>), whereas CNNs are able to learn relevant features directly. This has significant applications to spatial proteomics, which has enabled the systematic probing of changes of subcellular protein localizations, which are key to protein functions (<xref rid="btz353-B20" ref-type="bibr">Lundberg and Borner, 2019</xref>), as a response to various perturbations (<xref rid="btz353-B4" ref-type="bibr">Chong <italic>et al.</italic>, 2015</xref>; <xref rid="btz353-B15" ref-type="bibr">Kraus <italic>et al.</italic>, 2017</xref>). However, the encoding of rotation equivariance and invariance into CNNs to learn meaningful features for cell phenotyping is yet to be considered.</p>
    <p>Several approaches have been proposed recently for improving the performance of CNNs by encoding rotation equivariance. The most promising and popular of such methods is the group-equivariant CNN (G-CNN) (<xref rid="btz353-B5" ref-type="bibr">Cohen and Welling, 2016</xref>), which applies convolution over groups, such as rotation, translation and flips, thereby maintaining equivariance throughout the convolutional layers. Notably, G-CNNs have recently been applied to several biological imaging tasks, including cell boundary segmentation (<xref rid="btz353-B1" ref-type="bibr">Bekkers <italic>et al.</italic>, 2018</xref>; <xref rid="btz353-B27" ref-type="bibr">Weiler <italic>et al.</italic>, 2018</xref>), annotation of cancerous regions of tumors (<xref rid="btz353-B26" ref-type="bibr">Veeling <italic>et al.</italic>, 2018</xref>) and dermoscopy image segmentation (<xref rid="btz353-B18" ref-type="bibr">Li <italic>et al.</italic>, 2018</xref>).</p>
    <p>Here we consider the integration of rotation equivariance and invariance to analyze the localization of proteins in fluorescence images, which, to the best of our knowledge, is the first such work. Additionally, we propose a new simple and efficient rotation-equivariant convolutional scheme, called <italic>conic convolution</italic> as an effective alternative to group convolution, with advantages of computational and memory savings, interpretability of learned feature maps and improved performance. Rather than convolving each filter across the entire image, as in standard or group convolution, rotated filters are convolved only over corresponding conic regions of the input feature map that emanate from the origin, thereby intuitively transforming rotations in the input directly to rotations in the output. A comparison of conic convolution with other proposed convolution schemes is shown in <xref ref-type="fig" rid="btz353-F1">FigureÂ 1</xref>.
</p>
    <fig id="btz353-F1" orientation="portrait" position="float">
      <label>Fig. 1.</label>
      <caption>
        <p>Comparison of convolution schemes. The domain of filter âFâ in the input and its corresponding outputs in the feature map are colored red. That of the rotation of âFâ by 225 degrees is colored blue. The local support on the domain for the convolution at a few points for each scheme is shown in gray. Conic convolution, with rotations of 45 degrees in this example, encodes rotation equivariance without introducing distortion to the support of the filter in the original domain (unlike the log-polar transform) and without requiring additional storage for feature maps (unlike group convolution). The example shown for group convolution is the first layer of a G-CNN, mapping from <inline-formula id="IE1"><mml:math id="IM1"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> to the roto-translation group</p>
      </caption>
      <graphic xlink:href="btz353f1"/>
    </fig>
    <p>To encode rotation <italic>invariance</italic>, we propose the integration of the magnitude response of the 2D-discrete-Fourier transform (2D-DFT) into a transition layer between convolutional and fully-connected layers. The 2D-DFT is able to integrate mutual orientation information between different filter responses, yielding more informative features for subsequent layers than most previous approaches. Though the insight of using the DFT to encode rotational invariance has been employed for texture classification using wavelets (<xref rid="btz353-B3" ref-type="bibr">Charalampidis and Kasparis, 2002</xref>; <xref rid="btz353-B8" ref-type="bibr">Do and Vetterli, 2002</xref>; <xref rid="btz353-B12" ref-type="bibr">Jafari-Khouzani and Soltanian-Zadeh, 2005</xref>; <xref rid="btz353-B22" ref-type="bibr">Ojala <italic>et al.</italic>, 2002</xref>) and for general image classification (<xref rid="btz353-B25" ref-type="bibr">Schmidt and Roth, 2012</xref>), as of yet, its application to CNNs has been relatively overlooked. As in these prior works, rotations of the input are transformed to circular shifts, to which the magnitude response of the 2D-DFT is invariant, in the transformed space.</p>
    <p>We call our new method the Conic Convolution and DFT Network (CFNet). We demonstrate the effectiveness of the two novel contributions in CFNet, namely conic convolution and integration of the DFT, based on evaluations from both synthetic and real microscopy images for localizing proteins in budding yeast cells. We show that CFNet improves classification accuracy generally over the standard raster convolution formulation and over the equivariant method of G-CNN across these settings. We also show that the 2D-DFT clearly improves performance across these diverse datasets, and that not only for the proposed conic convolution, but also for group convolution.</p>
    <sec>
      <title>1.1 Related work</title>
      <p>To encode rotation equivariance for general image classification, a variety of methods exist. One straightforward strategy is to transform the domain of the image to an alternative domain, such as the log-polar domain (<xref rid="btz353-B10" ref-type="bibr">Henriques and Vedaldi, 2017</xref>; <xref rid="btz353-B25" ref-type="bibr">Schmidt and Roth, 2012</xref>) in which rotation becomes some other transformation that is easier to manage, but this can be unstable to translations and this warping will introduce distortion, as pixels near the center of the image are sampled more densely than pixels near the perimeter. Our proposed conic convolution also encodes global rotation equivariance about the origin, but without introducing such distortion, which greatly helps mitigate its susceptibility to translation. The recently developed spatial transform layer (<xref rid="btz353-B11" ref-type="bibr">Jaderberg <italic>et al.</italic>, 2015</xref>) and deformable convolutional layer (<xref rid="btz353-B6" ref-type="bibr">Dai <italic>et al.</italic>, 2017</xref>) allow the network to learn non-regular sampling patterns and can potentially help learning rotation invariance, though invariance is not explicitly enforced, which would most likely be a challenge for tasks with small training sets.</p>
      <p>An alternative, simple means for achieving rotation equivariance and invariance was proposed in (<xref rid="btz353-B7" ref-type="bibr">Dieleman <italic>et al.</italic>, 2016</xref>), in which feature maps of standard CNNs are made equivariant or invariant to rotation by combinations of cyclic slicing, stacking, rolling and pooling. RotEqNet (<xref rid="btz353-B21" ref-type="bibr">Marcos <italic>et al.</italic>, 2017</xref>) improved upon this idea by storing, for each feature map for a corresponding filter, only the maximal response across rotations and the value of the corresponding rotation, to preserve pose information, yielding improved results and considerable storage savings. Our proposed conic convolution is most similar to these methods and further decreases storage and computation requirements. The recently developed capsule network (<xref rid="btz353-B24" ref-type="bibr">Sabour <italic>et al.</italic>, 2017</xref>) is able to auto-encode affine transformation, including rotation, by the routing-by-agreement process. However, our CFNet developed in this paper works well even without augmentation because equivariance and invariance are encoded. Another related work extended G-CNN using steerable filters (<xref rid="btz353-B27" ref-type="bibr">Weiler <italic>et al.</italic>, 2018</xref>), as proposed in H-Net (<xref rid="btz353-B28" ref-type="bibr">Worrall <italic>et al.</italic>, 2017</xref>), to provide equivariance for finer angles. This can be considered as a parallel contribution to our work, which could also use a steerable filter design. In summary, CFNet improves upon previous methods by reducing computation and storage requirements and improving interpretability and performance.</p>
    </sec>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <p>We consider CFNet and G-CNN within the context of microscopy image analysis to classify cell features. Each network takes, as input, an image of a cell and predicts a label of interest, such as the localization of fluorescence-tagged proteins. The overall architecture of CFNet is illustrated in <xref ref-type="fig" rid="btz353-F2">FigureÂ 2</xref>. We first give a brief description of group-equivariant convolution and then describe our proposed conic convolution in CFNet, which uses similar notation from group theory. Next, we discuss the preservation of rotation equivariance through non-linear operations within a neural network as well as the efficiency of conic convolution. We then describe the integration of the 2D-DFT in CFNet as a transition layer between group or conic convolutional layers and subsequent fully-connected layers in a CNN.
</p>
    <fig id="btz353-F2" orientation="portrait" position="float">
      <label>Fig. 2.</label>
      <caption>
        <p>The overall architecture of CFNet. (<bold>a</bold>) Filtering the image by various filters at rotations in corresponding conic regions preserves rotation-equivariance. (<bold>b</bold>) Subsequent convolutional feature maps are filtered similarly. Rotation-invariance is encoded by the transition from convolutional to fully-connected layers, which consists of (<bold>c</bold>) element-wise multiplication and sum, denoted by <inline-formula id="IE2"><mml:math id="IM2"><mml:mo>â</mml:mo></mml:math></inline-formula>, with rotated weight tensors, transforming rotation to circular shift and (<bold>d</bold>) application of the magnitude response of the 2D-DFT to encode invariance to such shifts. (<bold>e</bold>) This output is reshaped and passed through the final, fully-connected layers</p>
      </caption>
      <graphic xlink:href="btz353f2"/>
    </fig>
    <sec>
      <title>2.1 Group-equivariant convolution</title>
      <p>For convenience, as in <xref rid="btz353-B5" ref-type="bibr">Cohen and Welling (2016)</xref>, we represent feature maps, of dimension <italic>K</italic>, <inline-formula id="IE3"><mml:math id="IM3"><mml:mrow><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>K</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and filters, <inline-formula id="IE4"><mml:math id="IM4"><mml:mrow><mml:mo>Ï</mml:mo><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>K</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, of a standard CNN as functions over the 2D space <inline-formula id="IE5"><mml:math id="IM5"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> of integers, or pixel locations in the case of images. The expression for convolution of a filter over a feature map in a standard CNN is given by:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi>f</mml:mi><mml:mo>â</mml:mo><mml:mo>Ï</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mo>Ï</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>â</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>The success of CNNs can be attributed largely to the fact that standard convolution is equivariant to translations and many image classification tasks are invariant to small local translations. However, standard convolution does not in general exhibit equivariance to other important transformations, such as rotations, unless certain constraints on the filters are met. The insight of <xref rid="btz353-B5" ref-type="bibr">Cohen and Welling (2016)</xref> was to generalize convolution to operate on functions on <italic>groups</italic>, thereby achieving equivariance for other types of transformations. A group is a mathematical term referring to a particular set paired with a binary operation, which together meet certain criteria. The set of indices <inline-formula id="IE6"><mml:math id="IM6"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> with the operation of translation is a particular instance of a group.</p>
      <p>A more relevant group for microscopy image data is the <italic>p</italic>4 group, or roto-translation group, which consists of both translations and rotations about the origin of <inline-formula id="IE7"><mml:math id="IM7"><mml:mrow><mml:mfrac><mml:mo>Ï</mml:mo><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula> of <inline-formula id="IE8"><mml:math id="IM8"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, and a function on this group is indexed not just by translation, such as the <italic>x</italic> position of feature maps of normal CNNs, but also by rotation. In this way, rotation information is preserved throughout the network and equivariance can thereby be maintained.</p>
      <p>We denote this group by <italic>G</italic>, where <inline-formula id="IE9"><mml:math id="IM9"><mml:mrow><mml:mi>g</mml:mi><mml:mo>â</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula> is the transformation of rotation about the origin and a translation. The first group convolutional layer of a G-CNN operates on functions on <inline-formula id="IE10"><mml:math id="IM10"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, over which the input image is defined, and is given by:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:mi>f</mml:mi><mml:mo>â</mml:mo><mml:mo>Ï</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mo>Ï</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>Whereas in standard convolution, the filter is translated over the image and the inner product is computed at each translation, in group convolution, the filter is transformed by each element <inline-formula id="IE11"><mml:math id="IM11"><mml:mrow><mml:mi>g</mml:mi><mml:mo>â</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula>. The output of group convolution is then a function of the group <italic>G</italic>. Subsequent layers of the network must therefore operate on such functions, and group convolution for these layers is defined as:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:mi>f</mml:mi><mml:mo>â</mml:mo><mml:mo>Ï</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>â</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mo>Ï</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>As shown in <xref rid="btz353-B5" ref-type="bibr">Cohen and Welling (2016)</xref>, standard operations used in neural networks, including pooling, batch normalization and activations, can be defined on the feature maps of group convolution to preserve the equivariance property, and a full G-CNN can be defined by the composition of such operations. We refer the reader to <xref rid="btz353-B5" ref-type="bibr">Cohen and Welling (2016)</xref> for more details.</p>
    </sec>
    <sec>
      <title>2.2 Rotation-equivariant quadrant convolutional layers</title>
      <p>Rather than operating on functions on groups, conic convolution is simpler in that it maintains rotation equivariance while operating still on functions on the spatial domain <inline-formula id="IE12"><mml:math id="IM12"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, as in standard convolution. We begin the formulation with a simpler, special case of conic convolution, which we call <italic>quadrant convolution</italic>. Its difference from standard convolution is that the filter being convolved is rotated by <inline-formula id="IE13"><mml:math id="IM13"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mo>Ï</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mi>r</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, depending upon the corresponding quadrant of the domain. We show that for quadrant convolution, rotations of <inline-formula id="IE14"><mml:math id="IM14"><mml:mrow><mml:mfrac><mml:mo>Ï</mml:mo><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula> of the input are straightforwardly associated with rotations of the output feature map, which is a special form of equivariance called <italic>same-equivariance</italic> [as coined by <xref rid="btz353-B7" ref-type="bibr">Dieleman <italic>et al.</italic> (2016</xref>)].</p>
      <p>Relevant to our formulation is the group of two-dimensional rotation matrices of <inline-formula id="IE15"><mml:math id="IM15"><mml:mrow><mml:mfrac><mml:mo>Ï</mml:mo><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>, which we denote by <italic>G</italic><sub>1</sub> and which can be easily parameterized by <italic>g</italic>(<italic>r</italic>), and which acts on points in <inline-formula id="IE16"><mml:math id="IM16"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> by matrix multiplication, i.e. for a given point <inline-formula id="IE17"><mml:math id="IM17"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>:
<disp-formula id="E4"><label>(4)</label><mml:math id="M4"><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>â</mml:mo><mml:mtext>cos</mml:mtext><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mo>Ï</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>â</mml:mo><mml:mtext>sin</mml:mtext><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mo>Ï</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>â</mml:mo><mml:mtext>sin</mml:mtext><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mo>Ï</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>â</mml:mo><mml:mtext>cos</mml:mtext><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mo>Ï</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>u</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>v</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>Let <italic>T<sub>g</sub></italic> denote the transformation of a function by a rotation in <italic>G</italic><sub>1</sub>, where <inline-formula id="IE18"><mml:math id="IM18"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msub><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> applies the inverse of <italic>g</italic> to an element of the domain of <italic>f</italic>. For an operation <inline-formula id="IE19"><mml:math id="IM19"><mml:mrow><mml:mo>Î¦</mml:mo><mml:mo>:</mml:mo><mml:mi mathvariant="script">F</mml:mi><mml:mo>â</mml:mo><mml:mi mathvariant="script">F</mml:mi><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mi mathvariant="script">F</mml:mi></mml:mrow></mml:math></inline-formula> being the set of <italic>K</italic>-dimensional functions <italic>f</italic> on <inline-formula id="IE20"><mml:math id="IM20"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> (which represent feature maps), to exhibit same-equivariance, applying rotation either before or after the operation yields the same result, i.e.
<disp-formula id="E5"><label>(5)</label><mml:math id="M5"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msub><mml:mo>Î¦</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>Î¦</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msub><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>Quadrant convolution can be interpreted as weighting the convolution for each rotation with a function <inline-formula id="IE21"><mml:math id="IM21"><mml:mrow><mml:mo>Ï</mml:mo><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>â</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> that simply âselectsâ the appropriate quadrant of the domain. The weighting function for the first quadrant is defined as:
<disp-formula id="E6"><label>(6)</label><mml:math id="M6"><mml:mrow><mml:mo>Ï</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mo>Â </mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>u</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>â</mml:mo><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>â</mml:mo><mml:mtext>else</mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p>Since the origin does not strictly belong to a particular quadrant, it is handled by averaging the response of the filter at all four rotations. Boundary values are averaged over the responses of the neighboring regions. The appropriate weighting function for other quadrants is just a rotation of <italic>Ï</italic> (i.e. <inline-formula id="IE22"><mml:math id="IM22"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msub><mml:mo>Ï</mml:mo></mml:mrow></mml:math></inline-formula>) by the appropriate angle. The output of the layer is then given by:
<disp-formula id="E7"><label>(7)</label><mml:math id="M7"><mml:mrow><mml:mo>Î¦</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msub><mml:mo>Ï</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msub><mml:mo>Ï</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>In our notation, parenthesis convey the parameter of a function, whereas square brackets merely clarify the order of operations. Example convolutional regions with appropriate filter rotations are shown in <xref ref-type="fig" rid="btz353-F1">FigureÂ 1</xref>.</p>
      <p>Note that the equivariance property is established (see our detailed proof in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>) independent of the definition of <italic>Ï</italic>, yet its definition will greatly influence the performance of the network. For example, if <italic>Ï</italic> is simply the constant 1/4, it is equivalent to merely averaging the filter responses.</p>
    </sec>
    <sec>
      <title>2.3 Generalization to conic convolutional layers</title>
      <p>The above formulation can be generalized to <italic>conic convolution</italic> in which the rotation angle is decreased by an arbitrary factor of <inline-formula id="IE23"><mml:math id="IM23"><mml:mrow><mml:mfrac><mml:mo>Ï</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, for some positive integer <italic>R</italic>, instead of being fixed to <inline-formula id="IE24"><mml:math id="IM24"><mml:mrow><mml:mfrac><mml:mo>Ï</mml:mo><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>. Rather than considering quadrants of the domain, we can consider conic regions emanating from the origin and their boundaries, defined by:
<disp-formula id="E8"><label>(8)</label><mml:math id="M8"><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:msubsup><mml:mo>:</mml:mo><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>arccot</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>/</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mfrac><mml:mo>Ï</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E9"><label>(9)</label><mml:math id="M9"><mml:mrow><mml:mi mathvariant="script">B</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:msubsup><mml:mo>:</mml:mo><mml:mi>arccot</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>/</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mo>Ï</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>The weighting function is changed to have value one only over this conic region:
<disp-formula id="E10"><label>(10)</label><mml:math id="M10"><mml:mrow><mml:msub><mml:mrow><mml:mo>Ï</mml:mo></mml:mrow><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mo>Â </mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi mathvariant="script">C</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi mathvariant="script">B</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>4</mml:mn><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>â</mml:mo><mml:mtext>else</mml:mtext><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
of which <inline-formula id="IE25"><mml:math id="IM25"><mml:mrow><mml:msub><mml:mrow><mml:mo>Ï</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mo>Ï</mml:mo></mml:mrow></mml:math></inline-formula> is a special case.</p>
      <p>If we consider feature maps to be functions over the continuous domain <inline-formula id="IE26"><mml:math id="IM26"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> instead of <inline-formula id="IE27"><mml:math id="IM27"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and define the group <italic>G<sub>R</sub></italic>, with parameterization:
<disp-formula id="E11"><label>(11)</label><mml:math id="M11"><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>â</mml:mo><mml:mtext>cos</mml:mtext><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mo>Ï</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>â</mml:mo><mml:mtext>sin</mml:mtext><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mo>Ï</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>â</mml:mo><mml:mtext>sin</mml:mtext><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mo>Ï</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>â</mml:mo><mml:mtext>cos</mml:mtext><mml:mo>â</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mo>Ï</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>u</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>v</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
for <inline-formula id="IE28"><mml:math id="IM28"><mml:mrow><mml:mi>r</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>â¦</mml:mo><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mi>R</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE29"><mml:math id="IM29"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, it is easy to show similarly as above that
<disp-formula id="E12"><label>(12)</label><mml:math id="M12"><mml:mrow><mml:msub><mml:mrow><mml:mo>Î¦</mml:mo></mml:mrow><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mo>â</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mo>Ï</mml:mo></mml:mrow><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msub><mml:mo>Ï</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
is equivariant to <italic>G<sub>R</sub></italic>.</p>
      <p>However, due to subsampling artifacts when discretizing <inline-formula id="IE30"><mml:math id="IM30"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> to <inline-formula id="IE31"><mml:math id="IM31"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, as in an image, rotation equivariance for arbitrary values of <italic>R</italic> cannot be guaranteed and can only be approximated. In particular, the filters will have to be interpolated for rotations that are not a multiple of <inline-formula id="IE32"><mml:math id="IM32"><mml:mrow><mml:mfrac><mml:mo>Ï</mml:mo><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>. In our experiments when applying CFNet, we chose nearest neighbor interpolation, which preserves the energy of the filter under rotations. This defect notwithstanding, it can be shown that conic convolution maintains equivariance to rotations of <inline-formula id="IE33"><mml:math id="IM33"><mml:mrow><mml:mfrac><mml:mo>Ï</mml:mo><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>, and as we found in our experiments, the approximation of finer angles of rotation can still improve performance. Additionally, we note that <italic>R</italic> need not be the same for each layer, and it may be advantageous to use a finer discretization of rotations for early layers, when the feature maps are larger, and gradually decrease <italic>R</italic>.</p>
      <p>A note must be made about subsequent nonlinear operations for a convolutional layer. It is typical in convolutional networks to perform subsampling, either by striding the convolution or by spatial pooling, to reduce the dimensionality of subsequent layers. Again, due to downsampling artifacts, rotational equivariance to rotations smaller than <inline-formula id="IE34"><mml:math id="IM34"><mml:mrow><mml:mfrac><mml:mo>Ï</mml:mo><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula> is not guaranteed. However, given that the indices of the plane of the feature map are in <inline-formula id="IE35"><mml:math id="IM35"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and are therefore centered about the origin, a downsampling of <inline-formula id="IE36"><mml:math id="IM36"><mml:mrow><mml:mi>D</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> can be applied while maintaining rotational equivariance for rotations of <inline-formula id="IE37"><mml:math id="IM37"><mml:mrow><mml:mfrac><mml:mo>Ï</mml:mo><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>, regardless of the choice of <italic>R</italic>. After subsampling, the result is passed through a non-linear activation function <inline-formula id="IE38"><mml:math id="IM38"><mml:mrow><mml:mo>Ï</mml:mo><mml:mo>:</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi><mml:mo>â</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:math></inline-formula>, such as ReLU, with an added offset <inline-formula id="IE39"><mml:math id="IM39"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:math></inline-formula>.</p>
    </sec>
    <sec>
      <title>2.4 Computational efficiency of conic convolution</title>
      <p>In CFNet, the response for each rotation in conic convolution is only needed over its corresponding conic region. However, since GPUs are more efficient operating on rectangular inputs, it is faster to compute the convolution over each quadrant in which the conic region resides. The output of conic convolution can be achieved by convolving over the corresponding quadrant, multiplying by the weighting function, summing the responses in each quadrant together, and then concatenating the responses of quadrants. For the special case of quadrant convolution, this process incurs negligible additional computation beyond standard convolution. Additionally, conic convolution produces only one feature map per filter as in standard convolution and therefore incurs no additional storage costs, in contrast to G-CNN and cyclic slicing, which both produce one map per rotation (<xref rid="btz353-B5" ref-type="bibr">Cohen and Welling, 2016</xref>; <xref rid="btz353-B7" ref-type="bibr">Dieleman <italic>et al.</italic>, 2016</xref>), and two for RotEqNet, one for the filter response and one for the orientation (<xref rid="btz353-B21" ref-type="bibr">Marcos <italic>et al.</italic>, 2017</xref>).</p>
    </sec>
    <sec>
      <title>2.5 Rotation-invariant transition using the magnitude of the 2D-DFT</title>
      <p>After the final convolutional layer of a CNN, some number of fully-connected layers will be applied to combine information from the various filter responses. In general, fully-connected layers will not maintain rotation equivariance or invariance properties. Commonly, convolution and downsampling are applied until the spatial dimensions are eliminated and the resulting feature map of the final convolutional layer is merely a vector, with dimension equal to the number of filters.</p>
      <p>Rather than encoding invariance for each filter separately, as in most other recent works (<xref rid="btz353-B5" ref-type="bibr">Cohen and Welling, 2016</xref>; <xref rid="btz353-B27" ref-type="bibr">Weiler <italic>et al.</italic>, 2018</xref>), in CFNet we consider instead to transform the collective filter responses to a space in which rotation becomes circular shift so that the 2D-DFT can be applied to encode invariance. The primary advantage of the 2D-DFT as an invariant transform is that each output node is a function of every input node, and not just the nodes of a particular filter response, thereby capturing mutual information across responses.</p>
      <p>Since the formulation of this transition involves the DFT, which is defined only for finite-length signals, we switch to represent feature maps as tensors, rather than functions. We denote the feature map generated by the penultimate convolutional layer by <inline-formula id="IE40"><mml:math id="IM40"><mml:mrow><mml:mi>f</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>Ã</mml:mo><mml:mi>M</mml:mi><mml:mo>Ã</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE41"><mml:math id="IM41"><mml:mrow><mml:mi>M</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
      <p>At the transition to fully-connected layers, the input <italic>f</italic> is passed through <italic>N</italic> fully-connected filters, <inline-formula id="IE42"><mml:math id="IM42"><mml:mrow><mml:msup><mml:mrow><mml:mo>Ï</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>Ã</mml:mo><mml:mi>M</mml:mi><mml:mo>Ã</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mi>n</mml:mi><mml:mo>â</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>â¦</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The operation of this layer can be interpreted as the inner product of the function and filter, <inline-formula id="IE43"><mml:math id="IM43"><mml:mrow><mml:mo>â©</mml:mo><mml:msup><mml:mrow><mml:mo>Ï</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>âª</mml:mo></mml:mrow></mml:math></inline-formula>. If we again consider rotations of the filter from the group <italic>G<sub>R</sub></italic>,
<disp-formula id="E13"><label>(13)</label><mml:math id="M13"><mml:mrow><mml:mo>Î¨</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mo>â©</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>Ï</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>âª</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
this is equivalent to the first layer of a G-CNN, mapping from the spatial domain to <italic>G<sub>R</sub></italic> (though this group does not include the translation group since the convolution is only applied at the origin), and rotations of the final convolutional layer <italic>f</italic> will correspond to permutations of <italic>G<sub>R</sub></italic>, which are just circular shifts in of the second dimension of the matrix <inline-formula id="IE44"><mml:math id="IM44"><mml:mo>Î¨</mml:mo></mml:math></inline-formula>.</p>
      <p>The magnitude response of the 2D-DFT is applied to <inline-formula id="IE45"><mml:math id="IM45"><mml:mo>Î¨</mml:mo></mml:math></inline-formula> to transform these circular shifts to an invariant space:
<disp-formula id="E14"><label>(14)</label><mml:math id="M14"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mi mathvariant="script">F</mml:mi><mml:mi mathvariant="script">T</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mo>Î¨</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>â²</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>â²</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>4</mml:mi><mml:mi>R</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mo>Î¨</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>â²</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>â²</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mi>j</mml:mi><mml:mn>2</mml:mn><mml:mo>Ï</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>â²</mml:mo></mml:msup><mml:mi>n</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>â²</mml:mo></mml:msup><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>This process of encoding rotation invariance corresponds to the âConvolutional-to-Full Transitionâ in <xref ref-type="fig" rid="btz353-F2">FigureÂ 2</xref>. The result is then vectorized and passed into fully-connected layers that precede the final output layer, as in a standard CNN.</p>
      <p>In addition, the 2D-DFT, as a rotation invariant transform, can also be integrated into other rotation-equivariant networks, such as G-CNN. At the final layer of a fully-convolutional G-CNN, since the spatial dimension has been eliminated through successive convolutions and spatial downsampling, rotation is encoded along contiguous stacks of feature maps <inline-formula id="IE46"><mml:math id="IM46"><mml:mrow><mml:mi>f</mml:mi><mml:mo>â</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>Ã</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> of each filter at four rotations. In this way, rotations similarly correspond to circular shifts in the final dimension. This representation <inline-formula id="IE47"><mml:math id="IM47"><mml:mo>Î¨</mml:mo></mml:math></inline-formula> is then passed through the 2D-DFT, as in <xref ref-type="disp-formula" rid="E14">Eqn. 14</xref>.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Application to rotated MNIST</title>
      <p>We first used the rotated MNIST dataset (<xref rid="btz353-B16" ref-type="bibr">Larochelle <italic>et al.</italic>, 2007</xref>), which has been utilized as a benchmark for previous works on rotation invariance, to place CFNet against results previously reported for G-CNN. The model was trained on 10 000 images, using training augmentation of rotations of arbitrary angles as in (<xref rid="btz353-B5" ref-type="bibr">Cohen and Welling, 2016</xref>) (Though the paper (<xref rid="btz353-B5" ref-type="bibr">Cohen and Welling, 2016</xref>) did not state the use of training augmentation, code posted by the authors at <ext-link ext-link-type="uri" xlink:href="https://github.com/tscohen/gconv_experiments">https://github.com/tscohen/gconv_experiments</ext-link> indicates that rotations of arbitrary angles were used.), and the best model parameters were selected based on scores on a validation set of 5000 images. Our best CFNet architecture consisted of six conic convolution layers, with <italic>Râ</italic>=<italic>â</italic>2 for the first three and <italic>Râ</italic>=<italic>â</italic>1 for the next three, followed by the DFT transition and an output softmax layer of 10 nodes. Filters were three pixels in size, with 15 filters per layer, and spatial max-pooling was applied after the second layer. This architecture was similar in terms of number of layers and filters per layer as that of the G-CNN of (<xref rid="btz353-B5" ref-type="bibr">Cohen and Welling, 2016</xref>). As shown in <xref rid="btz353-T1" ref-type="table">TableÂ 1</xref>, on a held-out set of 50 000 test images, CFNet achieved a 25% reduction in test error over G-CNN. To evaluate the G-CNN with the DFT, the only changes we made from the reported architecture for G-CNN was to reduce the number of filters for each layer to 7, to offset the addition of the 2D-DFT, which was applied to the output of the final convolutional layer. Incorporating the DFT transition into G-CNN further reduces the test error by 13%. These results demonstrate in a standard setting the value of incorporating mutual rotational information between filters, through the DFT, when encoding invariance and the added value of conic convolution.</p>
      <table-wrap id="btz353-T1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>Test error on the rotated MNIST dataset</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Algorithm</th>
              <th rowspan="1" colspan="1">Test error (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1"><xref rid="btz353-B5" ref-type="bibr">Cohen and WellingÂ (2016)</xref> (CNN)</td>
              <td rowspan="1" colspan="1">5.03</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <xref rid="btz353-B25" ref-type="bibr">Schmidt and RothÂ (2012)</xref>
              </td>
              <td rowspan="1" colspan="1">3.98</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"><xref rid="btz353-B5" ref-type="bibr">Cohen and WellingÂ (2016)</xref> (G-CNN)</td>
              <td rowspan="1" colspan="1">2.28</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">G-CNN + DFT</td>
              <td rowspan="1" colspan="1">2.00</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CFNet</td>
              <td rowspan="1" colspan="1">1.75</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>3.2 Application to synthetic biomarker images</title>
      <p>To precisely evaluate the advantage of encoding rotation equivariance, we created a set of synthetic microscopy images in which we could explicitly control the manifestation of rotations and intra- and inter-class variation. We utilized Gaussian-mixture models (GMMs), which have been used previously to emulate real-world fluorescence microscopy images of biological signals (<xref rid="btz353-B29" ref-type="bibr">Zhao and Murphy, 2007</xref>). Examples of synthetic images from across and within classes are shown in <xref ref-type="fig" rid="btz353-F3">FigureÂ 3a and b</xref>. Specifically, we defined 50 distribution patterns and generated 50 and 100 examples per class for training and 200 examples per class for testing. Each image consists of points sampled from several Gaussians, which have mean and variance defined by their particular class. Some intensity fluctuation, exponential noise and jitter are incorporated into the generating model to add variation. The image size was 50 pixels. A batch size of 50 examples, a learning rate of <inline-formula id="IE48"><mml:math id="IM48"><mml:mrow><mml:mn>5</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and a weight decay <inline-formula id="IE49"><mml:math id="IM49"><mml:mrow><mml:msub><mml:mrow><mml:mi>â</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> penalty of <inline-formula id="IE50"><mml:math id="IM50"><mml:mrow><mml:mn>5</mml:mn><mml:mo>Ã</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>â</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> were used during training. We used the Adam optimizer and decreased the learning rate by 0.95 every few epochs. To help all methods, we augmented the training data by rotations and random jitter of up to three pixels, as was done during image generation. A more detailed description of the approach for generating the synthetic images is provided in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>.
</p>
      <fig id="btz353-F3" orientation="portrait" position="float">
        <label>Fig. 3.</label>
        <caption>
          <p>Comparison of the results of CFNet, CNet (network with conic convolution but without the DFT), G-CNN, G-CNN+DFT and a standard CNN on the synthetic biomarker images. (<bold>a, b</bold>) Example images, shown as heat maps for detail, showing inter- and intra-class variation. The results with varying numbers <italic>N</italic> of training examples per class are in (<bold>c, d</bold>)</p>
        </caption>
        <graphic xlink:href="btz353f3"/>
      </fig>
      <p>Classification accuracies on the test dataset over training steps for various numbers of training samples, denoted by <italic>N</italic>, for several methods are shown in <xref ref-type="fig" rid="btz353-F3">FigureÂ 3c and d</xref>. A variety of configurations were trained for each network, and each configuration was trained three times. The darkest line shows the accuracy of the configuration that achieved the highest moving average, with a window size of 100 steps, for each method. The spread of each method, which is the area between the point-wise maximum and minimum of the error, is shaded with a light color, and three standard-deviations around the mean is shaded darker.</p>
      <p>We observed a consistent trend of CFNet outperforming G-CNN, which in turn outperforms the CNN, both in overall accuracy and in terms of the number of steps required to attain that accuracy <xref ref-type="fig" rid="btz353-F3">FigureÂ 3c and d</xref>. Additionally, the spread of CFNet is mostly above even the best performing models of G-CNN and the CNN, demonstrating that an instance of CFNet will outperform other methods even if the best set of hyperparameters has not been chosen. We also included a network consisting of conic convolutional layers, but without the DFT, noted as âCNetâ (<xref ref-type="fig" rid="btz353-F3">Fig.Â 3</xref>), to show the relative advantage of the DFT. CNet performs comparably to the standard CNN while requiring significantly less parameters to attain the same performance, though the true advantage of conic convolution is shown when integrated with the DFT to achieve global rotation invariance. In comparison, including the 2D-DFT increases the performance of G-CNN, to a comparable level with CFNet, though it does not train as quickly.</p>
    </sec>
    <sec>
      <title>3.3 Application to subcellular protein localization images in budding yeast cells</title>
      <p>To further demonstrate the advantage of rotation equivariant architectures and CFNet, we evaluated the models on real microscopy images of budding yeast cells generated from <xref rid="btz353-B15" ref-type="bibr">Kraus <italic>et al.</italic> (2017)</xref>, which were collected as follow-up analysis of the data from <xref rid="btz353-B4" ref-type="bibr">Chong <italic>et al.</italic> (2015)</xref> and are more challenging, since they include more subclasses. In this dataset, cells were first modified by homologous recombination and SGA protocol to express fluorescent markers and GFP fusion query proteins. The cells were then transferred into 384-well plates and ten images (1338âÃâ1003 pixels) were taken per plate per channel. As shown in <xref ref-type="fig" rid="btz353-F4">FigureÂ 4</xref>, each image consists of a single or few cells and three stains, where blue shows the cytoplasmic region, pink the nuclear region and green the protein of interest. The classification for each image is the subcellular compartment in which the protein is localized and expressed, such as the cell periphery, mitochondria, or eisosomes, some of which exhibit very subtle differences. Our goal therefore is to predict the protein localization for a given image.
</p>
      <fig id="btz353-F4" orientation="portrait" position="float">
        <label>Fig. 4.</label>
        <caption>
          <p>Evaluation results based on subcellular protein localization images from <xref rid="btz353-B4" ref-type="bibr">Chong <italic>et al.</italic> (2015)</xref>. (<bold>a</bold>) Example images. (<bold>bâc</bold>) Comparison results of CFNet, G-CNN and a standard CNN with varying numbers <italic>N</italic> of training examples per class. (<bold>d</bold>) Confusion matrix for the results for CFNet (X-axis) as compared to the true labels (Y-axis)</p>
        </caption>
        <graphic xlink:href="btz353f4"/>
      </fig>
      <p>We compared the performance of CFNet with G-CNN and a standard CNN. <xref ref-type="fig" rid="btz353-F4">FigureÂ 4b and c</xref> shows the results of each method for classifying the protein localization for each image. To compare with DeepLoc (<xref rid="btz353-B15" ref-type="bibr">Kraus <italic>et al.</italic>, 2017</xref>), we used the same reported architecture and hyperparameters for the CNN. For CFNet and G-CNN, we removed the last convolutional layer and reduced the number of filters per layer by roughly half to offset for encoding of equivariance and invariance. The same training parameters and data augmentation were used as for the synthetic data, except that a dropout probability of 0.8 was applied at the final layer and the maximum jitter was increased to five pixels, since many examples were not well-centered. For each method, several iterations were run, and the spread and the best performing model is shown. We found that CFNet consistently outperforms G-CNN and the standard CNN representing DeepLoc, when the number of training examples per class is either 50 or 100 (see <xref ref-type="fig" rid="btz353-F4">Fig.Â 4b and c</xref>), demonstrating that the gains of the 2D-DFT and conic convolution translate to real-world microscopy data. We note that the best reported algorithm that did not use deep learning, called ensLOC (<xref rid="btz353-B4" ref-type="bibr">Chong <italic>et al.</italic>, 2015</xref>; <xref rid="btz353-B13" ref-type="bibr">Koh <italic>et al.</italic>, 2015</xref>), was only able to achieve an average precision of 0.49 for a less challenging set of yeast phenotypes and with â¼20 000 samples, whereas all runs of CFNet achieved an average precision of between 0.60 and 0.67 with â¼10% of the data used for training.</p>
      <p>We further analyzed the variation of performance for different protein localization labels (<xref ref-type="fig" rid="btz353-F4">Fig.Â 4d</xref>). CFNet outperforms CNN on almost all classes. For instance, CFNet improves the accuracies on ânuclear peripheryâ, ânucleolusâ, ânucleusâ and âpunctate nuclearâ by 10, 14, 7 and 14%, respectively. Nucleolus and punctate nuclear are both structures inside the nucleus and their only difference is that punctate nuclear is generally smaller and rounder, which is rather subtle and CNN misassigns 13% of proteins that are in punctate nuclear with label ânucleolusâ. In contrast, CFNet decreases this misassignment to less than 5%. However, we also observed a few classes in particular for which CFNet could be further improved. For example, we found that CFNet tends to confuse the class âbudâ and âbudding peripheryâ, likely because many proteins are present in both locations. Nevertheless, the application of CFNet to the subcellular protein localization data demonstrates the effectiveness of the method.</p>
      <p>One of the most significant advantages of CFNet, especially for biological knowledge discovery, is its interpretability. <xref ref-type="fig" rid="btz353-F5">FigureÂ 5</xref> shows the activations of two particular filters from both CFNet and CNN at their third layer for example images of ânucleusâ and ânuclear peripheryâ localizations, two classes that are challenging to differentiate. Since rotations of the input correspond directly to rotations of the output of conic convolution, as seen, the activations of the learned features do not change, except for rotating, thereby eliminating rotation as a confounding source of variation. It is important to note that even for rotations of 45 degrees, which conic convolution with <italic>Râ</italic>=<italic>â</italic>2 approximates, the activations are noticeably similar. Conversely, the activations for the standard CNN significantly change based upon the orientation of the image. This is especially apparent for the activation of filter 1 for the nucleus sample, which has a high response at the nucleus that splits in half under 90 degree rotation. We also observe that the activation of the CNNâs filter 2 for the nuclear periphery sample only outlines the upper right boundary of the nucleus, since it is applied only at a specific orientation, whereas filter 1 of CFNet outlines the entire nucleus. The property of equivariance of conic convolution drastically enhances the ability to distinguish biological meaning of the learned representation from uninformative rotation.
</p>
      <fig id="btz353-F5" orientation="portrait" position="float">
        <label>Fig. 5.</label>
        <caption>
          <p>Visualization of learned features of CFNet and CNN. Example images with protein localized (<bold>a</bold>) in the nucleus and (<bold>b</bold>) at the nuclear periphery. (<bold>c</bold>) Activations of two particular filters from the third layer in CFNet (top row) and CNN (bottom row) for each input rotated by 0, 45 and 90 degrees</p>
        </caption>
        <graphic xlink:href="btz353f5"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <p>In this work, we explored the application of rotation equivariant and invariant neural networks to analyze cellular images. We have demonstrated the effectiveness of enforcing rotation equivariance and invariance in CNNs by means of the proposed conic convolutional layer and the 2D-DFT, even for group convolution. In addition, by applying our methods to a dataset of subcellular protein localizations, we showed that rotation equivariant models outperform the standard CNN and, in particular, CFNet with both conic convolutional layer and the 2D-DFT performs the best in our evaluations.</p>
    <p>There are a few directions that we can further improve our models. For example, CFNet could be potentially further improved by incorporating steerable filters (<xref rid="btz353-B9" ref-type="bibr">Freeman and Adelson, 1991</xref>; <xref rid="btz353-B19" ref-type="bibr">Liu <italic>et al.</italic>, 2014</xref>) for convolution, as was done in (<xref rid="btz353-B27" ref-type="bibr">Weiler <italic>et al.</italic>, 2018</xref>), to enhance group-equivariant convolution and in <xref rid="btz353-B28" ref-type="bibr">Worrall <italic>et al.</italic> (2017)</xref>, which allow for finer sampling of rotations of filters without inducing artifacts. Further evaluations would be needed to thoroughly assess these new approaches. Additionally, in the future, we intend to apply CFNet to full micrograph screens in a multiple-instance learning setting, as was done for CNNs in (<xref rid="btz353-B14" ref-type="bibr">Kraus <italic>et al.</italic>, 2016</xref>), since this is the setting with potentially more microscopy data and applications.</p>
    <p>We believe that the proposed enhancements to the standard CNN will have much utility for future applications in many problem settings, in particular, high-throughput molecular and cellular imaging data, where training data is usually sparse, especially for rare cellular events. One of the most exciting frontiers in current biomedical research is to understand different cellular identities at single cell resolution, their functions and their compositions in different contexts, including various human tissues. With the datasets from large-scale projects such as the ongoing Human Cell Atlas (<xref rid="btz353-B23" ref-type="bibr">Rozenblatt-Rosen <italic>et al.</italic>, 2017</xref>) and the Human BioMolecular Atlas Program (HuBMAP) becoming available, our methods have the potential to complement existing approaches to more effectively analyze high-throughput cellular images.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported in part by the National Science Foundation grant 1717205 (J.M.).</p>
    <p><italic>Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>btz353_Supplementary_Data</label>
      <media xlink:href="btz353_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btz353-B1">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Bekkers</surname><given-names>E.J.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) Roto-translation covariant convolutional networks for medical image analysis. In: <source>MICCAI, Granada, Spain</source>, pp. <fpage>440</fpage>â<lpage>448</lpage>. </mixed-citation>
    </ref>
    <ref id="btz353-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Boland</surname><given-names>M.V.</given-names></name>, <name name-style="western"><surname>Murphy</surname><given-names>R.F.</given-names></name></person-group> (<year>2001</year>) 
<article-title>A neural network classifier capable of recognizing the patterns of all major subcellular structures in fluorescence microscope images of HeLa cells</article-title>. <source>Bioinformatics</source>, <volume>17</volume>, <fpage>1213</fpage>â<lpage>1223</lpage>.<pub-id pub-id-type="pmid">11751230</pub-id></mixed-citation>
    </ref>
    <ref id="btz353-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Charalampidis</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Kasparis</surname><given-names>T.</given-names></name></person-group> (<year>2002</year>) 
<article-title>Wavelet-based rotational invariant roughness features for texture classification and segmentation</article-title>. <source>IEEE Trans. Image Process</source>., <volume>11</volume>, <fpage>825</fpage>â<lpage>837</lpage>.<pub-id pub-id-type="pmid">18244677</pub-id></mixed-citation>
    </ref>
    <ref id="btz353-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chong</surname><given-names>Y.T.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Yeast proteome dynamics from single cell imaging and automated analysis</article-title>. <source>Cell</source>, <volume>161</volume>, <fpage>1413</fpage>â<lpage>1424</lpage>.<pub-id pub-id-type="pmid">26046442</pub-id></mixed-citation>
    </ref>
    <ref id="btz353-B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Cohen</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Welling</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>) Group equivariant convolutional networks. In: <source>ICML, New York, NY, USA</source>, pp. <fpage>2990</fpage>â<lpage>2999</lpage>.</mixed-citation>
    </ref>
    <ref id="btz353-B6">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Dai</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) Deformable convolutional networks. In: <source>ICCV, Venice, Italy</source>, pp. <fpage>764</fpage>â<lpage>773</lpage>.</mixed-citation>
    </ref>
    <ref id="btz353-B7">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Dieleman</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) Exploiting cyclic symmetry in convolutional neural networks. In: <source>ICML, New York, NY, USA</source>, pp. <fpage>1889</fpage>â<lpage>1898</lpage>. <comment>JMLR.org</comment>.</mixed-citation>
    </ref>
    <ref id="btz353-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Do</surname><given-names>M.N.</given-names></name>, <name name-style="western"><surname>Vetterli</surname><given-names>M.</given-names></name></person-group> (<year>2002</year>) 
<article-title>Rotation invariant texture characterization and retrieval using steerable wavelet-domain hidden Markov models</article-title>. <source>IEEE Trans. Multimedia</source>, <volume>4</volume>, <fpage>517</fpage>â<lpage>527</lpage>.</mixed-citation>
    </ref>
    <ref id="btz353-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Freeman</surname><given-names>W.T.</given-names></name>, <name name-style="western"><surname>Adelson</surname><given-names>E.H.</given-names></name></person-group> (<year>1991</year>) 
<article-title>The design and use of steerable filters</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>., <volume>13</volume>, <fpage>891</fpage>â<lpage>906</lpage>.</mixed-citation>
    </ref>
    <ref id="btz353-B10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Henriques</surname><given-names>J.F.</given-names></name>, <name name-style="western"><surname>Vedaldi</surname><given-names>A.</given-names></name></person-group> (<year>2017</year>) Warped convolutions: efficient invariance to spatial transformations. In: <italic>ICML, Sydney, Australia</italic>, pp. <fpage>1461</fpage>â<lpage>1469</lpage>.</mixed-citation>
    </ref>
    <ref id="btz353-B11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Jaderberg</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) Spatial transformer networks. In: <source>NIPS, Montreal, Canada</source>, pp. <fpage>2017</fpage>â<lpage>2025</lpage>.</mixed-citation>
    </ref>
    <ref id="btz353-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jafari-Khouzani</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Soltanian-Zadeh</surname><given-names>H.</given-names></name></person-group> (<year>2005</year>) 
<article-title>Rotation-invariant multiresolution texture analysis using radon and wavelet transforms</article-title>. <source>IEEE Trans. Image Process</source>., <volume>14</volume>, <fpage>783</fpage>â<lpage>795</lpage>.<pub-id pub-id-type="pmid">15971777</pub-id></mixed-citation>
    </ref>
    <ref id="btz353-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Koh</surname><given-names>J.L.Y.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Cyclops: a comprehensive database constructed from automated analysis of protein abundance and subcellular localization patterns in <italic>Saccharomyces cerevisiae</italic></article-title>. <source>G3 Genes Genomes Genet</source>., <volume>5</volume>, <fpage>1223</fpage>â<lpage>1232</lpage>.</mixed-citation>
    </ref>
    <ref id="btz353-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kraus</surname><given-names>O.Z.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Classifying and segmenting microscopy images with deep multiple instance learning</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>i52</fpage>â<lpage>i59</lpage>.<pub-id pub-id-type="pmid">27307644</pub-id></mixed-citation>
    </ref>
    <ref id="btz353-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kraus</surname><given-names>O.Z.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Automated analysis of high-content microscopy data with deep learning</article-title>. <source>Mol. Syst. Biol</source>., <volume>13</volume>, <fpage>924.</fpage><pub-id pub-id-type="pmid">28420678</pub-id></mixed-citation>
    </ref>
    <ref id="btz353-B16">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Larochelle</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) An empirical evaluation of deep architectures on problems with many factors of variation. In: <italic>ICML, Corvalis, Oregon, USA</italic> ACM, pp. <fpage>473</fpage>â<lpage>480</lpage>.</mixed-citation>
    </ref>
    <ref id="btz353-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>1989</year>) 
<article-title>Generalization and network design strategies</article-title>. In: Pfeifer, <italic>et al.</italic> (eds) <source>Connectionism in Perspective, Zurich, Switzerland, Springer</source>, pp. <fpage>143</fpage>â<lpage>155</lpage>.</mixed-citation>
    </ref>
    <ref id="btz353-B18">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>X.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) Deeply supervised rotation equivariant network for lesion segmentation in dermoscopy images. In: <source>OR 2.0 Context-Aware Operating Theaters, Computer Assisted Robotic Endoscopy, Clinical Image-Based Procedures, and Skin Image Analysis</source>, Springer, pp. <fpage>235</fpage>â<lpage>243</lpage>.</mixed-citation>
    </ref>
    <ref id="btz353-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Rotation-invariant hog descriptors using Fourier analysis in polar and spherical coordinates</article-title>. <source>Int. J. Comput. Vis</source>., <volume>106</volume>, <fpage>342</fpage>â<lpage>364</lpage>.</mixed-citation>
    </ref>
    <ref id="btz353-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lundberg</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Borner</surname><given-names>G.H.</given-names></name></person-group> (<year>2019</year>) 
<article-title>Spatial proteomics: a powerful discovery tool for cell biology</article-title>. <source>Nat. Rev. Mol. Cell Biol</source>., <volume>20</volume>, <fpage>285</fpage>â<lpage>302</lpage>.<pub-id pub-id-type="pmid">30659282</pub-id></mixed-citation>
    </ref>
    <ref id="btz353-B21">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Marcos</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) Rotation equivariant vector field networks. In: <source>ICCV, Venice, Italy</source>, pp. <fpage>5058</fpage>â<lpage>5067</lpage>.</mixed-citation>
    </ref>
    <ref id="btz353-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ojala</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2002</year>) 
<article-title>Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>., <volume>24</volume>, <fpage>971</fpage>â<lpage>987</lpage>.</mixed-citation>
    </ref>
    <ref id="btz353-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rozenblatt-Rosen</surname><given-names>O.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>The human cell atlas: from vision to reality</article-title>. <source>Nature</source>, <volume>550</volume>, <fpage>451.</fpage><pub-id pub-id-type="pmid">29072289</pub-id></mixed-citation>
    </ref>
    <ref id="btz353-B24">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Sabour</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) <chapter-title>Dynamic routing between capsules</chapter-title> In: <person-group person-group-type="editor"><name name-style="western"><surname>Guyon</surname><given-names>I.</given-names></name></person-group><etal>et al</etal> (eds) <source>NIPS, Long Beach, CA, USA</source>. 
<publisher-name>Curran Associates, Inc</publisher-name>, pp. <fpage>3856</fpage>â<lpage>3866</lpage>.</mixed-citation>
    </ref>
    <ref id="btz353-B25">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Schmidt</surname><given-names>U.</given-names></name>, <name name-style="western"><surname>Roth</surname><given-names>S.</given-names></name></person-group> (<year>2012</year>) <chapter-title>Learning rotation-aware features: from invariant priors to equivariant descriptors</chapter-title> In: <source>CVPR, Providence, RI, USA</source>. 
<publisher-name>IEEE</publisher-name>, pp. <fpage>2050</fpage>â<lpage>2057</lpage>.</mixed-citation>
    </ref>
    <ref id="btz353-B26">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Veeling</surname><given-names>B.S.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) <chapter-title>Rotation equivariant CNNs for digital pathology</chapter-title> In: MICCAI, Granada, Spain, pp. <fpage>210</fpage>â<lpage>218</lpage>.</mixed-citation>
    </ref>
    <ref id="btz353-B27">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Weiler</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) Learning steerable filters for rotation equivariant CNNs. In: <source>CVPR, Salt Lake City, UT, USA</source>, pp. <fpage>849</fpage>â<lpage>858</lpage>.</mixed-citation>
    </ref>
    <ref id="btz353-B28">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Worrall</surname><given-names>D.E.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) Harmonic networks: deep translation and rotation equivariance. In: <source>CVPR, Honolulu, HI, USA</source>, pp. <fpage>5028</fpage>â<lpage>5037</lpage>.</mixed-citation>
    </ref>
    <ref id="btz353-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Murphy</surname><given-names>R.F.</given-names></name></person-group> (<year>2007</year>) 
<article-title>Automated learning of generative models for subcellular location: building blocks for systems biology</article-title>. <source>Cytometry A</source>, <volume>71</volume>, <fpage>978</fpage>â<lpage>990</lpage>.<pub-id pub-id-type="pmid">17972315</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
