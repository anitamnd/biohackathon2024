<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName archivearticle.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neurorobot</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neurorobot</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neurorobot.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neurorobotics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1662-5218</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6795684</article-id>
    <article-id pub-id-type="doi">10.3389/fnbot.2019.00076</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Methods</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Hybrid Brain-Computer-Interfacing for Human-Compliant Robots: Inferring Continuous Subjective Ratings With Deep Regression</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Fiederer</surname>
          <given-names>Lukas D. J.</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn001">
          <sup>†</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/421247/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Völker</surname>
          <given-names>Martin</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn001">
          <sup>†</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Schirrmeister</surname>
          <given-names>Robin T.</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Burgard</surname>
          <given-names>Wolfram</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff4">
          <sup>4</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/43415/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Boedecker</surname>
          <given-names>Joschka</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff4">
          <sup>4</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/31186/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ball</surname>
          <given-names>Tonio</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/21962/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Neuromedical AI Lab, Department of Neurosurgery, Medical Center – University of Freiburg, Faculty of Medicine, University of Freiburg</institution>, <addr-line>Freiburg</addr-line>, <country>Germany</country></aff>
    <aff id="aff2"><sup>2</sup><institution>BrainLinks BrainTools, Cluster of Excellence, University of Freiburg</institution>, <addr-line>Freiburg</addr-line>, <country>Germany</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Graduate School of Robotics, University of Freiburg</institution>, <addr-line>Freiburg</addr-line>, <country>Germany</country></aff>
    <aff id="aff4"><sup>4</sup><institution>Department of Computer Science, University of Freiburg</institution>, <addr-line>Freiburg</addr-line>, <country>Germany</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Guang Chen, Tongji University, China</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Theerawit Wilaiprasitporn, Vidyasirimedhi Institute of Science and Technology, Thailand; Qiuxuan Wu, Hangzhou Dianzi University, China</p>
      </fn>
      <corresp id="c001">*Correspondence: Tonio Ball <email>tonio.ball@uniklinik-freiburg.de</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>†These authors have contributed equally to this work</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>10</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>13</volume>
    <elocation-id>76</elocation-id>
    <history>
      <date date-type="received">
        <day>30</day>
        <month>11</month>
        <year>2018</year>
      </date>
      <date date-type="accepted">
        <day>27</day>
        <month>8</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2019 Fiederer, Völker, Schirrmeister, Burgard, Boedecker and Ball.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Fiederer, Völker, Schirrmeister, Burgard, Boedecker and Ball</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Appropriate robot behavior during human-robot interaction is a key part in the development of human-compliant assistive robotic systems. This study poses the question of how to continuously evaluate the quality of robotic behavior in a hybrid brain-computer interfacing (BCI) task, combining brain and non-brain signals, and how to use the collected information to adapt the robot's behavior accordingly. To this aim, we developed a rating system compatible with EEG recordings, requiring the users to execute only small movements with their thumb on a wireless controller to rate the robot's behavior on a continuous scale. The ratings were recorded together with dry EEG, respiration, ECG, and robotic joint angles in ROS. Pilot experiments were conducted with three users that had different levels of previous experience with robots. The results demonstrate the feasibility to obtain continuous rating data that give insight into the subjective user perception during direct human-robot interaction. The rating data suggests differences in subjective perception for users with no, moderate, or substantial previous robot experience. Furthermore, a variety of regression techniques, including deep CNNs, allowed us to predict the subjective ratings. Performance was better when using the position of the robotic hand than when using EEG, ECG, or respiration. A consistent advantage of features expected to be related to a motor bias could not be found. Across-user predictions showed that the models most likely learned a combination of general and individual features across-users. A transfer of pre-trained regressor to a new user was especially accurate in users with more experience. For future research, studies with more participants will be needed to evaluate the methodology for its use in practice. Data and code to reproduce this study are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/TNTLFreiburg/NiceBot">https://github.com/TNTLFreiburg/NiceBot</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>robot behavior</kwd>
      <kwd>autonomous robots</kwd>
      <kwd>BCI</kwd>
      <kwd>regression</kwd>
      <kwd>CNN</kwd>
      <kwd>deep learning</kwd>
      <kwd>random forests</kwd>
      <kwd>support vector machines</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">Deutsche Forschungsgemeinschaft<named-content content-type="fundref-id">10.13039/501100001659</named-content></funding-source>
      </award-group>
      <award-group>
        <funding-source id="cn002">Baden-Württemberg Stiftung<named-content content-type="fundref-id">10.13039/100008316</named-content></funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="10"/>
      <table-count count="3"/>
      <equation-count count="2"/>
      <ref-count count="50"/>
      <page-count count="17"/>
      <word-count count="10768"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <sec>
      <title>1.1. Brain and Non-brain Signals</title>
      <p>In brain-computer interfaces (BCIs) for the control of assistive robots, a safe and human-compliant behavior of the robot during the interaction with its user is a crucial factor. However, what behavior is assessed as “safe” depends strongly on subjective parameters (Feil-Seifer et al., <xref rid="B9" ref-type="bibr">2007</xref>). For example, users might differently react to robot movement at higher or lower speeds, or robotic poses in proximity to the user's body or face. Moreover, users might perceive the robot's behavior in different ways depending on personal variables, e.g., their previous exposure to robots. In this study, we describe a BCI-compatible method to continuously acquire subjective rating data about the quality of robotic behavior in real-time during a human-robot interaction task. The real-time nature and BCI compatibility are crucial as, combined with the measurement of electroencephalography (EEG), electrocardiography (ECG) and respiration, we aim to identify inter-personal commonalities and differences, specific rating strategies, and their stability over time. Further, we evaluate regression techniques to allow an automatic prediction of subjective ratings, which could be used to automatically adapt the robot's behavior to user-specific preferences using reinforcement-learning. We extend the traditional hybrid BCI framework (Pfurtscheller et al., <xref rid="B31" ref-type="bibr">2010</xref>), combining brain and non-brain signal, by including information from the robot into the regressions.</p>
    </sec>
    <sec>
      <title>1.2. Related Work</title>
      <p>In the field of human-robot interaction, the assessment of robotic behavior has been a key part in a number of studies. Huang and Mutlu (<xref rid="B14" ref-type="bibr">2012</xref>) developed a toolbox for behavioral assessment of humanoid robots. There, the authors focus especially on human-like social behavior in robots. Ratings for variables of robot behavior, e.g., naturalness, likability, and competence, were collected after the experiments rather than during the actual interaction. Tapus et al. (<xref rid="B44" ref-type="bibr">2008</xref>) proposed a robot personality matching for robot behavior adaptation in post-stroke rehabilitation. To adapt the robots behavior, the authors used a Policy Gradient Reinforcement Learning (PGRL) Algorithm. The robot collected feedback from the user with voice recognition, using discrete classes such as “yes,” “no,” and “stop.” Sekmen and Challa (<xref rid="B41" ref-type="bibr">2013</xref>) combined sensory input from speech recognition, natural language processing, face detection and recognition, and implemented a Bayesian learning mechanism to estimate and update a parameter set that models behaviors and preferences of users. Specifically, they predict future actions of their users to prepare the robot for these. In a recent study of Sarkar et al. (<xref rid="B37" ref-type="bibr">2017</xref>), the effects of robot experience and personality of a user on the assessment of, among other factors, trust into the robot were assessed. Interestingly, the group of participants with previous robot experience rated their safety during the interaction with the robot on a lower level than the group which had no previous experience with robots. Less experienced people also rated the robot as more intelligent in this study.</p>
      <p>Relevant to the decoding of perceived danger from EEG data, Kolkhorst et al. (<xref rid="B17" ref-type="bibr">2017</xref>) decoded the perceived hazardousness in traffic scenes from EEG data. This could also be used in human-robot interactions to prevent potentially dangerous situations. Kolkhorst et al. (<xref rid="B18" ref-type="bibr">2018</xref>) further developed an EEG-based target selection in collaboration with robotic effectors, which could harmonize well with assessment of robot behavior in human-machine interactions. Ehrlich and Cheng (<xref rid="B7" ref-type="bibr">2018</xref>) recently developed a system to validate robot actions by decoding error-related signals from EEG. Related to this, a number of studies in recent years have shown that the performance of robots in BCI scenarios can be enhanced with error decoding, e.g., in shared-control BCIs (Iturrate et al., <xref rid="B15" ref-type="bibr">2013</xref>), or during the observation of autonomous robots (Salazar-Gomez et al., <xref rid="B35" ref-type="bibr">2017</xref>).</p>
      <p>In recent years, promising new approaches to decoding information from brain signals for BCI control were developed, e.g., deep learning with convolutional neural networks (CNNs). A major advantage of CNNs is that feature extraction and classification are combined into a single learning process, removing the need to manually extract features. After pioneering achievements in the field of computer vision, they are increasingly being adapted to problems of EEG decoding (Manor and Geva, <xref rid="B24" ref-type="bibr">2015</xref>; Bashivan et al., <xref rid="B1" ref-type="bibr">2016</xref>) and are the subject of active research (e.g., Eitel et al., <xref rid="B8" ref-type="bibr">2015</xref>; Watter et al., <xref rid="B50" ref-type="bibr">2015</xref>; Oliveira et al., <xref rid="B28" ref-type="bibr">2016</xref>). These biologically inspired networks have a great potential to improve the accuracy of BCI applications (Burget et al., <xref rid="B3" ref-type="bibr">2017</xref>; Schirrmeister et al., <xref rid="B39" ref-type="bibr">2017</xref>; Kuhner et al., <xref rid="B19" ref-type="bibr">2019</xref>). They additionally can be applied to the raw EEG data, greatly simplifying the design of BCI pipelines. We further demonstrated the usefulness of CNNs for error decoding from noninvasive (Völker et al., <xref rid="B48" ref-type="bibr">2018c</xref>) and intracranial EEG (Völker et al., <xref rid="B47" ref-type="bibr">2018b</xref>).</p>
      <p>In contrast to discrete decoding problems, regression analysis with neural networks have become more popular in the recent time. Most use cases shown so far applied regression methods to video or image data. For example, Held et al. (<xref rid="B13" ref-type="bibr">2016</xref>) used regression to successfully track objects in videos at 100 frames per second. Shi et al. (<xref rid="B42" ref-type="bibr">2016</xref>) presented a regression approach to identify facial landmarks to subsequently align faces in images. In order to detect and localize robotic tools during robot-assisted surgery, Sarikaya et al. (<xref rid="B36" ref-type="bibr">2017</xref>) implemented a regression layer into a CNN. Miao et al. (<xref rid="B26" ref-type="bibr">2016</xref>) used regression techniques for a real-time 2D and 3D registration of X-ray images. With a CNN regressor, Viereck et al. (<xref rid="B45" ref-type="bibr">2017</xref>) improved the accuracy of robotic grasping and object recognition with respect to simulated depth images.</p>
    </sec>
    <sec>
      <title>1.3. Aims and Objectives</title>
      <p>The goal of this study was to assess the feasibility of acquiring continuous data on the subjective perception of the behavior of assistive robots in a BCI context. Considering this context, the approach should not be limited to robots of a certain design, e.g., humanoid or not. Rather, we aimed to create a generalizable method for the subjective assessment of robot behavior, utilizable in real-time during BCI (and other) experiments. In future applications, such real-time ratings could then be leveraged for reinforcement-learning algorithms designed to adapt robotic behavior during the interaction in a human-compliant manner. Importantly, we wanted to evaluate subjective perception not by discrete values, but with a continuous rating system, allowing a more fine-grained analysis of the outcome. By recording EEG, ECG, and respiratory data simultaneously, we aimed to allow a search for physiological correlates of these ratings, which could later be used as input for an implicit situation assessment, without the user needing to explicitly rate the robot's behavior. Finally, we aimed to use regression methods to create an automatic and continuous prediction of subjective ratings for each user, and evaluate which kinds of input data and EEG features are the most informative about subjective perception during direct interaction with a robotic assistant. Our procedure is schematically summarized in <xref ref-type="fig" rid="F1">Figure 1</xref>.</p>
      <fig id="F1" position="float">
        <label>Figure 1</label>
        <caption>
          <p>Structural diagram. A robot performs pre-programmed trajectories in the proximity of a user. The user subjectively and continuously rates the human compliance of the trajectories. In parallel, EEG, ECG, and respiration are measured. After the experiments, the EEG, ECG, respiration, and hand position data are used to regress the ratings. We report the correlation coefficient (Pearson's ρ) and root mean square error (RMSE) between ratings and regression outputs as intuitive quantitative measures. Our long-term goal is to provide real-time feedback to reinforcement-learning algorithms controlling the robot without explicit user ratings.</p>
        </caption>
        <graphic xlink:href="fnbot-13-00076-g0001"/>
      </fig>
    </sec>
  </sec>
  <sec id="s2">
    <title>2. Experiments</title>
    <p>We conducted a series of experiments to evaluate robot behavior during interaction with an assistive robot grasping an object and delivering it to the user. The users were instructed not to move any body part during the experiment, to keep the interaction similar to that of a paralyzed person with a robot and to prevent muscle activity from contaminating the EEG data. Users were informed that the robotic arm would simulate the grasping of an object, e.g., a cup of water, and bring it forward toward them. The users did not have any prior knowledge about the path or velocity the robot would use. In some of the trajectories, the robot would deviate from the correct trajectory, e.g., by stopping in a wrong position, i.e., not in range of the user, or by positioning its hand above and behind the user's head. The trajectories were pre-programmed to ensure comparable experiments across all users. <xref ref-type="fig" rid="F2">Figures 2A,B</xref> show the real and simulated experimental setup, respectively.</p>
    <fig id="F2" position="float">
      <label>Figure 2</label>
      <caption>
        <p>Paradigm setting, reconstruction, and rating system. <bold>(A)</bold> Photograph of the real setting. The picture was edited to make the user anonymous and to display the background in black and white. <bold>(B)</bold> Reconstruction in V-REP. <bold>(C)</bold> Robot behavior rating system with a wireless controller. The users rated the robot's behavior within a continuous range from −1 to 1. To do that, the users had to keep a thumbstick on a wireless controller on an outward circular path with their right thumb, where a left position represented a very bad rating (−1, red), a position to the right represented a good rating (1, blue), and a position straight up or down represented a neutral rating (0, yellow). This system was introduced so that the movement effort for the thumb was the same at each position, making an EEG analysis possible.</p>
      </caption>
      <graphic xlink:href="fnbot-13-00076-g0002"/>
    </fig>
    <sec>
      <title>2.1. Hardware</title>
      <p>As robotic arm, the LBR iiwa 7 R800 (KUKA Robotics), a 7 DOF lightweight robot combined with a three-fingered hand (Dexterous Hand 2.0, Schunk) was used. EEG was recorded using the g.SAHARA dry active electrode system and three g.USBamp amplifiers (Guger Technologies). Usable without electrode gel, dry electrodes have the advantage to be set up faster than systems with wet electrodes and may thus be more convenient for the user. The system used in this study is designed to capture a frequency range from 0.1 to 40 Hz. We recorded with 32 dry electrodes on the scalp positioned according to the 10-20 system at Fp1, Fpz, Fp2, AF7, AF3, AFz, AF4, AF8, F5, F3, F1, Fz, F2, F4, F6, FC1, FCz, FC2, C3, C1, Cz, C2, C4, CP3, CP1, CPz, CP2, CP4, P1, Pz, P2, and POz. The reference electrode was placed on the left mastoid, the ground electrode on the right mastoid. Further, ECG was recorded with two electrodes on the users' right clavicle and lowest left rib, and respiration was monitored with a respiration belt.</p>
    </sec>
    <sec>
      <title>2.2. Users</title>
      <p>Feasibility of continuous real-time rating of the subjective perception of robot behavior was evaluated in three users [age: 24 (S1), 26 (S2), and 30 (S3), all right-handed, S1 female]. S1 had no previous robot experience, S2 moderate experience with robots, i.e., worked with robots in a BCI context irregularly for approximately 2 years, and S3 already had a substantial amount of experience working with robots, i.e., worked extensively with robots at university and in in-depth projects for multiple years. All users were students of the University of Freiburg. Informed consent was provided before participation. The experiments were approved by the ethics committee of the University of Freiburg.</p>
      <p>The users were seated in a way so that they could observe the robot's movements without moving their head, and in a position in which the trajectories of the robotic arm could not intersect in any way with the users' body or head. 45, 95, and 95 trajectories were recorded in block of 15 for each user, respectively.</p>
    </sec>
    <sec>
      <title>2.3. Rating System</title>
      <p>The users were instructed to rate the quality of the behavior of the robot continuously during their interaction by moving the right thumbstick on a wireless controller (Logitech F710) into different directions (<xref ref-type="fig" rid="F2">Figure 2C</xref>). We did not ask for a more specific evaluation variable to gain a preferably generalizable evaluation of the robot's performance.</p>
      <p>To rate the robot's behavior as good, the users had to move the thumbstick to the right; a position on the left was linked to a bad rating, and a position in the middle corresponded to a neutral rating. This rating strategy was thus designed in a way that, at all times, the users had to keep the thumbstick at a maximal deflection, to generate a tonic motor output at a similar level irrespective of the rating conveyed and thus to minimize movement-related brain responses possibly confounding EEG correlates of the ratings (see section 5). Randomizing the direction of rating for each robot trajectory could be used to further avoid such a confound but would make the task more difficult for the users and could thus potentially introduce inadvertently wrong ratings. We therefore chose to keep the direction of rating constant across all robot trajectories in this pilot study.</p>
      <p>To calculate the rating, we first had to translate the x and y position of the thumbstick (both ranged from −1 to 1) into a rotation angle, and from that to a continuous rating from −1 (very bad) to 1 (very good), as shown in <xref ref-type="fig" rid="F2">Figure 2</xref>. Thus, the conversion of the thumbstick x, y position to the rating is defined as</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M1">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>r</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mi>g</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>a</mml:mi>
                    <mml:mi>b</mml:mi>
                    <mml:mi>s</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:mi>a</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mi>c</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>n</mml:mi>
                        <mml:mn>2</mml:mn>
                        <mml:mi>d</mml:mi>
                        <mml:mrow>
                          <mml:mo stretchy="false">(</mml:mo>
                          <mml:mrow>
                            <mml:mi>y</mml:mi>
                            <mml:mo>,</mml:mo>
                            <mml:mi>x</mml:mi>
                          </mml:mrow>
                          <mml:mo stretchy="false">)</mml:mo>
                        </mml:mrow>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                    <mml:mo>-</mml:mo>
                    <mml:mn>90</mml:mn>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mn>90</mml:mn>
                  </mml:mrow>
                </mml:mfrac>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where arctan2d is the four-quadrant inverse tangent in degrees.</p>
    </sec>
    <sec>
      <title>2.4. Real-Time Data Processing</title>
      <p>Robot joint angles were controlled with the MoveIt! motion planning framework (Chitta et al., <xref rid="B6" ref-type="bibr">2012</xref>) via the Robot Operating System (ROS) (Quigley et al., <xref rid="B33" ref-type="bibr">2009</xref>). EEG and peripheral (ECG, respiration) data were recorded at a 512-Hz sampling rate with the BCI2000 software (Schalk et al., <xref rid="B38" ref-type="bibr">2004</xref>), using the Matlab Signal Processing (Matlab 2014a, The MathWorks, USA) module for real-time access to the raw EEG signals. In Matlab, a network connection to the local ROS master, controlling the robotic arm, was established. During the recording, the data were stored in a ring buffer and processed 16 times per second. EEG data were re-referenced to common average and filtered with a Butterworth band-pass filter of 3rd order between 0.5 and 40 Hz. For this, the filter coefficients were passed on between the blocks to avert filter artifacts and allow filtering on such short time segments. A ROS custom message type was used for the broadcasting of the data. The collected EEG, ECG, respiration, and the rating data were sent 16 times per second to the ROS master, where they were timestamped and stored together with the seven joint angles of the robotic arm.</p>
    </sec>
    <sec>
      <title>2.5. <italic>Post-hoc</italic> Data Reconstruction</title>
      <p>Robot joint states, EEG, physiological recordings, and rating data were all stored in ROS bag files and later loaded into Matlab with help of ROS. To be able to reconstruct the exact trajectories of the robot, e.g., to calculate the distance between the users' head and the robotic hand and the hand's velocity, V-REP (Rohmer et al., <xref rid="B34" ref-type="bibr">2013</xref>) (Coppelia Robotics) was used together with its Matlab API (<xref ref-type="fig" rid="F2">Figure 2B</xref>). As replacement for the Schunk hand, which was not available in V-REP, we used the BarrettHand (BARRETT TECH) in the reconstruction, which has approximately the same size and shape, and was also equipped with three fingers. The trajectories, relative to the users head, are show in <xref ref-type="fig" rid="F3">Figure 3</xref>.</p>
      <fig id="F3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Effector trajectories and ratings. Users with no previous robot experience (S1), moderate robot experience (S2), and substantial robot experience (S3). The positions of the points correspond to the position of the base of the robotic hand. The color of each point displays the user's rating of the robot's behavior at this point in time. Red color indicates a negative rating, blue color indicated a positive rating; white corresponds with a neutral rating. The axes show the distance of the robotic hand from the user's head (gray sphere, triangle represents user's nose) in meters within the respective dimension (x, y, z).</p>
        </caption>
        <graphic xlink:href="fnbot-13-00076-g0003"/>
      </fig>
    </sec>
    <sec>
      <title>2.6. Evaluation Structure</title>
      <p>The data acquired during the pilot experiments is evaluated two-fold. First, to better understand the recorded data and investigate the influence of user robot experience, we analyse the data and present the results of this feature-driven analysis in section 3. To permit a richer analysis, the experiments were further recreated in reconstructions using V-REP, as described above. Second, to evaluate which kind of input data are potentially the most informative for proving feedback to reinforcement-learning algorithms, we perform end-to-end regression analyses of the data in the attempt to reconstruct the users' ratings of the robotic behavior. Regressions are performed within and across users. We quantify the regression results using correlations (Pearson's ρ) and the root mean square error (RMSE). These metrics are reported in section 4.5. <xref ref-type="fig" rid="F1">Figure 1</xref> schematically depicts the structure of our approach.</p>
    </sec>
  </sec>
  <sec id="s3">
    <title>3. Trajectory and Rating</title>
    <p>From the reconstruction in V-REP, we extracted the position of the robotic hand base over time. By combining the positions in 3D with the subjective rating at the same points in time, we were able to create trajectory-rating maps for each user, which are shown in <xref ref-type="fig" rid="F3">Figure 3</xref>.</p>
    <p>The trajectory-rating maps reveal a number of commonalities in the robot ratings across the users. For example, trajectories which were pre-programmed to end in an incorrect position (i.e., not within reach of the user) were expected and consistently labeled as “bad” by all users. Further, positions in which the robotic hand was above and behind a user's head, and thus not in the user's field of vision were also always rated as “bad.” The initial robot pose was mostly rated as neutral or close-to neutral. Other distinct positions of the robotic hand, like grasping, over-head, or correct/incorrect end-positions were rated more strongly, either positive or negative, than positions occurring during ongoing movements, or between the distinct poses.</p>
    <p>However, despite the small number of users investigated, we also observed some distinct inter-individual differences in the robot ratings. S1, with no previous robot experience, gave in general lower ratings (mean: 0.04 ± 0.34), even if the robot was fulfilling the task objectively correct, like in the grasping position. The correct end-position was also sometimes rated as bad, especially if the robotic hand approached the user in a relatively steep angle from above. The negative ratings were often only in the range of 0 to −0.5. S2, with moderate robot experience, rated the majority of poses as good (mean: 0.23 ± 0.59), except the positions in which the robotic hand was positioned over the user's head, and thus out of the field of vision, and the wrong end-position. These were rated as strongly negative. S3, with a substantial amount of robot experience, again rated the robot's behavior as overall more positive than the other two users (mean: 0.37 ± 0.44). In general, the trajectory ratings in S3 appeared to be more similar to the user with moderate robot experience. Over-head and wrong end-position were rated negatively by all users, but least so by S3.</p>
    <sec>
      <title>3.1. Key Poses During the Human-Robot Interaction</title>
      <p>From these trajectory-rating maps, we selected four key positions of the robot hand for deeper analysis: the grasping position, the over-head position, the correct end-position, and the incorrect end-position. These, and the initial position, are displayed from three different viewpoints in <xref ref-type="fig" rid="F4">Figure 4</xref>.</p>
      <fig id="F4" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Key robot poses during the human-robot interaction. In addition to the initial pose, four key poses were pre-programmed into the trajectories. The object-grasping pose, a pose above the user's head and outside of its field of view, the correct end pose where the grasped object is delivered to the user, and the wrong end pose where the grasped object cannot be properly delivered to the user. Screenshots from the V-REP reconstruction.</p>
        </caption>
        <graphic xlink:href="fnbot-13-00076-g0004"/>
      </fig>
      <p>In the four key positions, we calculated the mean rating for each user by identifying the spatial center of the robot hand position associated with each key pose and extracting the ratings within a cube with 40 cm side length around the center point. The mean ratings for the different users are listed in <xref rid="T1" ref-type="table">Table 1</xref> and illustrated in <xref ref-type="fig" rid="F5">Figure 5</xref>.</p>
      <table-wrap id="T1" position="float">
        <label>Table 1</label>
        <caption>
          <p>Mean ± std subjective ratings at key robot positions.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>User (rob. exp.)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Grasping</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Over-head</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Correct end</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Wrong end</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">S1 (no)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.00 ± 0.21</td>
              <td valign="top" align="center" rowspan="1" colspan="1">−0.33 ± 0.23</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.20 ± 0.42</td>
              <td valign="top" align="center" rowspan="1" colspan="1">−0.50 ± 0.21</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">S2 (moderate)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.64 ± 0.27</td>
              <td valign="top" align="center" rowspan="1" colspan="1">−0.52 ± 0.38</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.63 ± 0.33</td>
              <td valign="top" align="center" rowspan="1" colspan="1">−0.70 ± 0.25</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">S3 (substantial)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.84 ± 0.22</td>
              <td valign="top" align="center" rowspan="1" colspan="1">−0.28 ± 0.22</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.79 ± 0.25</td>
              <td valign="top" align="center" rowspan="1" colspan="1">−0.17 ± 0.38</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <fig id="F5" position="float">
        <label>Figure 5</label>
        <caption>
          <p>Mean subjective ratings at key robot positions. Key robot positions are depicted in <xref ref-type="fig" rid="F4">Figure 4</xref>. Rating sample from cube with 40 cm side length centered on key pose. The error bars show the standard deviation of the rating sample.</p>
        </caption>
        <graphic xlink:href="fnbot-13-00076-g0005"/>
      </fig>
    </sec>
    <sec>
      <title>3.2. Development of Ratings Over the Time of the Experiment</title>
      <p>As we expected that robot ratings might undergo systematic changes on the time scale of our experiment, we further analyzed the temporal development of ratings in these key positions (<xref ref-type="fig" rid="F6">Figure 6</xref>). The long-term stability was assessed by applying a moving-average filter to smooth out fast fluctuations. While the overall distribution of ratings for the key positions stayed quite stable over the experiment, the absolute strength of the ratings did vary over the course of the experiment, especially in S1, the user without robot experience. Specifically, S1 displayed a tendency to rate the objectively “positive” poses, i.e., the grasping pose and the correct end-position, lower toward the end than at the beginning of the experiment. In S2 and S3, the users with moderate and substantial robot experience, particularly the ratings of the positive poses stayed more stable over time. Together, these observations fit well to our expectations that robot perception may change over time, and that such changes may be depended on previous robot exposure.</p>
      <fig id="F6" position="float">
        <label>Figure 6</label>
        <caption>
          <p>Development of ratings at key robot positions during the experiment. The y-axis shows the subjective rating when the robotic hand was positioned for grasping (blue), in an over-head position (yellow), at the correct end-position for delivery (green), or at a wrong, too-high end-position (red). The x-axis shows the development of the ratings over the duration of the experiment. The ratings are smoothed with a moving-average filter consisting of a Gaussian weighted window with the size of <inline-formula><mml:math id="M2"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula>th of the total samples per condition, which was convolved with the raw ratings.</p>
        </caption>
        <graphic xlink:href="fnbot-13-00076-g0006"/>
      </fig>
    </sec>
    <sec>
      <title>3.3. Effect of Robot Distance and Velocity</title>
      <p>To understand the relationship of the subjective rating with both the velocity and the distance of the robotic hand in reference to the user's head, we calculated these metrics for each time point. For each user and feature, we estimated Pearson's Linear Correlation Coefficient (Pearson's ρ) (<xref rid="T2" ref-type="table">Table 2</xref>).</p>
      <table-wrap id="T2" position="float">
        <label>Table 2</label>
        <caption>
          <p>Linear correlation of robot distance and velocity with subjective ratings.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>Linear correlation with subjective rating</bold>
              </th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">
                <bold>Robot hand distance</bold>
              </th>
              <th valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">
                <bold>Robot hand velocity</bold>
              </th>
            </tr>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>User (rob. exp.)</bold>
              </th>
              <th valign="top" align="center" colspan="2" rowspan="1">
                <bold>Pearson's</bold>
                <bold>ρ</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">S1 (no)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">−0.002</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.226</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">S2 (moderate)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.369</td>
              <td valign="top" align="center" rowspan="1" colspan="1">−0.039</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">S3 (substantial)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.205</td>
              <td valign="top" align="center" rowspan="1" colspan="1">−0.197</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>The distance of the robotic hand correlated moderately positive with the rating of the users S2 and S3, while it did not exhibit any correlation for S1 (with no previous robot experience). The correlation of subjective rating with the hand velocity was only weakly negative for S3, and moderately positive in S1, while there was no such effects in S2.</p>
    </sec>
  </sec>
  <sec id="s4">
    <title>4. Regression for Rating Prediction</title>
    <p>To show the feasibility of our proposed method we trained diverse regressors to predict the subjective ratings from the recorded data within and across users. We focused on the aforementioned feasibility, exploring features proposed in the literature and probing for a possible motor bias. Regressions are performed with python using pytorch (Paszke et al., <xref rid="B29" ref-type="bibr">2017</xref>) version 1.0.0, braindecode (Schirrmeister et al., <xref rid="B39" ref-type="bibr">2017</xref>) version 0.4.7 and scikit-learn (Pedregosa et al., <xref rid="B30" ref-type="bibr">2011</xref>) version 0.20.2.</p>
    <sec>
      <title>4.1. Data Pre-processing</title>
      <p>For each user, we downsampled the data to 256 Hz, standardized to a mean of 0 and a variance of 1 using an exponentially weighted mean with factor 0.001 and split them three-fold. We kept the last three minutes of data separate for final evaluation of our regressors. The last 3 min before the final evaluation set were used as validation set during the manual hyper-parameter search. The remainder of the recording was used as training set. For the final evaluation, which is reported here, training set and validation set were combined into a single training set.</p>
    </sec>
    <sec>
      <title>4.2. Evaluation Metrics</title>
      <p>We evaluated the similarity of the predicted rating to the true rating by calculating their correlation coefficient (Pearson's ρ) and the RMSE of their difference. We report both metrics here because they reflect two different aspects of the predictions. The correlation coefficient only compares the shape of predictions and ratings while the RMSE compares the values. For reinforcement-learning, proper approximation of the shape of the rating is already sufficient. Using predicted ratings which additionally have correct scaling and value range is of course better but much more difficult to achieve. Thus, our preferred metric is the correlation coefficient.</p>
    </sec>
    <sec>
      <title>4.3. Feature Extraction</title>
      <p>To probe for differential information content, we split the data into different components. First the different modalities, the robot hand 3D coordinates (robot pos.), the ECG and respiration data (periphery data), and the EEG data were split. Each of these data modalities and their combinations were fed as-is to the regressors. To prevent to much redundancy in the results we only report the combination of all three modalities. For modality splits containing EEG data, we additionally tested three electrode selections and seven frequency-band selections. Based on literature (Cavanagh et al., <xref rid="B5" ref-type="bibr">2010</xref>; Cavanagh and Frank, <xref rid="B4" ref-type="bibr">2014</xref>; Spüler and Niethammer, <xref rid="B43" ref-type="bibr">2015</xref>; Völker et al., <xref rid="B46" ref-type="bibr">2018a</xref>), a potential target for a physiological brain signal underlying the rating could potentially be the delta (0–4 Hz) and theta (4–8 Hz) frequency bands, especially in the midline electrodes. Thus we, in addition to the raw EEG data, used band-passes from 0 to 4 and 4 to 8 Hz and selected all electrodes located on the head midline (all electrodes containing a z in the 10–20 nomenclature). To investigate the influence of a potential motor bias we further band-passed the EEG data from 8 to 14 Hz (alpha band), 14 to 20 Hz (low-beta band), 20 to 30 Hz (mid-beta band) and 30 to 40 Hz (high-beta/low-gamma band) and selected all electrodes located on the sensorimotor cortex (all electrodes containing a C in the 10–20 nomenclature). We report the results of all the triplet combinations of these features, i.e., four data modality selections, three electrode selections, and seven frequency-band selections, resulting in 44 (2+2 · 3 · 7) result samples per regressor described in sections 4.5, 4.6.</p>
    </sec>
    <sec>
      <title>4.4. Statistics</title>
      <p>In our visualizations and statistics, we focused on aspects which generalize over the whole sample, analysing the regression results pooled over all but the investigated aspect. We use this approach to compensate for the small user sample inherent to a pilot study. Trying to generate results for specific feature, user, and regressor combinations is possible but would most likely not generalize well to a larger user cohort. To evaluate whether a result sample of a given aspect (feature, user, or regressor) significantly differed from its peers, we performed non-parametric tests as our samples were non normal distributed. When pairs were available, we performed two-sided sign-tests using our own implementation. When no pairs were available, e.g., when comparing data selections containing EEG data to data selections without EEG data, we performed two-sided Mann–Whitney-<italic>U</italic>-tests with continuity correction, as implemented in scipy (Jones et al., <xref rid="B16" ref-type="bibr">2001</xref>) version 1.3.0. In both tests, ties correction was performed.</p>
      <p>To asses whether the reported regression performances were above chance level, we permuted the subjective ratings of the training set 10<sup>6</sup> times and compared them to the subjective ratings of the test set using the metrics listed in section 4.2. The length of permuted training subjective ratings was truncated to the length of test subjective ratings after permutation to ensure that our samples were drawn from the entire training subjective ratings distribution. The <italic>p</italic>-values of each regression performance was calculated as</p>
      <disp-formula id="E2">
        <label>(2)</label>
        <mml:math id="M3">
          <mml:mtable class="eqnarray" columnalign="right center left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>p</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>n</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>p</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mi>m</mml:mi>
                        <mml:mi>u</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>n</mml:mi>
                        <mml:mo>≥</mml:mo>
                        <mml:mi>r</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>g</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>n</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>+</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>n</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>p</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mi>m</mml:mi>
                        <mml:mi>u</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>n</mml:mi>
                        <mml:mi>s</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>+</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:mfrac>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>with <inline-formula><mml:math id="M4"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> and <italic>n</italic><sub><italic>permutation</italic>≥<italic>regression</italic></sub> the number of permutations having an equal or better performance than the tested regression result.</p>
      <p>All calculated <italic>p</italic>-values were corrected for multiple testing using the false discovery rate (FDR) correction for dependent <italic>p</italic>-values (Benjamini and Yekutieli, <xref rid="B2" ref-type="bibr">2001</xref>) as implemented in the multipletests function of statsmodels (Seabold and Perktold, <xref rid="B40" ref-type="bibr">2010</xref>) version 0.9.0. We report the FDR-corrected <italic>p</italic>-values as <italic>q</italic>-values.</p>
    </sec>
    <sec>
      <title>4.5. CNN Regression</title>
      <p>We adapted three CNN classification architectures for regression analysis by removing the softmax layer and applying the mean square error (MSE) loss function to the training. The specific architectures used were (i) a 6-layered CNN (Deep4Net, 4 convolution-pooling blocks), (ii) a 29-layered residual neural network (EEGResNet-29, 13 residual blocks), both described by Schirrmeister et al. (<xref rid="B39" ref-type="bibr">2017</xref>), and (iii) a compact CNN (EEGNet V4, 4 layers, 2 convolution-pooling blocks) (Lawhern et al., <xref rid="B20" ref-type="bibr">2018</xref>). These CNN networks were chosen because they have previously been shown to perform well for classification of EEG data. EEGNet and Deep4Net were used as implemented in the Braindecode toolbox (<ext-link ext-link-type="uri" xlink:href="https://github.com/TNTLFreiburg/braindecode/">https://github.com/TNTLFreiburg/braindecode/</ext-link>).</p>
      <p>The weights of the models were initialized using a uniform distribution as described in Glorot and Bengio (<xref rid="B12" ref-type="bibr">2010</xref>). The models were then trained for 200 epochs, using a batch size of 64, a learning rate of 0.001 and a weight decay of 0. As feasibility rather than performance is the main focus of this paper, we use identical hyper-parameters for all models. That the models do learn using these hyper-parameters was verified using the validation set. The input time length of the models was individually adapted so that the predictor time length (data used to compute the MSE loss) was exactly 1 s for all models, irrespective of receptive field size. We use AdamW (Loshchilov and Hutter, <xref rid="B23" ref-type="bibr">2017</xref>) with default parameters as optimizer and schedule the learning rate using cosine annealing (Loshchilov and Hutter, <xref rid="B22" ref-type="bibr">2016</xref>) without restarts. We do not perform early stopping. Instead we use the regressor of the end of the training to predict the ratings, irrespective whether a regressor with better validation accuracy existed during the training.</p>
    </sec>
    <sec>
      <title>4.6. Non-CNN Regression</title>
      <p>We use four regressors implemented in scikit-learn (Pedregosa et al., <xref rid="B30" ref-type="bibr">2011</xref>). A linear regressor, a linear support vector regressor (L-SVR), a non-linear (radial basis function kernel) SVR (RBF-SVR) and a Random Forest regressor (RFR). Using the validation set we adjusted the maximal iterations of the RBF-SVR from infinity to 100,000 and the number of trees of the RFR from 10 to 100. Higher values lead to better validation results for both hyper-parameters but had to be limited because of computational budget. All other parameters were kept to the defaults of scikit-learn version 0.20.2.</p>
    </sec>
    <sec>
      <title>4.7. Within-User Regression</title>
      <p>First, we evaluated the regression on different data modalities within each user. As input data, we either used the position of the robotic hand in 3D, the EEG data, the peripheral physiological data (ECG, respiration), or all combined. <xref rid="T3" ref-type="table">Table 3</xref> lists the results of all regressors. As a chance level regression baseline we report the best out of 10<sup>6</sup> random permutations of the training labels of each user compared to the test labels in <xref rid="T3" ref-type="table">Table 3a</xref>.</p>
      <table-wrap id="T3" position="float">
        <label>Table 3</label>
        <caption>
          <p>Regression test-set metrics for different data modalities.</p>
        </caption>
        <graphic xlink:href="fnbot-13-00076-i0001"/>
        <table-wrap-foot>
          <p><italic>EEG refers to EEG data without any feature selection. Italic entries highlight best results for each regressor. Underlined entries highlight best results for each user. Bold entries highlight overall best results. Gray entries had q ≥ 0.05 when corrected for 28 tests (4 modalities · 7 regressors)</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>Chance levels were not competitive (~0.02) and very similar (±0.001) across users for Pearson's ρ, indicating that the correct shape of the subjective rating cannot be easily guessed. At the same time, the RMSE chance levels were unexpectedly competitive for S1 and S3, confirming that these 2 users rated with lower variance than S2 (0.112 and 0.194 vs. 0.354). No RMSE regression result based on the data of S1 was significantly better than random permutations (q ≥ 0.05).</p>
      <p>In most cases, the performance in the final evaluation set was the highest with the robotic hand position as the only input data. Predictions from EEG only showed rather low performance, while predictions from ECG &amp; respiration yielded low performance. A combination of all input data did improve the performance in some cases, mostly so for non-linear methods. Predicted and actual rating in the 3-min test set are displayed in <xref ref-type="fig" rid="F7">Figure 7</xref> for the Deep4Net, trained on the robotic hand position data.</p>
      <fig id="F7" position="float">
        <label>Figure 7</label>
        <caption>
          <p>Regression of subjective rating, using the robotic hand position as input. The network used was the Deep4Net, with a predictor time length of 1 s. The orange curve shows the actual rating given by the users, the blue curve shows the predicted rating. mse, mean square error; r, Pearson's ρ; p, <italic>p</italic>-value (uncorrected for multiple testing).</p>
        </caption>
        <graphic xlink:href="fnbot-13-00076-g0007"/>
      </fig>
      <p>When considering the best results obtained for EEG, periphery data and the combination of all data types, it seems like EEG and periphery data each carry information related to the rating. In the case of the non-linear regressors, this information could be used to improve the regression performance. To investigate the origin of this information, we trained our regressors on different EEG electrode selections and EEG frequency bands.</p>
    </sec>
    <sec>
      <title>4.8. Regression Using Different EEG Features</title>
      <p>We trained our regressors on different EEG electrode selections and EEG frequency bands. The differentiation of electrodes and frequency bands has 3 purposes. (1) To investigate the origin of the subjective-rating related information contained in the EEG. (2) To test whether indications from the literature transfer to our paradigm. (3) To probe for a potential motor bias. The results of these regressions are visualized in <xref ref-type="fig" rid="F8">Figures 8C,D</xref> for electrode and frequency-band selections, respectively.</p>
      <fig id="F8" position="float">
        <label>Figure 8</label>
        <caption>
          <p>User, feature, and regressor overview. Each matrix is based on the results of the 924 test-set regressions (3 users · 44 features · 7 models), split according to visualized parameter. Numbers in the lower and upper triangles of each comparison matrix are the differences between median RMSE (blue) and Pearson's ρ (orange), respectively. Bold font indicates <italic>q</italic> &lt; 0.05. Arrows indicate the parameter with better performance of each pair. The gray-scale background of each cell codes for the corresponding <italic>q</italic>-value. Separate FDR corrections for each triangle of each matrix. <bold>(A)</bold> Users: FDR-correction for 3 tests. mod., moderate; subst., substantial; exp., experience. <bold>(B)</bold> Data types: FDR-correction for six tests. <bold>(C)</bold> Electrodes. FDR-correction for 3 tests. *All EEG electrodes, *z: midline electrodes, *C*: sensorimotor cortex electrodes. <bold>(D)</bold> Frequency bands: FDR-correction for 21 tests. The corner frequencies of band-pass filters used are indicated in the square brackets. <bold>(E)</bold> Regressor: FDR-correction for 21 tests. SV, support vector.</p>
        </caption>
        <graphic xlink:href="fnbot-13-00076-g0008"/>
      </fig>
      <p>For the electrode selection, the only statistically significant result is that, when considering the shape of the subjective rating (reflected in Pearson's ρ), using all electrodes is marginally better than using only the electrodes located above the sensorimotor cortex (*C*) (<italic>q</italic> = 0.016). Non-significant results when considering shape of the regression are that using all electrodes is marginally better than using only the midline (*z) electrodes (<italic>q</italic> = 0.252) and that using the *C* electrodes is marginally worse than using the *z electrodes (<italic>q</italic> = 0.130). When considering the values of the regressions (reflected by the RMSE), non-significant results are that using all electrodes is marginally worse than using both *z electrodes (<italic>q</italic> = 1) and *C* electrodes (<italic>q</italic> = 1) and that using *z electrodes is marginally better than using *C* electrodes (<italic>q</italic> = 1). Summarizing, no considerable effect could be found. It appears like using all electrodes is marginally best when considering the shape of the regression and using *z electrodes is marginally best when considering the values of the regression.</p>
      <p>For the frequency-band selections, four comparisons related to the shape of the regression and one comparison related to the values of the regressions were statistically significant. Using the low-beta (14–20 Hz) and high-beta/low-gamma (30–40 Hz) frequency bands gave marginally better (higher) correlation coefficients than using the alpha (8–14 Hz) band (<italic>q</italic> = 0.009 and 0.023, respectively). Using the alpha band was marginally better than using the mid-beta (20–30 Hz) band (<italic>q</italic> = 0.023). Using the mid-beta band was marginally better than using the theta (4–8 Hz) band (<italic>q</italic> = 0.043). Finally, the RMSE was marginally better (lower) using the theta band than using the delta (0–4 Hz) band. The general tendency seen in the statistically significant results, higher frequency bands providing better results than lower frequency bands, can also be seen in the non-significant results. Similar to the electrode selection, no considerable effect was found.</p>
      <p>An overview of the regression results across all within-user regressions can be found in <xref ref-type="fig" rid="F8">Figures 8A</xref>, <xref ref-type="fig" rid="F9">9B</xref> to compare users, <xref ref-type="fig" rid="F8">Figures 8B</xref>, <xref ref-type="fig" rid="F9">9A</xref> to compare data types and <xref ref-type="fig" rid="F8">Figure 8E</xref> to compare regressors. An overview of the across-user regression results is provided in the next section.</p>
      <fig id="F9" position="float">
        <label>Figure 9</label>
        <caption>
          <p>Regression within users. Pearson's ρ (orange, higher is better) and the RMSE (blue, lower is better) are displayed separately in split violins. As test-set, the last 3 min of the experiment was used for each user. The dashed lines within the split violins represent the quartiles (from bottom to top, 25th, 50th, and 75th percentiles, respectively). Overlaid to the split violins are dot-plots with horizontal jitter (for better visibility) representing the data sample underlying the split violins. The horizontal bars above and below the split violins indicate the performed significance tests with matching q-value (FDR-corrected <italic>p</italic>-value). Bold font indicates <italic>q</italic> &lt; 0.05. The evaluation metrics with their corresponding half-violins, dot-plots, and significance bars are color-coded. <bold>(A)</bold> Regression results split across data types: 21 or 441 test-set regressions per split violin (3 users · 1 features · 7 models or 3 users · 21 feature · 7 models). Significance (unpaired: Mann–Whitney-U, paired: sign-test) FDR-corrected for 6 tests. Separate corrections for each metric. <bold>(B)</bold> Regression results split across users: 308 test-set regressions (44 features · 7 models) per split violin. Significance (sign-test) FDR-corrected for 3 tests. Separate corrections for each metric.</p>
        </caption>
        <graphic xlink:href="fnbot-13-00076-g0009"/>
      </fig>
    </sec>
    <sec>
      <title>4.9. Across-User Regression</title>
      <p>We further applied the regressors trained on the robot data across users, to test for user-specific differences the regressors might have learned. The results are shown in <xref ref-type="fig" rid="F10">Figure 10</xref>. We now only consider Pearson's ρ as our performance metric.</p>
      <fig id="F10" position="float">
        <label>Figure 10</label>
        <caption>
          <p>Across-user transfer matrices. Regression across users trained on the position of the robotic hand. As test-set, the last 3 min of the experiment was used for each user. The transfer matrix displays the user on which the regressor was trained (y-axis) and the user on which it was evaluated (x-axis). The evaluation metric (Pearson's ρ) is displayed color-coded and in text within the respective field.</p>
        </caption>
        <graphic xlink:href="fnbot-13-00076-g0010"/>
      </fig>
      <p>A pattern common to all regressors was that models trained on a user with less experience performed similarly or better when tested to a user with more experience, relative to within-user. For example, training on S2 and testing on S3 resulted in higher correlations in 6 out of 7 regressors, relative to testing on S2. The 7th regressor (EEGNet v4) performed similarly (0.007 difference) when trained on S2 and tested on both S2 and S3. Training on a user with more experience and testing on a user with less experience, reversed the pattern. For example, training on S3 and testing on S1 and S2 always resulted in lower correlations than when testing on S3. Increases and decreases in performance scaled with the experience in 36 out of 42 transfers.</p>
      <p>Interestingly, performance gains relative to within-user regression could be achieved when transferring between S2 and S3 for some regressors. This was not the case for all regressors when transferring from S2 and S3 to S1. That is, training on S2 and testing on S3 resulted in better performance than within-user regression in S3 for both linear regressors. Similarly, training on S3 and testing on S2 resulted in better performance than within-user regression in S2 in 4 out of 7 regressors. Furthermore, the best performance in S2 was achieved by transferring from S3 using a CNN regressor (Deep4Net). For S1 and S3, best performance was achieved within-user (EEGRestNet-29 and EEGNet v4, respectively).</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s5">
    <title>5. Discussion</title>
    <p>The main contribution of the present study is to describe and evaluate a novel method for continuous rating of subjective user perception during direct human-robot interaction. Our rating approach is continuous in two respects: first, ratings are conveyed on a continuous scale, secondly, these ratings are acquired continuously over the whole time period during which users interact with the robotic system. This rating system was designed to minimize movement-related confounding effects (see section 5.4), and as we have demonstrated, allows to generate real-time user feedback during a grasp-and-deliver task of a robotic arm by means of a wireless controller. We have used our rating system to let users evaluate the general behavioral quality of the robot the users were interacting with, ranging from “positive” to “negative.” However, the rating system is amendable to any other continuous variable, and would thus also allow rating of factors like subjective valence and arousal, or trustworthiness of a robot, as in Sarkar et al. (<xref rid="B37" ref-type="bibr">2017</xref>).</p>
    <sec>
      <title>5.1. Subjective Ratings</title>
      <p>In the present study, we chose a scenario inspired by emerging brain-computer interfacing (BCI) applications. While our main focus was on demonstrating the basic feasibility and usefulness of our continuous rating system, the data that we obtained during the evaluation process in three users with different levels of previous direct experience with robotic systems lead to several preliminary observations that provide potentially useful starting points for further large scale investigations. As described in 3 in more detail, the three users showed distinct rating patterns with, for example, temporally more stable rating behavior in the users with more extensive previous robot exposure (<xref ref-type="fig" rid="F6">Figure 6</xref>). Of course, based on the current data, we cannot decide whether it was experience <italic>per se</italic> or possibly pre-existing personality traits such as anxiety, or a more or less positive basic attitude and trust in technology in general, and robots in particular, that had a modulating influence on the inter-individual differences that we have observed. This line of interpretation would also fit to the results demonstrated by Sarkar et al. (<xref rid="B37" ref-type="bibr">2017</xref>). There, users with more experience rated the robots as less intelligent and less safe than their counterparts with less experience. Such and related questions could be addressed using experimental procedures with fine-grained rating systems as we have described here.</p>
    </sec>
    <sec>
      <title>5.2. Regression of Subjective Ratings</title>
      <p>While explicit user ratings can be useful, in many situations it would be inconvenient or even impossible to obtain such ratings. For example, in a BCI scenario, as investigated here, where paralyzed patients may entirely loose the ability to convey motor responses. Implicit (based on physiological measurements from the users) or objective/contextual sources of information (based on the robot behavior or environmental factors) could help in such situations. We thus tested whether various neuro- and peripheral recordings (EEG, ECG, respiration) as well as kinematic properties of the robot actions contained information about the subjective ratings. As we did not observe strong linear correlations, we applied diverse regression methods to predict subjective ratings. At least based on the data available here, robot hand position was the best predictor for the subjective ratings, reaching correlation coefficients up to 0.869.</p>
      <p>Robot hand position based performance of the different regressors revealed a consistent ranking of the users. According to correlation coefficient, user rank scaled with experience. This could be interpreted as a sign that more experienced users produce subjective ratings which are more consistent over time and thus provide a better generalization form training to testing. The aspect of rating consistency was also discussed in 5.1. There, we noted that due to the small sample size of our pilot cohort other personal traits influencing rating consistency can not be ruled out. Additionally, the amount of training data also scaled with experience (cf. section 2.2), which could have biased the regression performance. Finally, according to rmse, user rank no longer scaled with experience, further supporting our caution.</p>
      <p>Dry EEG and peripheral physiological recordings were not very useful for the regression analysis. Only weak correlation values between predicted and actual rating up to 0.320 could be achieved in the final evaluation sets. A combination of different data modalities also did not improve the prediction beyond that obtained with the robot position as the only input. Dry EEG as used here has a generally lower signal-to-noise ratio than conventional EEG (Mathewson et al., <xref rid="B25" ref-type="bibr">2017</xref>), and the experiments were conducted in a high-noise setting. Thus, in future, improved dry EEG recording techniques (Fiedler et al., <xref rid="B10" ref-type="bibr">2015</xref>) or gel-filled EEG electrodes applied in recording conditions optimized for high EEG signal quality (Völker et al., <xref rid="B46" ref-type="bibr">2018a</xref>) could help to evaluate the full potential of EEG in the present context. Further peripheral physiological signals such as electrodermal activity (EDA), as an index of arousal and stress (Fowles, <xref rid="B11" ref-type="bibr">1980</xref>), could also be tested in future studies.</p>
    </sec>
    <sec>
      <title>5.3. EEG Signals Related to Subjective Ratings</title>
      <p>In a preliminary attempt to identify EEG signals related to the subjective ratings, we investigated the selection of EEG electrodes and frequency bands. As reported in section 4.8, it appeared like using all electrodes was marginally but partially statistically significantly best when considering the shape of the regression and using *z electrodes was marginally but non-significantly best when considering the values of the regression (cf. <xref ref-type="fig" rid="F8">Figure 8C</xref>). The latter would be in line with reports form the literature highlighting the importance of *z electrodes for detecting error-related brain activity (Cavanagh et al., <xref rid="B5" ref-type="bibr">2010</xref>; Cavanagh and Frank, <xref rid="B4" ref-type="bibr">2014</xref>; Spüler and Niethammer, <xref rid="B43" ref-type="bibr">2015</xref>; Völker et al., <xref rid="B46" ref-type="bibr">2018a</xref>). Unfortunately, we could not find further evidence relating our results to the literature as we could not show that the delta and theta bands play a dominant role in the encoding of error-related brain activity in our paradigm. Rather, frequencies in the alpha and beta bands seemed to be more informative, albeit only partially significantly (cf. <xref ref-type="fig" rid="F8">Figure 8D</xref>). At the moment it is still unclear whether this trend is related to error signals or a motor bias.</p>
    </sec>
    <sec>
      <title>5.4. Motor Bias</title>
      <p>In order to be compatible with EEG, we designed the experimental paradigm in such a way that each evaluation value corresponds to an equal tonic motor output. However, directional motor signals (Waldert et al., <xref rid="B49" ref-type="bibr">2009</xref>) could potentially still confound the interpretation of EEG signals correlated with subjective ratings. No evidence of a motor bias was found using the electrode selection as the *C* performed consistently worst. However, one would have expected to observe improved results when using the alpha and beta bands relative to the delta and theta bands, with a dominance of the alpha band. Indeed, alpha and beta bands were the best bands when considering non-significant RMSE results but with a dominance of the mid-beta band. In the partially significant Pearson's ρ results, the low-beta band dominated, which is not entirely typical of a motor bias, especially relative to the alpha band (Pfurtscheller and Lopes da Silva, <xref rid="B32" ref-type="bibr">1999</xref>). At this point, a motor bias cannot be entirely ruled out based on the frequency-band selection. More experiments and data analysis, for example by contrasting the EEG data during extremely negative with extremely positive subjective ratings, will be needed to resolve this question. Future studies should explicitly control for motor bias, e.g., by switching the vertical or horizontal axis of the rating system, either between measurement sessions or between users.</p>
    </sec>
    <sec>
      <title>5.5. Across-User Regression of Subjective Ratings</title>
      <p>To investigate whether the models have learned user-specific characteristics, we have applied the models trained on the training set of one user to the test sets of all users. In section 4.9 we have described the general patterns underlying our initial across-user regression results. Based on our 3 pilot users, we found that the user's prior experience with robots might also play a role when transferring models between users. Models trained on users with less experience mostly increased their performance when transferred to users with more experience (in 15 out of 21 transfers) and <italic>vice versa</italic> (in 21 out of 21 transfers). The more experience the transferred to user had the better the correlation coefficient and <italic>vice versa</italic> (in 36 out of 42 transfers). As the best across-user transfers always performed better on the tested user but only rarely surpassed the within-user test on the training user, our findings hint at the possibility that regressors have learned both user-specific and general characteristics. This seems to make it possible to generalize across users in decoding of subjective perception from kinematic data of the robot actions. At the same time, the results indicate that a good set of training data from which the CNN is able to generalize well might be more important than the user-specific differences of the users.</p>
      <p>In addition to our small sample size and differences in the amount of training data, it is here also important to consider that, as discussed in section 5.2, within-user performance (Pearson's ρ) appeared to scale with experience, which might be a confounding factor here. If the reported patterns still hold for larger samples, disentangling the factors, if possible, will be necessary to differentiate their contributions to the reported across user transfer effect. It will also be interesting to investigate whether the across-user performance can be improved beyond the within-user performance by combining the data of additional users in the across-user training. Including the training data of the tested user into a hybrid within/across-user training could also further improve regression performance. Although such data would not be directly available in an online BCI scenario an initial across-user model could be fine-tuned as more and more labeled data of a new user become available during an online experiment as e.g., we have done for online-adaptive classification using CNNs in Kuhner et al. (<xref rid="B19" ref-type="bibr">2019</xref>).</p>
    </sec>
  </sec>
  <sec id="s6">
    <title>6. Outlook</title>
    <p>The connection of subjective continuous ratings in real human-robot interaction with EEG allows the search for neurophysiological correlates of these ratings, which could then be used as features for automated behavior adaptation algorithms during robot interaction with paralyzed patients. Even if such a search would fail, the availability of continuous ratings would make it possible to generate fine-grained robotic behavior policies, which in turn could be used to improve robot behavior. In the future, the availability of continuous rating data may be useful both in <italic>post-hoc</italic> and real-time application scenarios. For example, <italic>post-hoc</italic> analysis of continuous robot-related user rating would allow to study how different aspects of robot behavior may shape user perception, and how user perception evolves over time. Real-time analysis of ratings could convey important teaching signals for real-time adaptation and personalization of robot behavior, for example for users with different levels of previous exposure to robots. Thus in the future, real-time ratings combined with reinforcement-learning methods, e.g., Deep Q-Networks (DQNs, Mnih et al., <xref rid="B27" ref-type="bibr">2015</xref>) or Deep Deterministic Policy Gradient algorithms (DDPG, Lillicrap et al., <xref rid="B21" ref-type="bibr">2015</xref>), could enable robot systems that keep optimizing their behavior in a human-compliant manner.</p>
  </sec>
  <sec id="s7">
    <title>Ethics Statement</title>
    <p>This study was carried out in with written informed consent from all subjects. All subjects gave written informed consent in accordance with the Declaration of Helsinki. The protocol was approved by the Ethics Committee, University of Freiburg.</p>
  </sec>
  <sec id="s8">
    <title>Author Contributions</title>
    <p>TB, JB, WB, MV, and LF contributed conception and design of the study. RS provided code and advice. MV and LF wrote code for the experiments and analyses. MV performed the EEG recordings and simulations. TB, MV, and LF wrote sections of the manuscript.</p>
    <sec>
      <title>Conflict of Interest</title>
      <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
    </sec>
  </sec>
</body>
<back>
  <ack>
    <p>The authors thank Jeremias Holub and Ramin Zohouri for their support in the robot trajectory control.</p>
  </ack>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> This work was supported by DFG grant EXC1086 BrainLinks-BrainTools, Baden-Württemberg Stiftung grant BMI-Bot, Graduate School of Robotics in Freiburg, Germany and the State Graduate Funding Program of Baden-Württemberg, Germany.</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bashivan</surname><given-names>P.</given-names></name><name><surname>Rish</surname><given-names>I.</given-names></name><name><surname>Yeasin</surname><given-names>M.</given-names></name><name><surname>Codella</surname><given-names>N.</given-names></name></person-group> (<year>2016</year>). <article-title>Learning representations from EEG with deep recurrent-convolutional neural networks</article-title>. <source>ArXiv e-prints</source>.</mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y.</given-names></name><name><surname>Yekutieli</surname><given-names>D.</given-names></name></person-group> (<year>2001</year>). <article-title>The control of the false discovery rate in multiple testing under dependency</article-title>. <source>Ann. Stat.</source>
<volume>29</volume>, <fpage>1165</fpage>–<lpage>1188</lpage>. <pub-id pub-id-type="doi">10.1214/aos/1013699998</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Burget</surname><given-names>F.</given-names></name><name><surname>Fiederer</surname><given-names>L. D. J.</given-names></name><name><surname>Kuhner</surname><given-names>D.</given-names></name><name><surname>Völker</surname><given-names>M.</given-names></name><name><surname>Aldinger</surname><given-names>J.</given-names></name><name><surname>Schirrmeister</surname><given-names>R. T.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Acting thoughts: towards a mobile robotic service assistant for users with limited communication skills</article-title>, in <source>2017 European Conference on Mobile Robots (ECMR)</source> (<publisher-loc>Paris</publisher-loc>), <fpage>1</fpage>–<lpage>6</lpage>.</mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname><given-names>J. F.</given-names></name><name><surname>Frank</surname><given-names>M. J.</given-names></name></person-group> (<year>2014</year>). <article-title>Frontal theta as a mechanism for cognitive control</article-title>. <source>Trends Cogn. Sci.</source>
<volume>18</volume>, <fpage>414</fpage>–<lpage>421</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2014.04.012</pub-id><?supplied-pmid 24835663?><pub-id pub-id-type="pmid">24835663</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname><given-names>J. F.</given-names></name><name><surname>Frank</surname><given-names>M. J.</given-names></name><name><surname>Klein</surname><given-names>T. J.</given-names></name><name><surname>Allen</surname><given-names>J. J. B.</given-names></name></person-group> (<year>2010</year>). <article-title>Frontal theta links prediction errors to behavioral adaptation in reinforcement learning</article-title>. <source>NeuroImage</source>
<volume>49</volume>, <fpage>3198</fpage>–<lpage>3209</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.11.080</pub-id><?supplied-pmid 19969093?><pub-id pub-id-type="pmid">19969093</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chitta</surname><given-names>S.</given-names></name><name><surname>Sucan</surname><given-names>I.</given-names></name><name><surname>Cousins</surname><given-names>S.</given-names></name></person-group> (<year>2012</year>). <article-title>Moveit![ros topics]</article-title>. <source>IEEE Robot. Autom. Mag.</source>
<volume>19</volume>, <fpage>18</fpage>–<lpage>19</lpage>. <pub-id pub-id-type="doi">10.1109/MRA.2011.2181749</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ehrlich</surname><given-names>S. K.</given-names></name><name><surname>Cheng</surname><given-names>G.</given-names></name></person-group> (<year>2018</year>). <article-title>A feasibility study for validating robot actions using EEG-based error-related potentials</article-title>. <source>Int. J. Soc. Robot.</source>
<volume>11</volume>, <fpage>1</fpage>–<lpage>13</lpage>. <pub-id pub-id-type="doi">10.1007/s12369-018-0501-8</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Eitel</surname><given-names>A.</given-names></name><name><surname>Springenberg</surname><given-names>J. T.</given-names></name><name><surname>Spinello</surname><given-names>L.</given-names></name><name><surname>Riedmiller</surname><given-names>M.</given-names></name><name><surname>Burgard</surname><given-names>W.</given-names></name></person-group> (<year>2015</year>). <article-title>Multimodal deep learning for robust rgb-d object recognition</article-title>, in <source>2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source> (<publisher-loc>Hamburg</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>681</fpage>–<lpage>687</lpage>.</mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feil-Seifer</surname><given-names>D.</given-names></name><name><surname>Skinner</surname><given-names>K.</given-names></name><name><surname>Matarić</surname><given-names>M. J.</given-names></name></person-group> (<year>2007</year>). <article-title>Benchmarks for evaluating socially assistive robotics</article-title>. <source>Interact. Stud.</source>
<volume>8</volume>, <fpage>423</fpage>–<lpage>439</lpage>. <pub-id pub-id-type="doi">10.1075/is.8.3.07fei</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiedler</surname><given-names>P.</given-names></name><name><surname>Griebel</surname><given-names>S.</given-names></name><name><surname>Pedrosa</surname><given-names>P.</given-names></name><name><surname>Fonseca</surname><given-names>C.</given-names></name><name><surname>Vaz</surname><given-names>F.</given-names></name><name><surname>Zentner</surname><given-names>L.</given-names></name><etal/></person-group> (<year>2015</year>). <article-title>Multichannel eeg with novel ti/tin dry electrodes</article-title>. <source>Sensors Actuat. A Phys.</source>
<volume>221</volume>, <fpage>139</fpage>–<lpage>147</lpage>. <pub-id pub-id-type="doi">10.1016/j.sna.2014.10.010</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fowles</surname><given-names>D. C.</given-names></name></person-group> (<year>1980</year>). <article-title>The three arousal model: implications of gray's two-factor learning theory for heart rate, electrodermal activity, and psychopathy</article-title>. <source>Psychophysiology</source>
<volume>17</volume>, <fpage>87</fpage>–<lpage>104</lpage>. <pub-id pub-id-type="doi">10.1111/j.1469-8986.1980.tb00117.x</pub-id><?supplied-pmid 6103567?><pub-id pub-id-type="pmid">6103567</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Glorot</surname><given-names>X.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name></person-group> (<year>2010</year>). <article-title>Understanding the difficulty of training deep feedforward neural networks</article-title>, in <source>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, Vol. 9 of <italic>Proceedings of Machine Learning Research</italic></source>, eds <person-group person-group-type="editor"><name><surname>Teh</surname><given-names>Y. W.</given-names></name><name><surname>Titterington</surname><given-names>M.</given-names></name></person-group> (Sardinia), <fpage>249</fpage>–<lpage>256</lpage>.</mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Held</surname><given-names>D.</given-names></name><name><surname>Thrun</surname><given-names>S.</given-names></name><name><surname>Savarese</surname><given-names>S.</given-names></name></person-group> (<year>2016</year>). <article-title>Learning to track at 100 fps with deep regression networks</article-title>, in <source>European Conference on Computer Vision</source> (<publisher-loc>Amsterdam</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>749</fpage>–<lpage>765</lpage>.</mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>C.-M.</given-names></name><name><surname>Mutlu</surname><given-names>B.</given-names></name></person-group> (<year>2012</year>). <article-title>Robot behavior toolkit: generating effective social behaviors for robots</article-title>, in <source>2012 7th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</source> (<publisher-loc>Boston, MA</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>25</fpage>–<lpage>32</lpage>.</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Iturrate</surname><given-names>I.</given-names></name><name><surname>Montesano</surname><given-names>L.</given-names></name><name><surname>Minguez</surname><given-names>J.</given-names></name></person-group> (<year>2013</year>). <article-title>Shared-control brain-computer interface for a two dimensional reaching task using EEG error-related potentials</article-title>, in <source>2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</source> (<publisher-loc>Osaka</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>5258</fpage>–<lpage>5262</lpage>. <?supplied-pmid 24110922?></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>E.</given-names></name><name><surname>Oliphant</surname><given-names>T.</given-names></name><name><surname>Peterson</surname><given-names>P.</given-names></name></person-group> (<year>2001</year>). <source>SciPy: Open Source scientific Tools for Python.</source>
<publisher-loc>Vienna</publisher-loc>
<pub-id pub-id-type="doi">10.1145/3029798.3034826</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kolkhorst</surname><given-names>H.</given-names></name><name><surname>Tangermann</surname><given-names>M.</given-names></name><name><surname>Burgard</surname><given-names>W.</given-names></name></person-group> (<year>2017</year>). <article-title>Decoding perceived hazardousness from user's brain states to shape human-robot interaction</article-title>, in <source>Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction</source> (<publisher-loc>Vienna</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>349</fpage>–<lpage>350</lpage>.</mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kolkhorst</surname><given-names>H.</given-names></name><name><surname>Tangermann</surname><given-names>M.</given-names></name><name><surname>Burgard</surname><given-names>W.</given-names></name></person-group> (<year>2018</year>). <article-title>Guess what i attend: interface-free object selection using brain signals</article-title>, in <source>2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source> (<publisher-loc>Madrid</publisher-loc>: <publisher-name>IEEE</publisher-name>).</mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhner</surname><given-names>D.</given-names></name><name><surname>Fiederer</surname><given-names>L. D. J.</given-names></name><name><surname>Aldinger</surname><given-names>J.</given-names></name><name><surname>Burget</surname><given-names>F.</given-names></name><name><surname>Völker</surname><given-names>M.</given-names></name><name><surname>Schirrmeister</surname><given-names>R. T.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>A service assistant combining autonomous robotics, flexible goal formulation, and deep-learning-based brain-computer interfacing</article-title>. <source>Robot. Auton. Syst</source>. <volume>116</volume>, <fpage>98</fpage>–<lpage>113</lpage>. <pub-id pub-id-type="doi">10.1016/j.robot.2019.02.015</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawhern</surname><given-names>V.</given-names></name><name><surname>Solon</surname><given-names>A.</given-names></name><name><surname>Waytowich</surname><given-names>N.</given-names></name><name><surname>Gordon</surname><given-names>S. M.</given-names></name><name><surname>Hung</surname><given-names>C.</given-names></name><name><surname>Lance</surname><given-names>B. J.</given-names></name></person-group> (<year>2018</year>). <article-title>EEGnet: a compact convolutional neural network for EEG-based brain–computer interfaces</article-title>. <source>J. Neural Eng</source>. <volume>15</volume>:<fpage>056013</fpage>. <pub-id pub-id-type="doi">10.1088/1741-2552/aace8c</pub-id><?supplied-pmid 29932424?><pub-id pub-id-type="pmid">29932424</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname><given-names>T. P.</given-names></name><name><surname>Hunt</surname><given-names>J. J.</given-names></name><name><surname>Pritzel</surname><given-names>A.</given-names></name><name><surname>Heess</surname><given-names>N.</given-names></name><name><surname>Erez</surname><given-names>T.</given-names></name><name><surname>Tassa</surname><given-names>Y.</given-names></name><etal/></person-group> (<year>2015</year>). <article-title>Continuous control with deep reinforcement learning</article-title>. <source>arXiv [Preprint]. arXiv:1509.02971</source>.</mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loshchilov</surname><given-names>I.</given-names></name><name><surname>Hutter</surname><given-names>F.</given-names></name></person-group> (<year>2016</year>). <article-title>SG DR: stochastic gradient descent with warm restarts</article-title>. <source>arXiv:1608.03983</source>.</mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Loshchilov</surname><given-names>I.</given-names></name><name><surname>Hutter</surname><given-names>F.</given-names></name></person-group> (<year>2017</year>). <article-title>Fixing weight decay regularization in Adam</article-title>. <source>arXiv:1711.05101</source>.</mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manor</surname><given-names>R.</given-names></name><name><surname>Geva</surname><given-names>A. B.</given-names></name></person-group> (<year>2015</year>). <article-title>Convolutional neural network for multi-category rapid serial visual presentation BCI</article-title>. <source>Front. Comput. Neurosci.</source>
<volume>9</volume>:<fpage>146</fpage>. <pub-id pub-id-type="doi">10.3389/fncom.2015.00146</pub-id><?supplied-pmid 26696875?><pub-id pub-id-type="pmid">26696875</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathewson</surname><given-names>K. E.</given-names></name><name><surname>Harrison</surname><given-names>T. J.</given-names></name><name><surname>Kizuk</surname><given-names>S. A.</given-names></name></person-group> (<year>2017</year>). <article-title>High and dry? Comparing active dry eeg electrodes to active and passive wet electrodes</article-title>. <source>Psychophysiology</source>
<volume>54</volume>, <fpage>74</fpage>–<lpage>82</lpage>. <pub-id pub-id-type="doi">10.1111/psyp.12536</pub-id><?supplied-pmid 28000254?><pub-id pub-id-type="pmid">28000254</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miao</surname><given-names>S.</given-names></name><name><surname>Wang</surname><given-names>Z. J.</given-names></name><name><surname>Liao</surname><given-names>R.</given-names></name></person-group> (<year>2016</year>). <article-title>A cnn regression approach for real-time 2D/3D registration</article-title>. <source>IEEE Trans. Med. Imaging</source>
<volume>35</volume>, <fpage>1352</fpage>–<lpage>1363</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2016.2521800</pub-id><pub-id pub-id-type="pmid">26829785</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mnih</surname><given-names>V.</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K.</given-names></name><name><surname>Silver</surname><given-names>D.</given-names></name><name><surname>Rusu</surname><given-names>A. A.</given-names></name><name><surname>Veness</surname><given-names>J.</given-names></name><name><surname>Bellemare</surname><given-names>M. G.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>Human-level control through deep reinforcement learning</article-title>. <source>Nature</source><volume>518</volume>:<fpage>529</fpage>. <pub-id pub-id-type="doi">10.1038/nature14236</pub-id><?supplied-pmid 25719670?><pub-id pub-id-type="pmid">25719670</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Oliveira</surname><given-names>G. L.</given-names></name><name><surname>Valada</surname><given-names>A.</given-names></name><name><surname>Bollen</surname><given-names>C.</given-names></name><name><surname>Burgard</surname><given-names>W.</given-names></name><name><surname>Brox</surname><given-names>T.</given-names></name></person-group> (<year>2016</year>). <article-title>Deep learning for human part discovery in images</article-title>, in <source>2016 IEEE International Conference on Robotics and Automation (ICRA)</source> (<publisher-loc>Stockholm</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1634</fpage>–<lpage>1641</lpage>.</mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A.</given-names></name><name><surname>Gross</surname><given-names>S.</given-names></name><name><surname>Chintala</surname><given-names>S.</given-names></name><name><surname>Chanan</surname><given-names>G.</given-names></name><name><surname>Yang</surname><given-names>E.</given-names></name><name><surname>DeVito</surname><given-names>Z.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Automatic differentiation in PyTorch</article-title>, in <source>NIPS Autodiff Workshop</source>. Long Beach.</mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F.</given-names></name><name><surname>Varoquaux</surname><given-names>G.</given-names></name><name><surname>Gramfort</surname><given-names>A.</given-names></name><name><surname>Michel</surname><given-names>V.</given-names></name><name><surname>Thirion</surname><given-names>B.</given-names></name><name><surname>Grisel</surname><given-names>O.</given-names></name><etal/></person-group> (<year>2011</year>). <article-title>Scikit-learn: machine learning in Python</article-title>. <source>J. Mach. Learn. Res.</source>
<volume>12</volume>, <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfurtscheller</surname><given-names>G.</given-names></name><name><surname>Allison</surname><given-names>B. Z.</given-names></name><name><surname>Bauernfeind</surname><given-names>G.</given-names></name><name><surname>Brunner</surname><given-names>C.</given-names></name><name><surname>Solis Escalante</surname><given-names>T.</given-names></name><name><surname>Scherer</surname><given-names>R.</given-names></name><etal/></person-group>. (<year>2010</year>). <article-title>The hybrid BCI</article-title>. <source>Front. Neurosci.</source><volume>4</volume>:<fpage>3</fpage>. <pub-id pub-id-type="doi">10.3389/fnpro.2010.00003</pub-id><?supplied-pmid 20582271?><pub-id pub-id-type="pmid">20582257</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfurtscheller</surname><given-names>G.</given-names></name><name><surname>Lopes da Silva</surname><given-names>F. H.</given-names></name></person-group> (<year>1999</year>). <article-title>Event-related EEG/MEG synchronization and desynchronization: basic principles</article-title>. <source>Clin. Neurophysiol.</source>
<volume>110</volume>, <fpage>1842</fpage>–<lpage>1857</lpage>. <pub-id pub-id-type="doi">10.1016/S1388-2457(99)00141-8</pub-id><?supplied-pmid 10576479?><pub-id pub-id-type="pmid">10576479</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Quigley</surname><given-names>M.</given-names></name><name><surname>Conley</surname><given-names>K.</given-names></name><name><surname>Gerkey</surname><given-names>B.</given-names></name><name><surname>Faust</surname><given-names>J.</given-names></name><name><surname>Foote</surname><given-names>T.</given-names></name><name><surname>Leibs</surname><given-names>J.</given-names></name><etal/></person-group> (<year>2009</year>). <article-title>Ros: an open-source robot operating system</article-title>, in <source>ICRA Workshop on Open Source Software, Vol. 3</source> (<publisher-loc>Kobe</publisher-loc>).</mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rohmer</surname><given-names>E.</given-names></name><name><surname>Singh</surname><given-names>S. P.</given-names></name><name><surname>Freese</surname><given-names>M.</given-names></name></person-group> (<year>2013</year>). <article-title>V-rep: A versatile and scalable robot simulation framework</article-title>, in <source>2013 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source> (<publisher-loc>Tokyo</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1321</fpage>–<lpage>1326</lpage>.</mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Salazar-Gomez</surname><given-names>A. F.</given-names></name><name><surname>DelPreto</surname><given-names>J.</given-names></name><name><surname>Gil</surname><given-names>S.</given-names></name><name><surname>Guenther</surname><given-names>F. H.</given-names></name><name><surname>Rus</surname><given-names>D.</given-names></name></person-group> (<year>2017</year>). <article-title>Correcting robot mistakes in real time using EEG signals</article-title>, in <source>2017 IEEE International Conference on Robotics and Automation (ICRA)</source> (<publisher-loc>Singapore</publisher-loc>), <fpage>6570</fpage>–<lpage>6577</lpage>.</mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarikaya</surname><given-names>D.</given-names></name><name><surname>Corso</surname><given-names>J. J.</given-names></name><name><surname>Guru</surname><given-names>K. A.</given-names></name></person-group> (<year>2017</year>). <article-title>Detection and localization of robotic tools in robot-assisted surgery videos using deep neural networks for region proposal and detection</article-title>. <source>IEEE Trans. Med. Imaging</source>
<volume>36</volume>, <fpage>1542</fpage>–<lpage>1549</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2017.2665671</pub-id><?supplied-pmid 28186883?><pub-id pub-id-type="pmid">28186883</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarkar</surname><given-names>S.</given-names></name><name><surname>Araiza-Illan</surname><given-names>D.</given-names></name><name><surname>Eder</surname><given-names>K.</given-names></name></person-group> (<year>2017</year>). <article-title>Effects of faults, experience, and personality on trust in a robot co-worker</article-title>. <source>arXiv [Preprint]. arXiv:1703.02335</source>.</mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schalk</surname><given-names>G.</given-names></name><name><surname>McFarl</surname><given-names>D. J.</given-names></name><name><surname>Hinterberger</surname><given-names>T.</given-names></name><name><surname>Birbaumer</surname><given-names>N.</given-names></name><name><surname>Wolpaw</surname><given-names>J. R.</given-names></name></person-group> (<year>2004</year>). <article-title>BCI2000: a general-purpose brain-computer interface (BCI) system</article-title>. <source>IEEE Trans. Biomed. Eng.</source>
<volume>51</volume>:<fpage>2004</fpage>. <pub-id pub-id-type="doi">10.1109/TBME.2004.827072</pub-id><?supplied-pmid 15188875?><pub-id pub-id-type="pmid">15188875</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schirrmeister</surname><given-names>R. T.</given-names></name><name><surname>Springenberg</surname><given-names>J. T.</given-names></name><name><surname>Fiederer</surname><given-names>L. D. J.</given-names></name><name><surname>Glasstetter</surname><given-names>M.</given-names></name><name><surname>Eggensperger</surname><given-names>K.</given-names></name><name><surname>Tangermann</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Deep learning with convolutional neural networks for EEG decoding and visualization</article-title>. <source>Hum. Brain Mapp.</source><volume>38</volume>, <fpage>5391</fpage>–<lpage>5420</lpage><pub-id pub-id-type="doi">10.1002/hbm.23730</pub-id><?supplied-pmid 28782865?><pub-id pub-id-type="pmid">28782865</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seabold</surname><given-names>S.</given-names></name><name><surname>Perktold</surname><given-names>J.</given-names></name></person-group> (<year>2010</year>). <article-title>Statsmodels: econometric and statistical modeling with python</article-title>, in <source>9th Python in Science Conference</source>. Austin.</mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sekmen</surname><given-names>A.</given-names></name><name><surname>Challa</surname><given-names>P.</given-names></name></person-group> (<year>2013</year>). <article-title>Assessment of adaptive human–robot interactions</article-title>. <source>Knowl. Based Syst.</source>
<volume>42</volume>, <fpage>49</fpage>–<lpage>59</lpage>. Seabold and Perktold, 2010</mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>B.</given-names></name><name><surname>Bai</surname><given-names>X.</given-names></name><name><surname>Liu</surname><given-names>W.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>Face alignment with deep regression</article-title>. <source>IEEE Trans. Neural Netw. Learn. Syst</source>. <volume>28</volume>, <fpage>183</fpage>–<lpage>194</lpage>. <pub-id pub-id-type="doi">10.1109/TNNLS.2016.2618340</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spüler</surname><given-names>M.</given-names></name><name><surname>Niethammer</surname><given-names>C.</given-names></name></person-group> (<year>2015</year>). <article-title>Error-related potentials during continuous feedback: using EEG to detect errors of different type and severity</article-title>. <source>Front. Hum. Neurosci.</source>
<volume>9</volume>:<fpage>155</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2015.00155</pub-id><?supplied-pmid 25859204?><pub-id pub-id-type="pmid">25859204</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tapus</surname><given-names>A.</given-names></name><name><surname>Ţăpuş</surname><given-names>C.</given-names></name><name><surname>Matarić</surname><given-names>M. J.</given-names></name></person-group> (<year>2008</year>). <article-title>User? Robot personality matching and assistive robot behavior adaptation for post-stroke rehabilitation therapy</article-title>. <source>Intell. Service Robot.</source>
<volume>1</volume>:<fpage>169</fpage>
<pub-id pub-id-type="doi">10.1007/s11370-008-0017-4</pub-id></mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viereck</surname><given-names>U.</given-names></name><name><surname>Pas</surname><given-names>A. t.</given-names></name><name><surname>Saenko</surname><given-names>K.</given-names></name><name><surname>Platt</surname><given-names>R.</given-names></name></person-group> (<year>2017</year>). <article-title>Learning a visuomotor controller for real world robotic grasping using simulated depth images</article-title>. <source>arXiv preprint arXiv:1706.04652</source>.</mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Völker</surname><given-names>M.</given-names></name><name><surname>Fiederer</surname><given-names>L. D. J.</given-names></name><name><surname>Berberich</surname><given-names>S.</given-names></name><name><surname>Hammer</surname><given-names>J.</given-names></name><name><surname>Behncke</surname><given-names>J.</given-names></name><name><surname>Kršek</surname><given-names>P.</given-names></name><etal/></person-group>. (<year>2018a</year>). <article-title>The dynamics of error processing in the human brain as reflected by high-gamma activity in noninvasive and intracranial EEG</article-title>. <source>NeuroImage</source><volume>173</volume>, <fpage>564</fpage>–<lpage>579</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.01.059</pub-id><?supplied-pmid 29471099?><pub-id pub-id-type="pmid">29471099</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Völker</surname><given-names>M.</given-names></name><name><surname>Hammer</surname><given-names>J.</given-names></name><name><surname>Schirrmeister</surname><given-names>R. T.</given-names></name><name><surname>Behncke</surname><given-names>J.</given-names></name><name><surname>Fiederer</surname><given-names>L. D. J.</given-names></name><name><surname>Schulze-Bonhage</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2018b</year>). <article-title>Intracranial error detection via deep learning</article-title>, in <source>IEEE International Conference on Systems, Man, and Cybernetics (SMC)</source> (<publisher-loc>Miyazaki</publisher-loc>).</mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Völker</surname><given-names>M.</given-names></name><name><surname>Schirrmeister</surname><given-names>R. T.</given-names></name><name><surname>Fiederer</surname><given-names>L. D. J.</given-names></name><name><surname>Burgard</surname><given-names>W.</given-names></name><name><surname>Ball</surname><given-names>T.</given-names></name></person-group> (<year>2018c</year>). <article-title>Deep transfer learning for error decoding from non-invasive EEG</article-title>, in <source>IEEE 6th International Conference on Brain-Computer Interface (BCI)</source> (<publisher-loc>High1-gil</publisher-loc>).</mixed-citation>
    </ref>
    <ref id="B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waldert</surname><given-names>S.</given-names></name><name><surname>Pistohl</surname><given-names>T.</given-names></name><name><surname>Braun</surname><given-names>C.</given-names></name><name><surname>Ball</surname><given-names>T.</given-names></name><name><surname>Aertsen</surname><given-names>A.</given-names></name><name><surname>Mehring</surname><given-names>C.</given-names></name></person-group> (<year>2009</year>). <article-title>A review on directional information in neural signals for brain-machine interfaces</article-title>. <source>J. Physiol.</source>
<volume>103</volume>, <fpage>244</fpage>–<lpage>254</lpage>. <pub-id pub-id-type="doi">10.1016/j.jphysparis.2009.08.007</pub-id><?supplied-pmid 19665554?><pub-id pub-id-type="pmid">19665554</pub-id></mixed-citation>
    </ref>
    <ref id="B50">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Watter</surname><given-names>M.</given-names></name><name><surname>Springenberg</surname><given-names>J.</given-names></name><name><surname>Boedecker</surname><given-names>J.</given-names></name><name><surname>Riedmiller</surname><given-names>M.</given-names></name></person-group> (<year>2015</year>). <article-title>Embed to control: a locally linear latent dynamics model for control from raw images</article-title>, in <source>Advances in Neural Information Processing Systems</source> (<publisher-loc>Montreal</publisher-loc>), <fpage>2746</fpage>–<lpage>2754</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
