<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7320615</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btaa225</article-id>
    <article-id pub-id-type="publisher-id">btaa225</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Databases and Ontologies</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>EVICAN—a balanced dataset for algorithm development in cell and nucleus segmentation</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Schwendy</surname>
          <given-names>Mischa</given-names>
        </name>
        <xref ref-type="corresp" rid="btaa225-cor1"/>
        <xref ref-type="aff" rid="btaa225-aff1">b1</xref>
        <!--<email>schwendymischa@gmail.com</email>-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Unger</surname>
          <given-names>Ronald E</given-names>
        </name>
        <xref ref-type="aff" rid="btaa225-aff2">b2</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-8522-1854</contrib-id>
        <name>
          <surname>Parekh</surname>
          <given-names>Sapun H</given-names>
        </name>
        <xref ref-type="corresp" rid="btaa225-cor1"/>
        <xref ref-type="aff" rid="btaa225-aff1">b1</xref>
        <xref ref-type="aff" rid="btaa225-aff3">b3</xref>
        <!--<email>sparekh@utexas.edu</email>-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Robinson</surname>
          <given-names>Peter</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <aff id="btaa225-aff1"><label>b1</label><institution>Max Planck Institute for Polymer Research</institution>, Mainz 55128, <country country="DE">Germany</country></aff>
    <aff id="btaa225-aff2"><label>b2</label><institution>Institute of Pathology</institution>, Universitätsmedizin-Mainz, Mainz 55131, <country country="DE">Germany</country></aff>
    <aff id="btaa225-aff3"><label>b3</label><institution>Department of Biomedical Engineering</institution>, University of Texas at Austin, Austin, TX 78712, <country country="US">USA</country></aff>
    <author-notes>
      <corresp id="btaa225-cor1">To whom correspondence should be addressed. <email>schwendymischa@gmail.com</email></corresp>
    </author-notes>
    <pub-date pub-type="epub" iso-8601-date="2020-04-01">
      <day>01</day>
      <month>4</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <day>15</day>
      <month>6</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>01</day>
      <month>4</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>36</volume>
    <issue>12</issue>
    <fpage>3863</fpage>
    <lpage>3870</lpage>
    <history>
      <date date-type="received">
        <day>30</day>
        <month>7</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>22</day>
        <month>3</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>27</day>
        <month>3</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license license-type="cc-by" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btaa225.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Deep learning use for quantitative image analysis is exponentially increasing. However, training accurate, widely deployable deep learning algorithms requires a plethora of annotated (ground truth) data. Image collections must contain not only thousands of images to provide sufficient example objects (i.e. cells), but also contain an adequate degree of image heterogeneity.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We present a new dataset, EVICAN—Expert visual cell annotation, comprising partially annotated grayscale images of 30 different cell lines from multiple microscopes, contrast mechanisms and magnifications that is readily usable as training data for computer vision applications. With 4600 images and ∼26 000 segmented cells, our collection offers an unparalleled heterogeneous training dataset for cell biology deep learning application development.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The dataset is freely available (<ext-link ext-link-type="uri" xlink:href="https://edmond.mpdl.mpg.de/imeji/collection/l45s16atmi6Aa4sI?q=">https://edmond.mpdl.mpg.de/imeji/collection/l45s16atmi6Aa4sI?q=</ext-link>). Using a Mask R-CNN implementation, we demonstrate automated segmentation of cells and nuclei from brightfield images with a mean average precision of 61.6 % at a Jaccard Index above 0.5.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Max Planck Graduate Center</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Welch Foundation</institution>
            <institution-id institution-id-type="DOI">10.13039/100000928</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>F-2008-20190330</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Human Frontiers in Science Program</institution>
          </institution-wrap>
        </funding-source>
        <award-id>RGP0045/2018</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>In recent years, microscopy has seen major advancements in both the optical and automation performance. The rise of automation in the industry: both in acquisition and analysis, has turned microscopes into powerful high-content screening systems. Researchers are now in need of rapid and accurate analyses to infer quantitative measures from the ever-increasing amount of imaging data. The two most essential steps in deriving quantitative data from images are segmentation and classification. Segmentation is the process of finding the outlines of objects within an image while classification identifies the object by assigning a class label (e.g. ‘Nucleus’ or ‘Cell’). In cell biology applications of microscopy, cellular and subcellular entities can be segmented and classified to enable single-cell analyses and relate measured quantities, such as cell shape or intensity of a fluorescent molecule within cells, in response to specific treatments.</p>
    <p>Traditionally, image processing in cell biology applications of microscopy have been based on fluorescent staining of cells (<xref rid="btaa225-B30" ref-type="bibr">Schwendy <italic>et al.</italic>, 2019</xref>; <xref rid="btaa225-B32" ref-type="bibr">Wählby <italic>et al.</italic>, 2002</xref>). Fluorescent staining offers two primary benefits: firstly, classification can be performed based on the compartmental or molecular specificity of different fluorophores (e.g. the fluorescence channel for membrane-stain corresponds to the ‘cell’-class, while the channel for a nucleus-stain corresponds to the ‘nucleus’-class). Secondly, image contrast in stained images arises from the abundance of fluorophores that accumulate in specific compartments against a zero background in the ideal case, which facilitates object segmentation. In contrast, cell segmentation and classification in (unstained) brightfield (BF) or phase-contrast (PhC) images is not trivial, as images exhibit a highly irregular appearance, non-specific contrast from cells and a non-zero background. While fluorescence staining is immensely powerful and convenient for cellular analyses and identification, it is nevertheless often associated with non-ideal requirements, such as the need for cell fixation and permeabilization and introduction of exogenous molecules, rendering it difficult to observe processes in native, live-cell experiments. Even if a dye does not require a sample preparation that results in cell death, it can introduce perturbations into the system that alter the experimental outcome (<xref rid="btaa225-B8" ref-type="bibr">Erba <italic>et al.</italic>, 1988</xref>; <xref rid="btaa225-B17" ref-type="bibr">Knight <italic>et al.</italic>, 2003</xref>; <xref rid="btaa225-B33" ref-type="bibr">Wiedenmann <italic>et al.</italic>, 2009</xref>). A segmentation and classification algorithm that performs robustly on unstained cell images would, therefore, be beneficial economically (reducing costs for fluorescent dyes and reducing hands-on work) and scientifically (possibility of unperturbed, quantitiative live-cell imaging).</p>
    <p>A promising potential solution lies in using ‘big data’ in conjunction with deep learning analyses. Specifically, convolutional neural networks (CNNs) offer the ability to train an algorithm to identify complex patterns and classify objects within images (<xref rid="btaa225-B18" ref-type="bibr">Krizhevsky <italic>et al.</italic>, 2012</xref>; <xref rid="btaa225-B19" ref-type="bibr">LeCun <italic>et al.</italic>, 1998</xref>). In recent years, CNNs have advanced beyond classifying image content to also localizing (<xref rid="btaa225-B10" ref-type="bibr">Girshick, 2015</xref>; <xref rid="btaa225-B11" ref-type="bibr">Girshick <italic>et al.</italic>, 2014</xref>; <xref rid="btaa225-B25" ref-type="bibr">Ren <italic>et al.</italic>, 2017</xref>), and finally segmenting, objects within an image (<xref rid="btaa225-B14" ref-type="bibr">He <italic>et al.</italic>, 2017</xref>). The new possibilities provided by CNNs have influenced a broad range of engineering and scientific fields, such as autonomous driving (<xref rid="btaa225-B2" ref-type="bibr">Bojarski <italic>et al.</italic>, 2016</xref>), face recognition (<xref rid="btaa225-B29" ref-type="bibr">Schroff and Philbin, 2015</xref>) and cancer detection (<xref rid="btaa225-B9" ref-type="bibr">Esteva <italic>et al.</italic>, 2017</xref>). CNNs have also begun to have an impact in cell biology as demonstrated by two very recent deep learning approaches to predict cellular architecture using a trained CNN. <xref rid="btaa225-B4" ref-type="bibr">Christiansen <italic>et al.</italic> (2018)</xref> trained a deep neural network for <italic>in silico</italic> prediction of fluorescent labels, such as nucleus-, membrane- and axon-staining. Their algorithm used several BF or PhC microscopic slices of varying z-depth to generate a predicted maximum-intensity projected fluorescent image (single plane). Another approach by <xref rid="btaa225-B23" ref-type="bibr">Ounkomol <italic>et al.</italic> (2018)</xref> achieved 3D fluorescent label prediction from 3D confocal stacks. Prediction of fluorescent labels is a powerful tool in cell microscopy but is not the same a single-cell quantification within images, for which appropriate segmentation steps are required. CNNs have also been applied for cell segmentation as first demonstrated by <xref rid="btaa225-B27" ref-type="bibr">Ronneberger <italic>et al.</italic> (2015)</xref>.</p>
    <p>A major drawback of CNNs, however, is their need for massive amounts of annotated data. Annotated image collections for everyday-scene analysis exceed 100 000, or even 1 000 000 images (<xref rid="btaa225-B6" ref-type="bibr">Deng <italic>et al.</italic>, 2009</xref>). For instance, the Common objects in context (COCO) dataset (<xref rid="btaa225-B20" ref-type="bibr">Lin <italic>et al.</italic>, 2014</xref>), a popular collection of images for the training of segmentation algorithms, contains more than 330 000 images with more than 2.5 million labeled instances of 91 different classes. Such large collections are needed to provide the CNN architecture with sufficient exemplary structures to create appropriate filters and thereby identify objects in images correctly. An adequate dataset is not only a question of size but also of heterogeneity, as a dataset needs to capture (instrument- and object-specific) image and shape variations to it to be generally applicable. In cell biology applications of microscopy, images vary by factors, such as light intensity, magnification, contrast, and uneven illumination. Additionally, objects within the images (cells, nuclei, granules, etc.) are highly heterogeneous entities that adopt various forms and sizes, e.g. cell shape can change from the ‘fried egg’ appearance to an elongated form under various stimuli. Low image heterogeneity in a dataset, e.g. by training on a single imaging setup can result in a need to retrain the resulting algorithm before usage with other imaging equipment (<xref rid="btaa225-B5" ref-type="bibr">De Fauw <italic>et al.</italic>, 2018</xref>).</p>
    <p>To our knowledge, most databases used for CNN approaches for cell biology applications are limited in terms of image heterogeneity such that images often come from a single setup, using the same objective, and exhibit limited variation in terms of cell types. To increase the ability for computer vision experts to develop image segmentation and quantitative image processing using CNNs that are applicable to more types of cells and microscopy conditions, we assembled a collection of more than 4600 images of 30 cell lines, acquired on 4 separate microscopy setups in three different laboratories with 9 different objectives having magnifications ranging from 10× to 40×. Cellular outlines and nuclei in our dataset were segmented manually by cell culture experts against fluorescently stained images. To our knowledge, this represents the first freely available, large-scale segmented dataset with more than 20 cell lines in the cell culture sector. With this new dataset, we hope to help close the gap between cell biologists and computer scientists, as it provides access to biological data specifically prepared for training of computer vision algorithms. As a proof-of-principle, we also trained a segmentation and classification algorithm on our dataset and achieved an average precision (AP) of 61.6% for intersection over union (IoU) scores above 0.5.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Cell culture</title>
      <p>All cells were maintained at 37°C, 90% relative humidity and 10 U/ml Penicillin/Streptomycin (Gibco) added to the respective medium. A complete list of used cell lines together with media and indicated culture supplements, such as fetal calf serum (FCS) and non-essential amino acids (NEAA), is given in <xref rid="btaa225-T1" ref-type="table">Table 1</xref>. At 24 h before imaging, cells were seeded into 96-well plates (Cellstar, Gibco; Screenstar, Greiner Bio-One) at 30%, 50% and 100% confluency. After incubation, cells were fixed with 4% para-formaldehyde in phosphate-buffered saline (PBS) for 10 min. Prior to imaging, cell membranes were stained with 0.01% CellMask Orange (Thermo Fisher Scientific) in PBS, and nuclei were stained with 1 µg/ml DAPI (Thermo Fisher Scientific) in PBS for 20 min at 37°C. Cells were subsequently washed with PBS three times prior to imaging.</p>
      <table-wrap id="btaa225-T1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>Cell lines represented in the EVICAN dataset</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">No</th>
              <th rowspan="1" colspan="1">Cell line</th>
              <th rowspan="1" colspan="1">Species</th>
              <th rowspan="1" colspan="1">Tissue</th>
              <th rowspan="1" colspan="1">Type</th>
              <th rowspan="1" colspan="1">Medium</th>
              <th rowspan="1" colspan="1">FCS (% in medium)</th>
              <th rowspan="1" colspan="1">NEAA added</th>
              <th rowspan="1" colspan="1">Code</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>1</bold>
              </td>
              <td rowspan="1" colspan="1">Colo 320</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Colon</td>
              <td rowspan="1" colspan="1">Colon adenocarcinoma</td>
              <td rowspan="1" colspan="1">RPMI</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">ACC 144 (DSMZ)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>2</bold>
              </td>
              <td rowspan="1" colspan="1">SW-480</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Colon</td>
              <td rowspan="1" colspan="1">Colorectal adenocarcinoma</td>
              <td rowspan="1" colspan="1">RPMI</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">CCL-228 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>3</bold>
              </td>
              <td rowspan="1" colspan="1">HT-29</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Colon</td>
              <td rowspan="1" colspan="1">Colorectal adenocarcinoma</td>
              <td rowspan="1" colspan="1">RPMI</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">HTB-38 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>4</bold>
              </td>
              <td rowspan="1" colspan="1">Caco-2</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Colon</td>
              <td rowspan="1" colspan="1">Colorectal adenocarcinoma</td>
              <td rowspan="1" colspan="1">EMEM</td>
              <td rowspan="1" colspan="1">20</td>
              <td rowspan="1" colspan="1">a</td>
              <td rowspan="1" colspan="1">HTB-37 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>5</bold>
              </td>
              <td rowspan="1" colspan="1">DLD-1</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Colon</td>
              <td rowspan="1" colspan="1">Colorectal adenocarcinoma</td>
              <td rowspan="1" colspan="1">RPMI</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">CCL-21 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>6</bold>
              </td>
              <td rowspan="1" colspan="1">HCT116</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Colon</td>
              <td rowspan="1" colspan="1">Colorectal carcinoma</td>
              <td rowspan="1" colspan="1">RPMI</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">CRL-247 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>7</bold>
              </td>
              <td rowspan="1" colspan="1">RKO</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Colon</td>
              <td rowspan="1" colspan="1">Colon carcinoma</td>
              <td rowspan="1" colspan="1">EMEM</td>
              <td rowspan="1" colspan="1">20</td>
              <td rowspan="1" colspan="1">a</td>
              <td rowspan="1" colspan="1">CRL-2577 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>8</bold>
              </td>
              <td rowspan="1" colspan="1">T47D</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Mammary gland</td>
              <td rowspan="1" colspan="1">Ductal carcinoma</td>
              <td rowspan="1" colspan="1">RPMI</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">HTB-133 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>9</bold>
              </td>
              <td rowspan="1" colspan="1">SK-BR-3</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Mammary gland (derived from pleural effusion)</td>
              <td rowspan="1" colspan="1">Adenocarcinoma</td>
              <td rowspan="1" colspan="1">RPMI</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">HTB-30 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>10</bold>
              </td>
              <td rowspan="1" colspan="1">MDA-MB-231</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Mammary gland (derived from pleural effusion)</td>
              <td rowspan="1" colspan="1">Adenocarcinoma</td>
              <td rowspan="1" colspan="1">RPMI</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">HTB-26 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>11</bold>
              </td>
              <td rowspan="1" colspan="1">MCF-7</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Mammary gland</td>
              <td rowspan="1" colspan="1">Adenosarcoma</td>
              <td rowspan="1" colspan="1">RPMI</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">HTB-22 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>12</bold>
              </td>
              <td rowspan="1" colspan="1">786-O</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Kidney</td>
              <td rowspan="1" colspan="1">Renal cell adenocarcinoma</td>
              <td rowspan="1" colspan="1">RPMI</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">CRL-1932 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>13</bold>
              </td>
              <td rowspan="1" colspan="1">769p</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Kidney</td>
              <td rowspan="1" colspan="1">Renal cell adenocarcinoma</td>
              <td rowspan="1" colspan="1">RPMI</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">CRL-1933 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>14</bold>
              </td>
              <td rowspan="1" colspan="1">ACHN</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Kidney</td>
              <td rowspan="1" colspan="1">Renal cell adenocarcinoma</td>
              <td rowspan="1" colspan="1">EMEM</td>
              <td rowspan="1" colspan="1">20</td>
              <td rowspan="1" colspan="1">a</td>
              <td rowspan="1" colspan="1">CRL-1611 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>15</bold>
              </td>
              <td rowspan="1" colspan="1">CAKI-2</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Kidney</td>
              <td rowspan="1" colspan="1">Clear-cell carcinoma</td>
              <td rowspan="1" colspan="1">RPMI</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">HTB-47 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>16</bold>
              </td>
              <td rowspan="1" colspan="1">PC-3</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Prostate</td>
              <td rowspan="1" colspan="1">Adenocarcinoma</td>
              <td rowspan="1" colspan="1">50/50 RPMI/F12</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">CRL-1435(ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>17</bold>
              </td>
              <td rowspan="1" colspan="1">LNCaP</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Prostate</td>
              <td rowspan="1" colspan="1">Carcinoma</td>
              <td rowspan="1" colspan="1">RPMI</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">ACC 256 (DSMZ)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>18</bold>
              </td>
              <td rowspan="1" colspan="1">DU-145</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Prostate (derived from metastatic site in brain)</td>
              <td rowspan="1" colspan="1">Carcinoma</td>
              <td rowspan="1" colspan="1">RPMI</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">HTB-81 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>19</bold>
              </td>
              <td rowspan="1" colspan="1">SH-SY5Y</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Bone marrow neuroblastoma</td>
              <td rowspan="1" colspan="1">Neuroblastoma</td>
              <td rowspan="1" colspan="1">DMEM</td>
              <td rowspan="1" colspan="1">20</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">CRL-2266 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>20</bold>
              </td>
              <td rowspan="1" colspan="1">MG-63</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Bone</td>
              <td rowspan="1" colspan="1">Osteosarcoma</td>
              <td rowspan="1" colspan="1">EMEM</td>
              <td rowspan="1" colspan="1">20</td>
              <td rowspan="1" colspan="1">a</td>
              <td rowspan="1" colspan="1">CRL-1427 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>21</bold>
              </td>
              <td rowspan="1" colspan="1">HeLa</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Cervix</td>
              <td rowspan="1" colspan="1">Adenocarcinoma</td>
              <td rowspan="1" colspan="1">DMEM</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">CCL-2 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>22</bold>
              </td>
              <td rowspan="1" colspan="1">HT-1080</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Connective tissue</td>
              <td rowspan="1" colspan="1">Fibrosarcoma</td>
              <td rowspan="1" colspan="1">DMEM</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">CCL-121 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>23</bold>
              </td>
              <td rowspan="1" colspan="1">NIH/3T3</td>
              <td rowspan="1" colspan="1">Mouse</td>
              <td rowspan="1" colspan="1">Embryo</td>
              <td rowspan="1" colspan="1">Fibroblast</td>
              <td rowspan="1" colspan="1">DMEM</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">CRL-1658(ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>24</bold>
              </td>
              <td rowspan="1" colspan="1">RAW 264.7</td>
              <td rowspan="1" colspan="1">Mouse</td>
              <td rowspan="1" colspan="1">Ascites (Abelson murine leukemia virus-induced tumor)</td>
              <td rowspan="1" colspan="1">Macrophage</td>
              <td rowspan="1" colspan="1">DMEM</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">TIB-71 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>25</bold>
              </td>
              <td rowspan="1" colspan="1">HEL 299</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Lung</td>
              <td rowspan="1" colspan="1">Fibroblast</td>
              <td rowspan="1" colspan="1">DMEM</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">CCL-137 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>26</bold>
              </td>
              <td rowspan="1" colspan="1">FaDu</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Pharynx</td>
              <td rowspan="1" colspan="1">Squamous cell carcinoma</td>
              <td rowspan="1" colspan="1">DMEM</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">HTB-43 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>27</bold>
              </td>
              <td rowspan="1" colspan="1">MCC26</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Skin</td>
              <td rowspan="1" colspan="1">Merkel carcinoma from skin</td>
              <td rowspan="1" colspan="1">DMEM</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">10092304 (Sigma Aldrich)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>28</bold>
              </td>
              <td rowspan="1" colspan="1">C2C12</td>
              <td rowspan="1" colspan="1">Mouse</td>
              <td rowspan="1" colspan="1">Muscle</td>
              <td rowspan="1" colspan="1">Myoblast</td>
              <td rowspan="1" colspan="1">RPMI</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">CRL-1772 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>29</bold>
              </td>
              <td rowspan="1" colspan="1">CHO-K1</td>
              <td rowspan="1" colspan="1">Hamster</td>
              <td rowspan="1" colspan="1">Ovary</td>
              <td rowspan="1" colspan="1">Epithelium</td>
              <td rowspan="1" colspan="1">F12</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">CCL-61 (ATCC)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>30</bold>
              </td>
              <td rowspan="1" colspan="1">hMSC</td>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">Bone marrow</td>
              <td rowspan="1" colspan="1">Mesenchymal stem cells</td>
              <td rowspan="1" colspan="1">DMEM</td>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">x</td>
              <td rowspan="1" colspan="1">PT-2501 (Lonza)</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>2.2 Imaging</title>
      <sec>
        <title>2.2.1 Image acquisition</title>
        <p>Image acquisition was performed on four different microscope setups: Opera Phenix (Perkin Elmer), AF7000 (Leica), IX81 (Olympus) and Biorevo BZ-9000 (Keyence). <xref rid="btaa225-T2" ref-type="table">Table 2</xref> summarizes the microscopy platforms as well as objectives used in this work.</p>
        <table-wrap id="btaa225-T2" orientation="portrait" position="float">
          <label>Table 2.</label>
          <caption>
            <p>Microscopes and objectives used for image acquisition</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Microscope</th>
                <th rowspan="1" colspan="1">Objective</th>
                <th rowspan="1" colspan="1">Contrast mode</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Opera Phenix (Perkin Elmer)</td>
                <td rowspan="1" colspan="1">10×/0.3 (air)</td>
                <td rowspan="1" colspan="1">BF</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">20×/0.4 (air)</td>
                <td rowspan="1" colspan="1">BF</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">40×/1.1 (water)</td>
                <td rowspan="1" colspan="1">BF</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">IX81 (Olympus)</td>
                <td rowspan="1" colspan="1">10×/0.3 (air)</td>
                <td rowspan="1" colspan="1">PhC</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">20×/0.4 (air)</td>
                <td rowspan="1" colspan="1">PhC</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">AF 7000 (Leica)</td>
                <td rowspan="1" colspan="1">10×/0.3 (air)</td>
                <td rowspan="1" colspan="1">PhC</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">20×/0.4 (air)</td>
                <td rowspan="1" colspan="1">PhC</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Biorevo BZ-9000 (Keyence)</td>
                <td rowspan="1" colspan="1">10×/0.3 (air)</td>
                <td rowspan="1" colspan="1">PhC</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">20×/0.45 (air)</td>
                <td rowspan="1" colspan="1">PhC</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec>
        <title>2.2.2 Image annotation and dataset assembly</title>
        <p>BF or PhC microscopy images were merged with region-matched fluorescence images of the nucleus and membrane to facilitate recognition of nuclei and cell borders using FIJI<bold><sup>®</sup></bold> (<xref rid="btaa225-B28" ref-type="bibr">Schindelin <italic>et al.</italic>, 2012</xref>). Cell culture experts and supervised personnel annotated 3–10 cells (on average 5.70 per image) and nuclei (on average 5.72 per image) within each image via the process depicted in <xref ref-type="fig" rid="btaa225-F1">Figure 1</xref>. To reduce human bias, cells and nuclei for annotation were picked at random by one of seven annotators. Need for corrections of the segmentation masks by the lead author was extremely rare (estimated at &lt;0.5%). For more detailed information on comparability between trained and untrained annotators, we refer the reader to work by <xref rid="btaa225-B15" ref-type="bibr">Hughes <italic>et al.</italic> (2018)</xref>. The final dataset contained three subsets:</p>
        <fig id="btaa225-F1" orientation="portrait" position="float">
          <label>Fig. 1.</label>
          <caption>
            <p>Dataset preparation pipeline. (<bold>a</bold>) Acquisition of BF and PhC microscopy images from various microscopes at different magnifications. (<bold>b</bold>) Overlay images of fluorescent nuclei and membrane, where available. (<bold>c</bold>) Manual segmentation of cell bodies (red) and nuclei (blue) were saved as binary masks and transformed into the COCO segmentation format. (Color version of this figure is available at <italic>Bioinformatics</italic> online.)</p>
          </caption>
          <graphic xlink:href="btaa225f1"/>
        </fig>
        <list list-type="order">
          <list-item>
            <p>Training dataset: 4464 images [3714 partially annotated; 750 background (blank) images; 42 317 instances]</p>
          </list-item>
          <list-item>
            <p>Validation dataset: 1176 images [926 partially annotated; 250 background (blank) images; 10 642 instances]</p>
          </list-item>
          <list-item>
            <p>Evaluation dataset: 98 images (fully annotated; 3222 instances).</p>
          </list-item>
        </list>
        <p>In total, 52 959 instances were segmented in the 4640 partially annotated images of the training and validation datasets. All annotations were exported as JavaScript Object Notation (JSON) document with IDs referring to the BF or PhC version of the original image, segmentations as x, y-polygons and category-IDs indicating cellular entity or nucleus. The export format was chosen to fit COCO annotation style to ensure maximal accessibility for modern machine-learning training. The dataset is available under a CC-BY license to allow far-ranging use. We encourage scientists to more annotations to the training and validation subsets and therefore supplied segmentation masks and raw images. The complete collection of dataset and annotation documents is provided under https://edmond.mpdl.mpg.de/imeji/collection/l45s16atmi6Aa4sI?q=.</p>
      </sec>
    </sec>
    <sec>
      <title>2.3 Convolution neural network training and evaluation</title>
      <sec>
        <title>2.3.1 Classifier training</title>
        <p>Training our detection and segmentation algorithm was performed on a Mask R-CNN implementation, previously released under an MIT license by Matterport Inc. (<xref rid="btaa225-B22" ref-type="bibr">Mask R-CNN implementation by Matterport Inc., 2018</xref>, <ext-link ext-link-type="uri" xlink:href="https://github.com/matterport/Mask_RCNN">https://github.com/matterport/Mask_RCNN</ext-link>). In this implementation, the Mask R-CNN approach is executed using the open-source Tensorflow and Keras libraries. As images were acquired on a variety of optical setups and with different cameras, all images were automatically adjusted to a size of 1024×024 pixels, with zero padding in cases where the raw image files were smaller by the Mask R-CNN implementation. We used a modification of the training scheme published previously by <xref rid="btaa225-B16" ref-type="bibr">Johnson (2018)</xref>. Briefly, a Resnet-101 feature pyramid network model with 101 layers organized in five stages was employed as the backbone; weights were initialized with pre-trained weights on the COCO dataset; training was performed for 52 epochs. A total of 40 epochs were used to train the so-called network heads, 8 epochs for layers 4 and above (4+), and 4 epochs for training of all layers [for more information on the network architecture, we refer the reader to <xref rid="btaa225-B13" ref-type="bibr">He <italic>et al</italic>. (2016)</xref> and <xref rid="btaa225-B14" ref-type="bibr">He <italic>et al.</italic> (2017)</xref>]. The learning rate for weight adjustment during training was set to an initial value of 0.001 at the start of heads, as well as 4+ layer training; this value was decreased by 50% after 20 epochs for heads training and after 4 epochs for 4+ layer training, respectively. For end-to-end training after epoch 48, we decreased the learning rate to 10% of the original value and after an additional 2 epochs, decreased it to 5% of the original value. All training steps were carried out on a desktop PC with an Intel Core i7-6700 CPU with four 3.4 GHz processors and 50 GB RAM for a full process duration of 31 days. No GPUs were used in our implementation. The code used to train the classifier is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/MischaSchwendy/EVICAN-MRCNN">https://github.com/MischaSchwendy/EVICAN-MRCNN</ext-link>.</p>
      </sec>
      <sec>
        <title>2.3.2 Classifier segmentation evaluation</title>
        <p>The evaluation was performed on 98 fully annotated BF and PhC images that were removed and kept separate from the training data. According to the image quality characteristics summarized in <xref rid="btaa225-T3" ref-type="table">Table 3</xref>, we categorized each evaluation image into one of three difficulty classes.</p>
        <table-wrap id="btaa225-T3" orientation="portrait" position="float">
          <label>Table 3.</label>
          <caption>
            <p>Quality characteristics of evaluation datasets</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Evaluation dataset</th>
                <th rowspan="1" colspan="1">Cellular appearance (in PhC or BF)</th>
                <th rowspan="1" colspan="1">Image quality, contrast mechanism</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Difficulty 1 33 images, 1084 instances</td>
                <td rowspan="1" colspan="1">2D cell growth, few cell–cell contacts, clear-cell outlines, most nuclei visible</td>
                <td rowspan="1" colspan="1">All cells in focus, most often PhC</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Difficulty 2 33 images, 1036 instances</td>
                <td rowspan="1" colspan="1">2D cell growth, several cell–cell contacts, most cell outlines visible, few nuclei visible</td>
                <td rowspan="1" colspan="1">Cells minimally defocused, mixed BF and PhC</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Difficulty 3 32 images, 1102 instances</td>
                <td rowspan="1" colspan="1">Occasional 3D growth, many cell–cell contacts/ colony formations, nuclei often invisible without staining</td>
                <td rowspan="1" colspan="1">Frequently defocused, mainly BF images</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>The three resulting evaluation datasets are intended to assess the capabilities of the trained classification-segmentation algorithm under varying imaging conditions. To guarantee accuracy of the ground truth masks, all annotations for quantitative comparison were generated on fluorescently stained images.</p>
        <p>Classification performance was evaluated according to the average precision (AP) metric. Predicted instances co-localized with corresponding ground truth instances were counted as true positives when exhibiting intersection over union (IoU) scores above a certain threshold. We monitored AP at IoU thresholds above 0.5 (AP<sub>0.5</sub>) and 0.75 (AP<sub>0.75</sub>) and report averaged values over all evaluation images. Additionally, a cumulative AP was computed, where the IoU threshold was incrementally increased from 50% to 95% in 5% steps, and the precision per image averaged over each step.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Dataset curation</title>
      <p>We assembled a dataset consisting of more than 4600 partially segmented BF and PhC microscopy images using several different microscopy setups. Up to 10 cellular and nuclear outlines were segmented per image, respectively, with a per image average of 11.4 total instances. As depicted in <xref ref-type="fig" rid="btaa225-F1">Figure 1</xref>, the pipeline we established for dataset production was to overlay PhC (or BF) microscopy images (<xref ref-type="fig" rid="btaa225-F1">Fig. 1a</xref>) with fluorescent channels from membrane and/or nucleus staining. Regarding the images in <xref ref-type="fig" rid="btaa225-F1">Figure 1c</xref>, the ratio of annotated instances to true instances within the image declined with lower magnification since more cells were included in the lower magnification images. Manual annotation was executed and validated by experienced cell biologists, which—while being quite laborious—was the only way to guarantee human-level accuracy for all images.</p>
      <p>Annotated images and corresponding grayscale versions (i.e. BF and PhC images) were compiled to a dataset consisting of a text file and an image collection in JPEG-format (<xref ref-type="fig" rid="btaa225-F2">Fig. 2</xref>). Manual annotations (i.e. segmentations) performed on corresponding stained overlay images were transformed into polygons (i.e. ‘X1, Y1; X2, Y2;…; Xn, Yn’) and saved together with image and object information, in a JSON text file. While annotations were performed on stained images, the information saved in the text file referred to the assembled PhC/BF image collection. Referring to the BF and PhC images for classifier training was necessary to ensure the usability of the resulting computer vision algorithms for unstained cell images after training on this dataset. Fluorescently stained images were not included in the compiled dataset.</p>
      <fig id="btaa225-F2" orientation="portrait" position="float">
        <label>Fig. 2.</label>
        <caption>
          <p>Dataset compilation. (<bold>a</bold>) Grayscale (PhC and BF) image versions were collected in a large-scale image collection. (<bold>b</bold>) Manual annotations (cell and nucleus segmentations) were transformed into polygons and saved in a JSON file. The compiled text file included information about images (ID, size and name) and objects (ID, class and polygon). All text information is referred to the grayscale image versions produced in (a)</p>
        </caption>
        <graphic xlink:href="btaa225f2"/>
      </fig>
      <p><xref ref-type="fig" rid="btaa225-F3">Figure 3a</xref> shows the number of images for each cell line, with the majority containing &gt;100 images. We provide two datasets in the COCO annotation format: the EVICAN2-version with two classes: ‘cell’ and ‘nucleus’, and the EVICAN60-version with nuclei and cells classified for each cell line, respectively, resulting in 60 class labels. As <xref ref-type="fig" rid="btaa225-F3">Figure 3b</xref> shows, we achieved a highly homogeneous distribution of nucleus and cell instances across nearly all cell lines. For most classes in the EVICAN60-version, we provide ∼1000 instances; for the cell and nucleus class in EVICAN2, we exceeded 26 000 instances per class. <xref ref-type="fig" rid="btaa225-F3">Figure 3c</xref> shows the moderate underrepresentation of PhC images in our dataset, which as described later, contributes to less prominent feature observability due to reduced contrast in BF compared to PhC images. Additional to the COCO-format annotations, we provide all masks as binary images with format ‘imageID_cellline.jpg’ to offer freedom for developers and researcher not using COCO-like datasets. The complete collection of images, masks and annotation documents is available at: <ext-link ext-link-type="uri" xlink:href="https://edmond.mpdl.mpg.de/imeji/collection/l45s16atmi6Aa4sI?q=">https://edmond.mpdl.mpg.de/imeji/collection/l45s16atmi6Aa4sI?q=</ext-link>.</p>
      <fig id="btaa225-F3" orientation="portrait" position="float">
        <label>Fig. 3.</label>
        <caption>
          <p>Overview of the EVICAN dataset. (<bold>a</bold>) Number of images per cell line, (<bold>b</bold>) numbers of instances per category in the dataset, and (<bold>c</bold>) relative number of images for BF and PhC imaging in the dataset</p>
        </caption>
        <graphic xlink:href="btaa225f3"/>
      </fig>
    </sec>
    <sec>
      <title>3.2 Comparison to other segmented cell datasets</title>
      <p>The EVICAN dataset generated above was compared to four other commonly used cell biology segmentation datasets:</p>
      <list list-type="order">
        <list-item>
          <p>The Allen Cell Explorer dataset [<xref rid="btaa225-B1" ref-type="bibr"><italic>Allen Institute for Cell Science, Allen Cell Explorer data,</italic> 2019</xref>, <ext-link ext-link-type="uri" xlink:href="http://www.allencell.org">https://www.allencell.org/data-downloading.html#DownloadImageData</ext-link>); <xref rid="btaa225-B26" ref-type="bibr">Roberts <italic>et al.</italic>, 2017]</xref>: a collection of ∼39 000 manually curated cells in ∼18 000 confocal 3D z-stacks (Personal correspondence with <xref rid="btaa225-B24" ref-type="bibr">Nathalie Gaudreault, Associate Director Microscopy Pipeline, 2019</xref>). Transmitted light images, as well as manually curated segmentation masks for 12 cellular components, are provided.</p>
        </list-item>
        <list-item>
          <p>The total number of PhC and DIC microscopy images of cell lines from the International Symposium on Biomedical Imaging (ISBI) cell-tracking challenges from 2014 and 2015: containing 6 time-lapse microscopy recordings, with 56 annotated frames of 3 cell types (HeLa, pancreatic stem cells and U373 cells) (<xref rid="btaa225-B27" ref-type="bibr">Ronneberger <italic>et al.</italic>, 2015</xref>; <xref rid="btaa225-B31" ref-type="bibr">Ulman <italic>et al.</italic>, 2017</xref>).</p>
        </list-item>
        <list-item>
          <p>Three combined datasets of PhC images by <xref rid="btaa225-B12" ref-type="bibr">Gurari <italic>et al.</italic> (2015)</xref>: 151 partially segmented images of 3 cell lines (fibroblasts, rabbit smooth muscle cells and rat smooth muscle cells).</p>
        </list-item>
        <list-item>
          <p>All DIC microscopy images of cell lines from the Broad Bioimage Benchmark Collection (BBBC) (<xref rid="btaa225-B21" ref-type="bibr">Ljosa <italic>et al.</italic>, 2012</xref>): 65 fully segmented images for 2 cell lines: CHO and red blood cells.</p>
        </list-item>
      </list>
      <p>In comparing datasets, the size, i.e. the number of images and segmented instances/cells, as well as heterogeneity was reviewed. The heterogeneity of a dataset should ideally reflect object- and instrument-specific variations to be complete. To account for object- (i.e. cell) specific variations, we assessed the number of cell lines included in a dataset, as well as cellular structures (e.g. cell, nucleus, and actin) annotated in a dataset: object variation=# cell lines×# cell structures. For instrument-specific variations, we checked for the number of different microscope models, magnifications, and contrast mechanisms used in each dataset: instrument variation=# magnifications×# microscope models×# contrast mechanisms (<xref rid="btaa225-T4" ref-type="table">Table 4</xref>).</p>
      <table-wrap id="btaa225-T4" orientation="portrait" position="float">
        <label>Table 4.</label>
        <caption>
          <p>Number of images and segmented cells in the five compared datasets</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">EVICAN</th>
              <th rowspan="1" colspan="1">Allen cell explorer</th>
              <th rowspan="1" colspan="1">ISBI</th>
              <th rowspan="1" colspan="1">Gurari</th>
              <th rowspan="1" colspan="1">BBBC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Images</td>
              <td rowspan="1" colspan="1">4640</td>
              <td rowspan="1" colspan="1">∼ 18 000</td>
              <td rowspan="1" colspan="1">56</td>
              <td rowspan="1" colspan="1">151</td>
              <td rowspan="1" colspan="1">65</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Segmented cells</td>
              <td rowspan="1" colspan="1">26 428</td>
              <td rowspan="1" colspan="1">∼ 39 000</td>
              <td rowspan="1" colspan="1">899</td>
              <td rowspan="1" colspan="1">151</td>
              <td rowspan="1" colspan="1">995</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Cell lines</td>
              <td rowspan="1" colspan="1">30</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">3</td>
              <td rowspan="1" colspan="1">3</td>
              <td rowspan="1" colspan="1">2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Cell structures</td>
              <td rowspan="1" colspan="1">2</td>
              <td rowspan="1" colspan="1">12</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">1</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Magnifications</td>
              <td rowspan="1" colspan="1">3</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">3</td>
              <td rowspan="1" colspan="1">2</td>
              <td rowspan="1" colspan="1">2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Microscope models</td>
              <td rowspan="1" colspan="1">4</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">3</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">1<xref ref-type="table-fn" rid="tblfn2"><sup>a</sup></xref></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Contrast mechanisms</td>
              <td rowspan="1" colspan="1">2</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">2</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">1</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Object variation</td>
              <td rowspan="1" colspan="1">60</td>
              <td rowspan="1" colspan="1">12</td>
              <td rowspan="1" colspan="1">3</td>
              <td rowspan="1" colspan="1">3</td>
              <td rowspan="1" colspan="1">2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Instrument variation</td>
              <td rowspan="1" colspan="1">24</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">18</td>
              <td rowspan="1" colspan="1">2</td>
              <td rowspan="1" colspan="1">2</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic>Note</italic>: Highest scoring datasets for each category are highlighted in gray.</p>
          </fn>
          <fn id="tblfn2">
            <label>a</label>
            <p>Potentially more microscope models.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>In particular, increasing object variation, i.e. ‘cell lines’ and ‘cellular structures’, is valuable for algorithm development, as it allows computer scientists to generate multiple classes. For instance, in our EVICAN2 dataset only <italic>segmented structures</italic> were introduced as classes, i.e. the two classes ‘nucleus’ and ‘cell’, while in the EVICAN60 dataset, each <italic>cell line</italic> and <italic>segmented structures</italic> were used to form 60 classes (nuclei and cellular outlines specific for each of the 30 cell lines).</p>
      <p>Only the Allen Cell Explorer dataset outranks the EVICAN dataset in terms of image number (18 000 versus 4600) and number of segmented cells (39 000 versus 26 400). The ISBI image collection shows a strong instrument-specific variation with multiple objective magnifications, microscope models and contrast mechanisms. In contrast, the Allen Cell Explorer data collection provides a strong object-specific variation, due to the multitude of segmented cellular structures that are available as 3D masks. However, none of the compared image collections achieves a heterogeneity greater than our EVICAN dataset, which offers large image and instance numbers combined with a balanced object and instrument variation. The comparison datasets either provide only a limited number of images (ISBI, Gurari and BBBC) or show limited instrument-specific heterogeneity (Allen Cell Explorer). Limited data and variation within a dataset reduce the robustness of trained algorithms for general use outside of, e.g. the specific imaging system or specific cell lines used in training.</p>
    </sec>
    <sec>
      <title>3.3 Dataset usage in segmentation analysis</title>
      <sec>
        <title>3.3.1 Classifier training</title>
        <p>As a proof-of-principle demonstration that the EVICAN dataset is useful for deep learning applications, we used the EVICAN2 version to train a deep learning classifier using a modified version of Matterport Inc.’s implementation of Mask R-CNN for image segmentation and object classification. To reduce the influence of unannotated cells on the background class, we prepared our dataset by Gaussian blurring (sigma =30 pixels) everything except for the annotated instances plus an extra 10-pixel border around their outlines. The blurred content in our images was our solution to using partially annotated images and minimizing the incorrect training of the classifier to consider non-segmented cells as a requirement for segmented cells. Additionally, several hundred unblurred background images (having no cells) were included in the training and validation dataset to allow for an appropriate training of the background class. The Mask R-CNN algorithm was then trained as described in Section 2. The trained classifier produced an algorithm for cellular and nuclear detection based on both BF and PhC images.</p>
      </sec>
      <sec>
        <title>3.3.2 Classifier evaluation</title>
        <p>We tested our trained algorithm on microscopy image evaluation datasets categorized in three classes of rising difficulty level. AP was computed at IoU thresholds above 0.5 (AP<sub>0.5</sub>), above 0.75 (AP<sub>0.75</sub>), and averaged over thresholds rising from AP = 0.5 to 0.95 in 0.05 steps. All values were averaged over all images within the respective evaluation dataset. <xref ref-type="fig" rid="btaa225-F4">Figure 4</xref> shows that with rising difficulty level of the evaluation data and with increased IoU thresholds, AP values decreased, as expected.</p>
        <fig id="btaa225-F4" orientation="portrait" position="float">
          <label>Fig. 4.</label>
          <caption>
            <p>AP of our trained algorithm on EVICAN2 evaluation images of three different difficulty levels. All AP values decrease with increasing difficulty and increasing IoU thresholds. Error bars are shown as standard deviation. AP values decrease with increasing difficulty level</p>
          </caption>
          <graphic xlink:href="btaa225f4"/>
        </fig>
        <p>When assessing the lowest difficulty evaluation dataset, we achieved an AP<sub>0.5</sub> of 0.61. The decreasing AP with increasing IoU thresholds (i.e. scores of AP<sub>0.75</sub> and AP) indicated that a majority of positive detections in AP<sub>0.5</sub> was based on IoU values below 0.75. With the combined annotation of cell bodies and nuclei in one dataset, we could also show that it was possible to detect cells and subcellular features mutually in one step. For qualitative assessment, <xref ref-type="fig" rid="btaa225-F5">Figure 5a</xref> shows exemplary input and output images for our algorithm. It is apparent that the algorithm produces better results on images with higher contrast (i.e. in PhC images, see <xref ref-type="fig" rid="btaa225-F5">Fig. 5</xref>, left and right columns).</p>
        <fig id="btaa225-F5" orientation="portrait" position="float">
          <label>Fig. 5.</label>
          <caption>
            <p>EVICAN2-trained classifier performance for grayscale image segmentation. Input (<bold>a</bold>) and output (<bold>b</bold>) example images for our algorithm. Varying colors indicate individual cell or nucleus segmentations, values (white) denote confidence of each detection (maximum = 1.0; all values above 0.7). Left: SW480 cells, imaged in PhC mode (20× objective), 100% correctly detected cells; middle: PC3 cells, imaged in BF mode (40× objective), nuclei not or incorrectly detected; right: CHO cells, imaged in PhC mode (20× objective), several false-positive detections. (Color version of this figure is available at <italic>Bioinformatics</italic> online.)</p>
          </caption>
          <graphic xlink:href="btaa225f5"/>
        </fig>
        <p>BF images, even with high magnification, often result in incorrect detections or missed cells/nuclei (see <xref ref-type="fig" rid="btaa225-F5">Fig. 5</xref>, middle column). While other datasets include colored or stained images, EVICAN relies solely on grayscale images, thereby limiting feature dimensionality.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <p>The EVICAN dataset provides a large-scale, multi-class, manually annotated and segmented, mixed BF and PhC microscopy image collection covering a broad range of cell lines (30 adherent cell lines). Training computer vision algorithms on our dataset should enable computer scientists to produce faster and more accurate, and more broadly usable, cell image segmentation and characterization tools using unstained images. This capability has the potential to strongly increase the ability of simple light microscopes to serve as quantitative instruments in cell biology labs. Machine-learning algorithms have been applied before to microscopy images, in part with remarkable success, such as work by <xref rid="btaa225-B4" ref-type="bibr">Christiansen <italic>et al.</italic> (2018)</xref> and <xref rid="btaa225-B23" ref-type="bibr">Ounkomol <italic>et al.</italic> (2018)</xref> who predicted fluorescent labels from transmitted light images. However, image processing is still far behind the technology for image acquisition. Despite the massive application of computer vision in other data-intensive sectors like face recognition (<xref rid="btaa225-B29" ref-type="bibr">Schroff and Philbin, 2015</xref>), progress in applying computer vision in cell microscopy has been comparatively slow. We believe that this slow progress is due to the two decoupled sectors: computer scientists usually have no access to a biolab with adequate image acquisition machinery and most biologists lack the knowledge and skills to create or retrain computer vision algorithms. Image collections like the ISBI cell-tracking challenge datasets offer relatively heterogeneous datasets but lack appropriate image numbers. In contrast, the Allen Cell Explorer dataset offers gigantic numbers of images and instances but lacks image heterogeneity. The scope of the Allen Cell Explorer dataset is to provide confocal z-stacks with a multitude of manually curated cellular structures that allow training of machine-learning algorithms to detect subcellular entities. The limited heterogeneity is a result of the focus on high-throughput experiments on a single-cell line (human-induced pluripotent stem cells), using identical microscope models, and providing the image data from a single objective magnification (100×). We note that an extensively annotated fluorescent dataset of nuclei was recently released by Carpenter and colleagues with 23 165 segmented nuclei as well as an evaluation of deep learning strategies (<xref rid="btaa225-B3" ref-type="bibr">Caicedo <italic>et al.</italic>, 2019</xref>).</p>
    <p>The limitations of current state-of-the-art image datasets for cell microscopy leave room for a balanced dataset of sufficient size. We hope to fill this gap by providing computer scientists (and other algorithm developers) with our image collection. We provide two editions of our dataset: EVICAN2 with classes ‘nucleus’ and ‘cell’ as well as EVICAN60 with 60 classes for 30 cell lines and their respective nuclei. Additionally, we provide three evaluation datasets accounting for varying image quality. As the dataset is only partially annotated, we encourage scientists and volunteers to add annotations and evaluate how the performance of classifiers changes.</p>
    <p>Using the EVICAN2 dataset—with partially annotated images—in a pilot machine-learning application for cell and nucleus identification, we generated a classification and segmentation algorithm with an AP up to 61.6% at IoU scores above 0.5. This result demonstrates that training a classifier with our dataset of partially annotated and blurred background images was sufficient to segment cells and nuclei in the (fully annotated) evaluation images with reasonable performance. Other groups have reported more robust results (<xref rid="btaa225-B14" ref-type="bibr">He <italic>et al.</italic>, 2017</xref>; <xref rid="btaa225-B16" ref-type="bibr">Johnson, 2018</xref>). However, these algorithms rely on colored or stained images while EVICAN2 training produced a detection algorithm for unstained, grayscale images. Feature availability is reduced in grayscale images, as one channel is used instead of three, which partially explains the lower performance of resulting detection algorithms. Increasing the number of annotations is likely one way to improve the precision though we cannot speculate on the exact benefit.</p>
    <p>The performance of our algorithm was best for the lowest difficulty images in our evaluation dataset. This can be explained with a higher degree of feature presentation in images with few cell–cell contacts, strong contrast (e.g. from PhC), high resolution and better-focused image conditions. The COCO dataset was designed with object types recognizable by a 4-year old (<xref rid="btaa225-B20" ref-type="bibr">Lin <italic>et al.</italic>, 2014</xref>), while the EVICAN dataset includes cellular outlines and incorporated nuclei, that overlap, share a strong resemblance, and are often challenging to see without staining, even by a trained individual.</p>
    <p>Better performance on high resolution and magnification images could arise from higher feature visibility that is lost in lower resolution. PhC images provide higher contrast; features appear more prominently, which facilitates feature detection in the convolutional process. Nevertheless, the limited dimensionality (as a consequence of the grayscale nature of the images) prevents the algorithm from searching for color-encoded features.</p>
  </sec>
  <sec>
    <title>5 Conclusion</title>
    <p>The proof-of-principle use of our dataset in the Mask-RCNN implementation performed adequately with our settings and computational resources, but we strongly encourage the scientific community to add further annotations or use more powerful computational tools to expand the capacity of resulting algorithms and increase segmentation accuracy. We believe that with the right tools (e.g. multi-GPU support) and advanced image augmentation, the EVICAN dataset, particularly the EVICAN60 version, can lead to transformative algorithm developments, similar to that seen in other computer vision fields. Such an algorithm, which is capable of not only segmenting cells and nuclei within an image but also discriminating among cell types in co-cultures would open the door to a new era of high-throughput cell microscopy. All cells in a microscopy field could be adequately measured label-free (i.e. quantification of cell spreading, elongation, circularity, etc.), multiple cell types in co-cultures could be instantly identified and changes in cell morphology could be e.g. linked to drug treatment or differentiation. We hope computer scientists and computational biologists use our dataset in efforts to achieve this goal.</p>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>We cordially thank our annotators Ravi Dhiman, Evelyn Schwendy, Kevin Kellner, Kevin Machel, Sabine Pütz and Mohamed Afhakama. Support by the IMB Mainz Microscopy Core Facility is gratefully acknowledged, especially from Maria Hanulova and Sandra Ritz who provided technical assistance and training on the Opera Phenix and AF7000 microscopes. We thank Frederik F. Fleissner who provided fruitful discussions.</p>
    <sec>
      <title>Funding</title>
      <p>This work was supported by the Max Planck Graduate Center, the Welch Foundation [F-2008-20190330] and the Human Frontiers in Science Program [RGP0045/2018].</p>
      <p><italic>Conflict of Interest</italic>: none declared.</p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btaa225-B1">
      <mixed-citation publication-type="other">Allen Institute for Cell Science, Allen Cell Explorer data. <ext-link ext-link-type="uri" xlink:href="https://www.allencell.org/data-downloading.html#DownloadImageData">https://www.allencell.org/data-downloading.html#DownloadImageData</ext-link> (16 April 2019, date last accessed).</mixed-citation>
    </ref>
    <ref id="btaa225-B2">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Bojarski</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>End to end learning for self-driving Cars</article-title>. <italic>arXiv preprint arXiv:1604.07316</italic>.</mixed-citation>
    </ref>
    <ref id="btaa225-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Caicedo</surname><given-names>J.C.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>Evaluation of deep learning strategies for nucleus segmentation in fluorescence images</article-title>. <source>Cytom. Part A</source>, <volume>95</volume>, <fpage>952</fpage>–<lpage>965</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa225-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Christiansen</surname><given-names>E.M.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>In silico labeling: predicting fluorescent labels in unlabeled images</article-title>. <source>Cell</source>, <volume>173</volume>, <fpage>792</fpage>–<lpage>803</lpage>.<pub-id pub-id-type="pmid">29656897</pub-id></mixed-citation>
    </ref>
    <ref id="btaa225-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>De Fauw</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Clinically applicable deep learning for diagnosis and referral in retinal disease</article-title>. <source>Nat. Med</source>., <volume>24</volume>, <fpage>1342</fpage>–<lpage>1350</lpage>.<pub-id pub-id-type="pmid">30104768</pub-id></mixed-citation>
    </ref>
    <ref id="btaa225-B6">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Deng</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) ImageNet: a large-scale hierarchical image database. In: <italic>IEEE Conference on Computer Vision and Pattern Recognition</italic>, Miami, FL, pp. <fpage>248</fpage>–<lpage>255</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa225-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dormand</surname><given-names>J.R.</given-names></name>, <name name-style="western"><surname>Prince</surname><given-names>P.J.</given-names></name></person-group> (<year>1980</year>) 
<article-title>A family of embedded Runge–Kutta formulae</article-title>. <source>J. Comp. Appl. Math</source>., <volume>6</volume>, <fpage>19</fpage>–<lpage>26</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa225-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Erba</surname><given-names>E.</given-names></name></person-group><etal>et al</etal> (<year>1988</year>) 
<article-title>DNA damage, cytotoxic effect and cell-cycle perturbation of hoechst 33342 on L1210 cells in vitro</article-title>. <source>Cytometry</source>, <volume>9</volume>, <fpage>1</fpage>–<lpage>6</lpage>.<pub-id pub-id-type="pmid">3409781</pub-id></mixed-citation>
    </ref>
    <ref id="btaa225-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Esteva</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Dermatologist-level classification of skin cancer with deep neural networks</article-title>. <source>Nature</source>, <volume>542</volume>, <fpage>115</fpage>–<lpage>118</lpage>.<pub-id pub-id-type="pmid">28117445</pub-id></mixed-citation>
    </ref>
    <ref id="btaa225-B10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name></person-group> (<year>2015</year>) Fast R-CNN. <source>In: <italic toggle="yes">IEEE International Conference on Computer Vision (ICCV)</italic>, Santiago, pp. 1440–1448.</source></mixed-citation>
    </ref>
    <ref id="btaa225-B11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Girshick</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) Rich feature hierarchies for accurate object detection and semantic segmentation. In: <italic>IEEE Conference on Computer Vision and Pattern Recognition</italic>, Columbus, OH, pp. <fpage>580</fpage>–<lpage>587</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa225-B12">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Gurari</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) How to collect segmentations for biomedical images? A benchmark evaluating the performance of experts, crowdsourced non-experts, and algorithms. In: <italic>IEEE Winter Conference on Applications of Computer Vision</italic>, Waikoloa, HI, pp. <fpage>1169</fpage>–<lpage>1176</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa225-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Deep residual learning for image recognition</article-title>. <source>In:<italic toggle="yes"> IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic>, Las Vegas, NV, pp. 770–778.</source></mixed-citation>
    </ref>
    <ref id="btaa225-B14">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) Mask R-CNN. In: <italic>IEEE International Conference on Computer Vision (ICCV)</italic>, Venice, pp. <fpage>2980</fpage>–<lpage>2988</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa225-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hughes</surname><given-names>A.J.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Quanti.us: a tool for rapid, flexible, crowd-based annotation of images</article-title>. <source>Nat. Methods</source>, <volume>15</volume>, <fpage>587</fpage>–<lpage>590</lpage>.<pub-id pub-id-type="pmid">30065368</pub-id></mixed-citation>
    </ref>
    <ref id="btaa225-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Johnson</surname><given-names>J. W.</given-names></name></person-group> (<year>2018</year>) 
<article-title>Adapting Mask-RCNN for automatic nucleus segmentation</article-title>. <source><italic toggle="yes">arXiv preprint arXiv:1805.00500</italic></source></mixed-citation>
    </ref>
    <ref id="btaa225-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Knight</surname><given-names>M.M.</given-names></name></person-group><etal>et al</etal> (<year>2003</year>) 
<article-title>Live cell imaging using confocal microscopy induces intracellular calcium transients and cell death</article-title>. <source>Am. J. Physiol. Cell Physiol</source>., <volume>284</volume>, <fpage>C1083</fpage>–<lpage>C1089</lpage>.<pub-id pub-id-type="pmid">12661552</pub-id></mixed-citation>
    </ref>
    <ref id="btaa225-B18">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Krizhevsky</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) ImageNet classification with deep convolutional neural networks. In: <source>Advances in Neural Information Processing Systems</source>, <italic>Stateline, NV</italic>, pp. <fpage>1097</fpage>–<lpage>1105</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa225-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>1998</year>) 
<article-title>Gradient-based learning applied to document recognition</article-title>. <source>Proc. IEEE</source>, <volume>86</volume>, <fpage>2278</fpage>–<lpage>2324</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa225-B20">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>T.Y.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) <italic>Microsoft COCO: Common Objects in Context</italic> Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). 8693 LNCS(PART 5), pp. <fpage>740</fpage>–<lpage>755</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa225-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ljosa</surname><given-names>V.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Annotated high-throughput microscopy image sets for validation</article-title>. <source>Nat. Methods</source>, <volume>9</volume>, <fpage>637</fpage>.<pub-id pub-id-type="pmid">22743765</pub-id></mixed-citation>
    </ref>
    <ref id="btaa225-B22">
      <mixed-citation publication-type="other">Mask R-CNN implementation by Matterport Inc. <ext-link ext-link-type="uri" xlink:href="https://github.com/matterport/Mask_RCNN">https://github.com/matterport/Mask_RCNN</ext-link> (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="btaa225-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ounkomol</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Label-free prediction of three-dimensional fluorescence images from transmitted-light microscopy</article-title>. <source>Nat. Methods</source>, <volume>15</volume>, <fpage>917</fpage>–<lpage>920</lpage>.<pub-id pub-id-type="pmid">30224672</pub-id></mixed-citation>
    </ref>
    <ref id="btaa225-B24">
      <mixed-citation publication-type="other">Personal correspondence with Nathalie Gaudreault<italic>, Associate Director Microscopy Pipeline</italic> (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="btaa225-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ren</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Faster R-CNN: towards real-time object detection with region proposal networks</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>., <volume>39</volume>, <fpage>1137</fpage>–<lpage>1149</lpage>.<pub-id pub-id-type="pmid">27295650</pub-id></mixed-citation>
    </ref>
    <ref id="btaa225-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Roberts</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Systematic gene tagging using CRISPR/Cas9 in human stem cells to illuminate cell organization</article-title>. <source>Mol. Biol. Cell</source>, <volume>28</volume>, <fpage>2854</fpage>–<lpage>2874</lpage>.<pub-id pub-id-type="pmid">28814507</pub-id></mixed-citation>
    </ref>
    <ref id="btaa225-B27">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Ronneberger</surname><given-names>O.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>U-Net: convolutional networks for biomedical image segmentation</article-title>. In: <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer, Cham</italic>, pp. 234–241.</mixed-citation>
    </ref>
    <ref id="btaa225-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schindelin</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Fiji—an open source platform for biological image analysis</article-title>. <source>Nat. Methods</source>, <volume>9</volume>, <fpage>676</fpage>–<lpage>682</lpage>.<pub-id pub-id-type="pmid">22743772</pub-id></mixed-citation>
    </ref>
    <ref id="btaa225-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schroff</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Philbin</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>) FaceNet: a unified embedding for face recognition and clustering. <source>In: </source><italic>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic>, Boston, MA, pp. 815–823.</mixed-citation>
    </ref>
    <ref id="btaa225-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schwendy</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>Automated cell segmentation in FIJI <sup>®</sup> using the DRAQ5 nuclear dye</article-title>. <source>BMC Bioinformatics</source>, <volume>20</volume>, <fpage>1</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">30606105</pub-id></mixed-citation>
    </ref>
    <ref id="btaa225-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ulman</surname><given-names>V.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>An objective comparison of cell-tracking algorithms</article-title>. <source>Nat. Methods</source>, <volume>14</volume>, <fpage>1141</fpage>–<lpage>1152</lpage>.<pub-id pub-id-type="pmid">29083403</pub-id></mixed-citation>
    </ref>
    <ref id="btaa225-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wählby</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2002</year>) 
<article-title>Algorithms for cytoplasm segmentation of fluorescence labelled cells</article-title>. <source><italic toggle="yes">Anal. Cell Pathol.</italic>, <bold>24</bold>, 101–111</source>.</mixed-citation>
    </ref>
    <ref id="btaa225-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wiedenmann</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>Fluorescent proteins for live cell imaging: opportunities, limitations, and challenges</article-title>. <source>IUBMB Life</source>, <volume>61</volume>, <fpage>1029</fpage>–<lpage>1042</lpage>.<pub-id pub-id-type="pmid">19859977</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
