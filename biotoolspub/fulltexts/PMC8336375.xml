<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Biomed Eng Online</journal-id>
    <journal-id journal-id-type="iso-abbrev">Biomed Eng Online</journal-id>
    <journal-title-group>
      <journal-title>BioMedical Engineering OnLine</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1475-925X</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8336375</article-id>
    <article-id pub-id-type="publisher-id">915</article-id>
    <article-id pub-id-type="doi">10.1186/s12938-021-00915-2</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Sch-net: a deep learning architecture for automatic detection of schizophrenia</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Fu</surname>
          <given-names>Jia</given-names>
        </name>
        <address>
          <email>jia_fu@stu.scu.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Yang</surname>
          <given-names>Sen</given-names>
        </name>
        <address>
          <email>sen.yang.scu@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>He</surname>
          <given-names>Fei</given-names>
        </name>
        <address>
          <email>feihe@stu.scu.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7168-2737</contrib-id>
        <name>
          <surname>He</surname>
          <given-names>Ling</given-names>
        </name>
        <address>
          <email>ling.he@scu.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Yuanyuan</given-names>
        </name>
        <address>
          <email>liyuanyuan@wchscu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Jing</given-names>
        </name>
        <address>
          <email>jing_zhang@scu.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Xiong</surname>
          <given-names>Xi</given-names>
        </name>
        <address>
          <email>flyxiongxi@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.13291.38</institution-id><institution-id institution-id-type="ISNI">0000 0001 0807 1581</institution-id><institution>College of Biomedical Engineering, </institution><institution>Sichuan University, </institution></institution-wrap>Chengdu, China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.412901.f</institution-id><institution-id institution-id-type="ISNI">0000 0004 1770 1022</institution-id><institution>Mental Health Center, </institution><institution>West China Hospital of Sichuan University, </institution></institution-wrap>Chengdu, China </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.411307.0</institution-id><institution-id institution-id-type="ISNI">0000 0004 1790 5236</institution-id><institution>School of Cybersecurity, </institution><institution>Chengdu University of Information Technology, </institution></institution-wrap>Chengdu, China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>3</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>3</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>20</volume>
    <elocation-id>75</elocation-id>
    <history>
      <date date-type="received">
        <day>17</day>
        <month>5</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>26</day>
        <month>7</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Schizophrenia is a chronic and severe mental disease, which largely influences the daily life and work of patients. Clinically, schizophrenia with negative symptoms is usually misdiagnosed. The diagnosis is also dependent on the experience of clinicians. It is urgent to develop an objective and effective method to diagnose schizophrenia with negative symptoms. Recent studies had shown that impaired speech could be considered as an indicator to diagnose schizophrenia. The literature about schizophrenic speech detection was mainly based on feature engineering, in which effective feature extraction is difficult because of the variability of speech signals.</p>
      </sec>
      <sec>
        <title>Methods</title>
        <p id="Par2">This work designs a novel Sch-net neural network based on a convolutional neural network, which is the first work for end-to-end schizophrenic speech detection using deep learning techniques. The Sch-net adds two components, skip connections and convolutional block attention module (CBAM), to the convolutional backbone architecture. The skip connections enrich the information used for the classification by emerging low- and high-level features. The CBAM highlights the effective features by giving learnable weights. The proposed Sch-net combines the advantages of the two components, which can avoid the procedure of manual feature extraction and selection.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par3">We validate our Sch-net through ablation experiments on a schizophrenic speech data set that contains 28 patients with schizophrenia and 28 healthy controls. The comparisons with the models based on feature engineering and deep neural networks are also conducted. The experimental results show that the Sch-net has a great performance on the schizophrenic speech detection task, which can achieve 97.68% accuracy on the schizophrenic speech data set. To further verify the generalization of our model, the Sch-net is tested on open access LANNA children speech database for specific language impairment detection. The results show that our model achieves 99.52% accuracy in classifying patients with SLI and healthy controls. Our code will be available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Scu-sen/Sch-net">https://github.com/Scu-sen/Sch-net</ext-link>.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par4">Extensive experiments show that the proposed Sch-net can provide aided information for the diagnosis of schizophrenia and specific language impairment.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Schizophrenia</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Skip connection</kwd>
      <kwd>Attention mechanism</kwd>
      <kwd>Pathological speech detection</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004829</institution-id>
            <institution>Department of Science and Technology of Sichuan Province</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2019YFS0236</award-id>
        <award-id>2019YJ0523</award-id>
        <principal-award-recipient>
          <name>
            <surname>He</surname>
            <given-names>Ling</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par25">Psychological and neurological disorders are two major categories of human disorders, which affect the thinking, speaking, and behavior capacity of human beings [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>]. At present, the global prevalence of psychological and neurological disorders is more than 12% and 10%, respectively [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR4">4</xref>]. Schizophrenia is a chronic psychological disease that affects about 1% of the population worldwide [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR6">6</xref>]. The disease often begins in late adolescence, and it has a large impact on patients’ social activity and brain development. Schizophrenia is characterized by disordered thinking, impaired speech, and abnormal behaviors. Clinical diagnosis of schizophrenia is generally based on a full psychiatric assessment and the speech/behaviors observed via clinical interviews. Symptoms of schizophrenia can be divided into two types, positive symptoms, and negative symptoms. Positive symptoms include delusions and hallucinations [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR7">7</xref>], and negative symptoms include flat affect, alogia, loss of interest, and disability in activities [<xref ref-type="bibr" rid="CR8">8</xref>]. Clinical experience had shown that it is harder to diagnose and treat patients with negative symptoms than those with positive symptoms [<xref ref-type="bibr" rid="CR9">9</xref>]. Positive symptoms are likely to be replaced by negative symptoms in the late episode of schizophrenia, and negative symptoms may persist even though after treatment [<xref ref-type="bibr" rid="CR10">10</xref>]. Negative symptoms contribute more to the long-term morbidity, higher rates of disability, and poor quality of life in most schizophrenic patients than positive symptoms do [<xref ref-type="bibr" rid="CR11">11</xref>–<xref ref-type="bibr" rid="CR15">15</xref>]. In addition, the clinical diagnosis relies on the experience of clinicians and is affected by patients’ retrospective recall biases and cognitive limitations [<xref ref-type="bibr" rid="CR16">16</xref>]. Hence, it is urgent to propose a method to diagnose schizophrenic patients with negative symptoms objectively and effectively.</p>
    <p id="Par26">Patients with schizophrenia exhibit brain structural abnormalities [<xref ref-type="bibr" rid="CR17">17</xref>–<xref ref-type="bibr" rid="CR19">19</xref>], which are accountable for speech disorders and cognitive impairments. Cohen [<xref ref-type="bibr" rid="CR20">20</xref>] discovered that speech characteristics are significantly related to the negative symptoms of schizophrenia. Rosenstein [<xref ref-type="bibr" rid="CR21">21</xref>] confirmed that adolescents with high-risk psychosis exhibit speech impairments for months/years before they are diagnosed. Flat affect and incoherent language expression are typical performances in schizophrenic patients with negative symptoms [<xref ref-type="bibr" rid="CR22">22</xref>]. Schizophrenic groups exhibit reduced pitch variation [<xref ref-type="bibr" rid="CR23">23</xref>], increased pauses [<xref ref-type="bibr" rid="CR24">24</xref>], and poverty of content [<xref ref-type="bibr" rid="CR25">25</xref>]. The number and duration of pauses are closely related to the evaluation of affective flattening [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR26">26</xref>, <xref ref-type="bibr" rid="CR27">27</xref>].</p>
    <p id="Par27">In general, most existing methods [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR28">28</xref>–<xref ref-type="bibr" rid="CR38">38</xref>] analyzed schizophrenic speech using feature engineering techniques, which were achieved by extracting fluency features, intensity-related features, spectrum-related features, and so on. These studies had proved that speech can be viewed as an automated biomarker for the diagnosis of schizophrenia. However, owing to the limitation in the amount of data and the difficulties in effective feature extraction, it is still difficult to propose a robust model. In this work, the Schizophrenia network (Sch-net) based on a convolutional neural network (CNN) is proposed to achieve the end-to-end schizophrenia detection based on speech signals. The proposed Sch-net can avoid the problems of feature extraction. The contributions of our work can be summarized as follows: <list list-type="order"><list-item><p id="Par28">This work proposes the Sch-net to detect schizophrenia based on speech signals. To the best of our knowledge, this is the first work to detect schizophrenic speech using CNN-based architecture.</p></list-item><list-item><p id="Par29">The proposed model adds the skip connection to the backbone network. It enriches the information via merging low-level feature maps with high-level feature maps, which avoids the manual feature extraction procedure.</p></list-item><list-item><p id="Par30">The proposed model utilizes the convolutional block attention module (CBAM). The CBAM performs the automatic feature selection function by giving learnable weights to the features in the feature maps.</p></list-item><list-item><p id="Par31">The proposed Sch-net is validated on the schizophrenic speech data set and specific language impairment (SLI) speech database. Experimental results have demonstrated that our method can provide aids for the diagnosis of schizophrenia and SLI.</p></list-item></list></p>
  </sec>
  <sec id="Sec2">
    <title>Related works</title>
    <p id="Par32">The detection of disordered speech in schizophrenia has been studied for the last few decades. Previous studies [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR28">28</xref>–<xref ref-type="bibr" rid="CR38">38</xref>] are mainly achieved based on feature engineering. In this section, we will review the related studies from the perspective of features. The features extracted can be roughly divided into two categories, time-domain features, and spectrum-related features.</p>
    <p id="Par33">Time-domain features: Schizophrenic patient with negative symptoms usually exhibits incoherent language that can be described by time-domain features, including pitch-related features, fluency features, and intensity-related features. (1) <italic>Pitch-related features:</italic> Pitch is the fundamental frequency of vocal cord vibration for voiced initial consonants and some unvoiced initial consonants [<xref ref-type="bibr" rid="CR39">39</xref>]. Pitch-related features are commonly used in analyzing the flat affect in schizophrenia. [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR28">28</xref>–<xref ref-type="bibr" rid="CR34">34</xref>]. Studies [<xref ref-type="bibr" rid="CR28">28</xref>, <xref ref-type="bibr" rid="CR30">30</xref>–<xref ref-type="bibr" rid="CR32">32</xref>] demonstrate that schizophrenic speech is characterized by less variability in vocal pitch than normal speech. (2)<italic> Fluency features:</italic> The incoherent expression in schizophrenia usually manifests as more pauses and a longer duration of pauses. Fluency features are employed to distinguish schizophrenic groups and controls in recent studies [<xref ref-type="bibr" rid="CR30">30</xref>, <xref ref-type="bibr" rid="CR35">35</xref>, <xref ref-type="bibr" rid="CR36">36</xref>], such as the number of pauses and natural turns, the duration of pauses, the proportion of silence and speaking, and speaking rate. (3) <italic>Intensity-related features:</italic> Voice intensity is an intuitive indicator for conveying emotional information in human communication [<xref ref-type="bibr" rid="CR40">40</xref>]. Previous studies [<xref ref-type="bibr" rid="CR28">28</xref>, <xref ref-type="bibr" rid="CR30">30</xref>, <xref ref-type="bibr" rid="CR32">32</xref>] calculate the intensity-related features based on the variability of energy per second/syllable, and the experimental results demonstrate that the voice intensity of patients with schizophrenia has less variation than that of controls.</p>
    <p id="Par34">Spectrum-related features: Spectrum-related features generally refer to the measurements computed based on the spectrum that contains time- and frequency-domain information. Spectrum-related features describe the energy distribution and the vocal tract characteristics during speech production. The typical spectrum-related features, such as formants, auditory-based spectral features, and spectral envelope features, have been proven to be effective for schizophrenia detection [<xref ref-type="bibr" rid="CR32">32</xref>, <xref ref-type="bibr" rid="CR33">33</xref>, <xref ref-type="bibr" rid="CR37">37</xref>, <xref ref-type="bibr" rid="CR38">38</xref>]. (1) <italic>Formants:</italic> Formant is the descriptor that reflects the resonance frequency of the vocal tract. Compton et.al [<xref ref-type="bibr" rid="CR32">32</xref>] demonstrate that the range of the second formant for schizophrenic speech is smaller than that for controls. Chhabra et.al [<xref ref-type="bibr" rid="CR37">37</xref>] conclude that patient with schizophrenia reduces the use of formant dispersion in the similarity-dissimilarity ratings. (2) <italic>Auditory-based spectral features and spectral envelope features:</italic> Auditory-based spectral features refer to the spectral parameters that are computed based on human auditory characteristics, and spectral envelope features refer to the envelope and its variants of the spectrum. Mel-frequency cepstral coefficient (MFCC) is one typical auditory-based spectral feature, and linear prediction coefficient (LPC) is a commonly used spectral envelope feature. MFCC is gained using Mel-frequency filters, in which the center frequency is computed according to the human auditory characteristics. LPC is calculated to estimate the resonance characteristics of the vocal tract during speech production. Studies [<xref ref-type="bibr" rid="CR33">33</xref>, <xref ref-type="bibr" rid="CR38">38</xref>] use MFCCs and LPCs to analyze the characteristics of schizophrenic speech. Results in [<xref ref-type="bibr" rid="CR38">38</xref>] show that the MFCC and LPC scores of schizophrenic speech are significantly lower and higher than those of controls, respectively.</p>
    <p id="Par35">Low-level acoustic features mentioned above [<xref ref-type="bibr" rid="CR28">28</xref>–<xref ref-type="bibr" rid="CR38">38</xref>] are generally extracted using OpenSMILE, pyAudioAnalysis, openEAR, and signal processing techniques. Classification experiments are conducted using classifiers (such as k-Nearest Neighbors, Decision Trees, Naive Bayes), combined with cross-validation (such as k-fold cross-validation and leave-one-out cross-validation). Studies [<xref ref-type="bibr" rid="CR30">30</xref>, <xref ref-type="bibr" rid="CR32">32</xref>–<xref ref-type="bibr" rid="CR36">36</xref>] have achieved 64–93% accuracy on schizophrenia detection tasks using 8–98 schizophrenic patients and 7–102 controls.</p>
  </sec>
  <sec id="Sec3">
    <title>Results</title>
    <p id="Par36">To demonstrate the effectiveness of the proposed model, comprehensive experiments are conducted. We first describe the schizophrenic speech data set and implementation details. Next, the ablation studies are presented to demonstrate the advantages of each component in the proposed Sch-net. Then comparisons with state-of-the-art methods based on feature engineering and deep learning techniques are conducted and analyzed. The network visualization is also presented using Grad-CAM. Finally, to further validate the generalization of proposed method, the classification experiments on the LANNA children speech database are conducted.</p>
    <sec id="Sec4">
      <title>Schizophrenic data set</title>
      <p id="Par37">Our study has 28 schizophrenic patients (18 females and 10 males) and 28 matched healthy controls (18 females and 10 males). The schizophrenic group is with a mean age of 40.6 years (SD 9.4 years), and the control group is with a mean age of 36.5 years (SD 9.1 years). All subjects are native Mandarin speakers, and they have no past or current disease affecting the speaking process. Patients were recruited from the Psychiatry Department of the Mental Health Center, Sichuan University. This department is one of the four major mental health centers in China. The schizophrenic group was diagnosed by clinicians based on the Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5) that outlines the concise and explicit criteria for the diagnosis of schizophrenia [<xref ref-type="bibr" rid="CR41">41</xref>]. All subjects provided the written informed consent.</p>
      <p id="Par38">The data set is composed of audio signals that are recorded in a 16-bit mono/dual-format at a sampling rate of 44.1kHz. Participants are asked to achieve the reading task. There are four texts with calm, happiness, anger, and fear sentiments, and each text comprises 8–10 sentences. We select a fixed sentence for each emotional recording, and the transcriptions of speech signals are listed in Table <xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Text for speech recording in Mandarin and its corresponded English translation</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Emotion</th><th align="left">Text (Mandarin)</th><th align="left">Text (English)</th></tr></thead><tbody><tr><td align="left">Calm</td><td align="left">Ta yi nian si ji dou ke yi kai hua, hua duo yi ban shi hong se huo fen se de.</td><td align="left">It can bloom all year round, and the flowers are generally red or pink.</td></tr><tr><td align="left">Anger</td><td align="left">Gen ni shuo le duo shao ci le, bu xu wan wo de wan! Kan ba, wan bei da sui le! Ni zhen de shi yao qi si wo!</td><td align="left">I told you so many times that you are not allowed to play with my bowls! Look, the bowl is shattered! You are really mad at me!</td></tr><tr><td align="left">Fear</td><td align="left">Ma ma, dui bu qi, wo...wo...wo bu shi gu yi de.</td><td align="left">Mom, I’m sorry, I...I...I didn’t mean it!</td></tr><tr><td align="left">Happiness</td><td align="left">Ha ha, tai hao la! Tai hao la! Ma ma, ma ma, wo kao le 98 fen!</td><td align="left">Awesome, it’s awesome! Mom, Mom, I got 98 points!</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec5">
      <title>Implementation details</title>
      <p id="Par39">In this study, all audios are converted to spectrograms using the Short-time Fourier Transform (STFT) method. To improve the invariance properties to geometric perturbations and noise, data augmentation methods are utilized, including random crop, random rotation, random rescaling, random Gaussian noise, masking blocks of frequency channels [<xref ref-type="bibr" rid="CR42">42</xref>], and masking blocks of time steps [<xref ref-type="bibr" rid="CR42">42</xref>].</p>
      <p id="Par40">The input image of the Sch-net is with the size of 128<inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M2"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="12938_2021_915_Article_IEq1.gif"/></alternatives></inline-formula>256 pixels. Table <xref rid="Tab2" ref-type="table">2</xref> shows the Sch-net architecture details. In this architecture, the size of each filter in Conv layers is set as 3 <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M4"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="12938_2021_915_Article_IEq2.gif"/></alternatives></inline-formula> 3. There are 64, 128, 256, 512 filters in the first to the fourth Conv layers, respectively. In addition, there are 512 filters in the three skip connections. The convolved images are normalized using a ReLU activation in Conv blocks. The max pooling and average pooling in pooling layers are obtained every 2 <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M6"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="12938_2021_915_Article_IEq3.gif"/></alternatives></inline-formula> 2, with a stride of 2. In the CBAM, 2048 filters of size 7 <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M8"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="12938_2021_915_Article_IEq4.gif"/></alternatives></inline-formula> 7 are used to highlight effective features. The highlighted features are convolved with 512 filters of size 3 <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M10"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="12938_2021_915_Article_IEq5.gif"/></alternatives></inline-formula> 3. In the FC neural network, there are 512 neurons in the first hidden layer and 2 neurons in the second layer. The final output is a vector of probabilities that the input sample will belong to each class.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Sch-net architecture details</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Layer</th><th align="left">Dimension</th></tr></thead><tbody><tr><td align="left">Conv1</td><td align="left">2×[3×3(64 filters)]</td></tr><tr><td align="left">Conv2</td><td align="left">2×[3×3(128 filters)]</td></tr><tr><td align="left">Conv3</td><td align="left">2×[3×3(256 filters)]</td></tr><tr><td align="left">Conv4</td><td align="left">2×[3×3(512 filters)]</td></tr><tr><td align="left">Conv5-8</td><td align="left">3×3(512 filters)</td></tr><tr><td align="left">Max-pooling</td><td align="left">2×2</td></tr><tr><td align="left">Average-pooling</td><td align="left">2×2</td></tr><tr><td align="left">CBAM</td><td align="left">7×7 (2048 filters)</td></tr><tr><td align="left">FC</td><td align="left">1×1×512, 1×1×2 (two hidden layers)</td></tr></tbody></table></table-wrap></p>
      <p id="Par41">In all experiments, the binary cross-entropy is adopted as the loss function, and Adam [<xref ref-type="bibr" rid="CR43">43</xref>] is used as the optimization algorithm. All experiments are implemented based on the PyTorch framework [<xref ref-type="bibr" rid="CR44">44</xref>] and trained on a workstation with Intel(R) Xeon(R) CPU E5-2680 v4 2.40 GHz processors and an NVIDIA Tesla P40 (24 GB) installed. The network is trained using batch size 16 for 50 epochs. The initial learning rate is set to 0.0003 and decreases by 10 times after 25 epochs. <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cdot$$\end{document}</tex-math><mml:math id="M12"><mml:mo>·</mml:mo></mml:math><inline-graphic xlink:href="12938_2021_915_Article_IEq6.gif"/></alternatives></inline-formula></p>
    </sec>
    <sec id="Sec6">
      <title>Ablation studies</title>
      <p id="Par42">In this subsection, the effectiveness of our network is verified. The Sch-net’s backbone network is based on CNN, with adding skip connections to enrich the feature information. In addition, the CBAM is applied to emphasize the more effective features with bigger weights. For this ablation study, we evaluate the contributions of the two key components to discriminate schizophrenic patients from healthy controls. To evaluate the performance of Sch-net and its components (backbone, skip connection, and CBAM), we run 30 iterations of tenfold cross-validation and compute seven metrics (accuracy, precision, recall, f1-score, sensitivity, specificity, and Area Under ROC Curve (AUC)) for each model. The 95% Confidence Intervals (CIs) for the metrics are listed in Table <xref rid="Tab3" ref-type="table">3</xref>, and the box plots of classification accuracies are shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Overall performance of schizophrenic speech detection using Sch-net and its components (backbone, skip connection (SC), and CBAM)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Evaluated indicators</th><th align="left" colspan="4">95% CI</th></tr><tr><th align="left">Backbone</th><th align="left">Backbone + SC</th><th align="left">Backbone + CBAM</th><th align="left">Sch-net (ours)</th></tr></thead><tbody><tr><td align="left" rowspan="2">Accuracy</td><td align="left">0.9323</td><td align="left">0.9494</td><td align="left">0.9563</td><td align="left">0.9768</td></tr><tr><td align="left">(0.9295,0.9351)</td><td align="left">(0.9460,0.9528)</td><td align="left">(0.9534,0.9591)</td><td align="left">(0.9739,0.9797)</td></tr><tr><td align="left" rowspan="2">Precision</td><td align="left">0.9480</td><td align="left">0.9634</td><td align="left">0.9513</td><td align="left">0.9639</td></tr><tr><td align="left">(0.9445,0.9515)</td><td align="left">(0.9564,0.9704)</td><td align="left">(0.9458,0.9568)</td><td align="left">(0.9585,0.9693)</td></tr><tr><td align="left" rowspan="2">Recall</td><td align="left">0.9149</td><td align="left">0.9348</td><td align="left">0.9622</td><td align="left">0.9908</td></tr><tr><td align="left">(0.9100,0.9197)</td><td align="left">(0.9326,0.9370)</td><td align="left">(0.9556,0.9688)</td><td align="left">(0.9898,0.9918)</td></tr><tr><td align="left" rowspan="2">F1-score</td><td align="left">0.9311</td><td align="left">0.9487</td><td align="left">0.9565</td><td align="left">0.9771</td></tr><tr><td align="left">(0.9280,0.9341)</td><td align="left">(0.9456,0.9519)</td><td align="left">(0.9536,0.9594)</td><td align="left">(0.9743,0.9799)</td></tr><tr><td align="left" rowspan="2">Sensitivity</td><td align="left">0.9176</td><td align="left">0.9619</td><td align="left">0.9902</td><td align="left">0.9914</td></tr><tr><td align="left">(0.9131,0.9221)</td><td align="left">(0.9581,0.9657)</td><td align="left">(0.9847,0.9956)</td><td align="left">(0.9863,0.9964)</td></tr><tr><td align="left" rowspan="2">Specificity</td><td align="left">0.9488</td><td align="left">0.9601</td><td align="left">0.9494</td><td align="left">0.9738</td></tr><tr><td align="left">(0.9415,0.9561)</td><td align="left">(0.9513,0.9689)</td><td align="left">(0.9437,0.9551)</td><td align="left">(0.9656,0.9820)</td></tr><tr><td align="left" rowspan="2">AUC</td><td align="left">0.9593</td><td align="left">0.9892</td><td align="left">0.9902</td><td align="left">0.9978</td></tr><tr><td align="left">(0.9577,0.9609)</td><td align="left">(0.9859,0.9924)</td><td align="left">(0.9880,0.9924)</td><td align="left">(0.9965,0.9990)</td></tr></tbody></table></table-wrap><fig id="Fig1"><label>Fig. 1</label><caption><p>Box plots of accuracy for classifying schizophrenic speech and controls using Sch-net and its components (backbone, skip connection (SC), and CBAM)</p></caption><graphic xlink:href="12938_2021_915_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par43">In each box plot in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, there are five points (the median, the upper and lower quartiles, and the minimum and maximum values) to display the distribution of classification accuracies for each model. As can be seen in Table <xref rid="Tab3" ref-type="table">3</xref> and Fig. <xref rid="Fig1" ref-type="fig">1</xref>, the skip connection enriches the information of feature maps and improves the classification accuracy by 1.71% on the schizophrenic speech data set. The CBAM selects the meaningful features for classification and improves accuracy by 2.40%. Significant improvement of 4.45% for classifying schizophrenic speech and normal speech is achieved when adding skip connections and CBAM to the backbone network. The proposed Sch-net combines the advantages of skip connection and CBAM, achieving better performance on the classification task.</p>
    </sec>
    <sec id="Sec7">
      <title>Comparison with the models based on feature engineering and classifiers</title>
      <p id="Par44">Previous studies about automatic schizophrenic speech detection [<xref ref-type="bibr" rid="CR28">28</xref>–<xref ref-type="bibr" rid="CR38">38</xref>] are almost based on feature engineering and pattern recognition technology. In this subsection, the performances of the combination of feature engineering and classifiers are displayed and analyzed. Four types of acoustic features are extracted, which are time-domain features, FFT-based spectral features, auditory-based spectral features, and spectral envelope features. Four classifiers are adopted, including random forest (RF), k-nearest neighbor (KNN), support vector machine (SVM), and linear discriminant analysis (LDA).</p>
      <p id="Par45">Time-domain features used in this work contain short-term energy (STE), pitch, and fluency features. The STE feature of speech signals reflects the amplitude variation, and the pitch indicates the vocal cords vibration in the pronunciation process. The fluency feature can reflect the degree of coherence in expression. Considering the reduced syntactic complexity and abnormal pauses in schizophrenic speech, five fluency features (total recording time, the total length of voice segments, the ratio of voice segments, max duration of pauses, mean length of syllables) are employed to construct a feature set.</p>
      <p id="Par46">FFT-based features refer to the features computed by the STFT. In this work, two FFT-based features (spectrogram and long-term average spectrum (LTAS)) are adopted in this work. The LTAS describes the resonance characteristics by computing the short-term Fourier magnitude spectra [<xref ref-type="bibr" rid="CR45">45</xref>], which have shown promising performance in speech sentiment analysis and pathological speech analysis [<xref ref-type="bibr" rid="CR46">46</xref>–<xref ref-type="bibr" rid="CR48">48</xref>].</p>
      <p id="Par47">Auditory-based features are proposed to simulate the clinical diagnosis. Schizophrenia is diagnosed by clinicians through a comprehensive evaluation of speech and behaviors. Therefore, speech signals are necessary to be analyzed by combining with human auditory characteristics. In this study, MFCC and its modification, Gammatone cepstral coefficient (GTCC) [<xref ref-type="bibr" rid="CR49">49</xref>], are extracted to detect schizophrenia. The MFCCs and GTCCs are computed using a series of filters that are designed according to the frequency response characteristics of the human auditory system.</p>
      <p id="Par48">The spectral envelope feature is also commonly used to describe the vocal tract characteristics in speech production. In this work, LP and its deformations, stabilized weighted linear prediction (SWLP) [<xref ref-type="bibr" rid="CR50">50</xref>] and extended weighted linear prediction (XLP) [<xref ref-type="bibr" rid="CR51">51</xref>], are tested on the schizophrenic speech data set. The SWLP is an improved WLP that is proposed to model speech by applying the temporal weighting of the square of the residual signal [<xref ref-type="bibr" rid="CR50">50</xref>]. The XLP is a further generation of WLP and SWLP methods, which allows temporal weighting on a finer time scale [<xref ref-type="bibr" rid="CR51">51</xref>]. The SWLP and XLP have performed well on the speech recognition tasks and pathological speech detection [<xref ref-type="bibr" rid="CR52">52</xref>, <xref ref-type="bibr" rid="CR53">53</xref>].</p>
      <p id="Par49">The features mentioned above combined with four classifiers are tested on schizophrenic speech data set. The overall performances are listed in Table <xref rid="Tab4" ref-type="table">4</xref> using accuracy, precision, recall, and F1-score. The bold font in Table <xref rid="Tab4" ref-type="table">4</xref> represents the highest value in each type of features using different classifiers. It can be seen that fluency feature, spectrogram, GTCC, and XLP achieve the highest F1-score in its corresponding feature group. When compared the results in Tables <xref rid="Tab3" ref-type="table">3</xref> and <xref rid="Tab4" ref-type="table">4</xref>, it can be seen that the proposed Sch-net has a better performance than the models based on feature engineering and classifiers.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Performance of feature engineering and classifiers on schizophrenic speech detection</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="3">Classifier</th><th align="left" rowspan="3"/><th align="left" colspan="10">Feature</th></tr><tr><th align="left" colspan="3">Time-domain feature</th><th align="left" colspan="2">FFT-based spectral feature</th><th align="left" colspan="2">Auditory-based spectral feature</th><th align="left" colspan="3">Spectral envelope feature</th></tr><tr><th align="left">STE</th><th align="left">Pitch</th><th align="left">Fluency feature</th><th align="left">LTAS</th><th align="left">Spectrogram</th><th align="left">MFCC</th><th align="left">GTCC</th><th align="left">LP</th><th align="left">SWLP</th><th align="left">XLP</th></tr></thead><tbody><tr><td align="left" rowspan="4">RF</td><td align="left">Accuracy</td><td align="left">0.7686</td><td align="left">0.5935</td><td align="left"><bold>0.8213</bold></td><td align="left">0.6464</td><td align="left"><bold>0.8972</bold></td><td align="left">0.8043</td><td align="left"><bold>0.8791</bold></td><td align="left">0.9245</td><td align="left">0.9377</td><td align="left"><bold>0.9423</bold></td></tr><tr><td align="left">Precision</td><td align="left">0.6251</td><td align="left">0.5847</td><td align="left"><bold>0.8281</bold></td><td align="left">0.6052</td><td align="left"><bold>0.8946</bold></td><td align="left">0.7818</td><td align="left"><bold>0.8487</bold></td><td align="left">0.9055</td><td align="left"><bold>0.9319</bold></td><td align="left">0.9282</td></tr><tr><td align="left">Recall</td><td align="left">0.7126</td><td align="left">0.5754</td><td align="left"><bold>0.8103</bold></td><td align="left">0.5545</td><td align="left"><bold>0.9103</bold></td><td align="left">0.8577</td><td align="left"><bold>0.9289</bold></td><td align="left">0.9549</td><td align="left">0.9466</td><td align="left"><bold>0.9644</bold></td></tr><tr><td align="left">F1-score</td><td align="left">0.6306</td><td align="left">0.6322</td><td align="left"><bold>0.8133</bold></td><td align="left">0.5513</td><td align="left"><bold>0.8972</bold></td><td align="left">0.8144</td><td align="left"><bold>0.8856</bold></td><td align="left">0.9280</td><td align="left">0.9391</td><td align="left"><bold>0.9453</bold></td></tr><tr><td align="left" rowspan="4">KNN</td><td align="left">Accuracy</td><td align="left"><bold>0.7723</bold></td><td align="left">0.5385</td><td align="left">0.7390</td><td align="left">0.7504</td><td align="left"><bold>0.8974</bold></td><td align="left">0.8626</td><td align="left"><bold>0.8753</bold></td><td align="left">0.9204</td><td align="left">0.9287</td><td align="left"><bold>0.9375</bold></td></tr><tr><td align="left">Precision</td><td align="left">0.6410</td><td align="left">0.5308</td><td align="left"><bold>0.7050</bold></td><td align="left">0.6489</td><td align="left"><bold>0.8566</bold></td><td align="left">0.8977</td><td align="left"><bold>0.9043</bold></td><td align="left">0.8939</td><td align="left">0.9117</td><td align="left"><bold>0.9196</bold></td></tr><tr><td align="left">Recall</td><td align="left">0.6063</td><td align="left">0.5597</td><td align="left"><bold>0.7985</bold></td><td align="left">0.6152</td><td align="left"><bold>0.9636</bold></td><td align="left">0.8312</td><td align="left"><bold>0.8494</bold></td><td align="left"><bold>0.9640</bold></td><td align="left">0.9549</td><td align="left">0.9636</td></tr><tr><td align="left">F1-score</td><td align="left">0.6123</td><td align="left">0.5382</td><td align="left"><bold>0.7418</bold></td><td align="left">0.6211</td><td align="left"><bold>0.9046</bold></td><td align="left">0.8591</td><td align="left"><bold>0.8700</bold></td><td align="left">0.9257</td><td align="left">0.9315</td><td align="left"><bold>0.9398</bold></td></tr><tr><td align="left" rowspan="4">SVM</td><td align="left">Accuracy</td><td align="left"><bold>0.7905</bold></td><td align="left">0.5172</td><td align="left">0.7746</td><td align="left">0.7358</td><td align="left"><bold>0.9024</bold></td><td align="left">0.8625</td><td align="left"><bold>0.8929</bold></td><td align="left">0.9164</td><td align="left">0.9291</td><td align="left"><bold>0.9334</bold></td></tr><tr><td align="left">Precision</td><td align="left">0.6447</td><td align="left">0.5087</td><td align="left"><bold>0.7657</bold></td><td align="left">0.6556</td><td align="left"><bold>0.8741</bold></td><td align="left">0.8555</td><td align="left"><bold>0.8762</bold></td><td align="left">0.8980</td><td align="left"><bold>0.9183</bold></td><td align="left">0.9126</td></tr><tr><td align="left">Recall</td><td align="left">0.5999</td><td align="left">0.4767</td><td align="left"><bold>0.7875</bold></td><td align="left">0.5435</td><td align="left"><bold>0.9549</bold></td><td align="left">0.8929</td><td align="left"><bold>0.9198</bold></td><td align="left">0.9470</td><td align="left">0.9466</td><td align="left"><bold>0.9636</bold></td></tr><tr><td align="left">F1-score</td><td align="left">0.6155</td><td align="left">0.4644</td><td align="left"><bold>0.7627</bold></td><td align="left">0.5813</td><td align="left"><bold>0.9091</bold></td><td align="left">0.8689</td><td align="left"><bold>0.8960</bold></td><td align="left">0.9206</td><td align="left">0.9317</td><td align="left"><bold>0.9356</bold></td></tr><tr><td align="left" rowspan="4">LDA</td><td align="left">Accuracy</td><td align="left"><bold>0.7858</bold></td><td align="left">0.5087</td><td align="left">0.7452</td><td align="left">0.7314</td><td align="left"><bold>0.8385</bold></td><td align="left">0.8887</td><td align="left"><bold>0.9198</bold></td><td align="left">0.9026</td><td align="left">0.9069</td><td align="left"><bold>0.9109</bold></td></tr><tr><td align="left">Precision</td><td align="left">0.6447</td><td align="left">0.4625</td><td align="left"><bold>0.7083</bold></td><td align="left">0.6622</td><td align="left"><bold>0.8053</bold></td><td align="left">0.9394</td><td align="left"><bold>0.9479</bold></td><td align="left">0.8963</td><td align="left"><bold>0.9053</bold></td><td align="left">0.8868</td></tr><tr><td align="left">Recall</td><td align="left">0.5898</td><td align="left">0.5391</td><td align="left"><bold>0.7522</bold></td><td align="left">0.5380</td><td align="left"><bold>0.9095</bold></td><td align="left">0.8474</td><td align="left"><bold>0.8933</bold></td><td align="left">0.9198</td><td align="left">0.9111</td><td align="left"><bold>0.9462</bold></td></tr><tr><td align="left">F1-score</td><td align="left">0.6093</td><td align="left">0.4710</td><td align="left"><bold>0.7104</bold></td><td align="left">0.5821</td><td align="left"><bold>0.8498</bold></td><td align="left">0.8807</td><td align="left"><bold>0.9161</bold></td><td align="left">0.9060</td><td align="left">0.9079</td><td align="left"><bold>0.9146</bold></td></tr></tbody></table></table-wrap></p>
      <sec id="Sec8">
        <title>Time-domain feature</title>
        <p id="Par50">As shown in Table <xref rid="Tab4" ref-type="table">4</xref>, the F1-score of schizophrenic speech detection using the STE reaches 0.6306. Owing to the difficulty in expression for schizophrenic patients, the intensity of schizophrenic speech tends to be lower than that of controls. The STE feature can describe the intensity of speech, but it may be influenced by the different distances between the recording equipment and speakers. Thus, the performance of the STE feature is not as good as the fluency feature.</p>
        <p id="Par51">Though studies [<xref ref-type="bibr" rid="CR28">28</xref>, <xref ref-type="bibr" rid="CR30">30</xref>–<xref ref-type="bibr" rid="CR32">32</xref>] have proved that there are significant differences in pitch between schizophrenic speech and normal speech, the pitch gains the worst performance among time-domain features. The results are consistent with the results in [<xref ref-type="bibr" rid="CR30">30</xref>, <xref ref-type="bibr" rid="CR37">37</xref>], in which the distribution of pitch shows no significant differences between the two groups.</p>
        <p id="Par52">Fluency feature performs well on the schizophrenic speech detection, owing to the disordered thought and language impairments of patients [<xref ref-type="bibr" rid="CR54">54</xref>]. The cognitive impairment also contributes to the incoherence of speech.</p>
      </sec>
      <sec id="Sec9">
        <title>FFT-based spectral feature</title>
        <p id="Par53">The LTAS achieves 62.11% accuracy on the schizophrenic speech data set. The LTAS is calculated as the average of a spectrogram, reflecting the spectrum of glottal source and vocal tract [<xref ref-type="bibr" rid="CR55">55</xref>]. Results in [<xref ref-type="bibr" rid="CR30">30</xref>] have shown that schizophrenic speech has lower variations in energy than normal speech. The unexpected accuracy using LTAS may be caused by the average operation that eliminates the differences in variations between two groups.</p>
        <p id="Par54">The spectrogram achieves better performance than the LTAS, which is the time-frequency representation of speech. It not only contains the energy distribution in frequency bands but also reflects the pitch and formant information. It has been proven that schizophrenic speech have less variability in pitch and voice intensity, smaller range of second formant than normal speech [<xref ref-type="bibr" rid="CR28">28</xref>, <xref ref-type="bibr" rid="CR30">30</xref>–<xref ref-type="bibr" rid="CR32">32</xref>]. Thus, the spectrogram covers more effective features for discriminating patients from controls than the LTAS does.</p>
      </sec>
      <sec id="Sec10">
        <title>Auditory-based spectral feature</title>
        <p id="Par55">The GTCC achieves a better performance than the MFCC on the schizophrenic speech detection task, which is caused using different auditory filters. The MFCC is computed based on a series of triangular bandpass filters with equal bandwidth. The GTCC employs the Gammatone filters to model the human auditory response, which are with equivalent rectangular bandwidth [<xref ref-type="bibr" rid="CR56">56</xref>]. The use of Gammatone filters minimizes the loss of spectrum information and increases the correlation among the outputs of the filters [<xref ref-type="bibr" rid="CR56">56</xref>]. Therefore, the GTCC contains more effective information to detect schizophrenia than the MFCC.</p>
      </sec>
      <sec id="Sec11">
        <title>Spectral envelope feature</title>
        <p id="Par56">The F1-scores of schizophrenic speech detection using LP, SWLP and XLP are above 0.9. The SWLP and XLP have slightly better results than LP. The results of spectral envelop features are gained when the order of LP is set as 38 [<xref ref-type="bibr" rid="CR57">57</xref>, <xref ref-type="bibr" rid="CR58">58</xref>]. Results in [<xref ref-type="bibr" rid="CR32">32</xref>, <xref ref-type="bibr" rid="CR37">37</xref>] have shown that formant is an indicator to distinguish schizophrenic speech from controls. The LP reflects the characteristics of the vocal tract, such as the frequency of formants. However, the LP analysis relies on the excitation signal, which is usually affected by the harmonics. The SWLP reduces the effect by composing the temporal weights on the closed-phase interval of the glottal cycle [<xref ref-type="bibr" rid="CR53">53</xref>]. In addition, the XLP improves the time scale on the spectral envelop by weighting each lagged speech signal separately [<xref ref-type="bibr" rid="CR53">53</xref>]. The SWLP and XLP highlight the formant information that can be used to distinguish patients from controls. Thus, the SWLP and XLP achieve better performance on classifying schizophrenia and controls than the LP.</p>
      </sec>
    </sec>
    <sec id="Sec12">
      <title>Comparison with classic deep neural networks</title>
      <p id="Par57">In this subsection, comparisons between five neural networks and our model are conducted. The five networks are AlexNet [<xref ref-type="bibr" rid="CR59">59</xref>], VGG16 [<xref ref-type="bibr" rid="CR60">60</xref>], ResNet34 [<xref ref-type="bibr" rid="CR61">61</xref>], DenseNet121 [<xref ref-type="bibr" rid="CR62">62</xref>], and Xception [<xref ref-type="bibr" rid="CR63">63</xref>], which are commonly used for speech recognition and classification tasks [<xref ref-type="bibr" rid="CR64">64</xref>–<xref ref-type="bibr" rid="CR68">68</xref>]. AlexNet [<xref ref-type="bibr" rid="CR59">59</xref>] is the winner of the ImageNet Large Scale Visual Recognition Challenge in 2012, which reduces overfitting and controls the model complexity of the FC layers using dropout. VGG16 [<xref ref-type="bibr" rid="CR60">60</xref>] is a good benchmark architecture for classification tasks, which is consisted of 13 Conv layers, 3 FC layers, and 5 pooling layers. ResNet34 [<xref ref-type="bibr" rid="CR61">61</xref>] is introduced to alleviate the degradation problem caused by increasing stacked layers via adding shortcut connections. To reduce the impact on vanishing gradient, the feed-forward fashion in the connection between each layer to every other layer is used in DenseNet121 [<xref ref-type="bibr" rid="CR62">62</xref>]. DenseNet121 also can strengthen the propagation of features and reduce the number of parameters [<xref ref-type="bibr" rid="CR62">62</xref>]. To obtain fast convergence and good performance on the model’s expressive ability, Xception [<xref ref-type="bibr" rid="CR63">63</xref>] replaces the inception modules with depthwise separable convolutions in deep CNN. Table <xref rid="Tab5" ref-type="table">5</xref> lists the 95% CIs for seven metrics of classifying schizophrenic speech and normal speech using the five deep neural networks and our method. Fig. <xref rid="Fig2" ref-type="fig">2</xref> presents the box plots of the classification accuracies for the models.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Performance of schizophrenic speech detection using classic deep neural networks and the proposed Sch-net</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Evaluated indicators</th><th align="left" colspan="6">95% CI</th></tr><tr><th align="left">AlexNet</th><th align="left">VGG16</th><th align="left">ResNet34</th><th align="left">DenseNet121</th><th align="left">Xception</th><th align="left">Sch-net (ours)</th></tr></thead><tbody><tr><td align="left" rowspan="2">Accuracy</td><td align="left">0.9272</td><td align="left">0.9247</td><td align="left">0.9439</td><td align="left">0.9469</td><td align="left">0.9503</td><td align="left"><bold>0.9768</bold></td></tr><tr><td align="left">(0.9249,0.9295)</td><td align="left">(0.9225,0.9269)</td><td align="left">(0.9398,0.9480)</td><td align="left">(0.9449,0.9489)</td><td align="left">(0.9482,0.9524)</td><td align="left"><bold>(0.9739,0.9797)</bold></td></tr><tr><td align="left" rowspan="2">Precision</td><td align="left">0.9279</td><td align="left">0.8937</td><td align="left">0.9074</td><td align="left">0.9555</td><td align="left">0.9462</td><td align="left"><bold>0.9639</bold></td></tr><tr><td align="left">(0.9226,0.9333)</td><td align="left">(0.8900,0.8973)</td><td align="left">(0.9024,0.9124)</td><td align="left">(0.9516,0.9594)</td><td align="left">(0.9421,0.9503)</td><td align="left"><bold>(0.9585,0.9693)</bold></td></tr><tr><td align="left" rowspan="2">Recall</td><td align="left">0.9268</td><td align="left">0.9643</td><td align="left">0.9890</td><td align="left">0.9375</td><td align="left">0.9551</td><td align="left"><bold>0.9908</bold></td></tr><tr><td align="left">(0.9251,0.9285)</td><td align="left">(0.9643,0.9643)</td><td align="left">(0.9822,0.9958)</td><td align="left">(0.9375,0.9375)</td><td align="left">(0.9545,0.9556)</td><td align="left"><bold>(0.9898,0.9918)</bold></td></tr><tr><td align="left" rowspan="2">F1-score</td><td align="left">0.9273</td><td align="left">0.9276</td><td align="left">0.9463</td><td align="left">0.9464</td><td align="left">0.9506</td><td align="left"><bold>0.9771</bold></td></tr><tr><td align="left">(0.9252,0.9293)</td><td align="left">(0.9257,0.9295)</td><td align="left">(0.9423,0.9503)</td><td align="left">(0.9445,0.9483)</td><td align="left">(0.9486,0.9526)</td><td align="left"><bold>(0.9743,0.9799)</bold></td></tr><tr><td align="left" rowspan="2">Sensitivity</td><td align="left">0.6399</td><td align="left">0.8795</td><td align="left">0.9798</td><td align="left">0.9262</td><td align="left">0.9482</td><td align="left"><bold>0.9914</bold></td></tr><tr><td align="left">(0.6244,0.6554)</td><td align="left">(0.8715,0.8875)</td><td align="left">(0.9725,0.9870)</td><td align="left">(0.9244,0.9280)</td><td align="left">(0.9409,0.9556)</td><td align="left"><bold>(0.9863,0.9964)</bold></td></tr><tr><td align="left" rowspan="2">Specificity</td><td align="left">0.8747</td><td align="left">0.9268</td><td align="left">0.9399</td><td align="left"><bold>0.9938</bold></td><td align="left">0.9646</td><td align="left">0.9738</td></tr><tr><td align="left">(0.8564,0.893)</td><td align="left">(0.9164,0.9372)</td><td align="left">(0.9331,0.9467)</td><td align="left"><bold>(0.9910,0.9965)</bold></td><td align="left">(0.9567,0.9724)</td><td align="left">(0.9656,0.9820)</td></tr><tr><td align="left" rowspan="2">AUC</td><td align="left">0.7935</td><td align="left">0.9447</td><td align="left">0.9888</td><td align="left">0.9908</td><td align="left">0.9924</td><td align="left"><bold>0.9978</bold></td></tr><tr><td align="left">(0.7868,0.8003)</td><td align="left">(0.9422,0.9472)</td><td align="left">(0.9855,0.9921)</td><td align="left">(0.9899,0.9917)</td><td align="left">(0.9912,0.9936)</td><td align="left"><bold>(0.9965,0.9990)</bold></td></tr></tbody></table></table-wrap><fig id="Fig2"><label>Fig. 2</label><caption><p>Box plots of accuracy for classifying schizophrenic speech and controls using five neural networks and Sch-net</p></caption><graphic xlink:href="12938_2021_915_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par58">As shown in Table <xref rid="Tab5" ref-type="table">5</xref> and Fig. <xref rid="Fig2" ref-type="fig">2</xref>, the accuracies of schizophrenic speech detection using AlexNet and VGG16 are 92.72% (95% CI: 92.49–92.95%) and 92.47% (95% CI: 92.25–92.69%), respectively. The depth of AlexNet and VGG16 is shallow, contributing to the insufficient information in feature maps. ResNet34 achieves 94.39% (95% CI: 93.98–94.80%) accuracy on the schizophrenic speech data set, owing to the introduction of the residual module. DenseNet121 and Xception gain slightly better results than ResNet34, owing to the networks not only adopt the shortcut connections but also utilize dense connection/depthwise separable convolutions to make more efficient use of model parameters. The proposed Sch-net in this work achieves a better performance than the five networks, because it can gain the local and global features simultaneously via CBAM and skip connections. The feature map contains more abundant information to better distinguish schizophrenia from controls.</p>
    </sec>
    <sec id="Sec13">
      <title>Network visualization using Grad-CAM</title>
      <p id="Par59">In recent years, deep learning methods have already achieved high accuracy that approaches the manual diagnosis accuracy in many fields through improving the computing capabilities and expanding the data set. It can simplify and speed up the diagnosis, and reduce the workload of doctors. However, the process of generating predicted labels from input data is still uninterpretable. To make the decision-making process in deep learning transparent, this work applies the Grad-CAM [<xref ref-type="bibr" rid="CR69">69</xref>] to Sch-net using speech samples from schizophrenic group and healthy group. Grad-CAM is a visualization method to show the importance of each neuron for the classification using the gradient information in the last Conv layer [<xref ref-type="bibr" rid="CR69">69</xref>]. The Grad-CAM highlights the more discriminative parts as brighter regions in the heatmap. We attempt to consider how the Sch-net works on making good use of features, through observing the spectrogram and activation maps. In this subsection, the input spectrogram and its corresponding activation map generated in the last Conv layer of normal speech and schizophrenic speech are shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>.<fig id="Fig3"><label>Fig. 3</label><caption><p>Spectrogram and corresponding activation map of normal speech and schizophrenic speech in four emotions. <bold>a</bold> The spectrogram and corresponding activation map of normal speech and schizophrenic speech in calm emotion. <bold>b</bold> The spectrogram and corresponding activation map of normal speech and schizophrenic speech in anger emotion. <bold>c</bold> The spectrogram and corresponding activation map of normal speech and schizophrenic speech in fear emotion. <bold>d</bold> The spectrogram and corresponding activation map of normal speech and schizophrenic speech in happiness emotion</p></caption><graphic xlink:href="12938_2021_915_Fig3_HTML" id="MO3"/></fig></p>
      <p id="Par60">In Fig. <xref rid="Fig3" ref-type="fig">3</xref>, spectrograms of normal speech and schizophrenic speech are shown in a and c, respectively. Activation maps of normal speech and schizophrenic speech are depicted in b and d. The brighter region in the spectrogram means more energy concentrated, and that in the activation map means larger weight located.</p>
      <p id="Par61">As shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>a, c, schizophrenic speech and normal speech have different distributions of concentrated energy in the spectrogram. Through the horizontal comparison, two findings of two groups can be seen in this figure, which can be listed as follows:<list list-type="order"><list-item><p id="Par62">The energy concentration in the frequency domain of schizophrenic speech is almost below 5000 Hz, while normal speech has a wider range of energy concentration bands, that can be extended from 8000 to 10,000 Hz. Blunted affect is a typical symptom in schizophrenia [<xref ref-type="bibr" rid="CR70">70</xref>]. Patients with negative symptoms may speak with a dull monotone voice [<xref ref-type="bibr" rid="CR71">71</xref>], resulting in a small range of the energy concentration region. While healthy controls have a more flexible emotional expression. The angry, fearful and happy speech exhibit a higher intonation, faster speed rate, and more energy in higher frequencies [<xref ref-type="bibr" rid="CR72">72</xref>]. And the sad speech changes slowly and has high energy concentration in lower frequencies [<xref ref-type="bibr" rid="CR73">73</xref>]. Thus, normal speech has a wider range of energy distribution than schizophrenic speech.</p></list-item><list-item><p id="Par63">It can be seen that schizophrenic speech and normal speech both have concentrated energy region and apparent formant horizontal stripes in the low-frequency bands below 2000 Hz. The difference between the two groups is the shape of formant horizontal stripes. For schizophrenic speech, the stripes are almost continuous, which is inconsistent with the energy distribution characteristics of vowels and consonants. The vowels have energy concentration in both low- and high-frequency range [<xref ref-type="bibr" rid="CR74">74</xref>]. The unvoiced consonants mainly have high-frequency energy components, and they rarely have formants [<xref ref-type="bibr" rid="CR75">75</xref>]. According to the texture used in this work, the continuous-time speech signals comprise both vowels and consonants. Therefore, there are supposed to show a short disappearance of formant horizontal stripes on the spectrogram. It can be guessed that the continuous stripes in the spectrogram of schizophrenic speech may be caused by the incorrect placement of articulators during speech production. The wrong articulation process leads to the unvoiced consonants are produced as voiced consonants.</p></list-item></list></p>
      <p id="Par64">Observing both the spectrogram and its corresponding activation map in Fig. <xref rid="Fig3" ref-type="fig">3</xref>, it can be seen that the Sch-net can capture the features in high-frequency bands for normal speech, and can give larger weights to the features in low-frequency bands for schizophrenic speech. The results of Sch-net are consistent with human visual perception, which is difficult to achieve using the models based on feature engineering. The Sch-net has excellent learning ability to extract features, and it achieves better performances on schizophrenic speech detection than traditional feature engineering models adopted in this work.</p>
    </sec>
    <sec id="Sec14">
      <title>Further validation of the proposed Sch-net using LANNA children speech database</title>
      <p id="Par65">Schizophrenia is a neurodevelopmental disorder affecting the language expression of patients [<xref ref-type="bibr" rid="CR76">76</xref>]. SLI, also termed development dysphasia, is described as a neurological disorder of the brain [<xref ref-type="bibr" rid="CR77">77</xref>–<xref ref-type="bibr" rid="CR80">80</xref>]. Patients with SLI exhibit delayed language acquisition [<xref ref-type="bibr" rid="CR81">81</xref>], slower linguistic processing [<xref ref-type="bibr" rid="CR82">82</xref>], and difficulties in grammar or specific subcomponents of grammar [<xref ref-type="bibr" rid="CR83">83</xref>, <xref ref-type="bibr" rid="CR84">84</xref>]. To further validate model effectiveness and generalization, the Sch-net is tested on LANNA children speech database [<xref ref-type="bibr" rid="CR85">85</xref>] for the classification of patients with SLI and healthy controls in this subsection.</p>
      <p id="Par66">LANNA children speech database [<xref ref-type="bibr" rid="CR85">85</xref>] is the first and only publicly open speech corpora for children with SLI, which comprises 2173 speech signals from 54 children with SLI (aged from 6 to 11 years) and 1680 speech signals from 44 controls (aged from 6 to 10 years). This data set is composed of 13 parts: vowels, consonants, syllables, six types of words, sentences, auditory differentiation, and description of the picture. Audios were recorded in a schoolroom and a consulting room using Dictaphone, MD and microphone. The background noise in natural environments affects the quality of speech signals, leading to difficulties in speech signal processing.</p>
      <p id="Par67">Previous studies [<xref ref-type="bibr" rid="CR85">85</xref>–<xref ref-type="bibr" rid="CR91">91</xref>] had demonstrated that speech can be viewed as a symbol of diagnosing SLI. In [<xref ref-type="bibr" rid="CR85">85</xref>–<xref ref-type="bibr" rid="CR87">87</xref>], 1582 acoustic features were extracted from 34 low-level descriptors and its 21 statistical functionals. The features were given as inputs of the SVM, achieving 96.94% accuracy on the LANNA children speech database. In [<xref ref-type="bibr" rid="CR88">88</xref>], Gaussian posteriorgrams trained on MFCC features were employed to discriminate patients with SLI and healthy controls. The kernel extreme learning machine were trained with the speech signals, and it performed an accuracy of 99.41% on the test data. Apart from MFCC, in [<xref ref-type="bibr" rid="CR89">89</xref>], Tonnetz and Chroma were calculated, combined with SVM, RF and Recurrent Neural Network to detect SLI. The Tonnetz and Chroma reached accuracies of 70% and 71%, respectively. In the four studies [<xref ref-type="bibr" rid="CR85">85</xref>–<xref ref-type="bibr" rid="CR89">89</xref>], high accuracies had been achieved for speaker-dependent classification.</p>
      <p id="Par68">In contrast, some methods were proposed for speaker-independent classification in [<xref ref-type="bibr" rid="CR90">90</xref>, <xref ref-type="bibr" rid="CR91">91</xref>]. The top-20 LPC features were selected from 408 LPCs using Mann–Whitney<italic> U</italic> test and Spearman’s correlation in [<xref ref-type="bibr" rid="CR90">90</xref>], which achieved an accuracy of 97.90% on the SLI detection task. In [<xref ref-type="bibr" rid="CR91">91</xref>], a feed-forward neural network was proposed for classifying patients with SLI and healthy controls. The glottal features and MFCCs were adopted as the inputs of the network and the classification accuracy reached up to 98.82%.</p>
      <p id="Par69">In this subsection, fivefold cross-validation is employed. SLI data set is divided with 80% for training and 20% for testing. Table <xref rid="Tab6" ref-type="table">6</xref> gives the classification results using state-of-the-art methods, deep neural networks and the proposed Sch-net. As can be seen, our method outperforms the classic deep neural network and state-of-the-art methods. The proposed Sch-net can extract discriminant features of speech signals for classifying healthy individuals and those suffered from SLI.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Results of SLI detection using state-of-the-art methods, classic deep neural networks and the proposed Sch-net</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="2">Method</th><th align="left">Accuracy</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-score</th></tr></thead><tbody><tr><td align="left" rowspan="5">State-of-the-art method</td><td align="left">Grill [<xref ref-type="bibr" rid="CR85">85</xref>–<xref ref-type="bibr" rid="CR87">87</xref>]</td><td align="left">0.9694</td><td align="left">1.0000</td><td align="left">0.9474</td><td align="left">0.9730</td></tr><tr><td align="left">Ramarao [<xref ref-type="bibr" rid="CR88">88</xref>]</td><td align="left">0.9941</td><td align="left"><bold>-</bold></td><td align="left"><bold>-</bold></td><td align="left"><bold>-</bold></td></tr><tr><td align="left">Slogrove [<xref ref-type="bibr" rid="CR89">89</xref>]</td><td align="left">0.9800</td><td align="left">0.9900</td><td align="left">0.9900</td><td align="left">0.9900</td></tr><tr><td align="left">Sharma [<xref ref-type="bibr" rid="CR90">90</xref>]</td><td align="left">0.9790</td><td align="left">-</td><td align="left">-</td><td align="left"><bold>-</bold></td></tr><tr><td align="left">Reddy [<xref ref-type="bibr" rid="CR91">91</xref>]</td><td align="left">0.9882</td><td align="left"><bold>-</bold></td><td align="left"><bold>-</bold></td><td align="left"><bold>-</bold></td></tr><tr><td align="left" rowspan="6">Deep Neural Network</td><td align="left">AlexNet</td><td align="left">0.9132</td><td align="left">0.9585</td><td align="left">0.8810</td><td align="left">0.9181</td></tr><tr><td align="left">VGG16</td><td align="left">0.9230</td><td align="left">0.9897</td><td align="left">0.8787</td><td align="left">0.9309</td></tr><tr><td align="left">ResNet34</td><td align="left">0.9329</td><td align="left">0.9489</td><td align="left">0.9286</td><td align="left">0.9386</td></tr><tr><td align="left">DenseNet121</td><td align="left">0.9461</td><td align="left">0.9397</td><td align="left">0.9643</td><td align="left">0.9518</td></tr><tr><td align="left">Xception</td><td align="left">0.9622</td><td align="left">0.9514</td><td align="left">0.9863</td><td align="left">0.9685</td></tr><tr><td align="left">Sch-net (our)</td><td align="left">0.9952</td><td align="left">0.9979</td><td align="left">0.9937</td><td align="left">0.9958</td></tr></tbody></table></table-wrap></p>
    </sec>
  </sec>
  <sec id="Sec15">
    <title>Conclusions</title>
    <p id="Par70">In this work, we propose an Sch-net neural network for automatic detection of schizophrenia based on speech signals. This is the first work to detect schizophrenic speech using deep learning techniques. The Sch-net is performed using a set of convolutional layers. The global and local features are merged using skip connections, and the effective features are highlighted by using CBAM. In the experiments, the advantages of embedding the SC and CBAM into the backbone architecture are verified in ablation studies. The proposed model can learn the differences in speech patterns between patients and healthy controls automatically, avoiding the requirements of domain knowledge for designers. The comparisons with the models based on feature engineering and classic deep neural networks are conducted on a schizophrenic speech data set that contains 28 schizophrenic patients and 28 healthy controls. The experimental results show that the Sch-net has achieved 97.68% accuracy. In addition, we visualize how the model performs on extracting features given an input spectrogram. The Grad-CAM heatmaps show the region that the Sch-net focuses on is consistent with human visual perception. Finally, the proposed method is further validated on the open access LANNA children speech database, achieving 99.52% accuracy on classifying patients with SLI and healthy controls.</p>
    <p id="Par71">The clinical diagnosis of schizophrenia is made by expertise psychiatrists based on a full psychiatric assessment, which depends on the experience of psychiatrists. The reports are often affected by the patients’ retrospective recall bias and cognitive limitations. Moreover, the diagnosis is high-cost and time-consuming, and the high patients-to-clinicians ratio leads to the heavy workload of clinicians. The proposed model can serve as an aid to psychiatrists for the diagnosis of schizophrenia. It can automatically discriminate schizophrenic speech from controls, which may be helpful to the preliminary screening for schizophrenia. In addition, it can provide low-cost and long-term monitoring for patients with schizophrenia, and reduce the workload of clinicians.</p>
    <p id="Par72">Our work still has several limitations. First, the proposed model can only achieve the classification of patients and healthy controls, but cannot assess disease severity. Second, the generalization of the proposed model needs to be further verified. Future work will seek to perform extensive validation using a larger number of databases that record speech signals of patients with psychological/neurological disorders.</p>
  </sec>
  <sec id="Sec16">
    <title>Methods</title>
    <p id="Par73">
      <fig id="Fig4">
        <label>Fig. 4</label>
        <caption>
          <p>Architecture of Sch-net for automatic schizophrenic speech detection</p>
        </caption>
        <graphic xlink:href="12938_2021_915_Fig4_HTML" id="MO4"/>
      </fig>
    </p>
    <p id="Par74">In this work, we have developed a CNN-based architecture, termed Sch-net, to classify schizophrenic speech and normal speech. The architecture of the proposed model is depicted in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. The input is the spectrogram containing time–frequency domain information of speech signals. There are 12 convolutional (Conv) layers, 6 pooling layers, skip connections, an attention module and a fully connected (FC) layer. The FC layer is composed of two hidden layers. A softmax function is employed to the output of the FC layer, and the output of the softmax is the classification result of speech samples. The backbone network and two essential components (skip connections and CBAM) of Sch-net are described below.</p>
    <sec id="Sec17">
      <title>Backbone network of Sch-net</title>
      <p id="Par75">
        <fig id="Fig5">
          <label>Fig. 5</label>
          <caption>
            <p>Diagram of the backbone network of Sch-net</p>
          </caption>
          <graphic xlink:href="12938_2021_915_Fig5_HTML" id="MO5"/>
        </fig>
      </p>
      <p id="Par76">The backbone network of Sch-net shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref> is consisted of Conv layer, pooling layer, batch normalization (BN) component, rectified linear unit (ReLU) and FC layer. When spectrogram is given as the input of Sch-net, local features in spectrogram are extracted via Conv layer. The dimension of features and the amount of computation are reduced in the pooling layer via max pooling operation [<xref ref-type="bibr" rid="CR92">92</xref>]. As the number of hidden layers increases, the network would suffer from the gradient vanishing and exploding problems. To address these problems, the BN layer and ReLU activation function are adopted. The introduction of BN components can also speed up the convergence, cut down the regularization process, and enable to train the network with a larger learning rate [<xref ref-type="bibr" rid="CR93">93</xref>, <xref ref-type="bibr" rid="CR94">94</xref>]. ReLU is a typical activation function in deep learning, which works better than sigmoid and tanh activation functions in speech recognition tasks [<xref ref-type="bibr" rid="CR95">95</xref>, <xref ref-type="bibr" rid="CR96">96</xref>]. It removes the negative values in the feature map and is identity for all positive values [<xref ref-type="bibr" rid="CR97">97</xref>]. The networks can be trained effectively using the ReLU even without pre-training [<xref ref-type="bibr" rid="CR98">98</xref>]. At the end of the network, the FC layer and softmax function are employed to achieve the classification task. The FC layer is essential to transfer CNN-based network visual representation in classification tasks [<xref ref-type="bibr" rid="CR99">99</xref>]. Each node in the FC layer is connected to all activation values in the previous layers.</p>
    </sec>
    <sec id="Sec18">
      <title>Skip connections</title>
      <p id="Par77">The backbone network of Sch-net can extract the local features in spectrogram via shallow layers and max-pooling operation. There is no evidence that schizophrenic patients have a special pattern in pronunciation or schizophrenic speech has prominent local acoustic features. Thus, global features are supposed to be extracted for schizophrenic speech detection. To retain more original and global information in the input feature map, average pooling operation and skip connections are added to the backbone network of Sch-net. Average pooling considers all the values in the batch that has an equal size with the pooling kernel. Skip connection allows the low-level feature map to skip some layers in the neural network and merge with high-level feature maps [<xref ref-type="bibr" rid="CR100">100</xref>]. This connection combines the features after max-pooling and average-pooling, superimposed into a feature. Skip connections expand the dimensions of features in the network, providing more information for the classification task. The diagram of the backbone network of Sch-net with skip connections is given in Fig.<xref rid="Fig6" ref-type="fig">6</xref>.<fig id="Fig6"><label>Fig. 6</label><caption><p>Diagram of the backbone network + skip connections of Sch-net</p></caption><graphic xlink:href="12938_2021_915_Fig6_HTML" id="MO6"/></fig></p>
    </sec>
    <sec id="Sec19">
      <title>Attention mechanism</title>
      <p id="Par78">The output of skip connections contains low-level and high-level features. To emphasize the meaningful features and suppress the unnecessary ones for the classification task, an attention module is added to the backbone network. The output of the attention module is calculated as the weighted sum of the input values [<xref ref-type="bibr" rid="CR101">101</xref>]. The bigger weights mean the more attention would be paid to the input. This work adopts a lightweight and general module, CBAM [<xref ref-type="bibr" rid="CR102">102</xref>], to improve the performance of the network. The CBAM is composed of channel and spatial attention modules [<xref ref-type="bibr" rid="CR102">102</xref>]. The channel attention module focuses on “what” is the effective part in the feature map by utilizing max-pooling and average-pooling with a shared network [<xref ref-type="bibr" rid="CR102">102</xref>]. The spatial attention module tells “where” to focus or suppress by employing a Conv layer [<xref ref-type="bibr" rid="CR102">102</xref>]. The CBAM used in the Sch-net can effectively refine the intermediate feature map with negligible computation and overheads.</p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>CNN</term>
        <def>
          <p id="Par5">Convolutional Neural Network</p>
        </def>
      </def-item>
      <def-item>
        <term>CBAM</term>
        <def>
          <p id="Par6">Convolutional Block Attention Module</p>
        </def>
      </def-item>
      <def-item>
        <term>MFCC</term>
        <def>
          <p id="Par7">Mel-frequency Cepstral Coefficient</p>
        </def>
      </def-item>
      <def-item>
        <term>LP</term>
        <def>
          <p id="Par8">Linear Prediction</p>
        </def>
      </def-item>
      <def-item>
        <term>LPC</term>
        <def>
          <p id="Par9">Linear Prediction Coefficient</p>
        </def>
      </def-item>
      <def-item>
        <term>Conv</term>
        <def>
          <p id="Par10">Convolutional</p>
        </def>
      </def-item>
      <def-item>
        <term>FC</term>
        <def>
          <p id="Par11">Fully connected</p>
        </def>
      </def-item>
      <def-item>
        <term>BN</term>
        <def>
          <p id="Par12">Batch Normalization</p>
        </def>
      </def-item>
      <def-item>
        <term>ReLU</term>
        <def>
          <p id="Par13">Rectified Linear Unit</p>
        </def>
      </def-item>
      <def-item>
        <term>FFT</term>
        <def>
          <p id="Par14">Fast Fourier Transform</p>
        </def>
      </def-item>
      <def-item>
        <term>RF</term>
        <def>
          <p id="Par15">Random Forest</p>
        </def>
      </def-item>
      <def-item>
        <term>KNN</term>
        <def>
          <p id="Par16">K-Nearest Neighbor</p>
        </def>
      </def-item>
      <def-item>
        <term>SVM</term>
        <def>
          <p id="Par17">Support Vector Machine</p>
        </def>
      </def-item>
      <def-item>
        <term>LDA</term>
        <def>
          <p id="Par18">Linear Discriminant Analysis</p>
        </def>
      </def-item>
      <def-item>
        <term>STE</term>
        <def>
          <p id="Par19">Short-term Energy</p>
        </def>
      </def-item>
      <def-item>
        <term>LTAS</term>
        <def>
          <p id="Par20">Long-term Average Spectrum</p>
        </def>
      </def-item>
      <def-item>
        <term>GTCC</term>
        <def>
          <p id="Par21">Gammatone Cepstral Coefficient</p>
        </def>
      </def-item>
      <def-item>
        <term>WLP</term>
        <def>
          <p id="Par22">Weighted Linear Prediction</p>
        </def>
      </def-item>
      <def-item>
        <term>SWLP</term>
        <def>
          <p id="Par23">Stabilized Weighted Linear Prediction</p>
        </def>
      </def-item>
      <def-item>
        <term>XLP</term>
        <def>
          <p id="Par24">Extended Weighted Linear Prediction</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>Jia Fu and Sen Yang contributed equally to this work</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Not applicable.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors' contributions</title>
    <p>JF and SY presented the ideas, designed and conducted relevant experiments in the manuscript. JF and FH wrote the manuscript. LH and JZ are responsible for guiding the idea and final review of the manuscript. YL and XX collected the samples used for the experiments. All authors contributed to analyzing the data and reviewing the literature, and revising the manuscript. All authors read and approved the manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This research was supported by the Department of Science and Technology of Sichuan Province, China (Grant Nos. 2019YFS0236 and 2019YJ0523).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>Not applicable.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar2">
      <title>Ethics approval and consent to participate</title>
      <p id="Par79">Not applicable.</p>
    </notes>
    <notes id="FPar3" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par80">The authors declare that they have no competing interests.</p>
    </notes>
    <notes id="FPar4">
      <title>Consent for publication</title>
      <p id="Par81">Not applicable.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kaur</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Diagnosis of human psychological disorders using supervised learning and nature-inspired computing techniques: a meta-analysis</article-title>
        <source>J Med Syst</source>
        <year>2019</year>
        <volume>43</volume>
        <issue>7</issue>
        <fpage>1</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="doi">10.1007/s10916-019-1341-2</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gautam</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Prevalence and diagnosis of neurological disorders using different deep learning techniques: a meta-analysis</article-title>
        <source>J Med Syst</source>
        <year>2020</year>
        <volume>44</volume>
        <issue>2</issue>
        <fpage>1</fpage>
        <lpage>24</lpage>
        <pub-id pub-id-type="doi">10.1007/s10916-019-1519-7</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Morin</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Edinger</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Beaulieu-Bonneau</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ivers</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Krystal</surname>
            <given-names>AD</given-names>
          </name>
          <name>
            <surname>Guay</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Bélanger</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Cartwright</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Simmons</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Lamy</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Effectiveness of sequential psychological and medication therapies for insomnia disorder: a randomized clinical trial</article-title>
        <source>JAMA Psychiatry</source>
        <year>2020</year>
        <volume>77</volume>
        <issue>11</issue>
        <fpage>1107</fpage>
        <lpage>1115</lpage>
        <pub-id pub-id-type="doi">10.1001/jamapsychiatry.2020.1767</pub-id>
        <pub-id pub-id-type="pmid">32639561</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ismail</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>KY</given-names>
          </name>
          <name>
            <surname>Sutrisno Tanjung</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ahmad Jelani</surname>
            <given-names>IA</given-names>
          </name>
          <name>
            <surname>Abdul Latiff</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Abdul Razak</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Ahmad Shauki</surname>
            <given-names>NI</given-names>
          </name>
        </person-group>
        <article-title>The prevalence of psychological distress and its association with coping strategies among medical interns in Malaysia: a national-level cross-sectional study</article-title>
        <source>Asia-Pacif Psychiatry</source>
        <year>2021</year>
        <volume>13</volume>
        <issue>2</issue>
        <fpage>12417</fpage>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McGrath</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Saha</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Chant</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Welham</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Schizophrenia: a concise overview of incidence, prevalence, and mortality</article-title>
        <source>Epidemiol Rev</source>
        <year>2008</year>
        <volume>30</volume>
        <issue>1</issue>
        <fpage>67</fpage>
        <lpage>76</lpage>
        <pub-id pub-id-type="doi">10.1093/epirev/mxn001</pub-id>
        <pub-id pub-id-type="pmid">18480098</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Lavretsky H. History of Schizophrenia as a Psychiatric Disorder, 2008.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>DiPiro</surname>
            <given-names>JT</given-names>
          </name>
          <name>
            <surname>Talbert</surname>
            <given-names>RL</given-names>
          </name>
          <name>
            <surname>Yee</surname>
            <given-names>GC</given-names>
          </name>
          <name>
            <surname>Wells</surname>
            <given-names>BG</given-names>
          </name>
          <name>
            <surname>Posey</surname>
            <given-names>LM</given-names>
          </name>
        </person-group>
        <source>Pharmacotherapy: a pathophysiologic approach</source>
        <year>2014</year>
        <edition>9</edition>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>McGraw-Hill Medical</publisher-name>
        <fpage>1019</fpage>
        <lpage>1046</lpage>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marder</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Galderisi</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>The current conceptualization of negative symptoms in schizophrenia</article-title>
        <source>World Psychiatry</source>
        <year>2017</year>
        <volume>16</volume>
        <issue>1</issue>
        <fpage>14</fpage>
        <lpage>24</lpage>
        <pub-id pub-id-type="doi">10.1002/wps.20385</pub-id>
        <pub-id pub-id-type="pmid">28127915</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Murphy</surname>
            <given-names>BP</given-names>
          </name>
          <name>
            <surname>Chung</surname>
            <given-names>YC</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>TW</given-names>
          </name>
          <name>
            <surname>McGorry</surname>
            <given-names>PD</given-names>
          </name>
        </person-group>
        <article-title>Pharmacological treatment of primary negative symptoms in schizophrenia: a systematic review</article-title>
        <source>Schizophr Res</source>
        <year>2006</year>
        <volume>88</volume>
        <issue>1–3</issue>
        <fpage>5</fpage>
        <lpage>25</lpage>
        <pub-id pub-id-type="doi">10.1016/j.schres.2006.07.002</pub-id>
        <pub-id pub-id-type="pmid">16930948</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mucci</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Merlotti</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Üçok</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Aleman</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Galderisi</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Primary and persistent negative symptoms: concepts, assessments and neurobiological bases</article-title>
        <source>Schizophr Res.</source>
        <year>2017</year>
        <volume>186</volume>
        <fpage>19</fpage>
        <lpage>28</lpage>
        <pub-id pub-id-type="doi">10.1016/j.schres.2016.05.014</pub-id>
        <pub-id pub-id-type="pmid">27242069</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kirkpatrick</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Buchanan</surname>
            <given-names>RW</given-names>
          </name>
          <name>
            <surname>Ross</surname>
            <given-names>DE</given-names>
          </name>
          <name>
            <surname>Carpenter</surname>
            <given-names>WT</given-names>
          </name>
        </person-group>
        <article-title>A separate disease within the syndrome of schizophrenia</article-title>
        <source>Arch General Psychiatry</source>
        <year>2001</year>
        <volume>58</volume>
        <issue>2</issue>
        <fpage>165</fpage>
        <lpage>171</lpage>
        <pub-id pub-id-type="doi">10.1001/archpsyc.58.2.165</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Milev</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Ho</surname>
            <given-names>BC</given-names>
          </name>
          <name>
            <surname>Arndt</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Andreasen</surname>
            <given-names>NC</given-names>
          </name>
        </person-group>
        <article-title>Predictive values of neurocognition and negative symptoms on functional outcome in schizophrenia: a longitudinal first-episode study with 7-year follow-up</article-title>
        <source>Am J Psychiatry</source>
        <year>2005</year>
        <volume>162</volume>
        <issue>3</issue>
        <fpage>495</fpage>
        <lpage>506</lpage>
        <pub-id pub-id-type="doi">10.1176/appi.ajp.162.3.495</pub-id>
        <pub-id pub-id-type="pmid">15741466</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kurtz</surname>
            <given-names>MM</given-names>
          </name>
          <name>
            <surname>Moberg</surname>
            <given-names>PJ</given-names>
          </name>
          <name>
            <surname>Ragland</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Gur</surname>
            <given-names>RC</given-names>
          </name>
          <name>
            <surname>Gur</surname>
            <given-names>RE</given-names>
          </name>
        </person-group>
        <article-title>Symptoms versus neurocognitive test performance as predictors of psychosocial status in schizophrenia: a 1- and 4-year prospective study</article-title>
        <source>Schizophr Bull</source>
        <year>2005</year>
        <volume>31</volume>
        <issue>1</issue>
        <fpage>167</fpage>
        <lpage>174</lpage>
        <pub-id pub-id-type="doi">10.1093/schbul/sbi004</pub-id>
        <pub-id pub-id-type="pmid">15888434</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kirkpatrick</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Fenton</surname>
            <given-names>WS</given-names>
          </name>
          <name>
            <surname>Carpenter</surname>
            <given-names>WT</given-names>
          </name>
          <name>
            <surname>Marder</surname>
            <given-names>SR</given-names>
          </name>
        </person-group>
        <article-title>The nimh-matrics consensus statement on negative symptoms</article-title>
        <source>Schizophr Bull</source>
        <year>2006</year>
        <volume>32</volume>
        <issue>2</issue>
        <fpage>214</fpage>
        <lpage>219</lpage>
        <pub-id pub-id-type="doi">10.1093/schbul/sbj053</pub-id>
        <pub-id pub-id-type="pmid">16481659</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rabinowitz</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Levine</surname>
            <given-names>SZ</given-names>
          </name>
          <name>
            <surname>Garibaldi</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Bugarski-Kirola</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Berardo</surname>
            <given-names>CG</given-names>
          </name>
          <name>
            <surname>Kapur</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Negative symptoms have greater impact on functioning than positive symptoms in schizophrenia: analysis of catie data</article-title>
        <source>Schizophr Res</source>
        <year>2012</year>
        <volume>137</volume>
        <issue>1</issue>
        <fpage>147</fpage>
        <lpage>150</lpage>
        <pub-id pub-id-type="doi">10.1016/j.schres.2012.01.015</pub-id>
        <pub-id pub-id-type="pmid">22316568</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Low</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Bentley</surname>
            <given-names>KH</given-names>
          </name>
          <name>
            <surname>Ghosh</surname>
            <given-names>SS</given-names>
          </name>
        </person-group>
        <article-title>Automated assessment of psychiatric disorders using speech: a systematic review</article-title>
        <source>Laryngoscope Investig Otolaryngol</source>
        <year>2020</year>
        <volume>5</volume>
        <issue>1</issue>
        <fpage>96</fpage>
        <lpage>116</lpage>
        <pub-id pub-id-type="doi">10.1002/lio2.354</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>DeLisi</surname>
            <given-names>LE</given-names>
          </name>
        </person-group>
        <article-title>Speech disorder in schizophrenia: review of the literature and exploration of its relation to the uniquely human capacity for language</article-title>
        <source>Schizophr Bull</source>
        <year>2001</year>
        <volume>27</volume>
        <issue>3</issue>
        <fpage>481</fpage>
        <lpage>496</lpage>
        <pub-id pub-id-type="doi">10.1093/oxfordjournals.schbul.a006889</pub-id>
        <pub-id pub-id-type="pmid">11596849</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Branch</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Ardekani</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Bertisch</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Hicks</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>DeLisi</surname>
            <given-names>LE</given-names>
          </name>
        </person-group>
        <article-title>fmri study of language activation in schizophrenia, schizoaffective disorder and in individuals genetically at high risk</article-title>
        <source>Schizophr Res</source>
        <year>2007</year>
        <volume>96</volume>
        <issue>1–3</issue>
        <fpage>14</fpage>
        <lpage>24</lpage>
        <pub-id pub-id-type="doi">10.1016/j.schres.2007.07.013</pub-id>
        <pub-id pub-id-type="pmid">17719745</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Branch</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Bertisch</surname>
            <given-names>HC</given-names>
          </name>
          <name>
            <surname>Brown</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Szulc</surname>
            <given-names>KU</given-names>
          </name>
          <name>
            <surname>Ardekani</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>DeLisi</surname>
            <given-names>LE</given-names>
          </name>
        </person-group>
        <article-title>An fmri study of language processing in people at high genetic risk for schizophrenia</article-title>
        <source>Schizophr Res</source>
        <year>2007</year>
        <volume>91</volume>
        <issue>1–3</issue>
        <fpage>62</fpage>
        <lpage>72</lpage>
        <pub-id pub-id-type="doi">10.1016/j.schres.2006.12.016</pub-id>
        <pub-id pub-id-type="pmid">17306963</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cohen</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>Najolia</surname>
            <given-names>GM</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Dinzeo</surname>
            <given-names>TJ</given-names>
          </name>
        </person-group>
        <article-title>On the boundaries of blunt affect/alogia across severe mental illness: implications for research domain criteria</article-title>
        <source>Schizophr Res</source>
        <year>2012</year>
        <volume>140</volume>
        <fpage>41</fpage>
        <lpage>45</lpage>
        <pub-id pub-id-type="doi">10.1016/j.schres.2012.07.001</pub-id>
        <pub-id pub-id-type="pmid">22831770</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rosenstein</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Foltz</surname>
            <given-names>PW</given-names>
          </name>
          <name>
            <surname>DeLisi</surname>
            <given-names>LE</given-names>
          </name>
          <name>
            <surname>Elvevåg</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Language as a biomarker in those at high-risk for psychosis</article-title>
        <source>Schizophr Res</source>
        <year>2015</year>
        <volume>165</volume>
        <fpage>249</fpage>
        <lpage>250</lpage>
        <pub-id pub-id-type="doi">10.1016/j.schres.2015.04.023</pub-id>
        <pub-id pub-id-type="pmid">25956631</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Rockville M. Mental health: A report of the surgeon general. US Department of Health and Human Services. 1999.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Parola</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Simonsen</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bliksted</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Fusaroli</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>T138. acoustic patterns in schizophrenia: a systematic review and meta-analysis</article-title>
        <source>Schizophr Bull.</source>
        <year>2018</year>
        <volume>44</volume>
        <issue>Suppl–1</issue>
        <fpage>169</fpage>
        <pub-id pub-id-type="doi">10.1093/schbul/sby016.414</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stein</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Vocal alterations in schizophrenic speech</article-title>
        <source>J Nerv Ment Dis</source>
        <year>1993</year>
        <volume>181</volume>
        <issue>1</issue>
        <fpage>59</fpage>
        <lpage>62</lpage>
        <pub-id pub-id-type="doi">10.1097/00005053-199301000-00012</pub-id>
        <pub-id pub-id-type="pmid">8419517</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rezaii</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Walker</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Wolff</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>A machine learning approach to predicting psychosis using semantic density and latent content analysis</article-title>
        <source>NPJ Schizophr.</source>
        <year>2019</year>
        <volume>5</volume>
        <fpage>1</fpage>
        <lpage>2</lpage>
        <pub-id pub-id-type="doi">10.1038/s41537-019-0077-9</pub-id>
        <pub-id pub-id-type="pmid">30643138</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kring</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Alpert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Neale</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Harvey</surname>
            <given-names>PD</given-names>
          </name>
        </person-group>
        <article-title>A multimethod, multichannel assessment of affective flattening in schizophrenia</article-title>
        <source>Psychiatry Res</source>
        <year>1994</year>
        <volume>54</volume>
        <fpage>211</fpage>
        <lpage>222</lpage>
        <pub-id pub-id-type="doi">10.1016/0165-1781(94)90008-6</pub-id>
        <pub-id pub-id-type="pmid">7761554</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alpert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kotsaftis</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pouget</surname>
            <given-names>ER</given-names>
          </name>
        </person-group>
        <article-title>Speech fluency and schizophrenic negative signs</article-title>
        <source>Schizophr Bull</source>
        <year>1997</year>
        <volume>23</volume>
        <issue>2</issue>
        <fpage>171</fpage>
        <lpage>177</lpage>
        <pub-id pub-id-type="doi">10.1093/schbul/23.2.171</pub-id>
        <pub-id pub-id-type="pmid">9165627</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stassen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Albers</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Püschel</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Scharfetter</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Tewesmeier</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Woggon</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Speaking behavior and voice sound characteristics associated with negative schizophrenia</article-title>
        <source>J Psychiatric Res</source>
        <year>1995</year>
        <volume>29</volume>
        <fpage>277</fpage>
        <lpage>296</lpage>
        <pub-id pub-id-type="doi">10.1016/0022-3956(95)00004-O</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alpert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rosenberg</surname>
            <given-names>SD</given-names>
          </name>
          <name>
            <surname>Pouget</surname>
            <given-names>ER</given-names>
          </name>
          <name>
            <surname>Shaw</surname>
            <given-names>RJ</given-names>
          </name>
        </person-group>
        <article-title>Prosody and lexical accuracy in flat affect schizophrenia</article-title>
        <source>Psychiatry Res</source>
        <year>2000</year>
        <volume>97</volume>
        <issue>2</issue>
        <fpage>107</fpage>
        <lpage>118</lpage>
        <pub-id pub-id-type="doi">10.1016/S0165-1781(00)00231-6</pub-id>
        <pub-id pub-id-type="pmid">11166083</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rapčan</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>D’Arcy</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Yeap</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Afzal</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Thakore</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Reilly</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Acoustic and temporal analysis of speech: a potential biomarker for schizophrenia</article-title>
        <source>Med Eng Phy.</source>
        <year>2010</year>
        <volume>32</volume>
        <fpage>1074</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1016/j.medengphy.2010.07.013</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bernardini</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Lunden</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Covington</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Broussard</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Halpern</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Alolayan</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Crisafio</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pauselli</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Balducci</surname>
            <given-names>PM</given-names>
          </name>
          <name>
            <surname>Capulong</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Attademo</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Lucarini</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Salierno</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Natalicchi</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Quartesan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Compton</surname>
            <given-names>MT</given-names>
          </name>
        </person-group>
        <article-title>Associations of acoustically measured tongue/jaw movements and portion of time speaking with negative symptom severity in patients with schizophrenia in Italy and the United States</article-title>
        <source>Psychiatry Res</source>
        <year>2016</year>
        <volume>239</volume>
        <fpage>253</fpage>
        <lpage>258</lpage>
        <pub-id pub-id-type="doi">10.1016/j.psychres.2016.03.037</pub-id>
        <pub-id pub-id-type="pmid">27039009</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Compton</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lunden</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cleary</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Pauselli</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Alolayan</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Halpern</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Broussard</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Crisafio</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Capulong</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Balducci</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Bernardini</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Covington</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>The aprosody of schizophrenia: computationally derived acoustic phonetic underpinnings of monotone speech</article-title>
        <source>Schizophr Res</source>
        <year>2018</year>
        <volume>197</volume>
        <fpage>392</fpage>
        <lpage>399</lpage>
        <pub-id pub-id-type="doi">10.1016/j.schres.2018.01.007</pub-id>
        <pub-id pub-id-type="pmid">29449060</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Chakraborty D, Yang Z, Tahir Y, Maszczyk T, Dauwels J, Thalmann N, Zheng J, Maniam Y, Amirah N, Tan B, Lee J. Prediction of negative symptoms of schizophrenia from emotion related low-level speech signals. 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018;6024–6028.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Chakraborty D, Xu S, Yang Z, Chua Y, Tahir Y, Dauwels J, Thalmann N, Tan B, Lee J. Prediction of negative symptoms of schizophrenia from objective linguistic, acoustic and non-verbal conversational cues. In: 2018 International Conference on Cyberworlds (CW), 2018;pp. 280–283.</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Tahir Y, Chakraborty D, Dauwels J, Magnenat-Thalmann N, Thalmann D, Lee J. Non-verbal speech analysis of interviews with schizophrenic patients. 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016;5810–5814.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Gosztolya G, Bagi A, Szalóki S, Szendi I, Hoffmann I. Identifying schizophrenia based on temporal parameters in spontaneous speech. In: INTERSPEECH, 2018;pp. 3408–3412.</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chhabra</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Badcock</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Maybery</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Leung</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Voice identity discrimination in schizophrenia</article-title>
        <source>Neuropsychologia</source>
        <year>2012</year>
        <volume>50</volume>
        <fpage>2730</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2012.08.006</pub-id>
        <pub-id pub-id-type="pmid">22910275</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Gui</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Cui</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Clinical investigation of speech signal features among patients with schizophrenia</article-title>
        <source>Shanghai Arch Psychiatry</source>
        <year>2016</year>
        <volume>28</volume>
        <issue>2</issue>
        <fpage>95</fpage>
        <lpage>102</lpage>
        <pub-id pub-id-type="pmid">27605865</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Titze</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Riede</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Mau</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Predicting achievable fundamental frequency ranges in vocalization across species</article-title>
        <source>PLoS Comput Biol.</source>
        <year>2016</year>
        <volume>12</volume>
        <issue>6</issue>
        <fpage>e1004907</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004907</pub-id>
        <pub-id pub-id-type="pmid">27309543</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Nordström H. Emotional communication in the human voice. PhD thesis. 2019.</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Association</surname>
            <given-names>AP</given-names>
          </name>
          <etal/>
        </person-group>
        <source>Diagnostic and statistical manual of mental disorders (DSM-5®)</source>
        <year>2013</year>
        <publisher-loc>Washigton</publisher-loc>
        <publisher-name>American Psychiatric Pub</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Park DS, Chan W, Zhang Y, Chiu CC, Zoph B, Cubuk ED, Le QV. Specaugment: A simple data augmentation method for automatic speech recognition. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1904.08779">arXiv:1904.08779</ext-link>. 2019.</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Da K. A method for stochastic optimization. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">arXiv:1412.6980</ext-link> 2014.</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Paszke A, Gross S, Chintala S, Chanan G, Yang E, DeVito Z, Lin Z, Desmaison A, Antiga L, Lerer A. Automatic differentiation in pytorch. In: NIPS Workshop. 2017.</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Kinnunen T, Hautamäki V, Fränti P. On the use of long-term average spectrum in automatic speaker recognition. In: 5th Internat. Symposium on Chinese Spoken Language Processing (ISCSLP’06), 2006;pp. 559–567.</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Yenigalla P, Kumar A, Tripathi S, Singh C, Kar S, Vepa J. Speech emotion recognition using spectrogram and phoneme embedding, 2018;pp. 3688–3692.</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sundberg</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Salomão</surname>
            <given-names>GL</given-names>
          </name>
          <name>
            <surname>Scherer</surname>
            <given-names>KR</given-names>
          </name>
        </person-group>
        <article-title>Analyzing emotion expression in singing via flow glottograms, long-term-average spectra, and expert listener evaluation</article-title>
        <source>J Voice</source>
        <year>2019</year>
        <pub-id pub-id-type="doi">10.1016/j.jvoice.2019.08.007</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Abdel-Hamid</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Egyptian Arabic speech emotion recognition using prosodic, spectral and wavelet features</article-title>
        <source>Speech Commun</source>
        <year>2020</year>
        <volume>122</volume>
        <fpage>19</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="doi">10.1016/j.specom.2020.04.005</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <mixed-citation publication-type="other">Liu JM, You MY, Li GZ, Wang Z, Xu XH, Qiu Z, Xie WJ, An C, Chen SL. Cough signal recognition with gammatone cepstral coefficients, 2013;pp. 160–164.</mixed-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Magi</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Pohjalainen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bäckström</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Alku</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Stabilised weighted linear prediction</article-title>
        <source>Speech Commun</source>
        <year>2009</year>
        <volume>51</volume>
        <issue>5</issue>
        <fpage>401</fpage>
        <lpage>411</lpage>
        <pub-id pub-id-type="doi">10.1016/j.specom.2008.12.005</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">Pohjalainen J, Saeidi R, Kinnunen T, Alku P. Extended weighted linear prediction (xlp) analysis of speech and its application to speaker verification in adverse conditions. In: INTERSPEECH. 2010.</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other">Jouni Pohjalainen PA Carlo Magi. Enhancing noise robustness in automatic speech recognition using stabilized weighted linear prediction (swlp). In: ISCA Tutorial and Research Workshop (ITRW) on Speech Analysis and Processing for Knowledge Discovery. 2008.</mixed-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Automatic hypernasality grade assessment in cleft palate speech based on the spectral envelope method</article-title>
        <source>Biomed Eng</source>
        <year>2020</year>
        <volume>65</volume>
        <issue>1</issue>
        <fpage>73</fpage>
        <lpage>86</lpage>
        <pub-id pub-id-type="doi">10.1515/bmt-2018-0181</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cohen</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Alpert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nienow</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Dinzeo</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Docherty</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Computerized measurement of negative symptoms in schizophrenia</article-title>
        <source>J Psychiatric Res</source>
        <year>2008</year>
        <volume>42</volume>
        <fpage>827</fpage>
        <lpage>36</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jpsychires.2007.08.008</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tjaden</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Sussman</surname>
            <given-names>JE</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Wilding</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Long-term average spectral (ltas) measures of dysarthria and their relationship to perceived severity</article-title>
        <source>J Medi Speech Lang Pathol</source>
        <year>2010</year>
        <volume>18</volume>
        <issue>4</issue>
        <fpage>125</fpage>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Valero</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Alias</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Gammatone cepstral coefficients: biologically inspired features for non-speech audio classification</article-title>
        <source>IEEE Trans Multimedia</source>
        <year>2012</year>
        <volume>14</volume>
        <issue>6</issue>
        <fpage>1684</fpage>
        <lpage>1689</lpage>
        <pub-id pub-id-type="doi">10.1109/TMM.2012.2199972</pub-id>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <mixed-citation publication-type="other">Deller JR, Hansen JHL, Proakis JG. Discrete-time processing of speech signals. In: Institute of Electrical and Electronics Engineers. 2000.</mixed-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rah</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ko</surname>
            <given-names>YI</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>DW</given-names>
          </name>
        </person-group>
        <article-title>A noninvasive estimation of hypernasality using a linear predictive model</article-title>
        <source>Ann Biomed Eng</source>
        <year>2001</year>
        <volume>29</volume>
        <fpage>587</fpage>
        <lpage>94</lpage>
        <pub-id pub-id-type="doi">10.1114/1.1380422</pub-id>
        <pub-id pub-id-type="pmid">11501623</pub-id>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <mixed-citation publication-type="other">Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. In: Proceddings of the Advances in Neural Information Processing Systems, 2012;pp. 1097–1105.</mixed-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <mixed-citation publication-type="other">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. In: arXiv Preprint arXiv, 2014;pp. 1409–1556.</mixed-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <mixed-citation publication-type="other">He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016;pp. 770–778.</mixed-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <mixed-citation publication-type="other">Huang G, Liu Z, van der Maaten L, Weinberger KQ. Densely connected convolutional networks. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017;pp. 2261–2269.</mixed-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <mixed-citation publication-type="other">Chollet F. Xception: Deep learning with depthwise separable convolutions. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017;pp. 1800–1807.</mixed-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <mixed-citation publication-type="other">Stolar MN, Lech M, Bolia RS, Skinner M. Real time speech emotion recognition using rgb image classification and transfer learning. In: 2017 11th International Conference on Signal Processing and Communication Systems (ICSPCS), 2017;pp. 1–8.</mixed-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <mixed-citation publication-type="other">Beckmann P, Kegler M, Saltini H, Cernak M. Speech-vgg: A deep feature extractor for speech processing. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1910.09909">arXiv:1910.09909</ext-link> 2019.</mixed-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <mixed-citation publication-type="other">Ford L, Tang H, Grondin F, Glass JR. A deep residual network for large-scale acoustic scene analysis. In: INTERSPEECH. 2019.</mixed-citation>
    </ref>
    <ref id="CR67">
      <label>67.</label>
      <mixed-citation publication-type="other">Li CY, Vu NT. Densely connected convolutional networks for speech recognition. In: Speech Communication; 13th ITG-Symposium, 2018;pp. 1–5.</mixed-citation>
    </ref>
    <ref id="CR68">
      <label>68.</label>
      <mixed-citation publication-type="other">Xu K, Feng D, Mi H, Zhu B, Wang D, Zhang L, Cai H, Liu S. Mixup-based acoustic scene classification using multi-channel convolutional neural network. In: Pacific Rim Conference on Multimedia, pp. 14–23. Springer, Cham. 2018.</mixed-citation>
    </ref>
    <ref id="CR69">
      <label>69.</label>
      <mixed-citation publication-type="other">Selvaraju RR, Cogswell M, Das A, Vedantam R, Parikh D, Batra D. Grad-cam: Visual explanations from deep networks via gradient-based localization. In: Proceedings of the IEEE International Conference on Computer Vision, 2017;pp. 618–626.</mixed-citation>
    </ref>
    <ref id="CR70">
      <label>70.</label>
      <mixed-citation publication-type="other">Barabassy A, Szatmári B, Laszlovszky I, Németh G. Negative Symptoms of Schizophrenia. Constructs, Burden, and Management. 2018.</mixed-citation>
    </ref>
    <ref id="CR71">
      <label>71.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Hales</surname>
            <given-names>E</given-names>
          </name>
          <etal/>
        </person-group>
        <source>The American psychiatric publishing textbook of psychiatry</source>
        <year>2013</year>
        <edition>5</edition>
        <publisher-loc>Washigton</publisher-loc>
        <publisher-name>American Psychiatric Pub</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR72">
      <label>72.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schaerlaeken</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Grandjean</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Unfolding and dynamics of affect bursts decoding in humans</article-title>
        <source>PLoS ONE</source>
        <year>2018</year>
        <volume>13</volume>
        <issue>10</issue>
        <fpage>1</fpage>
        <lpage>21</lpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0206216</pub-id>
      </element-citation>
    </ref>
    <ref id="CR73">
      <label>73.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Human emotion recognition by optimally fusing facial expression and speech feature</article-title>
        <source>Signal Process</source>
        <year>2020</year>
        <volume>84</volume>
        <fpage>115831</fpage>
      </element-citation>
    </ref>
    <ref id="CR74">
      <label>74.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Hoffman</surname>
            <given-names>RR</given-names>
          </name>
          <name>
            <surname>Palermo</surname>
            <given-names>DS</given-names>
          </name>
        </person-group>
        <source>Cognition and the symbolic processes: applied and ecological perspectives</source>
        <year>2014</year>
        <publisher-loc>London</publisher-loc>
        <publisher-name>Psychology Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR75">
      <label>75.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rani</surname>
            <given-names>BMS</given-names>
          </name>
          <name>
            <surname>Rani</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Ravi</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sree</surname>
            <given-names>MD</given-names>
          </name>
        </person-group>
        <article-title>Basic fundamental recognition of voiced unvoiced and silence region of a speech</article-title>
        <source>Int J Eng Adv Technol</source>
        <year>2014</year>
        <volume>4</volume>
        <fpage>83</fpage>
        <lpage>86</lpage>
      </element-citation>
    </ref>
    <ref id="CR76">
      <label>76.</label>
      <mixed-citation publication-type="other">Weinberger DR, Marenco S. Schizophrenia as a Neurodevelopmental Disorder, 2003;pp. 326–348.</mixed-citation>
    </ref>
    <ref id="CR77">
      <label>77.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tuckova</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Komarek</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Effectiveness of speech analysis by self-organizing maps in children with developmental language disorders</article-title>
        <source>Neuroendocrinol Lett</source>
        <year>2008</year>
        <volume>29</volume>
        <issue>6</issue>
        <fpage>939</fpage>
        <pub-id pub-id-type="pmid">19112409</pub-id>
      </element-citation>
    </ref>
    <ref id="CR78">
      <label>78.</label>
      <mixed-citation publication-type="other">Grill P, Tuckova J. Formant analysisis–vowel detection of children with developmental dysphasia. Digital Technologies. 2010.</mixed-citation>
    </ref>
    <ref id="CR79">
      <label>79.</label>
      <mixed-citation publication-type="other">Vranova M, Tuckova J, Kyncl M, Grill P, Komarek V, et al. In: In: In AKL Congress, , editor., et al., Mri abnormalities of speech and computerised processing of speech of children with developmental dysphasia. Tabor, Czech Republic; 2011.</mixed-citation>
    </ref>
    <ref id="CR80">
      <label>80.</label>
      <mixed-citation publication-type="other">Grill P, Tuckova J. Formants application to diagnose of children with developmental dysphasia. TBMI VŠB. 2011;98–101.</mixed-citation>
    </ref>
    <ref id="CR81">
      <label>81.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kohnert</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Windsor</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ebert</surname>
            <given-names>KD</given-names>
          </name>
        </person-group>
        <article-title>Primary or "specific" language impairment and children learning a second language</article-title>
        <source>Brain Lang.</source>
        <year>2009</year>
        <volume>109</volume>
        <issue>2–3</issue>
        <fpage>101</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bandl.2008.01.009</pub-id>
        <pub-id pub-id-type="pmid">18313136</pub-id>
      </element-citation>
    </ref>
    <ref id="CR82">
      <label>82.</label>
      <mixed-citation publication-type="other">Grela B, Collisson B, Arthur D. Language processing in children with language impairment. The handbook of psycholinguistic and cognitive processes: Perspectives in communication disorders. 2011;373.</mixed-citation>
    </ref>
    <ref id="CR83">
      <label>83.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Clahsen</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>The grammatical characterization of developmental dysphasia</article-title>
        <source>Linguistics.</source>
        <year>1989</year>
        <volume>27</volume>
        <issue>5</issue>
        <fpage>897</fpage>
        <lpage>920</lpage>
        <pub-id pub-id-type="doi">10.1515/ling.1989.27.5.897</pub-id>
      </element-citation>
    </ref>
    <ref id="CR84">
      <label>84.</label>
      <mixed-citation publication-type="other">Gopnik M, Dalalakis J, Fukuda S, Fukuda SE, Kehayia E. Genetic language impairment. Unruly grammars; 1996.</mixed-citation>
    </ref>
    <ref id="CR85">
      <label>85.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Grill</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Tucková</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Speech databases of typical children and children with sli</article-title>
        <source>PLoS ONE</source>
        <year>2016</year>
        <volume>11</volume>
        <fpage>e0150365</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0150365</pub-id>
        <pub-id pub-id-type="pmid">26963508</pub-id>
      </element-citation>
    </ref>
    <ref id="CR86">
      <label>86.</label>
      <mixed-citation publication-type="other">Grill P, Tuckova J. Classification and Detection of Specific Language Impairments in Children Based on their Speech Skills. 2017. p. 24.</mixed-citation>
    </ref>
    <ref id="CR87">
      <label>87.</label>
      <mixed-citation publication-type="other">Grill P. Classification of children with sli through their speech utterances. In: World Congress on Medical Physics and Biomedical Engineering 2018. Singapore: Springer; 2019. p. 441–7.</mixed-citation>
    </ref>
    <ref id="CR88">
      <label>88.</label>
      <mixed-citation publication-type="other">Ramarao D, Singh C, Shahnawazuddin S, Adiga N, Pradhan G. Detecting developmental dysphasia in children using speech data. In: 2018 International Conference on Signal Processing and Communications (SPCOM), 2018;pp. 100–104.</mixed-citation>
    </ref>
    <ref id="CR89">
      <label>89.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Slogrove</surname>
            <given-names>KJ</given-names>
          </name>
          <name>
            <surname>van der Haar</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Abramowicz</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Klein</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Specific language impairment detection through voice analysis</article-title>
        <source>Bus Inf Syst</source>
        <year>2020</year>
        <publisher-loc>Cham</publisher-loc>
        <publisher-name>Springer</publisher-name>
        <fpage>130</fpage>
        <lpage>41</lpage>
      </element-citation>
    </ref>
    <ref id="CR90">
      <label>90.</label>
      <mixed-citation publication-type="other">Sharma Y, Singh BK. Prediction of specific language impairment in children using speech linear predictive coding coefficients. In: 2020 First International Conference on Power, Control and Computing Technologies (ICPC2T), 2020; p. 305–310.</mixed-citation>
    </ref>
    <ref id="CR91">
      <label>91.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reddy</surname>
            <given-names>MK</given-names>
          </name>
          <name>
            <surname>Alku</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Rao</surname>
            <given-names>KS</given-names>
          </name>
        </person-group>
        <article-title>Detection of specific language impairment in children using glottal source features</article-title>
        <source>IEEE Access</source>
        <year>2020</year>
        <volume>8</volume>
        <fpage>15273</fpage>
        <lpage>15279</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2020.2967224</pub-id>
      </element-citation>
    </ref>
    <ref id="CR92">
      <label>92.</label>
      <mixed-citation publication-type="other">O’Shea K, Nash R. An introduction to convolutional neural networks. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1511.08458">arXiv:1511.08458</ext-link>. 2015.</mixed-citation>
    </ref>
    <ref id="CR93">
      <label>93.</label>
      <mixed-citation publication-type="other">Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1502.03167">arXiv:1502.03167</ext-link> (2015)</mixed-citation>
    </ref>
    <ref id="CR94">
      <label>94.</label>
      <mixed-citation publication-type="other">Bjorck N, Gomes CP, Selman B, Weinberger KQ. Understanding batch normalization. In: Advances in Neural Information Processing Systems, 2018;pp. 7694–7705.</mixed-citation>
    </ref>
    <ref id="CR95">
      <label>95.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maas</surname>
            <given-names>AL</given-names>
          </name>
          <name>
            <surname>Hannun</surname>
            <given-names>AY</given-names>
          </name>
          <name>
            <surname>Ng</surname>
            <given-names>AY</given-names>
          </name>
        </person-group>
        <article-title>Rectifier nonlinearities improve neural network acoustic models</article-title>
        <source>Proc. Icml</source>
        <year>2013</year>
        <volume>30</volume>
        <fpage>3</fpage>
      </element-citation>
    </ref>
    <ref id="CR96">
      <label>96.</label>
      <mixed-citation publication-type="other">Zeiler MD, Ranzato M, Monga R, Mao M, Yang K, Le QV, Nguyen P, Senior A, Vanhoucke V, Dean J, <italic>et al.</italic> On rectified linear units for speech processing. In: 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, 2013;pp. 3517–3521. IEEE</mixed-citation>
    </ref>
    <ref id="CR97">
      <label>97.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Kuen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Shahroudy</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Shuai</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Recent advances in convolutional neural networks</article-title>
        <source>Pattern Recogn</source>
        <year>2018</year>
        <volume>77</volume>
        <fpage>354</fpage>
        <lpage>377</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patcog.2017.10.013</pub-id>
      </element-citation>
    </ref>
    <ref id="CR98">
      <label>98.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Russakovsky</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Krause</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Satheesh</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Karpathy</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Khosla</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bernstein</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Imagenet large scale visual recognition challenge</article-title>
        <source>Int J Comput Vision</source>
        <year>2015</year>
        <volume>115</volume>
        <issue>3</issue>
        <fpage>211</fpage>
        <lpage>252</lpage>
        <pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id>
      </element-citation>
    </ref>
    <ref id="CR99">
      <label>99.</label>
      <mixed-citation publication-type="other">Zhang CL, Luo JH, Wei XS, Wu JX. In defense of fully connected layers in visual representation transfer. In: Pacific Rim Conference on Multimedia, Springer. 2017;p. 807–817.</mixed-citation>
    </ref>
    <ref id="CR100">
      <label>100.</label>
      <mixed-citation publication-type="other">Sermanet P, Kavukcuoglu K, Chintala S, Lecun Y. Pedestrian detection with unsupervised multi-stage feature learning. In: Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3626–3633. IEEE Computer Society, Oregon. 2013.</mixed-citation>
    </ref>
    <ref id="CR101">
      <label>101.</label>
      <mixed-citation publication-type="other">Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. In: Advances in Neural Information Processing Systems, 2017;pp. 5998–6008.</mixed-citation>
    </ref>
    <ref id="CR102">
      <label>102.</label>
      <mixed-citation publication-type="other">Woo S, Park J, Lee JY, So Kweon I. Cbam: Convolutional block attention module. In: Proceedings of the European Conference on Computer Vision (ECCV), 2018;pp. 3–19.</mixed-citation>
    </ref>
  </ref-list>
</back>
