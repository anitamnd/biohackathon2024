<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7003318</article-id>
    <article-id pub-id-type="pmid">32024462</article-id>
    <article-id pub-id-type="publisher-id">3363</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-020-3363-7</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Multi-template matching: a versatile tool for object-localization in microscopy images</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7686-3249</contrib-id>
        <name>
          <surname>Thomas</surname>
          <given-names>Laurent S. V.</given-names>
        </name>
        <address>
          <email>l.thomas@acquifer.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Gehrig</surname>
          <given-names>Jochen</given-names>
        </name>
        <address>
          <email>j.gehrig@acquifer.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label>Acquifer is a division of Ditabis, Digital Biomedical Imaging Systems AG, Pforzheim, Germany </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.5253.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 0328 4908</institution-id><institution>Centre of Paediatrics and Adolescent Medicine, </institution><institution>University Hospital Heidelberg, </institution></institution-wrap>Heidelberg, Germany </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>5</day>
      <month>2</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>5</day>
      <month>2</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>21</volume>
    <elocation-id>44</elocation-id>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>7</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>13</day>
        <month>1</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s). 2020, corrected publication 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">The localization of objects of interest is a key initial step in most image analysis workflows. For biomedical image data, classical image-segmentation methods like thresholding or edge detection are typically used. While those methods perform well for labelled objects, they are reaching a limit when samples are poorly contrasted with the background, or when only parts of larger structures should be detected. Furthermore, the development of such pipelines requires substantial engineering of analysis workflows and often results in case-specific solutions. Therefore, we propose a new straightforward and generic approach for object-localization by template matching that utilizes multiple template images to improve the detection capacity.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We provide a new implementation of template matching that offers higher detection capacity than single template approach, by enabling the detection of multiple template images. To provide an easy-to-use method for the automatic localization of objects of interest in microscopy images, we implemented multi-template matching as a Fiji plugin, a KNIME workflow and a python package. We demonstrate its application for the localization of entire, partial and multiple biological objects in zebrafish and medaka high-content screening datasets. The Fiji plugin can be installed by activating the Multi-Template-Matching and IJ-OpenCV update sites. The KNIME workflow is available on nodepit and KNIME Hub. Source codes and documentations are available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/multi-template-matching">https://github.com/multi-template-matching</ext-link>).</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par3">The novel multi-template matching is a simple yet powerful object-localization algorithm, that requires no data-pre-processing or annotation. Our implementation can be used out-of-the-box by non-expert users for any type of 2D-image. It is compatible with a large variety of applications including, for instance, analysis of large-scale datasets originating from automated microscopy, detection and tracking of objects in time-lapse assays, or as a general image-analysis step in any custom processing pipelines. Using different templates corresponding to distinct object categories, the tool can also be used for classification of the detected regions.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Fiji</kwd>
      <kwd>KNIME</kwd>
      <kwd>OpenCV</kwd>
      <kwd>Template matching</kwd>
      <kwd>Object-recognition</kwd>
      <kwd>Object-localization</kwd>
      <kwd>Pattern recognition</kwd>
      <kwd>Classification</kwd>
      <kwd>Zebrafish</kwd>
      <kwd>Medaka</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010665</institution-id>
            <institution>H2020 Marie Skłodowska-Curie Actions</institution>
          </institution-wrap>
        </funding-source>
        <award-id>721537</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par8">In microscopy images, objects of interest usually represent only a fraction of the field of view and are randomly positioned. Typically, detection of objects in microscopy images relies on classic intensity-based segmentation techniques that perform well for the localization of fluorescent objects. However, these approaches often require the creation of complex analysis workflows and the adjustment of multiple parameters, resulting in highly application-specific solutions [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR4">4</xref>]. In many cases, such as whole organism imaging, such methods might not even be applicable when it comes to the identification of a particular organ or tissue that is poorly contrasted with the rest of the specimen or that is non-specifically labelled. Alternatively, methods based on the detection of edges and shapes (e.g. circular Hough transform [<xref ref-type="bibr" rid="CR5">5</xref>]) can perform well with low contrast images, but they are limited to a given shape or are sensitive to noise. Machine learning methods offer powerful object-detection capacities [<xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR10">10</xref>]; however, setting up the software environment can be overwhelming and training the machine requires large amounts of annotated data, thus rendering it often inaccessible to most microscopy users. In contrast, template-based approaches allow the computation of the most probable positions of a single template image within a larger image with negligible manual annotation and at minimal computational cost [<xref ref-type="bibr" rid="CR11">11</xref>]. However, using a single template, the detection capacity is limited, as the algorithm searches for a single intensity pattern which might not generalize well to objects with different perspectives or characteristics. To overcome current limitations in object-recognition, we report a new implementation of template matching with enhanced detection capacity by performing the search with multiple template images, thus improving the range of detectable patterns. The individual template detections are combined and filtered to keep the most probable detections using a custom non-maxima suppression (NMS). To address the current lack of open-source tools for generic and accessible object-detection in end-user software, we further present the design and establishment of previously unavailable multi-template-matching functionalities in Fiji [<xref ref-type="bibr" rid="CR12">12</xref>] and KNIME [<xref ref-type="bibr" rid="CR13">13</xref>] in a user-friendly manner.</p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <p id="Par9">Extending on OpenCV functions, we implemented multi-template matching for end-users as both a Fiji plugin and a KNIME workflow compatible with images of any bit depth. To this extent, we developed a python implementation that can be installed with its dependencies like a regular python package via pip (Multi-Template Matching). The KNIME workflow uses this python implementation via a python scripting node. The Fiji plugins rely on a collection of related Jython scripts that are automatically installed with the plugins upon activation of the <italic>Multi-Template Matching</italic> update site. Additionally, the <italic>IJ-OpenCV plugins</italic> update site [<xref ref-type="bibr" rid="CR14">14</xref>] should be activated to install the OpenCV dependencies in Fiji. The resulting pipelines can be installed on any system running Fiji or KNIME/Python (see Additional file <xref rid="MOESM13" ref-type="media">13</xref> for further details and availability of source code). A flowchart of the implementation is provided in Additional file <xref rid="MOESM4" ref-type="media">4</xref>: Figure S1.</p>
    <p id="Par10">To predict the position of a template within a target image (Figs. <xref rid="Fig1" ref-type="fig">1</xref>a, <xref rid="Fig2" ref-type="fig">2</xref>a), the algorithm first computes a correlation map using the function <italic>matchTemplate</italic> from OpenCV (Figs. <xref rid="Fig1" ref-type="fig">1</xref>b, <xref rid="Fig2" ref-type="fig">2</xref>b). If information about the approximate position of the object is known a priori, the computation can be limited to a user-defined rectangular search region, which significantly speeds up the execution by reducing image size (Fig. <xref rid="Fig1" ref-type="fig">1</xref>a, b; Additional file <xref rid="MOESM7" ref-type="media">7</xref>: Figure S4, Additional file <xref rid="MOESM9" ref-type="media">9</xref>: Figure S6, Additional file <xref rid="MOESM11" ref-type="media">11</xref>: Figure S8 and Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Movie S1). In this case, the image is first cropped to the search region before the computation of the correlation map, and the position of the predicted bounding boxes are recalculated for the initial image shape.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Head region detection in oriented zebrafish larvae using single template matching. <bold>a</bold> Searched image (2048 × 2048 pixels, scale bar: 1 mm) with template as inset (188 × 194 pixels), search region in orange (1820 × 452 pixels) and predicted location in blue. The red cross corresponds to the position of the global maximum of the correlation map (as in <bold>b</bold>). <bold>b</bold> Correlation map with global maximum (red cross) indicating the position of the bounding box in <bold>a</bold>. The grid area indicates the smaller size of the correlation map compared to the image in which the search is performed (see also Additional file <xref rid="MOESM13" ref-type="media">13</xref>). <bold>c</bold> Montage of detected head regions within a 96 well plate</p></caption><graphic xlink:href="12859_2020_3363_Fig1_HTML" id="MO1"/></fig><fig id="Fig2"><label>Fig. 2</label><caption><p>Multi-template matching and Non-Maxima Suppression for the detection of randomly oriented and positioned medaka embryos. <bold>a</bold> Image in which the search is performed (2048 × 2048 pixels - scale bar: 1 mm) and template as inset (400 × 414 pixels). The search was performed with a set of templates (original template, vertical and horizontal flip, each rotated by 90°, 180° and 270°). Parameters for the detection: score type: 0-mean normalized cross-correlation, <italic>N</italic> = 4 expected objects per image, score threshold: 0.35, maximal overlap between bounding boxes: 0.25. <bold>b</bold> One of the derived correlation maps from A: red crosses indicate possible local maxima before Non-Maxima Suppression (NMS). The grid area indicates the smaller size of the correlation map compared to the image in which the search is performed as explained in Additional file <xref rid="MOESM13" ref-type="media">13</xref>. <bold>c</bold> Bounding boxes associated to the maxima shown in <bold>b</bold> and overlaid on the searched image. Colours are highlighting overlapping bounding boxes. The bounding box dimensions are identical to the dimensions of the template used for the search. <bold>d</bold>, <bold>e</bold> Preventing overlapping detections by NMS. Shown are 2 overlapping bounding boxes predicting possible object locations. Each predicted location is associated to a probability score <italic>S</italic> to contain an object. The ratio between the intersection (<bold>d</bold>) and the union (<bold>e</bold>) area of the bounding boxes (Intersection over Union or IoU) is computed to decide whether the 2 overlapping bounding boxes are likely to predict the location of the same object (IoU close to 1) or the locations of distinct objects that are close to each other (IoU close to 0). For a detailed description of Non-Maxima Suppression see Additional file <xref rid="MOESM13" ref-type="media">13</xref>. <bold>f</bold> Yielded object detections after NMS with a maximal IoU of 0.25, to return the N_objects = 4 best detections</p></caption><graphic xlink:href="12859_2020_3363_Fig2_HTML" id="MO2"/></fig></p>
    <p id="Par11">
      <media xlink:href="12859_2020_3363_MOESM1_ESM.mp4" id="MOESM1">
        <caption>
          <p><bold>Additional file 1: Movie S1.</bold> Installation and single object detection. Available on YouTube at <ext-link ext-link-type="uri" xlink:href="https://youtu.be/KlzIqSG5XBU">https://youtu.be/KlzIqSG5XBU</ext-link>.</p>
        </caption>
      </media>
    </p>
    <p id="Par12">Template matching performs the search by translation of the template over the image, i.e. it will find objects that show a similar orientation as in the template. To maximize the capacity of object-detection, our implementation allows to provide a set of templates to be searched (e.g. additional object perspectives, scales), or initial templates can be transformed by selecting additional flipping and rotation in the plugin interface. Using templates representing different objects, simultaneous object-detection for different categories can be performed, e.g. for classification of the detected regions (Additional file <xref rid="MOESM11" ref-type="media">11</xref>: Figure S8 and Additional file <xref rid="MOESM12" ref-type="media">12</xref>: Figure S9).</p>
    <p id="Par13">Detection of multiple objects from a single template search, but especially from multiple consecutive template searches can lead to overlapping detections when a simple ranking of corresponding correlation scores is performed (Fig. <xref rid="Fig2" ref-type="fig">2</xref>c). These redundant detections need to be recognized and eliminated in order to enable genuine and robust multi-template matching. To address this, we developed a custom strategy combining maxima finding in correlation maps, followed by NMS [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>], which excludes redundant detections based on a user-defined degree of overlap between predicted bounding-boxes (Fig. <xref rid="Fig2" ref-type="fig">2</xref>d, e, f). The detailed procedure for maxima detection and NMS can be found in Additional file <xref rid="MOESM13" ref-type="media">13</xref>.</p>
    <p id="Par14">The parameters for the detection (template rotation/flipping, expected number of objects, score type and if <italic>N</italic> &gt; 1, threshold on the score and maximal overlap between predicted locations) are specified via a graphical user interface in both Fiji and KNIME (Additional file <xref rid="MOESM5" ref-type="media">5</xref>: Figure S2A and Additional file <xref rid="MOESM6" ref-type="media">6</xref>: Figure S3B). The detected regions are returned as rectangular regions of interest (ROI) in Fiji and as part of a mask image in KNIME, along with a result table listing the name of the template, the score and the coordinates for each detection (Additional file <xref rid="MOESM5" ref-type="media">5</xref>: Figure S2B and Additional file <xref rid="MOESM6" ref-type="media">6</xref>: Figure S3C). The tools are intuitively accessible and demand no programming experience. Importantly, the Fiji plugin is macro-recordable, and can thus be readily integrated in custom image processing workflows (see Additional file <xref rid="MOESM3" ref-type="media">3</xref> for 2-step template matching).</p>
  </sec>
  <sec id="Sec3">
    <title>Results</title>
    <p id="Par15">We successfully tested our pipelines with whole organism screening datasets, e.g. for the detection of organs like head, trunk and eyes in oriented zebrafish larvae [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR18">18</xref>] (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, Additional file <xref rid="MOESM7" ref-type="media">7</xref>: Figure S4, Additional file <xref rid="MOESM9" ref-type="media">9</xref>: Figure S6 and Additional file <xref rid="MOESM11" ref-type="media">11</xref>: Figure S8, Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Movie S1), or for the localization of multiple randomly oriented and positioned medaka embryos [<xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR20">20</xref>] (Fig. <xref rid="Fig2" ref-type="fig">2</xref>, Additional file <xref rid="MOESM10" ref-type="media">10</xref>: Figure S7 and Additional file <xref rid="MOESM2" ref-type="media">2</xref>: Movie S2). The specificity of template matching for head detection in aligned zebrafish larvae was tested within full frame images and within a search region (Additional file <xref rid="MOESM7" ref-type="media">7</xref>: Figure S4). The implemented method performs robustly in the presence of experimental variability, such as slight changes of specimen morphology (Additional file <xref rid="MOESM7" ref-type="media">7</xref>: Figure S4C, e.g. well C7), altered orientation (Additional file <xref rid="MOESM7" ref-type="media">7</xref>: Figure S4C, e.g. well B8), partial occlusion (Additional file <xref rid="MOESM7" ref-type="media">7</xref>: Figure S4C, e.g. well E10) or noise (Additional file <xref rid="MOESM8" ref-type="media">8</xref>: Figure S5). In contrast, detection of comparably small structures by template matching within large heterogenous images can be challenging due to the increased likelihood of false detections. This can be largely improved using search regions (Additional file <xref rid="MOESM9" ref-type="media">9</xref>: Figure S6), or by successive template matching detections to first robustly detect an object and then the subregions within the object. For example, we illustrate the robust localization of zebrafish eye regions within previously detected head region using a custom 2-step template matching Fiji macro (Additional file <xref rid="MOESM9" ref-type="media">9</xref>: Figure S6B, C, Additional file <xref rid="MOESM3" ref-type="media">3</xref>).</p>
    <p id="Par16">
      <media xlink:href="12859_2020_3363_MOESM2_ESM.mp4" id="MOESM2">
        <caption>
          <p><bold>Additional file 2: Movie S2.</bold> Multiple object detection. Available on YouTube at <ext-link ext-link-type="uri" xlink:href="https://youtu.be/-PoZihjJIjQ">https://youtu.be/-PoZihjJIjQ</ext-link>.</p>
        </caption>
      </media>
    </p>
    <p id="Par17">Our implementation relies on the OpenCV library which provides a rapid computation of the correlation map (about 0.45 s/image in Fiji with a 188 × 194 template and 2048 × 2048 image) on a laptop with an intel i7-7500U CPU (2.7GHz) and 16 Gb of RAM. This computation can be even faster if a search region is provided (0.06 s/image with a 1820 × 452 pixels search area as in Fig. <xref rid="Fig1" ref-type="fig">1</xref>a, b, see also Additional file <xref rid="MOESM7" ref-type="media">7</xref>: Figure S4D and Additional file <xref rid="MOESM9" ref-type="media">9</xref>: Figure S6D).</p>
    <p id="Par18">In contrast to other template matching implementations, multi-template matching enables the robust detection of multiple objects displaying different intensity patterns by searching for several templates (e.g. additional geometrical transformations or different object categories) (Fig. <xref rid="Fig2" ref-type="fig">2</xref>, Additional file <xref rid="MOESM10" ref-type="media">10</xref>: Figure S7). The corresponding searches are consecutively executed for each template; thus, accuracy is achieved at the expense of computation time (Additional file <xref rid="MOESM9" ref-type="media">9</xref>: Figure S6B, S6D and Additional file <xref rid="MOESM10" ref-type="media">10</xref>: Figure S7B, S7C). Using templates representing different objects, simultaneous object-detection for different categories e.g. for classification of the detected regions can be performed. For instance, the multi-template approach could be used to score certain organ and tissue-specific phenotypes in whole-organism screening, as demonstrated by the categorization of zebrafish embryonic kidney phenotypes in a benchmark dataset [<xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR22">22</xref>] (Additional file <xref rid="MOESM12" ref-type="media">12</xref>: Figure S9).</p>
    <p id="Par19">Previous implementations of template matching [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR23">23</xref>, <xref ref-type="bibr" rid="CR24">24</xref>] do not allow to use different templates for object detections, neither do they provide a way to prevent multiple detections of the same object, which motivated our implementation of a NMS strategy. A comparison to previous template matching tools available in common end-user bioimage analysis software is given in Table <xref rid="Tab1" ref-type="table">1</xref>. Importantly, we aimed at keeping our implementation simple and accessible to researchers of any background. Therefore, we limited the number of parameters to a minimum, and provide extensive documentation in the form of a wiki and online video tutorials (see links in Additional files 1 and 2). For the python implementation, we host several jupyter notebook tutorials on GitHub that can be executed directly in a web-browser without any installation using Binder [<xref ref-type="bibr" rid="CR26">26</xref>].
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Comparison of end-user implementations of template matching, available in common bioimage analysis software</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Name</th><th>Availability</th><th>Open source</th><th>Doc.</th><th>Multiple detections</th><th>Non-Maxima Suppression</th><th>Search region(s)</th><th>Transformations</th><th>Multiple templates</th><th>Reference</th></tr></thead><tbody><tr><td>Template matching</td><td>Fiji</td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>[<xref ref-type="bibr" rid="CR14">14</xref>]</td></tr><tr><td>MatchTemplate</td><td>CellProfiler</td><td>✓</td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>[<xref ref-type="bibr" rid="CR25">25</xref>]</td></tr><tr><td>Template Matching</td><td>ImageJ</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td><ext-link ext-link-type="uri" xlink:href="https://imagej.nih.gov/ij/plugins/template-matching.html">Link</ext-link></td></tr><tr><td>cvMatch_Template</td><td>ImageJ</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>[<xref ref-type="bibr" rid="CR24">24</xref>]</td></tr><tr><td>Pattern Matching</td><td>NI Vision Development</td><td>✗</td><td>✓</td><td>✓</td><td>?</td><td>✓</td><td>✓</td><td>✗</td><td><ext-link ext-link-type="uri" xlink:href="http://www.ni.com/example/30594/en/">Link</ext-link></td></tr><tr><td>Multi-Template Matching</td><td>Fiji, Python, KNIME</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>This paper</td></tr></tbody></table><table-wrap-foot><p>The table lists and compares available implementations and associated functionalities of template matching in common end-user software (Doc.: Documentation available,). The column <italic>Transformations</italic> corresponds to optional search with additional templates, generated by geometric transformation (e.g. flipping, rotations) of the initial templates. Link 1: <ext-link ext-link-type="uri" xlink:href="https://imagej.nih.gov/ij/plugins/template-matching.html">https://imagej.nih.gov/ij/plugins/template-matching.html</ext-link> - Link 2: <ext-link ext-link-type="uri" xlink:href="http://www.ni.com/example/30594/en/">http://www.ni.com/example/30594/en/</ext-link></p></table-wrap-foot></table-wrap></p>
  </sec>
  <sec id="Sec4">
    <title>Discussion</title>
    <p id="Par20">Template matching for object-localization is easy to handle and understand by non-experts. It relies on normalized grey level comparisons between a template image and successive image patches from a sliding window. It considers the full intensity pattern of the template image that usually includes object and surrounding context information. This intensity signature has a powerful discriminative power, even upon occlusion of the object or noise in the image, and the normalized score renders the detection robust against shifted illumination conditions (Additional file <xref rid="MOESM7" ref-type="media">7</xref>: Figure S4 and Additional file <xref rid="MOESM8" ref-type="media">8</xref>: Figure S5). However, template matching is limited to searching for a single intensity pattern, rendering it sensitive to changes in orientation or perspective of objects. In our approach, we overcome this limitation of existing implementations by combining results from multiple template images to increase the range of detectable intensity patterns. We then used a custom NMS strategy based on the degree of overlap between predicted bounding boxes to prevent redundant detections of a given object. Using the overlap for NMS is the most generic way to filter detections by bounding boxes of possibly different sizes and aspect-ratios.</p>
    <p id="Par21">Template images are typically generated by cropping objects of interest from source images, but we also provide the automated generation of additional templates through geometrical transformations. To ensure user-friendliness, we restricted the optional template transformations to flipping and rotation, which represent the common object-transformations expected in microscopy images. However, the tool accepts an arbitrary number of template images representing other potential transformations such as scaling or distortion. While our implementation accepts discrete rotation angle values, rotational search should be balanced with required detection efficiency to keep the overall number of generated templates and thus computation time low. In this study, image examples originating from zebrafish screening studies [<xref ref-type="bibr" rid="CR17">17</xref>] were obtained using sample mounting strategies to constrain the rotational orientation of specimen [<xref ref-type="bibr" rid="CR18">18</xref>], thus preventing the need for rotational search and providing datasets that can be rapidly profiled with template matching approaches. A number of methods have been reported in the literature to prevent repeated searches with rotated templates [<xref ref-type="bibr" rid="CR27">27</xref>–<xref ref-type="bibr" rid="CR32">32</xref>]. However, the complexity of these approaches limits their usability by non-experts, or their implementation is not publicly available.</p>
    <p id="Par22">Template matching can also be used for supervised classification, e.g. for nearest-neighbour search based on a set of annotated templates. This method has the advantage that it does not require any pre-processing of the image, or computation and selection of features. However, the quality of the classification depends heavily on the choice of the templates, and the method is rotation and scale sensitive.</p>
    <p id="Par23">Besides the number of templates, the computation time is a function of image and template sizes. As illustrated in the result section (Additional file <xref rid="MOESM7" ref-type="media">7</xref>: Figure S4D, Additional file <xref rid="MOESM9" ref-type="media">9</xref>: Figure S6D, Additional file <xref rid="MOESM11" ref-type="media">11</xref>: Figure S8D and Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Movie S1), the processing speed can be drastically increased by limiting the analysis to a search region in which the object of interest is expected. Alternatively, the search could be performed with downscaled versions of the image and templates followed by rescaling and placement of bounding boxes. Yet, because downscaling degrades the searched intensity pattern, this approach may lead to non-specific and potentially shifted detections; therefore, we do not provide this option in the Fiji and KNIME versions. However, advanced users can refer to the online tutorial of the python implementation for an example of how to use downscaling to accelerate the detection (see Additional file <xref rid="MOESM13" ref-type="media">13</xref>).</p>
    <p id="Par24">The current implementation is mainly targeted towards analysis of single channel grayscale images. RGB image data can be used as input but is automatically converted to grayscale average projections. Further major developments would be required to expand the tool to also consider colour information of objects of interest. The template matching originates from machine vision applications for the automated inspection of two-dimensional image data. Nevertheless, for certain applications it could also be used to search for the most probable XYZ positions of an object in volumetric data, provided that objects can be robustly discriminated between single z-slices.</p>
  </sec>
  <sec id="Sec5">
    <title>Conclusion</title>
    <p id="Par25">We demonstrate a novel implementation of template matching for object-localization in 2D images using multiple template images, thus drastically improving overall sensitivity and applicability of the method. Our implementation requires only few parameters and is easy to handle by non-expert users via an intuitive graphical interface in Fiji and KNIME. Advanced users are provided with a dedicated python implementation to create custom workflows. We demonstrate the utility of multi-template matching for the detection and possible classification of entire or partial biological specimen in microscopy images. Multi-template matching is highly flexible and can be used as a general image-analysis step for a multitude of applications and samples, as its detection potential mainly depends on the choice of appropriate template image. The usage of multiple template images for the search typically improves detection capacity but increases the computation time accordingly. To improve computing efficiency, the parallelization of template searches or GPU computing with OpenCV could be explored. Finally, the demonstrated template matching tools could also facilitate feedback microscopy applications by interfacing it with the control software of automated microscopes, thus enabling the automated acquisition of ROI for tracking or automated zooming-in on target structures without manual intervention [<xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR23">23</xref>].</p>
  </sec>
  <sec id="Sec6">
    <title>Availability and requirements</title>
    <p id="Par26"><bold>Project name:</bold> Multi-Template Matching</p>
    <p id="Par27">
      <bold>Project home page:</bold>
      <ext-link ext-link-type="uri" xlink:href="https://multi-template-matching.github.io/Multi-Template-Matching/">https://multi-template-matching.github.io/Multi-Template-Matching/</ext-link>
    </p>
    <p id="Par28"><bold>Operating system(s):</bold> Platform independent</p>
    <p id="Par29"><bold>Programming languag</bold>e: Python</p>
    <p id="Par30"><bold>Other requirements:</bold> Fiji with minimum Image 1.52o, IJ-OpenCV 1.2.1</p>
    <p id="Par31"><bold>License:</bold> GPL v3.0</p>
    <p id="Par32"><bold>Any restrictions to use by non-academics:</bold> Any derived work should be under GPL-compatible license</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec7">
      <p>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="12859_2020_3363_MOESM3_ESM.txt">
            <caption>
              <p><bold>Additional file 3: Macro.</bold> 2step-TemplateMatching.ijm, available on the GitHub repository at <ext-link ext-link-type="uri" xlink:href="https://github.com/multi-template-matching/MultiTemplateMatching-Fiji/blob/master/Fiji.app/scripts/Plugins/Multi-Template-Matching/2step-TemplateMatching.ijm">https://github.com/multi-template-matching/MultiTemplateMatching-Fiji/blob/master/Fiji.app/scripts/Plugins/Multi-Template-Matching/2step-TemplateMatching.ijm</ext-link>. See <italic>Supplementary Material.pdf</italic> and <italic>Supplementary Figures.pdf</italic> for further information.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM4">
          <media xlink:href="12859_2020_3363_MOESM4_ESM.pptx">
            <caption>
              <p><bold>Additional file 4: Figure S1.</bold> Flowchart of the implemented multi-template matching. The chart illustrates the sequential execution of the tool, for correlation-based score. For difference-based score, the pipeline is identical except that a difference map is computed, minima are detected instead of maxima and the lowest minima are returned. (IoU: Intersection over Union).x</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM5">
          <media xlink:href="12859_2020_3363_MOESM5_ESM.pptx">
            <caption>
              <p><bold>Additional file 5: Figure S2.</bold> Implementation in Fiji. (A) Graphical user interface for the plugin <italic>“Template Matching Image”</italic> with: (1) Dropdown menu to select the template image of the object of interest. The template must be smaller than the image specified in 2, (2) dropdown menu to select an image (or stack of images) in which to search for the template, (3) tick-boxes to optionally generate additional templates by horizontal/vertical flipping of the initial template, (4) input field for rotation angles to generate additional templates by rotations of the initial and, if selected, flipped templates. The angles are specified in degrees with clockwise orientation and must be separated by commas, (5) dropdown menu to choose the score used for the computation of the score map (normalised square-difference, normalised cross-correlation or 0-mean normalised cross-correlation), (6) input field to specify the number of objects expected in the image, (7) input field to enter a score-threshold in the range 0–1. If the normalised square-difference is selected, only local minima with values below the threshold are returned. While for cross-correlation scores, maxima above this value are returned, (8) input field to specify the maximum value in range 0–1 for the intersection over union (IoU) between a pair of overlapping bounding boxes (Non-Maxima Suppression), (9) tick-box to select if the detected Regions Of Interest (ROI) should be added to Fiji ROI Manager, (10) tick-box to specify if the result table should be displayed at the end of the execution. Parameters 7 and 8 are only required if several objects are expected in each image. (B) Outputs of the plugin with (1) result table with each row containing the names of the image and template, the prediction score and coordinates of the top left corner and centre of the predicted bounding box, and (2) the detected ROI appended to the ROI Manager and highlighted on the image (yellow).</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM6">
          <media xlink:href="12859_2020_3363_MOESM6_ESM.pptx">
            <caption>
              <p><bold>Additional file 6: Figure S3.</bold> Implementation in KNIME. (A) Screenshot of the KNIME workflow. The template and images are provided in the <italic>Image Reader</italic> nodes on the left side, the processing happens in the central metanode called ‘<italic>Multi-Template Matching</italic>’ containing a python node calling the python implementation. The parameters for multi-template matching can be configured via a graphical user interface (see B) by right clicking on the node. The predicted locations can be visualised in the <italic>Interactive Segmentation View</italic> node on the right side (as shown in C). A result table containing the bounding box position, dimension and correlation score is also returned (Table view node, output not shown). (B) Graphical user interface of the central ‘<italic>Multi-Template Matching</italic>’ metanode for the configuration of the detection parameters, similarly to the Fiji implementation (see Additional file <xref rid="MOESM5" ref-type="media">5</xref>: Figure S2A). (C) Predicted locations as viewed in the <italic>Interactive Segmentation View</italic> node. The predicted locations are composed into a mask and overlaid on the image.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM7">
          <media xlink:href="12859_2020_3363_MOESM7_ESM.pptx">
            <caption>
              <p><bold>Additional file 7: Figure S4.</bold> Template matching for head region detection in oriented zebrafish larvae. (A) Single template (188 × 194 pixels, no additional transformation) and image (2048 × 2048 pixels, scale bar: 1 mm) in which the search is performed. The orange rectangle shows the optionally used restricted search region (1820 × 452 pixels). Parameters for the detection: score type: 0-mean normalised cross-correlation – <italic>N</italic> = 1 expected object per image. (B) Result of the detection for <italic>N</italic> = 96 images, with and without search region (both 100% detection rate). (C) Montage of detected zebrafish larval head regions within a 96 well plate (as in Fig. <xref rid="Fig1" ref-type="fig">1</xref>c). (D) Mean computation time per image (error bars show standard deviation) for the different conditions as in B using the same computing hardware as in the main text. Prior information about the position of the sample within the field of view (e.g. due to standardized sample mounting) can be used to specify a search region, drastically accelerating the computation and reducing the chance of incorrect predictions.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM8">
          <media xlink:href="12859_2020_3363_MOESM8_ESM.pptx">
            <caption>
              <p><bold>Additional file 8: Figure S5.</bold> Multi-template matching is robust to noise. (A) Original image (2048 × 2048 pixels). (B) Image as in A corrupted with artificial noise (normally distributed random noise – mean:0, standard deviation:50). (C, D) Result of multi-template matching for respectively A and B. The template used is a crop of the specimen in the middle of image A (hence a correlation score of 1 for the first row of Table C). Parameters for the detection: rotation of the template: 90,180° - score type: 0-mean normalised cross-correlation - <italic>N</italic> = 4 expected objects per image – score threshold: 0.3 – maximal overlap between bounding boxes: 0.3.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM9">
          <media xlink:href="12859_2020_3363_MOESM9_ESM.pptx">
            <caption>
              <p><bold>Additional file 9: Figure S6.</bold> Multi-template matching for eye-region detection in oriented zebrafish larvae. (A) Templates (108 × 76 pixels) and image in which the search is performed (2048 × 2048 pixels, scale bar: 1 mm). Orange rectangle indicates optional search region (1820 × 452 pixels), and blue dotted rectangle the head-region template used for 2-step template matching (see B and D). Parameters for the detection: Vertical flipping of the templates (only if <italic>FlipV</italic> indicated), score type: 0-mean normalized cross-correlation, <italic>N</italic> = 2 expected objects per image, score threshold: 0.5, maximal overlap between bounding boxes: 0.25. For 2-step template matching the search region in orange is used for the 1st step (head-detection with a single head template, N = 1 expected object), then a single eye template is used with flipping for the detection of the eyes within the previously detected head region. (B) Result of the detections for <italic>N</italic> = 94 images. <italic>2 eyes/1 eye/No eyes</italic> in figure legend refer to the outcome of eye-region detection in each larva. Vertical flipping of the templates readily increases the number of genuine matches. The 2-step template matching approach (search of template within a previously identified ROI) offers the best results and is recommended for more challenging template images (see Additional file <xref rid="MOESM3" ref-type="media">3</xref>). (C) Montage of the eye regions detections (yellow) for the 2-step matching approach as in B and D. Specimen in well B8 and F7 are excluded from the count in B as they are not dorsally oriented. (D) Mean computation time per image (error bars show standard deviation) for the different conditions (as in B) using the same hardware as in the main text.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM10">
          <media xlink:href="12859_2020_3363_MOESM10_ESM.pptx">
            <caption>
              <p><bold>Additional file 10: Figure S7.</bold> Multi-template matching for the localization of randomly oriented and positioned medaka embryos. (A) Initial template (410 × 420 pixels) and one of the images in which the search is performed (2048 × 2048 pixels, scale bar of 1 mm). The yellow bounding boxes indicate predicted locations when only the original template in A is used for the search, the green boxes indicate predicted locations when using a set of templates (original, horizontal and vertical flipping, rotation of the original and flipped templates by 90°,180° and 270°). Parameters for the detection: score type: 0-mean normalized cross-correlation, N = 4 expected objects per image, score threshold:0.35, maximal overlap between bounding boxes:0.25. (B) Result of the detections for 10 images each containing 4 embryos (i.e. 40 embryos in total) See detected region in D. (C) Mean computation time per image (error bars show standard deviation) for the different conditions using the same hardware as in the main text. The computation time for each image scales with the number of templates. (D) Montage of the detected regions for 10 images similar to A, each containing 4 embryos (1 column/image). The montage corresponds to the benchmark <italic>“1 Template + transformations”</italic> as in B and C. Yellow bounding boxes indicate the 2 detections classified as <italic>Partial</italic> in B.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM11">
          <media xlink:href="12859_2020_3363_MOESM11_ESM.pptx">
            <caption>
              <p><bold>Additional file 11: Figure S8.</bold> Multi-template matching for simultaneous head and trunk region detection in oriented zebrafish larvae. (A) Head (188 × 194 pixels) and trunk region (264 × 192 pixels) templates. Image (2048 × 2048 pixels, scale bar: 1 mm) in which the search is performed. The orange rectangle shows the optionally used restricted search region (1820 × 452 pixels). Parameters for the detection: Vertical flipping of the templates - score type: 0-mean normalised cross-correlation – N = 2 expected objects per image – score threshold: 0.6 – maximal overlap between bounding boxes: 0.35. (B) Result of the detection for <italic>N</italic> = 96 images, with and without search region. (C) Montage of the detected head regions in 96 zebrafish larvae when the search region is used. The head region was not detected in 3 specimens, 2 of them were not properly dorsally aligned. (see. Additional file <xref rid="MOESM7" ref-type="media">7</xref>: Figure S4C). (D) Montage of the detected trunk regions in 96 zebrafish larvae when the search region is used. When 2 trunks were detected in one image (instead of one trunk and one head), the trunk with the best score was used for the montage. Prior information about the position of the sample within the field of view (e.g. due to standardized sample mounting) can be used to specify a search region, drastically accelerating the computation and reducing the chance of incorrect predictions. (E) Mean computation time per image (N = 96 - error bars show standard deviation) for the different conditions as in B using the same computing hardware as in the main text.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM12">
          <media xlink:href="12859_2020_3363_MOESM12_ESM.pptx">
            <caption>
              <p><bold>Additional file 12: Figure S9.</bold> Using multi-template matching for phenotypes classification. (A) Manually annotated templates used for the classification of phenotypes of embryonic zebrafish kidneys. (B) Example of image to classify and (C) resulting correlation-scores for the 3 classes. In this case, the image is correctly classified as cystic. (D) Confusion matrix depicting the results for the classification of 167 annotated images (50 normal, 52 cystic, 65 long). The class <italic>Cystic</italic> and <italic>Normal</italic> are particularly well predicted. A number of <italic>Long</italic> were classified as <italic>Cystic</italic>, this can be expected as those 2 morphologies show similar elongated regions.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM13">
          <media xlink:href="12859_2020_3363_MOESM13_ESM.docx">
            <caption>
              <p><bold>Additional file 13. </bold>Supplementary information. Supplementary Material for <italic>Multi-Template Matching: a versatile tool for object-localization in microscopy images.</italic></p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>Doc</term>
        <def>
          <p id="Par4">Documentation</p>
        </def>
      </def-item>
      <def-item>
        <term>IoU</term>
        <def>
          <p id="Par5">Intersection over Union</p>
        </def>
      </def-item>
      <def-item>
        <term>NMS</term>
        <def>
          <p id="Par6">Non-Maxima Suppression</p>
        </def>
      </def-item>
      <def-item>
        <term>ROI</term>
        <def>
          <p id="Par7">Region of Interest</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>The original version of this article was revised: the broken link and Additional file 3 have been corrected and updated.</p>
    </fn>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>
        <bold>Change history</bold>
      </p>
      <p>1/4/2022</p>
      <p>A Correction to this paper has been published: 10.1186/s12859-021-04524-7</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p><bold>Supplementary information</bold> accompanies this paper at 10.1186/s12859-020-3363-7.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank Franz Schaefer, Jens Westhoff (Children’s Hospital, Heidelberg), and Jochen Wittbrodt (COS, Heidelberg) for general support, Jakob Gierten (COS, Heidelberg) for sharing image data and Gunjan Pandey (Children’s Hospital, Heidelberg) for proof-reading the manuscript. We are also grateful to the Fiji and KNIME communities for the support on their respective forum.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>LT wrote the code and developed the tool. LT implemented and maintains the tool in Python, Fiji and KNIME. LT wrote the user documentation. LT and JG benchmarked, tested the software, and developed examples. LT and JG conceived the tools. JG designed and supervised the project. LT and JG wrote the manuscript. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This project has received funding from the European Union’s Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant agreement No 721537 “ImageInLife”. The funding body did not play any role in the design of the study, the collection, analysis, and interpretation of data nor in the writing of the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The image datasets used for this study are available on Zenodo (links below).</p>
    <p>- Dataset used for Fig. <xref rid="Fig1" ref-type="fig">1</xref>, Additional file <xref rid="MOESM7" ref-type="media">7</xref>: Figure S4, Additional file <xref rid="MOESM9" ref-type="media">9</xref>: Figure S6 and Additional file <xref rid="MOESM11" ref-type="media">11</xref>: Figure S8.</p>
    <p>Gehrig, Jochen. (2019). 3dpf zebrafish larvae, 96 well plate, Tg (wt1b:EGFP), dorsal view, ACQUIFER Imaging Machine [Data set]. Zenodo. 10.5281/zenodo.2650162 -.</p>
    <p>- Dataset used for Fig. <xref rid="Fig2" ref-type="fig">2</xref>, Additional file <xref rid="MOESM8" ref-type="media">8</xref>: Figure S5 and Additional file <xref rid="MOESM10" ref-type="media">10</xref>: Figure S7.</p>
    <p>Gierten Jakob &amp; Gehrig Jochen. (2019). 102 hpf medaka embryos in 96 well plate (4 embryo/well) - brightfield - 2X magnification - ACQUIFER Imaging Machine (Version 1) [Data set]. Zenodo. 10.5281/zenodo.2650147</p>
  </notes>
  <notes id="FPar1">
    <title>Ethics approval and consent to participate</title>
    <p id="Par33">Not applicable.</p>
  </notes>
  <notes id="FPar2">
    <title>Consent for publication</title>
    <p id="Par34">Not applicable.</p>
  </notes>
  <notes id="FPar3" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par35">Both authors are employees of ACQUIFER, a division of DITABIS Digital Biomedical Imaging Systems AG.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <mixed-citation publication-type="other">Teixidó E, Kießling TR, Krupp E, Quevedo C, Muriana A, Scholz S. Automated morphological feature assessment for zebrafish embryo developmental toxicity screens. Toxicol Sci. 2019;167(2):438–49.</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vogt</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automated image-based phenotypic analysis in zebrafish embryos</article-title>
        <source>Dev Dyn</source>
        <year>2009</year>
        <volume>238</volume>
        <issue>3</issue>
        <fpage>656</fpage>
        <lpage>663</lpage>
        <pub-id pub-id-type="doi">10.1002/dvdy.21892</pub-id>
        <pub-id pub-id-type="pmid">19235725</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Spomer</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Pfriem</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Alshut</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Just</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Pylatiuk</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>High-throughput screening of Zebrafish embryos using automated heart detection and imaging</article-title>
        <source>J Lab Autom</source>
        <year>2012</year>
        <volume>17</volume>
        <issue>6</issue>
        <fpage>435</fpage>
        <lpage>442</lpage>
        <pub-id pub-id-type="doi">10.1177/2211068212464223</pub-id>
        <pub-id pub-id-type="pmid">23053930</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gehrig</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automated high-throughput mapping of promoter-enhancer interactions in zebrafish embryos</article-title>
        <source>Nat Methods</source>
        <year>2009</year>
        <volume>6</volume>
        <issue>12</issue>
        <fpage>911</fpage>
        <lpage>916</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.1396</pub-id>
        <pub-id pub-id-type="pmid">19898487</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Marcato</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <source>An automated and high-throughput photomotor response platform for chemical screens</source>
        <year>2015</year>
        <fpage>7728</fpage>
        <lpage>7731</lpage>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gallego</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Glomerulus classification and detection based on convolutional neural networks</article-title>
        <source>J Imaging</source>
        <year>2018</year>
        <volume>4</volume>
        <issue>1</issue>
        <fpage>20</fpage>
        <pub-id pub-id-type="doi">10.3390/jimaging4010020</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Waithe D, Brown JM, Reglinski K, Diez-Sevilla I, Roberts D, Eggeling C. Object detection networks and augmented reality for cellular detection in fluorescence microscopy acquisition and analysis. bioRxiv. 2019. 10.1101/544833.</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Falk</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>U-net: deep learning for cell counting, detection, and morphometry</article-title>
        <source>Nat Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <issue>1</issue>
        <fpage>67</fpage>
        <lpage>70</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-018-0261-2</pub-id>
        <pub-id pub-id-type="pmid">30559429</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>W</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SSD: single shot multibox detector</article-title>
        <source>arXiv:1512.02325 [cs]</source>
        <year>2016</year>
        <volume>9905</volume>
        <fpage>21</fpage>
        <lpage>37</lpage>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Girshick</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Donahue</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Darrell</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Malik</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Region-based convolutional networks for accurate object detection and segmentation</article-title>
        <source>IEEE Trans Pattern Anal Mach Intell</source>
        <year>2016</year>
        <volume>38</volume>
        <issue>1</issue>
        <fpage>142</fpage>
        <lpage>158</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2015.2437384</pub-id>
        <pub-id pub-id-type="pmid">26656583</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Brunelli</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <source>Template matching techniques in computer vision: theory and practice</source>
        <year>2009</year>
        <publisher-loc>Chichester</publisher-loc>
        <publisher-name>Wiley</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schindelin</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Fiji: an open-source platform for biological-image analysis</article-title>
        <source>Nat Methods</source>
        <year>2012</year>
        <volume>9</volume>
        <issue>7</issue>
        <fpage>676</fpage>
        <lpage>682</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.2019</pub-id>
        <pub-id pub-id-type="pmid">22743772</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Berthold</surname>
            <given-names>MR</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>KNIME - the Konstanz information miner: version 2.0 and beyond</article-title>
        <source>ACM SIGKDD Explorations Newsl</source>
        <year>2009</year>
        <volume>11</volume>
        <issue>1</issue>
        <fpage>26</fpage>
        <pub-id pub-id-type="doi">10.1145/1656274.1656280</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Domínguez</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Heras</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Pascual</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>IJ-OpenCV: combining ImageJ and OpenCV for processing images in biomedicine</article-title>
        <source>Comput Biol Med</source>
        <year>2017</year>
        <volume>84</volume>
        <fpage>189</fpage>
        <lpage>194</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiomed.2017.03.027</pub-id>
        <pub-id pub-id-type="pmid">28390286</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alexe</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Deselaers</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Ferrari</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Measuring the objectness of image windows</article-title>
        <source>IEEE Trans Pattern Anal Mach Intell</source>
        <year>2012</year>
        <volume>34</volume>
        <issue>11</issue>
        <fpage>2189</fpage>
        <lpage>2202</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2012.28</pub-id>
        <pub-id pub-id-type="pmid">22248633</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Felzenszwalb</surname>
            <given-names>PF</given-names>
          </name>
          <name>
            <surname>Girshick</surname>
            <given-names>RB</given-names>
          </name>
          <name>
            <surname>McAllester</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ramanan</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Object detection with discriminatively trained part-based models</article-title>
        <source>IEEE Trans Pattern Anal Mach Intell</source>
        <year>2010</year>
        <volume>32</volume>
        <issue>9</issue>
        <fpage>1627</fpage>
        <lpage>1645</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2009.167</pub-id>
        <pub-id pub-id-type="pmid">20634557</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Gehrig J. 3dpf zebrafish larvae, 96 well plate, Tg (wt1b:EGFP), dorsal view, ACQUIFER imaging machine: Zenodo; 2019. 10.5281/zenodo.2650162.</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wittbrodt</surname>
            <given-names>JN</given-names>
          </name>
          <name>
            <surname>Liebel</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Gehrig</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Generation of orientation tools for automated zebrafish screening assays using desktop 3D printing</article-title>
        <source>BMC Biotechnol</source>
        <year>2014</year>
        <volume>14</volume>
        <issue>1</issue>
        <fpage>36</fpage>
        <pub-id pub-id-type="doi">10.1186/1472-6750-14-36</pub-id>
        <pub-id pub-id-type="pmid">24886511</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Gierten J, et al. Automated high-throughput heart rate measurement in medaka and zebrafish embryos under physiological conditions. bioRxiv. 2019. 10.1101/548594.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Gierten J, Gehrig J. 102 hpf medaka embryos in 96 well plate (4 embryo/well) - brightfield - 2X magnification - ACQUIFER imaging machine: Zenodo; 2019. 10.5281/zenodo.2650147.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pandey</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Westhoff</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Schaefer</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Gehrig</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A smart imaging workflow for organ-specific screening in a cystic kidney zebrafish disease model</article-title>
        <source>Int J Mol Sci</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>6</issue>
        <fpage>1290</fpage>
        <pub-id pub-id-type="doi">10.3390/ijms20061290</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Westhoff</surname>
            <given-names>JH</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Development of an automated imaging pipeline for the analysis of the zebrafish larval kidney</article-title>
        <source>PLoS ONE</source>
        <year>2013</year>
        <volume>8</volume>
        <issue>12</issue>
        <fpage>e82137</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0082137</pub-id>
        <pub-id pub-id-type="pmid">24324758</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peravali</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automated feature detection and imaging for high-resolution screening of zebrafish embryos</article-title>
        <source>BioTechniques</source>
        <year>2011</year>
        <volume>50</volume>
        <issue>5</issue>
        <fpage>319</fpage>
        <lpage>324</lpage>
        <pub-id pub-id-type="doi">10.2144/000113669</pub-id>
        <pub-id pub-id-type="pmid">21548893</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tseng</surname>
            <given-names>Q</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A new micropatterning method of soft substrates reveals that different tumorigenic signals can promote or reduce cell contraction levels</article-title>
        <source>Lab Chip</source>
        <year>2011</year>
        <volume>11</volume>
        <issue>13</issue>
        <fpage>2231</fpage>
        <pub-id pub-id-type="doi">10.1039/c0lc00641f</pub-id>
        <pub-id pub-id-type="pmid">21523273</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Carpenter AE, et al. CellProfiler: image analysis software for identifying and quantifying cell phenotypes. Genome Biol. 2006;7:11.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Forde</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <source>Binder 2.0 - Reproducible, interactive, sharable environments for science at scale</source>
        <year>2018</year>
        <fpage>113</fpage>
        <lpage>120</lpage>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Choi</surname>
            <given-names>M-S</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>W-Y</given-names>
          </name>
        </person-group>
        <article-title>A novel two stage template matching method for rotation and illumination invariance</article-title>
        <source>Pattern Recogn</source>
        <year>2002</year>
        <volume>35</volume>
        <issue>1</issue>
        <fpage>119</fpage>
        <lpage>129</lpage>
        <pub-id pub-id-type="doi">10.1016/S0031-3203(01)00025-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Marimon</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ebrahimi</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <source>Efficient rotation-discriminative template matching</source>
        <year>2007</year>
        <fpage>221</fpage>
        <lpage>230</lpage>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>HY</given-names>
          </name>
          <name>
            <surname>De Araújo</surname>
            <given-names>SA</given-names>
          </name>
        </person-group>
        <article-title>Grayscale template-matching invariant to rotation, scale, translation, brightness and contrast</article-title>
        <source>Pacific-rim symposium on image and video technology</source>
        <year>2007</year>
        <fpage>100</fpage>
        <lpage>113</lpage>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">PhD Thesis, Series of Publications A, Report A-2001-3. Helsinki; 2001. p. 139. ISSN 1238-8645, ISBN 952-10-0009-0.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Fredriksson K, Mäkinen V, and Navarro G. Rotation and lighting invariant template matching. Information and Computation. 2007;205:1096–113. 10.1016/j.ic.2007.03.002.</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Fageot J, Uhlmann V, Püspöki Z, Beck B, Unser M, Depeursinge A. Principled design and implementation of steerable detectors. arXiv:1811.00863 [eess, stat]. 2018.</mixed-citation>
    </ref>
  </ref-list>
</back>
