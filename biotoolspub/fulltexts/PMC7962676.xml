<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.0 20120330//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Eye Mov Res</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Eye Mov Res</journal-id>
    <journal-id journal-id-type="publisher-id">Jemr</journal-id>
    <journal-title-group>
      <journal-title>Journal of Eye Movement Research</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1995-8692</issn>
    <publisher>
      <publisher-name>Bern Open Publishing</publisher-name>
      <publisher-loc>Bern, Switzerland</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7962676</article-id>
    <article-id pub-id-type="doi">10.16910/jemr.12.6.5</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>VisME: Visual Microsaccades Explorer</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Munz</surname>
          <given-names>Tanja</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chuang</surname>
          <given-names>Lewis</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pannasch</surname>
          <given-names>Sebastian</given-names>
        </name>
        <xref ref-type="aff" rid="aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Weiskopf</surname>
          <given-names>Daniel</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
      </contrib>
      <aff id="aff1"><institution>VISUS, University of Stuttgart</institution>, <country>Germany</country></aff>
      <aff id="aff2"><institution>Institute of Informatics, LMU Munich</institution>, <country>Germany</country></aff>
      <aff id="aff3"><institution>Faculty of Psychology, Technische Universität Dresden</institution>, <country>Germany</country></aff>
    </contrib-group>
    <pub-date date-type="pub" publication-format="electronic">
      <day>12</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date date-type="collection" publication-format="electronic">
      <year>2019</year>
    </pub-date>
    <volume>12</volume>
    <issue>6</issue>
    <elocation-id>10.16910/jemr.12.6.5</elocation-id>
    <permissions>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Munz, T, Chuang, L., Pannasch, S., &amp; Weiskopf, D.</copyright-holder>
      <license license-type="open-access">
        <license-p>This work is licensed under a Creative Commons Attribution 4.0 International License,
( <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">
https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>This work presents a visual analytics approach to explore microsaccade distributions in high-frequency eye tracking data. Research studies often apply filter algorithms and parameter values for microsaccade detection. Even when the same algorithms are employed, different parameter values might be adopted across different studies. In this paper, we present a visual analytics system (VisME) to promote reproducibility in the data analysis of microsaccades. It allows users to interactively vary the parametric values for microsaccade filters and evaluate the resulting influence on microsaccade behavior across individuals and on a group level. In particular, we exploit brushing-and-linking techniques that allow the microsaccadic properties of space, time, and movement direction to be extracted, visualized, and compared across multiple views. We demonstrate in a case study the use of our visual analytics system on data sets collected from natural scene viewing and show in a qualitative usability study the usefulness of this approach for eye tracking researchers. We believe that interactive tools such as VisME will promote greater transparency in eye movement research by providing researchers with the ability to easily understand complex eye tracking data sets; such tools can also serve as teaching systems. VisME is provided as open source software.</p>
    </abstract>
    <kwd-group>
      <kwd>Microsaccades</kwd>
      <kwd>visual analytics</kwd>
      <kwd>eye movement</kwd>
      <kwd>eye tracking</kwd>
      <kwd>parameters fixation</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <title>Introduction</title>
    <p>Eye movements are often separated into fixations, the periods when the eyes stay rather still, and saccades, the rapid eye movements between multiple fixations. Furthermore, microsaccades (also known as “fixational saccades” (<xref rid="b36" ref-type="bibr">36</xref>)) are small and fast involuntary eye movements within fixations (<xref rid="b28" ref-type="bibr">28</xref>); they can be seen as small versions of saccades. Humans are not aware of them, but they play an important role in visual perception. Amongst other aspects, they indicate covert attention (<xref rid="b17" ref-type="bibr">17</xref>). Apart from microsaccades, there are other involuntary eye movements located within fixations (e.g., drifts and tremors) that are unlikely to contribute to visual information processing per se. Glissades and square wave jerks, in contrast, have a similar appearance to microsaccades, but their temporal occurrence and properties within a fixation are different. Although experienced eye movement researchers can often discriminate between these different types of eye movements manually, filter algorithms have been developed to extract them automatically and more efficiently.</p>
    <p>For the automatic detection and labeling of microsaccades, different algorithms have been proposed (e.g., <xref rid="b49" ref-type="bibr">49</xref>, <xref rid="b45" ref-type="bibr">45</xref>; <xref rid="b5" ref-type="bibr">5</xref>), most notably by Engbert &amp; Kliegl (2003b)<xref rid="b18" ref-type="bibr">18</xref>. Nonetheless, the parameters for detecting and identifying microsaccades can vary immensely across different research studies. Furthermore, some studies (see Table 1 for examples) might modify basic algorithms with the introduction of additional conditional parameters (e.g., min / max amplitude, inter-saccadic interval). This lack of consistency is often tolerated in order to accommodate unavoidable variances in eye tracking data, individual behavior, and experiment designs. However, this also poses a barrier for researchers in determining whether reports of microsaccadic behavior are consistent from one study to another and the extent to which they are shaped by the chosen parameter values themselves. To facilitate the reproducibility of research results, we sought to provide a system that would support eye movement researchers in exploring and reviewing the properties of microsaccades in a given data set as well as to compare it with another. Such a system would also serve to instruct unexperienced researchers in understanding the consequences of microsaccadic filtering.</p>
    <table-wrap id="t01" orientation="portrait" position="float">
      <label>Table 1</label>
      <caption>
        <p>to do</p>
      </caption>
      <table frame="hsides" rules="groups" cellpadding="3">
        <thead>
          <tr>
            <td rowspan="1" colspan="1">
Paper
</td>
            <td rowspan="1" colspan="1">
Method
</td>
            <td rowspan="1" colspan="1">
Inter-saccadic Interval
</td>
            <td rowspan="1" colspan="1">
Amplitude
</td>
            <td rowspan="1" colspan="1">
Duration
</td>
            <td rowspan="1" colspan="1">
λ
</td>
            <td rowspan="1" colspan="1">
Binocular
</td>
            <td rowspan="1" colspan="1">
Data
</td>
            <td rowspan="1" colspan="1">
Other Features
</td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="1" colspan="1">
Engbert &amp; Kliegl (2003b)
</td>
            <td rowspan="1" colspan="1">
New method
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
≥ 12 ms
</td>
            <td rowspan="1" colspan="1">
6
</td>
            <td rowspan="1" colspan="1">
yes
</td>
            <td rowspan="1" colspan="1">
Eyelink System (SMI), 250 Hz
</td>
            <td rowspan="1" colspan="1">
-
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Engbert &amp; Mergenthaler (2006)
</td>
            <td rowspan="1" colspan="1">
Engbert &amp; Kliegl (2003b)
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
≤1°
</td>
            <td rowspan="1" colspan="1">
≥ 6 ms
</td>
            <td rowspan="1" colspan="1">
5
</td>
            <td rowspan="1" colspan="1">
yes
</td>
            <td rowspan="1" colspan="1">
Eyelink II, SR Research, 500 Hz
</td>
            <td rowspan="1" colspan="1">
-
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Dimigen et al. (2009)
</td>
            <td rowspan="1" colspan="1">
Engbert &amp; Mergenthaler (2006)
</td>
            <td rowspan="1" colspan="1">
50 ms
</td>
            <td rowspan="1" colspan="1">
&lt;1°
</td>
            <td rowspan="1" colspan="1">
≥ 6 ms / 3 samples
</td>
            <td rowspan="1" colspan="1">
5
</td>
            <td rowspan="1" colspan="1">
no
</td>
            <td rowspan="1" colspan="1">
IView-X Hi-Speed 1250, SMI GmbH, 500 Hz
</td>
            <td rowspan="1" colspan="1">
-
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Hsieh &amp; Tse (2009)
</td>
            <td rowspan="1" colspan="1">
Engbert &amp; Kliegl (2003b)
</td>
            <td rowspan="1" colspan="1">
80 ms
</td>
            <td rowspan="1" colspan="1">
0.15°−2°
</td>
            <td rowspan="1" colspan="1">
≥ 4 samples
</td>
            <td rowspan="1" colspan="1">
10
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
Eyelink2, 250 Hz
</td>
            <td rowspan="1" colspan="1">
(semi-)blinks and 400 ms before/600 ms after them removed
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Pastukho &amp; Braun (2010)
</td>
            <td rowspan="1" colspan="1">
Engbert &amp; Kliegl (2003b),Engbert &amp; Kliegl (2003a)
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
yes
</td>
            <td rowspan="1" colspan="1">
Eyelink 2000, SR Research, 1000 Hz
</td>
            <td rowspan="1" colspan="1">
modified algorithm to accommodate for a higher sampling rate
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Mergenthaler &amp; Engbert (2010)
</td>
            <td rowspan="1" colspan="1">
Engbert &amp; Kliegl (2003b), Engbert &amp; Mergenthaler (2006)
</td>
            <td rowspan="1" colspan="1">
30 ms
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
≥ 6 ms
</td>
            <td rowspan="1" colspan="1">
3 (free viewing);<break/>
4 (fixation task)
</td>
            <td rowspan="1" colspan="1">
yes
</td>
            <td rowspan="1" colspan="1">
Eyelink II, SR Research, 500 Hz
</td>
            <td rowspan="1" colspan="1">
-
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Bonneh et al. (2010)
</td>
            <td rowspan="1" colspan="1">
Engbert &amp; Kliegl (2003b)
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
0.08°−2°
</td>
            <td rowspan="1" colspan="1">
≥ 9 ms
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
no
</td>
            <td rowspan="1" colspan="1">
iViewX Hi-Speed, SMI, 240 Hz and Eyelink II, SR Research, 1000 Hz
</td>
            <td rowspan="1" colspan="1">
raw data smoothed with a window of 15 ms; velocity range: 8°/s−150°/s
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Benedetto et al. (2011)
</td>
            <td rowspan="1" colspan="1">
Salvucci &amp; Goldberg (2000)
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
&lt; 1°
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
no
</td>
            <td rowspan="1" colspan="1">
SMI X-HEAD, 200 Hz
</td>
            <td rowspan="1" colspan="1">
-
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Otero-Millan et al. (2012)
</td>
            <td rowspan="1" colspan="1">
Engbert &amp; Mergenthaler (2006)
</td>
            <td rowspan="1" colspan="1">
20 ms
</td>
            <td rowspan="1" colspan="1">
&lt; 2°
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
6
</td>
            <td rowspan="1" colspan="1">
yes
</td>
            <td rowspan="1" colspan="1">
Eyelink 1000, SR Research, 500 Hz
</td>
            <td rowspan="1" colspan="1">
-
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Yokoyama et al. (2012)
</td>
            <td rowspan="1" colspan="1">
Engbert &amp; Kliegl (2003b)
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
≥ 3 samples
</td>
            <td rowspan="1" colspan="1">
6
</td>
            <td rowspan="1" colspan="1">
yes
</td>
            <td rowspan="1" colspan="1">
Eyelink CL 1000, SR Research, 500 Hz
</td>
            <td rowspan="1" colspan="1">
no trials with eye blinks or eye position more than 2° away from center
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Hicheur et al. (2013)
</td>
            <td rowspan="1" colspan="1">
Engbert &amp; Mergenthaler (2006)
</td>
            <td rowspan="1" colspan="1">
25 ms
</td>
            <td rowspan="1" colspan="1">
&lt; 1°
</td>
            <td rowspan="1" colspan="1">
≥ 10 ms
</td>
            <td rowspan="1" colspan="1">
4
</td>
            <td rowspan="1" colspan="1">
yes
</td>
            <td rowspan="1" colspan="1">
Eyelink 1000, SR Research, 1000 Hz
</td>
            <td rowspan="1" colspan="1">
microsaccades within 50 ms after a saccade were not considered as microsaccades
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Pastukhov et al. (2013)
</td>
            <td rowspan="1" colspan="1">
Engbert &amp; Kliegl (2003b)
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
&lt; 60′
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
yes
</td>
            <td rowspan="1" colspan="1">
Eyelink 2000, SR Research, 1000 Hz
</td>
            <td rowspan="1" colspan="1">
square wave jerks
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Di Stasi et al. (2013)
</td>
            <td rowspan="1" colspan="1">
Engbert &amp; Kliegl (2003b)
</td>
            <td rowspan="1" colspan="1">
20 ms
</td>
            <td rowspan="1" colspan="1">
&lt; 1°
</td>
            <td rowspan="1" colspan="1">
≥ 6 ms
</td>
            <td rowspan="1" colspan="1">
6
</td>
            <td rowspan="1" colspan="1">
yes
</td>
            <td rowspan="1" colspan="1">
Eyelink 1000, SR Research, 500 Hz
</td>
            <td rowspan="1" colspan="1">
(semi-)blinks and 200 ms before/after them removed
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
McCamy, Najafian Jazi et al. (2013)
</td>
            <td rowspan="1" colspan="1">
Engbert &amp; Kliegl (2003b)
</td>
            <td rowspan="1" colspan="1">
20 ms
</td>
            <td rowspan="1" colspan="1">
&lt; 2°
</td>
            <td rowspan="1" colspan="1">
≥ 6 ms
</td>
            <td rowspan="1" colspan="1">
4
</td>
            <td rowspan="1" colspan="1">
yes
</td>
            <td rowspan="1" colspan="1">
Eyelink 1000, SR Research, 500 Hz
</td>
            <td rowspan="1" colspan="1">
(semi-)blinks and 200 ms before/after them removed
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Costela et al. (2013)
</td>
            <td rowspan="1" colspan="1">
Engbert &amp; Kliegl (2003b)
</td>
            <td rowspan="1" colspan="1">
20 ms
</td>
            <td rowspan="1" colspan="1">
&lt; 1°
</td>
            <td rowspan="1" colspan="1">
≥ 6 ms
</td>
            <td rowspan="1" colspan="1">
4
</td>
            <td rowspan="1" colspan="1">
yes
</td>
            <td rowspan="1" colspan="1">
Eyelink 1000, SR Research
</td>
            <td rowspan="1" colspan="1">
(semi-)blinks and 200 ms before/after them removed
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
McCamy, Collins et al. (2013)
</td>
            <td rowspan="1" colspan="1">
Engbert &amp; Kliegl (2003b)
</td>
            <td rowspan="1" colspan="1">
20 ms
</td>
            <td rowspan="1" colspan="1">
≤ 2°
</td>
            <td rowspan="1" colspan="1">
≥ 6 ms
</td>
            <td rowspan="1" colspan="1">
6
</td>
            <td rowspan="1" colspan="1">
yes
</td>
            <td rowspan="1" colspan="1">
Eyelink II, SR Research, 500 Hz
</td>
            <td rowspan="1" colspan="1">
(semi-)blinks and 200 ms before/after them removed
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
McCamy et al. (2014)
</td>
            <td rowspan="1" colspan="1">
Engbert &amp; Kliegl (2003b), Engbert &amp; Mergenthaler (2006)
</td>
            <td rowspan="1" colspan="1">
20 ms
</td>
            <td rowspan="1" colspan="1">
&lt; 1°
</td>
            <td rowspan="1" colspan="1">
≥ 6 ms
</td>
            <td rowspan="1" colspan="1">
6
</td>
            <td rowspan="1" colspan="1">
yes
</td>
            <td rowspan="1" colspan="1">
Eyelink II, SR Research, 500 Hz
</td>
            <td rowspan="1" colspan="1">
(semi-)blinks and 200 ms before/after them removed
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Privitera et al. (2014)
</td>
            <td rowspan="1" colspan="1">
Engbert &amp; Kliegl (2003b)
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
&lt; 1.2°
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
6 and 3
</td>
            <td rowspan="1" colspan="1">
no
</td>
            <td rowspan="1" colspan="1">
Eyelink 1000, SR Research, 1000 Hz
</td>
            <td rowspan="1" colspan="1">
first peak velocities were determined then their extent
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Yuval-Greenberg et al. (2014)
</td>
            <td rowspan="1" colspan="1">
Engbert &amp; Mergenthaler (2006)
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
&lt; 1°
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
6
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
Eyelink 1000, SR Research, 1000/500 Hz
</td>
            <td rowspan="1" colspan="1">
-
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Fried et al. (2014)
</td>
            <td rowspan="1" colspan="1">
Bonneh et al. (2010)
</td>
            <td rowspan="1" colspan="1">
yes
</td>
            <td rowspan="1" colspan="1">
&gt; 0.1° ∧ ࣘ≤ 2°
</td>
            <td rowspan="1" colspan="1">
≥ 6 ms
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
Eyelink 1000, SR Research, 500 Hz
</td>
            <td rowspan="1" colspan="1">
(semi-)blinks and 20 ms before/after them removed; min velocity: 10°/s; peak velocity: &gt; 18°/s
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Poletti &amp; Rucci (2016)
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
15 ms
</td>
            <td rowspan="1" colspan="1">
3′−30′
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
yes
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
-
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Krejtz et al. (2018)
</td>
            <td rowspan="1" colspan="1">
Engbert &amp; Kliegl (2003)
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
≥ 6 ms
</td>
            <td rowspan="1" colspan="1">
6
</td>
            <td rowspan="1" colspan="1">
no
</td>
            <td rowspan="1" colspan="1">
Eyelink 1000, SR Research, 500 Hz
</td>
            <td rowspan="1" colspan="1">
microsaccades detected within fixations; average position of right and left gaze points are used
</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">
Summary
</td>
            <td rowspan="1" colspan="1">
-
</td>
            <td rowspan="1" colspan="1">
0−80 ms
</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1">
6−16 ms
</td>
            <td rowspan="1" colspan="1">
3 - 10
</td>
            <td rowspan="1" colspan="1">
yes/no
</td>
            <td rowspan="1" colspan="1">
200 - 1000 Hz
</td>
            <td rowspan="1" colspan="1">
min velocity: 8°/s−18°/s; max velocity: 150°/s; vel. window: 5−15 ms; ignore time after saccade: 0−50 ms; (semi-)blinks and 200 ms before/after them; ...
</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>Recent years have witnessed an increase in the adoption of advanced visualization techniques by eye tracking researchers (see <xref rid="b7" ref-type="bibr">7</xref>, for a survey). However, no visualization techniques have been designed specifically for the analysis of microsaccades. Conventional visualization techniques tend to focus on low-frequency data (i.e., dwells, fixations, and saccades); for instance, scanpaths that visualize sequences of fixations and saccades (e.g., <xref rid="b28" ref-type="bibr">28</xref>) or attention maps (e.g., <xref rid="b60" ref-type="bibr">60</xref>). Visualizations can communicate derived statistics, which are often more informative than simply plotting the raw data, such as summary statistics of microsaccade direction and amplitude. Polar plots and rose plots are common methods to indicate the distribution of eye movement directions (e.g., <xref rid="b18" ref-type="bibr">18</xref>; <xref rid="b34" ref-type="bibr">34</xref>; <xref rid="b59" ref-type="bibr">59</xref>; <xref rid="b37" ref-type="bibr">37</xref>, <xref rid="b54" ref-type="bibr">54</xref>, <xref rid="b23" ref-type="bibr">23</xref>). Scatterplots are useful in depicting the main sequence relationship between peak velocities and microsaccade amplitudes (e.g., <xref rid="b18" ref-type="bibr">18</xref>) and histograms illustrate the data set’s distribution of microsaccade peak velocities, magnitudes, or durations (e.g., <xref rid="b51" ref-type="bibr">51</xref>). Otero-Millan et al. (2008) and McCamy et al. (2014), for example, use figures where raw data samples are plotted on top of the stimulus, highlighting microsaccades. In order to show temporal positions of microsaccades in relation to the eye movement, timelines are employed by McCamy et al. (2015) and Otero-Millan et al. (2014).</p>
    <p>Eye tracking data are oftentimes large data sets of time series prior to feature extraction. With visualization alone, it might be difficult to handle all of the data and to fully understand it. Visual analytics can be an invaluable tool in allowing researchers to understand and compare complex data sets (<xref rid="b11" ref-type="bibr">11</xref>). According to Thomas and Cook (2005), “visual analytics is the science of analytical reasoning facilitated by interactive visual interfaces.” Following their definition, visual analytics (<xref rid="b61" ref-type="bibr">61</xref>; <xref rid="b31" ref-type="bibr">31</xref>; <xref rid="b12" ref-type="bibr">12</xref>) can be a useful choice in the development of an eye tracking exploration system; different techniques are used by people to gain insights. In particular, interaction is an important addition to visualizations for complex data analysis and to support the analytical process. Interaction and filtering allow the user to extract important information and to further explore it. For example, interaction techniques such as brushing-and-linking (<xref rid="b62" ref-type="bibr">62</xref>) can allow researchers to connect corresponding data points across different data visualizations for the same data set. Furthermore, interactive filtering can allow researchers to vary the type and amount of data used for visualization in real time. For eye tracking data in general, different visual analytics methods have been investigated by Andrienko et al. (2012). Kurzhals and Weiskopf (2013) introduce a visual analytics method for dynamic stimuli. To the best of our knowledge, no visual analytics system has been developed for the visual exploration of microsaccade behavior that takes eye tracking data as input and allows for the real time adaptation of filter algorithms.</p>
    <p>Studies on microsaccadic behavior are often highly controlled psychophysical experiments (e.g., <xref rid="b18" ref-type="bibr">18</xref>; <xref rid="b34" ref-type="bibr">34</xref>; <xref rid="b25" ref-type="bibr">25</xref>), as opposed to studies that involve natural viewing tasks (c.f., <xref rid="b64" ref-type="bibr">64</xref>; <xref rid="b24" ref-type="bibr">24</xref>). As far as we know, no specific visualization software system exists to support the exploration of microsaccade distributions of eye movement data sets for the natural viewing of complex scenes. Our approach allows the analysis of both types of experiments. Generally, VisME is agnostic to experiment design and stimulus. This means that it can also support the comparison of data sets across different studies.</p>
    <p>In this paper, we present an interactive visual analytics system for high-frequency eye tracking data, with a focus on microsaccade exploration. We provide different visualization and interaction techniques to visualize conventional properties of microsaccade behavior (i.e., amplitude, direction, peak velocity, duration, and temporal and spatial distribution). In an analytical process, a combination of these techniques can be used for the exploration of eye tracking data sets. One key property of this system is that data visualizations can be achieved on different levels of analysis (i.e., fixation, trial, and test condition level) for individuals as well as groups of participants. The analysis is supported by brushing-and-linking and different visualizations that are shown simultaneously in multiple views. We have added the following visualization types to VisME to understand the relationship between space, time, and movement: stimulus view, timeline view, polar / rose plots, histograms, and scatterplots. Thereby, this is the first technical solution that allows users to interact and dynamically explore eye tracking data for the statistics of microsaccadic behavior. Given its popularity, we adopted the microsaccade detection algorithm by Engbert and Kliegl (2003b) as a starting point and included additional features to allow for interactive parameter control. Using our system, researchers are encouraged to vary these parameter values continuously in order to graphically compare their impact on the data set. In a what-if analysis, researchers are able to explore data sets and the influence of parameters. We present in a case study how this system can be employed on different eye movement data sets for natural scene viewing and demonstrate how interactive filtering allows researchers to inspect influences on the number and distribution of microsaccades across different levels (i.e., for fixations, participants, trials, and test conditions). In a usability study, we collected feedback from eye tracking researchers. They agreed that our system is useful for teaching purposes and eye tracking research alike. In particular, we report how visual analytics tools like VisME can promote data transparency, which is consistent with the current aims of open science (<xref rid="b48" ref-type="bibr">48</xref>).</p>
    <p>This work contributes by simplifying the exploration of microsaccadic data sets through interactive visual analytics. First, our system can be used to better understand how changes in the parameter values of microsaccade filters can influence the spatial and temporal distributions of microsaccades. Next, it is convenient for general visual exploration of microsaccades using interaction techniques like filtering to analyze microsaccades on different levels. In line with the increasing availability of eye movement data sets, our visual analytics system will help researchers and their reviewers critically discuss and (re-)analyze data. We provide the source code of our system (Munz, 2019) on GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/MunzT/VisME" xlink:show="new">https://github.com/MunzT/VisME</ext-link>.
</p>
    <p>In the next sections, we first provide some domain background and requirements for the system we propose. Then, we describe features of VisME, the visual analytics system we developed for analyzing microsaccades. In a subsequent case study, we demonstrate for two externally collected eye tracking data sets how our system can be used and present the qualitative feedback we collected from eye tracking experts in a usability study. A short conclusion and ideas for future work finalize our paper.</p>
  </sec>
  <sec id="S2">
    <title>Methods</title>
    <p>In this section, we provide details on the definition of microsaccades in the eye tracking domain and the requirements for the system we implemented. Afterward, we give an overview of our system’s features.</p>
    <p>We used a formative process for the development of our design, following the nested model by Munzner (2009). We focused on the outer parts of the model: domain problem characterization, data / operation abstraction design, and especially the encoding / interaction design. The process was performed in close cooperation with an eye tracking expert. In multiple sessions over a period of 16 months, our system was repeatedly refined.</p>
    <sec id="S2a">
      <title>Background to Microsaccade Detection</title>
      <p>The parameter settings of filter algorithms are crucial for the detection of eye movements (<xref rid="b2" ref-type="bibr">2</xref>; <xref rid="b8" ref-type="bibr">8</xref>; <xref rid="b30" ref-type="bibr">30</xref>). Poletti and Rucci (2016) addressed the challenges of defining microsaccades. Table 1 shows how the chosen parameter values and algorithmic features have differed across recent studies (i.e., 2009–2018) on microsaccades (a similar table for the period of 2004–2009 is provided by Martinez-Conde et al., 2009). In particular, Table 1 summarizes how the critical parameter values of the algorithm by Engbert and Kliegl (2003b) (e.g., λ, which is used to calculate velocity thresholds) vary across published studies. It is noteworthy that even the original values chosen by Engbert and Kliegl (2003b) and Engbert et al. (2006) differed, in order to accommodate for irrelevant variances across their different data sets. For example, Malinov et al. (2000) detected only two eye movements as microsaccades out of 3375 saccades using a value of 0.2° as maximum amplitude. In Table 1, it is visible that for all experiments larger amplitudes were used.</p>
      <p>Eye movements that are similar to microsaccades (i.e., glissades and square wave jerks) can be detected with the same algorithms (<xref rid="b28" ref-type="bibr">28</xref>) and are treated according to the researchers’ discretion. Square wave jerks are involuntary eye movements that move the eye first away from the visual target and then back onto it; they consist of two small saccades in opposite directions and some latency in-between (<xref rid="b28" ref-type="bibr">28</xref>). Glissades are post-saccadic eye movements before the eye comes to a halt; depending on the fixation filter, they are assigned to either fixations or saccades (<xref rid="b28" ref-type="bibr">28</xref>). In the literature, the term glissade might occasionally be applied with different definitions (e.g., <xref rid="b4" ref-type="bibr">4</xref>; <xref rid="b63" ref-type="bibr">63</xref>; <xref rid="b14" ref-type="bibr">14</xref>). In the following, we regard as glissades all types of high-velocity over- and undershoots directly succeeding saccades (<xref rid="b28" ref-type="bibr">28</xref>). Abadi and Gowen (2004) do not differentiate between microsaccades and glissades; Hafed and Clark (2002) handle square wave jerks as a pair of microsaccades in opposite directions. </p>
      <p>Given that the extraction of microsaccades can differ across different studies, it is unclear if findings can be expected to reasonably generalize from one study to another. It is often unclear why specific parameter values have been chosen, or if other values might have produced different results. Thus, VisME was developed to support researchers in inspecting the influence of parameter variations.</p>
    </sec>
    <sec id="S2b">
      <title>System Requirements</title>
      <p>We identified the following requirements for our application to explore microsaccades, serving as a basis for the visual encoding and interaction design:</p>
      <p>Ability to:</p>
      <p>explore microsaccades in the context of the entire eye tracking data in time and space.</p>
      <p>explore the relationship between space and time for microsaccades.</p>
      <p>explore microsaccadic properties.</p>
      <p>explore individuals and groups of participants.</p>
      <p>explore the location of microsaccades within fixations.</p>
      <p>change parameters for microsaccade detection.</p>
      <p>study statistical values when changing parameters.</p>
      <p>differentiate between microsaccades and similar eye movements.</p>
      <p>explore the influence of fixation filters on microsaccades.</p>
      <p>explore the relationship between microsaccades and test conditions.</p>
      <p>integrate VisME into existing analysis pipelines.</p>
      <p>We believe that interactive visualization along with data filtering is a most appropriate approach to support scientists exploring these aspects in a visual analytics system.</p>
    </sec>
    <sec id="S2c">
      <title>Visual Analytics System</title>
      <p>In this section, we detail the implemented filters for microsaccades and fixations, and the different visualization and interaction techniques employed to explore high-frequency eye tracking data. For the analysis, fixations have to be determined first. Afterward, microsaccades can be detected within their time ranges and different views can be used for visualizations and interaction. The general user interface design can be seen in Figure 1.
</p>
      <fig id="fig01" fig-type="figure" orientation="portrait" position="float">
        <label>Figure 1.</label>
        <caption>
          <p>Screenshot of the user interface showing visualizations of high-frequency eye tracking data in three linked views: (top left) gaze positions with highlighted fixation and microsaccade samples on top of the stimulus, (bottom left) temporal dependency of the eye movements and microsaccades, (right) rose plot of microsaccade directions.</p>
        </caption>
        <graphic id="graph01" xlink:href="jemr-12-06-e-figure-01"/>
      </fig>
    </sec>
    <sec id="S2d">
      <title>Eye Movement Filters</title>
      <p>Eye tracking data can be grouped into different eye movement classes. Often, these movements can be pre-calculated by the eye tracking system itself. In VisME, it is possible to explore eye movements (fixations and microsaccades) that were determined in pre-processing or with the system itself. In the following paragraphs, we describe the filters we use in our application to interactively label microsaccades and fixations.</p>
      <p><bold>Microsaccade Filter</bold> – In contrast to detecting fixations, eye trackers do usually not provide filters for microsaccades. For interactive microsaccade detection, we chose the velocity-threshold algorithm by Engbert and Kliegl (2003b) as an example; other algorithms could have been used as well. A free parameter λ is used for a velocity threshold in 2D space and a minimum microsaccade duration is also fixed.</p>
      <p>Available parameters for microsaccade detection in our system are:</p>
      <p>λ for the velocity threshold</p>
      <p>minimum and maximum duration</p>
      <p>minimum and maximum amplitude</p>
      <p>minimum and maximum peak velocity</p>
      <p>velocity window size</p>
      <p>time being ignored at beginning / end of fixations (e.g., to ignore glissades)</p>
      <p>time being ignored after a microsaccade, i.e. minimum inter-saccadic interval (to ignore overshoots)</p>
      <p>time being ignored before / after missing data</p>
      <p>monocular or binocular microsaccades</p>
      <p>We decided upon these parameters as they were prominently mentioned in previous work (see Table 1). Initially, we set the parameters in our system according to the values mentioned by Engbert and Mergenthaler (2006) (λ = 5, minimum duration = 6 ms, velocity window size = 5 samples, binocular microsaccade detection, maximum amplitude = 1°) and set minimum inter-saccadic interval to 20 ms and ignored the first 20 ms of fixations.</p>
      <p><bold>Fixation / Saccade Filter</bold> – Our method uses the same algorithm that we implemented for detecting microsaccades but with different default parameter values (λ = 8, minimum saccade duration: 3 ms, velocity window size: 9 samples, minimum saccade amplitude: 1°, minimum inter-saccadic interval: 50 ms). This method is a saccade filter and fixations are defined as the intervals between two saccades. Mergenthaler and Engbert (2010), Otero-Millan et al. (2008), Sinn and Engbert (2011), and Laubrock et al. (2010) also used this algorithm to detect saccades. When applying different fixation filters or parameter values, it is possible that some fixations are not detected at all or multiple fixations are detected as only one fixation. While one algorithm detects two fixations connected by a saccade, another might detect just one fixation that contains a microsaccade. This, of course, influences the relationship between microsaccades and fixations. Also, if more or fewer data samples are included within a fixation, the center of a fixation shifts.</p>
    </sec>
    <sec id="S2e">
      <title>Visualizations</title>
      <p>To explore the distribution of microsaccades, our application provides multiple linked views that can be explored interactively: A stimulus view, which shows the stimulus, the raw data, and a scanpath visualization; a timeline view for visualizing the temporal distribution of microsaccades in relation to the eye movement; data plots used for visualizing descriptive statistics and details on microsaccade distribution; and histograms and scatterplots for further microsaccadic properties.</p>
      <p><bold>Stimulus view</bold> – The stimulus view provides an overview of the eye tracking data. In this view, all raw eye tracking samples, fixation samples, microsaccade samples, and missing data ranges can be shown or highlighted (Figure 1 (top left) and Figure 2 (a)). A plot with directional microsaccade distributions can be displayed with lines that connect the start and end samples of microsaccades colored with a gradient to encode the directions and locations of the microsaccades on the stimulus (Figure 2 (b)). A common scanpath visualization for fixations and saccades is also available (Figure 2 (c)). The size of the fixations can be either in relation to the duration of fixations, the number of microsaccades within fixations, or in such a way that each circle has the same size. Using filters, it can be decided which information shall be visible in this view.</p>
      <fig id="fig02" fig-type="figure" orientation="portrait" position="float">
        <label>Figure 2.</label>
        <caption>
          <p>Different aspects of the eye tracking data can be visualized in the stimulus view. (a) Connected raw samples (green), connected fixation samples (black), connected microsaccade samples (pink), and missing data range (purple). (b) Directional microsaccade lines: pink for start sample, white for end sample. (c) Scanpath (blue) with fixations represented by circles (their size is in relation to the number of microsaccades within them) and saccades by lines.</p>
        </caption>
        <graphic id="graph02" xlink:href="jemr-12-06-e-figure-02"/>
      </fig>
      <p><bold>Timeline</bold> – The timeline gives a better understanding of the relationship between time, the eye positions, and eye events. It is visible where fixations are located and microsaccades appear. As some experiments depend on specific temporal events, event positions can be highlighted as well. This view is zoomable to see more details of the surroundings of microsaccades and fixations, see Figure 1 (bottom left) and Figure 3.
</p>
      <fig id="fig03" fig-type="figure" orientation="portrait" position="float">
        <label>Figure 3.</label>
        <caption>
          <p>Timeline view: timeline (top) and zoomed timeline (bottom) to explore more details. Timelines show the eye positions in x (orange) and y (turquoise) direction, the velocity (gray) determined by these positions, missing data ranges (purple) in x and y direction, fixation areas (blue), and microsaccade locations (pink).</p>
        </caption>
        <graphic id="graph03" xlink:href="jemr-12-06-e-figure-03"/>
      </fig>
      <p><bold>Data Plots</bold> – In a separate section of our system, descriptive statistical graphics (e.g., rose plots) and further fixation-related visualizations are used to show more details on the data set in relation to microsaccades and the chosen parameters. Rose plots (see Figure 1 on the right side) or polar plots are used to show the direction of microsaccades; the data is aggregated to a specified number of bins (if not mentioned otherwise, 12 bins are used in all graphics). If multiple test conditions are specified, data of each test condition type can be visualized with another color value. Additionally, the mean direction and standard deviation are shown in black. To see the microsaccade distributions, especially the number and locations within fixations, the plot type can be changed; see Figure 4 (a) for an example. In this image, all fixations of a trial are plotted on top of each other; microsaccades are highlighted in pink. A similar plot that can be created with VisME shows only microsaccade directions in relation to the fixation center. The same start and end points of microsaccades are used as shown in the image, and a color gradient is applied to indicate their direction similar to the direction plots in the stimulus view.</p>
      <fig id="fig04" fig-type="figure" orientation="portrait" position="float">
        <label>Figure 4.</label>
        <caption>
          <p>(a) Position of microsaccades relative to fixations of a trial in the context of the fixations that are plotted on top of each other (microsaccades are highlighted in pink). The marker value indicates the extent at the marked position on the x and y axis; values are measured in visual degree. (b) Histogram for microsaccade peak velocity and (c) the relationship between amplitude and peak velocity as scatterplot using a logarithmic scale for the painting data set.</p>
        </caption>
        <graphic id="graph04" xlink:href="jemr-12-06-e-figure-04"/>
      </fig>
      <p>While all the data visualized in this area can be shown with microsaccade directions in screen coordinates, it is also possible to transform the data in such a way that directions of microsaccades are rotated toward the next fixation. For these plots, the direction to the next fixation points to the top (e.g., 0° in rose plots). This transformation is useful for a better understanding of microsaccades and the whole eye movement. To determine the relationship toward the next fixation, a direction vector between two fixation centers is used. For the rose plots, a microsaccade within the first fixation is translated in such a way that its start point is located at the center of the corresponding fixation. The angle between the vector of a microsaccade (from start to end sample) and the direction vector determine the angle used for the direction. For the other data plots, the direction vector determines how all data of a fixation has to be rotated.</p>
      <p><bold>Histograms and Scatterplots</bold> – A further area of the system provides the possibility to explore the temporal locations, durations, amplitudes, and peak velocities of microsaccades in histograms and scatterplots (Figure 4 (b) and (c)). Especially scatterplots that show the relationship between amplitude and peak velocity are commonly used for both saccades and microsaccades to explore the main sequence. Additionally, histograms show the temporal distribution of microsaccades within fixations.</p>
    </sec>
    <sec id="S2f">
      <title>Interaction Techniques</title>
      <p>Our visual analytics system provides many interaction methods to explore the raw data on different levels, i.e., for fixations, participants, trials, and test conditions. Interaction supports the analysis process for microsaccades for which we will show some examples in our case study. With brushing-and-linking, a combined perspective of different aspects of the data shown in different views can be obtained. The parameters for detecting microsaccades can be modified, the visible data can be filtered on different levels, and standard navigation techniques such as zooming and panning are available. It is possible to select which eye data (left / right / averaged) should be used for analysis and a rubber band is available to select a time range in the timeline. The filters allow exploration of the data on different levels: for participants, trials, and test conditions. Furthermore, the amount of data visualized in the different views can be adapted by hiding different elements. As all views are linked, it is possible to select a fixation (if a single trial is being explored) in either time, space, or a list of all fixations to highlight this fixation; it will be highlighted in the other views as well, the corresponding data plots will be shown, and some details on the fixation will be displayed. Additionally, it is possible to walk through the scanpath for every fixation. As it is also of interest to see relationships of sequential fixations, it can be specified how many neighboring fixations shall be visualized and the data plots will include these fixations as well. Moreover, some statistical values about the selected trials are updated when parameters change. These include information about fixations (e.g., count, duration, or percentage containing microsaccades) and microsaccades (e.g., count, duration, amplitude, peak velocity, count per second / fixation, or inter-saccadic interval). </p>
    </sec>
    <sec id="S2g">
      <title>Data Import and Export</title>
      <p>Our application uses its own format for input data to be independent of any eye tracker. High-frequency eye tracking data is required as input, with a minimum frequency of 200 Hz in order to detect microsaccades (Holmqvist et al., 2011). For each participant, a separate file is required that can contain multiple trials specifying raw samples, fixations, microsaccades, and event locations; a second file type can contain information about test conditions (e.g., tasks or other trial-specific circumstances). Files with the current (possibly calculated) eye movement data (raw data, fixations, microsaccades, and events) can be exported for further analysis with other statistical software such as R, Python, or MATLAB and for later import into VisME. Additionally, aggregated statistics for different properties of fixations and microsaccades containing data for each participant and test condition can be exported for analysis in other applications. More details about the file formats used in VisME are available by Munz (2019) and on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/MunzT/VisME" xlink:show="new">https://github.com/MunzT/VisME</ext-link>).
</p>
      <p>Our system can be integrated in full study and analysis pipelines. Researchers have to convert their eye tracking data into our expected input format. It is possible to import detected microsaccades determined in other steps of their analysis to verify their properties or to calculate a new set of microsaccades with VisME by visually inspecting the result. Our system can then be used to explore the data and to export the raw data along with detected microsaccades as well as aggregated data for further processing.</p>
    </sec>
    <sec id="S2h">
      <title>Implementation Details</title>
      <p>Our application is platform-independent and was tested on Windows 10 and Linux. It is implemented in C++ with Qt 5.9 using the Graphics View Framework for interactive visualizations. We reimplemented the R code available by Engbert et al. (2015) to detect microsaccades and added some additional conditions as described previously.
</p>
      <p>In pre-processing steps for the case study and user study, movements recorded by an EyeLink eye tracker were first converted from .edf files to .asc files using the converter available by SR Research: the EyeLink EDF2ASC Converter. Then, we used a script written in Python to create an input file in the format expected by our application, containing eye tracking positions and fixations.</p>
      <p>The source code of our implementation, a detailed description of the input and output formats and the Python script are provided by Munz (2019) and on GitHub together with a more detailed manual for the system, so that the system can be readily used and the data file converter can be easily adapted for other raw data. Currently, only the microsaccade filter as described before is implemented, but the system is designed that other methods can be implemented and added with little effort. In order to explore microsaccades detected with other algorithms, the import of the data into our system is possible.</p>
    </sec>
  </sec>
  <sec id="S3">
    <title>Results</title>
    <p>To demonstrate how our application can be used, we use externally collected high-frequency eye tracking data of two independent experiments for our case study and in a usability study. </p>
    <sec id="S3a">
      <title>Eye Movement Data Set</title>
      <p>In both experiments that were conducted to collect the two data sets, participants were asked to look at images with some given task for the exploration. Eye movements were recorded with remote eye trackers (Eyelink 1000, SR Research) using a chin rest to stabilize heads of participants. Fixations were determined by the eye tracking software of the systems. While for the first data set monocular data is available, the second data set allows binocular analysis as well.</p>
      <p>As a first data set (photo data set), we use data collected from experiment 3 of Greene et al. (2012). It contains eye tracking data on 16 participants who viewed 20 grayscale natural images for 60 seconds whilst performing one of four tasks. The tasks were: memorize the picture (memory), identify the decade in which the picture was taken (decade), assess how well the people in the picture know each other (people), and determine the wealth of the people in the picture (wealth). The data was recorded at 1000 Hz and only the right eye was tracked. The data set was originally created to verify Yarbus’s assumption that the eye movement is highly influenced by an observer’s task. We chose this data set because it is high-frequency data that allows extraction of microsaccades and participants performed different high-level cognitive tasks that were likely to have engaged covert attention even if the study was not designed to investigate this aspect of gaze behavior. Many researchers believe that covert attention can influence the frequency of microsaccades and their directions (e.g., <xref rid="b18" ref-type="bibr">18</xref>; <xref rid="b19" ref-type="bibr">19</xref>; <xref rid="b25" ref-type="bibr">25</xref>; <xref rid="b26" ref-type="bibr">26</xref>; <xref rid="b52" ref-type="bibr">52</xref>; <xref rid="b34" ref-type="bibr">34</xref>).
</p>
      <p>The second data set (painting data set) was recorded to explore the occurrence of microsaccades in free-viewing conditions. It is available at 500 Hz for both eyes. Gaze samples for averaged eye movements were determined as the mean value of right and left eye positions and fixations as the maximum fixation areas of both eyes. 20 participants looked at 60 randomized colored paintings showing multiple persons for about 15 seconds per image. The participants’ task was to pay attention to the presented people, their mood, and relationship to each other (acquaintance) and they had to answer questions afterward.</p>
    </sec>
    <sec id="S3b">
      <title>Case Study</title>
      <p>In our case study, we demonstrate how it is possible to visually explore the directions and distributions of microsaccades with VisME. Additional examples and details are provided in the supplemental material.</p>
      <p>We use the initial parameter values mentioned before for our exploration.</p>
      <p><bold>Detecting Glissades</bold> – Glissades are eye movements that immediately succeed saccades. In order to demonstrate the confusability of microsaccades and glissades at the start of fixation periods, we changed the potential glissade duration to 0 ms; this is an adjustable parameter in VisME. Subsequently, we inspected the stimulus view and timeline with highlighted microsaccades of one trial. We noticed that many detected microsaccades were both spatially and temporally located at the beginning of fixations, right after saccades. Velocity peaks also indicated that they might be microsaccades. However, as they are located right after saccades, they are more likely to be glissades (see Figure 5). Additionally, we had a look at the histogram for the temporal location of microsaccades within fixations (Figure 6 (a) and (b)): it is visible that for both the current trial and the whole data set there is a high peak for potential microsaccades within the first 40 ms, which might be glissades instead. This suggests that this phenomenon is present in the whole data set. Thus, we modified the parameter for the potential glissade duration from 0 ms to 40 ms for the painting data set for the remaining part of the analysis. Note that this parameter would be arbitrarily determined by most researchers with no opportunity for re-analysis by others, in the absence of a tool like VisME. In Figure 6 (c) and (d), the early fixation period was excluded from the analysis, which resulted in a more suitable distribution of detected microsaccades. There is still a peak in the second bin (Figure 6 (d)) but visibly diminished relative to the previous first bin. This could explain why Hicheur et al. (2013) did not consider microsaccades within even 50 ms after a saccade. We noticed this behavior in both data sets; for the photo data set, we chose a threshold of 20 ms to obtain a similar distribution. Depending on the chosen fixation filter and possible other reasons we are not aware of, this value might have to be chosen differently for other data sets. Our system can be used to find a suitable minimum time range to remove glissades without removing actual microsaccades from further analysis.</p>
      <fig id="fig05" fig-type="figure" orientation="portrait" position="float">
        <label>Figure 5.</label>
        <caption>
          <p>Eye movement detected as microsaccade (pink); it can be identified as glissade on the stimulus and in the timeline: (a) Connected samples that belong to the microsaccade are located at the beginning of the fixation (black). (b) Zoomed view of the timeline: the detected microsaccade is located at the beginning of a fixation (blue) right after a saccade. (x values: orange, y values: turquoise, velocity: gray). Example used from the painting data set.</p>
        </caption>
        <graphic id="graph05" xlink:href="jemr-12-06-e-figure-05"/>
      </fig>
      <fig id="fig06" fig-type="figure" orientation="portrait" position="float">
        <label>Figure 6.</label>
        <caption>
          <p>Temporal positions of detected microsaccades as a histogram for (a) one trial and (b) all trials of all participants in the painting data set (bin size: 40 ms). A high peak in the first bin indicates that many glissades (overshoots right after saccades, here in the subsequent 40 ms) were detected as microsaccades. Figures (c) and (d) show the same data when all microsaccades within the first 40 ms of a fixation are ignored from the analysis.</p>
        </caption>
        <graphic id="graph06" xlink:href="jemr-12-06-e-figure-06"/>
      </fig>
      <p><bold>Microsaccade Directions for Test Conditions</bold> – To visualize the microsaccade directions across different tasks, rose plots were employed to illustrate, for all available trials, the distribution of microsaccades in every direction (Figure 7). This reveals potential asymmetry in the circular distribution of microsaccadic movements. We use both data sets and differentiate trials for the photo data set by different tasks and for the second one we visualize all data together. In the first row, the photo data set reveals a tendency for microsaccades to be oriented horizontally and vertically within the image. For the painting data set, microsaccadic movements were biased toward the top, which could be indicative of a recording bias. The rose plots in the second row illustrate the directions of microsaccades relative to the next fixation (toward the next fixation means to the top of the graph). The photo data set revealed a tendency of microsaccades toward the next fixation (especially for the task people), indicating that microsaccades predict the next fixation. Additionally, there is also a strong tendency toward the opposite direction. For the painting data set, a similar vertical bias was visible that was less pronounced and favored the opposite direction to the next fixation. In these plots, no correlation between the two tasks related to people (people and acquaintance) are visible, but a similarity of data recorded in the same experiment can be seen. A possible reason why these different results are obtained might be due to data quality and existing noise when using monocular data.</p>
      <fig id="fig07" fig-type="figure" orientation="portrait" position="float">
        <label>Figure 7.</label>
        <caption>
          <p>Rose plots for microsaccade distributions for the 4 different tasks of the photo data set (memory, decade, people, and wealth) and the painting data set (acquaintance). First row: directions of microsaccades in screen coordinates as visible on the stimulus. Second row: microsaccade directions in local coordinates: they are rotated in such a way that 0° means that a microsaccade is in the direction to the next fixation. Min and max specify the values in the middle of the plot and on the circle, respectively; values are measured in microsaccade count.</p>
        </caption>
        <graphic id="graph07" xlink:href="jemr-12-06-e-figure-07"/>
      </fig>
      <p><bold>Microsaccade Directions for Participants</bold> – Our next step was to examine the data on an individual level for different participants. For selected participants, images are visible in Figure 8. It is possible to see how data can vary between different participants: the images show results for participants that might indicate that microsaccades move toward the next fixation, the opposite direction, or in arbitrary directions. Such visualizations could support researchers in identifying different microsaccade patterns available for certain participants and compare if patterns are similar to the aggregated directional distribution of all participants (see Figure 7 (acquaintance, bottom)) or rather outliers. </p>
      <fig id="fig08" fig-type="figure" orientation="portrait" position="float">
        <label>Figure 8.</label>
        <caption>
          <p>Rose plots showing microsaccade directions rotated toward the next fixation for a selection of participants from the painting data set. Participants whose microsaccades have a direction that (a) tend to be toward the next fixation, (b) toward the opposite direction, and (c) that are equally distributed toward each direction.</p>
        </caption>
        <graphic id="graph08" xlink:href="jemr-12-06-e-figure-08"/>
      </fig>
      <p><bold>Microsaccade Detection for Changed Parameters</bold> – In order to see the strong influence of parameters on the detection of microsaccades we used two different settings of parameter values taken from the value ranges visible in Table 1: the first settings contain values that result in many microsaccades (minimum inter-saccadic interval: 80 ms, maximum amplitude: 1°, minimum duration: 12 ms, λ = 8, velocity window: 5 samples, ignore time at beginning of fixations: 50 ms; parameters not mentioned were deactivated) and the other ones in few (maximum amplitude: 2°, minimum duration: 6 ms, λ = 3, velocity window: 5 samples). The use of the first parameter settings missed many actual microsaccades and the second one detected many false microsaccades. The difference in the number of detected microsaccades varied for the trial visible in Figure 9 from only 4 to 184 (if glissades were excluded (here, the first 20 ms of a fixation), still 138). It is apparent that there is a large range of possible sets of microsaccades that can be detected with values chosen between these two different settings. The visual analytics system can help in exploring the data to choose appropriate values for a given data set. A visual inspection in the stimulus view is necessary to confirm if the parameter changes improve the detection of microsaccades or result in false positives or missed microsaccades. </p>
      <fig id="fig09" fig-type="figure" orientation="portrait" position="float">
        <label>Figure 9.</label>
        <caption>
          <p>(a) Result of the microsaccade filter when changing all parameter values in such a way that more microsaccades can be detected using values in the range as given in Table 1 and (b) when using parameter values to limit the detection. A trial from the <italic>photo</italic> data set is shown.</p>
        </caption>
        <graphic id="graph09" xlink:href="jemr-12-06-e-figure-09"/>
      </fig>
      <p>To further explore the influence of parameter choices, we provide as supplemental material an overview of polar plots, histograms, and main sequences for both data sets when changing individual parameter values to the maximum or minimum values given in Table 1. We created aggregated statistical distributions for all participants and trials to show how they change when adapting parameter values. In the following, we compare the results of our default settings to some adaptions. We can see a strong influence for different velocity threshold (λ) values. Mergenthaler and Engbert (2010) use λ = 3 for free-viewing experiments. Using the same value for our data sets shows an increase in the number of detected microsaccades by a factor of three. The main direction changes toward the bottom and there are especially more detected microsaccades with smaller velocity, amplitude, and duration. Some of them are also located at a later position during a long fixation. When using λ = 10 (<xref rid="b29" ref-type="bibr">29</xref>) instead, fewer microsaccades are detected, which are mostly moving to the left and right side. The number of microsaccades is reduced for small peak velocities, high duration, and small amplitudes. When the threshold for the minimum duration is increased, fewer microsaccades with smaller duration and smaller amplitude are detected. Changing the size of the velocity window (<xref rid="b9" ref-type="bibr">9</xref>) shows only for the painting data set a noticeable different pattern. In the main sequence, we see microsaccades with very large velocity values; this might be an indication for noise. The other plots also show slightly different patterns (e.g., the duration histogram seems to be shifted). As the painting data set was recorded for both eyes, we can compare the influence of using monocular or binocular detection. Using the monocular detection, more microsaccades are detected. There is an increase of microsaccades with higher peak velocity, smaller duration, and smaller amplitude. We can also see that many microsaccades have a direction toward the bottom of the screen.</p>
      <p><bold>Influence of Fixation Filters</bold> – As the fixations themselves also influence microsaccades, we used the fixation filter of our system (see section Eye Movement Filters for details) to detect different fixations than the ones already determined by the software of the eye tracker. In Figure 10, the raw data with highlighted fixation and microsaccade samples is visible for an example scene. The left images were created for the fixations provided by the eye tracker, the right ones for the built-in fixation filter. It is visible that the fixations change most notably in size and hence the microsaccade amount and directional pattern change as well. </p>
      <fig id="fig10" fig-type="figure" orientation="portrait" position="float">
        <label>Figure 10.</label>
        <caption>
          <p>Usage of another fixation filter influences the sample ranges used for microsaccade detection and thus the rose plot visualizations. The left images use the fixations from the eye tracker, the right ones the fixations calculated by the application. Microsaccade detection was performed in both cases using the microsaccades filter with default parameter values. In the first row, some differences in the detected fixations are highlighted in blue. In the second row, it is visible that the microsaccade distribution for the trial changes as well.</p>
        </caption>
        <graphic id="graph10" xlink:href="jemr-12-06-e-figure-10"/>
      </fig>
      <p>In our exploration process, we observed that microsaccade distributions depended on different factors. Fixation labeling defined the data that was used for the extraction of microsaccades and defined the spatial relationship between microsaccades and the next fixation. Microsaccade parameters are important for the calculation of microsaccades within these fixations. They influence, for example, if confusable eye movements (e.g., glissades) will be regarded as microsaccades, which has a strong influence on the overall statistics. By looking at individual trials, it is possible to determine if small microsaccades were missed or if fixation samples might have been mislabeled as microsaccades. Overall it is possible to interactively explore the effects of parameter value changes using all previously mentioned visualizations and further statistical values provided by the system. For instance, for unknown data sets it is possible to verify if parameter choices for microsaccades exploration were appropriate. Additionally, we showed that our system can handle both monocular and binocular eye movement analysis and it is not limited to one type only. It is up to the researcher to handle noise in pre-processing and to verify if detected microsaccades are correct. Aligned with our idea to support reproducibility, critical readers of research reports who want to verify the plausibility of suggested research results benefit from such a system. While there is an increase in sharing raw data when publishing research, it additionally requires a software system to reanalyze the data or to reproduce the results. With VisME, researchers will be able to check for themselves if applied parameter settings or filters are appropriate or if there might be other settings that would better fit the data.</p>
    </sec>
    <sec id="S3c">
      <title>Usability Study</title>
      <p>We performed a usability study with a think aloud protocol analysis (<xref rid="b21" ref-type="bibr">21</xref>) to collect qualitative feedback on the usefulness and usability of VisME for eye movement researchers. First, participants completed a questionnaire on their research background. Next, we introduced them to our system by explaining VisME’s main features. Subsequently, participants used the system to explore the painting data set of our case study. We provided a list of tasks to guide the participants’ use of the system and to support their awareness of available features. Nonetheless, participants were also encouraged to freely explore the use of the system for whichever aspects interested them. The participants were able to explore how adjusting different parameter values influenced derived microsaccadic statistics by observing how doing so impacted the different visualizations. Some participants asked for specific features and if they were available in the system; we demonstrated them if such features were already implemented. Finally, the participants were asked to complete a standardized questionnaire on usability (<xref rid="b10" ref-type="bibr">10</xref>) that was extended to include some eye tracking specific questions. Each question was rated on a Likert scale from 1 (strongly disagree) to 5 (strongly agree). Participants could provide additional text feedback and suggestions for future improvements.</p>
      <p>We conducted the user study with 12 voluntary and independent eye tracking experts (3 women and 9 men; 11 participants were between 20 and 39 years old and one was at least 50 years old) who are not authors of this paper. The background of our participants includes computer science, physics, and neuroscience. We wanted to have eye tracking experts with different goals in their research and especially some who are working with microsaccades. The eye tracking researchers belong to four different research groups with specializations on different eye tracking and eye movement related areas. These areas include the analysis of microsaccades, the relationship of eye movements and neuroscience, the development of medical devices to enhance vision, and the development of new eye movement related algorithms. Three of our participants had more than five years of experience with eye tracking, four between three and five years, and five less than three years. Eight of the researchers rated their eye tracking experience from 4 (high) to 5 (very high) on a scale from 1 to 5 (mean: 3.9). Proficiency with microsaccades varied among participants: Four stated to have high and very high experience (4 and 5), whereas six stated to have little and very little experience (1 and 2); the mean value was 3. More than half of all participants claimed to use visualizations often in their analysis. </p>
      <p>Before starting the actual experiment, we asked the participants about their understanding of microsaccades and how they would filter for them. Depending on their proficiency with microsaccades, they could provide detailed information. Most researchers mentioned that microsaccades are located within fixations. The maximum amplitude of microsaccades was stated ranging from 0.5° up to 2° by different participants but most of them employ a threshold of 1°. Furthermore, some mentioned that a manual inspection of each detected microsaccade is very important. For the detection of microsaccades, the algorithm by Engbert and Kliegl (2003b) was named by a few researchers, which confirms that our initial algorithm choice was appropriate. For some researchers, it is very important that the detection is done on binocular data while others use monocular data. This also shows that it is important to support both types of data analysis in our system. Additionally, we asked participants how they would process eye tracking data to explore microsaccade distributions. Most of them described similar approaches to the one we realized in VisME. Additionally, they would verify the data quality at the beginning. Most participants would use MATLAB as analysis software.</p>
      <p>All participants liked our system, were able to use it without any problems, and gave a lot of positive feedback. They agreed that our system could be beneficial for teaching purposes (mean: 4.9) and that the eye tracking community would benefit from such a tool (mean: 4.3). The question if participants would prefer to use this software over the steps they described initially for exploring microsaccades had a mean value of 3 and the highest standard deviation. This roughly correlates with the proficiency with microsaccades analysis. Researchers who have less experience were more likely to choose this software. All researchers agreed that VisME served its intended purpose of rendering the analysis of microsaccades more reliable and transparent. </p>
      <p>In particular, the interactive and visual features of our system were received positively. The possibility to click on fixations that are then highlighted in both time and space, and the possibility to scroll through trials were highlighted as a preferred feature. Many researchers stated that VisME is especially useful to get a quick overview of the data.</p>
      <p>Recommendations for additional features varied widely depending on the participants’ research background and interests. For a system supporting the whole analysis process, participants suggested that our system should be able to deal with pre-processing steps like data smoothing or processing of eye blinks as well; currently, pre-processing of raw data has to be performed externally. Furthermore, manual inspection of microsaccades is very important. Therefore, manual correction of both fixation and microsaccade areas would be required to adapt detected eye movements that were not marked correctly. Additionally, analysis related to specific events is very important, especially in controlled experiments. In our system, it is possible to visualize positions of events, but it is not possible to consider temporal aspects related to the events in the analysis. Currently, it is possible to analyze microsaccade directions in relation to neighboring fixations; two participants asked for an extension to allow this analysis to be performed in relation to arbitrary target positions on the stimulus (e.g., the center of an image). As there are many different approaches available to detect microsaccades, some participants wished support of further algorithms.</p>
    </sec>
  </sec>
  <sec id="S4">
    <title>Discussion</title>
    <p>We present a visual analytics system for exploring eye tracking data, with focus on microsaccade analysis. With this system, eye movement researchers will be able to explore and understand microsaccade distributions in time and space. In particular, the interactive nature of the visualizations, namely the ability to vary multiple parameter values allows researchers to determine how sensitive their findings are to parameter variations. This system is tailored for research purposes in that it allows researchers to analyze microsaccadic patterns on the level of participants, trials, and test conditions. It also allows for flexible adjustments of parametric values across these levels in order to account for huge individual differences, if necessary.</p>
    <p>Our system will allow for more transparent discourse between researchers and increase the value of public data sets; it will support reproducibility and promote open research. Eye movement researchers will be able to decide for themselves if appropriate parameter values were used as well as to discover unexpected eye movement behavior or verify novel hypotheses on old data. Our system is especially helpful to get an overview of available data sets and provides a simple approach to explore microsaccades for researchers with little experience. In addition, it can serve as an instruction system to help researchers better understand microsaccade movements and issues in their detection. While there remain many possible additions for the application, we believe that eye movement researchers will profit from its visual analytics features to explore microsaccades.</p>
    <p>Based on the informal recommendations of eye movement researchers and the feedback from our user study, we have identified some aspects of the current implementation that can be improved as well as features that might be appreciated in future versions; many suggestions have already been mentioned in the results of the user study.</p>
    <p>To begin, we implemented the algorithm by Engbert and Kliegl (2003b) for both microsaccade and saccade detection. However, others might be interested in employing different filters for microsaccades (e.g., <xref rid="b49" ref-type="bibr">49</xref>; <xref rid="b45" ref-type="bibr">45</xref>; <xref rid="b5" ref-type="bibr">5</xref>). The introduction of plug-ins would allow researchers to more easily introduce alternative filters. Additionally, auxiliary algorithms exist explicitly to identify eye movements such as square wave jerks. Such algorithms can be easily introduced to the current implementation as an additional and optional processing step.</p>
    <p>Currently, the direction vectors between consecutive fixations are defined in terms of their centroids. It has been suggested that we could allow for this direction vector to be flexibly defined – for example, in terms of the last data point of a fixation and the first data point of the subsequent fixation.</p>
    <p>The current microsaccades filter assumes that a participant has the same distance to the display for all trials; this distance is used to compute visual angles required for detecting microsaccades. An alternative that would be more accurate is the use of 3D eye position data of each time step (which is provided by, e.g., the Tobii Pro Spectrum) to determine more precise visual angle values for every time step that then can be used to get more reliable results with the eye movement filters.</p>
    <p>Furthermore, we intend to improve the visualization of microsaccadic distributions by introducing temporal aspects. This will allow us to visualize variations between early, mid, and late microsaccades. Speculatively, this distinction could result in different patterns – for example, late microsaccades might be more predictive of the following fixation.</p>
    <sec id="S4a" sec-type="COI-statement">
      <title>Ethics and Conflict of Interest</title>
      <p>The author(s) declare(s) that the contents of the article are in agreement with the ethics described in <ext-link ext-link-type="uri" xlink:href="http://biblio.unibe.ch/portale/elibrary/BOP/jemr/ethics.html" xlink:show="new">http://biblio.unibe.ch/portale/elibrary/BOP/jemr/ethics.html</ext-link> and that there is no conflict of interest regarding the publication of this paper. </p>
    </sec>
    <sec id="S4b">
      <title>Acknowledgements</title>
      <p>This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 251654672 – TRR 161 (Projects B01 and C06) and under Germany’s Excellence Strategy – EXE-2075 – 390740016. The authors wish to thank Jeremy Wolfe, Harvard Medical School, and Michelle R. Greene, Bates College, for their permission to use their data set to exemplify the application. Furthermore, the authors would like to thank all researchers who participated in their user study or gave informal feedback to their system.</p>
    </sec>
  </sec>
</body>
<back>
  <ref-list>
    <ref id="b1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abadi</surname>, <given-names>R. V.</given-names></string-name>, &amp; <string-name><surname>Gowen</surname>, <given-names>E.</given-names></string-name></person-group> ( <year>2004</year>). <article-title>Characteristics of saccadic intrusions.</article-title>
<source>Vision Research</source>, <volume>44</volume>( <issue>23</issue>), <fpage>2675</fpage>– <lpage>2690</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2004.05.009</pub-id>
<issn>0042-6989</issn>
<?supplied-pmid 15358063?><pub-id pub-id-type="pmid">15358063</pub-id></mixed-citation>
    </ref>
    <ref id="b2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andersson</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Larsson</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Holmqvist</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Stridh</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Nyström</surname>, <given-names>M.</given-names></string-name></person-group> ( <year>2017</year>). <article-title>One algorithm to rule them all? An evaluation and discussion of ten eye movement event-detection algorithms.</article-title>
<source>Behavior Research Methods</source>, <volume>49</volume>( <issue>2</issue>), <fpage>616</fpage>– <lpage>637</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-016-0738-9</pub-id>
<issn>1554-351X</issn>
<?supplied-pmid 27193160?><pub-id pub-id-type="pmid">27193160</pub-id></mixed-citation>
    </ref>
    <ref id="b3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andrienko</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Andrienko</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Burch</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Weiskopf</surname>, <given-names>D.</given-names></string-name></person-group> ( <year>2012</year>). <article-title>Visual analytics methodology for eye movement studies.</article-title>
<source>IEEE Transactions on Visualization and Computer Graphics</source>, <volume>18</volume>( <issue>12</issue>), <fpage>2889</fpage>– <lpage>2898</lpage>. <pub-id pub-id-type="doi">10.1109/TVCG.2012.276</pub-id>
<issn>1077-2626</issn>
<?supplied-pmid 26357198?><pub-id pub-id-type="pmid">26357198</pub-id></mixed-citation>
    </ref>
    <ref id="b4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bahill</surname>, <given-names>A. T.</given-names></string-name>, <string-name><surname>Clark</surname>, <given-names>M. R.</given-names></string-name>, &amp; <string-name><surname>Stark</surname>, <given-names>L.</given-names></string-name></person-group> ( <year>1975</year>). <article-title>Glissades–eye movements generated by mismatched components of the saccadic motoneuronal control signal.</article-title>
<source>Mathematical Biosciences</source>, <volume>26</volume>( <issue>3</issue>), <fpage>303</fpage>– <lpage>318</lpage>. <pub-id pub-id-type="doi">10.1016/0025-5564(75)90018-8</pub-id>
<issn>0025-5564</issn>
</mixed-citation>
    </ref>
    <ref id="b5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bellet</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Bellet</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Nienborg</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Hafed</surname>, <given-names>Z. M.</given-names></string-name>, &amp; <string-name><surname>Berens</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2019</year>).<article-title>Human-level saccade detection performance using deep neural networks.</article-title>
<source>Journal of Neurophysiology</source>,<volume>121</volume>(<issue>2</issue>),<fpage>646</fpage>–<lpage>661</lpage>.<pub-id pub-id-type="doi">10.1152/jn.00601.2018</pub-id>
<issn>0022-3077</issn>
<?supplied-pmid 30565968?><pub-id pub-id-type="pmid">30565968</pub-id></mixed-citation>
    </ref>
    <ref id="b6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benedetto</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pedrotti</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Bridgeman</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2011</year>).<article-title>Microsaccades and exploratory saccades in a naturalistic environment.</article-title>
<source>Journal of Eye Movement Research</source>,<volume>4</volume>(<issue>2</issue>).<comment>Advance online publication</comment>.<pub-id pub-id-type="doi">10.16910/jemr.4.2.2</pub-id>
<issn>1995-8692</issn>
</mixed-citation>
    </ref>
    <ref id="b7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blascheck</surname>, <given-names>T.</given-names></string-name>,<string-name><surname>Kurzhals</surname>, <given-names>K.</given-names></string-name>,<string-name><surname>Raschke</surname>, <given-names>M.</given-names></string-name>,<string-name><surname>Burch</surname>, <given-names>M.</given-names></string-name>,<string-name><surname>Weiskopf</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Ertl</surname>,<given-names>T.</given-names></string-name></person-group> (<year>2017</year>).<article-title>Visualization of eye tracking data: A taxonomy and survey.</article-title>
<source>Computer Graphics Forum</source>,<volume>36</volume>(<issue>8</issue>),<fpage>260</fpage>–<lpage>284</lpage>.<pub-id pub-id-type="doi">10.1111/cgf.13079</pub-id>
<issn>0167-7055</issn>
</mixed-citation>
    </ref>
    <ref id="b8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blignaut</surname>,<given-names>P.</given-names></string-name></person-group> (<year>2009</year>).<article-title>Fixation identification: The optimum threshold for a dispersion algorithm.</article-title>
<source>Attention, Perception &amp; Psychophysics</source>,<volume>71</volume>(<issue>4</issue>),<fpage>881</fpage>–<lpage>895</lpage>.<pub-id pub-id-type="doi">10.3758/APP.71.4.881</pub-id>
<issn>1943-3921</issn>
<?supplied-pmid 19429966?><pub-id pub-id-type="pmid">19429966</pub-id></mixed-citation>
    </ref>
    <ref id="b9">
      <mixed-citation publication-type="unknown"><person-group person-group-type="author"><string-name><surname>Bonneh</surname>,<given-names>Y. S.</given-names></string-name>,<string-name><surname>Donner</surname>,<given-names>T. H.</given-names></string-name>,<string-name><surname>Sagi</surname>,<given-names>D.</given-names></string-name>,<string-name><surname>Fried</surname>,<given-names>M.</given-names></string-name>,<string-name><surname>Cooperman</surname>,<given-names>A.</given-names></string-name>,<string-name><surname>Heeger</surname>,<given-names>D. J.</given-names></string-name>, &amp; <string-name><surname>Arieli</surname>,<given-names>A.</given-names></string-name></person-group> (<year>2010</year>). Motion-induced blindness and microsaccades: Cause and effect. Journal of Vision, 10(14), 22, 1-15. doi:<pub-id pub-id-type="doi">10.1167/10.14.22</pub-id>
</mixed-citation>
    </ref>
    <ref id="b10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brooke</surname>,<given-names>J.</given-names></string-name></person-group> (<year>1996</year>).<article-title>SUS-A quick and dirty usability scale.</article-title>
<source>Usability Evaluation in Industry</source>,<volume>189</volume>(<issue>194</issue>),<fpage>4</fpage>–<lpage>7</lpage>.
</mixed-citation>
    </ref>
    <ref id="b11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burch</surname>,<given-names>M.</given-names></string-name>,<string-name><surname>Chuang</surname>,<given-names>L.</given-names></string-name>,<string-name><surname>Duchowski</surname>,<given-names>A.</given-names></string-name>,<string-name><surname>Daniel</surname>,<given-names>W.</given-names></string-name>, &amp; <string-name><surname>Groner</surname>,<given-names>R.</given-names></string-name></person-group> (<year>2018</year>).<article-title>Eye tracking and visualization. Introduction to the special thematic issue.</article-title>
<source>Journal of Eye Movement Research</source>,<volume>10</volume>(<issue>5</issue>).<comment>Advance online publication</comment>.<pub-id pub-id-type="doi">10.16910/jemr.10.5.1</pub-id>
<issn>1995-8692</issn>
</mixed-citation>
    </ref>
    <ref id="b12">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Chen</surname>,<given-names>C.</given-names></string-name>,<string-name><surname>Hou</surname>,<given-names>H.</given-names></string-name>,<string-name><surname>Hu</surname>,<given-names>Z.</given-names></string-name>, &amp; <string-name><surname>Liu</surname>,<given-names>S.</given-names></string-name></person-group> (<year>2012</year>).<chapter-title>An illuminated path: The impact of the work of Jim Thomas</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>J.</given-names><surname>Dill</surname></string-name>,<string-name><given-names>R.</given-names><surname>Earnshaw</surname></string-name>,<string-name><given-names>D.</given-names><surname>Kasik</surname></string-name>,<string-name><given-names>J.</given-names><surname>Vince</surname></string-name>, &amp; <string-name><given-names>P. C.</given-names><surname>Wong</surname></string-name> (<role>Eds.</role>),
</person-group>
<source>Expanding the Frontiers of Visual Analytics and Visualization</source> (pp. <fpage>9</fpage>–<lpage>30</lpage>).<publisher-name>Springer</publisher-name>.,<pub-id pub-id-type="doi">10.1007/978-1-4471-2804-5_2</pub-id>
</mixed-citation>
    </ref>
    <ref id="b13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Costela</surname>,<given-names>F. M.</given-names></string-name>,<string-name><surname>McCamy</surname>,<given-names>M. B.</given-names></string-name>,<string-name><surname>Macknik</surname>,<given-names>S. L.</given-names></string-name>,<string-name><surname>Otero-Millan</surname>,<given-names>J.</given-names></string-name>, &amp; <string-name><surname>Martinez-Conde</surname>,<given-names>S.</given-names></string-name></person-group> (<year>2013</year>).<article-title>Microsaccades restore the visibility of minute foveal targets.</article-title>
<source>PeerJ</source>,<volume>1</volume>,<fpage>e119</fpage>.<pub-id pub-id-type="doi">10.7717/peerj.119</pub-id>
<issn>2167-8359</issn>
<?supplied-pmid 23940832?><pub-id pub-id-type="pmid">23940832</pub-id></mixed-citation>
    </ref>
    <ref id="b14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deubel</surname>,<given-names>H.</given-names></string-name>, &amp; <string-name><surname>Bridgeman</surname>,<given-names>B.</given-names></string-name></person-group> (<year>1995</year>).<article-title>Perceptual consequences of ocular lens overshoot during saccadic eye movements.</article-title>
<source>Vision Research</source>,<volume>35</volume>(<issue>20</issue>),<fpage>2897</fpage>–<lpage>2902</lpage>.<pub-id pub-id-type="doi">10.1016/0042-6989(95)00042-X</pub-id>
<issn>0042-6989</issn>
<?supplied-pmid 8533329?><pub-id pub-id-type="pmid">8533329</pub-id></mixed-citation>
    </ref>
    <ref id="b15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Di Stasi</surname>,<given-names>L. L.</given-names></string-name>,<string-name><surname>McCamy</surname>,<given-names>M. B.</given-names></string-name>,<string-name><surname>Catena</surname>,<given-names>A.</given-names></string-name>,<string-name><surname>Macknik</surname>,<given-names>S. L.</given-names></string-name>,<string-name><surname>Cañas</surname>,<given-names>J. J.</given-names></string-name>, &amp; <string-name><surname>Martinez-Conde</surname>,<given-names>S.</given-names></string-name></person-group> (<year>2013</year>).<article-title>Microsaccade and drift dynamics reflect mental fatigue.</article-title>
<source>The European Journal of Neuroscience</source>,<volume>38</volume>(<issue>3</issue>),<fpage>2389</fpage>–<lpage>2398</lpage>.<pub-id pub-id-type="doi">10.1111/ejn.12248</pub-id>
<issn>0953-816X</issn>
<?supplied-pmid 23675850?><pub-id pub-id-type="pmid">23675850</pub-id></mixed-citation>
    </ref>
    <ref id="b16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dimigen</surname>,<given-names>O.</given-names></string-name>,<string-name><surname>Valsecchi</surname>,<given-names>M.</given-names></string-name>,<string-name><surname>Sommer</surname>,<given-names>W.</given-names></string-name>, &amp; <string-name><surname>Kliegl</surname>,<given-names>R.</given-names></string-name></person-group> (<year>2009</year>).<article-title>Human microsaccade-related visual brain responses.</article-title>
<source>The Journal of Neuroscience : The Official Journal of the Society for Neuroscience</source>,<volume>29</volume>(<issue>39</issue>),<fpage>12321</fpage>–<lpage>12331</lpage>.<pub-id pub-id-type="doi">10.1523/JNEUROSCI.0911-09.2009</pub-id>
<issn>0270-6474</issn>
<?supplied-pmid 19793991?><pub-id pub-id-type="pmid">19793991</pub-id></mixed-citation>
    </ref>
    <ref id="b17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Engbert</surname>,<given-names>R.</given-names></string-name>, &amp; <string-name><surname>Kliegl</surname>,<given-names>R.</given-names></string-name></person-group> (<year>2003</year>a). <chapter-title>Binocular coordination in microsaccades</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>J.</given-names><surname>Hyönä</surname></string-name>,<string-name><given-names>R.</given-names><surname>Radach</surname></string-name>, &amp; <string-name><given-names>H.</given-names><surname>Deubel</surname></string-name> (<role>Eds.</role>),
</person-group>
<source>The Mind’s Eye: Cognitive and Applied Aspects of Eye Movement Research</source> (pp. <fpage>103</fpage>–<lpage>117</lpage>).<publisher-name>North-Holland</publisher-name>.,<pub-id pub-id-type="doi">10.1016/B978-044451020-4/50007-4</pub-id>
</mixed-citation>
    </ref>
    <ref id="b18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Engbert</surname>,<given-names>R.</given-names></string-name>, &amp; <string-name><surname>Kliegl</surname>,<given-names>R.</given-names></string-name></person-group> (<year>2003</year>b). <article-title>Microsaccades uncover the orientation of covert attention.</article-title>
<source>Vision Research</source>,<volume>43</volume>(<issue>9</issue>),<fpage>1035</fpage>–<lpage>1045</lpage>.<pub-id pub-id-type="doi">10.1016/S0042-6989(03)00084-1</pub-id>
<issn>0042-6989</issn>
<?supplied-pmid 12676246?><pub-id pub-id-type="pmid">12676246</pub-id></mixed-citation>
    </ref>
    <ref id="b19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Engbert</surname>,<given-names>R.</given-names></string-name>, &amp; <string-name><surname>Mergenthaler</surname>,<given-names>K.</given-names></string-name></person-group> (<year>2006</year>).<article-title>Microsaccades are triggered by low retinal image slip.</article-title>
<source>Proceedings of the National Academy of Sciences of the United States of America</source>,<volume>103</volume>(<issue>18</issue>),<fpage>7192</fpage>–<lpage>7197</lpage>.<pub-id pub-id-type="doi">10.1073/pnas.0509557103</pub-id>
<issn>0027-8424</issn>
<?supplied-pmid 16632611?><pub-id pub-id-type="pmid">16632611</pub-id></mixed-citation>
    </ref>
    <ref id="b20">
      <mixed-citation publication-type="web-page"><person-group person-group-type="author"><string-name><surname>Engbert</surname>,<given-names>R.</given-names></string-name>,<string-name><surname>Sinn</surname>,<given-names>P.</given-names></string-name>,<string-name><surname>Mergenthaler</surname>,<given-names>K.</given-names></string-name>, &amp; <string-name><surname>Trukenbrod</surname>,<given-names>H.</given-names></string-name></person-group> (<year>2015</year>).<article-title>Microsaccade Toolbox.</article-title> From <ext-link ext-link-type="uri" xlink:href="http://read.psych.uni-potsdam.de/attachments/article/140/MS_Toolbox_R.zip">http://read.psych.uni-potsdam.de/attachments/article/140/MS_Toolbox_R.zip</ext-link>
</mixed-citation>
    </ref>
    <ref id="b21">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ericsson</surname>,<given-names>K. A.</given-names></string-name>, &amp; <string-name><surname>Simon</surname>,<given-names>H. A.</given-names></string-name></person-group> (<year>1993</year>).<source>Protocol Analysis</source>.<publisher-name>MIT Press</publisher-name>.<pub-id pub-id-type="doi">10.7551/mitpress/5657.001.0001</pub-id>
</mixed-citation>
    </ref>
    <ref id="b22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fried</surname>,<given-names>M.</given-names></string-name>,<string-name><surname>Tsitsiashvili</surname>,<given-names>E.</given-names></string-name>,<string-name><surname>Bonneh</surname>,<given-names>Y. S.</given-names></string-name>,<string-name><surname>Sterkin</surname>,<given-names>A.</given-names></string-name>,<string-name><surname>Wygnanski-Jaffe</surname>,<given-names>T.</given-names></string-name>,<string-name><surname>Epstein</surname>,<given-names>T.</given-names></string-name>, &amp; <string-name><surname>Polat</surname>,<given-names>U.</given-names></string-name></person-group> (<year>2014</year>).<article-title>ADHD subjects fail to suppress eye blinks and microsaccades while anticipating visual stimuli but recover with medication.</article-title>
<source>Vision Research</source>,<volume>101</volume>,<fpage>62</fpage>–<lpage>72</lpage>.<pub-id pub-id-type="doi">10.1016/j.visres.2014.05.004</pub-id>
<issn>0042-6989</issn>
<?supplied-pmid 24863585?><pub-id pub-id-type="pmid">24863585</pub-id></mixed-citation>
    </ref>
    <ref id="b23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gao</surname>,<given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Sabel</surname>,<given-names>B. A.</given-names></string-name></person-group> (<year>2017</year>).<article-title>Microsaccade dysfunction and adaptation in hemianopia after stroke.</article-title>
<source>Restorative Neurology and Neuroscience</source>,<volume>35</volume>(<issue>4</issue>),<fpage>365</fpage>–<lpage>376</lpage>.<pub-id pub-id-type="doi">10.3233/RNN-170749</pub-id>
<issn>0922-6028</issn>
<?supplied-pmid 28800343?><pub-id pub-id-type="pmid">28800343</pub-id></mixed-citation>
    </ref>
    <ref id="b24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greene</surname>,<given-names>M. R.</given-names></string-name>,<string-name><surname>Liu</surname>,<given-names>T.</given-names></string-name>, &amp; <string-name><surname>Wolfe</surname>,<given-names>J. M.</given-names></string-name></person-group> (<year>2012</year>).<article-title>Reconsidering Yarbus: A failure to predict observers’ task from eye movement patterns.</article-title>
<source>Vision Research</source>,<volume>62</volume>,<fpage>1</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="doi">10.1016/j.visres.2012.03.019</pub-id>
<issn>0042-6989</issn>
<?supplied-pmid 22487718?><pub-id pub-id-type="pmid">22487718</pub-id></mixed-citation>
    </ref>
    <ref id="b25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hafed</surname>,<given-names>Z. M.</given-names></string-name>, &amp; <string-name><surname>Clark</surname>,<given-names>J. J.</given-names></string-name></person-group> (<year>2002</year>).<article-title>Microsaccades as an overt measure of covert attention shifts.</article-title>
<source>Vision Research</source>,<volume>42</volume>(<issue>22</issue>),<fpage>2533</fpage>–<lpage>2545</lpage>.<pub-id pub-id-type="doi">10.1016/S0042-6989(02)00263-8</pub-id>
<issn>0042-6989</issn>
<?supplied-pmid 12445847?><pub-id pub-id-type="pmid">12445847</pub-id></mixed-citation>
    </ref>
    <ref id="b26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hermens</surname>,<given-names>F.</given-names></string-name>, &amp; <string-name><surname>Walker</surname>,<given-names>R.</given-names></string-name></person-group> (<year>2010</year>).<article-title>What determines the direction of microsaccades?</article-title>
<source>Journal of Eye Movement Research</source>,<volume>3</volume>(<issue>4</issue>).<comment>Advance online publication</comment>.<pub-id pub-id-type="doi">10.16910/jemr.3.4.1</pub-id>
<issn>1995-8692</issn>
</mixed-citation>
    </ref>
    <ref id="b27">
      <mixed-citation publication-type="unknown"><person-group person-group-type="author"><string-name><surname>Hicheur</surname>,<given-names>H.</given-names></string-name>,<string-name><surname>Zozor</surname>,<given-names>S.</given-names></string-name>,<string-name><surname>Campagne</surname>,<given-names>A.</given-names></string-name>, &amp; <string-name><surname>Chauvin</surname>,<given-names>A.</given-names></string-name></person-group> (<year>2013</year>).<article-title>Microsaccades are modulated by both attentional demands of a visual discrimination task and background noise.</article-title> Journal of Vision, 13(13), 18, 1-20. doi:<pub-id pub-id-type="doi">10.1167/13.13.18</pub-id>
</mixed-citation>
    </ref>
    <ref id="b28">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Holmqvist</surname>,<given-names>K.</given-names></string-name>,<string-name><surname>Nyström</surname>,<given-names>M.</given-names></string-name>,<string-name><surname>Andersson</surname>,<given-names>R.</given-names></string-name>,<string-name><surname>Dewhurst</surname>,<given-names>R.</given-names></string-name>,<string-name><surname>Jarodzka</surname>,<given-names>H.</given-names></string-name>, &amp; <string-name><surname>Van de Weijer</surname>,<given-names>J.</given-names></string-name></person-group> (<year>2011</year>).<source>Eye Tracking: A Comprehensive Guide to Methods and Measures</source>.<publisher-name>Oxford University Press</publisher-name>.
</mixed-citation>
    </ref>
    <ref id="b29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hsieh</surname>,<given-names>P.-J.</given-names></string-name>, &amp; <string-name><surname>Tse</surname>,<given-names>P. U.</given-names></string-name></person-group> (<year>2009</year>).<article-title>Microsaccade rate varies with subjective visibility during motion-induced blindness.</article-title>
<source>PLoS One</source>,<volume>4</volume>(<issue>4</issue>),<fpage>e5163</fpage>.<pub-id pub-id-type="doi">10.1371/journal.pone.0005163</pub-id>
<issn>1932-6203</issn>
<?supplied-pmid 19357789?><pub-id pub-id-type="pmid">19357789</pub-id></mixed-citation>
    </ref>
    <ref id="b30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Karsh</surname>,<given-names>R.</given-names></string-name>, &amp; <string-name><surname>Breitenbach</surname>,<given-names>F. W.</given-names></string-name></person-group> (<year>1983</year>).<chapter-title>Looking at looking: The amorphous fixation measure</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>R.</given-names><surname>Groner</surname></string-name>,<string-name><given-names>C.</given-names><surname>Menz</surname></string-name>,<string-name><given-names>D. F.</given-names><surname>Fisher</surname></string-name>, &amp; <string-name><given-names>R. A.</given-names><surname>Monty</surname></string-name> (<role>Eds.</role>),
</person-group>
<source>Eye Movements and Psychological Functions: International Views</source> (pp. <fpage>53</fpage>–<lpage>64</lpage>).<publisher-name>Lawrence Erlbaum Assoc Incorporated</publisher-name>.
</mixed-citation>
    </ref>
    <ref id="b31">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Keim</surname>,<given-names>D.</given-names></string-name>,<string-name><surname>Mansmann</surname>,<given-names>F.</given-names></string-name>,<string-name><surname>Schneidewind</surname>,<given-names>J.</given-names></string-name>,<string-name><surname>Thomas</surname>,<given-names>J.</given-names></string-name>, &amp; <string-name><surname>Ziegler</surname>,<given-names>H.</given-names></string-name></person-group> (<year>2008</year>).<chapter-title>Visual analytics: Scope and challenges</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>S. J.</given-names><surname>Simoff</surname></string-name>,<string-name><given-names>M. H.</given-names><surname>Böhlen</surname></string-name>, &amp; <string-name><given-names>A.</given-names><surname>Mazeika</surname></string-name> (<role>Eds.</role>),
</person-group>
<source>Visual Data Mining: Theory, Techniques and Tools for Visual Analytics</source> (pp. <fpage>76</fpage>–<lpage>90</lpage>).<publisher-name>Springer</publisher-name>.,<pub-id pub-id-type="doi">10.1007/978-3-540-71080-6_6</pub-id>
</mixed-citation>
    </ref>
    <ref id="b32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krejtz</surname>,<given-names>K.</given-names></string-name>,<string-name><surname>Duchowski</surname>,<given-names>A. T.</given-names></string-name>,<string-name><surname>Niedzielska</surname>,<given-names>A.</given-names></string-name>,<string-name><surname>Biele</surname>,<given-names>C.</given-names></string-name>, &amp; <string-name><surname>Krejtz</surname>,<given-names>I.</given-names></string-name></person-group> (<year>2018</year>).<article-title>Eye tracking cognitive load using pupil diameter and microsaccades with fixed gaze.</article-title>
<source>PLoS One</source>,<volume>13</volume>(<issue>9</issue>),<fpage>e0203629</fpage>.<pub-id pub-id-type="doi">10.1371/journal.pone.0203629</pub-id>
<issn>1932-6203</issn>
<?supplied-pmid 30216385?><pub-id pub-id-type="pmid">30216385</pub-id></mixed-citation>
    </ref>
    <ref id="b33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kurzhals</surname>,<given-names>K.</given-names></string-name>, &amp; <string-name><surname>Weiskopf</surname>,<given-names>D.</given-names></string-name></person-group> (<year>2013</year>).<article-title>Space-time visual analytics of eye-tracking data for dynamic stimuli.</article-title>
<source>IEEE Transactions on Visualization and Computer Graphics</source>,<volume>19</volume>(<issue>12</issue>),<fpage>2129</fpage>–<lpage>2138</lpage>.<pub-id pub-id-type="doi">10.1109/TVCG.2013.194</pub-id>
<issn>1077-2626</issn>
<?supplied-pmid 24051779?><pub-id pub-id-type="pmid">24051779</pub-id></mixed-citation>
    </ref>
    <ref id="b34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Laubrock</surname>,<given-names>J.</given-names></string-name>,<string-name><surname>Engbert</surname>,<given-names>R.</given-names></string-name>, &amp; <string-name><surname>Kliegl</surname>,<given-names>R.</given-names></string-name></person-group> (<year>2005</year>).<article-title>Microsaccade dynamics during covert attention.</article-title>
<source>Vision Research</source>,<volume>45</volume>(<issue>6</issue>),<fpage>721</fpage>–<lpage>730</lpage>.<pub-id pub-id-type="doi">10.1016/j.visres.2004.09.029</pub-id>
<issn>0042-6989</issn>
<?supplied-pmid 15639499?><pub-id pub-id-type="pmid">15639499</pub-id></mixed-citation>
    </ref>
    <ref id="b35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Laubrock</surname>,<given-names>J.</given-names></string-name>,<string-name><surname>Kliegl</surname>,<given-names>R.</given-names></string-name>,<string-name><surname>Rolfs</surname>,<given-names>M.</given-names></string-name>, &amp; <string-name><surname>Engbert</surname>,<given-names>R.</given-names></string-name></person-group> (<year>2010</year>).<article-title>When do microsaccades follow spatial attention?</article-title>
<source>Attention, Perception &amp; Psychophysics</source>,<volume>72</volume>(<issue>3</issue>),<fpage>683</fpage>–<lpage>694</lpage>.<pub-id pub-id-type="doi">10.3758/APP.72.3.683</pub-id>
<issn>1943-3921</issn>
<?supplied-pmid 20348575?><pub-id pub-id-type="pmid">20348575</pub-id></mixed-citation>
    </ref>
    <ref id="b36">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Liversedge</surname>,<given-names>S.</given-names></string-name>,<string-name><surname>Gilchrist</surname>,<given-names>I.</given-names></string-name>, &amp; <string-name><surname>Everling</surname>,<given-names>S.</given-names></string-name></person-group> (<year>2011</year>).<source>The Oxford Handbook of Eye Movements</source>.<publisher-name>Oxford University Press</publisher-name>.,<pub-id pub-id-type="doi">10.1093/oxfordhb/9780199539789.001.0001</pub-id>
</mixed-citation>
    </ref>
    <ref id="b37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lowet</surname>,<given-names>E.</given-names></string-name>,<string-name><surname>Gomes</surname>,<given-names>B.</given-names></string-name>,<string-name><surname>Srinivasan</surname>,<given-names>K.</given-names></string-name>,<string-name><surname>Zhou</surname>,<given-names>H.</given-names></string-name>,<string-name><surname>Schafer</surname>,<given-names>R. J.</given-names></string-name>, &amp; <string-name><surname>Desimone</surname>,<given-names>R.</given-names></string-name></person-group> (<year>2018</year>).<article-title>Enhanced neural processing by covert attention only during microsaccades directed toward the attended stimulus.</article-title>
<source>Neuron</source>,<volume>99</volume>(<issue>1</issue>),<fpage>207</fpage>–<lpage>214.e3</lpage>.<pub-id pub-id-type="doi">10.1016/j.neuron.2018.05.041</pub-id>
<issn>0896-6273</issn>
<?supplied-pmid 29937279?><pub-id pub-id-type="pmid">29937279</pub-id></mixed-citation>
    </ref>
    <ref id="b38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Malinov</surname>,<given-names>I. V.</given-names></string-name>,<string-name><surname>Epelboim</surname>,<given-names>J.</given-names></string-name>,<string-name><surname>Herst</surname>,<given-names>A. N.</given-names></string-name>, &amp; <string-name><surname>Steinman</surname>,<given-names>R. M.</given-names></string-name></person-group> (<year>2000</year>).<article-title>Characteristics of saccades and vergence in two kinds of sequential looking tasks.</article-title>
<source>Vision Research</source>,<volume>40</volume>(<issue>16</issue>),<fpage>2083</fpage>–<lpage>2090</lpage>.<pub-id pub-id-type="doi">10.1016/S0042-6989(00)00063-8</pub-id>
<issn>0042-6989</issn>
<?supplied-pmid 10878269?><pub-id pub-id-type="pmid">10878269</pub-id></mixed-citation>
    </ref>
    <ref id="b39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Martinez-Conde</surname>,<given-names>S.</given-names></string-name>,<string-name><surname>Macknik</surname>,<given-names>S. L.</given-names></string-name>,<string-name><surname>Troncoso</surname>,<given-names>X. G.</given-names></string-name>, &amp; <string-name><surname>Hubel</surname>,<given-names>D. H.</given-names></string-name></person-group> (<year>2009</year>).<article-title>Microsaccades: A neurophysiological analysis.</article-title>
<source>Trends in Neurosciences</source>,<volume>32</volume>(<issue>9</issue>),<fpage>463</fpage>–<lpage>475</lpage>.<pub-id pub-id-type="doi">10.1016/j.tins.2009.05.006</pub-id>
<issn>0166-2236</issn>
<?supplied-pmid 19716186?><pub-id pub-id-type="pmid">19716186</pub-id></mixed-citation>
    </ref>
    <ref id="b40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McCamy</surname>,<given-names>M. B.</given-names></string-name>,<string-name><surname>Collins</surname>,<given-names>N.</given-names></string-name>,<string-name><surname>Otero-Millan</surname>,<given-names>J.</given-names></string-name>,<string-name><surname>Al-Kalbani</surname>,<given-names>M.</given-names></string-name>,<string-name><surname>Macknik</surname>,<given-names>S. L.</given-names></string-name>,<string-name><surname>Coakley</surname>,<given-names>D.</given-names></string-name>,<string-name><surname>Troncoso</surname>,<given-names>X. G.</given-names></string-name>,<string-name><surname>Boyle</surname>,<given-names>G.</given-names></string-name>,<string-name><surname>Narayanan</surname>,<given-names>V.</given-names></string-name>,<string-name><surname>Wolf</surname>,<given-names>T. R.</given-names></string-name>, &amp; <string-name><surname>Martinez-Conde</surname>,<given-names>S.</given-names></string-name></person-group> (<year>2013</year>).<article-title>Simultaneous recordings of ocular microtremor and microsaccades with a piezoelectric sensor and a video-oculography system.</article-title>
<source>PeerJ</source>,<volume>1</volume>,<fpage>e14</fpage>.<pub-id pub-id-type="doi">10.7717/peerj.14</pub-id>
<issn>2167-8359</issn>
<?supplied-pmid 23638348?><pub-id pub-id-type="pmid">23638348</pub-id></mixed-citation>
    </ref>
    <ref id="b41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McCamy</surname>,<given-names>M. B.</given-names></string-name>,<string-name><surname>Najafian Jazi</surname>,<given-names>A.</given-names></string-name>,<string-name><surname>Otero-Millan</surname>,<given-names>J.</given-names></string-name>,<string-name><surname>Macknik</surname>,<given-names>S. L.</given-names></string-name>, &amp; <string-name><surname>Martinez-Conde</surname>,<given-names>S.</given-names></string-name></person-group> (<year>2013</year>).<article-title>The effects of fixation target size and luminance on microsaccades and square-wave jerks.</article-title>
<source>PeerJ</source>,<volume>1</volume>,<fpage>e9</fpage>.<pub-id pub-id-type="doi">10.7717/peerj.9</pub-id>
<issn>2167-8359</issn>
<?supplied-pmid 23638403?><pub-id pub-id-type="pmid">23638403</pub-id></mixed-citation>
    </ref>
    <ref id="b42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McCamy</surname>,<given-names>M. B.</given-names></string-name>,<string-name><surname>Otero-Millan</surname>,<given-names>J.</given-names></string-name>,<string-name><surname>Di Stasi</surname>,<given-names>L. L.</given-names></string-name>,<string-name><surname>Macknik</surname>,<given-names>S. L.</given-names></string-name>, &amp; <string-name><surname>Martinez-Conde</surname>,<given-names>S.</given-names></string-name></person-group> (<year>2014</year>).<article-title>Highly informative natural scene regions increase microsaccade production during visual scanning.</article-title>
<source>The Journal of Neuroscience : The Official Journal of the Society for Neuroscience</source>,<volume>34</volume>(<issue>8</issue>),<fpage>2956</fpage>–<lpage>2966</lpage>.<pub-id pub-id-type="doi">10.1523/JNEUROSCI.4448-13.2014</pub-id>
<issn>0270-6474</issn>
<?supplied-pmid 24553936?><pub-id pub-id-type="pmid">24553936</pub-id></mixed-citation>
    </ref>
    <ref id="b43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McCamy</surname>,<given-names>M. B.</given-names></string-name>,<string-name><surname>Otero-Millan</surname>,<given-names>J.</given-names></string-name>,<string-name><surname>Leigh</surname>,<given-names>R. J.</given-names></string-name>,<string-name><surname>King</surname>,<given-names>S. A.</given-names></string-name>,<string-name><surname>Schneider</surname>,<given-names>R. M.</given-names></string-name>,<string-name><surname>Macknik</surname>,<given-names>S. L.</given-names></string-name>, &amp; <string-name><surname>Martinez-Conde</surname>,<given-names>S.</given-names></string-name></person-group> (<year>2015</year>).<article-title>Simultaneous recordings of human microsaccades and drifts with a contemporary video eye tracker and the search coil technique.</article-title>
<source>PLoS One</source>,<volume>10</volume>(<issue>6</issue>),<fpage>e0128428</fpage>.<pub-id pub-id-type="doi">10.1371/journal.pone.0128428</pub-id>
<issn>1932-6203</issn>
<?supplied-pmid 26035820?><pub-id pub-id-type="pmid">26035820</pub-id></mixed-citation>
    </ref>
    <ref id="b44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mergenthaler</surname>,<given-names>K.</given-names></string-name>, &amp; <string-name><surname>Engbert</surname>,<given-names>R.</given-names></string-name></person-group> (<year>2010</year>).<article-title>Microsaccades are different from saccades in scene perception.</article-title>
<source>Experimental Brain Research</source>,<volume>203</volume>(<issue>4</issue>),<fpage>753</fpage>–<lpage>757</lpage>.<pub-id pub-id-type="doi">10.1007/s00221-010-2272-9</pub-id>
<issn>0014-4819</issn>
<?supplied-pmid 20467731?><pub-id pub-id-type="pmid">20467731</pub-id></mixed-citation>
    </ref>
    <ref id="b45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mihali</surname>,<given-names>A.</given-names></string-name>,<string-name><surname>Opheusden</surname>,<given-names>B.</given-names></string-name>, &amp; <string-name><surname>Ma</surname>,<given-names>W. J.</given-names></string-name></person-group> (<year>2017</year>).<article-title>Bayesian microsaccade detection.</article-title>
<source>Journal of Vision</source>,<volume>17</volume>(<issue>1</issue>),<fpage>1</fpage>–<lpage>23</lpage>
<pub-id pub-id-type="doi">10.1167/17.1.13</pub-id>
</mixed-citation>
    </ref>
    <ref id="b46">
      <mixed-citation publication-type="web-page"><person-group person-group-type="author"><string-name><surname>Munz</surname>,<given-names>T.</given-names></string-name></person-group> (<year>2019</year>).<article-title>VisME software v1.2.</article-title> doi:<pub-id pub-id-type="doi">10.5281/zenodo.3352236</pub-id>
</mixed-citation>
    </ref>
    <ref id="b47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Munzner</surname>,<given-names>T.</given-names></string-name></person-group> (<year>2009</year>).<article-title>A nested model for visualization design and validation.</article-title>
<source>IEEE Transactions on Visualization and Computer Graphics</source>,<volume>15</volume>(<issue>6</issue>),<fpage>921</fpage>–<lpage>928</lpage>.<pub-id pub-id-type="doi">10.1109/TVCG.2009.111</pub-id>
<issn>1077-2626</issn>
<?supplied-pmid 19834155?><pub-id pub-id-type="pmid">19834155</pub-id></mixed-citation>
    </ref>
    <ref id="b48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nosek</surname>,<given-names>B. A.</given-names></string-name>,<string-name><surname>Alter</surname>,<given-names>G.</given-names></string-name>,<string-name><surname>Banks</surname>,<given-names>G. C.</given-names></string-name>,<string-name><surname>Borsboom</surname>,<given-names>D.</given-names></string-name>,<string-name><surname>Bowman</surname>,<given-names>S. D.</given-names></string-name>,<string-name><surname>Breckler</surname>,<given-names>S. J.</given-names></string-name>,<string-name><surname>Buck</surname>,<given-names>S.</given-names></string-name>,<string-name><surname>Chambers</surname>,<given-names>C. D.</given-names></string-name>,<string-name><surname>Chin</surname>,<given-names>G.</given-names></string-name>,<string-name><surname>Christensen</surname>,<given-names>G.</given-names></string-name>,<string-name><surname>Contestabile</surname>,<given-names>M.</given-names></string-name>,<string-name><surname>Dafoe</surname>,<given-names>A.</given-names></string-name>,<string-name><surname>Eich</surname>,<given-names>E.</given-names></string-name>,<string-name><surname>Freese</surname>,<given-names>J.</given-names></string-name>,<string-name><surname>Glennerster</surname>,<given-names>R.</given-names></string-name>,<string-name><surname>Goroff</surname>,<given-names>D.</given-names></string-name>,<string-name><surname>Green</surname>,<given-names>D. P.</given-names></string-name>,<string-name><surname>Hesse</surname>,<given-names>B.</given-names></string-name>,<string-name><surname>Humphreys</surname>,<given-names>M.</given-names></string-name>,<etal>. . .</etal><string-name><surname>Yarkoni</surname>,<given-names>T.</given-names></string-name></person-group> (<year>2015</year>).<article-title>SCIENTIFIC STANDARDS. Promoting an open research culture.</article-title>
<source>Science</source>,<volume>348</volume>(<issue>6242</issue>),<fpage>1422</fpage>–<lpage>1425</lpage>.<pub-id pub-id-type="doi">10.1126/science.aab2374</pub-id>
<issn>0036-8075</issn>
<?supplied-pmid 26113702?><pub-id pub-id-type="pmid">26113702</pub-id></mixed-citation>
    </ref>
    <ref id="b49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Otero-Millan</surname>,<given-names>J.</given-names></string-name>,<string-name><surname>Castro</surname>,<given-names>J. L.</given-names></string-name>,<string-name><surname>Macknik</surname>,<given-names>S. L.</given-names></string-name>, &amp; <string-name><surname>Martinez-Conde</surname>,<given-names>S.</given-names></string-name></person-group> (<year>2014</year>).<article-title>Unsupervised clustering method to detect microsaccades.</article-title>
<source>Journal of Vision</source>,<volume>14</volume>(<issue>2</issue>),<fpage>1</fpage>–<lpage>17</lpage>.<pub-id pub-id-type="doi">10.1167/14.2.18</pub-id>
</mixed-citation>
    </ref>
    <ref id="b50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Otero-Millan</surname>,<given-names>J.</given-names></string-name>,<string-name><surname>Macknik</surname>,<given-names>S. L.</given-names></string-name>, &amp; <string-name><surname>Martinez-Conde</surname>,<given-names>S.</given-names></string-name></person-group> (<year>2012</year>).<article-title>Microsaccades and blinks trigger illusory rotation in the “rotating snakes” illusion.</article-title>
<source>The Journal of Neuroscience : The Official Journal of the Society for Neuroscience</source>,<volume>32</volume>(<issue>17</issue>),<fpage>6043</fpage>–<lpage>6051</lpage>.<pub-id pub-id-type="doi">10.1523/JNEUROSCI.5823-11.2012</pub-id>
<issn>0270-6474</issn>
<?supplied-pmid 22539864?><pub-id pub-id-type="pmid">22539864</pub-id></mixed-citation>
    </ref>
    <ref id="b51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Otero-Millan</surname>,<given-names>J.</given-names></string-name>,<string-name><surname>Troncoso</surname>,<given-names>X. G.</given-names></string-name>,<string-name><surname>Macknik</surname>,<given-names>S. L.</given-names></string-name>,<string-name><surname>Serrano-Pedraza</surname>,<given-names>I.</given-names></string-name>, &amp; <string-name><surname>Martinez-Conde</surname>,<given-names>S.</given-names></string-name></person-group> (<year>2008</year>).<article-title>Saccades and microsaccades during visual fixation, exploration, and search: Foundations for a common saccadic generator.</article-title>
<source>Journal of Vision</source>,<volume>8</volume>(<issue>14</issue>),<fpage>1</fpage>–<lpage>18</lpage>
<pub-id pub-id-type="doi">10.1167/8.14.21</pub-id>
</mixed-citation>
    </ref>
    <ref id="b52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pastukhov</surname>,<given-names>A.</given-names></string-name>, &amp; <string-name><surname>Braun</surname>,<given-names>J.</given-names></string-name></person-group> (<year>2010</year>).<article-title>Rare but precious: Microsaccades are highly informative about attentional allocation.</article-title>
<source>Vision Research</source>,<volume>50</volume>(<issue>12</issue>),<fpage>1173</fpage>–<lpage>1184</lpage>.<pub-id pub-id-type="doi">10.1016/j.visres.2010.04.007</pub-id>
<issn>0042-6989</issn>
<?supplied-pmid 20382176?><pub-id pub-id-type="pmid">20382176</pub-id></mixed-citation>
    </ref>
    <ref id="b53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pastukhov</surname>,<given-names>A.</given-names></string-name>,<string-name><surname>Vonau</surname>,<given-names>V.</given-names></string-name>,<string-name><surname>Stonkute</surname>,<given-names>S.</given-names></string-name>, &amp; <string-name><surname>Braun</surname>,<given-names>J.</given-names></string-name></person-group> (<year>2013</year>).<article-title>Spatial and temporal attention revealed by microsaccades.</article-title>
<source>Vision Research</source>,<volume>85</volume>,<fpage>45</fpage>–<lpage>57</lpage>.<pub-id pub-id-type="doi">10.1016/j.visres.2012.11.004</pub-id>
<issn>0042-6989</issn>
<?supplied-pmid 23164746?><pub-id pub-id-type="pmid">23164746</pub-id></mixed-citation>
    </ref>
    <ref id="b54">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Piras</surname>,<given-names>A.</given-names></string-name>,<string-name><surname>Raffi</surname>,<given-names>M.</given-names></string-name>,<string-name><surname>Perazzolo</surname>,<given-names>M.</given-names></string-name>,<string-name><surname>Malagoli Lanzoni</surname>,<given-names>I.</given-names></string-name>, &amp; <string-name><surname>Squatrito</surname>,<given-names>S.</given-names></string-name></person-group> (<year>2019</year>).<article-title>Microsaccades and interest areas during free-viewing sport task.</article-title>
<source>Journal of Sports Sciences</source>,<volume>37</volume>(<issue>9</issue>),<fpage>980</fpage>–<lpage>987</lpage>.<pub-id pub-id-type="doi">10.1080/02640414.2017.1380893</pub-id>
<issn>0264-0414</issn>
<?supplied-pmid 28922090?><pub-id pub-id-type="pmid">28922090</pub-id></mixed-citation>
    </ref>
    <ref id="b55">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Poletti</surname>,<given-names>M.</given-names></string-name>, &amp; <string-name><surname>Rucci</surname>,<given-names>M.</given-names></string-name></person-group> (<year>2016</year>).<article-title>A compact field guide to the study of microsaccades: Challenges and functions.</article-title>
<source>Vision Research</source>,<volume>118</volume>,<fpage>83</fpage>–<lpage>97</lpage>.<pub-id pub-id-type="doi">10.1016/j.visres.2015.01.018</pub-id>
<issn>0042-6989</issn>
<?supplied-pmid 25689315?><pub-id pub-id-type="pmid">25689315</pub-id></mixed-citation>
    </ref>
    <ref id="b56">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Privitera</surname>,<given-names>C. M.</given-names></string-name>,<string-name><surname>Carney</surname>,<given-names>T.</given-names></string-name>,<string-name><surname>Klein</surname>,<given-names>S.</given-names></string-name>, &amp; <string-name><surname>Aguilar</surname>,<given-names>M.</given-names></string-name></person-group> (<year>2014</year>).<article-title>Analysis of microsaccades and pupil dilation reveals a common decisional origin during visual search.</article-title>
<source>Vision Research</source>,<volume>95</volume>,<fpage>43</fpage>–<lpage>50</lpage>.<pub-id pub-id-type="doi">10.1016/j.visres.2013.12.001</pub-id>
<issn>0042-6989</issn>
<?supplied-pmid 24333280?><pub-id pub-id-type="pmid">24333280</pub-id></mixed-citation>
    </ref>
    <ref id="b57">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Salvucci</surname>,<given-names>D. D.</given-names></string-name>, &amp; <string-name><surname>Goldberg</surname>,<given-names>J. H.</given-names></string-name></person-group> (<year>2000</year>).<article-title>Identifying fixations and saccades in eye-tracking protocols.</article-title>
<source>Proceedings of the 2000 Symposium on Eye Tracking Research &amp; Applications</source> (pp. <fpage>71</fpage>-<lpage>78</lpage>).<publisher-loc>New York, NY, USA</publisher-loc>:<publisher-name>ACM</publisher-name>. doi:<pub-id pub-id-type="doi">10.1145/355017.355028</pub-id>
</mixed-citation>
    </ref>
    <ref id="b58">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sinn</surname>,<given-names>P.</given-names></string-name>, &amp; <string-name><surname>Engbert</surname>,<given-names>R.</given-names></string-name></person-group> (<year>2011</year>).<article-title>Saccadic facilitation by modulation of microsaccades in natural backgrounds.</article-title>
<source>Attention, Perception &amp; Psychophysics</source>,<volume>73</volume>(<issue>4</issue>),<fpage>1029</fpage>–<lpage>1033</lpage>.<pub-id pub-id-type="doi">10.3758/s13414-011-0107-9</pub-id>
<issn>1943-3921</issn>
<?supplied-pmid 21331671?><pub-id pub-id-type="pmid">21331671</pub-id></mixed-citation>
    </ref>
    <ref id="b59">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sinn</surname>,<given-names>P.</given-names></string-name>, &amp; <string-name><surname>Engbert</surname>,<given-names>R.</given-names></string-name></person-group> (<year>2016</year>).<article-title>Small saccades versus microsaccades: Experimental distinction and model-based unification.</article-title>
<source>Vision Research</source>,<volume>118</volume>,<fpage>132</fpage>–<lpage>143</lpage>.<pub-id pub-id-type="doi">10.1016/j.visres.2015.05.012</pub-id>
<issn>0042-6989</issn>
<?supplied-pmid 26049035?><pub-id pub-id-type="pmid">26049035</pub-id></mixed-citation>
    </ref>
    <ref id="b60">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Špakov</surname>,<given-names>O.</given-names></string-name>, &amp; <string-name><surname>Miniotas</surname>,<given-names>D.</given-names></string-name></person-group> (<year>2007</year>).<article-title>Visualization of eye gaze data using heat maps.</article-title>
<source>Electronics and Electrical Engineering</source>,<volume>2</volume>(<issue>74</issue>),<fpage>55</fpage>–<lpage>58</lpage>.
</mixed-citation>
    </ref>
    <ref id="b61">
      <mixed-citation publication-type="book"><person-group person-group-type="editor"><string-name><surname>Thomas</surname>,<given-names>J. J.</given-names></string-name>, &amp; <string-name><surname>Cook</surname>,<given-names>K. A.</given-names></string-name> (<role>Eds.</role>)
</person-group>. (<year>2005</year>).<source>Illuminating the path: The research and development agenda for visual analytics</source>.<publisher-name>IEEE Computer Society</publisher-name>.
</mixed-citation>
    </ref>
    <ref id="b62">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ward</surname>,<given-names>M. O.</given-names></string-name></person-group> (<year>2009</year>).<chapter-title>Linking and Brushing</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>L.</given-names><surname>Liu</surname></string-name> &amp; <string-name><given-names>M. T.</given-names><surname>Özsu</surname></string-name> (<role>Eds.</role>),
</person-group>
<source>Encyclopedia of Database Systems</source> (pp. <fpage>1623</fpage>–<lpage>1626</lpage>).<publisher-name>Springer</publisher-name>.,<pub-id pub-id-type="doi">10.1007/978-0-387-39940-9_1129</pub-id>
</mixed-citation>
    </ref>
    <ref id="b63">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weber</surname>,<given-names>R. B.</given-names></string-name>, &amp; <string-name><surname>Daroff</surname>,<given-names>R. B.</given-names></string-name></person-group> (<year>1972</year>).<article-title>Corrective movements following refixation saccades: Type and control system analysis.</article-title>
<source>Vision Research</source>,<volume>12</volume>(<issue>3</issue>),<fpage>467</fpage>–<lpage>475</lpage>.<pub-id pub-id-type="doi">10.1016/0042-6989(72)90090-9</pub-id>
<issn>0042-6989</issn>
<?supplied-pmid 5021911?><pub-id pub-id-type="pmid">5021911</pub-id></mixed-citation>
    </ref>
    <ref id="b64">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Yarbus</surname>,<given-names>A. L.</given-names></string-name></person-group> (<year>1967</year>).<chapter-title>Eye movements during perception of complex objects</chapter-title>. In <source>Eye Movements and Vision</source> (pp. <fpage>171</fpage>–<lpage>211</lpage>).<publisher-name>Springer</publisher-name>.,<pub-id pub-id-type="doi">10.1007/978-1-4899-5379-7_8</pub-id>
</mixed-citation>
    </ref>
    <ref id="b65">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yokoyama</surname>,<given-names>T.</given-names></string-name>,<string-name><surname>Noguchi</surname>,<given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Kita</surname>,<given-names>S.</given-names></string-name></person-group> (<year>2012</year>).<article-title>Attentional shifts by gaze direction in voluntary orienting: Evidence from a microsaccade study.</article-title>
<source>Experimental Brain Research</source>,<volume>223</volume>(<issue>2</issue>),<fpage>291</fpage>–<lpage>300</lpage>.<pub-id pub-id-type="doi">10.1007/s00221-012-3260-z</pub-id>
<issn>0014-4819</issn>
<?supplied-pmid 23001417?><pub-id pub-id-type="pmid">23001417</pub-id></mixed-citation>
    </ref>
    <ref id="b66">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yuval-Greenberg</surname>,<given-names>S.</given-names></string-name>,<string-name><surname>Merriam</surname>,<given-names>E. P.</given-names></string-name>, &amp; <string-name><surname>Heeger</surname>,<given-names>D. J.</given-names></string-name></person-group> (<year>2014</year>).<article-title>Spontaneous microsaccades reflect shifts in covert attention.</article-title>
<source>The Journal of Neuroscience : The Official Journal of the Society for Neuroscience</source>,<volume>34</volume>(<issue>41</issue>),<fpage>13693</fpage>–<lpage>13700</lpage>.<pub-id pub-id-type="doi">10.1523/JNEUROSCI.0582-14.2014</pub-id>
<issn>0270-6474</issn>
<?supplied-pmid 25297096?><pub-id pub-id-type="pmid">25297096</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
