<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10060701</article-id>
    <article-id pub-id-type="pmid">33769494</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btab207</article-id>
    <article-id pub-id-type="publisher-id">btab207</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Data and Text Mining</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SumGNN: multi-typed drug interaction prediction via efficient knowledge graph summarization</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Yu</surname>
          <given-names>Yue</given-names>
        </name>
        <aff><institution>College of Computing, Georgia Institute of Technology</institution>, Atlanta, GA 30332, <country country="US">USA</country></aff>
        <xref rid="btab207-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-6693-8390</contrib-id>
        <name>
          <surname>Huang</surname>
          <given-names>Kexin</given-names>
        </name>
        <aff><institution>Health Data Science, Harvard T.H. Chan School of Public Health</institution>, Boston, MA 02115, <country country="US">USA</country></aff>
        <xref rid="btab207-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Chao</given-names>
        </name>
        <aff><institution>College of Computing, Georgia Institute of Technology</institution>, Atlanta, GA 30332, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Glass</surname>
          <given-names>Lucas M</given-names>
        </name>
        <aff><institution>Analytic Center of Excellence, IQVIA</institution>, Cambridge, MA 02139, <country country="US">USA</country></aff>
        <aff><institution>Department of Statistics, Temple University</institution>, Philadelphia, PA 19122, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sun</surname>
          <given-names>Jimeng</given-names>
        </name>
        <aff><institution>Department of Computer Science, University of Illinois at Urbana-Champaign</institution>, Urbana, IL 61801, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Xiao</surname>
          <given-names>Cao</given-names>
        </name>
        <xref rid="btab207-cor1" ref-type="corresp"/>
        <aff><institution>Analytic Center of Excellence, IQVIA</institution>, Cambridge, MA 02139, <country country="US">USA</country></aff>
        <!--cao.xiao@iqvia.com-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Wren</surname>
          <given-names>Jonathan</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <fn id="btab207-FM1">
        <p>The authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors.</p>
      </fn>
      <corresp id="btab207-cor1">To whom correspondence should be addressed. <email>cao.xiao@iqvia.com</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>15</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2021-03-26">
      <day>26</day>
      <month>3</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>26</day>
      <month>3</month>
      <year>2021</year>
    </pub-date>
    <volume>37</volume>
    <issue>18</issue>
    <fpage>2988</fpage>
    <lpage>2995</lpage>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>10</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>07</day>
        <month>2</month>
        <year>2021</year>
      </date>
      <date date-type="editorial-decision">
        <day>17</day>
        <month>3</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>24</day>
        <month>3</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btab207.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Thanks to the increasing availability of drug–drug interactions (DDI) datasets and large biomedical knowledge graphs (KGs), accurate detection of adverse DDI using machine learning models becomes possible. However, it remains largely an open problem how to effectively utilize large and noisy biomedical KG for DDI detection. Due to its sheer size and amount of noise in KGs, it is often less beneficial to directly integrate KGs with other smaller but higher quality data (e.g. experimental data). Most of existing approaches ignore KGs altogether. Some tries to directly integrate KGs with other data via graph neural networks with limited success. Furthermore most previous works focus on binary DDI prediction whereas the multi-typed DDI pharmacological effect prediction is more meaningful but harder task.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>To fill the gaps, we propose a new method SumGNN: <italic toggle="yes">knowledge summarization graph neural network</italic>, which is enabled by a subgraph extraction module that can efficiently anchor on relevant subgraphs from a KG, a self-attention based subgraph summarization scheme to generate reasoning path within the subgraph, and a multi-channel knowledge and data integration module that utilizes massive external biomedical knowledge for significantly improved multi-typed DDI predictions. SumGNN outperforms the best baseline by up to 5.54%, and performance gain is particularly significant in low data relation types. In addition, SumGNN provides interpretable prediction via the generated reasoning paths for each prediction.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The code is available in <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Science Foundation</institution>
            <institution-id institution-id-type="DOI">10.13039/100000001</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>SCH-2014438</award-id>
        <award-id>IIS-1418511</award-id>
        <award-id>CCF-1533768</award-id>
        <award-id>IIS-1838042</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institute of Health</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>NIH</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>R01 1R01NS107291-01</award-id>
        <award-id>R56HL138415</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Adverse drug–drug interactions (DDI) are modifications of the effect of a drug when administered with another drug, which is a common and dangerous scenario for patients with complicated conditions. Undetected adverse DDIs have become serious health threats and caused nearly 74 000 emergency room visits and 195 000 hospitalizations each year in the United States alone (<xref rid="btab207-B24" ref-type="bibr">Percha and Altman, 2013</xref>). To mitigate these risks and costs, accurate prediction of DDIs becomes a clinically important task. Two types of data are being utilized for developing DDI detection models: Manually curated DDI networks and large biomedical knowledge graphs (KGs).</p>
    <p><bold>Curated DDI networks:</bold> Researchers have curated DDI networks based on experimental datasets and literature such as TWOSIDES (<xref rid="btab207-B38" ref-type="bibr">Tatonetti <italic toggle="yes">et al.</italic>, 2012</xref>), MINER (<xref rid="btab207-B54" ref-type="bibr">Zitnik <italic toggle="yes">et al.</italic>, 2018a</xref>) and DrugBank (<xref rid="btab207-B28" ref-type="bibr">Ryu <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btab207-B48" ref-type="bibr">Wishart <italic toggle="yes">et al.</italic>, 2018</xref>). These curated data are <italic toggle="yes">of higher quality but expensive to create and usually smaller in size.</italic></p>
    <p><bold>Knowledge graph:</bold> Over the years, large KG such as (<xref rid="btab207-B27" ref-type="bibr">Rotmensch <italic toggle="yes">et al.</italic>, 2017</xref>), Hetionet (<xref rid="btab207-B11" ref-type="bibr">Himmelstein and Baranzini, 2015</xref>) and DRKG (<xref rid="btab207-B15" ref-type="bibr">Ioannidis <italic toggle="yes">et al.</italic>, 2020</xref>) have been constructed from literature mining and database integration. However, these KGs are <italic toggle="yes">large and noisy</italic>: out of their tens of thousands of nodes with millions of edges, only a small subgraph is relevant to a prediction target.</p>
    <p><bold>Deep learning</bold>: Graph neural networks (GNN) have achieved great performance by casting DDI prediction as a link prediction problem on DDI graphs (<xref rid="btab207-B10" ref-type="bibr">Gysi <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab207-B14" ref-type="bibr">Huang <italic toggle="yes">et al.</italic>, 2020b</xref>; <xref rid="btab207-B55" ref-type="bibr">Zitnik <italic toggle="yes">et al.</italic>, 2018b</xref>). However, existing deep learning models are often trained only based on the DDI dataset at hand, ignoring the large biomedical KG (<xref rid="btab207-B11" ref-type="bibr">Himmelstein and Baranzini, 2015</xref>; <xref rid="btab207-B15" ref-type="bibr">Ioannidis <italic toggle="yes">et al.</italic>, 2020</xref>) which can benefit the DDI predictions since DDI is driven by complicated biomedical mechanism. Some recent works (<xref rid="btab207-B17" ref-type="bibr">Karim <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btab207-B22" ref-type="bibr">Lin <italic toggle="yes">et al.</italic>, 2020</xref>) tried to integrate KG into the DDI prediction via direct integration of standard KG and GNN methods. But DDI prediction presents unique modeling difficulties since the input KG is large and noisy while the pertinent information for a drug pair is local. Moreover, most existing works also only make binary classification—predicting the presence of DDIs, despite that predicting the particular DDI type is a more meaningful task.</p>
    <p><bold>Our approach</bold>. In this work, we propose a new method SumGNN that efficiently uses KG to aid drug interaction prediction. SumGNN enjoys improved predictive performance, efficiency, inductiveness and interpretability. SumGNN provides the following technical contributions:
</p>
    <list list-type="order">
      <list-item>
        <p><bold>Local subgraph for identifying useful information</bold>. We use local subgraph in the KG around drug pairs to extract useful information, instead of the entire KG. The subgraph formulation allows noise reduction by anchoring on relevant information and is highly scalable since the message passing receptive field is significantly decreased.</p>
      </list-item>
      <list-item>
        <p><bold>Subgraph summarization scheme for generating reasoning path.</bold> We then propose a summarization scheme to generate mechanism pathway for drug interactions. We develop a layer-independent self-attention mechanism to generate signal intensity score for each edge in the subgraph and prune out a KG subgraph pathways that have high scores. As this pruned subgraph is sparse, it provides insights on the biological processes that drive drug interactions.</p>
      </list-item>
      <list-item>
        <p><bold>Multi-channel data and knowledge integration for improved multi-typed DDI predictions</bold>. We propose to use multi-channel neural encoding to aggregate diverse set of data sources, ranging from the summarized subgraph embedding to chemical structures. It enables utilization of massive external biomedical knowledge for significantly improved multi-typed DDI predictions. In addition, the neural encoding takes different subgraph in each propagation, forming an inductive bias that promotes generalizability in low-resource DDI types.</p>
      </list-item>
    </list>
    <p>We conduct extensive experiments to show SumGNN improves DDI prediction significantly. It has up to 5.54% increase over the best baseline in F1 while the inference time is greatly reduced. Moreover, SumGNN excels at low-resource settings whereas previous works do not. SumGNN is also able to provide reasonable clues about the underlying mechanism of the drug interactions.</p>
  </sec>
  <sec>
    <title>2 Related works</title>
    <p><bold>External knowledge graph integration</bold>. Recently, several efforts have attempted to leverage the KG for downstream tasks such as recommendation (<xref rid="btab207-B35" ref-type="bibr">Sun <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btab207-B45" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2019a</xref>,<xref rid="btab207-B46" ref-type="bibr">b</xref>), information extraction (<xref rid="btab207-B21" ref-type="bibr">Liang <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab207-B44" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2018</xref>) and drug interaction prediction (<xref rid="btab207-B5" ref-type="bibr">Celebi <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btab207-B17" ref-type="bibr">Karim <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btab207-B22" ref-type="bibr">Lin <italic toggle="yes">et al.</italic>, 2020</xref>). For drug interaction prediction, <xref rid="btab207-B36" ref-type="bibr">Takeda <italic toggle="yes">et al.</italic> (2017)</xref> integrate the pharmacokinetic (PK) or pharmacodynamic (PD) side-effect when predicting drug interaction and <xref rid="btab207-B20" ref-type="bibr">Li <italic toggle="yes">et al.</italic> (2015)</xref> develop a Bayesian network to combine molecular similarity and drug side-effect similarity to predict the drug effect. However, these methods only consider side effect as the external knowledge, which may not be comprehensive enough in our task. With the emergence of the biomedical KGs (<xref rid="btab207-B11" ref-type="bibr">Himmelstein and Baranzini, 2015</xref>; <xref rid="btab207-B15" ref-type="bibr">Ioannidis <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab207-B27" ref-type="bibr">Rotmensch <italic toggle="yes">et al.</italic>, 2017</xref>), more types of entities and relations have been applied to this task, as <xref rid="btab207-B47" ref-type="bibr">Wang (2017)</xref>, <xref rid="btab207-B3" ref-type="bibr">Burkhardt <italic toggle="yes">et al.</italic> (2019)</xref>, <xref rid="btab207-B5" ref-type="bibr">Celebi <italic toggle="yes">et al.</italic> (2019)</xref>, <xref rid="btab207-B17" ref-type="bibr">Karim <italic toggle="yes">et al.</italic> (2019)</xref> and <xref rid="btab207-B8" ref-type="bibr">Dai <italic toggle="yes">et al.</italic> (2020)</xref> project each entity and relation to a dense vector with KG embedding techniques (<xref rid="btab207-B2" ref-type="bibr">Bordes <italic toggle="yes">et al.</italic>, 2013</xref>; <xref rid="btab207-B34" ref-type="bibr">Su <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab207-B40" ref-type="bibr">Trouillon <italic toggle="yes">et al.</italic>, 2016</xref>) and then feed them to neural networks for prediction. However, they do not directly harness the neighborhood information for target entities during inference, thus the external knowledge information are not sufficiently exploited. To tackle the above drawback (<xref rid="btab207-B22" ref-type="bibr">Lin <italic toggle="yes">et al.</italic>, 2020</xref>) adopts graph convolutional networks with neighborhood sampling to explicitly model the neighborhood relations with higher inference speed. However, as each neighboring entity could play a crucial role in the drug interaction mechanism, random sampling could potentially dropout these important factors and hinders the prediction performance. In contrast, SumGNN provides a learnable way to extract useful information in the neighborhood. In addition, the previous works all focus on binary DDI prediction whereas SumGNN evaluates on multi-type relation network.</p>
    <p><bold>Subgraph graph neural network.</bold> Graph neural networks have been proposed for modeling the relation between nodes (<xref rid="btab207-B19" ref-type="bibr">Kipf and Welling, 2017</xref>; <xref rid="btab207-B29" ref-type="bibr">Schlichtkrull <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btab207-B33" ref-type="bibr">Srinivasa <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab207-B42" ref-type="bibr">Veličković <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btab207-B50" ref-type="bibr">Yu <italic toggle="yes">et al.</italic>, 2020a</xref>) and have been successfully applied to various domains (<xref rid="btab207-B30" ref-type="bibr">Shang <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btab207-B32" ref-type="bibr">Shi <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab207-B51" ref-type="bibr">Yu <italic toggle="yes">et al.</italic>, 2020b</xref>). Subgraph structure contains rich information for many graph learning tasks (<xref rid="btab207-B12" ref-type="bibr">Huang and Zitnik, 2020</xref>; <xref rid="btab207-B39" ref-type="bibr">Teru and Hamilton, 2020</xref>; <xref rid="btab207-B43" ref-type="bibr">Veličković <italic toggle="yes">et al.</italic>, 2019</xref>). For instance, Ego-CNN applies local ego network to identify structures for graph classification (<xref rid="btab207-B41" ref-type="bibr">Tzeng and Wu, 2019</xref>). <xref rid="btab207-B1" ref-type="bibr">Alsentzer <italic toggle="yes">et al.</italic> (2020)</xref> formulate a multi-channel way to subgraph classification. Cluster-GCN (<xref rid="btab207-B6" ref-type="bibr">Chiang <italic toggle="yes">et al.</italic>, 2019</xref>) and GraphSAINT (<xref rid="btab207-B52" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic>, 2020</xref>) use subgraphs to improve GNN scalability. More relevant to us, <xref rid="btab207-B53" ref-type="bibr">Zhang and Chen (2018)</xref> apply local subgraph for link prediction and GraIL (<xref rid="btab207-B39" ref-type="bibr">Teru and Hamilton, 2020</xref>) extend this idea into KG completion task via utilizing multi-relational information. In contrast, SumGNN is driven by the domain DDI prediction problem and is the first to design a graph summarization module on subgraphs to obtain tractable pathway. It also integrates a new multi-channel neural encoding mechanism. We show these modules significantly improve predictive performance over these related works in Section 4.2.</p>
  </sec>
  <sec>
    <title>3 Materials and methods</title>
    <p>We present SumGNN in this section (The SumGNN code is available at <ext-link xlink:href="https://github.com/yueyu1030/SumGNN" ext-link-type="uri">https://github.com/yueyu1030/SumGNN</ext-link>.). We summarize problem settings in Section 3.1 and describe our method in detail in Section 3.2. Our method can be decomposed into three modules. First, we extract the local subgraph in the KG around drug pairs to obtain useful information. Then, we propose a summarization scheme to generate a mechanism pathway for drug interactions. After that, we describe a multi-channel neural encoding layer to predict the pharmacological effect.</p>
    <sec>
      <title>3.1 Problem settings</title>
      <statement id="mthst1">
        <label>Definition 1</label>
        <p>(<bold>Drug Interaction Graph</bold>). Given drugs <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mi mathvariant="script">D</mml:mi></mml:math></inline-formula> and pharmacological effects <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the drug interaction graph <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">DDI</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is defined as a set of triplets <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">DDI</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, where each triplet (<italic toggle="yes">u</italic>, <italic toggle="yes">r</italic>, <italic toggle="yes">v</italic>) represents that drug <italic toggle="yes">u</italic> and drug <italic toggle="yes">v</italic> have pharmacological effect <italic toggle="yes">r</italic>.</p>
      </statement>
      <statement id="mthst2">
        <label>Definition 2</label>
        <p>(<bold>External Biomedical Knowledge Graph</bold>). Given a set of various biomedical entities <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mi mathvariant="script">E</mml:mi></mml:math></inline-formula> and the biomedical relation among the entities <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mi mathvariant="script">R</mml:mi></mml:math></inline-formula>, the external biomedical KG <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is defined as <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">E</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">R</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> with each item (<italic toggle="yes">h</italic>, <italic toggle="yes">r</italic>, <italic toggle="yes">t</italic>) describes a biomedical relation <italic toggle="yes">r</italic> between entity <italic toggle="yes">h</italic> and entity <italic toggle="yes">t</italic>. Note that we aggregate the drug entities in <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">DDI</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, i.e. <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="script">R</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">E</mml:mi></mml:mrow></mml:math></inline-formula>.</p>
      </statement>
      <statement id="mthst3">
        <label>Problem 1</label>
        <p>(<bold>Multi-relational DDI Prediction</bold>). The Drug–drug interaction (DDI) prediction is to output the pharmacological effect given the a pair of drugs. Mathematically, it is to learn a mapping <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">F</mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from a drug pair <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> to the pharmacological effect <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
      </statement>
    </sec>
    <sec>
      <title>3.2 The SumGNN method</title>
      <p>SumGNN is composed of three modules: subgraph anchoring, knowledge summarization and multi-channel neural encoding. For a given drug pair, we anchor to a subgraph of potential biomedical entities that are close to the pairs in the KG. Then, we propose a new graph neural network that has a summarization scheme to provide a condense pathway to reason about the drug interaction mechanism. Given this pathway graph, we use multi-channel neural encoding, to integrate diverse sources of available information to generate a sufficient drug pair representation. At last, a decoding classifier is followed to predict the interaction outcome. We initialize all entity embedding using KG method TransE (<xref rid="btab207-B2" ref-type="bibr">Bordes <italic toggle="yes">et al.</italic>, 2013</xref>), where a entity is denoted as <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> (<xref rid="btab207-F1" ref-type="fig">Fig. 1</xref>).</p>
      <fig position="float" id="btab207-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>SumGNN illustration</p>
        </caption>
        <graphic xlink:href="btab207f1" position="float"/>
      </fig>
      <p>
        <bold>(A) The local subgraph extraction module</bold>
      </p>
      <p>The biomedical KG describes the complicated mechanism of human biology. Modulation in several nodes (drug-pairs) in the KG can perturb the connected nodes (e.g. disease, cellular component, etc.) which creates a ripple effect that eventually result in various physiological outcomes (<xref rid="btab207-B11" ref-type="bibr">Himmelstein and Baranzini, 2015</xref>). The effect is diffused as distance between the drug pairs and the biomedical entities increases. Thus, to understand the drug interactions, we focus on local subgraphs in the KG around the drug pairs. Specifically, for drug pairs <italic toggle="yes">u</italic> and <italic toggle="yes">v</italic>, we first extract the <italic toggle="yes">k</italic>-hop neighboring nodes for both <italic toggle="yes">u</italic> and <italic toggle="yes">v</italic>, <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>s</mml:mi><mml:mo>|</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>k</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>s</mml:mi><mml:mo>|</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>k</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> stands for the distance between two nodes on <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Then, we obtain the enclosing subgraph based on the intersection of these nodes, <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Sub</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∩</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">R</mml:mi></mml:mrow></mml:math></inline-formula>}.</p>
      <p>Motivated by <xref rid="btab207-B53" ref-type="bibr">Zhang and Chen (2018)</xref> which highlights the importance of node relative position to the central node <italic toggle="yes">u</italic>, <italic toggle="yes">v</italic> in the subgraph, we augment the initial node embedding in the subgraph by concatenating a position vector. For each node <italic toggle="yes">i</italic> in the subgraph <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Sub</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, we first compute the shortest path length <italic toggle="yes">d</italic>(<italic toggle="yes">i</italic>, <italic toggle="yes">u</italic>) and <italic toggle="yes">d</italic>(<italic toggle="yes">i</italic>, <italic toggle="yes">v</italic>) between <italic toggle="yes">i</italic> and the center drug pairs nodes <italic toggle="yes">u</italic>, <italic toggle="yes">v</italic>. After that, we convert it into a position vector <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mtext>one</mml:mtext><mml:mo>-</mml:mo><mml:mtext>hot</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>⊕</mml:mo><mml:mtext>one</mml:mtext><mml:mo>-</mml:mo><mml:mtext>hot</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. Then, we update the node <italic toggle="yes">i</italic> representation as <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
      <p>
        <bold>(B) The knowledge summarization module</bold>
      </p>
      <p>To provide biological insights in addition to the predictive outcome, we design a knowledge summarization module to summarize the subgraph information into a graph-based pathway for potential drug interactions. Note that the pathway is not a linear line, but a sparse subgraph since drug interactions are usually due to complicated interplays among many types of biomedical entities. The summarization requires that we need to retain edges that contain most useful signals for the drug interactions while removing paths that are not important. To achieve this, we adopt a <italic toggle="yes">layer-independent</italic>, <italic toggle="yes">relation-aware</italic> self-attention module to assign a weight for every edge in <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Sub</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. These weights are generated based on the input featurization <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and represent the interaction signal intensities for edge pruning.</p>
      <p>Specifically, we denote the interaction signal intensity score for the edge connecting any biomedical entity <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic> as <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Inspired by the relation-aware transformer architecture (<xref rid="btab207-B31" ref-type="bibr">Shaw <italic toggle="yes">et al.</italic>, 2018</xref>), we use self-attention mechanism, which takes account into all neighbor nodes in the subgraph to generate the attention weight. This attention mechanism is ideal for us because it generates the signal intensity score after examining all biomedical entities in the subgraph around the drug-pairs. Here, <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is calculated as</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M1" display="block" overflow="scroll">
          <mml:mrow>
            <mml:msub>
              <mml:mrow>
                <mml:mo>α</mml:mo>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>i</mml:mi>
                <mml:mo>,</mml:mo>
                <mml:mi>j</mml:mi>
              </mml:mrow>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:mtext>Threshold</mml:mtext>
            <mml:mrow>
              <mml:mo>(</mml:mo>
              <mml:mrow>
                <mml:mo>ϕ</mml:mo>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mrow>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:msubsup>
                          <mml:mrow>
                            <mml:mi mathvariant="bold">h</mml:mi>
                          </mml:mrow>
                          <mml:mi>j</mml:mi>
                          <mml:mrow>
                            <mml:mo stretchy="false">(</mml:mo>
                            <mml:mn>0</mml:mn>
                            <mml:mo stretchy="false">)</mml:mo>
                          </mml:mrow>
                        </mml:msubsup>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mi mathvariant="bold">W</mml:mi>
                          </mml:mrow>
                          <mml:mi>J</mml:mi>
                        </mml:msup>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mrow>
                              <mml:mo stretchy="false">(</mml:mo>
                              <mml:msubsup>
                                <mml:mrow>
                                  <mml:mi mathvariant="bold">h</mml:mi>
                                </mml:mrow>
                                <mml:mi>i</mml:mi>
                                <mml:mrow>
                                  <mml:mo stretchy="false">(</mml:mo>
                                  <mml:mn>0</mml:mn>
                                  <mml:mo stretchy="false">)</mml:mo>
                                </mml:mrow>
                              </mml:msubsup>
                              <mml:msup>
                                <mml:mrow>
                                  <mml:mi mathvariant="bold">W</mml:mi>
                                </mml:mrow>
                                <mml:mi>I</mml:mi>
                              </mml:msup>
                              <mml:mo>+</mml:mo>
                              <mml:msub>
                                <mml:mrow>
                                  <mml:mi mathvariant="bold">r</mml:mi>
                                </mml:mrow>
                                <mml:mrow>
                                  <mml:mi>i</mml:mi>
                                  <mml:mi>j</mml:mi>
                                </mml:mrow>
                              </mml:msub>
                              <mml:mo stretchy="false">)</mml:mo>
                            </mml:mrow>
                          </mml:mrow>
                          <mml:mi>T</mml:mi>
                        </mml:msup>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mrow>
                              <mml:msqrt>
                                <mml:mi>d</mml:mi>
                              </mml:msqrt>
                            </mml:mrow>
                          </mml:mrow>
                          <mml:mi>k</mml:mi>
                        </mml:msub>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:mrow>
                  <mml:mo>)</mml:mo>
                </mml:mrow>
                <mml:mo>,</mml:mo>
                <mml:mo>γ</mml:mo>
              </mml:mrow>
              <mml:mo>)</mml:mo>
            </mml:mrow>
            <mml:mo>,</mml:mo>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <p>where the <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mi>J</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> are the self-attention key weights that contain representation for each node in the subgraph, <italic toggle="yes">r<sub>ij</sub></italic> encodes the relationship between the two entities <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic>, <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the size of feature vector <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for normalization, <italic toggle="yes">γ</italic> is the signal threshold and <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> is the tanh function for non-linear transformation. Intuitively, this function first computes the dot product between <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> to get an attention score between node <italic toggle="yes">i</italic> and every neighbor node in the subgraph. Then we sum it up with the relation embedding, followed up the same procedure to calculate attention score between node <italic toggle="yes">j</italic> and every other node through dot product between <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE37"><mml:math id="IM37" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mi>J</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. By taking the dot product with every other nodes for both <italic toggle="yes">i</italic>, <italic toggle="yes">j</italic>, the final score considers all the subgraph information. Then, after non-linear transformation, we calculates the signal intensity score ranging from –1 to 1 for this edge. At last, we apply a threshold function to screen out edges that are below an intensity score threshold <italic toggle="yes">γ</italic> by setting them with weight 0 since they are not important for the interaction prediction and setting them 0 would prune these edges from message passing process in the graph neural network. This step is applied to every edge in the subgraph.</p>
      <p>Note that existing graph attention approaches (<xref rid="btab207-B4" ref-type="bibr">Cai and Lam, 2020</xref>; <xref rid="btab207-B31" ref-type="bibr">Shaw <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btab207-B42" ref-type="bibr">Veličković <italic toggle="yes">et al.</italic>, 2018</xref>) generate attention weights for every edge in every layer. However, this way can provide potentially contradicting signals across layers for the same edge, precluding the generation of interpretable pathways. In the contrast, SumGNN adopts a layer-independent attention mechanism, which only depends on the first layer embedding to prune edges. It provides an unequivocal pathway for model explainability. As many biological networks are constructed through text mining where many edges are potentially false positives, this pruning mechanism also allows noise reduction.</p>
      <p>
        <bold>(C) The multi-channel integration module</bold>
      </p>
      <p>To obtain a powerful representation for drug interaction prediction, we integrate a diverse set of information sources.</p>
      <p><bold>Channel 1: Summarized knowledge</bold> Using the knowledge summarization approach we described above, we identify a summarized subgraph that is important to input drug pairs. We want to generate the latent representation that leverages this subgraph for the input drug pairs. We integrate it using the following message-passing scheme. For each node <italic toggle="yes">v</italic>, we compute a relation-aware message weighted by the signal intensity score <inline-formula id="IE38"><mml:math id="IM38" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> at layer <italic toggle="yes">l</italic> using the attention score as</p>
      <disp-formula id="E2">
        <label>(2)</label>
        <mml:math id="M2" display="block" overflow="scroll">
          <mml:mrow>
            <mml:msubsup>
              <mml:mrow>
                <mml:mi mathvariant="bold">b</mml:mi>
              </mml:mrow>
              <mml:mi>v</mml:mi>
              <mml:mrow>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mi>l</mml:mi>
                <mml:mo stretchy="false">)</mml:mo>
              </mml:mrow>
            </mml:msubsup>
            <mml:mo>=</mml:mo>
            <mml:munder>
              <mml:mo>∑</mml:mo>
              <mml:mrow>
                <mml:mi>u</mml:mi>
                <mml:mo>∈</mml:mo>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi mathvariant="script">N</mml:mi>
                  </mml:mrow>
                  <mml:mi>v</mml:mi>
                </mml:msub>
              </mml:mrow>
            </mml:munder>
            <mml:mrow>
              <mml:msubsup>
                <mml:mrow>
                  <mml:mo>α</mml:mo>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>u</mml:mi>
                  <mml:mo>,</mml:mo>
                  <mml:mi>v</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mi>l</mml:mi>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
              </mml:msubsup>
            </mml:mrow>
            <mml:mrow>
              <mml:mo>(</mml:mo>
              <mml:mrow>
                <mml:msubsup>
                  <mml:mrow>
                    <mml:mi mathvariant="bold">h</mml:mi>
                  </mml:mrow>
                  <mml:mi>u</mml:mi>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mi>l</mml:mi>
                    <mml:mo>−</mml:mo>
                    <mml:mn>1</mml:mn>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                </mml:msubsup>
                <mml:msubsup>
                  <mml:mrow>
                    <mml:mi mathvariant="bold">W</mml:mi>
                  </mml:mrow>
                  <mml:mi>r</mml:mi>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mi>l</mml:mi>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                </mml:msubsup>
              </mml:mrow>
              <mml:mo>)</mml:mo>
            </mml:mrow>
            <mml:mo>,</mml:mo>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <p>where <inline-formula id="IE39"><mml:math id="IM39" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the neighbors of node <italic toggle="yes">v</italic> in subgraph <inline-formula id="IE40"><mml:math id="IM40" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Sub</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the weight matrix to transform hidden representation for node <italic toggle="yes">u</italic>, <italic toggle="yes">v’</italic>s relation <italic toggle="yes">r</italic> in layer <italic toggle="yes">l</italic>. To avoid overfitting, we use basis decomposition (<xref rid="btab207-B29" ref-type="bibr">Schlichtkrull <italic toggle="yes">et al.</italic>, 2018</xref>) to decompose <inline-formula id="IE41"><mml:math id="IM41" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> into the linear combination of a small number of basis matrices <inline-formula id="IE42"><mml:math id="IM42" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> as</p>
      <disp-formula id="E3">
        <label>(3)</label>
        <mml:math id="M3" display="block" overflow="scroll">
          <mml:mrow>
            <mml:msubsup>
              <mml:mrow>
                <mml:mi mathvariant="bold">W</mml:mi>
              </mml:mrow>
              <mml:mi>r</mml:mi>
              <mml:mrow>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mi>l</mml:mi>
                <mml:mo stretchy="false">)</mml:mo>
              </mml:mrow>
            </mml:msubsup>
            <mml:mo>=</mml:mo>
            <mml:munderover>
              <mml:mo>∑</mml:mo>
              <mml:mrow>
                <mml:mi>b</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mn>1</mml:mn>
              </mml:mrow>
              <mml:mi>B</mml:mi>
            </mml:munderover>
            <mml:mrow>
              <mml:msubsup>
                <mml:mrow>
                  <mml:mi>a</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>r</mml:mi>
                  <mml:mi>b</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mi>l</mml:mi>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
              </mml:msubsup>
            </mml:mrow>
            <mml:msubsup>
              <mml:mrow>
                <mml:mi mathvariant="bold">V</mml:mi>
              </mml:mrow>
              <mml:mi>b</mml:mi>
              <mml:mrow>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mi>l</mml:mi>
                <mml:mo stretchy="false">)</mml:mo>
              </mml:mrow>
            </mml:msubsup>
            <mml:mo>.</mml:mo>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <p>Then, we propagate the message <inline-formula id="IE43"><mml:math id="IM43" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> to the updated representation <inline-formula id="IE44"><mml:math id="IM44" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> of node <italic toggle="yes">v</italic> via</p>
      <disp-formula id="E4">
        <label>(4)</label>
        <mml:math id="M4" display="block" overflow="scroll">
          <mml:mrow>
            <mml:msubsup>
              <mml:mrow>
                <mml:mi mathvariant="bold">h</mml:mi>
              </mml:mrow>
              <mml:mi>v</mml:mi>
              <mml:mrow>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mi>l</mml:mi>
                <mml:mo stretchy="false">)</mml:mo>
              </mml:mrow>
            </mml:msubsup>
            <mml:mo>=</mml:mo>
            <mml:mtext>ReLU</mml:mtext>
            <mml:mrow>
              <mml:mo>(</mml:mo>
              <mml:mrow>
                <mml:msubsup>
                  <mml:mrow>
                    <mml:mi mathvariant="bold">W</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mtext>self</mml:mtext>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mi>l</mml:mi>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                </mml:msubsup>
                <mml:msubsup>
                  <mml:mrow>
                    <mml:mi mathvariant="bold">h</mml:mi>
                  </mml:mrow>
                  <mml:mi>v</mml:mi>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mi>l</mml:mi>
                    <mml:mo>−</mml:mo>
                    <mml:mn>1</mml:mn>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                </mml:msubsup>
                <mml:mo>+</mml:mo>
                <mml:msubsup>
                  <mml:mrow>
                    <mml:mi mathvariant="bold">b</mml:mi>
                  </mml:mrow>
                  <mml:mi>v</mml:mi>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mi>l</mml:mi>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                </mml:msubsup>
              </mml:mrow>
              <mml:mo>)</mml:mo>
            </mml:mrow>
            <mml:mo>,</mml:mo>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <p>where <inline-formula id="IE45"><mml:math id="IM45" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>self</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the weight matrix to transform the node embedding itself.</p>
      <p><bold>Channel 2: Subgraph features</bold> To obtain the embeddings for subgraphs (denoted as <inline-formula id="IE46"><mml:math id="IM46" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Sub</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>), we take the average of all node embeddings in <inline-formula id="IE47"><mml:math id="IM47" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Sub</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> at layer <italic toggle="yes">l</italic> projected by a linear layer as</p>
      <disp-formula id="E5">
        <label>(5)</label>
        <mml:math id="M5" display="block" overflow="scroll">
          <mml:mrow>
            <mml:msubsup>
              <mml:mrow>
                <mml:mi mathvariant="bold">h</mml:mi>
              </mml:mrow>
              <mml:mrow>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi mathvariant="script">G</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi mathvariant="italic">Sub</mml:mi>
                  </mml:mrow>
                </mml:msub>
              </mml:mrow>
              <mml:mrow>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mi>l</mml:mi>
                <mml:mo stretchy="false">)</mml:mo>
              </mml:mrow>
            </mml:msubsup>
            <mml:mo>=</mml:mo>
            <mml:mtext>Mean</mml:mtext>
            <mml:mrow>
              <mml:mo>(</mml:mo>
              <mml:mrow>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi mathvariant="bold">W</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mtext>Sub</mml:mtext>
                  </mml:mrow>
                </mml:msub>
                <mml:msubsup>
                  <mml:mrow>
                    <mml:mi mathvariant="bold">h</mml:mi>
                  </mml:mrow>
                  <mml:mi>i</mml:mi>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mi>l</mml:mi>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                </mml:msubsup>
              </mml:mrow>
              <mml:mo>)</mml:mo>
            </mml:mrow>
            <mml:mo>.</mml:mo>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <p><bold>Channel 3: Drug fingerprint</bold> Molecular information such as chemical fingerprints have shown to be powerful predictor of drug interactions (<xref rid="btab207-B13" ref-type="bibr">Huang <italic toggle="yes">et al.</italic>, 2020a</xref>). Thus, in addition to the network representation, we obtain the Morgan fingerprint <inline-formula id="IE48"><mml:math id="IM48" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (<xref rid="btab207-B26" ref-type="bibr">Rogers and Hahn, 2010</xref>), which is a predictive descriptor of drugs, for each drug <italic toggle="yes">v</italic>. Note that it is infeasible to use this feature as the input node feature in the KG since KG consists of various types of nodes other than drugs (e.g. Side Effect, Disease and genes) and they cannot be represented by Morgan Fingerprints, which lead to inconsistent node features for GNN propagation.</p>
      <p><bold>Layer-wise channels aggregation.</bold> To assemble various representation generated via each layer, we adopt the layer-aggregation mechanism (<xref rid="btab207-B49" ref-type="bibr">Xu <italic toggle="yes">et al.</italic>, 2018</xref>). We concatenate node/subgraph embeddings in every layer, i.e. <inline-formula id="IE49"><mml:math id="IM49" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE50"><mml:math id="IM50" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Sub</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Sub</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Sub</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Sub</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> where is <italic toggle="yes">L</italic> is the layer size. To integrate chemical fingerprints, we update the layer-aggregated embedding by concatenation of chemical representation: <inline-formula id="IE51"><mml:math id="IM51" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>⊕</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
      <p>At last, we combine the various channels together to obtain the input drug-pairs representation <inline-formula id="IE52"><mml:math id="IM52" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>u</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Sub</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. To predict the relation, we obtain a prediction probability vector <inline-formula id="IE53"><mml:math id="IM53" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> where each value in the vector corresponds to a the likelihood of a relation. <inline-formula id="IE54"><mml:math id="IM54" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is computed via feeding the drug pair representation to a decoder parameterized by <inline-formula id="IE55"><mml:math id="IM55" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext>pred</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>:</p>
      <disp-formula id="E6">
        <label>(6)</label>
        <mml:math id="M6" display="block" overflow="scroll">
          <mml:mrow>
            <mml:msub>
              <mml:mrow>
                <mml:mi mathvariant="bold">p</mml:mi>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>u</mml:mi>
                <mml:mo>,</mml:mo>
                <mml:mi>v</mml:mi>
              </mml:mrow>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:msub>
              <mml:mrow>
                <mml:mi mathvariant="bold">W</mml:mi>
              </mml:mrow>
              <mml:mrow>
                <mml:mtext>pred</mml:mtext>
              </mml:mrow>
            </mml:msub>
            <mml:msub>
              <mml:mrow>
                <mml:mi mathvariant="bold">h</mml:mi>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>u</mml:mi>
                <mml:mo>,</mml:mo>
                <mml:mi>v</mml:mi>
              </mml:mrow>
            </mml:msub>
            <mml:mo>.</mml:mo>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <sec>
        <title>3.2.1 Training and inference</title>
        <p>During training, for multi-class classification task, we adopt the cross entropy loss <inline-formula id="IE56"><mml:math id="IM56" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mtext>CE</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for each edge (<italic toggle="yes">u</italic>, <italic toggle="yes">r</italic>, <italic toggle="yes">v</italic>) as</p>
        <disp-formula id="E7">
          <label>(7)</label>
          <mml:math id="M7" display="block" overflow="scroll">
            <mml:mrow>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>ℓ</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mtext>CE</mml:mtext>
                </mml:mrow>
              </mml:msub>
              <mml:mo stretchy="false">(</mml:mo>
              <mml:mi>u</mml:mi>
              <mml:mo>,</mml:mo>
              <mml:mi>r</mml:mi>
              <mml:mo>,</mml:mo>
              <mml:mi>v</mml:mi>
              <mml:mo stretchy="false">)</mml:mo>
              <mml:mo>=</mml:mo>
              <mml:mo>−</mml:mo>
              <mml:munderover>
                <mml:mo>∑</mml:mo>
                <mml:mrow>
                  <mml:mi>r</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
                <mml:mi>R</mml:mi>
              </mml:munderover>
              <mml:mrow>
                <mml:mo> </mml:mo>
                <mml:mtext>log</mml:mtext>
                <mml:mo> </mml:mo>
              </mml:mrow>
              <mml:mo stretchy="false">(</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mover accent="true">
                    <mml:mi>y</mml:mi>
                    <mml:mo>^</mml:mo>
                  </mml:mover>
                </mml:mrow>
                <mml:mi>r</mml:mi>
              </mml:msub>
              <mml:mo stretchy="false">)</mml:mo>
              <mml:mo>·</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>y</mml:mi>
                </mml:mrow>
                <mml:mi>r</mml:mi>
              </mml:msub>
              <mml:mo>,</mml:mo>
            </mml:mrow>
          </mml:math>
        </disp-formula>
        <p>where <inline-formula id="IE57"><mml:math id="IM57" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>softmax</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>R</mml:mi></mml:munderover><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> and <italic toggle="yes">y<sub>r</sub></italic> is the binary indicator if class <italic toggle="yes">r</italic> is the correct label for <italic toggle="yes">u</italic> and <italic toggle="yes">v</italic>. For multi-label classification task, given the edge (<italic toggle="yes">u</italic>, <italic toggle="yes">r</italic>, <italic toggle="yes">v</italic>), we adopt the binary cross entropy loss <inline-formula id="IE58"><mml:math id="IM58" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mtext>BCE</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> as</p>
        <disp-formula id="E8">
          <label>(8)</label>
          <mml:math id="M8" display="block" overflow="scroll">
            <mml:mrow>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>ℓ</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mtext>BCE</mml:mtext>
                </mml:mrow>
              </mml:msub>
              <mml:mo stretchy="false">(</mml:mo>
              <mml:mi>u</mml:mi>
              <mml:mo>,</mml:mo>
              <mml:mi>r</mml:mi>
              <mml:mo>,</mml:mo>
              <mml:mi>v</mml:mi>
              <mml:mo stretchy="false">)</mml:mo>
              <mml:mo>=</mml:mo>
              <mml:mo>−</mml:mo>
              <mml:mtext>log</mml:mtext>
              <mml:mo> </mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mover accent="true">
                    <mml:mi>y</mml:mi>
                    <mml:mo>^</mml:mo>
                  </mml:mover>
                </mml:mrow>
                <mml:mi>r</mml:mi>
              </mml:msub>
              <mml:mo>−</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi mathvariant="double-struck">E</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>w</mml:mi>
                  <mml:mo>∼</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>P</mml:mi>
                    </mml:mrow>
                    <mml:mi>w</mml:mi>
                  </mml:msub>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mi>v</mml:mi>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
              </mml:msub>
              <mml:mo> </mml:mo>
              <mml:mtext>log</mml:mtext>
              <mml:mo> </mml:mo>
              <mml:mrow>
                <mml:mo>(</mml:mo>
                <mml:mrow>
                  <mml:mn>1</mml:mn>
                  <mml:mo>−</mml:mo>
                  <mml:msubsup>
                    <mml:mrow>
                      <mml:mi>y</mml:mi>
                    </mml:mrow>
                    <mml:mi>r</mml:mi>
                    <mml:mrow>
                      <mml:mi>u</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mi>w</mml:mi>
                    </mml:mrow>
                  </mml:msubsup>
                </mml:mrow>
                <mml:mo>)</mml:mo>
              </mml:mrow>
              <mml:mo>,</mml:mo>
            </mml:mrow>
          </mml:math>
        </disp-formula>
        <p>where (<italic toggle="yes">u</italic>, <italic toggle="yes">r</italic>, <italic toggle="yes">w</italic>) is the sampled negative edge for relation <italic toggle="yes">r</italic>. This is achieved by replacing node <italic toggle="yes">v</italic> to node <italic toggle="yes">w</italic> that is sampled randomly according to a distribution <inline-formula id="IE59"><mml:math id="IM59" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> (<xref rid="btab207-B23" ref-type="bibr">Mikolov <italic toggle="yes">et al.</italic>, 2013</xref>). Then <inline-formula id="IE60"><mml:math id="IM60" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>sigmoid</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mi>u</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>sigmoid</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the prediction score for two edges. Considering all edges, the final loss <inline-formula id="IE61"><mml:math id="IM61" display="inline" overflow="scroll"><mml:mi>ℓ</mml:mi></mml:math></inline-formula> in SumGNN is</p>
        <disp-formula id="E9">
          <label>(9)</label>
          <mml:math id="M9" display="block" overflow="scroll">
            <mml:mrow>
              <mml:mi>ℓ</mml:mi>
              <mml:mo>=</mml:mo>
              <mml:munder>
                <mml:mo>∑</mml:mo>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mi>u</mml:mi>
                  <mml:mo>,</mml:mo>
                  <mml:mi>r</mml:mi>
                  <mml:mo>,</mml:mo>
                  <mml:mi>v</mml:mi>
                  <mml:mo stretchy="false">)</mml:mo>
                  <mml:mo>∈</mml:mo>
                  <mml:mi mathvariant="script">E</mml:mi>
                </mml:mrow>
              </mml:munder>
              <mml:mi>ℓ</mml:mi>
              <mml:mo stretchy="false">(</mml:mo>
              <mml:mi>u</mml:mi>
              <mml:mo>,</mml:mo>
              <mml:mi>r</mml:mi>
              <mml:mo>,</mml:mo>
              <mml:mi>v</mml:mi>
              <mml:mo stretchy="false">)</mml:mo>
              <mml:mo>,</mml:mo>
            </mml:mrow>
          </mml:math>
        </disp-formula>
        <p>where <inline-formula id="IE62"><mml:math id="IM62" display="inline" overflow="scroll"><mml:mi>ℓ</mml:mi></mml:math></inline-formula> is either <xref rid="E7" ref-type="disp-formula">Equations (7)</xref> or (8) depending on the task type. During training, we learn the model parameter by minimizing the total loss <inline-formula id="IE63"><mml:math id="IM63" display="inline" overflow="scroll"><mml:mi>ℓ</mml:mi></mml:math></inline-formula> using stochastic gradient optimizers such as Adam (<xref rid="btab207-B18" ref-type="bibr">Kingma and Ba, 2014</xref>).</p>
        <p>During inference, an unseen node pair <italic toggle="yes">u</italic>, <italic toggle="yes">v’</italic>s subgraph in the KG is extracted and fed into the same pipeline to calculate the relation vector. For multi-class task, we use the highest probability relation as the predicted relation and for multi-label task, we collect all scores from both positive and negative counterparts for all relations.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>4 Experiments</title>
    <sec>
      <title>4.1 Experiment setup</title>
      <p><bold>Datasets</bold> (i) <underline>DrugBank</underline> dataset (<xref rid="btab207-B48" ref-type="bibr">Wishart <italic toggle="yes">et al.</italic>, 2018</xref>) contains 1709 drugs (nodes) and 136 351 drug pairs (edges), which are associated with 86 types of pharmacological relations between drugs, such as increase of cardiotoxic activity, decrease of serum concentration, etc. Each drug pair can contain one or two relations. As more than 99.8% of edges have only one edge type (Ryu <italic toggle="yes">et al.</italic>, 2018), we filtered the edge with more than one type in our study. (ii) <underline>TWOSIDES</underline> (<xref rid="btab207-B38" ref-type="bibr">Tatonetti <italic toggle="yes">et al.</italic>, 2012</xref>) dataset contains 645 drugs (nodes) and 46 221 drug–drug pairs (edges) with 200 different drug side effect types as labels. For each edge, it may be associated with multiple labels. Following (<xref rid="btab207-B8" ref-type="bibr">Dai <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab207-B55" ref-type="bibr">Zitnik <italic toggle="yes">et al.</italic>, 2018b</xref>), we keep 200 <italic toggle="yes">commonly-occurring</italic> DDI types ranging from Top-600 to Top-800 to ensure every DDI type has at least 900 drug combinations. (iii) For external knowledge base, we use <underline>HetioNet</underline> (<xref rid="btab207-B11" ref-type="bibr">Himmelstein and Baranzini, 2015</xref>), which is a large heterogeneous KG merged from 29 public databases. To <bold>ensure no information leakage</bold>, we remove all the overlapping DDI edges between HetioNet and the dataset. In the end, we obtain 33 765 nodes out of 11 types (e.g. gene, disease, pathway, molecular function, etc.) with 1 690 693 edges from 23 relation types.</p>
      <p><bold>Baselines</bold> We compare our models with several baselines (Further details on baseline methods, implementation and parameters are in the supplementary.).
</p>
      <list list-type="bullet">
        <list-item>
          <p><underline>MLP</underline> (<xref rid="btab207-B26" ref-type="bibr">Rogers and Hahn, 2010</xref>) uses a two-layer MLP on Morgan fingerprint to directly predict drug interactions.</p>
        </list-item>
        <list-item>
          <p><underline>Deepwalk</underline> (<xref rid="btab207-B25" ref-type="bibr">Perozzi <italic toggle="yes">et al.</italic>, 2014</xref>) first learns the embeddings for drugs in the network via random walk. Then, it predict the relation for drug pairs via a linear layer over embeddings.</p>
        </list-item>
        <list-item>
          <p><underline>LINE</underline> (<xref rid="btab207-B37" ref-type="bibr">Tang <italic toggle="yes">et al.</italic>, 2015</xref>) use a one-layer feedforward neural network to learn the embeddings for drugs. Then, it stack a linear layer over embeddings to predict the relation.</p>
        </list-item>
        <list-item>
          <p><underline>Node2vec</underline> (<xref rid="btab207-B9" ref-type="bibr">Grover and Leskovec, 2016</xref>) first learns the embeddings for drugs in the network. Similar to Deepwalk, it predict the relation via a linear layer over embeddings.</p>
        </list-item>
        <list-item>
          <p><underline>Decagon</underline> (<xref rid="btab207-B55" ref-type="bibr">Zitnik <italic toggle="yes">et al.</italic>, 2018b</xref>) adopts multi-relational graph convolutional network (<xref rid="btab207-B29" ref-type="bibr">Schlichtkrull <italic toggle="yes">et al.</italic>, 2018</xref>) on the DDI network for drug interaction prediction.</p>
        </list-item>
        <list-item>
          <p><underline>GAT</underline> (<xref rid="btab207-B42" ref-type="bibr">Veličković <italic toggle="yes">et al.</italic>, 2018</xref>) uses attention networks to aggregate neighborhood information in DDI network.</p>
        </list-item>
        <list-item>
          <p><underline>SkipGNN</underline> (Huang <italic toggle="yes">et al.</italic>, 2020b) predicts drug interactions by aggregating information from both direct interactions and second-order interactions via two GNNs.</p>
        </list-item>
        <list-item>
          <p>PRD (<xref rid="btab207-B47" ref-type="bibr">Wang, 2017</xref>) first use KG embeddings for drugs in the KG, then pass through a linear layer for drug interaction prediction.</p>
        </list-item>
        <list-item>
          <p><underline>KG-DDI</underline> (<xref rid="btab207-B17" ref-type="bibr">Karim <italic toggle="yes">et al.</italic>, 2019</xref>) first extracts KG embeddings for drugs in the KG, then adopts a Conv-LSTM model using the embeddings for drug interaction prediction.</p>
        </list-item>
        <list-item>
          <p><underline>GraIL</underline> (<xref rid="btab207-B39" ref-type="bibr">Teru and Hamilton, 2020</xref>) is for inductive relation prediction on KGs, which uses local subgraph.</p>
        </list-item>
        <list-item>
          <p><underline>KGNN</underline> (<xref rid="btab207-B22" ref-type="bibr">Lin <italic toggle="yes">et al.</italic>, 2020</xref>) samples and aggregates neighborhoods for each node from their local receptives via GNN and with external KG, which achieves the state-of-the-art result on binary DDI prediction problem.</p>
        </list-item>
      </list>
      <p><bold>Metrics</bold> The task on the DrugBank dataset is a multi-class classification, thus we consider the following metrics:
</p>
      <list list-type="bullet">
        <list-item>
          <p><underline>F1 Score</underline>: average F1 score over <italic toggle="yes">different classes</italic> as <inline-formula id="IE64"><mml:math id="IM64" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mn>1</mml:mn><mml:mo> </mml:mo><mml:mtext>Score</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <italic toggle="yes">N</italic> is the # of classes and <italic toggle="yes">P<sub>k</sub></italic>, <italic toggle="yes">R<sub>k</sub></italic> is the precision and recall for <italic toggle="yes">k</italic>th class. Since it gives equal weights for each classes, they are <italic toggle="yes">more sensitive</italic> to the results for classes where samples are fewer.</p>
        </list-item>
        <list-item>
          <p><underline>Accuracy</underline>: Accuracy over all samples <inline-formula id="IE65"><mml:math id="IM65" display="inline" overflow="scroll"><mml:mrow><mml:mtext>Accuracy</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>∩</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, <italic toggle="yes">Y<sub>k</sub></italic> is the predicted labels at <italic toggle="yes">k</italic> and <inline-formula id="IE66"><mml:math id="IM66" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the ground-truth labels.</p>
        </list-item>
        <list-item>
          <p><underline>Cohen’s Kappa</underline> (<xref rid="btab207-B7" ref-type="bibr">Cohen, 1960</xref>) measures the inter-annotator agreement as <inline-formula id="IE67"><mml:math id="IM67" display="inline" overflow="scroll"><mml:mrow><mml:mo>κ</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mi>o</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mi>e</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mi>e</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <italic toggle="yes">p<sub>o</sub></italic> is the observed agreement (identical to accuracy), <italic toggle="yes">p<sub>e</sub></italic> is the probability of randomly seeing each class.</p>
        </list-item>
      </list>
      <p>The task on the TWOSIDES dataset is a multi-label prediction. We follow (<xref rid="btab207-B55" ref-type="bibr">Zitnik <italic toggle="yes">et al.</italic>, 2018b</xref>) and consider the following measure. For each side effect type, we calculate the performance individually and use the average performance over all side effects as the final result.
</p>
      <list list-type="bullet">
        <list-item>
          <p><underline>ROC-AUC</underline> is the average area under the receiver operating characteristics curve as <inline-formula id="IE68"><mml:math id="IM68" display="inline" overflow="scroll"><mml:mrow><mml:mtext>ROC</mml:mtext><mml:mo>-</mml:mo><mml:mtext>AUC</mml:mtext><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mtext>TP</mml:mtext></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>Δ</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mtext>FP</mml:mtext></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <italic toggle="yes">k</italic> is <italic toggle="yes">k</italic>th true-positive and false-positive operating point <inline-formula id="IE69"><mml:math id="IM69" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mtext>TP</mml:mtext></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mtext>FP</mml:mtext></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p><underline>PR-AUC</underline> is the average area under precision-recall curve <inline-formula id="IE70"><mml:math id="IM70" display="inline" overflow="scroll"><mml:mrow><mml:mtext>PR</mml:mtext><mml:mo>-</mml:mo><mml:mtext>AUC</mml:mtext><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mtext>Prec</mml:mtext></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>Δ</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mtext>Rec</mml:mtext></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> where <italic toggle="yes">k</italic> is <italic toggle="yes">k</italic>th precision/recall operating point <inline-formula id="IE71"><mml:math id="IM71" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mtext>Prec</mml:mtext></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mtext>Rec</mml:mtext></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
        </list-item>
        <list-item>
          <p><underline>AP@50</underline> is the average precision at 50, where <inline-formula id="IE72"><mml:math id="IM72" display="inline" overflow="scroll"><mml:mrow><mml:mtext>AP</mml:mtext><mml:mo>@</mml:mo><mml:mi mathvariant="normal">k</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="normal">Y</mml:mi></mml:mrow></mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:msub><mml:mo>∩</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mtext>Y</mml:mtext><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="normal">Y</mml:mi></mml:mrow></mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, <italic toggle="yes">Y<sub>k</sub></italic> is the predicted labels at <italic toggle="yes">k</italic> and <inline-formula id="IE73"><mml:math id="IM73" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the ground-truth labels.</p>
        </list-item>
      </list>
      <p><bold>Evaluation strategy.</bold> For both datasets, we split it into 7:1:2 as train, development and test set. For the <underline>DrugBank</underline> dataset, since the label distribution is highly imbalanced, we ensure train/dev/test set contain samples from all classes. For the <underline>TWOSIDES</underline> dataset, we use the same method in (<xref rid="btab207-B55" ref-type="bibr">Zitnik <italic toggle="yes">et al.</italic>, 2018b</xref>) to generate negative counterparts for each positive edge by sampling the complement set of positive examples. For every experiment, we conduct five independent runs and select the best performing model based on the loss value on the validation set.</p>
    </sec>
    <sec>
      <title>4.2 SumGNN achieves superior predictive performance</title>
      <p>We report the performance of our model and all baselines in <xref rid="btab207-T1" ref-type="table">Table 1</xref>. From the result, we find that SumGNN achieves the best performance in DDI prediction on two datasets, accurately predicting the correct DDI pharmacological effect consistently. Particularly, SumGNN has 27.19%, 5.47%, 4.65% absolute increase over the best baseline without KG on three metrics respectively on DrugBank dataset and 2.84%, 2.45%, 4.50% increase on TWOSIDES dataset. Also, SumGNN achieves 5.54%, 1.77%, 1.08% and 1.97%, 2.25%. 2.54% absolute increase over the state-of-the-art baselines with KG on two datasets. These results clearly verifies the efficacy of our method.</p>
      <table-wrap position="float" id="btab207-T1">
        <label>Table 1.</label>
        <caption>
          <p>SumGNN achieves the best predictive performance compared to state-of-the-art baselines in DDI prediction</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Dataset</th>
              <th colspan="3" rowspan="1">Dataset 1: DrugBank<hr/></th>
              <th colspan="3" rowspan="1">Dataset 2: TWOSIDES<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Classification task</th>
              <th colspan="3" rowspan="1">Multi-class<hr/></th>
              <th colspan="3" rowspan="1">Multi-label<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Methods</th>
              <th rowspan="1" colspan="1">F1 Score</th>
              <th rowspan="1" colspan="1">Accuracy</th>
              <th rowspan="1" colspan="1">Cohen’s Kappa</th>
              <th rowspan="1" colspan="1">ROC-AUC</th>
              <th rowspan="1" colspan="1">PR-AUC</th>
              <th rowspan="1" colspan="1">AP@50</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">MLP (<xref rid="btab207-B26" ref-type="bibr">Rogers and Hahn, 2010</xref>)</td>
              <td rowspan="1" colspan="1">61.10 ± 0.38</td>
              <td rowspan="1" colspan="1">82.14 ± 0.33</td>
              <td rowspan="1" colspan="1">80.50 ± 0.18</td>
              <td rowspan="1" colspan="1">82.60 ± 0.26</td>
              <td rowspan="1" colspan="1">81.23 ± 0.14</td>
              <td rowspan="1" colspan="1">73.45 ± 0.28</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Deepwalk (<xref rid="btab207-B25" ref-type="bibr">Perozzi <italic toggle="yes">et al.</italic>, 2014</xref>)</td>
              <td rowspan="1" colspan="1">24.77 ± 0.40</td>
              <td rowspan="1" colspan="1">68.50 ± 0.38</td>
              <td rowspan="1" colspan="1">58.44 ± 0.23</td>
              <td rowspan="1" colspan="1">88.27 ± 0.09</td>
              <td rowspan="1" colspan="1">85.42 ± 0.14</td>
              <td rowspan="1" colspan="1">81.09 ± 0.16</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LINE (<xref rid="btab207-B37" ref-type="bibr">Tang <italic toggle="yes">et al.</italic>, 2015</xref>)</td>
              <td rowspan="1" colspan="1">30.26 ± 0.45</td>
              <td rowspan="1" colspan="1">76.57 ± 0.49</td>
              <td rowspan="1" colspan="1">70.91 ± 0.53</td>
              <td rowspan="1" colspan="1">91.20 ± 0.34</td>
              <td rowspan="1" colspan="1">90.02 ± 0.28</td>
              <td rowspan="1" colspan="1">84.07 ± 0.19</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Node2Vec (<xref rid="btab207-B9" ref-type="bibr">Grover and Leskovec, 2016</xref>)</td>
              <td rowspan="1" colspan="1">24.92 ± 0.32</td>
              <td rowspan="1" colspan="1">71.09 ± 0.40</td>
              <td rowspan="1" colspan="1">63.79 ± 0.37</td>
              <td rowspan="1" colspan="1">90.66 ± 0.13</td>
              <td rowspan="1" colspan="1">88.87 ± 0.23</td>
              <td rowspan="1" colspan="1">83.00 ± 0.30</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Decagon (<xref rid="btab207-B55" ref-type="bibr">Zitnik <italic toggle="yes">et al</italic>., 2018b</xref>)</td>
              <td rowspan="1" colspan="1">57.35 ± 0.26</td>
              <td rowspan="1" colspan="1">87.19 ± 0.28</td>
              <td rowspan="1" colspan="1">86.07 ± 0.08</td>
              <td rowspan="1" colspan="1">91.72 ± 0.04</td>
              <td rowspan="1" colspan="1">90.60 ± 0.12</td>
              <td rowspan="1" colspan="1">82.06 ± 0.45</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GAT (<xref rid="btab207-B42" ref-type="bibr">Veličković <italic toggle="yes">et al.</italic>, 2018</xref>)</td>
              <td rowspan="1" colspan="1">33.49 ± 0.36</td>
              <td rowspan="1" colspan="1">77.18 ± 0.15</td>
              <td rowspan="1" colspan="1">74.20 ± 0.23</td>
              <td rowspan="1" colspan="1">91.18 ± 0.14</td>
              <td rowspan="1" colspan="1">89.86 ± 0.05</td>
              <td rowspan="1" colspan="1">82.80 ± 0.17</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SkipGNN (Huang <italic toggle="yes">et al.</italic>, 2020b)</td>
              <td rowspan="1" colspan="1">59.66 ± 0.26</td>
              <td rowspan="1" colspan="1">85.83 ± 0.18</td>
              <td rowspan="1" colspan="1">84.20 ± 0.16</td>
              <td rowspan="1" colspan="1">92.04 ± 0.08</td>
              <td rowspan="1" colspan="1">90.90 ± 0.10</td>
              <td rowspan="1" colspan="1">84.25 ± 0.25</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PRD (<xref rid="btab207-B47" ref-type="bibr">Wang, 2017</xref>)</td>
              <td rowspan="1" colspan="1">40.73 ± 0.44</td>
              <td rowspan="1" colspan="1">81.52 ± 0.34</td>
              <td rowspan="1" colspan="1">78.80 ± 0.36</td>
              <td rowspan="1" colspan="1">88.75 ± 0.23</td>
              <td rowspan="1" colspan="1">85.26 ± 0.33</td>
              <td rowspan="1" colspan="1">79.86 ± 0.26</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">KG-DDI (<xref rid="btab207-B17" ref-type="bibr">Karim <italic toggle="yes">et al.</italic>, 2019</xref>)</td>
              <td rowspan="1" colspan="1">36.39 ± 0.32</td>
              <td rowspan="1" colspan="1">82.48 ± 0.12</td>
              <td rowspan="1" colspan="1">78.89 ± 0.27</td>
              <td rowspan="1" colspan="1">90.75 ± 0.07</td>
              <td rowspan="1" colspan="1">88.16 ± 0.12</td>
              <td rowspan="1" colspan="1">83.48 ± 0.05</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GraIL (<xref rid="btab207-B39" ref-type="bibr">Teru and Hamilton, 2020</xref>)</td>
              <td rowspan="1" colspan="1">81.31 ± 0.30</td>
              <td rowspan="1" colspan="1">89.89 ± 0.24</td>
              <td rowspan="1" colspan="1">88.07 ± 0.20</td>
              <td rowspan="1" colspan="1">92.89 ± 0.09</td>
              <td rowspan="1" colspan="1">91.10 ± 0.19</td>
              <td rowspan="1" colspan="1">86.21 ± 0.05</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">KGNN (<xref rid="btab207-B22" ref-type="bibr">Lin <italic toggle="yes">et al.</italic>, 2020</xref>)</td>
              <td rowspan="1" colspan="1">73.99 ± 0.11</td>
              <td rowspan="1" colspan="1">90.89 ± 0.20</td>
              <td rowspan="1" colspan="1">89.64 ± 0.24</td>
              <td rowspan="1" colspan="1">92.84 ± 0.07</td>
              <td rowspan="1" colspan="1">90.78 ± 0.20</td>
              <td rowspan="1" colspan="1">86.05 ± 0.12</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SumGNN (Ours)</td>
              <td rowspan="1" colspan="1">
                <bold>86.85 ± 0.44</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>92.66 ± 0.14</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>90.72 ± 0.13</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>94.86 ± 0.21</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>93.35 ± 0.14</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>88.75 ± 0.22</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SumGNN-KG</td>
              <td rowspan="1" colspan="1">78.35 ± 0.51</td>
              <td rowspan="1" colspan="1">89.05 ± 0.36</td>
              <td rowspan="1" colspan="1">87.28 ± 0.08</td>
              <td rowspan="1" colspan="1">92.62 ± 0.13</td>
              <td rowspan="1" colspan="1">90.80 ± 0.40</td>
              <td rowspan="1" colspan="1">85.75 ± 0.10</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SumGNN-Sum (w/o Summarization)</td>
              <td rowspan="1" colspan="1">83.20 ± 0.34</td>
              <td rowspan="1" colspan="1">90.83 ± 0.19</td>
              <td rowspan="1" colspan="1">90.14 ± 0.10</td>
              <td rowspan="1" colspan="1">94.09 ± 0.16</td>
              <td rowspan="1" colspan="1">92.55 ± 0.24</td>
              <td rowspan="1" colspan="1">87.65 ± 0.24</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SumGNN-SF (w/o Subgraph Features)</td>
              <td rowspan="1" colspan="1">84.47 ± 0.22</td>
              <td rowspan="1" colspan="1">91.88 ± 0.21</td>
              <td rowspan="1" colspan="1">90.26 ± 0.19</td>
              <td rowspan="1" colspan="1">93.94 ± 0.11</td>
              <td rowspan="1" colspan="1">92.45 ± 0.22</td>
              <td rowspan="1" colspan="1">87.69 ± 0.08</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SumGNN-CF (w/o Chemical Features)</td>
              <td rowspan="1" colspan="1">83.57 ± 0.36</td>
              <td rowspan="1" colspan="1">91.31 ± 0.17</td>
              <td rowspan="1" colspan="1">90.07 ± 0.11</td>
              <td rowspan="1" colspan="1">94.35 ± 0.11</td>
              <td rowspan="1" colspan="1">92.86 ± 0.20</td>
              <td rowspan="1" colspan="1">88.10 ± 0.07</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SumGNN-LIA (w/o Layer Independent Attention)</td>
              <td rowspan="1" colspan="1">86.54 ± 0.22</td>
              <td rowspan="1" colspan="1">92.44 ± 0.30</td>
              <td rowspan="1" colspan="1">90.34 ± 0.10</td>
              <td rowspan="1" colspan="1">93.92 ± 0.31</td>
              <td rowspan="1" colspan="1">92.33 ± 0.19</td>
              <td rowspan="1" colspan="1">86.15 ± 0.13</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic toggle="yes">Note</italic>: Average and standard deviation of five runs are reported. For these metrics, higher values always indicate better performance.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>4.3 SumGNN excels at low-data imbalanced relations predictions</title>
      <p>We observe that the improvement of SumGNN is much <italic toggle="yes">more significant</italic> on DrugBank than TWOSIDES dataset. We take a closer look at this problem and find that the major difference of these two datasets is that the data distribution of DrugBank is <italic toggle="yes">more imbalanced—</italic>more than 30% of the relation types occurs less than 50 times in the training set while more than 10% of the relation types occurs more than 1000 times, as shown in <xref rid="btab207-F2" ref-type="fig">Figure 2(a)</xref>.</p>
      <fig position="float" id="btab207-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>The dataset statistics and the average F1 score for different relations with various number of training samples on DrugBank dataset. Here, in (<bold>a</bold>), the <italic toggle="yes">x</italic>-axis indicates the index of classes (there are 86 classes in total) and the <italic toggle="yes">y</italic>-axis indicates the number of training samples in the corresponding class. In (<bold>b</bold>), the <italic toggle="yes">x</italic>-axis indicate the range for number of training samples in this group as well as the proportion of classes in this group to all classes and the <italic toggle="yes">y</italic>-axis stands for the average prediction F1 score for the classes in the group</p>
        </caption>
        <graphic xlink:href="btab207f2" position="float"/>
      </fig>
      <p>To examine the model’s prediction performance on the size of training data associated with each relation, we first split the relations types into 5 groups with various amounts of training data and then plot the average F1 score of these bins in <xref rid="btab207-F2" ref-type="fig">Figure 2</xref>. By comparing the performance of SumGNN against the strongest baseline on Accuracy (KGNN) and the variant of no KG (Decagon), we find that SumGNN can effectively boost the performance when the samples are extremely scarce. When the size of the training samples of the relation is less than 10, both decagon and KGNN cannot give any correct predictions, while our model can still achieve 57.14% F1 score (i.e. choose the correct relation out of the 86 relations). One potential reason is that SumGNN feeds different subgraphs in every GNN propagation, which forms a much-needed inductive bias over unseen subgraphs, such as the ones in the low-data relations. This is in sharp contrast to previous approaches such as KGNN. It also justifies that SumGNN ’s knowledge summarization via subgraphs is more effective to harness the external knowledge. In addition, we find SumGNN also brings at least 38.19% improvement on F1 score for relations occuring less than 50 times. This is an important finding since a high overall accuracy does not mean good performance across all relations. The low-data relations are the hardest to predict correctly and we show that SumGNN can be used for these low-data relations whereas other previous models cannot.</p>
    </sec>
    <sec>
      <title>4.4 External knowledge improves prediction</title>
      <p>We find that the first seven method cannot obtain satisfactory result compared to SumGNN. MLP, Deepwalk, LINE and Node2vec only use shallow embedding layers to learn the features for drugs while do not use graph neural networks for modeling the drug interactions, which have limited expression power. Moreover, although GAT considers the attention on different edges, it fails to consider the multi-relation information, as its performance is also not good. By comparing the latter four methods with the former five methods, we find the use of KG significantly improves DDI performance, highlighting the necessity of combining external knowledge usage with graph neural networks, as it can provide complementary information for DDI task.</p>
    </sec>
    <sec>
      <title>4.5 Knowledge summarization module is the best to capture external knowledge</title>
      <p>Comparing SumGNN with KGNN and KG-DDI, we show that our model can consistently outperform them on both datasets (more than 4% on average for DrugBank and 2% on average for TWOSIDES), which indicates that simply adopting KG embeddings as well as neighborhood sampling are <italic toggle="yes">insufficient to fully harness the KG information</italic> for DDI prediction. SumGNN provides the best approach to leverage the external KG and also corroborates with our motivation that the use of subgraph reduces noise and irrelevant information. Although GraIL also extracts subgraphs for downstream tasks, it does not use any knowledge summarization techniques, which potentially further eliminating the irrelevant information in the local subgraph. In addition, GraIL merely considers the position information while neglecting the multi-channel features during information propagation.</p>
    </sec>
    <sec>
      <title>4.6 Ablation study</title>
      <p>To study the usage of KG, we remove the knowledge graph (SumGNN-KG) and perform prediction on the DDI graph. We see SumGNN has 8.5% absolute increase in Macro F1 on DrugBank, highlighting the usefulness of KG. To evaluate the knowledge summarization module, we remove the summarization component (SumGNN-Sum) and use the raw local subgraph to predict the outcome. We see SumGNN has 3.65% increase for DrugBank and 2.24% increase for TWOSIDES on Macro F1, suggesting that the summarization further condenses the relevant information and elevate the performance.</p>
      <p>To study the effect of multi-channel neural encoding, we compare the result of SumGNN with several variants that remove specific channel information (i.e. subgraph features and chemical features), and we find that these channel information all contribute to the overall performance. Particularly, after removing the subgraph embedding (SumGNN-SF), the Macro F1 drops by 2.38% on DrugBank and 0.92% on TWOSIDES respectively. When removing chemical fingerprint (SumGNN-CF), the performance also degrades 3.28% on Macro F1 for DrugBank, corroborating with the indispensable roles of each of the channel. We also see that by replacing the layer independent attention (SumGNN-LIA) with the previous layer-dependent ones, the performance drops. This suggests that our attention mechanism not only provides interpretability but also increases predictive performance.</p>
    </sec>
    <sec>
      <title>4.7 Case study</title>
      <p>The usefulness of this model lies in twofolds. First, given the high predictive performance, it can identify novel drug–drug interactions that are flagged high by the model while not in the dataset. Second, using the external knowledge summarization module, we are able to discover signal edges, which provide biological pathways to hint at the potential mechanism of DDIs. We provide a case study of a novel drug pair predicted by the model, Paroxetine and Hydroflumethiazide. Paroxetine is an antidepressant, and Hydroflumethiazide is used to treat hypertension and edema. SumGNN assigns highest probability for the DDI type ‘increase of the central nervous system depressant activities’. We then visualize the generated pathway from SumGNN ’s summarization scheme in <xref rid="btab207-F3" ref-type="fig">Figure 3</xref>. We see that the model significantly reduces irrelevant nodes and edges in the subgraph of the KG and focuses on a sparse set of nodes to make prediction. We examine the nodes that have high signals connection to the target pairs and find literature evidence support. Particularly, the model assigns high weights to two side effects nodes, orthostatic hypotension and aplastic anemia. Orthostatic hypotension refers to a sudden drop in blood pressure when standing up, and aplastic anemia is a condition in which the body stops producing enough new blood cells. Notably, orthostatic hypotension is closely related to multiple system atrophy, a central nervous system problem (<xref rid="btab207-B16" ref-type="bibr">Jones <italic toggle="yes">et al.</italic>, 2015</xref>). As both drugs incur risk in the side effects, the complication on the central nervous systems could be aggravated when these drugs are taken together, supporting our model prediction. This case study illustrates how to use SumGNN for potential DDI prediction.</p>
      <fig position="float" id="btab207-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>SumGNN generates a short reasoning path to provide clues for understanding drug interactions. The shade of color indicates the strength of attention weight. Low-weight edges in the extracted subgraph are pruned by SumGNN and SumGNN focus on a sparse set of signal edges and nodes</p>
        </caption>
        <graphic xlink:href="btab207f3" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>4.8 Parameter studies</title>
      <p>We study the effect of key parameters. When evaluating one parameter, we fix other parameters to their default values.</p>
      <p><inline-formula id="IE74"><mml:math id="IM74" display="inline" overflow="scroll"><mml:mo>•</mml:mo></mml:math></inline-formula>  <bold>Effect of the hop of the subgraph <italic toggle="yes">k</italic></bold>: <xref rid="btab207-F4" ref-type="fig">Figure 4</xref> shows the result of SumGNN with varying <italic toggle="yes">k</italic>. From the result, we find that for DrugBank dataset, the performance first increase when <italic toggle="yes">k</italic> is small. However, when <italic toggle="yes">k</italic> increases from 3 to 4, we observe slight performance drops on F1 score. These indicate that the larger subgraph can bring more useful information while when <italic toggle="yes">k</italic> is too large, it may also bring some noise and hurt the performance. For TWOSIDES dataset, we find the result is more stable with different <italic toggle="yes">k</italic>, indicating even 1-hop subgraph provides adequate information for DDI task.</p>
      <fig position="float" id="btab207-F4">
        <label>Fig. 4.</label>
        <caption>
          <p>Model performance given different parameter <italic toggle="yes">k</italic></p>
        </caption>
        <graphic xlink:href="btab207f4" position="float"/>
      </fig>
      <fig position="float" id="btab207-F5">
        <label>Fig. 5</label>
        <caption>
          <p>Model performance given different parameter <italic toggle="yes">d</italic></p>
        </caption>
        <graphic xlink:href="btab207f5" position="float"/>
      </fig>
      <p>Moreover, to show how <bold>subgraph formulation drives efficiency</bold>, we compare the training time between SumGNN with varying subgraph size and SumGNN with the entire KG to propagate (See <xref rid="btab207-T2" ref-type="table">Table 2</xref>). We find SumGNN saves 80% of training time via subgraph anchoring, which demonstrates the efficiency of our approach.</p>
      <table-wrap position="float" id="btab207-T2">
        <label>Table 2.</label>
        <caption>
          <p>The running time for one epoch of SumGNN for two datasets</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="2" colspan="1">Dataset</th>
              <th colspan="4" rowspan="1">Size of Hop <italic toggle="yes">k</italic><hr/></th>
              <th rowspan="2" colspan="1">w/o subgraph</th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">1</th>
              <th rowspan="1" colspan="1">2</th>
              <th rowspan="1" colspan="1">3</th>
              <th rowspan="1" colspan="1">4</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">DrugBank</td>
              <td rowspan="1" colspan="1">132 s</td>
              <td rowspan="1" colspan="1">141 s</td>
              <td rowspan="1" colspan="1">178 s</td>
              <td rowspan="1" colspan="1">205 s</td>
              <td rowspan="1" colspan="1">1279 s</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">TWOSIDES</td>
              <td rowspan="1" colspan="1">62 s</td>
              <td rowspan="1" colspan="1">75 s</td>
              <td rowspan="1" colspan="1">91 s</td>
              <td rowspan="1" colspan="1">102 s</td>
              <td rowspan="1" colspan="1">471 s</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <p><italic toggle="yes">Note</italic>: Note that w/o Subgraph is a variant that directly aggregates the information for all neighbors on KG.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p><inline-formula id="IE75"><mml:math id="IM75" display="inline" overflow="scroll"><mml:mo>•</mml:mo></mml:math></inline-formula>  <bold>Effect of the dimension of embeddings <italic toggle="yes">d</italic></bold>: <xref rid="btab207-F5" ref-type="fig">Figure 5</xref> exhibits the influence of embedding dimension <italic toggle="yes">d</italic>. The result indicates that when <italic toggle="yes">d</italic> is small, increasing <italic toggle="yes">d</italic> can boost the performance. But when <italic toggle="yes">d</italic> becomes large, the gain is marginal.</p>
      <fig position="float" id="btab207-F6">
        <label>Fig. 6.</label>
        <caption>
          <p>Model performance given different parameter <italic toggle="yes">γ</italic></p>
        </caption>
        <graphic xlink:href="btab207f6" position="float"/>
      </fig>
      <p><inline-formula id="IE76"><mml:math id="IM76" display="inline" overflow="scroll"><mml:mo>•</mml:mo></mml:math></inline-formula>  <bold>Effect of the threshold for summarization <italic toggle="yes">γ</italic></bold>: <xref rid="btab207-F6" ref-type="fig">Figure 6</xref> shows the result of SumGNN with different threshold <italic toggle="yes">γ</italic>, which demonstrates that when <italic toggle="yes">γ</italic> is small, the performance is rather stable as filtering edges with low score has little effect on the final prediction. This also means SumGNN is able to achieve similar predictive performance while removing many irrelevant edges. However, when <italic toggle="yes">γ</italic> is large (<inline-formula id="IE77"><mml:math id="IM77" display="inline" overflow="scroll"><mml:mrow><mml:mo>γ</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0.4</mml:mn></mml:mrow></mml:math></inline-formula> for DrugBank and <inline-formula id="IE78"><mml:math id="IM78" display="inline" overflow="scroll"><mml:mrow><mml:mo>γ</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:math></inline-formula> for TWOSIDES), it is clear that the performance drops more. In such cases, the summarized graph is more sparse and we might filter out potential useful edges. To sum up, there is a trade-off between the explainability and performance.</p>
    </sec>
  </sec>
  <sec>
    <title>5 Conclusion</title>
    <p>In this article, we propose a new method SumGNN: <italic toggle="yes">knowledge summarization graph neural network</italic> for multi-typed DDI predictions, which is mainly enabled by a local subgraph module that can efficiently anchor on relevant subgraphs from a KG, a self-attention based subgraph summarization scheme that can generate reasoning path within the subgraph, and a multi-channel knowledge and data integration module that utilizes massive external biomedical knowledge for significantly improved multi-typed DDI predictions. Experiments on real-world datasets demonstrated the strong performance of SumGNN.</p>
    <p>In addition, computational approaches depend heavily on the training data. If the number of training data associated with one specific drug interaction type is low, it is difficult to predict accurately. In contrast to other works, SumGNN is able to generate good performance in low-resource settings. SumGNN is also a general framework and can be adapted to predict any other interactions such as drug-disease interaction. The ability to low-resource learning could also mean to excel at finding drugs for rare diseases.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was in part supported by the National Science Foundation award SCH-2014438, IIS-1418511, CCF-1533768, IIS-1838042, the National Institute of Health award NIH R01 1R01NS107291-01 and R56HL138415.</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="data-availability">
    <title>Data Availability</title>
    <p>All data and code for SumGNN are available at <ext-link xlink:href="https://github.com/yueyu1030/SumGNN" ext-link-type="uri">https://github.com/yueyu1030/SumGNN</ext-link>.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btab207_supplementary_data</label>
      <media xlink:href="btab207_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btab207-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alsentzer</surname><given-names>E.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) 
<article-title>Subgraph neural networks</article-title>. In: <source>NeurIPS, Virtual.</source></mixed-citation>
    </ref>
    <ref id="btab207-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bordes</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2013</year>) 
<article-title>Translating embeddings for modeling multi-relational data</article-title>. In <source>NeurIPS</source>, Lake Tahoe, USA.</mixed-citation>
    </ref>
    <ref id="btab207-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burkhardt</surname><given-names>H.A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>Predicting adverse drug–drug interactions with neural embedding of semantic predications</article-title>. <source>bioRxiv</source>, <fpage>752022</fpage>.</mixed-citation>
    </ref>
    <ref id="btab207-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cai</surname><given-names>D.</given-names></string-name>, <string-name><surname>Lam</surname><given-names>W.</given-names></string-name></person-group> (<year>2020</year>) 
<article-title>Graph transformer for graph-to-sequence learning</article-title>. <source>AAAI</source>, New York, USA, <volume>34</volume>, <fpage>7464</fpage>–<lpage>7471</lpage>.</mixed-citation>
    </ref>
    <ref id="btab207-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Celebi</surname><given-names>R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>Evaluation of knowledge graph embedding approaches for drug–drug interaction prediction in realistic settings</article-title>. <source>BMC Bioinformatics</source>, <volume>20</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">30606105</pub-id></mixed-citation>
    </ref>
    <ref id="btab207-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chiang</surname><given-names>W.-L.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>Cluster-gcn: an efficient algorithm for training deep and large graph convolutional networks</article-title>. In: <source>KDD, Anchorage, USA</source>.</mixed-citation>
    </ref>
    <ref id="btab207-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cohen</surname><given-names>J.</given-names></string-name></person-group> (<year>1960</year>) 
<article-title>A coefficient of agreement for nominal scales</article-title>. <source>Educ. Psychol. Measur</source>., <volume>20</volume>, <fpage>37</fpage>–<lpage>46</lpage>.</mixed-citation>
    </ref>
    <ref id="btab207-B8">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Dai</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) 
<article-title>Wasserstein adversarial autoencoders for knowledge graph embedding based drug–drug interaction prediction</article-title>. <source>arXiv</source>, <fpage>07341</fpage>.</mixed-citation>
    </ref>
    <ref id="btab207-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grover</surname><given-names>A.</given-names></string-name>, <string-name><surname>Leskovec</surname><given-names>J.</given-names></string-name></person-group> (<year>2016</year>) 
<article-title>node2vec: scalable feature learning for networks</article-title>. In: <source>KDD</source>, San Francisco, USA, pp. <fpage>855</fpage>–<lpage>864</lpage>.<pub-id pub-id-type="pmid">27853626</pub-id></mixed-citation>
    </ref>
    <ref id="btab207-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gysi</surname><given-names>D.M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) 
<article-title>Network medicine framework for identifying drug repurposing opportunities for covid-19</article-title>. <source>arXiv</source>, <fpage>07229</fpage>.</mixed-citation>
    </ref>
    <ref id="btab207-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Himmelstein</surname><given-names>D.S.</given-names></string-name>, <string-name><surname>Baranzini</surname><given-names>S.E.</given-names></string-name></person-group> (<year>2015</year>) 
<article-title>Heterogeneous network edge prediction: a data integration approach to prioritize disease-associated genes</article-title>. <source>PLoS Comput. Biol</source>., 11, e1004259.</mixed-citation>
    </ref>
    <ref id="btab207-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname><given-names>K.</given-names></string-name>, <string-name><surname>Zitnik</surname><given-names>M.</given-names></string-name></person-group> (<year>2020</year>) 
<article-title>Graph meta learning via local subgraphs</article-title>. In: <source>NeurIPS</source>, Virtual.</mixed-citation>
    </ref>
    <ref id="btab207-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname><given-names>K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020a</year>) 
<article-title>Caster: predicting drug interactions with chemical substructure representation</article-title>. <source>AAAI</source>, New York, USA, <volume>34</volume>, <fpage>702</fpage>–<lpage>709</lpage>.</mixed-citation>
    </ref>
    <ref id="btab207-B14">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Huang</surname><given-names>K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020b</year>) SkipGNN: predicting molecular interactions with skip-graph networks. <italic toggle="yes">Scientific Rep.,</italic> <bold>10</bold>(1), 1–16.</mixed-citation>
    </ref>
    <ref id="btab207-B15">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ioannidis</surname><given-names>V.N.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) DRKG – Drug Repurposing Knowledge Graph for Covid-19. <ext-link xlink:href="https://github.com/gnn4dr/DRKG/" ext-link-type="uri">https://github.com/gnn4dr/DRKG/</ext-link>.</mixed-citation>
    </ref>
    <ref id="btab207-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jones</surname><given-names>P.K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) 
<article-title>Orthostatic hypotension: managing a difficult problem</article-title>. <source>Exp. Rev. Cardiovasc. Ther</source>., <volume>13</volume>, <fpage>1263</fpage>–<lpage>1276</lpage>.</mixed-citation>
    </ref>
    <ref id="btab207-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Karim</surname><given-names>M.R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>Drug–drug interaction prediction based on knowledge graph embeddings and convolutional-lstm network</article-title>. In: <source>ACM-BCB</source>, Niagara Falls, USA, pp. <fpage>113</fpage>–<lpage>123</lpage>.</mixed-citation>
    </ref>
    <ref id="btab207-B18">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kingma</surname><given-names>D.P.</given-names></string-name>, <string-name><surname>Ba</surname><given-names>J.</given-names></string-name></person-group> (<year>2014</year>) Adam: a method for stochastic optimization. <italic toggle="yes">arXiv, 1412.6980.</italic></mixed-citation>
    </ref>
    <ref id="btab207-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kipf</surname><given-names>T.N.</given-names></string-name>, <string-name><surname>Welling</surname><given-names>M.</given-names></string-name></person-group> (<year>2017</year>) 
<article-title>Semi-supervised classification with graph convolutional networks</article-title>. In: <source>ICLR</source>, Toulon, France.</mixed-citation>
    </ref>
    <ref id="btab207-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) 
<article-title>Large-scale exploration and analysis of drug combinations</article-title>. <source>Bioinformatics</source>, <volume>31</volume>, <fpage>2007</fpage>–<lpage>2016</lpage>.<pub-id pub-id-type="pmid">25667546</pub-id></mixed-citation>
    </ref>
    <ref id="btab207-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liang</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) 
<article-title>Bond: bert-assisted open-domain named entity recognition with distant supervision</article-title>. In: <source>KDD</source>, Virtual, pp. <fpage>1054</fpage>–<lpage>1064</lpage>.</mixed-citation>
    </ref>
    <ref id="btab207-B22">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>X.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <part-title>KGNN: knowledge graph neural network for drug–drug interaction prediction</part-title>. In: <source>IJCAI</source>, Virtual.</mixed-citation>
    </ref>
    <ref id="btab207-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mikolov</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2013</year>) 
<article-title>Distributed representations of words and phrases and their compositionality</article-title>. In: <source>NeurIPS</source>, Lake Tahoe, USA.</mixed-citation>
    </ref>
    <ref id="btab207-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Percha</surname><given-names>B.</given-names></string-name>, <string-name><surname>Altman</surname><given-names>R.B.</given-names></string-name></person-group> (<year>2013</year>) 
<article-title>Informatics confronts drug–drug interactions</article-title>. <source>Trends Pharmacol. Sci</source>., <volume>34</volume>, <fpage>178</fpage>–<lpage>184</lpage>.<pub-id pub-id-type="pmid">23414686</pub-id></mixed-citation>
    </ref>
    <ref id="btab207-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Perozzi</surname><given-names>B.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) 
<article-title>Deepwalk: online learning of social representations</article-title>. In: <source>KDD</source>, New York, USA.</mixed-citation>
    </ref>
    <ref id="btab207-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rogers</surname><given-names>D.</given-names></string-name>, <string-name><surname>Hahn</surname><given-names>M.</given-names></string-name></person-group> (<year>2010</year>) 
<article-title>Extended-connectivity fingerprints</article-title>. <source>J. Chem. Inf. Model</source>., <volume>50</volume>, <fpage>742</fpage>–<lpage>754</lpage>.<pub-id pub-id-type="pmid">20426451</pub-id></mixed-citation>
    </ref>
    <ref id="btab207-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rotmensch</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) 
<article-title>Learning a health knowledge graph from electronic medical records</article-title>. <source>Sci. Rep</source>., <volume>7</volume>, <fpage>5994</fpage>.<pub-id pub-id-type="pmid">28729710</pub-id></mixed-citation>
    </ref>
    <ref id="btab207-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ryu</surname><given-names>J.Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Deep learning improves prediction of drug–drug and drug–food interactions</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>115</volume>, <fpage>E4304</fpage>–<lpage>4311</lpage>.<pub-id pub-id-type="pmid">29666228</pub-id></mixed-citation>
    </ref>
    <ref id="btab207-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schlichtkrull</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Modeling relational data with graph convolutional networks</article-title>. In: <source>ESWC</source>, Crete, Greece.</mixed-citation>
    </ref>
    <ref id="btab207-B30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Shang</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <part-title>Pre-training of graph augmented transformers for medication recommendation</part-title>. In: <source>IJCAI</source>, Macao, China.</mixed-citation>
    </ref>
    <ref id="btab207-B31">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Shaw</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <part-title>Self-attention with relative position representations</part-title>. In: <source>NAACL</source>, New Orleans, USA.</mixed-citation>
    </ref>
    <ref id="btab207-B32">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Shi</surname><given-names>H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>). <part-title>Predicting origin-destination flow via multi-perspective graph convolutional network</part-title>. In: <source>ICDE</source>, Virtual.</mixed-citation>
    </ref>
    <ref id="btab207-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Srinivasa</surname><given-names>R.S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) 
<article-title>Fast graph attention networks using effective resistance based graph sparsification</article-title>. <source>arXiv</source>, <fpage>08796</fpage>.</mixed-citation>
    </ref>
    <ref id="btab207-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Su</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) 
<article-title>Network embedding in biomedical data science</article-title>. <source>Brief. Bioinf</source>., <volume>21</volume>, <fpage>182</fpage>–<lpage>197</lpage>.</mixed-citation>
    </ref>
    <ref id="btab207-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sun</surname><given-names>Z.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Recurrent knowledge graph embedding for effective recommendation</article-title>. In: <source>RecSys</source>, Vancouver, Canada, pp. <fpage>297</fpage>–<lpage>305</lpage>.</mixed-citation>
    </ref>
    <ref id="btab207-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Takeda</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) 
<article-title>Predicting drug–drug interactions through drug structural similarities and interaction networks incorporating pharmacokinetics and pharmacodynamics knowledge</article-title>. <source>Journal of Cheminformatics</source>, <volume>9</volume>, <fpage>2688</fpage>–<lpage>2694</lpage>.</mixed-citation>
    </ref>
    <ref id="btab207-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) 
<article-title>Line: large-scale information network embedding</article-title>. In: <source>WWW</source>, Florence, Italy, pp. <fpage>1067</fpage>–<lpage>1077</lpage>.</mixed-citation>
    </ref>
    <ref id="btab207-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tatonetti</surname><given-names>N.P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2012</year>) 
<article-title>Data-driven prediction of drug effects and interactions</article-title>. <source>Sci. Transl. Med</source>., <volume>4</volume>, <fpage>125ra31</fpage>.</mixed-citation>
    </ref>
    <ref id="btab207-B39">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Teru</surname><given-names>K.</given-names></string-name>, <string-name><surname>Hamilton</surname><given-names>W.</given-names></string-name></person-group> (<year>2020</year>) <part-title>Inductive relation prediction on knowledge graphs</part-title>. In: <source>ICML</source>, Virtual.</mixed-citation>
    </ref>
    <ref id="btab207-B40">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Trouillon</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <part-title>Complex embeddings for simple link prediction</part-title>. In: <source>ICML</source>, New York, USA.</mixed-citation>
    </ref>
    <ref id="btab207-B41">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Tzeng</surname><given-names>R.-C.</given-names></string-name>, <string-name><surname>Wu</surname><given-names>S.-H.</given-names></string-name></person-group> (<year>2019</year>) <part-title>Distributed, egocentric representations of graphs for detecting critical structures</part-title>. In: <source>ICML</source>, Long Beach, USA.</mixed-citation>
    </ref>
    <ref id="btab207-B42">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Veličković</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <part-title>Graph attention networks</part-title>. In: <source>ICLR</source>, Vancouver, Canada.</mixed-citation>
    </ref>
    <ref id="btab207-B43">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Veličković</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <part-title>Deep graph infomax</part-title>. In: <source>ICLR</source>, New Orleans, USA.</mixed-citation>
    </ref>
    <ref id="btab207-B44">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>G.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <part-title>Label-free distant supervision for relation extraction via knowledge graph embedding</part-title>. In: <source>EMNLP</source>, Brussels, Belgium.</mixed-citation>
    </ref>
    <ref id="btab207-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019a</year>) 
<article-title>Knowledge-aware graph neural networks with label smoothness regularization for recommender systems</article-title>. In: <source>KDD</source>, Anchorage, USA, pp. <fpage>968</fpage>–<lpage>977</lpage>.</mixed-citation>
    </ref>
    <ref id="btab207-B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>X.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019b</year>) 
<article-title>KGAT: knowledge graph attention network for recommendation</article-title>. In: <source>KDD</source>, Anchorage, USA.</mixed-citation>
    </ref>
    <ref id="btab207-B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>M.</given-names></string-name></person-group> (<year>2017</year>) 
<article-title>Predicting rich drug–drug interactions via biomedical knowledge graphs and text jointly embedding</article-title>. <source>arXiv Preprint arXiv</source>, <volume>1712</volume>, <fpage>08875</fpage>.</mixed-citation>
    </ref>
    <ref id="btab207-B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wishart</surname><given-names>D.S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Drugbank 5.0: a major update to the drugbank database for 2018</article-title>. <source>Nucleic Acids Res</source>., <volume>46</volume>, <fpage>D1074</fpage>–<lpage>D1082</lpage>.<pub-id pub-id-type="pmid">29126136</pub-id></mixed-citation>
    </ref>
    <ref id="btab207-B49">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Xu</surname><given-names>K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <part-title>Representation learning on graphs with jumping knowledge networks</part-title>. In: <source>ICML</source>, Stockholm, Sweden.</mixed-citation>
    </ref>
    <ref id="btab207-B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020a</year>) 
<article-title>Generalized multi-relational graph convolution network</article-title>. <source>arXiv</source>, <fpage>07331</fpage>.</mixed-citation>
    </ref>
    <ref id="btab207-B51">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Yu</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020b</year>) 
<article-title>Semantic-aware spatio-temporal app usage representation via graph convolutional network</article-title>. In: <source>IMWUT</source>, Virtual.</mixed-citation>
    </ref>
    <ref id="btab207-B52">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <part-title>Graphsaint: graph sampling based inductive learning method</part-title>. In: <source>ICLR</source>, Virtual.</mixed-citation>
    </ref>
    <ref id="btab207-B53">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>M.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>Y.</given-names></string-name></person-group> (<year>2018</year>) <part-title>Link prediction based on graph neural networks</part-title>. In: <source>NeurIPS</source>, Montreal, Canada.</mixed-citation>
    </ref>
    <ref id="btab207-B54">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zitnik</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018a</year>) <italic toggle="yes">BioSNAP Datasets: Stanford Biomedical Network Dataset Collection</italic>. <ext-link xlink:href="http://snap.stanford.edu/biodata" ext-link-type="uri">http://snap.stanford.edu/biodata</ext-link>.</mixed-citation>
    </ref>
    <ref id="btab207-B55">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zitnik</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018b</year>) 
<article-title>Modeling polypharmacy side effects with graph convolutional networks</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>i457</fpage>–<lpage>i466</lpage>.<pub-id pub-id-type="pmid">29949996</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
