<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neurosci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neurosci</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neurosci.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroscience</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1662-4548</issn>
    <issn pub-type="epub">1662-453X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8249868</article-id>
    <article-id pub-id-type="doi">10.3389/fnins.2021.676220</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>PupilEXT: Flexible Open-Source Platform for High-Resolution Pupillometry in Vision Research</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Zandi</surname>
          <given-names>Babak</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/1193358/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lode</surname>
          <given-names>Moritz</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Herzog</surname>
          <given-names>Alexander</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sakas</surname>
          <given-names>Georgios</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Khanh</surname>
          <given-names>Tran Quoc</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Laboratory of Lighting Technology, Department of Electrical Engineering and Information Technology, Technical University of Darmstadt</institution>, <addr-line>Darmstadt</addr-line>, <country>Germany</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Interactive Graphic Systems, Department of Computer Science, Technical University of Darmstadt</institution>, <addr-line>Darmstadt</addr-line>, <country>Germany</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Ting Zhao, Janelia Research Campus, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Enkelejda Kasneci, University of Tübingen, Germany; Cristian Rotariu, Grigore T. Popa University of Medicine and Pharmacy, Romania</p>
      </fn>
      <corresp id="c001">*Correspondence: Babak Zandi, <email>zandi@lichttechnik.tu-darmstadt.de</email></corresp>
      <fn fn-type="other" id="fn004">
        <p>This article was submitted to Neural Technology, a section of the journal Frontiers in Neuroscience</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>18</day>
      <month>6</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>15</volume>
    <elocation-id>676220</elocation-id>
    <history>
      <date date-type="received">
        <day>04</day>
        <month>3</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>28</day>
        <month>4</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2021 Zandi, Lode, Herzog, Sakas and Khanh.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Zandi, Lode, Herzog, Sakas and Khanh</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>The human pupil behavior has gained increased attention due to the discovery of the intrinsically photosensitive retinal ganglion cells and the afferent pupil control path’s role as a biomarker for cognitive processes. Diameter changes in the range of 10<sup>–2</sup> mm are of interest, requiring reliable and characterized measurement equipment to accurately detect neurocognitive effects on the pupil. Mostly commercial solutions are used as measurement devices in pupillometry which is associated with high investments. Moreover, commercial systems rely on closed software, restricting conclusions about the used pupil-tracking algorithms. Here, we developed an open-source pupillometry platform consisting of hardware and software competitive with high-end commercial stereo eye-tracking systems. Our goal was to make a professional remote pupil measurement pipeline for laboratory conditions accessible for everyone. This work’s core outcome is an integrated cross-platform (macOS, Windows and Linux) pupillometry software called PupilEXT, featuring a user-friendly graphical interface covering the relevant requirements of professional pupil response research. We offer a selection of six state-of-the-art open-source pupil detection algorithms (Starburst, Swirski, ExCuSe, ElSe, PuRe and PuReST) to perform the pupil measurement. A developed 120-fps pupillometry demo system was able to achieve a calibration accuracy of 0.003 mm and an averaged temporal pupil measurement detection accuracy of 0.0059 mm in stereo mode. The PupilEXT software has extended features in pupil detection, measurement validation, image acquisition, data acquisition, offline pupil measurement, camera calibration, stereo vision, data visualization and system independence, all combined in a single open-source interface, available at <ext-link ext-link-type="uri" xlink:href="https://github.com/openPupil/Open-PupilEXT">https://github.com/openPupil/Open-PupilEXT</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>pupillometry</kwd>
      <kwd>pupil measurement</kwd>
      <kwd>stereo camera</kwd>
      <kwd>vision research</kwd>
      <kwd>pupil diameter</kwd>
      <kwd>eye tracking</kwd>
      <kwd>open source</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">Deutsche Forschungsgemeinschaft<named-content content-type="fundref-id">10.13039/501100001659</named-content></funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="11"/>
      <table-count count="1"/>
      <equation-count count="2"/>
      <ref-count count="172"/>
      <page-count count="24"/>
      <word-count count="0"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <title>Introduction</title>
    <p>The pupil diameter is an essential metric in visual neuroscience, as it has a direct impact on the retinal irradiance, visual acuity and visual performance of the eye (<xref rid="B20" ref-type="bibr">Campbell, 1957</xref>; <xref rid="B21" ref-type="bibr">Campbell and Gubisch, 1966</xref>; <xref rid="B163" ref-type="bibr">Woodhouse, 1975</xref>; <xref rid="B132" ref-type="bibr">Schwiegerling, 2000</xref>). Since the early days of pupillary research (<xref rid="B118" ref-type="bibr">Reeves, 1918</xref>), the modeling of the pupil light response and its retinal processing path was the main focus of investigations (<xref rid="B168" ref-type="bibr">Zandi and Khanh, 2021</xref>). Additionally, the pupil diameter is used as a biomarker in research disciplines such as cognitive science (<xref rid="B2" ref-type="bibr">Aminihajibashi et al., 2020</xref>; <xref rid="B25" ref-type="bibr">Cherng et al., 2020</xref>; <xref rid="B28" ref-type="bibr">Clewett et al., 2020</xref>; <xref rid="B134" ref-type="bibr">Sibley et al., 2020</xref>), circadian photoentrainment (<xref rid="B103" ref-type="bibr">Münch et al., 2012</xref>; <xref rid="B17" ref-type="bibr">Bonmati-Carrion et al., 2016</xref>; <xref rid="B139" ref-type="bibr">Spitschan et al., 2019</xref>; <xref rid="B145" ref-type="bibr">Tähkämö et al., 2019</xref>; <xref rid="B154" ref-type="bibr">Van Egroo et al., 2019</xref>), clinical diagnostics (<xref rid="B87" ref-type="bibr">Lim et al., 2016</xref>; <xref rid="B71" ref-type="bibr">Joyce et al., 2018</xref>; <xref rid="B26" ref-type="bibr">Chougule et al., 2019</xref>) or neuroscience (<xref rid="B130" ref-type="bibr">Schwalm and Jubal, 2017</xref>; <xref rid="B23" ref-type="bibr">Carle et al., 2019</xref>). Pupil changes of 0.015 to 0.5 mm are the range of interest in such studies, leading to increased resolution and robustness requirements for pupil measurement equipment. Closed commercial eye-tracking systems are common in pupil examinations, associated with high investments without offering the possibilities of validating the pupil detection’s measurement accuracy. Additionally, with closed systems, it is not possible to identify the applied pupil detection algorithm, making it challenging to reproduce experiments since small inaccuracies in a range of 0.01 mm could propagate errors to the statistical evaluation of the pupil diameter. Apart from commercial solutions, there is currently a lack of an end-to-end open-source measurement platform that can be easily set up for high-precision pupillometry under laboratory conditions. Therefore, we developed a freely available hardware and software platform for pupil measurements to support the increased interest of interdisciplinary research groups in studying the pupil behavior. Our proposed platform is a comprehensive solution for performing accurate, verifiable and reproducible pupil examinations, competitive with high-end commercial stereo eye-tracking systems.</p>
    <p>The core outcome of this work is an integrated cross-platform (macOS, Windows and Linux) pupillometry software called <italic>PupilEXT</italic>, featuring a user-friendly graphical interface (C++, QT), covering the relevant requirements of professional pupil behavior research (<xref ref-type="fig" rid="F1">Figure 1</xref>). The open-source philosophy offers insight into how the pupil measurement framework performs, motivating to more transparency in collecting pupil data. We aimed to provide a plug-and-play integrated hardware and software platform, allowing interdisciplinary research groups a precise pupil behavior research without high investments. The proposed software is designed to incorporate high-resolution industrial cameras that can be run either individually or in a stereo camera arrangement. We guarantee a stable frame rate and synchronous operation of stereo cameras by using a microcontroller as an external hardware trigger. The integrated solution with hardware and software is provided in a way that even scientists with a non-technical background can reproduce the system. Users simply need to purchase industrial cameras and run the proposed <italic>PupilEXT</italic> software.</p>
    <fig id="F1" position="float">
      <label>FIGURE 1</label>
      <caption>
        <p>The graphical user interface of the <italic>PupilEXT</italic> software during a pupil measurement with one connected industrial camera. The measured pupil values are listed in real-time in a table or can be visualized graphically. We provide a selection of six state-of-the-art pupil detection algorithms from the literature. Stereo camera systems can be connected and calibrated seamlessly to acquire the absolute pupil diameter. The accuracy of a pupil measurement or calibration can be verified by implemented routines.</p>
      </caption>
      <graphic xlink:href="fnins-15-676220-g001"/>
    </fig>
    <p>Inspired by the eye-tracking software <italic>EyeRecToo</italic> (<xref rid="B124" ref-type="bibr">Santini et al., 2017</xref>) from Santini et al., we offer end-users a selection of six state-of-the-art open-source pupil detection algorithms (<italic>Starburst</italic>, <italic>Swirski</italic>, <italic>ExCuSe</italic>, <italic>Else</italic>, <italic>PuRe</italic> and <italic>PuReST</italic>) to perform the pupil measurement. The system allows researchers to report the used pupil algorithm with the respective parameters since the pupil detection method itself could influence the captured data. Additionally, end-users will be able to determine the pupil diameter from externally acquired image sequences through the software suite. The integrated platform is available to other research groups as an open-source project, ensuring continuous development in the future. We aimed to bridge the gap between visual neuroscience or experimental psychology and engineering sciences, making professional remote pupil measurements under laboratory conditions accessible for everyone, without suffering the features of commercial solutions.</p>
    <p>The first section of this work deals with the scientific background of pupil behavior research and the rising popularity of this topic, from which we derive the motivation of the proposed pupil measurement platform. Based on that, the current state of pupillometry and the availability of suitable open-source frameworks are highlighted. Next, we conducted a meta-analysis of existing pupil detection algorithms from the literature intending to select and integrate appropriate algorithms in the proposed <italic>PupilEXT</italic> software. The functionality of the platform is covered by starting with the hardware components, consisting of cameras, microcontroller and a near-infrared (NIR) illumination. Here, we describe the possible hardware topologies with which end-users can conduct a pupil measurement or offline analysis of external captured images. In particular, we show the possibilities of validating a pupil measurement and camera calibration with the <italic>PupilEXT</italic> software. Finally, the performance of the system is demonstrated with an experiment concerning the pupil light response, clarifying the provided pupil metrics for reliable data evaluation.</p>
  </sec>
  <sec id="S2">
    <title>The Rising Popularity of Pupil Light Response Research</title>
    <p>The human retina contains receptors with distinct photopigments, capable of transforming light quanta of different wavelengths λ into frequency-coded action potentials with information on color and brightness features from a visual stimulus. Photoreceptors in the retina are classified according to their broad spectral sensitivity in the visible spectrum range and respective peak response λ<sub>Peak</sub>. In the photopic adapted eye, the retinal image-forming pathway is mainly controlled by the short-wavelength (S, λ<sub>Peak</sub> 420 nm), medium-wavelength (M, λ<sub>Peak</sub> 535 nm) and long-wavelength (L, λ<sub>Peak</sub> 565 nm) sensitive cones (<xref rid="B141" ref-type="bibr">Stockman and Sharpe, 2000</xref>; <xref rid="B136" ref-type="bibr">Solomon and Lennie, 2007</xref>; <xref rid="B93" ref-type="bibr">Lucas et al., 2014</xref>). At scotopic and mesopic light conditions, the more sensitive rods (λ<sub>Peak</sub> 498 nm) dominate the vision. Both cones and rods transmit, depending on the adaptation state of the eye, integrated signals in different stages through ganglion cells to the visual cortex of the brain (<xref rid="B155" ref-type="bibr">Van Meeteren, 1978</xref>; <xref rid="B135" ref-type="bibr">Smith et al., 2008</xref>; <xref rid="B68" ref-type="bibr">Jennings and Martinovic, 2014</xref>). In 1924, the International Commission on Illumination (CIE) introduced the photopic luminous efficiency function <italic>V</italic>(λ) to estimate the visual effectiveness of light spectra for humans (<xref rid="B15" ref-type="bibr">Bodmann, 1992</xref>; <xref rid="B133" ref-type="bibr">Sharpe et al., 2005</xref>; <xref rid="B122" ref-type="bibr">Sagawa, 2006</xref>).</p>
    <p>A standard value in estimating the human brightness perception is the luminance <italic>L</italic> given in cd/m<sup>2</sup>, which is a <italic>V</italic>(λ) weighted photometric quantity (<xref rid="B9" ref-type="bibr">Berman et al., 1990</xref>; <xref rid="B84" ref-type="bibr">Lennie et al., 1993</xref>; <xref rid="B162" ref-type="bibr">Withouck et al., 2013</xref>). The luminance is merely a first approximation of the brightness perception, as only the additive contribution of L- and M-cones to the image-forming pathway is managed by <italic>V</italic>(λ) (<xref rid="B27" ref-type="bibr">CIE, 2011</xref>; <xref rid="B12" ref-type="bibr">Besenecker and Bullough, 2017</xref>; <xref rid="B59" ref-type="bibr">Hermans et al., 2018</xref>; <xref rid="B166" ref-type="bibr">Zandi et al., 2021</xref>). Since 1926, about eight pupil models were proposed that integrated the luminance as a main dependent parameter, assuming that the afferent pupil control pathway can be described by a <italic>V</italic>(λ) weighted quantity (<xref rid="B61" ref-type="bibr">Holladay, 1926</xref>; <xref rid="B31" ref-type="bibr">Crawford, 1936</xref>; <xref rid="B99" ref-type="bibr">Moon and Spencer, 1944</xref>; <xref rid="B33" ref-type="bibr">de Groot and Gebhard, 1952</xref>; <xref rid="B140" ref-type="bibr">Stanley and Davies, 1995</xref>; <xref rid="B14" ref-type="bibr">Blackie and Howland, 1999</xref>; <xref rid="B6" ref-type="bibr">Barten, 1999</xref>; <xref rid="B159" ref-type="bibr">Watson and Yellott, 2012</xref>; <xref rid="B169" ref-type="bibr">Zandi et al., 2020</xref>).</p>
    <p>The discovery of a new type of receptors in the outer retina called intrinsically photosensitive retinal ganglion cells (ipRGCs) was a turning point of vision science (<xref rid="B114" ref-type="bibr">Provencio et al., 1998</xref>, <xref rid="B115" ref-type="bibr">2000</xref>; <xref rid="B52" ref-type="bibr">Gooley et al., 2001</xref>; <xref rid="B11" ref-type="bibr">Berson et al., 2002</xref>; <xref rid="B55" ref-type="bibr">Hattar, 2002</xref>; <xref rid="B104" ref-type="bibr">Mure, 2021</xref>), which has led to a rethinking of classical retinal processing models. This subset of ganglion cells are part of the non-image-forming mechanism of the eye because of their projection to regions of the suprachiasmatic nucleus (SCN) and olivary pretectal nucleus (OPN) (<xref rid="B120" ref-type="bibr">Ruby et al., 2002</xref>; <xref rid="B10" ref-type="bibr">Berson, 2003</xref>; <xref rid="B57" ref-type="bibr">Hattar et al., 2003</xref>; <xref rid="B37" ref-type="bibr">Do et al., 2009</xref>; <xref rid="B40" ref-type="bibr">Ecker et al., 2010</xref>; <xref rid="B1" ref-type="bibr">Allen et al., 2019</xref>; <xref rid="B36" ref-type="bibr">Do, 2019</xref>). As a result, the ipRGCs can modulate the circadian rhythm (<xref rid="B42" ref-type="bibr">Freedman, 1999</xref>; <xref rid="B18" ref-type="bibr">Brainard et al., 2001</xref>; <xref rid="B147" ref-type="bibr">Thapan et al., 2001</xref>; <xref rid="B117" ref-type="bibr">Rea and Figueiro, 2018</xref>; <xref rid="B151" ref-type="bibr">Truong et al., 2020</xref>) and pupil light response (<xref rid="B92" ref-type="bibr">Lucas et al., 2001</xref>, <xref rid="B91" ref-type="bibr">2020</xref>; <xref rid="B50" ref-type="bibr">Gamlin et al., 2007</xref>; <xref rid="B165" ref-type="bibr">Young and Kimura, 2008</xref>; <xref rid="B5" ref-type="bibr">Barrionuevo et al., 2018</xref>; <xref rid="B106" ref-type="bibr">Murray et al., 2018</xref>) via a processing path that works independently of the classical image-forming pathway (<xref rid="B56" ref-type="bibr">Hattar et al., 2006</xref>; <xref rid="B54" ref-type="bibr">Güler et al., 2008</xref>; <xref rid="B128" ref-type="bibr">Schmidt et al., 2014</xref>; <xref rid="B137" ref-type="bibr">Spitschan, 2019a</xref>). Recent studies showed that the pupil light response cannot be described by the <italic>V</italic>(λ) weighted luminance alone, making a revision of classical pupil models necessary (<xref rid="B167" ref-type="bibr">Zandi et al., 2018</xref>, <xref rid="B169" ref-type="bibr">2020</xref>; <xref rid="B138" ref-type="bibr">Spitschan, 2019b</xref>; <xref rid="B170" ref-type="bibr">Zele et al., 2019</xref>). Therefore, one key topic in pupillary research is the development of a valid empirical model (<xref rid="B169" ref-type="bibr">Zandi et al., 2020</xref>), providing a spectral and time-variant function with dynamic receptor weighting to predict the temporal aperture across individuals (<xref rid="B116" ref-type="bibr">Rao et al., 2017</xref>; <xref rid="B168" ref-type="bibr">Zandi and Khanh, 2021</xref>). When using stimulus spectra along the Planckian locus for triggering the pupil light response, it is essential in measurements that amplitudes in the range of 0.1 to 0.4 mm are captured accurately to specify intrasubject variability (<xref rid="B77" ref-type="bibr">Kobashi et al., 2012</xref>) in a pupil model. However, a special requirement for pupil measurements arises when the pupil is used as a biomarker for quantifying the cognitive state (<xref rid="B100" ref-type="bibr">Morad et al., 2000</xref>; <xref rid="B98" ref-type="bibr">Merritt et al., 2004</xref>; <xref rid="B105" ref-type="bibr">Murphy et al., 2014</xref>; <xref rid="B109" ref-type="bibr">Ostrin et al., 2017</xref>; <xref rid="B149" ref-type="bibr">Tkacz-Domb and Yeshurun, 2018</xref>; <xref rid="B65" ref-type="bibr">Hu et al., 2019</xref>; <xref rid="B154" ref-type="bibr">Van Egroo et al., 2019</xref>; <xref rid="B34" ref-type="bibr">de Winter et al., 2021</xref>; <xref rid="B153" ref-type="bibr">Van der Stoep et al., 2021</xref>) or clinical symptoms of diseases (<xref rid="B64" ref-type="bibr">Hreidarsson, 1982</xref>; <xref rid="B94" ref-type="bibr">Maclean and Dhillon, 1993</xref>; <xref rid="B29" ref-type="bibr">Connelly et al., 2014</xref>; <xref rid="B87" ref-type="bibr">Lim et al., 2016</xref>; <xref rid="B53" ref-type="bibr">Granholm et al., 2017</xref>; <xref rid="B160" ref-type="bibr">Wildemeersch et al., 2018</xref>; <xref rid="B26" ref-type="bibr">Chougule et al., 2019</xref>). Cognitive processes such as memory load, arousal, circadian status, or sleepiness have a transient impact (<xref rid="B159" ref-type="bibr">Watson and Yellott, 2012</xref>) on the pupil diameter with aperture changes of 0.015 to 0.53 mm (<xref rid="B8" ref-type="bibr">Beatty and Wagoner, 1978</xref>; <xref rid="B7" ref-type="bibr">Beatty, 1982</xref>; <xref rid="B127" ref-type="bibr">Schluroff et al., 1986</xref>; <xref rid="B69" ref-type="bibr">Jepma and Nieuwenhuis, 2011</xref>; <xref rid="B111" ref-type="bibr">Pedrotti et al., 2014</xref>; <xref rid="B16" ref-type="bibr">Bombeke et al., 2016</xref>; <xref rid="B152" ref-type="bibr">Tsukahara et al., 2016</xref>; <xref rid="B161" ref-type="bibr">Winn et al., 2018</xref>), making the reproducibility of such effects difficult if the accuracy of the measurement equipment has not been sufficiently validated.</p>
    <p>Today, the pupil behavior has become an interdisciplinary field of research (<xref rid="B80" ref-type="bibr">La Morgia et al., 2018</xref>; <xref rid="B129" ref-type="bibr">Schneider et al., 2020</xref>; <xref rid="B70" ref-type="bibr">Joshi, 2021</xref>; <xref rid="B113" ref-type="bibr">Pinheiro and da Costa, 2021</xref>) in which the number of involved scientists rises, as the trend of the number of publications with the keywords “pupil diameter” or “pupillometry” reveals (<xref ref-type="fig" rid="F2">Figure 2</xref>). The renewed attention to the temporal pupil aperture (<xref rid="B13" ref-type="bibr">Binda and Gamlin, 2017</xref>), its application in clinical diagnostics (<xref rid="B53" ref-type="bibr">Granholm et al., 2017</xref>; <xref rid="B71" ref-type="bibr">Joyce et al., 2018</xref>; <xref rid="B26" ref-type="bibr">Chougule et al., 2019</xref>; <xref rid="B75" ref-type="bibr">Kercher et al., 2020</xref>; <xref rid="B144" ref-type="bibr">Tabashum et al., 2021</xref>) and increasing popularity of chromatic pupillometry (<xref rid="B121" ref-type="bibr">Rukmini et al., 2017</xref>; <xref rid="B32" ref-type="bibr">Crippa et al., 2018</xref>) topics requires additional efforts in terms of standardization and provision of consistent tools, contributing to comparability in measurement and pre-processing methodologies. For instance, one key point of standardization is the prevention of artificially induced changes to raw data by the used tools, as in cognitive or vision-related pupillary research small diameter margins are of interest. The main methodology factors that could influence the research results or reliability of pupil behavior studies are as follows:</p>
    <list list-type="simple">
      <list-item>
        <label>(1)</label>
        <p>Number and depth of described experimental metrics when reporting the results concerning the stimulus modality or pre-conditioning state of the subjects.</p>
      </list-item>
      <list-item>
        <label>(2)</label>
        <p>The used pre-processing method to smooth out and clean the measured pupil raw data.</p>
      </list-item>
      <list-item>
        <label>(3)</label>
        <p>The used measurement hardware and software framework in collecting pupil data.</p>
      </list-item>
    </list>
    <fig id="F2" position="float">
      <label>FIGURE 2</label>
      <caption>
        <p>The number of publications with the keywords “pupil diameter” and “pupillometry” since 1985 to 2019, based on the Web of Science database. The rising count of publications in recent years indicates that the topic of pupil behavior is becoming more important. Due to the interdisciplinary field of research, standardization of measurement methodology and data processing is favorable, making study results comparable.</p>
      </caption>
      <graphic xlink:href="fnins-15-676220-g002"/>
    </fig>
    <p>In order to minimize the influencing factors, there are actions in the research community to provide the essential tools for pupil research to lower the barrier of entering the topic and ensuring the comparability of future research. A major step in this direction was the work “Standards in Pupillography” by Kelbsch et al., which summarized the current knowledge on pupil behavior and defined recommendations to be considered by author groups when reporting pupil study results (<xref rid="B74" ref-type="bibr">Kelbsch et al., 2019</xref>). The standardization approach mainly dealt with the minimal set of metrics that authors need to specify in published research, allowing third parties to reproduce experiments when necessary. Regarding the topic of data pre-processing, the focus is on which methods should be used to detect and remove artificially induced pupil changes, caused by eye blinks and fast gaze jumps during pupil recording sessions. Ranging from catching artifacts to smoothing out the measured raw data, a large number of software libraries and guidelines exist that can assist researchers in carrying out such tasks (<xref rid="B110" ref-type="bibr">Pedrotti et al., 2011</xref>; <xref rid="B22" ref-type="bibr">Canver et al., 2014</xref>; <xref rid="B83" ref-type="bibr">Lemercier et al., 2014</xref>; <xref rid="B4" ref-type="bibr">Attard-Johnson et al., 2019</xref>; <xref rid="B78" ref-type="bibr">Kret and Sjak-Shie, 2019</xref>; <xref rid="B156" ref-type="bibr">van Rij et al., 2019</xref>).</p>
    <p>The research area of pupil behavior benefits from the interdisciplinarity of the research groups, which is promoted by the provision of tools and predefined standardized methodologies. However, the pupillometry technique itself is a significant hurdle, since there are no standardized requirements or reliable end-to-end open-source systems for recording pupil data in high-precision experiments under laboratory conditions.</p>
  </sec>
  <sec id="S3">
    <title>The Issue of Pupillometry</title>
    <p>Typically, a pupil measurement can be performed manually by using a double-pinhole pupillometer (<xref rid="B61" ref-type="bibr">Holladay, 1926</xref>) or photographs with a reference object (<xref rid="B31" ref-type="bibr">Crawford, 1936</xref>) or through an integrated eye-tracking system. A higher proportion of pupil behavior studies is conducted by using an eye-tracking system, as identifying the pupil region is often a necessary step before estimating the gaze position (<xref rid="B82" ref-type="bibr">Lee et al., 2012</xref>). Commercial eye trackers from Tobii Pro, Smart Eye Pro or Eyelink are common solutions, which are easy to set up and usable without a technical background but cost approximately between 5,000 and 40,000 euros (<xref rid="B63" ref-type="bibr">Hosp et al., 2020</xref>; <xref rid="B95" ref-type="bibr">Manuri et al., 2020</xref>). Purchasing a set of high-resolution professional industrial cameras costs about 200 to 600 euros, with which an optical accuracy of 0.01 mm/px or more could be achieved. Thus, the price gap from commercial products results from the integrated software and license fees.</p>
    <p>Commercial systems rely on closed software, restricting thereby conclusions about the used pupil-tracking algorithms, which is essential for the reproducibility. Additionally, based on the authors’ best knowledge, there is no commercial eye-tracking system that states the accuracy of their measured pupil diameter in the datasheet nor is a manual validation possible, as their solutions’ primarily focus is on gaze tracking. Especially in studies where pupil diameter effects are in a range of 10<sup>–2</sup> mm, a validation of the system’s pupil measurement accuracy through a reference object is desirable.</p>
    <p>The open-source head-mounted eye tracker project by <italic>Pupil Labs</italic> (<xref rid="B72" ref-type="bibr">Kassner et al., 2014</xref>) is an alternative to fully commercialized solutions, allowing free head movements and experiments in natural environments where a classic remote eye-tracking set-up is not possible. However, we do not recommend this system for precise pupil measurement applications, due to the cameras’ positions which are highly off-axis, causing pupil foreshortening errors (<xref rid="B58" ref-type="bibr">Hayes and Petrov, 2016</xref>). Additionally, the absolute pupil diameter is calculated indirectly by a method from which conversion accuracy is not yet fully validated for pupil measurements. Therefore, the solution provided by <italic>Pupil Labs</italic> is more suitable for experiments in which only the relative pupil diameter is of interest.</p>
    <p>Remote tracking systems, positioned on the optical axis of the eye, are better suited for reliable pupil measurements. Various published approaches provide isolated components to build a custom remote stereo camera system (<xref rid="B60" ref-type="bibr">Hiley et al., 2006</xref>; <xref rid="B90" ref-type="bibr">Long et al., 2007</xref>; <xref rid="B79" ref-type="bibr">Kumar et al., 2009</xref>; <xref rid="B123" ref-type="bibr">San Agustin et al., 2010</xref>), which is not always feasible for interdisciplinary research groups, leading to a preference for commercial solutions. However, a groundbreaking project called <italic>EyeRecToo</italic> by <xref rid="B124" ref-type="bibr">Santini et al. (2017)</xref> has taken the first steps in establishing the idea of a competitive open eye-tracking software suite, which even has the option of choosing between different state-of-the-art pupil detection algorithms. Unfortunately, the software is mainly designed for head-mounted eye trackers or webcams and the use-cases are not targeted for the experimental pipeline of pupil research under laboratory conditions. For instance, a stereo camera arrangement with extrinsic calibration and the subsequent validation of a camera’s accuracy is not possible, to our best knowledge. Additionally, the software does not offer the option for evaluating external captured images from a stereo or mono camera system.</p>
    <p>The success of the <italic>Pupil Labs</italic> project shows that end-users wish to have a fully integrated system consisting of hardware and software, packed with the functionalities of a professional commercial solution. Thus, in developing our proposed platform, we have focused not only on the functionalities and requirements of pupil researchers but also on the end-user’s experience, which should provide an easy way to build and run a pupil measurement system.</p>
  </sec>
  <sec id="S4">
    <title>Choosing Pupil Detection Algorithms for PupilEXT</title>
    <p>The main application for an eye-tracking system is the estimation of a subject’s gaze location, which usually needs to recognize the pupil contour and its center position. Due to the high contrast between the sclera and the pupil region in a digital image, the recognition of the pupil is in principle possible through a combination of thresholding, edge detection and morphological operations (<xref rid="B51" ref-type="bibr">Goñi et al., 2004</xref>; <xref rid="B73" ref-type="bibr">Keil et al., 2010</xref>; <xref rid="B150" ref-type="bibr">Topal et al., 2017</xref>). State-of-the-art pupil detection approaches have additional steps in the image processing pipeline, ensuring a more robust contour fit while having a high and accurate detection rate. Under laboratory conditions, eye images are mainly captured using a NIR light source to avoid cornea reflections of the ambient environment, leading to optimized pupil detection. However, accurate pupil detection is an essential step in eye-tracking systems since a flawed edge detection could have an impact on the performance of an eye tracker (<xref rid="B125" ref-type="bibr">Santini et al., 2018a</xref>). Therefore, pupil detection methods intended for eye-tracking systems can also be used for pupil measurement, if an algorithm features the detection of aperture sizes.</p>
    <p>There are three different illumination set-ups proposed for capturing a series of eye images that need to be in line with the used pupil detection algorithm (<xref rid="B85" ref-type="bibr">Li et al., 2005</xref>). In the bright-pupil method, a NIR-light source is placed close to the optical axis of a camera, resulting in a positive contrast between the iris and pupil region (<xref rid="B66" ref-type="bibr">Hutchinson et al., 1989</xref>). Due to the retinal reflection of the illumination back to the camera, the pupil region appears brighter than the iris and sclera itself (<xref rid="B85" ref-type="bibr">Li et al., 2005</xref>). In the dark-pupil method, the light source is placed off-axis to the camera. Thus, the pupil appears as a dark spot surrounded by the brighter iris (negative contrast). A third method called the image-difference technique leverages the image difference between dark- and bright-pupil to extract the pupil’s contour. For this, one NIR illumination should be positioned close to the camera’s optical axis (NIR 1) and a second one off-axis (NIR 2). By synchronizing the illuminations’ flashing interval with the sampling rate of a camera, one positive contrast image can be captured in a first frame (NIR 1, ON; NIR 2, OFF) and a second frame with negative contrast (NIR 1, OFF; NIR 2, ON). This approach can lead to a more robust pupil detection but has the drawback that more effort has to be invested in the illumination. Furthermore, two frames are needed for each captured pupil size value, reducing the overall sampling rate. The recent work of <xref rid="B38" ref-type="bibr">Ebisawa (1994</xref>, <xref rid="B39" ref-type="bibr">2004)</xref>, <xref rid="B101" ref-type="bibr">Morimoto et al. (2002)</xref>, and <xref rid="B60" ref-type="bibr">Hiley et al. (2006)</xref> used this image-difference technique.</p>
    <p>However, the core of a pupil measurement system is the algorithm that is used to determine the pupil diameter. Recently published works developed state-of-the-art approaches that can be applied in our proposed software <italic>PupilEXT</italic>. Similar to the work of <xref rid="B150" ref-type="bibr">Topal et al. (2017)</xref>, we conducted a meta-analysis of 35 published pupil detection methods (<xref rid="T1" ref-type="table">Table 1</xref>) to evaluate and select suitable algorithms for our proposed measurement platform.</p>
    <table-wrap id="T1" position="float">
      <label>TABLE 1</label>
      <caption>
        <p>Comparison of the pupil detection algorithms identified in the literature.</p>
      </caption>
      <table frame="hsides" rules="groups" cellspacing="5" cellpadding="5">
        <thead>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Algorithm</td>
            <td valign="top" align="left" rowspan="1" colspan="1">Approach basis</td>
            <td valign="top" align="center" rowspan="1" colspan="1">Downscaling</td>
            <td valign="top" align="center" rowspan="1" colspan="1">Bright/dark pupil</td>
            <td valign="top" align="center" rowspan="1" colspan="1">Thresholding</td>
            <td valign="top" align="center" rowspan="1" colspan="1">Ellipse fitting</td>
            <td valign="top" align="center" rowspan="1" colspan="1">Center of mass</td>
            <td valign="top" align="center" rowspan="1" colspan="1">Temporal information</td>
            <td valign="top" align="center" rowspan="1" colspan="1">Runtime in ms</td>
            <td valign="top" align="center" rowspan="1" colspan="1">Pupil size output</td>
            <td valign="top" align="center" rowspan="1" colspan="1">Blink detection</td>
            <td valign="top" align="center" rowspan="1" colspan="1">Confidence measure</td>
            <td valign="top" align="center" rowspan="1" colspan="1">Implementation available</td>
            <td valign="top" align="center" rowspan="1" colspan="1">Pupil size evaluation</td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B38" ref-type="bibr">Ebisawa, 1994</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Image-diff.</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">⊙</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B172" ref-type="bibr">Zhu et al., 1999</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Curvature</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">LSM</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B102" ref-type="bibr">Morimoto et al., 2000</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Image-diff.</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">⊙</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">67</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B112" ref-type="bibr">Pérez et al., 2003</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Threshold</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">40</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B89" ref-type="bibr">Lin et al., 2003</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Edge</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">LSM</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">166</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B51" ref-type="bibr">Goñi et al., 2004</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Threshold</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">□</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">33</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B39" ref-type="bibr">Ebisawa, 2004</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Image-diff.</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">⊙</td>
            <td valign="top" align="center" rowspan="1" colspan="1">*</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">20</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <italic>Starburst</italic>
              <xref rid="B85" ref-type="bibr">Li et al., 2005</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Rays</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">*</td>
            <td valign="top" align="center" rowspan="1" colspan="1">RANSAC</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">100<sup>(3)</sup></td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">○</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B60" ref-type="bibr">Hiley et al., 2006</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Image-diff.</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">12<sup>(2)</sup></td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B90" ref-type="bibr">Long et al., 2007</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Threshold</td>
            <td valign="top" align="center" rowspan="1" colspan="1">○</td>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑<sup>(1)</sup></td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">6.67</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B35" ref-type="bibr">Dey and Samanta, 2007</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Threshold</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">*</td>
            <td valign="top" align="center" rowspan="1" colspan="1">Circle</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">127</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B123" ref-type="bibr">San Agustin et al., 2010</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Threshold</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">RANSAC</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B79" ref-type="bibr">Kumar et al., 2009</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Edge</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">*</td>
            <td valign="top" align="center" rowspan="1" colspan="1">LSM</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B73" ref-type="bibr">Keil et al., 2010</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Threshold</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">60</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B88" ref-type="bibr">Lin et al., 2010</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Threshold</td>
            <td valign="top" align="center" rowspan="1" colspan="1">○</td>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑<sup>(1)</sup></td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">○</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B81" ref-type="bibr">Lanatà et al., 2011</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Threshold</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">LSM</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B142" ref-type="bibr">Świrski et al., 2012</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Threshold</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">*</td>
            <td valign="top" align="center" rowspan="1" colspan="1">RANSAC</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">3.77</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B131" ref-type="bibr">Schwarz et al., 2012</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Threshold</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B143" ref-type="bibr">Świrski and Dodgson, 2013</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">3D model</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">*</td>
            <td valign="top" align="center" rowspan="1" colspan="1">RANSAC</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B72" ref-type="bibr">Kassner et al., 2014</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Edge</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">LSM</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">45<sup>(3)</sup></td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B24" ref-type="bibr">Chen and Epps, 2014</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Threshold</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">*</td>
            <td valign="top" align="center" rowspan="1" colspan="1">LSM</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">60<sup>(2)</sup></td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">○</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1"><italic>ExCuSe</italic> (<xref rid="B45" ref-type="bibr">Fuhl et al., 2015</xref>)</td>
            <td valign="top" align="left" rowspan="1" colspan="1">Edge</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">LSM</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">7</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1"><italic>SET</italic> (<xref rid="B67" ref-type="bibr">Javadi et al., 2015</xref>)</td>
            <td valign="top" align="left" rowspan="1" colspan="1">Threshold</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">100</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1"><italic>ElSe</italic> (<xref rid="B48" ref-type="bibr">Fuhl et al., 2016a</xref>)</td>
            <td valign="top" align="left" rowspan="1" colspan="1">Edge</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">*</td>
            <td valign="top" align="center" rowspan="1" colspan="1">LSM</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">7</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">○</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1"><italic>PupilNet</italic> (<xref rid="B46" ref-type="bibr">Fuhl et al., 2016b</xref>)</td>
            <td valign="top" align="left" rowspan="1" colspan="1">CNN</td>
            <td valign="top" align="center" rowspan="1" colspan="1">○</td>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1"><italic>APPD</italic> (<xref rid="B150" ref-type="bibr">Topal et al., 2017</xref>)</td>
            <td valign="top" align="left" rowspan="1" colspan="1">Curvature</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">MSM</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">5.37</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">○</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1"><italic>PuRe</italic> (<xref rid="B125" ref-type="bibr">Santini et al., 2018a</xref>)</td>
            <td valign="top" align="left" rowspan="1" colspan="1">Edge</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">LSM</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">5.17</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1"><italic>PuReST</italic> (<xref rid="B126" ref-type="bibr">Santini et al., 2018b</xref>)</td>
            <td valign="top" align="left" rowspan="1" colspan="1">Edge</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">*</td>
            <td valign="top" align="center" rowspan="1" colspan="1">LSM</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">1.88</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B86" ref-type="bibr">Li et al., 2018</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Edge</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">LSM</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1"><italic>DeepEye</italic> (<xref rid="B157" ref-type="bibr">Vera-Olmos et al., 2018</xref>)</td>
            <td valign="top" align="left" rowspan="1" colspan="1">CNN</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">*</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">33<sup>(4)</sup></td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1"><italic>FREDA</italic> (<xref rid="B96" ref-type="bibr">Martinikorena et al., 2018</xref>)</td>
            <td valign="top" align="left" rowspan="1" colspan="1">Image-diff.</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">63<sup>(2)</sup></td>
            <td valign="top" align="center" rowspan="1" colspan="1">○</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1"><italic>CBF</italic> (<xref rid="B44" ref-type="bibr">Fuhl et al., 2018b</xref>)</td>
            <td valign="top" align="left" rowspan="1" colspan="1">Feature-class.</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">6.8</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1"><italic>BORE</italic> (<xref rid="B43" ref-type="bibr">Fuhl et al., 2018a</xref>)</td>
            <td valign="top" align="left" rowspan="1" colspan="1">Edge</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">15</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1"><italic>DeepVOG</italic> (<xref rid="B164" ref-type="bibr">Yiu et al., 2019</xref>)</td>
            <td valign="top" align="left" rowspan="1" colspan="1">CNN</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">17<sup>(4)</sup></td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <xref rid="B41" ref-type="bibr">Eivazi et al., 2019</xref>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">CNN</td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td valign="top" align="center" rowspan="1" colspan="1">■</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td valign="top" align="center" rowspan="1" colspan="1">8<sup>(4)</sup></td>
            <td valign="top" align="center" rowspan="1" colspan="1">🌑</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <attrib>
          <italic>We classified the algorithms by their applied technique and properties. This meta-analysis is inspired by the approach of <xref rid="B150" ref-type="bibr">Topal et al. (2017)</xref>. Only a small portion of the proposed algorithms was evaluated through the detected pupil size, as stated in the last column. Typically, the authors assessed the pupil center’s accuracy, as eye tracking was the main application of most algorithms. The algorithms in the gray marked rows are available in our proposed pupil measurements software PupilExt. ○ only partial, ■ dark pupil, □ bright pupil, ⊙ bright and dark pupil, <sup>∗</sup> adaptive thresholding, 🌑 yes. (1) Symmetric center of mass, (2) MATLAB implementation, (3) Python implementation, and (4) graphics processing unit (GPU) aided.</italic>
        </attrib>
      </table-wrap-foot>
    </table-wrap>
    <p>The potential algorithms need to estimate the pupil size, as this is the main focus of this work. From the 35 evaluated algorithms, we can rule out 11 approaches since they are not able to output the pupil size (<xref rid="T1" ref-type="table">Table 1</xref>). We decided to consider only algorithms designed for dark-pupil detection, serving to more freedom in setting up the position of the NIR light source. Another criterion for the selection was the availability of the implementation since we started from the working hypothesis that published procedures with existing programming code are ready for practical applications. Since our graphical user interface (GUI) should offer real-time pupil detection, only C++-implemented approaches were of interest.</p>
    <p>Based on these criteria and taking the algorithms’ recency into account, we selected a total of six pupil detection approaches for <italic>PupilEXT</italic>. First, we decided to use the robust <italic>Starburst</italic> algorithm by <xref rid="B85" ref-type="bibr">Li et al. (2005)</xref>, which was considered as a standard approach in pupil detection for a long time, implemented in several works throughout the years. Furthermore, we added the algorithm by <xref rid="B142" ref-type="bibr">Świrski et al. (2012)</xref>, <italic>ExCuSe</italic> by <xref rid="B45" ref-type="bibr">Fuhl et al. (2015)</xref>, <italic>ElSe</italic> by <xref rid="B48" ref-type="bibr">Fuhl et al. (2016a)</xref>, <italic>PuReST</italic> by <xref rid="B126" ref-type="bibr">Santini et al. (2018b)</xref> and <italic>PuRe</italic> by <xref rid="B125" ref-type="bibr">Santini et al. (2018a)</xref>. The algorithms <italic>ElSe</italic>, <italic>ExCuSe</italic>, <italic>PuRe</italic> and <italic>PuReST</italic> are licensed for non-commercial use only. The pupil detection algorithm from Swirski et al. is licensed under MIT, and the <italic>Starburst</italic> algorithm under GNU GPL. More details about the licensing terms of the detection algorithms can be found on the project page of <italic>PupilEXT</italic><sup><xref ref-type="fn" rid="footnote1">1</xref></sup>.</p>
    <p>We did not select pupil detection approaches based on neural networks (<xref rid="B97" ref-type="bibr">Mazziotti et al., 2021</xref>). Models such as <italic>DeepEye</italic> (<xref rid="B157" ref-type="bibr">Vera-Olmos et al., 2018</xref>) and <italic>PupilNet</italic> (<xref rid="B46" ref-type="bibr">Fuhl et al., 2016b</xref>, <xref rid="B47" ref-type="bibr">2017</xref>) reveal promising results, but their computational complexity is still too high for real-time pupil measurement applications without special hardware.</p>
    <p>The user has the option to choose between these state-of-the-art algorithms for pupil measurement in the proposed <italic>PupilEXT</italic> platform. Additionally, the algorithms’ parameter can be checked and adjusted in the user interface to increase the software-based measurement accuracy, if necessary. By default, the <italic>PuRe</italic> algorithm is selected because it is considered as a top performer and the number of parameters are relatively user-friendly, making it to a generalized procedure for different measurement settings (<xref rid="B125" ref-type="bibr">Santini et al., 2018a</xref>, <xref rid="B126" ref-type="bibr">b</xref>). While the algorithms are solely based on recent publications from various author groups, the interested readership is referred to the original works of the respective pupil detection methods or works that already reviewed the algorithms (<xref rid="B150" ref-type="bibr">Topal et al., 2017</xref>; <xref rid="B95" ref-type="bibr">Manuri et al., 2020</xref>).</p>
  </sec>
  <sec id="S5">
    <title>Hardware Set-Up of the Camera System</title>
    <p>We linked the <italic>PupilEXT</italic> software with a specific camera brand (Basler) to provide a comprehensive platform for pupillometry. In this way, we allow a plug-and-play usage of the proposed system since the software is adapted to the hardware. The Pylon SDK is used to interface the cameras with the measurement software <italic>PupilEXT</italic>. Thus, any Basler branded industrial camera is integrable into the pupillometry platform. We explicitly do not support consumer webcams since <italic>PupilEXT</italic> is intended for reliable and accurate research applications. Generally, live or post-acquisition pupil measurements are supported through different measurement configurations (<xref ref-type="fig" rid="F3">Figure 3</xref>).</p>
    <fig id="F3" position="float">
      <label>FIGURE 3</label>
      <caption>
        <p>Illustration of the possible measurement configurations that can be realized with the <italic>PupilExt</italic> software. <bold>(A)</bold> In stereo vision mode, two cameras are connected to the computer via USB3.0. A microcontroller is connected to the IO-pins of the camera, which triggers a synchronized image acquisition on both cameras. We recommend using a Nucleo-STM32 microcontroller since the provided source code can be used to flash the electronic. We implemented a UART communication protocol in the microcontroller so that <italic>PupilExt</italic> can automatically control and synchronize the stereo camera system via the connected hardware, via the electronics. <bold>(B)</bold> In mono camera vision mode, it is possible to control the image acquisition by an external hardware trigger, which has the advantage that the recording time can be set accurately. When capturing multi-image sequences, the hardware trigger consists of a square wave signal in which each rising edge triggers an image acquisition. <bold>(C)</bold> The use of a microcontroller is optional when connecting a single camera. Without the use of a microcontroller, a software trigger is used for image acquisition. <bold>(D)</bold> Prototype of a pupillometry set-up with a single camera and respective near-infrared (NIR) illumination unit. <bold>(E)</bold> The software <italic>PupilExt</italic> can be used without connected cameras in an offline mode to detect the pupil diameter from externally captured images.</p>
      </caption>
      <graphic xlink:href="fnins-15-676220-g003"/>
    </fig>
    <p>Two cameras are needed for the stereo camera arrangement to detect the absolute pupil diameter directly (<xref ref-type="fig" rid="F3">Figure 3A</xref>). One essential factor in the processing accuracy of such a configuration is the synchronization level between the cameras. Therefore, we synchronized the cameras through an external hardware trigger, leading to a stable system comparable with a professional manufactured commercial solution. Such a hardware trigger is needed to acquire images from both cameras simultaneously. In low-budget systems, the image acquisition is usually made by a software trigger that cannot guarantee synchronized image acquisitions, leading to reduced measurement accuracy. In our proposed system, the trigger signal is generated through a microcontroller, which is automatically controlled by <italic>PupilEXT</italic>. Additionally, we support pupil measurements with a single camera (<xref ref-type="fig" rid="F3">Figures 3B–D</xref>). Here, the integration of a microcontroller for triggering an image acquisition is optional (<xref ref-type="fig" rid="F3">Figure 3B</xref>). However, by including a microcontroller in the one-camera set-up, the duration of a recording session can be set. Note that when using a single camera, the pupil diameter is measured in pixels. Through an extra recording trial with a reference object, the pixel values can be manually converted to millimeters. If cameras are connected to <italic>PupilEXT</italic>, a real-time pupil measurement with one of the six pupil detection algorithms can be carried out. Furthermore, we support the option of recording images without pupil detection. In this way, it is possible to analyze the images in a post-acquisition mode without connected cameras (<xref ref-type="fig" rid="F3">Figure 3E</xref>). In such an offline mode, image sequences from externally recorded cameras can also be loaded, making it possible to leverage the software on already existing pupil image datasets.</p>
    <p>We recommend a NIR illumination unit to avoid corneal light reflections in the eye from the visible spectrum, which could impact the accuracy of pupil detection. For this, a NIR bandpass filter should be mounted in front of the camera’s lens. The advantage of a NIR-based measurement is that the image quality does not suffer in pupil light response experiments. Both the source code of the microcontroller for generating the hardware trigger and the respective NIR circuit board design (<xref ref-type="fig" rid="F3">Figure 3D</xref>) are provided together with the <italic>PupilEXT</italic> software, allowing to set up the system effortlessly. The following subsections deal with the different operational configurations of the platform (<xref ref-type="fig" rid="F3">Figure 3</xref>) and the needed hardware elements in more detail, ensuring the reproducibility of the measurement platform.</p>
    <sec id="S5.SS1">
      <title>Camera Set-Up</title>
      <p>We built a prototype consisting of two Basler acA2040-120um cameras with 50-mm lenses to validate the pupillometry platform in a sample study. The cameras operated in stereo vision mode to measure the absolute pupil diameter. The cameras support a resolution of 2,048 px × 1,536 px with a maximal frame rate of 120 fps. We positioned the system in front of an observer at a working distance of 700 mm, with a baseline distance between the cameras of 75 mm in which the secondary camera has an angle of 8° to the main camera (<xref ref-type="fig" rid="F3">Figure 3A</xref>). A NIR illumination unit, consisting of four LEDs with a peak wavelength of 850 nm (SFH-4715AS), is placed near the subject’s head without obstructing the view of the cameras. Furthermore, the camera lenses are equipped with a high-pass infrared filter (Schneider IF 092 SH) with a transmission range of 747 to 2,000 nm, which should reduce artifacts from the ambient illumination.</p>
      <p>The cameras are connected through their USB 3.0 interface with the computer for data transmission. Additionally, the IO-Pin connector of the cameras is used to adjust the timing, execution and synchronization of the image capturing. A microcontroller (Nucleo STM32F767ZI) is integrated into the pupillometry platform, controlling the cameras’ capturing interval through a shared digital signal.</p>
      <p>For this, the microcontroller transmits a periodic square waveform modulated signal with a voltage amplitude of 3.3 V. Each rising edge of the signal triggers an image (<xref ref-type="fig" rid="F3">Figure 3B</xref>). The frequency and duration of the square wave signal are adjustable through <italic>PupilEXT</italic>, affecting the frame rate and recording time of the camera. While the use of a microcontroller is obligatory when shooting stereo vision, it can be used optionally in the single-camera set-up (<xref ref-type="fig" rid="F3">Figures 3B,C</xref>). Before an absolute pupil measurement can be carried out in stereo vision mode, extrinsic and intrinsic calibrations of the cameras need to be performed in <italic>PupilEXT</italic>.</p>
    </sec>
    <sec id="S5.SS2">
      <title>Embedded Hardware Trigger</title>
      <p>In stereo vision mode, the microcontroller must be connected to the computer so that <italic>PupilEXT</italic> can communicate with the embedded electronic via UART. We have implemented a simple text-based protocol in the microcontroller, for starting and stopping the trigger signal. Control commands can be dispatched via the graphical interface in <italic>PupilEXT</italic> or manually through a serial port terminal application like <italic>CoolTerm</italic> or <italic>HTerm</italic>. If the provided embedded microcontroller source code is not used, users can easily implement the protocol themselves in their preferred microcontroller brand.</p>
      <p>To start a trigger signal, the parameters <italic>COUNT_OF_TRIGGER</italic> and <italic>TIME_TRIGGER_ON</italic> must be set in the protocol. The parameter <italic>COUNT_OF_TRIGGER</italic> indicates how many rising flags should be transmitted. The parameter <italic>TIME_TRIGGER_ON</italic> sets the pulse width in microseconds, which is used to set the sampling rate of the camera. Both parameters are set with the string command <italic>&lt; TxCOUNT_OF_TRIGGERxTIME_TRIGGER_ ON &gt;</italic> via the UART interface of the microcontroller. The “x” term is used as a separator between the parameters. For instance, if a trigger signal should be used for capturing a total of 100 images with a rate of 10 ms, the protocol would correspond to <italic>&lt; Tx100x5000 &gt;</italic>. A detailed introduction of how to flash and install the embedded electronic is provided on the project’s webpage.</p>
    </sec>
  </sec>
  <sec id="S6">
    <title>The Cross-Platform Software Suite</title>
    <p>The core of the pupillometry platform consists of the software <italic>PupilEXT</italic>, structured and implemented based on the requirements of scientifically oriented pupil behavior research. Although pupil measurements can be performed with commercial eye-tracking solutions, the closed system design blocks the transparency of used pupil detection algorithm and the determination of its pupil measurement accuracy. Moreover, such commercial systems are not fully intended for absolute pupil measurements. With <italic>PupilEXT</italic>, we offer not only a free alternative to commercial solutions but also extended features in the topics of pupil detection, measurement resolution, data acquisition, image acquisition, offline measurement, camera calibration, stereo vision, data visualization and system independence, all combined in a single open-source interface.</p>
    <p>It is possible to choose between the six discussed pupil algorithms (<italic>Starburst</italic>, <italic>Swirski</italic>, <italic>ExCuSe</italic>, <italic>ElSe</italic>, <italic>PuRe</italic> and <italic>PuReST</italic>) and to freely adjust their processing parameters and to optimize the pupil contour’s detection accuracy. Additionally, the parameters of a pupil detection method can be reported, leading to an increase in the reproducibility of pupil examinations. We have integrated the pupil detection methods into one unified framework by using a standard pupil detection interface (<xref ref-type="fig" rid="F4">Figure 4</xref>).</p>
    <fig id="F4" position="float">
      <label>FIGURE 4</label>
      <caption>
        <p>UML diagram of the <italic>PupilDetectionMethod</italic> interface used to implement the various pupil detection algorithms. Additionally, the Pupil class is used for collecting a pupil detection result.</p>
      </caption>
      <graphic xlink:href="fnins-15-676220-g004"/>
    </fig>
    <p>For this, the <italic>PupilDetectionMethod</italic> interface is adapted from the <italic>EyeRecToo</italic> eye-tracking software (<xref rid="B124" ref-type="bibr">Santini et al., 2017</xref>), which employs an interface to integrate multiple pupil detection algorithms. It defines a set of abstract methods like <italic>run</italic> and <italic>hasConfidence</italic>, which are concretized through the specific algorithm implementation (<xref rid="B124" ref-type="bibr">Santini et al., 2017</xref>). The <italic>run</italic> method defines the respective pupil detection algorithm that returns a detected pupil from an image. Through <italic>hasConfidence</italic>, we verify the availability of a confidence measure from a respective algorithm. The interface provides a general confidence measure that can be used if an algorithm does not provide its confidence measure (<xref rid="B125" ref-type="bibr">Santini et al., 2018a</xref>). An additional component that is adapted from <italic>EyeRecToo</italic> (<xref rid="B124" ref-type="bibr">Santini et al., 2017</xref>) is the <italic>Pupil</italic> class, which aggregates all data of a detected pupil and its fitted ellipse into one class. A simplified UML diagram of the adapted structure is illustrated in <xref ref-type="fig" rid="F4">Figure 4</xref>.</p>
    <p>In <italic>PupilEXT</italic>, the camera frame rate is adjustable up to 120 Hz. Pupil measurement data are stored in a comma-separated value (CSV) file containing the pupil diameter, confidence measure and ellipse parameters. Besides recording real-time pupil data, the software features storage of raw images for later pupil evaluation. A comprehensive stereo and mono calibration procedure within the software guarantees an accurate and validatable measurement pipeline. The unique feature is the integration of professional industrial cameras with stereo vision capabilities, dedicated to absolute pupil diameter measurements. Metrics are visualized in real-time during pupil measurements, providing an <italic>ad-hoc</italic> evaluation of metrics.</p>
    <sec id="S6.SS1">
      <title>Camera Interface</title>
      <p>Before <italic>PupilEXT</italic> can perform a remote pupil detection, images must be grabbed from the camera(s). We access the Basler cameras with their USB 3.0 interface using a manufacturer-provided programming library called <italic>Pylon</italic>. Through the library, we configure both the camera preferences and activate an image capturing trigger for passing to the image processing pipeline. We distinguish between two image acquisition modes of a camera. With a software trigger, the camera acquisition is controlled over the <italic>Pylon</italic> library interface to record images at a specified frame rate continuously. In the single-camera mode, commonly, the software trigger is used, and the hardware trigger is optional. The hardware trigger is mainly implemented for the stereo vision mode, in which two cameras synchronously capture images upon a receiving a signal flag on an IO-pin. In stereo camera set-ups, the integration of the hardware trigger is obligatory. In such set-ups, a software trigger cannot guarantee that both cameras capture an image at the same time, affecting the performance of a stereo system. Connection establishment and message transmission to the microcontroller is accomplished via a serial port. The microcontroller configuration includes the settings for a camera frame rate as well as the duration of the recording.</p>
      <p>To integrate the camera(s) in <italic>PupilEXT</italic>, a <italic>Camera</italic> interface was created, defining a set of functions for all camera types (<xref ref-type="fig" rid="F5">Figure 5</xref>). Three types of cameras are differentiated: a single camera, a file camera and a stereo camera consisting of the main and secondary cameras (<xref ref-type="fig" rid="F5">Figure 5</xref>). The file camera can be viewed as a camera emulation used in offline pupil detection sessions from previously recorded images retrieved from disk storage. However, by emulating the playback of images as a camera object, it can be integrated seamlessly into existing functions of <italic>PupilEXT</italic>. For the representation of the camera image, the <italic>CameraImage</italic> class is defined (<xref ref-type="fig" rid="F4">Figure 4</xref>). The image distribution in the <italic>PupilEXT</italic> software from the camera(s) is organized with an internal event handler function. For this, the <italic>Pylon</italic> camera library provides an interface that is called every time a corresponding camera acquires a new image. However, for a stereo camera set-up, an image recording consists of two corresponding images that will be delivered by two separate function calls.</p>
      <fig id="F5" position="float">
        <label>FIGURE 5</label>
        <caption>
          <p>UML diagram of the Camera interface and its implementations modeling for different types of cameras in the <italic>PupilEXT</italic> software. The <italic>CameraImage</italic> class is used to represent resulting images and their corresponding metadata.</p>
        </caption>
        <graphic xlink:href="fnins-15-676220-g005"/>
      </fig>
      <p>The initial approach was to leverage a camera internal timestamp to associate the two corresponding images. However, matching the two cameras, internal timestamps of corresponding images led to a buggy image rectification. Therefore, it was necessary to find a more reliable approach. Besides the camera(s) internal timestamp, additional metadata such internal frame count is provided by the <italic>Pylon</italic> API. As long as both cameras start the acquisition simultaneously, the frame counts match. This approach ensures a fixed and reliable order of stereo image acquisitions processed by <italic>PupilEXT</italic>.</p>
    </sec>
    <sec id="S6.SS2">
      <title>Image Recording and Reading for Offline Analysis</title>
      <p>For retrospective detection of the pupil diameter, raw image sequences from the camera can be stored directly on the hard disk. Here, a decision about the format of the images needs to be made. Users can choose between Windows Bitmap (BMP), Tagged Image File Format (TIFF) and JPEG in the preferences of <italic>PupilEXT</italic>. The BMP format represents an uncompressed image format, resulting in large file size. In contrast, JPEG is a lossy compressed format commonly used in consumer photography due to its small size. The TIFF cannot be directly categorized into either of these classes, as it represents an adaptable container that can hold both compressed and uncompressed image formats. A clear-cut decision on which format to use cannot be made easily. While uncompressed formats such as BMP would result in the highest quality of images, the size of data that needs to be handled cannot be underestimated. For the use case of recording images on a disk, one needs to be able to write image data with a rate up to the camera’s maximal frame rate, i.e., 120 fps.</p>
      <p>Given the camera(s) of the reference system with a resolution of 2,048 px × 1,536 px and assuming a bit depth of 8 bits for greyscale images, the resulting image size is ≈3.15 MB. However, with 120 images per seconds, this results in a required writing speed of ≈377.49 MB/s for a single camera and ≈755 MB/s for the stereo set-up. Image size for compressed formats such as JPEG cannot be estimated this easily. Thus, an average image size observed from sample recordings of the reference system is taken. Results are greyscale images with an average size of around 840 kB. Consequently, JPEG requires a writing speed of up to ≈100 MB/s for a single camera and around 200 MB/s in a stereo camera setting. Solely based on the required writing speed without incorporating delays from, i.e., the computational overhead of compression, the speed of traditional hard disk drives (HDDs) is only sufficient for writing JPEG images in a single-camera set-up. More modern hardware in form of SATA 3, solid-state drives (SSDs) can further handle single and stereo camera set-ups for JPEG images, or just a single camera using BMP images. For recent NVMe-based SSDs, the writing speed is theoretically sufficient for writing BMP images in a stereo camera set-up. Note that the discussed rates all referred to the maximal frame rate of 120 fps. Saving images for later analysis is generally recommended for short recordings where the accuracy of the various pupil detection algorithms is of interest.</p>
    </sec>
    <sec id="S6.SS3">
      <title>Pupil Diameter Recording</title>
      <p>Pupil data are recorded in CSV files that store all acquired values of a pupil measurement. Pupil values can be recorded in an online measurement with connected cameras or in an offline measurement in which images are loaded in <italic>PupilEXT</italic> for post-acquisition evaluation. For online measurements, each pupil measurement is associated with a timestamp provided by the system time in milliseconds since Unix epoch, which is synchronized with the camera’s internal hardware clock. In offline measurements, where images are read from files, no timestamp is available. Thus, the corresponding filename is used to associate each measurement. The fitted ellipse can be reconstructed from the stored ellipse parameters: width, height, center position and angle. Further recorded data for analysis are the pupil diameter, circumference and confidence measure. The pupil diameter is stated in pixel by default, and when in stereo mode, it is additionally stated in absolute units.</p>
      <p>Regarding the pupil detection confidence, a value is only available when the applied pupil detection algorithm provides such a measure. However, a second confidence value called outline confidence is provided independently of the used algorithm. This confidence measure is based on the outline contrast of the inner and outer regions of the fitted ellipse (<xref rid="B125" ref-type="bibr">Santini et al., 2018a</xref>). The goal of such value is to describe the reliability of the detected pupil diameter. These measures are useful to directly filter pupil detections that may constitute a false detection or include high uncertainty. Filtering out such detections is a common practice in the pre-processing of pupil detection results (<xref rid="B78" ref-type="bibr">Kret and Sjak-Shie, 2019</xref>). <xref rid="B125" ref-type="bibr">Santini et al. (2018a</xref>, <xref rid="B126" ref-type="bibr">b)</xref> apply a combination of different metrics for their confidence measure. Besides the outline confidence, the ellipse axis ratio and an angular edge spread metric are used. The ellipse axis ratio describes the ratio between major and minor axes of the ellipse, aiming to state the degree of distortion of pupil fit. The angular edge spread measures the spread of the found points on the fitted ellipse. If the points are evenly distributed, it is more likely that they originate from an exact pupil contour. We simplified the accessibility of the data by using a tabular text-based format, i.e., in the form of a CSV file. This format is independent on the used system and is commonly used for measurement recordings.</p>
    </sec>
    <sec id="S6.SS4">
      <title>Camera Calibration</title>
      <p>The goal of the camera calibration is to remove distortions caused by the camera lens and to estimate a projective transformation for mapping world coordinates to image coordinates. A camera projection matrix in the form of <italic>M</italic> = <italic>K</italic>[<italic>R</italic>⋅<italic>T</italic>] is used for mapping. <italic>K</italic> denotes the intrinsic parameter and <italic>R</italic>⋅<italic>T</italic> the extrinsic parameter matrices. The intrinsic matrix <italic>K</italic> projects points in the camera coordinate system to the image coordinate system with the values of the focal lengths (<italic>f</italic><sub><italic>x</italic></sub>, <italic>f</italic><sub><italic>y</italic></sub>) and the optical center (<italic>c</italic><sub><italic>x</italic></sub>, <italic>c</italic><sub><italic>y</italic></sub>) of a camera. These parameters are independent on the viewed scene and are reusable. The extrinsic matrix [<italic>R</italic>⋅<italic>T</italic>] represents the projection of world coordinates to camera coordinates, consisting of a 3 × 3 rotation matrix <italic>R</italic> and the 3 × 1 translation vector <italic>T</italic> (<xref rid="B108" ref-type="bibr">OpenCV, 2020</xref>). By using the camera projection matrix M, an image coordinate <italic>P<sub>c</sub></italic> can be projected into the associated world coordinates <italic>P<sub>W</sub></italic>. Such projection is typically applied in stereo vision, where the camera matrices of two or more cameras are used to estimate the depth and position of a point in world coordinates captured by these cameras. A further application of camera calibration is the correction of lens-induced distortion. Here, two types of distortion exist, radial and tangential distortions. For correcting distortions in a pinhole camera model, the calibration process estimates coefficients representing the distortions in the image, resulting in the five distortion coefficients <italic>C</italic> = (<italic>k</italic><sub>1</sub>, <italic>k</italic><sub>2</sub>, <italic>p</italic><sub>1</sub>, <italic>p</italic><sub>2</sub>, <italic>k</italic><sub>3</sub>).</p>
      <sec id="S6.SS4.SSS1">
        <title>Implementing Single-Camera Calibration</title>
        <p>In <italic>PupilEXT</italic>, we perform the single-camera calibration, e.g., the estimation of the camera parameters <italic>K</italic> with the computer vision library <italic>OpenCV</italic> library and its calibration routines. For this, a total of 30 images are collected with a rate of 0.5 fps, independently from the adjusted camera frame rate. After one image is collected, the depicted calibration pattern is detected, and feature points of the pattern were extracted. Successfully detected feature points and their positions are then stored and visualized in the calibration interface of <italic>PupilEXT</italic>. If the detection was not successful, the image is discarded, and the process will be applied again to the next camera image. This procedure is repeated until the specified number of images is collected. The camera calibration process is performed when enough feature points are collected. This function optimizes the camera parameters by minimizing the reprojection error according to the algorithm of Zhan (<xref rid="B171" ref-type="bibr">Zhang, 2000</xref>). The reprojection error describes the root mean square error (RMSE) distance between the reprojection of the observed feature points using the current camera parameters and their known position in the calibration pattern.</p>
        <p>After successful camera calibration, the quality of the resulting calibration is an essential metric. Its quality is primarily dependent on the accuracy of the detected feature points, which is an edge detection task similar to pupil detection. We report in the <italic>PupilEXT</italic> interface the final reprojection error in the form of the RMSE. However, as this error constitutes a mean squared distance, it may be less intuitive for the user. Therefore, we compute an additional error using the mean absolute error (MAE), measuring the arithmetic mean of the absolute distances between the observed feature points and their projected estimation. The reprojection procedure of the MAE distance is identical to the reprojection error returned by the calibration routine. A set of ideal feature points of the calibration pattern in world coordinates are projected into the image plane using the estimated intrinsic and extrinsic parameters <italic>K</italic>, <italic>R</italic> and <italic>T</italic>. After the projection of the ideal feature point positions into their estimated image coordinates, they can be compared with the actual detected feature points in the captured image. The deviation is stated in the form of the Euclidian distance between the detected and idealized point positions, describing how well the camera parameter approximates the actual camera projection.</p>
      </sec>
      <sec id="S6.SS4.SSS2">
        <title>Validate Single-Camera Calibration</title>
        <p>The reported reprojection error is based on the camera’s projection matrix, optimized for the collected set of images during calibration. Therefore, the reprojection error may contain a bias due to overfitting. For quantifying potential overfitting, an additional verification feature is implemented in <italic>PupilEXT</italic>, performing the same procedure as in the calibration step but using fixed camera parameters. For this, we capture new calibration pattern images during the verification and calculate the reprojection error again, representing an unbiased approximation of the calibration quality. For instance, our prototyped single-camera system (<xref ref-type="fig" rid="F3">Figure 3D</xref>) achieved an RMSE reprojection error of 0.341 px, where values under one pixel are commonly referred to as good calibration quality. For the MAE reprojection error, we achieved a value of 0.041 px, meaning that the average feature point coordinate was projected into the image plane with such a distance error. The verification with a new set of images showed a MAE reprojection error of 0.040 px.</p>
        <p>In <italic>PupilEXT</italic>, the calibration parameters are stored to support the reuse at a later point. For this, a configuration file is saved after a successful calibration is completed. The file contains all essential information to reproduce and assess the state of the camera calibration, such as the attributes of the calibration pattern, the estimated camera parameter matrices and all projection error measures. The functionality of saving and restoring the calibration configuration enables an additional use case, the correction of image distortions in offline pupil measurements.</p>
      </sec>
    </sec>
    <sec id="S6.SS5">
      <title>Stereo Camera Calibration</title>
      <p>Stereo vision offers the possibility of tracking the depth information and absolute pupil size from two or more images captured by cameras of known position. By using the calibration matrices <italic>M<sub>i</sub></italic> of two cameras, it is possible to triangulate image coordinates in both images to their corresponding world coordinates <italic>P<sub>W</sub></italic>. For this, matched points in both images must be found. Therefore, the pupil detection must be applied to images from both cameras (<xref ref-type="fig" rid="F6">Figure 6A</xref>). For absolute pupil size calculation, the ends of the major axis of the ellipse are extracted and triangulated into world coordinates, and their distance was computed through the Euclidian distance (<xref ref-type="fig" rid="F6">Figure 6A</xref>).</p>
      <fig id="F6" position="float">
        <label>FIGURE 6</label>
        <caption>
          <p>Illustration of calculating the absolute pupil diameter with the stereo vision set-up. <bold>(A)</bold> For the corresponding stereo images, two pupil detections are carried out. We use the ellipses of the pupil detections and their minimal encompassing rectangle as feature points for matching. Through triangulation, the corresponding stereo images are transformed into world coordinates. The absolute pupil diameter is calculated with the Euclidian distance between the two world coordinates. <bold>(B)</bold> Procedure of the stereo transformation with the main and second cameras’ images.</p>
        </caption>
        <graphic xlink:href="fnins-15-676220-g006"/>
      </fig>
      <p>Triangulation determines the world position of an image point through its perspective projection in two or more images. Each projection point in an image corresponds to a projection line in world coordinates, representing all possible world coordinate positions that could have projected this point into the image. The projection lines of corresponding points can be used to determine their intersection in world coordinates. <xref ref-type="fig" rid="F6">Figure 6B</xref> shows two corresponding image points of the main and secondary cameras (<italic>d</italic><sub><italic>p</italic>11</sub>, <italic>d</italic><sub><italic>p</italic>21</sub>) and their intersection point <italic>d</italic><sub><italic>pW1</italic></sub> in the world coordinate system. There are two challenges with this approach. First, the corresponding pupil detections in both images are required to retrieve matching points.</p>
      <p>Second, extraction of feature points from a pupil contour may be ambiguous due to blurriness of the edge. If an identical pupil detection in both images cannot be guaranteed, potential deviations can be prevented by detecting and filtering those situations from the data stream. In <italic>PupilEXT</italic>, we use the corners of the minimal encompassing rectangle of the fitted ellipse (<italic>d</italic><sub><italic>p</italic>11</sub>, <italic>d</italic><sub><italic>p</italic>21</sub>) and (<italic>d</italic><sub><italic>p</italic>21</sub>, <italic>d</italic><sub><italic>p</italic>22</sub>) as feature points for triangulation (<xref ref-type="fig" rid="F6">Figure 6A</xref>). The corner points correspond to the major axis of the ellipse for having a more robust feature selection in both images.</p>
      <sec id="S6.SS5.SSS1">
        <title>Implementation of Stereo Vision</title>
        <p>Given the two recognized pupil ellipse results from the main and second cameras (<xref ref-type="fig" rid="F6">Figure 6B</xref>), we check the success of pupil detection and confidence in both images. Naturally, if one of the detections failed, no matching points (<xref ref-type="fig" rid="F6">Figure 6A</xref>) can be extracted or triangulated into the world coordinate system. In valid cases, the feature points (<italic>d</italic><sub><italic>p</italic>11</sub>, <italic>d</italic><sub><italic>p</italic>12</sub>) and (<italic>d</italic><sub><italic>p</italic>21</sub>, <italic>d</italic><sub><italic>p</italic>22</sub>) of both ellipse fits are extracted. Here, the bounding rectangle of the ellipse fit is leveraged, and the corner points from the major axis are extracted (<xref ref-type="fig" rid="F6">Figure 6A</xref>).</p>
        <p>Assuming the calibration parameters of both cameras are available, the paired ellipse image point coordinates (<italic>d</italic><sub><italic>p</italic>11</sub>, <italic>d</italic><sub><italic>p</italic>12</sub>) and (<italic>d</italic><sub><italic>p</italic>21</sub>, <italic>d</italic><sub><italic>p</italic>22</sub>) are corrected for potential distortions using the distortion coefficient matrices. Next, the corresponding image feature points (<italic>d</italic><sub><italic>p</italic>11</sub>, <italic>d</italic><sub><italic>p</italic>21</sub>) and (<italic>d</italic><sub><italic>p</italic>12</sub>, <italic>d</italic><sub><italic>p</italic>22</sub>) are triangulated using the OpenCV function <italic>cv::triangulatePoints</italic>. The triangulation results <italic>P</italic><sub><italic>H</italic>1</sub> and <italic>P</italic><sub><italic>H</italic>2</sub> are represented in homogeneous coordinates, which then are converted into Cartesian coordinates (Eqs. 1 and 2).</p>
        <disp-formula id="S6.E1">
          <label>(1)</label>
          <mml:math id="M1">
            <mml:mrow>
              <mml:mrow>
                <mml:mpadded width="+3.3pt">
                  <mml:msub>
                    <mml:mi>P</mml:mi>
                    <mml:mi>H</mml:mi>
                  </mml:msub>
                </mml:mpadded>
                <mml:mo rspace="5.8pt">=</mml:mo>
                <mml:mrow>
                  <mml:mo>[</mml:mo>
                  <mml:mtable displaystyle="true" rowspacing="0pt">
                    <mml:mtr>
                      <mml:mtd columnalign="center">
                        <mml:msub>
                          <mml:mi>X</mml:mi>
                          <mml:mi>H</mml:mi>
                        </mml:msub>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd columnalign="center">
                        <mml:msub>
                          <mml:mi>Y</mml:mi>
                          <mml:mi>H</mml:mi>
                        </mml:msub>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd columnalign="center">
                        <mml:msub>
                          <mml:mi>Z</mml:mi>
                          <mml:mi>H</mml:mi>
                        </mml:msub>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd columnalign="center">
                        <mml:msub>
                          <mml:mi>W</mml:mi>
                          <mml:mi>H</mml:mi>
                        </mml:msub>
                      </mml:mtd>
                    </mml:mtr>
                  </mml:mtable>
                  <mml:mo>]</mml:mo>
                </mml:mrow>
              </mml:mrow>
              <mml:mo>,</mml:mo>
              <mml:mrow>
                <mml:mrow>
                  <mml:mi>home2cart</mml:mi>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:msub>
                      <mml:mi>P</mml:mi>
                      <mml:mi>H</mml:mi>
                    </mml:msub>
                    <mml:mo rspace="5.8pt">)</mml:mo>
                  </mml:mrow>
                </mml:mrow>
                <mml:mo rspace="5.8pt">=</mml:mo>
                <mml:mrow>
                  <mml:mo>[</mml:mo>
                  <mml:mtable displaystyle="true" rowspacing="0pt">
                    <mml:mtr>
                      <mml:mtd columnalign="center">
                        <mml:mrow>
                          <mml:msub>
                            <mml:mi>X</mml:mi>
                            <mml:mi>H</mml:mi>
                          </mml:msub>
                          <mml:mo>/</mml:mo>
                          <mml:mi mathvariant="normal">ω</mml:mi>
                        </mml:mrow>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd columnalign="center">
                        <mml:mrow>
                          <mml:msub>
                            <mml:mi>Y</mml:mi>
                            <mml:mi>H</mml:mi>
                          </mml:msub>
                          <mml:mo>/</mml:mo>
                          <mml:mi mathvariant="normal">ω</mml:mi>
                        </mml:mrow>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd columnalign="center">
                        <mml:mrow>
                          <mml:msub>
                            <mml:mi>Z</mml:mi>
                            <mml:mi>H</mml:mi>
                          </mml:msub>
                          <mml:mo>/</mml:mo>
                          <mml:mi mathvariant="normal">ω</mml:mi>
                        </mml:mrow>
                      </mml:mtd>
                    </mml:mtr>
                  </mml:mtable>
                  <mml:mo>]</mml:mo>
                </mml:mrow>
              </mml:mrow>
            </mml:mrow>
          </mml:math>
        </disp-formula>
        <disp-formula id="S6.E2">
          <label>(2)</label>
          <mml:math id="M2">
            <mml:mrow>
              <mml:mpadded width="+3.3pt">
                <mml:mi mathvariant="normal">ω</mml:mi>
              </mml:mpadded>
              <mml:mo rspace="5.8pt">=</mml:mo>
              <mml:mrow>
                <mml:mo>{</mml:mo>
                <mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt">
                  <mml:mtr>
                    <mml:mtd columnalign="center">
                      <mml:mrow>
                        <mml:msub>
                          <mml:mi>W</mml:mi>
                          <mml:mi>H</mml:mi>
                        </mml:msub>
                        <mml:mo>,</mml:mo>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd columnalign="center">
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mi>if</mml:mi>
                          <mml:msub>
                            <mml:mi>W</mml:mi>
                            <mml:mi>H</mml:mi>
                          </mml:msub>
                        </mml:mrow>
                        <mml:mo>≠</mml:mo>
                        <mml:mn>0</mml:mn>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr>
                    <mml:mtd columnalign="center">
                      <mml:mrow>
                        <mml:mn>1</mml:mn>
                        <mml:mo>,</mml:mo>
                      </mml:mrow>
                    </mml:mtd>
                    <mml:mtd columnalign="center">
                      <mml:mi>otherwise</mml:mi>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
                <mml:mi/>
              </mml:mrow>
            </mml:mrow>
          </mml:math>
        </disp-formula>
        <p>With the transformed points in the world coordinate system (<italic>d</italic><sub><italic>p</italic><italic>W</italic>1</sub>, <italic>d</italic><sub><italic>p</italic><italic>W</italic>2</sub>), we determine the absolute pupil diameter through the Euclidian distance (<xref ref-type="fig" rid="F6">Figure 6B</xref>). In the experiments, the computation time of this procedure (feature extraction, distortion correction and triangulation) was on average 0.03 ms, which should not significantly influence the maximum possible processing rate of pupil measurements.</p>
        <p>However, no further criteria are applied for checking the reliability of the stereo vision result, as it is left open for the user applying the post-processing procedure. We did not consider a general threshold for pre-filtering to be necessary since the user should have full control over the evaluation of the data. For this, we provide all necessary raw data from both cameras in the recorded CSV file.</p>
      </sec>
      <sec id="S6.SS5.SSS2">
        <title>Calibration of Stereo Vision</title>
        <p>A requirement for the stereo triangulation is the projection matrices <italic>M<sub>i</sub></italic> of both cameras. As discussed in the <italic>Camera Calibration</italic> section, the parameters of a single camera are estimated in the calibration procedure, resulting in the intrinsic parameters of the cameras. As the projection matrix <italic>M</italic> consists of both the intrinsic and extrinsic parameters, the extrinsic parameters are estimated through a OpenCV stereo calibration procedure, which takes the intrinsic parameters of each camera, returning the extrinsic parameter in the form of the rotation matrix <italic>R</italic> and the translation matrix <italic>T</italic>. Thereby (<italic>R</italic>, <italic>T</italic>) describe the relative position and orientation of the main camera with respect to the secondary camera coordinate system (<xref rid="B108" ref-type="bibr">OpenCV, 2020</xref>). After the estimation of these extrinsic parameters, the projecting matrices <italic>M</italic><sub>1</sub>, <italic>M</italic><sub>2</sub> can be calculated with the equation <italic>M</italic> = <italic>K</italic>[<italic>R</italic>⋅<italic>T</italic>]. Notably, in a stereo camera set-up, the main camera is typically selected as the origin of the stereo camera coordinate system. Thus, the projection matrix of the main camera does not apply rotation or translation and is therefore given by <italic>M</italic><sub>1</sub> = <italic>K</italic>⋅[<italic>I</italic>|0], where <italic>T</italic> is replaced with the identity matrix <italic>I</italic> and <italic>R</italic> is replaced with the zero vector.</p>
      </sec>
      <sec id="S6.SS5.SSS3">
        <title>Validate Quality of Calibration</title>
        <p>Similar to the single-camera calibration, the reprojection error is returned as RMSE by the stereo calibration procedure. In stereo vision mode, the reprojection error states the distance between the observed and reprojected feature points combined for both cameras in image coordinates. However, for the user, it would be more useful to be able to assess the quality of the stereo calibration in terms of absolute units. Therefore, we leveraged the predefined size of the calibration pattern to calculate the measurement error of the calibration in absolute units. For this, we measure the absolute square size of, i.e., the chessboard pattern, using the detected feature points from both cameras in the calibration routine. The detected feature points of the calibration pattern are undistorted, stereo triangulated and converted into Cartesian world coordinates.</p>
        <p>Next, the measured square size is compared with the known distance between two corner feature points of the calibration pattern. As a result, we report the calculated error of the stereo camera system in absolute units calculated by the distance deviation between the measured and idealized sizes of the pattern. However, the stated error again could be biased by the overfitting in the calibration routine. Therefore, we implemented a verification routine that checks the absolute measurement error using a new set of images with the calculated projection matrices. Similar to the single-camera mode, the stereo calibration matrix can be saved and loaded into the software for the next usage, reducing new calibration effort. Here, we recommend verifying the old calibration before a pupil measurement is conducted. If the lens settings or camera position are slightly changed, the transformation matrix needs to be re-created by a new calibration procedure. The necessity can be quickly checked using the verification function in <italic>PupilEXT</italic>.</p>
      </sec>
    </sec>
    <sec id="S6.SS6">
      <title>Performance of PupilEXT</title>
      <p>The performance of <italic>PupilEXT</italic> in pupil measurements depends on various factors such as processing power of the system, frame rate of the camera and the applied pupil detection algorithm. As listed in <xref rid="T1" ref-type="table">Table 1</xref>, the runtimes of the pupil detection algorithms vary significantly. For the goal of conducting pupil measurements with a frame rate of 120 fps, a maximal runtime of around 8 ms or less is necessary. Additional computations such as correcting lens distortion can increase the needed computation time per image. We optimize the computational complexity in <italic>PupilEXT</italic> by using a region of interest (ROI), reducing the amount of pixel that needs to be processed. The ROI can be adjusted interactively by the user in the interface.</p>
      <p>In combination with the <italic>PuRe</italic> pupil detection algorithm, we achieved a stable pupil measurement at 120 fps on full images. With manually specified ROI selection, the frame rate can be pushed further, as <italic>PupilEXT</italic> is completely implemented in C++, supported by parallel computation using CPU threads.</p>
    </sec>
    <sec id="S6.SS7">
      <title>The Graphical User Interface of PupilEXT</title>
      <p><xref ref-type="fig" rid="F7">Figure 7</xref> illustrates the GUI of <italic>PupilEXT</italic> during a pupil measurement in the stereo camera mode. Via the taskbar of the GUI (<xref ref-type="fig" rid="F7">Figure 7</xref>, points 1 to 9, blue), the essential function of the software is linked. Before a pupil measurement, the camera mode and the respective cameras must be selected to establish a connection (<xref ref-type="fig" rid="F7">Figure 7</xref>, point 1, blue). In the camera settings also a connection to the microcontroller can be established if a hardware trigger is required. After successful connection to the cameras, a window with a live image view of the cameras is opened. Camera parameters such as gain factor, exposure time or maximum frame rate can be changed at any time via a quick start button (<xref ref-type="fig" rid="F7">Figure 7</xref>, point 3, blue). Next, one of the six pupil algorithms can be selected in the pupil detection preferences (<xref ref-type="fig" rid="F7">Figure 7</xref>, point 1, green). In addition to the algorithms, the parameters of the method can be set to optimize the detection accuracy when necessary (<xref ref-type="fig" rid="F7">Figure 7</xref>, point 3, green). We have provided a preset of parameters that can be selected (<xref ref-type="fig" rid="F7">Figure 7</xref>, point 2, green). In addition to the standard parameters from the original papers, we have added optimized values that are adapted to different ROI sizes. We have set the <italic>PuRe</italic> method as a standard method in <italic>PupilEXT</italic>.</p>
      <fig id="F7" position="float">
        <label>FIGURE 7</label>
        <caption>
          <p>The graphical user interface of the programmed software <italic>PupilEXT</italic> during a pupil recording in stereo camera mode. In the main window’s taskbar, various functions can be accessed for quick actions. In the pupil detection sub-window, a pupil algorithm with respective parameters can be adjusted. The camera calibration can be done directly in <italic>PupilEXT</italic>. Calibration files can be saved and validated to give an outline of the camera system’s edge detection accuracy, which is essential for a pupil measurement pipeline.</p>
        </caption>
        <graphic xlink:href="fnins-15-676220-g007"/>
      </fig>
      <p>The pupil detection of the captured live images can be started with the eye symbol in the main window (<xref ref-type="fig" rid="F7">Figure 7</xref>, point 4, blue). We provided in the live view window a quick action menu (<xref ref-type="fig" rid="F7">Figure 7</xref>, point 1, red), which can be used to adjust the image size, setting the ROI or displaying magnification of the pupil. The ROI features allow placement of a rectangular area over the eye to improve performance further when recordings at a higher frame rate of 120 Hz are needed. Note that for the stereo camera mode, a calibration should be carried out; otherwise, the absolute pupil diameter will not be available. The calibration window can be reached through the taskbar in the main window (<xref ref-type="fig" rid="F6">Figure 6</xref>, point 5, blue).</p>
      <p>In the calibration window (<xref ref-type="fig" rid="F7">Figure 7</xref>, point 4, green), one can select the type of calibration pattern. Next, the calibration can be started, resulting in the calibration file that is saved locally on the hard disk. If a calibration file already exists, it can be loaded via the calibration window (<xref ref-type="fig" rid="F7">Figure 7</xref>, point 4, green), and its validity can be again verified. The stated calibration accuracy can be recorded in a CSV file during the validation procedure.</p>
      <p>After the calibration is completed, the absolute pupil diameter is displayed in the data view, which also lists all tracked pupil values in real-time (<xref ref-type="fig" rid="F7">Figure 7</xref>, point 3, red). Each of these values can be visualized in a real-time plot by selecting the specific value in the data view. For recording the pupil measurements, a disk location can be selected to save the pupil data in a CSV file (<xref ref-type="fig" rid="F7">Figure 7</xref>, point 7, blue). The data can be saved continuously with the recording button (<xref ref-type="fig" rid="F7">Figure 7</xref>, point 8, blue). The raw images can be saved with the blue recording button (<xref ref-type="fig" rid="F7">Figure 7</xref>, item 9, blue) for later offline pupil detection in <italic>PupilEXT</italic>. In the <xref ref-type="supplementary-material" rid="TS1">Supplementary Materials</xref>, we have added hands-on video materials to illustrate the pipeline of usage and the features. Additionally, we offer the feature of creating and loading custom profiles (<xref ref-type="fig" rid="F7">Figure 7</xref>, point 6, blue), which opens the software in a specified state to avoid the workload when <italic>PupilEXT</italic> is started next time.</p>
    </sec>
  </sec>
  <sec id="S7">
    <title>Demonstration of a Measurement Pipeline With PupilExt</title>
    <p>To illustrate the measuring procedure with <italic>PupilEXT</italic>, we performed an exemplary experiment on the wavelength-dependent pupil light response. We recorded the pupil diameter of an observer with six repetitions (trials) using <italic>PupilEXT</italic>, while different light spectra were turned on at a steady luminance. For this, a subject looked into a 700 mm × 700 mm sized homogeneously illuminated observation chamber. The illumination was generated by a custom-made temperature-controlled (30°C ± 0.1°C) multi-channel LED luminaire, which was used to trigger the pupil diameter with chromatic stimulus spectra (<xref rid="B169" ref-type="bibr">Zandi et al., 2020</xref>). Pupil foreshortening error (<xref rid="B58" ref-type="bibr">Hayes and Petrov, 2016</xref>) was minimized by using a chin rest for positioning the subject’s head. Additionally, the gaze point was fixed with a 0.8° sized fixation target (<xref rid="B146" ref-type="bibr">Thaler et al., 2013</xref>) in the middle of the adaptation area. On the left eye’s optical axis, a stereo camera system consisting of two Basler acA2040-120um cameras with 50-mm lenses was set up (<xref ref-type="fig" rid="F3">Figure 3A</xref>).</p>
    <p>The pupil diameter was triggered using chromatic LED spectra with peak wavelengths <bold>λ<sub><bold>P</bold><italic>e</italic><italic>a</italic><italic>k</italic></sub></bold> of 450 nm [full width at half maximum (FWHM): 18 nm, <bold>L</bold> = 100.4 cd/m<sup>2</sup> ± SD 0.23 cd/m<sup>2</sup>) and 630 nm (FWHM: 16 nm, <bold>L</bold> = 101 cd/m<sup>2</sup> ± SD 0.31 cd/m<sup>2</sup>], which were switched on for 30 s. Before each stimulus spectrum, a phosphor-converted white-colored LED with a correlated color temperature of 5,500 K (<bold>L</bold> = 201 cd/m<sup>2</sup> ± SD 0.48 cd/m<sup>2</sup>) was presented to adapt the pupil diameter to its baseline. The order of the chromatic stimulus spectra was randomized. One pupil measurement trial lasted 240 s, as the anchor spectrum (5,500 K) was switched on twice between each chromatic stimulus for 90 s, and the main stimuli (450 and 630 nm) were switched on 30 s. The spectra were measured 20 times before and after the experiment using a Konica Minolta CS2000 spectroradiometer. We controlled the luminaire with a custom-made MATLAB script, which stored the switch-on times of the spectra in a CSV-File. Possible switch-on latency times during the command transmission from MATLAB to the luminaire’s hardware were taken into account by tracking the processing time in the embedded software. We recorded stereo eye images with 30 fps (<sup>∗</sup>.bmp) during each pupil examination trial (240 s), making it possible to detect the pupil diameter from the images with different detection algorithms, later on using the offline pupil analysis mode of <italic>PupilEXT</italic>. The pupil data were synchronized with the luminaire’s switch-on times afterward using a MATLAB script.</p>
    <sec id="S7.SS1">
      <title>Pre-processing the Measured Raw Data</title>
      <p>Recorded raw pupil data are usually occupied by artifacts or other non-physiological pupil changes that need to be pre-processed (<xref ref-type="fig" rid="F8">Figure 8A</xref>). For the pupil data recorded by <italic>PupilEXT</italic>, we recommend a two-step filtering procedure. First, every data point that has an outline confidence measure (<xref rid="B125" ref-type="bibr">Santini et al., 2018a</xref>) lower than 1 should be left. With this step, artifacts caused by eye blinks are detected robustly (<xref ref-type="fig" rid="F8">Figure 8A</xref>). Other artifacts can occur if the matching points (<xref ref-type="fig" rid="F6">Figure 6B</xref>) between the first and second cameras differ, resulting in a non-physiological shift of the pupil diameter, visible through slight peaks in the data. We identify matching point errors by comparing the stated axis ratio of the ellipses between the main and second cameras. The axis ratios differ because of the second camera’s positioning causing a perspective pupil area change. However, the ellipse axis ratio difference between the ellipses of cameras 1 and 2 should remain constant within a certain range. Thus, the reliability of the matching points (<xref ref-type="fig" rid="F6">Figure 6B</xref>) can be detected by calculating the difference of the axis ratio across the data points and removing all strong outliers from the sample dataset (<xref ref-type="fig" rid="F8">Figure 8A</xref>). We have pre-processed the recorded pupil data according to this two-step procedure. The results of one raw pupil measurement trial (240 s) using the <italic>PuRe</italic> algorithm and respective pre-processed pupil data are shown in <xref ref-type="fig" rid="F8">Figure 8B</xref>. Eye blinks can approximately be tracked by identifying the outline confidence areas that fall below one. However, an eye-blink detection via the outline confidence measure can only work if the algorithm’s detection rate is robust; i.e., the pupil is detected in more than 90% of valid eye image cases. We implemented the proposed two-step pre-processing method in MATLAB. The script is available on the GitHub repository of the <italic>PupilEXT</italic> project. Additionally, the recorded eye images are made available online together with the stereo calibration file. The data can directly be loaded into <italic>PupilEXT</italic> for a hands-on experience.</p>
      <fig id="F8" position="float">
        <label>FIGURE 8</label>
        <caption>
          <p>Recorded pupil data and proposed pre-processing procedure of pupil diameter data collected by <italic>PupilEXT</italic>. <bold>(A)</bold> A two-step pre-processing procedure is proposed, which uses the outline confidence and the axis ratio of the cameras’ tracked ellipses. <bold>(B)</bold> Recorded pupil data from our sample experiment to illustrate the performance of <italic>PupilEXT</italic>. The outline confidence can be used to identify eye occlusions in the data approximately. The two-step pre-processing can remove artifacts and other unnatural physiological pupil diameter changes.</p>
        </caption>
        <graphic xlink:href="fnins-15-676220-g008"/>
      </fig>
    </sec>
    <sec id="S7.SS2">
      <title>Comparison of the Pupil Detection Approaches</title>
      <p>A majority of pupil detection algorithms was evaluated based on their accuracy in estimating the pupil center (<xref rid="T1" ref-type="table">Table 1</xref>), as they are mainly intended for eye-tracking applications. One of the works evaluating the pupil fit was <xref rid="B142" ref-type="bibr">Świrski et al. (2012)</xref> in which their approach was compared against the Starburst algorithm. The pupil fit was assessed utilizing hand-labeled pupil measurements and the Hausdorff distance. The Hausdorff distance (<xref rid="B119" ref-type="bibr">Rote, 1991</xref>) thereby describes the maximum Euclidean distance of one ellipse to any point on the other ellipse (<xref rid="B142" ref-type="bibr">Świrski et al., 2012</xref>). Results show that the <italic>Swirski</italic> algorithm improves the detection rate for a five-pixel error threshold from 15% for Starburst to 87%, showing that not every eye-tracking algorithm is suited for pupil measurements. <xref rid="B45" ref-type="bibr">Fuhl et al. (2015)</xref> evaluated the <italic>ExCuSe</italic> algorithm, comparing their approach with the <italic>Swirski</italic> and <italic>Starburst</italic> algorithms. However, only the distance between the pupil center estimation and ground-truth was evaluated. The evaluation was performed on 18 datasets of pupil images captured under highly challenging real-world conditions. The detection rate for a five-pixel error threshold shows an average rate of 17% for <italic>Starburst</italic>, 40% for <italic>Swirski</italic> and 63% for <italic>ExCuSe</italic>.</p>
      <p>A similar evaluation was repeated in the works of <italic>ElSe</italic> (<xref rid="B48" ref-type="bibr">Fuhl et al., 2016a</xref>), <italic>PuRe</italic> (<xref rid="B125" ref-type="bibr">Santini et al., 2018a</xref>), and <italic>PuReST</italic> (<xref rid="B126" ref-type="bibr">Santini et al., 2018b</xref>), where they conducted evaluations using overlapping datasets and the pupil center distance as a performance value. Within a five-pixel error threshold, the algorithm of Starburst shows a detection rate of 13.44, 28 to 36% for <italic>Swirski</italic>, 50 to 58% for <italic>ExCuSe</italic>, 66 to 69% for <italic>ElSe</italic>, 72% for <italic>PuRe</italic> and 87% for <italic>PuReST</italic>. In these evaluations, a performance loss for highly challenging recorded images was observed. Specifically, images with low-intensity contrast and pupils containing small reflections impaired the pupil detection algorithms. Santini et al. showed that the average runtime of the <italic>PuReST</italic> algorithm is 1.88 ms, compared with <italic>PuRe</italic> with 5.17 ms (<xref rid="B126" ref-type="bibr">Santini et al., 2018b</xref>), making <italic>PuReST</italic> the fastest approach with the highest pupil center detection rate. Note that these results apply to images that do not occur under laboratory conditions. <xref rid="B150" ref-type="bibr">Topal et al. (2017)</xref> evaluated the <italic>APPD</italic> algorithm (<xref rid="B150" ref-type="bibr">Topal et al., 2017</xref>) together with <italic>Starburst</italic>, <italic>ElSe</italic> and <italic>Swirski</italic>. The pupil fit and processing time were used to quantify the performance of the algorithms. For the pupil fit, the pupil localization was used, which quantifies the overlap ratio between the detected ellipse and the ground-truth, stated as [0, 1]. The results indicate a high pupil localization of 0.97 for <italic>APPD</italic> compared with 0.93 for <italic>Swirski</italic>, 0.92 for <italic>ElSe</italic> and 0.77 for Starburst. Additionally, Topal et al. measured an average computation time of 5.37 ms for <italic>APPD</italic>, 7.12 ms for <italic>Else</italic> (7 ms), 47.17 ms for <italic>Swirski</italic> (3.77 ms) and 49.22 ms for <italic>Starburst</italic> (100 ms). The numbers in parentheses define the originally reported runtime of the respective algorithms.</p>
      <p>Based on the literature, it can be stated that <italic>PuReST</italic> is the top performer when evaluating the pupil’s center detection rate with highly noisy images. However, these results represent the detection rate with a five-pixel error threshold and do not state the accuracy of their pupil size measurements. Only the evaluation of <xref rid="B150" ref-type="bibr">Topal et al. (2017)</xref>, <xref rid="B142" ref-type="bibr">Świrski et al. (2012)</xref> carried out a performance test on the pupil fit. Their results state a different picture, with <italic>Swirski</italic> performing better than <italic>ElSe</italic>.</p>
      <p>Another aspect that could significantly affect the performance of a pupil detection algorithm is the parameters’ count. Each algorithm has a set of parameters that need to be tuned by the user to match the image composition. Selecting appropriate values may constitute a challenge for the user. Thus, the fewer parameters an algorithm possesses, the simpler its application. Comparing the number of parameters of the pupil detection algorithms, <italic>Swirski</italic> includes 11, followed by <italic>Starburst</italic> with five and <italic>PuRe</italic> and <italic>PuReST</italic> with three. <italic>Else</italic> and <italic>ExCuSe</italic> have only two parameters. We have stored in our proposed software <italic>PupilEXT</italic> the standard values of the algorithms as stated by the authors and additionally optimized three sets of parameters for pupil measurement applications under different image compositions.</p>
    </sec>
    <sec id="S7.SS3">
      <title>Validation of the Pupil Detection Algorithms</title>
      <p>We evaluated the captured eye images from our pupil experiment using the six available pupil detection algorithms in <italic>PupilEXT</italic>. Ideally, the pupil diameter should remain steady across the detection algorithms, as the same eye image sets were used for evaluation. However, due to the algorithms’ different parameters settings and approaches, the measured diameter may differ. In <xref ref-type="fig" rid="F9">Figure 9A</xref>, we have plotted the detected raw pupil diameter from one experimental trial (240 s) to illustrate how differently the algorithms perform based on the same acquired image set. For each raw data plot panel, the respective pre-processed pupil data are illustrated, which were obtained using the proposed two-step method. The <italic>ElSe</italic>, <italic>ExCuSe</italic>, <italic>PuRe</italic> and <italic>PuReST</italic> algorithms achieved an acceptable pupil detection rate, visually noticeable through the lower number of artifacts in the respective raw dataset (<xref ref-type="fig" rid="F9">Figure 9A</xref>). As discussed, the artifacts in the raw data can be filtered by removing the detected pupil diameter with an outline confidence of less than 1. In <xref ref-type="fig" rid="F9">Figure 9B</xref>, we illustrated a sample of recorded pupil images with the respective outline confidence, showing that an invalid pupil fit can be detected and removed when using such a metric.</p>
      <fig id="F9" position="float">
        <label>FIGURE 9</label>
        <caption>
          <p>Comparison of the pupil detection algorithms based on the same eye image set and visualization of the pupil ellipse fit as a function of the outline confidence. <bold>(A)</bold> Eye images from one subject were recorded during a chromatic pupillometry experiment using <italic>PupilEXT</italic>. The pupil was exposed to LED spectra of the peak wavelengths 450 nm (L = 100.4 cd/m<sup>2</sup> ± SD 0.23) and 630 nm (L = 101 cd/m<sup>2</sup> ± SD 0.31) for 30 s. An anchor spectrum with a correlated color temperature (CCT) of 5,500 K (L = 201 cd/m<sup>2</sup> ± SD 0.48) was turned on for 90 s between each stimulus. The pupil diameter from the recorded images was extracted using the available algorithms in <italic>PupilEXT</italic> and pre-processed to illustrate the algorithms’ detection differences. <bold>(B)</bold> For each detected diameter, an outline confidence measure is provided and used as an indicator to filter unreliable pupil fits from the dataset. Pupil fits from different measurement sessions are illustrated as a function of the outline confidence. We recommend discarding all pupil diameters with a lower outline confidence measure of 1.</p>
        </caption>
        <graphic xlink:href="fnins-15-676220-g009"/>
      </fig>
      <p>The <italic>Starburst</italic> algorithm caused a higher number of artifacts. Subsequent pre-processing of the raw data using the two-step method was not helpful, as the <italic>Starburst</italic> algorithm caused too many false detections. The <italic>Swirski</italic> algorithm had difficulties in detecting small pupil diameter at the 450-nm stimulus. After the invalid pupil data were filtered from the 450-nm time frame, there were almost no valid data left for linearly interpolating the missing values. Also, the <italic>Swirski</italic> algorithm had no robust detection rate for the pupil recording with the 630-nm spectrum. However, the cameras’ lenses were equipped with optical IR-high-pass filters so that the spectral-dependent detection quality was not due to the type of light spectrum. Each pupil detection algorithm has a certain number of parameters that need to be adjusted depending on the image resolution or how large the pupil is in relation to the image size. An incorrect combination of parameters could affect the pupil detection at differently sized diameters, as the algorithm itself could rule out smaller pupils.</p>
      <p>The proposed technique for detecting eye blinks based on an outline confidence (<xref ref-type="fig" rid="F8">Figure 8B</xref>) is highly affected by the detection rate. For example, it is no longer possible to distinguish between a false pupil fit or a closed eyelid at a higher rate of pupil detection artifacts (<xref ref-type="fig" rid="F9">Figure 9A</xref>). Additionally, the <italic>ExCuSe</italic> algorithm offers a threshold value that can be used to detect eye blinks. In this way, values that indicate a closed eyelid will automatically be removed by the respective pupil detection algorithm itself, leading to the fact that a subsequent analysis of eye blinks is no longer possible. Therefore, an eye-blink recognition using the outline confidence seems to work well only with <italic>PuRe</italic> and <italic>PuReST</italic>.</p>
      <p>In <xref ref-type="fig" rid="F10">Figure 10</xref>, we calculated the average percentage of the invalid data rate for each algorithm and spectrum separately to illustrate the pupil detection algorithms’ performance across the conducted pupil measurement trials. The invalid data rate is defined as the number of diameter values that had to be removed from the raw dataset when using the two-step pre-processing approach (<xref ref-type="fig" rid="F8">Figure 8B</xref>). The <italic>ElSe</italic>, <italic>ExCuSe</italic>, <italic>PuRe</italic> and <italic>PuReST</italic> algorithms had a lower invalid data rate of 10%, indicating good detection performance across all measurement trials (<xref ref-type="fig" rid="F10">Figure 10</xref>). The <italic>Starburst</italic> algorithm failed to perform a valid pupil fit at 450 nm in 58.46% SD 5.93% of cases. At the second reference spectrum (5,500 K), the pupil detections from <italic>Starburst</italic> failed in 36.82% SD 7.1% of the cases. Since the invalid pupil detection rate was higher than 10% for every stimulus spectrum, we assume that the performance of <italic>Starburst</italic> is independent of the parameter settings; possibly, the false pupil fits arise due to the contrast or resolution in the eye image. The <italic>Swirski</italic> algorithm’s performance suffered mainly at the 450-nm stimulus with an average error rate of 81.09% SD 10.10%. This behavior seems to be due to the algorithm’s parameters adjustments, as the invalid data rate is higher for smaller pupil diameters. Our results are in line with previous benchmarks from the literature, which showed that the <italic>Starburst</italic> and <italic>Swirski</italic> algorithms had lower detection rates (<xref rid="B45" ref-type="bibr">Fuhl et al., 2015</xref>, <xref rid="B48" ref-type="bibr">2016a</xref>; <xref rid="B125" ref-type="bibr">Santini et al., 2018a</xref>, <xref rid="B126" ref-type="bibr">b</xref>). Note that the <italic>Swirski</italic> algorithm could have a better pupil fit, as it does not downscale the eye images before processing (<xref rid="B142" ref-type="bibr">Świrski et al., 2012</xref>; <xref rid="B150" ref-type="bibr">Topal et al., 2017</xref>). Since the <italic>Swirski</italic> algorithm has 11 free parameters that need to be adjusted, it is not a practical algorithm in our view because the detection method could suffer its robustness when using the wrong settings. The advantage of the pupil algorithms <italic>ElSe</italic>, <italic>ExCuSe</italic>, <italic>PuRe</italic> and <italic>PuReST</italic> is the smaller number of parameters that need to be set, leading to less error-proneness and practicability in conducting pupil measurements.</p>
      <fig id="F10" position="float">
        <label>FIGURE 10</label>
        <caption>
          <p>Percentage of invalid pupil fits inside the raw data, and the averaged steady-state pupil diameter of the last 5 s. Pupil data are from one subject with six repetitions in each condition. <bold>(A)</bold> Mean of the invalid data point count (outline confidence &lt; 1) in per cent for each algorithm and used spectrum. A higher invalid data rate indicates that more raw data need to be removed due to inaccurate pupil fits. The <italic>Starburst</italic> algorithm did not provide an adequate detection rate, mainly observed at 450 nm (57.46% ± SD 5.93%) and the second reference spectrum (36.82% ± SD 7.1%). The <italic>Swirski</italic> algorithm showed a significant invalid detection rate of 81.09% ± SD 10.10% at 450 nm. Since the cameras were equipped with an IR-high-pass filter, the spectral-dependent pupil detection rate is mainly due to the differently sized pupil diameter. Due to the increased number of parameters from <italic>Starburst</italic> and <italic>Swirski</italic>, a generalization for small and large pupil diameters is more challenging. <bold>(B)</bold> The temporal pupil diameter was averaged over the last 5 s to compare how differently the pupil algorithms evaluate the same dataset. The <italic>ElSe</italic> and <italic>ExCuSe</italic> algorithms have approximately the same pupil diameter in all scenarios, which is due to the computation method’s similarities. The same applies to the <italic>PuRe</italic> and <italic>PuReST</italic> algorithms. The measured pupil diameter’s uncertainty range is on average 0.05 mm ± SD 0.004 mm, originating from the different detection results with the same dataset.</p>
        </caption>
        <graphic xlink:href="fnins-15-676220-g010"/>
      </fig>
      <p>To better estimate how much the pupil diameter deviates depending on the used pupil algorithm, we evaluated the acquired eye images with the top-performing algorithms (<italic>ElSe</italic>, <italic>ExCuSe</italic>, <italic>PuRe</italic> and <italic>PuReST</italic>) and calculated the steady-state equilibrium pupil diameter. For this, we calculated the pupil diameter’s mean value over the last 5 s of a measurement. <xref ref-type="fig" rid="F10">Figure 10B</xref> shows the steady-state pupil diameters from the six measurement trials. The scatter within a pupil algorithm is due to the pupil diameter’s intrasubject variability, which is mainly induced by cognitive effects and can be up to 0.5 mm (<xref rid="B169" ref-type="bibr">Zandi et al., 2020</xref>; <xref rid="B168" ref-type="bibr">Zandi and Khanh, 2021</xref>). The absolute mean pupil diameter differences between the <italic>ElSe</italic> and <italic>ExCuSe</italic> algorithms are negligible with 6 ⋅ 10<sup>–4</sup> mm at 450 nm and 0.0041 mm at 630 nm, which are due to the same detection approaches. The same was applied for the <italic>PuRe</italic> and <italic>PuReST</italic> algorithms with an absolute mean diameter difference of 0.0061 mm at 450 nm and 0.0051 mm at 630 nm. The <italic>PuReST</italic> algorithm was an extension of <italic>PuRe</italic>, allowing faster pupil detections and explaining the similar pupil fits. However, the mean difference between the algorithm groups <italic>ElSe</italic>/<italic>ExCuse</italic> and <italic>PuRe</italic>/<italic>PuReST</italic> is 0.054 mm SD 0.0043 mm. This is particularly interesting because it indicates how much the measured pupil diameter can deviate when different detection method approaches are applied to the same eye image set. Therefore, in cognitive studies in which the pupil diameters’ mean difference is less than 0.1 mm, we highly recommend reporting the algorithm and respective parameter settings. The parameters that we used for our pupil detection experiments are stored in the <italic>PupilEXT</italic> software and also available on the GitHub repository of this project.</p>
    </sec>
    <sec id="S7.SS4">
      <title>Determining the Pupil Measurement Accuracy</title>
      <p>The accuracy of the pupil measurement can by characterized with <italic>PupilEXT</italic> by two approaches. First, the validation process of the stereo calibration determines the quality of the system, indicated by the reprojection error in MAE within <italic>PupilEXT</italic>. However, such a metric does not include the inaccuracies caused by pupil detection methods. Therefore, it is advisable for checking the validity of the system by a circular formed reference object. For this, we placed a reference object of known size (5 mm) in front of the subject’s eye and determined the diameter using a pupil detection algorithm in <italic>PupilEXT</italic> (<xref ref-type="fig" rid="F11">Figure 11</xref>).</p>
      <fig id="F11" position="float">
        <label>FIGURE 11</label>
        <caption>
          <p>Determination of the used stereo camera system’s measurement accuracy. A reference object of known size (5 mm) was placed in front of the observers’ occluded eye. The diameter of the object was tracked using the <italic>PuRe</italic> algorithm. Based on the raw data, a mean absolute error (MAE) of 0.014 mm can be determined. However, the raw data contain artifacts that we have removed using the proposed two-step pre-processing approach. After pre-processing, we can state a measurement accuracy of 0.0059 mm (MAE).</p>
        </caption>
        <graphic xlink:href="fnins-15-676220-g011"/>
      </fig>
      <p>The measured raw data of the reference object showed a MAE of 0.014 mm. After pre-processing the data with the two-step method, a MAE accuracy of 0.0059 mm was achieved with our prototyped system. It should be noted that such a measurement accuracy is still an idealized approximation since the reference object was kept still without interference. After pre-processing, isolated peaks remained with an amplitude of 0.1 mm. However, remaining pupil data are usually smoothed, making such remaining isolated peaks negligible.</p>
    </sec>
    <sec id="S7.SS5">
      <title>Limitations of the Proposed Pupillometry Toolbox</title>
      <p>The current version of <italic>PupilEXT</italic> offers a comprehensive solution for pupillometry. However, the software is not designed for two-eye measurements, as only one eye at the same time can be captured. We recommend positioning the ROI in the live view of <italic>PupilEXT</italic> software over one eye to let the algorithms iterate inside the specified region if two eyes are visible in the image. Furthermore, an online pupil measurement can only be carried out with Basler branded cameras. In the future, the integration of other camera brands is possible through the implemented camera class. However, externally acquired images from other camera brands can be loaded into <italic>PupilEXT</italic> for offline pupil detections, making it possible to use the software even without purchasing a Basler camera.</p>
      <p>Currently, the implemented pupil algorithms perform their computations on the CPU. Therefore, we recommend using the <italic>PuRe</italic> or <italic>PuReST</italic> algorithm for real-time pupil measurements with a higher frame rate between 60 and 120 fps, as the detection approaches shine with low processing times. In the future, it would be desirable to perform calculations directly on a graphics processing unit (GPU) during an online measurement, making higher frame rates for all integrated pupil detection methods possible. Note that we did not implement a limiting threshold of the frame rate level inside the <italic>PupilEXT</italic> software. The frame rate is limited by the respective pupil detection algorithm’s processing time, which can vary depending on the used computer. If the frame rate is too high for the computer during an online pupil measurement, the images will be stored in the machine’s memory buffer and fed to the pupil algorithm one by one. In such cases, there is the risk that the working memory will overflow when operating <italic>PupilEXT</italic> for longer times in such a mode. Therefore, the camera fps should ideally be on the same level as the processing fps. Both metrics are stated in the live view panel of <italic>PupilEXT</italic>. Note that on our computer (Intel Core i7-9700K), we performed pupil measurements in stereo mode at 120 Hz without any issues when using the <italic>PuReST</italic> or <italic>PuRe</italic> algorithm. Even higher frame rates are possible in the single-camera mode because only the image from one camera has to be processed. Alternatively, eye images can be captured on the disk for later pupil detection, allowing higher frame rates. This function is available for both mono and stereo camera modes.</p>
    </sec>
  </sec>
  <sec id="S8">
    <title>Discussion</title>
    <p>The idea of replacing commercial systems with open-source solutions is currently pushed by working groups topically working on eye-tracking devices (<xref rid="B124" ref-type="bibr">Santini et al., 2017</xref>; <xref rid="B3" ref-type="bibr">Arvin et al., 2020</xref>). The advantage of eye-tracking research is that standardized metrics exist that reflect the accuracy of a detected gaze point (<xref rid="B62" ref-type="bibr">Holmqvist et al., 2012</xref>). In pupillometry research, metrics on the pupil fit’s measurement accuracy is usually not stated, mainly because most applied systems do not allow manual verification after conducted experiments. The lack of missing pupil fit metrics in commercial eye-tracking systems applied for pupil measurement motivated recent works, attempting to develop procedures or provide at least pupil measurement error information of widely used systems (<xref rid="B76" ref-type="bibr">Klingner, 2010</xref>; <xref rid="B49" ref-type="bibr">Gagl et al., 2011</xref>; <xref rid="B19" ref-type="bibr">Brisson et al., 2013</xref>; <xref rid="B58" ref-type="bibr">Hayes and Petrov, 2016</xref>; <xref rid="B107" ref-type="bibr">Murray et al., 2017</xref>; <xref rid="B158" ref-type="bibr">Wang et al., 2017</xref>; <xref rid="B148" ref-type="bibr">Titz et al., 2018</xref>; <xref rid="B30" ref-type="bibr">Coyne et al., 2019</xref>). Mathematically, the pupil center’s accuracy detection is just an indicator for a good pupil fit but does not ensure it. For example, the pupil center can be correct for cases in which the gaze point differs from the camera’s optical axis (eye rotation), but the detected pupil diameter can be estimated incorrectly due to the perspective distortion of the pupil image (pupil foreshortening error) (<xref rid="B58" ref-type="bibr">Hayes and Petrov, 2016</xref>). Additionally, it is not directly possible to reproduce the pupil fit’s accuracy from the pupil center accuracy, which is mainly stated in the datasheet of eye-tracking devices. Suppose studies indicate an effect on the pupil diameter of 0.5 mm. In that case, ideally, there should be a procedure to verify that both the camera system and the applied pupil detection method can detect such small diameter margins. For example, the recently published work “Standards in Pupillography” (<xref rid="B74" ref-type="bibr">Kelbsch et al., 2019</xref>) rarely paid attention to possible technical- and software-induced measurement errors, although this could highly affect the validity and conclusions of research results. By comparing the pupil detection algorithms, we showed that a measurement error of up to 0.05 mm could occur with identical eye images, induced solely by the type of used detection algorithm itself. In commercial systems where it is usually unknown which pupil detection algorithm is applied, comparisons between study results in such a measurement range are difficult. Therefore, the camera’s spatial resolution specification or the pupil center’s measurement accuracy is insufficient for pupil measurements. From our perspective, a uniform measurement platform is essential for pupillometry, ensuring comparability and reproducibility. By verifying our proposed <italic>PupilEXT</italic> set up with a reference object, we offer the possibility to test and state the accuracy of the pupil’s fit directly. Furthermore, the proposed system ensures reproducing pupil examinations results by using the captured images in the offline analysis mode of <italic>PupilEXT</italic>.</p>
    <p>With <italic>PupilEXT</italic>, we have developed the first freely accessible integrated end-to-end pupil measurement platform consisting of hardware and software for professional pupillometry research in vision science. Pupil measurement can be carried out in a one- or two-camera mode. The calibration and validation procedure in <italic>PupilEXT</italic> are intended to provide a transparent way in reporting the measurement accuracy of a conducted pupil study. The specification of measurement accuracies is currently a major issue in pupil research since only in few publications is the validity of the pupil tracking’s accuracy stated. This is mainly due to the use of commercial systems that usually do not support validation procedures of pupil measurement pipelines. The complete software, embedded code and printed circuit board (PCB) layout of the NIR illumination are provided as an open-source project. We provide three <xref ref-type="supplementary-material" rid="TS1">Supplementary Videos</xref> to illustrate the handling of <italic>PupilEXT</italic>. The instruction, details about the installation and video tutorials can be found at the project’s website (see text footnote 1).</p>
    <p>As a next step, it is planned to add a gaze calibration routine to <italic>PupilEXT</italic> to support eye-tracking applications. Currently, we only support Basler branded cameras, but it is possible to add additional industrial camera types into <italic>PupilEXT</italic> since the camera access is separated from the core of the proposed software. The feature of determining the pupil diameter from externally captured images could perhaps make <italic>PupilEXT</italic> a standardized measurement suitable for pupil research. For this aim, we will investigate in the next studies the tracking accuracy of the integrated pupil algorithms with ground-truth images, leading to a better estimation of real-world inaccuracies under laboratory conditions.</p>
  </sec>
  <sec sec-type="data-availability" id="S9">
    <title>Data Availability Statement</title>
    <p>The software PupilEXT as well the embedded program of the microcontroller, and PCB layout of the NIR illumination are available on the following GitHub page: <ext-link ext-link-type="uri" xlink:href="https://github.com/openPupil/Open-PupilEXT">https://github.com/openPupil/Open-PupilEXT</ext-link>.</p>
  </sec>
  <sec id="S10">
    <title>Ethics Statement</title>
    <p>The studies involving human participants were reviewed and approved by Technical University of Darmstadt. The patients/participants provided their written informed consent to participate in this study. Written informed consent was obtained from the individual(s) for the publication of any potentially identifiable images or data included in this article.</p>
  </sec>
  <sec id="S11">
    <title>Author Contributions</title>
    <p>BZ had the initial idea, supervised the project, and designed the first concept of the pupillometry platform. ML was the core software developer with contributions and supervision of BZ. BZ wrote the original draft of the manuscript with contributions of ML. ML provided the literature research and summary of existing pupil detection algorithms under the supervision of BZ. BZ created the figures with contributions of ML and AH. BZ developed and programmed the hardware trigger concept. GS, AH, TK, ML, and BZ performed review and editing of the original draft. GS, AH, TK, ML, and BZ developed the testing methodology. ML and BZ performed testing. All authors read and agreed to the submitted version of the manuscript.</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – 450636577.</p>
    </fn>
  </fn-group>
  <ack>
    <p>We thank the German Research Foundation (DFG) by funding the research (Grant Number 450636577). This project was made possible by the outstanding previously published open-source projects in the field of pupil detection and eye tracking. Therefore, we would like to thank the authors of the ground-breaking algorithms <italic>PuRe</italic>, <italic>PuReST</italic>, <italic>ElSe</italic>, <italic>ExCuSe</italic>, <italic>Starburst</italic> and <italic>Swirski</italic>, who made their methods available to the public. We have to thank Wolfgang Fuhl, Thiago Santini, Thomas Kübler, EK, Katrin Sippel, Wolfgang Rosenstiel, Li, D. Winfield, D. Parkhurst, Lech Swirski, Andreas Bulling, and Neil Dodgson for their open-source contributions, which are part of <italic>PupilEXT</italic>. Additionally, we would like to thank the outstanding developers of the software <italic>EyeRecToo</italic>, whose open-source eye-tracking software inspired us for this work. We used the implementation of the <italic>EyeRecToo</italic>’s pupil class and the integrated detection methods for <italic>PupilEXT</italic>. We appreciate the contributions of Paul Myland, who supported us as a co-supervisor in a bachelor thesis, which topically worked on one part of this project. We highly welcome the contribution of Mohammad Zidan for the mechanical construction of the stereo camera system and the NIR illumination, which was done during his bachelor thesis, supervised by BZ. Finally, we would like to thank Felix Wirth and Thomas Lautenschläger, who joined us as student assistants in the initial phase of the project.</p>
  </ack>
  <fn-group>
    <fn id="footnote1">
      <label>1</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="https://github.com/openPupil/Open-PupilEXT">https://github.com/openPupil/Open-PupilEXT</ext-link>
      </p>
    </fn>
  </fn-group>
  <sec id="S14" sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fnins.2021.676220/full#supplementary-material">https://www.frontiersin.org/articles/10.3389/fnins.2021.676220/full#supplementary-material</ext-link></p>
    <supplementary-material content-type="local-data" id="TS1">
      <media xlink:href="Table_1.DOCX">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>A. E.</given-names></name><name><surname>Martial</surname><given-names>F. P.</given-names></name><name><surname>Lucas</surname><given-names>R. J.</given-names></name></person-group> (<year>2019</year>). <article-title>Form vision from melanopsin in humans.</article-title>
<source><italic>Nat. Commun.</italic></source>
<volume>10</volume>
<fpage>1</fpage>–<lpage>10</lpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-10113-3</pub-id>
<?supplied-pmid 31118424?><pub-id pub-id-type="pmid">30602773</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aminihajibashi</surname><given-names>S.</given-names></name><name><surname>Hagen</surname><given-names>T.</given-names></name><name><surname>Andreassen</surname><given-names>O. A.</given-names></name><name><surname>Laeng</surname><given-names>B.</given-names></name><name><surname>Espeseth</surname><given-names>T.</given-names></name></person-group> (<year>2020</year>). <article-title>The effects of cognitive abilities and task demands on tonic and phasic pupil sizes.</article-title>
<source><italic>Biol. Psychol.</italic></source>
<volume>156</volume>:<issue>107945</issue>. <pub-id pub-id-type="doi">10.1016/j.biopsycho.2020.107945</pub-id>
<?supplied-pmid 32889001?><pub-id pub-id-type="pmid">32889001</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arvin</surname><given-names>S.</given-names></name><name><surname>Rasmussen</surname><given-names>R.</given-names></name><name><surname>Yonehara</surname><given-names>K.</given-names></name></person-group> (<year>2020</year>). <article-title>EyeLoop: an open-source, high-speed eye-tracker designed for dynamic experiments.</article-title>
<source><italic>bioRxiv</italic></source> [Preprint]. <pub-id pub-id-type="doi">10.1101/2020.07.03.186387</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attard-Johnson</surname><given-names>J.</given-names></name><name><surname>Ciardha</surname><given-names>C. Ó</given-names></name><name><surname>Bindemann</surname><given-names>M.</given-names></name></person-group> (<year>2019</year>). <article-title>Comparing methods for the analysis of pupillary response.</article-title>
<source><italic>Behav. Res. Methods</italic></source>
<volume>51</volume>
<fpage>83</fpage>–<lpage>95</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-018-1108-6</pub-id>
<?supplied-pmid 30324564?><pub-id pub-id-type="pmid">30324564</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrionuevo</surname><given-names>P. A.</given-names></name><name><surname>McAnany</surname><given-names>J. J.</given-names></name><name><surname>Zele</surname><given-names>A. J.</given-names></name><name><surname>Cao</surname><given-names>D.</given-names></name></person-group> (<year>2018</year>). <article-title>Non-linearities in the rod and cone photoreceptor inputs to the afferent pupil light response.</article-title>
<source><italic>Front. Neurol.</italic></source>
<volume>9</volume>:<issue>1140</issue>. <pub-id pub-id-type="doi">10.3389/fneur.2018.01140</pub-id>
<?supplied-pmid 30622511?><pub-id pub-id-type="pmid">30622511</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barten</surname><given-names>P. G.</given-names></name></person-group> (<year>1999</year>). “<article-title>Contrast sensitivity of the human eye and its effects on image quality</article-title>,” in <source><italic>Proceedings of the Contrast Sensit. Hum. Eye Its Eff. Image Qual</italic></source>, <fpage>27</fpage>–<lpage>29</lpage>. <pub-id pub-id-type="doi">10.1117/3.353254</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beatty</surname><given-names>J.</given-names></name></person-group> (<year>1982</year>). <article-title>Task-evoked pupillary responses, processing load, and the structure of processing resources.</article-title>
<source><italic>Psychol. Bull.</italic></source>
<volume>91</volume>
<fpage>276</fpage>–<lpage>292</lpage>. <pub-id pub-id-type="doi">10.1037/0033-2909.91.2.276</pub-id><pub-id pub-id-type="pmid">7071262</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beatty</surname><given-names>J.</given-names></name><name><surname>Wagoner</surname><given-names>B. L.</given-names></name></person-group> (<year>1978</year>). <article-title>Pupillometric signs of brain activation vary with level of cognitive processing.</article-title>
<source><italic>Science</italic></source>
<volume>199</volume>
<fpage>1216</fpage>–<lpage>1218</lpage>. <pub-id pub-id-type="doi">10.1126/science.628837</pub-id>
<?supplied-pmid 628837?><pub-id pub-id-type="pmid">628837</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berman</surname><given-names>S. M.</given-names></name><name><surname>Jewett</surname><given-names>D. L.</given-names></name><name><surname>Fein</surname><given-names>G.</given-names></name><name><surname>Saika</surname><given-names>G.</given-names></name><name><surname>Ashford</surname><given-names>F.</given-names></name></person-group> (<year>1990</year>). <article-title>Photopic luminance does not always predict perceived room brightness.</article-title>
<source><italic>Light. Res. Technol.</italic></source>
<volume>22</volume>
<fpage>37</fpage>–<lpage>41</lpage>. <pub-id pub-id-type="doi">10.1177/096032719002200103</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berson</surname><given-names>D. M.</given-names></name></person-group> (<year>2003</year>). <article-title>Strange vision: ganglion cells as circadian photoreceptors.</article-title>
<source><italic>Trends Neurosci.</italic></source>
<volume>26</volume>
<fpage>314</fpage>–<lpage>320</lpage>. <pub-id pub-id-type="doi">10.1016/S0166-2236(03)00130-9</pub-id><pub-id pub-id-type="pmid">12798601</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berson</surname><given-names>D. M.</given-names></name><name><surname>Dunn</surname><given-names>F. A.</given-names></name><name><surname>Takao</surname><given-names>M.</given-names></name></person-group> (<year>2002</year>). <article-title>Phototransduction by retinal ganglion cells that set the circadian clock.</article-title>
<source><italic>Science</italic></source>
<volume>295</volume>
<fpage>1070</fpage>–<lpage>1073</lpage>. <pub-id pub-id-type="doi">10.1126/science.1067262</pub-id>
<?supplied-pmid 11834835?><pub-id pub-id-type="pmid">11834835</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Besenecker</surname><given-names>U. C.</given-names></name><name><surname>Bullough</surname><given-names>J. D.</given-names></name></person-group> (<year>2017</year>). <article-title>Investigating visual mechanisms underlying scene brightness.</article-title>
<source><italic>Light. Res. Technol.</italic></source>
<volume>49</volume>
<fpage>16</fpage>–<lpage>32</lpage>. <pub-id pub-id-type="doi">10.1177/1477153516628168</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binda</surname><given-names>P.</given-names></name><name><surname>Gamlin</surname><given-names>P. D.</given-names></name></person-group> (<year>2017</year>). <article-title>Renewed attention on the pupil light reflex.</article-title>
<source><italic>Trends Neurosci.</italic></source>
<volume>40</volume>
<fpage>455</fpage>–<lpage>457</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2017.06.007</pub-id>
<?supplied-pmid 28693846?><pub-id pub-id-type="pmid">28693846</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blackie</surname><given-names>C. A.</given-names></name><name><surname>Howland</surname><given-names>H. C.</given-names></name></person-group> (<year>1999</year>). <article-title>An extension of an accommodation and convergence model of emmetropization to include the effects of illumination intensity.</article-title>
<source><italic>Ophthalmic Physiol. Opt.</italic></source>
<volume>19</volume>
<fpage>112</fpage>–<lpage>125</lpage>. <pub-id pub-id-type="doi">10.1016/S0275-5408(98)00077-5</pub-id><pub-id pub-id-type="pmid">10615447</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bodmann</surname><given-names>H. W.</given-names></name></person-group> (<year>1992</year>). <article-title>Elements of photometry, brightness and visibility.</article-title>
<source><italic>Light. Res. Technol.</italic></source>
<volume>24</volume>
<fpage>29</fpage>–<lpage>42</lpage>. <pub-id pub-id-type="doi">10.1177/096032719202400104</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bombeke</surname><given-names>K.</given-names></name><name><surname>Duthoo</surname><given-names>W.</given-names></name><name><surname>Mueller</surname><given-names>S. C.</given-names></name><name><surname>Hopf</surname><given-names>J. M.</given-names></name><name><surname>Boehler</surname><given-names>C. N.</given-names></name></person-group> (<year>2016</year>). <article-title>Pupil size directly modulates the feedforward response in human primary visual cortex independently of attention.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>127</volume>
<fpage>67</fpage>–<lpage>73</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.11.072</pub-id>
<?supplied-pmid 26658931?><pub-id pub-id-type="pmid">26658931</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonmati-Carrion</surname><given-names>M. A.</given-names></name><name><surname>Hild</surname><given-names>K.</given-names></name><name><surname>Isherwood</surname><given-names>C.</given-names></name><name><surname>Sweeney</surname><given-names>S. J.</given-names></name><name><surname>Revell</surname><given-names>V. L.</given-names></name><name><surname>Skene</surname><given-names>D. J.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Relationship between human pupillary light reflex and circadian system status.</article-title>
<source><italic>PLoS One</italic></source>
<volume>11</volume>:<issue>e0162476</issue>. <pub-id pub-id-type="doi">10.1371/journal.pone.0162476</pub-id>
<?supplied-pmid 27636197?><pub-id pub-id-type="pmid">27636197</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>G. C.</given-names></name><name><surname>Hanifin</surname><given-names>J. R.</given-names></name><name><surname>Greeson</surname><given-names>J. M.</given-names></name><name><surname>Byrne</surname><given-names>B.</given-names></name><name><surname>Glickman</surname><given-names>G.</given-names></name><name><surname>Gerner</surname><given-names>E.</given-names></name><etal/></person-group> (<year>2001</year>). <article-title>Action spectrum for melatonin regulation in humans: evidence for a novel circadian photoreceptor.</article-title>
<source><italic>J. Neurosci.</italic></source>
<volume>21</volume>
<fpage>6405</fpage>–<lpage>6412</lpage>. <pub-id pub-id-type="doi">10.1523/jneurosci.21-16-06405.2001</pub-id>
<?supplied-pmid 11487664?><pub-id pub-id-type="pmid">11487664</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brisson</surname><given-names>J.</given-names></name><name><surname>Mainville</surname><given-names>M.</given-names></name><name><surname>Mailloux</surname><given-names>D.</given-names></name><name><surname>Beaulieu</surname><given-names>C.</given-names></name><name><surname>Serres</surname><given-names>J.</given-names></name><name><surname>Sirois</surname><given-names>S.</given-names></name></person-group> (<year>2013</year>). <article-title>Pupil diameter measurement errors as a function of gaze direction in corneal reflection eyetrackers.</article-title>
<source><italic>Behav. Res. Methods</italic></source>
<volume>45</volume>
<fpage>1322</fpage>–<lpage>1331</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-013-0327-0</pub-id>
<?supplied-pmid 23468182?><pub-id pub-id-type="pmid">23468182</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>F. W.</given-names></name></person-group> (<year>1957</year>). <article-title>The depth of field of the human eye.</article-title>
<source><italic>Opt. Acta Int. J. Opt.</italic></source>
<volume>4</volume>
<fpage>157</fpage>–<lpage>164</lpage>. <pub-id pub-id-type="doi">10.1080/713826091</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>F. W.</given-names></name><name><surname>Gubisch</surname><given-names>R. W.</given-names></name></person-group> (<year>1966</year>). <article-title>Optical quality of the human eye.</article-title>
<source><italic>J. Physiol.</italic></source>
<volume>186</volume>
<fpage>558</fpage>–<lpage>578</lpage>. <pub-id pub-id-type="doi">10.1113/jphysiol.1966.sp008056</pub-id>
<?supplied-pmid 5972153?><pub-id pub-id-type="pmid">5972153</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Canver</surname><given-names>M. C.</given-names></name><name><surname>Canver</surname><given-names>A. C.</given-names></name><name><surname>Revere</surname><given-names>K. E.</given-names></name><name><surname>Amado</surname><given-names>D.</given-names></name><name><surname>Bennett</surname><given-names>J.</given-names></name><name><surname>Chung</surname><given-names>D. C.</given-names></name></person-group> (<year>2014</year>). <article-title>Novel mathematical algorithm for pupillometric data analysis.</article-title>
<source><italic>Comput. Methods Programs Biomed.</italic></source>
<volume>113</volume>
<fpage>221</fpage>–<lpage>225</lpage>. <pub-id pub-id-type="doi">10.1016/j.cmpb.2013.08.008</pub-id>
<?supplied-pmid 24129048?><pub-id pub-id-type="pmid">24129048</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carle</surname><given-names>C. F.</given-names></name><name><surname>James</surname><given-names>A. C.</given-names></name><name><surname>Rosli</surname><given-names>Y.</given-names></name><name><surname>Maddess</surname><given-names>T.</given-names></name></person-group> (<year>2019</year>). <article-title>Localization of neuronal gain control in the pupillary response.</article-title>
<source><italic>Front. Neurol.</italic></source>
<volume>10</volume>:<issue>203</issue>. <pub-id pub-id-type="doi">10.3389/fneur.2019.00203</pub-id>
<?supplied-pmid 30930833?><pub-id pub-id-type="pmid">30930833</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>S.</given-names></name><name><surname>Epps</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>). <article-title>Efficient and robust pupil size and blink estimation from near-field video sequences for human-machine interaction.</article-title>
<source><italic>IEEE Trans. Cybern.</italic></source>
<volume>44</volume>
<fpage>2356</fpage>–<lpage>2367</lpage>. <pub-id pub-id-type="doi">10.1109/TCYB.2014.2306916</pub-id>
<?supplied-pmid 24691198?><pub-id pub-id-type="pmid">24691198</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cherng</surname><given-names>Y.-G.</given-names></name><name><surname>Baird</surname><given-names>T.</given-names></name><name><surname>Chen</surname><given-names>J.-T.</given-names></name><name><surname>Wang</surname><given-names>C.-A.</given-names></name></person-group> (<year>2020</year>). <article-title>Background luminance effects on pupil size associated with emotion and saccade preparation.</article-title>
<source><italic>Sci. Rep.</italic></source>
<volume>10</volume>:<issue>15718</issue>. <pub-id pub-id-type="doi">10.1038/s41598-020-72954-z</pub-id>
<?supplied-pmid 32973283?><pub-id pub-id-type="pmid">32973283</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chougule</surname><given-names>P. S.</given-names></name><name><surname>Najjar</surname><given-names>R. P.</given-names></name><name><surname>Finkelstein</surname><given-names>M. T.</given-names></name><name><surname>Kandiah</surname><given-names>N.</given-names></name><name><surname>Milea</surname><given-names>D.</given-names></name></person-group> (<year>2019</year>). <article-title>Light-induced pupillary responses in Alzheimer’s disease.</article-title>
<source><italic>Front. Neurol.</italic></source>
<volume>10</volume>:<issue>360</issue>. <pub-id pub-id-type="doi">10.3389/fneur.2019.00360</pub-id>
<?supplied-pmid 31031692?><pub-id pub-id-type="pmid">31031692</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><collab>CIE</collab> (<year>2011</year>). <source><italic>CIE:200:2001: Supplementary System of Photometry.</italic></source> Available online at: <ext-link ext-link-type="uri" xlink:href="http://cie.co.at/publications/cie-supplementary-system-photometry">http://cie.co.at/publications/cie-supplementary-system-photometry</ext-link>
<comment>(accessed July 23, 2020)</comment>.</mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clewett</surname><given-names>D.</given-names></name><name><surname>Gasser</surname><given-names>C.</given-names></name><name><surname>Davachi</surname><given-names>L.</given-names></name></person-group> (<year>2020</year>). <article-title>Pupil-linked arousal signals track the temporal organization of events in memory.</article-title>
<source><italic>Nat. Commun.</italic></source>
<volume>11</volume>:<issue>4007</issue>. <pub-id pub-id-type="doi">10.1038/s41467-020-17851-9</pub-id>
<?supplied-pmid 32782282?><pub-id pub-id-type="pmid">32782282</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Connelly</surname><given-names>M. A.</given-names></name><name><surname>Brown</surname><given-names>J. T.</given-names></name><name><surname>Kearns</surname><given-names>G. L.</given-names></name><name><surname>Anderson</surname><given-names>R. A.</given-names></name><name><surname>St Peter</surname><given-names>S. D.</given-names></name><name><surname>Neville</surname><given-names>K. A.</given-names></name></person-group> (<year>2014</year>). <article-title>Pupillometry: a non-invasive technique for pain assessment in paediatric patients.</article-title>
<source><italic>Arch. Dis. Child.</italic></source>
<volume>99</volume>
<fpage>1125</fpage>–<lpage>1131</lpage>. <pub-id pub-id-type="doi">10.1136/archdischild-2014-306286</pub-id>
<?supplied-pmid 25187497?><pub-id pub-id-type="pmid">25187497</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coyne</surname><given-names>J. T.</given-names></name><name><surname>Brown</surname><given-names>N.</given-names></name><name><surname>Foroughi</surname><given-names>C. K.</given-names></name><name><surname>Sibley</surname><given-names>C. M.</given-names></name></person-group> (<year>2019</year>). <article-title>Improving pupil diameter measurement accuracy in a remote eye tracking system.</article-title>
<source><italic>Proc. Hum. Factors Ergon. Soc. Annu. Meet.</italic></source>
<volume>63</volume>
<fpage>49</fpage>–<lpage>53</lpage>. <pub-id pub-id-type="doi">10.1177/1071181319631176</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crawford</surname><given-names>B. H.</given-names></name></person-group> (<year>1936</year>). <article-title>The dependence of pupil size upon external light stimulus under static and variable conditions.</article-title>
<source><italic>Proc. R. Soc. London. Ser. B Biol. Sci.</italic></source>
<volume>121</volume>
<fpage>376</fpage>–<lpage>395</lpage>. <pub-id pub-id-type="doi">10.1098/rspb.1936.0072</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crippa</surname><given-names>S. V.</given-names></name><name><surname>Domellöf</surname><given-names>F. P.</given-names></name><name><surname>Kawasaki</surname><given-names>A.</given-names></name></person-group> (<year>2018</year>). <article-title>Chromatic pupillometry in children.</article-title>
<source><italic>Front. Neurol.</italic></source>
<volume>9</volume>:<issue>669</issue>. <pub-id pub-id-type="doi">10.3389/fneur.2018.00669</pub-id>
<?supplied-pmid 30174642?><pub-id pub-id-type="pmid">30174642</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Groot</surname><given-names>S. G.</given-names></name><name><surname>Gebhard</surname><given-names>J. W.</given-names></name></person-group> (<year>1952</year>). <article-title>Pupil size as determined by adapting luminance.</article-title>
<source><italic>J. Opt. Soc. Am.</italic></source>
<volume>42</volume>:<issue>492</issue>. <pub-id pub-id-type="doi">10.1364/JOSA.42.000492</pub-id>
<?supplied-pmid 14939111?><pub-id pub-id-type="pmid">14939111</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Winter</surname><given-names>J. C. F.</given-names></name><name><surname>Petermeijer</surname><given-names>S. M.</given-names></name><name><surname>Kooijman</surname><given-names>L.</given-names></name><name><surname>Dodou</surname><given-names>D.</given-names></name></person-group> (<year>2021</year>). <article-title>Replicating five pupillometry studies of Eckhard Hess.</article-title>
<source><italic>Int. J. Psychophysiol</italic></source>
<volume>165</volume>
<fpage>145</fpage>–<lpage>205</lpage>. <pub-id pub-id-type="doi">10.1016/j.ijpsycho.2021.03.003</pub-id>
<?supplied-pmid 33766646?><pub-id pub-id-type="pmid">33766646</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dey</surname><given-names>S.</given-names></name><name><surname>Samanta</surname><given-names>D.</given-names></name></person-group> (<year>2007</year>). “<article-title>An efficient approach for pupil detection in iris images</article-title>,” in <source><italic>Proceedings of the 15th Int. Conf. Adv. Comput. Commun. ADCOM 2007</italic></source>, <publisher-loc>Guwahati</publisher-loc>, <fpage>382</fpage>–<lpage>387</lpage>. <pub-id pub-id-type="doi">10.1109/adcom.2007.79</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Do</surname><given-names>M. T. H.</given-names></name></person-group> (<year>2019</year>). <article-title>Melanopsin and the intrinsically photosensitive retinal ganglion cells: biophysics to behavior.</article-title>
<source><italic>Neuron</italic></source>
<volume>104</volume>
<fpage>205</fpage>–<lpage>226</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2019.07.016</pub-id>
<?supplied-pmid 31647894?><pub-id pub-id-type="pmid">31647894</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Do</surname><given-names>M. T. H.</given-names></name><name><surname>Kang</surname><given-names>S. H.</given-names></name><name><surname>Xue</surname><given-names>T.</given-names></name><name><surname>Zhong</surname><given-names>H.</given-names></name><name><surname>Liao</surname><given-names>H. W.</given-names></name><name><surname>Bergles</surname><given-names>D. E.</given-names></name><etal/></person-group> (<year>2009</year>). <article-title>Photon capture and signalling by melanopsin retinal ganglion cells.</article-title>
<source><italic>Nature</italic></source>
<volume>457</volume>
<fpage>281</fpage>–<lpage>287</lpage>. <pub-id pub-id-type="doi">10.1038/nature07682</pub-id>
<?supplied-pmid 19118382?><pub-id pub-id-type="pmid">19118382</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ebisawa</surname><given-names>Y.</given-names></name></person-group> (<year>1994</year>). “<article-title>Improved video-based eye-gaze detection method</article-title>,” in <source><italic>Proceedings of the Conf. Proc. - 10th Anniv. IMTC 1994 Adv. Technol. I M. 1994 IEEE Instrum. Meas. Technol. Conf</italic></source>, <publisher-loc>Hamamatsu</publisher-loc>, <fpage>963</fpage>–<lpage>966</lpage>. <pub-id pub-id-type="doi">10.1109/IMTC.1994.351964</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ebisawa</surname><given-names>Y.</given-names></name></person-group> (<year>2004</year>). “<article-title>Realtime 3D position detection of human pupil</article-title>,” in <source><italic>Proceedings of the 2004 IEEE Symp. Virtual Environ. Human-Computer Interfaces Meas. Syst. VECIMS</italic></source>, <publisher-loc>Boston, MA</publisher-loc>, <fpage>8</fpage>–<lpage>12</lpage>. <pub-id pub-id-type="doi">10.1109/vecims.2004.1397176</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ecker</surname><given-names>J. L.</given-names></name><name><surname>Dumitrescu</surname><given-names>O. N.</given-names></name><name><surname>Wong</surname><given-names>K. Y.</given-names></name><name><surname>Alam</surname><given-names>N. M.</given-names></name><name><surname>Chen</surname><given-names>S.-K.</given-names></name><name><surname>LeGates</surname><given-names>T.</given-names></name><etal/></person-group> (<year>2010</year>). <article-title>Melanopsin-expressing retinal ganglion-cell photoreceptors: cellular diversity and role in pattern vision.</article-title>
<source><italic>Neuron</italic></source>
<volume>67</volume>
<fpage>49</fpage>–<lpage>60</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2010.05.023</pub-id>
<?supplied-pmid 20624591?><pub-id pub-id-type="pmid">20624591</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Eivazi</surname><given-names>S.</given-names></name><name><surname>Santini</surname><given-names>T.</given-names></name><name><surname>Keshavarzi</surname><given-names>A.</given-names></name><name><surname>Kübler</surname><given-names>T.</given-names></name><name><surname>Mazzei</surname><given-names>A.</given-names></name></person-group> (<year>2019</year>). “<article-title>Improving real-time CNN-based pupil detection through domain-specific data augmentation</article-title>,” in <source><italic>Proceedings of the Eye Tracking Research and Applications Symposium (ETRA)</italic></source>, (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>), <fpage>1</fpage>–<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1145/3314111.3319914</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freedman</surname><given-names>M. S.</given-names></name></person-group> (<year>1999</year>). <article-title>Regulation of mammalian circadian behavior by non-rod, non-cone, ocular photoreceptors.</article-title>
<source><italic>Science</italic></source>
<volume>284</volume>
<fpage>502</fpage>–<lpage>504</lpage>. <pub-id pub-id-type="doi">10.1126/science.284.5413.502</pub-id>
<?supplied-pmid 10205061?><pub-id pub-id-type="pmid">10205061</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fuhl</surname><given-names>W.</given-names></name><name><surname>Eivazi</surname><given-names>S.</given-names></name><name><surname>Hosp</surname><given-names>B.</given-names></name><name><surname>Eivazi</surname><given-names>A.</given-names></name><name><surname>Rosenstiel</surname><given-names>W.</given-names></name><name><surname>Kasneci</surname><given-names>E.</given-names></name></person-group> (<year>2018a</year>). “<article-title>BORE: boosted-oriented edge optimization for robust, real time remote pupil center detection</article-title>,” in <source><italic>Proceedings of the Eye Tracking Research and Applications Symposium (ETRA)</italic></source>, (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>), <fpage>1</fpage>–<lpage>5</lpage>. <pub-id pub-id-type="doi">10.1145/3204493.3204558</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fuhl</surname><given-names>W.</given-names></name><name><surname>Geisler</surname><given-names>D.</given-names></name><name><surname>Santini</surname><given-names>T.</given-names></name><name><surname>Appel</surname><given-names>T.</given-names></name><name><surname>Rosenstiel</surname><given-names>W.</given-names></name><name><surname>Kasneci</surname><given-names>E.</given-names></name></person-group> (<year>2018b</year>). “<article-title>CBF: circular binary features for robust and real-time pupil center detection</article-title>,” in <source><italic>Proceedings of the Eye Tracking Research and Applications Symposium (ETRA)</italic></source>, (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>), <fpage>1</fpage>–<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1145/3204493.3204559</pub-id></mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fuhl</surname><given-names>W.</given-names></name><name><surname>Kübler</surname><given-names>T.</given-names></name><name><surname>Sippel</surname><given-names>K.</given-names></name><name><surname>Rosenstiel</surname><given-names>W.</given-names></name><name><surname>Kasneci</surname><given-names>E.</given-names></name></person-group> (<year>2015</year>). “<article-title>ExCuSe: robust pupil detection in real-world scenarios</article-title>,” in <source><italic>Computer Analysis of Images and Patterns</italic></source>, <role>eds</role>
<person-group person-group-type="editor"><name><surname>Azzopardi</surname><given-names>G.</given-names></name><name><surname>Petkov</surname><given-names>N.</given-names></name></person-group> (<publisher-loc>Cham</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>39</fpage>–<lpage>51</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-23192-1_4</pub-id></mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuhl</surname><given-names>W.</given-names></name><name><surname>Santini</surname><given-names>T.</given-names></name><name><surname>Kasneci</surname><given-names>G.</given-names></name><name><surname>Kasneci</surname><given-names>E.</given-names></name></person-group> (<year>2016b</year>). <source><italic>PupilNet: Convolutional Neural Networks for Robust Pupil Detection.</italic></source> Available online at: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1601.04902">http://arxiv.org/abs/1601.04902</ext-link>
<comment>(accessed October 6, 2020)</comment>.</mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuhl</surname><given-names>W.</given-names></name><name><surname>Santini</surname><given-names>T.</given-names></name><name><surname>Kasneci</surname><given-names>G.</given-names></name><name><surname>Rosenstiel</surname><given-names>W.</given-names></name><name><surname>Kasneci</surname><given-names>E.</given-names></name></person-group> (<year>2017</year>). <source><italic>PupilNet v2.0: Convolutional Neural Networks for CPU based real time Robust Pupil Detection.</italic></source> Available online at: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1711.00112">http://arxiv.org/abs/1711.00112</ext-link>
<comment>(accessed October 6, 2020)</comment>.</mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuhl</surname><given-names>W.</given-names></name><name><surname>Santini</surname><given-names>T. C.</given-names></name><name><surname>Kübler</surname><given-names>T.</given-names></name><name><surname>Kasneci</surname><given-names>E.</given-names></name></person-group> (<year>2016a</year>). <article-title>ElSe: ellipse selection for robust pupil detection in real-world environments.</article-title>
<source><italic>Eye Track. Res. Appl. Symp.</italic></source>
<volume>14</volume>
<fpage>123</fpage>–<lpage>130</lpage>. <pub-id pub-id-type="doi">10.1145/2857491.2857505</pub-id></mixed-citation>
    </ref>
    <ref id="B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gagl</surname><given-names>B.</given-names></name><name><surname>Hawelka</surname><given-names>S.</given-names></name><name><surname>Hutzler</surname><given-names>F.</given-names></name></person-group> (<year>2011</year>). <article-title>Systematic influence of gaze position on pupil size measurement: analysis and correction.</article-title>
<source><italic>Behav. Res. Methods</italic></source>
<volume>43</volume>
<fpage>1171</fpage>–<lpage>1181</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-011-0109-5</pub-id>
<?supplied-pmid 21637943?><pub-id pub-id-type="pmid">21637943</pub-id></mixed-citation>
    </ref>
    <ref id="B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gamlin</surname><given-names>P. D. R.</given-names></name><name><surname>McDougal</surname><given-names>D. H.</given-names></name><name><surname>Pokorny</surname><given-names>J.</given-names></name><name><surname>Smith</surname><given-names>V. C.</given-names></name><name><surname>Yau</surname><given-names>K.-W.</given-names></name><name><surname>Dacey</surname><given-names>D. M.</given-names></name></person-group> (<year>2007</year>). <article-title>Human and macaque pupil responses driven by melanopsin-containing retinal ganglion cells.</article-title>
<source><italic>Vision Res.</italic></source>
<volume>47</volume>
<fpage>946</fpage>–<lpage>954</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2006.12.015</pub-id>
<?supplied-pmid 17320141?><pub-id pub-id-type="pmid">17320141</pub-id></mixed-citation>
    </ref>
    <ref id="B51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goñi</surname><given-names>S.</given-names></name><name><surname>Echeto</surname><given-names>J.</given-names></name><name><surname>Villanueva</surname><given-names>A.</given-names></name><name><surname>Cabeza</surname><given-names>R.</given-names></name></person-group> (<year>2004</year>). <article-title>Robust algorithm for pupil-glint vector detection in a video-oculography eyetracking system.</article-title>
<source><italic>Proc. Int. Conf. Pattern Recognit.</italic></source>
<volume>4</volume>
<fpage>941</fpage>–<lpage>944</lpage>. <pub-id pub-id-type="doi">10.1109/ICPR.2004.1333928</pub-id></mixed-citation>
    </ref>
    <ref id="B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gooley</surname><given-names>J. J.</given-names></name><name><surname>Lu</surname><given-names>J.</given-names></name><name><surname>Chou</surname><given-names>T. C.</given-names></name><name><surname>Scammell</surname><given-names>T. E.</given-names></name><name><surname>Saper</surname><given-names>C. B.</given-names></name></person-group> (<year>2001</year>). <article-title>Melanopsin in cells of origin of the retinohypothalamic tract.</article-title>
<source><italic>Nat. Neurosci.</italic></source>
<volume>4</volume>:<issue>1165</issue>. <pub-id pub-id-type="doi">10.1038/nn768</pub-id>
<?supplied-pmid 11713469?><pub-id pub-id-type="pmid">11713469</pub-id></mixed-citation>
    </ref>
    <ref id="B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Granholm</surname><given-names>E. L.</given-names></name><name><surname>Panizzon</surname><given-names>M. S.</given-names></name><name><surname>Elman</surname><given-names>J. A.</given-names></name><name><surname>Jak</surname><given-names>A. J.</given-names></name><name><surname>Hauger</surname><given-names>R. L.</given-names></name><name><surname>Bondi</surname><given-names>M. W.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Pupillary responses as a biomarker of early risk for Alzheimer’s disease.</article-title>
<source><italic>J. Alzheimer’s Dis.</italic></source>
<volume>56</volume>
<fpage>1419</fpage>–<lpage>1428</lpage>. <pub-id pub-id-type="doi">10.3233/JAD-161078</pub-id>
<?supplied-pmid 28157098?><pub-id pub-id-type="pmid">28157098</pub-id></mixed-citation>
    </ref>
    <ref id="B54">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güler</surname><given-names>A. D.</given-names></name><name><surname>Ecker</surname><given-names>J. L.</given-names></name><name><surname>Lall</surname><given-names>G. S.</given-names></name><name><surname>Haq</surname><given-names>S.</given-names></name><name><surname>Altimus</surname><given-names>C. M.</given-names></name><name><surname>Liao</surname><given-names>H. W.</given-names></name><etal/></person-group> (<year>2008</year>). <article-title>Melanopsin cells are the principal conduits for rod-cone input to non-image-forming vision.</article-title>
<source><italic>Nature</italic></source>
<volume>453</volume>
<fpage>102</fpage>–<lpage>105</lpage>. <pub-id pub-id-type="doi">10.1038/nature06829</pub-id>
<?supplied-pmid 18432195?><pub-id pub-id-type="pmid">18432195</pub-id></mixed-citation>
    </ref>
    <ref id="B55">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hattar</surname><given-names>S.</given-names></name></person-group> (<year>2002</year>). <article-title>Melanopsin-containing retinal ganglion cells: architecture, projections, and intrinsic photosensitivity.</article-title>
<source><italic>Science</italic></source>
<volume>295</volume>
<fpage>1065</fpage>–<lpage>1070</lpage>. <pub-id pub-id-type="doi">10.1126/science.1069609</pub-id>
<?supplied-pmid 11834834?><pub-id pub-id-type="pmid">11834834</pub-id></mixed-citation>
    </ref>
    <ref id="B56">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hattar</surname><given-names>S.</given-names></name><name><surname>Kumar</surname><given-names>M.</given-names></name><name><surname>Park</surname><given-names>A.</given-names></name><name><surname>Tong</surname><given-names>P.</given-names></name><name><surname>Tung</surname><given-names>J.</given-names></name><name><surname>Yau</surname><given-names>K.-W.</given-names></name><etal/></person-group> (<year>2006</year>). <article-title>Central projections of melanopsin-expressing retinal ganglion cells in the mouse.</article-title>
<source><italic>J. Comp. Neurol.</italic></source>
<volume>497</volume>
<fpage>326</fpage>–<lpage>349</lpage>. <pub-id pub-id-type="doi">10.1002/cne.20970</pub-id>
<?supplied-pmid 16736474?><pub-id pub-id-type="pmid">16736474</pub-id></mixed-citation>
    </ref>
    <ref id="B57">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hattar</surname><given-names>S.</given-names></name><name><surname>Lucas</surname><given-names>R. J.</given-names></name><name><surname>Mrosovsky</surname><given-names>N.</given-names></name><name><surname>Thompson</surname><given-names>S.</given-names></name><name><surname>Douglas</surname><given-names>R. H.</given-names></name><name><surname>Hankins</surname><given-names>M. W.</given-names></name><etal/></person-group> (<year>2003</year>). <article-title>Melanopsin and rod—cone photoreceptive systems account for all major accessory visual functions in mice.</article-title>
<source><italic>Nature</italic></source>
<volume>424</volume>
<fpage>76</fpage>–<lpage>81</lpage>. <pub-id pub-id-type="doi">10.1038/nature01761</pub-id>
<?supplied-pmid 12808468?><pub-id pub-id-type="pmid">12808468</pub-id></mixed-citation>
    </ref>
    <ref id="B58">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayes</surname><given-names>T. R.</given-names></name><name><surname>Petrov</surname><given-names>A. A.</given-names></name></person-group> (<year>2016</year>). <article-title>Mapping and correcting the influence of gaze position on pupil size measurements.</article-title>
<source><italic>Behav. Res. Methods</italic></source>
<volume>48</volume>
<fpage>510</fpage>–<lpage>527</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-015-0588-x</pub-id>
<?supplied-pmid 25953668?><pub-id pub-id-type="pmid">25953668</pub-id></mixed-citation>
    </ref>
    <ref id="B59">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hermans</surname><given-names>S.</given-names></name><name><surname>Smet</surname><given-names>K. A. G.</given-names></name><name><surname>Hanselaer</surname><given-names>P.</given-names></name></person-group> (<year>2018</year>). <article-title>Brightness model for neutral self-luminous stimuli and backgrounds.</article-title>
<source><italic>LEUKOS J. Illum. Eng. Soc. North Am.</italic></source>
<volume>14</volume>
<fpage>231</fpage>–<lpage>244</lpage>. <pub-id pub-id-type="doi">10.1080/15502724.2018.1448280</pub-id></mixed-citation>
    </ref>
    <ref id="B60">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hiley</surname><given-names>J. B.</given-names></name><name><surname>Redekopp</surname><given-names>A. H.</given-names></name><name><surname>Fazel-Rezai</surname><given-names>R.</given-names></name></person-group> (<year>2006</year>). <article-title>A low cost human computer interface based on eye tracking.</article-title>
<source><italic>Annu. Int. Conf. IEEE Eng. Med. Biol. Proc.</italic></source>
<volume>2006</volume>
<fpage>3226</fpage>–<lpage>3229</lpage>. <pub-id pub-id-type="doi">10.1109/IEMBS.2006.260774</pub-id>
<?supplied-pmid 17946167?><pub-id pub-id-type="pmid">17946167</pub-id></mixed-citation>
    </ref>
    <ref id="B61">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holladay</surname><given-names>L. L.</given-names></name></person-group> (<year>1926</year>). <article-title>The fundamentals of glare and visibility.</article-title>
<source><italic>J. Opt. Soc. Am.</italic></source>
<volume>12</volume>:<issue>271</issue>. <pub-id pub-id-type="doi">10.1364/JOSA.12.000271</pub-id></mixed-citation>
    </ref>
    <ref id="B62">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Holmqvist</surname><given-names>K.</given-names></name><name><surname>Nyström</surname><given-names>M.</given-names></name><name><surname>Mulvey</surname><given-names>F.</given-names></name></person-group> (<year>2012</year>). “<article-title>Eye tracker data quality: what it is and how to measure it</article-title>,” in <source><italic>Proceedings of the Symposium on Eye Tracking Research and Applications ETRA ‘12</italic></source>, (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM Press</publisher-name>), 45. <pub-id pub-id-type="doi">10.1145/2168556.2168563</pub-id></mixed-citation>
    </ref>
    <ref id="B63">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hosp</surname><given-names>B.</given-names></name><name><surname>Eivazi</surname><given-names>S.</given-names></name><name><surname>Maurer</surname><given-names>M.</given-names></name><name><surname>Fuhl</surname><given-names>W.</given-names></name><name><surname>Geisler</surname><given-names>D.</given-names></name><name><surname>Kasneci</surname><given-names>E.</given-names></name></person-group> (<year>2020</year>). <article-title>RemoteEye: an open-source high-speed remote eye tracker: Implementation insights of a pupil- and glint-detection algorithm for high-speed remote eye tracking.</article-title>
<source><italic>Behav. Res. Methods</italic></source>
<volume>52</volume>
<fpage>1387</fpage>–<lpage>1401</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-019-01305-2</pub-id>
<?supplied-pmid 32212086?><pub-id pub-id-type="pmid">32212086</pub-id></mixed-citation>
    </ref>
    <ref id="B64">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hreidarsson</surname><given-names>A. B.</given-names></name></person-group> (<year>1982</year>). <article-title>Pupil size in insulin-dependent diabetes. Relationship to duration, metabolic control, and long-term manifestations.</article-title>
<source><italic>Diabetes</italic></source>
<volume>31</volume>
<fpage>442</fpage>–<lpage>448</lpage>. <pub-id pub-id-type="doi">10.2337/diab.31.5.442</pub-id>
<?supplied-pmid 6759259?><pub-id pub-id-type="pmid">6759259</pub-id></mixed-citation>
    </ref>
    <ref id="B65">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>X.</given-names></name><name><surname>Hisakata</surname><given-names>R.</given-names></name><name><surname>Kaneko</surname><given-names>H.</given-names></name></person-group> (<year>2019</year>). <article-title>Effects of spatial frequency and attention on pupillary response.</article-title>
<source><italic>J. Opt. Soc. Am. A</italic></source>
<volume>36</volume>:<issue>1699</issue>. <pub-id pub-id-type="doi">10.1364/josaa.36.001699</pub-id>
<?supplied-pmid 31674435?><pub-id pub-id-type="pmid">31674435</pub-id></mixed-citation>
    </ref>
    <ref id="B66">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hutchinson</surname><given-names>T. E.</given-names></name><name><surname>White</surname><given-names>K. P.</given-names></name><name><surname>Martin</surname><given-names>W. N.</given-names></name><name><surname>Reichert</surname><given-names>K. C.</given-names></name><name><surname>Frey</surname><given-names>L. A.</given-names></name></person-group> (<year>1989</year>). <article-title>Human-computer interaction using eye-gaze input.</article-title>
<source><italic>IEEE Trans. Syst. Man Cybern.</italic></source>
<volume>19</volume>
<fpage>1527</fpage>–<lpage>1534</lpage>. <pub-id pub-id-type="doi">10.1109/21.44068</pub-id></mixed-citation>
    </ref>
    <ref id="B67">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Javadi</surname><given-names>A. H.</given-names></name><name><surname>Hakimi</surname><given-names>Z.</given-names></name><name><surname>Barati</surname><given-names>M.</given-names></name><name><surname>Walsh</surname><given-names>V.</given-names></name><name><surname>Tcheang</surname><given-names>L.</given-names></name></person-group> (<year>2015</year>). <article-title>Set: a pupil detection method using sinusoidal approximation.</article-title>
<source><italic>Front. Neuroeng.</italic></source>
<volume>8</volume>:<issue>4</issue>. <pub-id pub-id-type="doi">10.3389/fneng.2015.00004</pub-id>
<?supplied-pmid 25914641?><pub-id pub-id-type="pmid">25914641</pub-id></mixed-citation>
    </ref>
    <ref id="B68">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jennings</surname><given-names>B. J.</given-names></name><name><surname>Martinovic</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>). <article-title>Luminance and color inputs to mid-level and high-level vision.</article-title>
<source><italic>J. Vis.</italic></source>
<volume>14</volume>
<fpage>1</fpage>–<lpage>17</lpage>. <pub-id pub-id-type="doi">10.1167/14.2.9</pub-id></mixed-citation>
    </ref>
    <ref id="B69">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jepma</surname><given-names>M.</given-names></name><name><surname>Nieuwenhuis</surname><given-names>S.</given-names></name></person-group> (<year>2011</year>). <article-title>Pupil diameter predicts changes in the exploration-exploitation trade-off: evidence for the adaptive gain theory.</article-title>
<source><italic>J. Cogn. Neurosci.</italic></source>
<volume>23</volume>
<fpage>1587</fpage>–<lpage>1596</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2010.21548</pub-id>
<?supplied-pmid 20666595?><pub-id pub-id-type="pmid">20666595</pub-id></mixed-citation>
    </ref>
    <ref id="B70">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joshi</surname><given-names>S.</given-names></name></person-group> (<year>2021</year>). <article-title>Pupillometry: arousal state or state of mind?</article-title>
<source><italic>Curr. Biol.</italic></source>
<volume>31</volume>
<fpage>R32</fpage>–<lpage>R34</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2020.11.001</pub-id>
<?supplied-pmid 33434486?><pub-id pub-id-type="pmid">33434486</pub-id></mixed-citation>
    </ref>
    <ref id="B71">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joyce</surname><given-names>D. S.</given-names></name><name><surname>Feigl</surname><given-names>B.</given-names></name><name><surname>Kerr</surname><given-names>G.</given-names></name><name><surname>Roeder</surname><given-names>L.</given-names></name><name><surname>Zele</surname><given-names>A. J.</given-names></name></person-group> (<year>2018</year>). <article-title>Melanopsin-mediated pupil function is impaired in Parkinson’s disease.</article-title>
<source><italic>Sci. Rep.</italic></source>
<volume>8</volume>:<issue>7796</issue>. <pub-id pub-id-type="doi">10.1038/s41598-018-26078-0</pub-id>
<?supplied-pmid 29773814?><pub-id pub-id-type="pmid">29773814</pub-id></mixed-citation>
    </ref>
    <ref id="B72">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kassner</surname><given-names>M.</given-names></name><name><surname>Patera</surname><given-names>W.</given-names></name><name><surname>Bulling</surname><given-names>A.</given-names></name></person-group> (<year>2014</year>). “<article-title>Pupil: an open source platform for pervasive eye tracking and mobile gaze-based interaction</article-title>,” in <source><italic>Proceedings of the UbiComp 2014 - Adjun. Proc. 2014 ACM Int. Jt. Conf. Pervasive Ubiquitous Comput</italic></source>, <publisher-loc>Seattle WA</publisher-loc>, <fpage>1151</fpage>–<lpage>1160</lpage>. <pub-id pub-id-type="doi">10.1145/2638728.2641695</pub-id></mixed-citation>
    </ref>
    <ref id="B73">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Keil</surname><given-names>A.</given-names></name><name><surname>Albuquerque</surname><given-names>G.</given-names></name><name><surname>Berger</surname><given-names>K.</given-names></name><name><surname>Magnor</surname><given-names>M. A.</given-names></name></person-group> (<year>2010</year>). “<article-title>Real-time gaze tracking with a consumer-grade video camera</article-title>,” in <source><italic>Proceedings of the 18th Int. Conf. Cent. Eur. Comput. Graph. Vis. Comput. Vision, WSCG 2010 - Co-operation with EUROGRAPHICS, Full Pap. Proc</italic></source>, <publisher-loc>Plzen, Czech Republic</publisher-loc>. <fpage>129</fpage>–<lpage>134</lpage>.</mixed-citation>
    </ref>
    <ref id="B74">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelbsch</surname><given-names>C.</given-names></name><name><surname>Strasser</surname><given-names>T.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Feigl</surname><given-names>B.</given-names></name><name><surname>Gamlin</surname><given-names>P. D.</given-names></name><name><surname>Kardon</surname><given-names>R.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Standards in pupillography.</article-title>
<source><italic>Front. Neurol.</italic></source>
<volume>10</volume>:<issue>129</issue>. <pub-id pub-id-type="doi">10.3389/fneur.2019.00129</pub-id>
<?supplied-pmid 30853933?><pub-id pub-id-type="pmid">30853933</pub-id></mixed-citation>
    </ref>
    <ref id="B75">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kercher</surname><given-names>C.</given-names></name><name><surname>Azinfar</surname><given-names>L.</given-names></name><name><surname>Dinalankara</surname><given-names>D. M. R.</given-names></name><name><surname>Takahashi</surname><given-names>T. N.</given-names></name><name><surname>Miles</surname><given-names>J. H.</given-names></name><name><surname>Yao</surname><given-names>G.</given-names></name></person-group> (<year>2020</year>). <article-title>A longitudinal study of pupillary light reflex in 6- to 24-month children.</article-title>
<source><italic>Sci. Rep.</italic></source>
<volume>10</volume>:<issue>1205</issue>. <pub-id pub-id-type="doi">10.1038/s41598-020-58254-6</pub-id>
<?supplied-pmid 31988320?><pub-id pub-id-type="pmid">31988320</pub-id></mixed-citation>
    </ref>
    <ref id="B76">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Klingner</surname><given-names>J.</given-names></name></person-group> (<year>2010</year>). “<article-title>The pupillometric precision of a remote video eye tracker</article-title>,” in <source><italic>Proceedings of the 2010 Symposium on Eye-Tracking Research &amp; Applications, ETRA 2010</italic></source>, <publisher-loc>Austin, T</publisher-loc>X, <fpage>259</fpage>–<lpage>262</lpage>. <pub-id pub-id-type="doi">10.1145/1743666.1743727</pub-id></mixed-citation>
    </ref>
    <ref id="B77">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobashi</surname><given-names>H.</given-names></name><name><surname>Kamiya</surname><given-names>K.</given-names></name><name><surname>Ishikawa</surname><given-names>H.</given-names></name><name><surname>Goseki</surname><given-names>T.</given-names></name><name><surname>Shimizu</surname><given-names>K.</given-names></name></person-group> (<year>2012</year>). <article-title>Daytime variations in pupil size under photopic conditions.</article-title>
<source><italic>Optom. Vis. Sci.</italic></source>
<volume>89</volume>
<fpage>197</fpage>–<lpage>202</lpage>. <pub-id pub-id-type="doi">10.1097/OPX.0b013e31824048a9</pub-id>
<?supplied-pmid 22179219?><pub-id pub-id-type="pmid">22179219</pub-id></mixed-citation>
    </ref>
    <ref id="B78">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kret</surname><given-names>M. E.</given-names></name><name><surname>Sjak-Shie</surname><given-names>E. E.</given-names></name></person-group> (<year>2019</year>). <article-title>Preprocessing pupil size data: guidelines and code.</article-title>
<source><italic>Behav. Res. Methods</italic></source>
<volume>51</volume>
<fpage>1336</fpage>–<lpage>1342</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-018-1075-y</pub-id>
<?supplied-pmid 29992408?><pub-id pub-id-type="pmid">29992408</pub-id></mixed-citation>
    </ref>
    <ref id="B79">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>N.</given-names></name><name><surname>Kohlbecher</surname><given-names>S.</given-names></name><name><surname>Schneider</surname><given-names>E.</given-names></name></person-group> (<year>2009</year>). “<article-title>A novel approach to video-based pupil tracking</article-title>,” in <source><italic>Proceedings of the Conf. Proc. IEEE Int. Conf. Syst. Man Cybern</italic></source>, <publisher-loc>San Antonio, TX</publisher-loc>, <fpage>1255</fpage>–<lpage>1262</lpage>. <pub-id pub-id-type="doi">10.1109/ICSMC.2009.5345909</pub-id></mixed-citation>
    </ref>
    <ref id="B80">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>La Morgia</surname><given-names>C.</given-names></name><name><surname>Carelli</surname><given-names>V.</given-names></name><name><surname>Carbonelli</surname><given-names>M.</given-names></name></person-group> (<year>2018</year>). <article-title>Melanopsin retinal ganglion cells and pupil: clinical implications for neuro-ophthalmology.</article-title>
<source><italic>Front. Neurol.</italic></source>
<volume>9</volume>:<issue>1047</issue>. <pub-id pub-id-type="doi">10.3389/fneur.2018.01047</pub-id>
<?supplied-pmid 30581410?><pub-id pub-id-type="pmid">30581410</pub-id></mixed-citation>
    </ref>
    <ref id="B81">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lanatà</surname><given-names>A.</given-names></name><name><surname>Armato</surname><given-names>A.</given-names></name><name><surname>Valenza</surname><given-names>G.</given-names></name><name><surname>Scilingo</surname><given-names>E. P.</given-names></name></person-group> (<year>2011</year>). “<article-title>Eye tracking and pupil size variation as response to affective stimuli: a preliminary study</article-title>,” in <source><italic>Proceedings of the 2011 5th International Conference on Pervasive Computing Technologies for Healthcare and Workshops, PervasiveHealth</italic></source>
<publisher-loc>Dublin</publisher-loc>, <fpage>78</fpage>–<lpage>84</lpage>. <pub-id pub-id-type="doi">10.4108/icst.pervasivehealth.2011.246056</pub-id></mixed-citation>
    </ref>
    <ref id="B82">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>J. W.</given-names></name><name><surname>Cho</surname><given-names>C. W.</given-names></name><name><surname>Shin</surname><given-names>K. Y.</given-names></name><name><surname>Lee</surname><given-names>E. C.</given-names></name><name><surname>Park</surname><given-names>K. R.</given-names></name></person-group> (<year>2012</year>). <article-title>3D gaze tracking method using Purkinje images on eye optical model and pupil.</article-title>
<source><italic>Opt. Lasers Eng.</italic></source>
<volume>50</volume>
<fpage>736</fpage>–<lpage>751</lpage>. <pub-id pub-id-type="doi">10.1016/j.optlaseng.2011.12.001</pub-id></mixed-citation>
    </ref>
    <ref id="B83">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lemercier</surname><given-names>A.</given-names></name><name><surname>Guillot</surname><given-names>G.</given-names></name><name><surname>Courcoux</surname><given-names>P.</given-names></name><name><surname>Garrel</surname><given-names>C.</given-names></name><name><surname>Baccino</surname><given-names>T.</given-names></name><name><surname>Schlich</surname><given-names>P.</given-names></name></person-group> (<year>2014</year>). <article-title>Pupillometry of taste: methodological guide – from acquisition to data processing - and toolbox for MATLAB.</article-title>
<source><italic>Quant. Methods Psychol.</italic></source>
<volume>10</volume>
<fpage>179</fpage>–<lpage>195</lpage>. <pub-id pub-id-type="doi">10.20982/tqmp.10.2.p179</pub-id></mixed-citation>
    </ref>
    <ref id="B84">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lennie</surname><given-names>P.</given-names></name><name><surname>Pokorny</surname><given-names>J.</given-names></name><name><surname>Smith</surname><given-names>V. C.</given-names></name></person-group> (<year>1993</year>). <article-title>Luminance.</article-title>
<source><italic>J. Opt. Soc. Am. A</italic></source>
<volume>10</volume>:<issue>1283</issue>. <pub-id pub-id-type="doi">10.1364/JOSAA.10.001283</pub-id>
<?supplied-pmid 8320586?><pub-id pub-id-type="pmid">8320586</pub-id></mixed-citation>
    </ref>
    <ref id="B85">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Li</surname><given-names>D.</given-names></name><name><surname>Winfield</surname><given-names>D.</given-names></name><name><surname>Parkhurst</surname><given-names>D. J.</given-names></name></person-group> (<year>2005</year>). “<article-title>Starburst: a hybrid algorithm for video-based eye tracking combining feature-based and model-based approaches</article-title>,” in <source><italic>Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05) - Workshops</italic></source>, (<publisher-loc>San Diego, CA</publisher-loc>: <publisher-name>IEEE</publisher-name>), 79. <pub-id pub-id-type="doi">10.1109/CVPR.2005.531</pub-id></mixed-citation>
    </ref>
    <ref id="B86">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>S.</given-names></name><name><surname>Chen</surname><given-names>T.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name></person-group> (<year>2018</year>). <article-title>A geometry-appearance-based pupil detection method for near-infrared head-mounted cameras.</article-title>
<source><italic>IEEE Access</italic></source>
<volume>6</volume>
<fpage>23242</fpage>–<lpage>23252</lpage>. <pub-id pub-id-type="doi">10.1109/ACCESS.2018.2828400</pub-id></mixed-citation>
    </ref>
    <ref id="B87">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lim</surname><given-names>J. K. H.</given-names></name><name><surname>Li</surname><given-names>Q. X.</given-names></name><name><surname>He</surname><given-names>Z.</given-names></name><name><surname>Vingrys</surname><given-names>A. J.</given-names></name><name><surname>Wong</surname><given-names>V. H. Y.</given-names></name><name><surname>Currier</surname><given-names>N.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>The eye as a biomarker for Alzheimer’s disease.</article-title>
<source><italic>Front. Neurosci.</italic></source>
<volume>10</volume>:<issue>536</issue>. <pub-id pub-id-type="doi">10.3389/fnins.2016.00536</pub-id>
<?supplied-pmid 27909396?><pub-id pub-id-type="pmid">27909396</pub-id></mixed-citation>
    </ref>
    <ref id="B88">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>L.</given-names></name><name><surname>Pan</surname><given-names>L.</given-names></name><name><surname>Wei</surname><given-names>L. F.</given-names></name><name><surname>Yu</surname><given-names>L.</given-names></name></person-group> (<year>2010</year>). “<article-title>A robust and accurate detection of pupil images</article-title>,” in <source><italic>Proceedings of the - 2010 3rd Int. Conf. Biomed. Eng. Informatics, BMEI 2010</italic></source>, <publisher-loc>Yantai</publisher-loc>, <fpage>70</fpage>–<lpage>74</lpage>. <pub-id pub-id-type="doi">10.1109/BMEI.2010.5639646</pub-id></mixed-citation>
    </ref>
    <ref id="B89">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>X.</given-names></name><name><surname>Craig</surname><given-names>J.</given-names></name><name><surname>Dean</surname><given-names>S.</given-names></name><name><surname>Klette</surname><given-names>G.</given-names></name><name><surname>Klette</surname><given-names>R.</given-names></name></person-group> (<year>2003</year>). <source><italic>Accurately Measuring the Size of the Pupil of the Eye.</italic></source>
<publisher-loc>Auckland</publisher-loc>: <publisher-name>CITR, The University of Auckland</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B90">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Long</surname><given-names>X.</given-names></name><name><surname>Tonguz</surname><given-names>O. K.</given-names></name><name><surname>Kiderman</surname><given-names>A.</given-names></name></person-group> (<year>2007</year>). “<article-title>A high speed eye tracking system with robust pupil center estimation algorithm</article-title>,” in <source><italic>Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology</italic></source>, <publisher-loc>Lyon</publisher-loc>, <fpage>3331</fpage>–<lpage>3334</lpage>. <pub-id pub-id-type="doi">10.1109/IEMBS.2007.4353043</pub-id>
<?supplied-pmid 18002709?></mixed-citation>
    </ref>
    <ref id="B91">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lucas</surname><given-names>R. J.</given-names></name><name><surname>Allen</surname><given-names>A. E.</given-names></name><name><surname>Milosavljevic</surname><given-names>N.</given-names></name><name><surname>Storchi</surname><given-names>R.</given-names></name><name><surname>Woelders</surname><given-names>T.</given-names></name></person-group> (<year>2020</year>). <article-title>Can We See with Melanopsin?</article-title>
<source><italic>Annu. Rev. Vis. Sci.</italic></source>
<volume>6</volume>
<fpage>453</fpage>–<lpage>468</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-vision-030320-041239</pub-id>
<?supplied-pmid 32491960?><pub-id pub-id-type="pmid">32491960</pub-id></mixed-citation>
    </ref>
    <ref id="B92">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lucas</surname><given-names>R. J.</given-names></name><name><surname>Douglas</surname><given-names>R. H.</given-names></name><name><surname>Foster</surname><given-names>R. G.</given-names></name></person-group> (<year>2001</year>). <article-title>Characterization of an ocular photopigment capable of driving pupillary constriction in mice.</article-title>
<source><italic>Nat. Neurosci.</italic></source>
<volume>4</volume>
<fpage>621</fpage>–<lpage>626</lpage>. <pub-id pub-id-type="doi">10.1038/88443</pub-id>
<?supplied-pmid 11369943?><pub-id pub-id-type="pmid">11369943</pub-id></mixed-citation>
    </ref>
    <ref id="B93">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lucas</surname><given-names>R. J.</given-names></name><name><surname>Peirson</surname><given-names>S. N.</given-names></name><name><surname>Berson</surname><given-names>D. M.</given-names></name><name><surname>Brown</surname><given-names>T. M.</given-names></name><name><surname>Cooper</surname><given-names>H. M.</given-names></name><name><surname>Czeisler</surname><given-names>C. A.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>Measuring and using light in the melanopsin age.</article-title>
<source><italic>Trends Neurosci.</italic></source>
<volume>37</volume>
<fpage>1</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2013.10.004</pub-id>
<?supplied-pmid 24287308?><pub-id pub-id-type="pmid">24287308</pub-id></mixed-citation>
    </ref>
    <ref id="B94">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maclean</surname><given-names>H.</given-names></name><name><surname>Dhillon</surname><given-names>B.</given-names></name></person-group> (<year>1993</year>). <article-title>Pupil cycle time and human immunodeficiency virus (hiv) infection.</article-title>
<source><italic>Eye</italic></source>
<volume>7</volume>
<fpage>785</fpage>–<lpage>786</lpage>. <pub-id pub-id-type="doi">10.1038/eye.1993.184</pub-id>
<?supplied-pmid 8119434?><pub-id pub-id-type="pmid">8119434</pub-id></mixed-citation>
    </ref>
    <ref id="B95">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manuri</surname><given-names>F.</given-names></name><name><surname>Sanna</surname><given-names>A.</given-names></name><name><surname>Petrucci</surname><given-names>C. P.</given-names></name></person-group> (<year>2020</year>). <article-title>PDIF: pupil detection after isolation and fitting.</article-title>
<source><italic>IEEE Access</italic></source>
<volume>8</volume>
<fpage>30826</fpage>–<lpage>30837</lpage>. <pub-id pub-id-type="doi">10.1109/ACCESS.2020.2973005</pub-id></mixed-citation>
    </ref>
    <ref id="B96">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martinikorena</surname><given-names>I.</given-names></name><name><surname>Cabeza</surname><given-names>R.</given-names></name><name><surname>Villanueva</surname><given-names>A.</given-names></name><name><surname>Urtasun</surname><given-names>I.</given-names></name><name><surname>Larumbe</surname><given-names>A.</given-names></name></person-group> (<year>2018</year>). <article-title>Fast and robust ellipse detection algorithm for head-mounted eye tracking systems.</article-title>
<source><italic>Mach. Vis. Appl.</italic></source>
<volume>29</volume>
<fpage>845</fpage>–<lpage>860</lpage>. <pub-id pub-id-type="doi">10.1007/s00138-018-0940-0</pub-id></mixed-citation>
    </ref>
    <ref id="B97">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazziotti</surname><given-names>R.</given-names></name><name><surname>Carrara</surname><given-names>F.</given-names></name><name><surname>Viglione</surname><given-names>A.</given-names></name><name><surname>Lupori</surname><given-names>L.</given-names></name><name><surname>Lo Verde</surname><given-names>L.</given-names></name><name><surname>Benedetto</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>MEYE: web-app for translational and real-time pupillometry.</article-title>
<source><italic>bioRxiv</italic></source> [Preprint]. <pub-id pub-id-type="doi">10.1101/2021.03.09.434438</pub-id></mixed-citation>
    </ref>
    <ref id="B98">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merritt</surname><given-names>S. L.</given-names></name><name><surname>Schnyders</surname><given-names>H. C.</given-names></name><name><surname>Patel</surname><given-names>M.</given-names></name><name><surname>Basner</surname><given-names>R. C.</given-names></name><name><surname>O’Neill</surname><given-names>W.</given-names></name></person-group> (<year>2004</year>). <article-title>Pupil staging and EEG measurement of sleepiness.</article-title>
<source><italic>Int. J. Psychophysiol.</italic></source>
<volume>52</volume>
<fpage>97</fpage>–<lpage>112</lpage>. <pub-id pub-id-type="doi">10.1016/j.ijpsycho.2003.12.007</pub-id>
<?supplied-pmid 15003376?><pub-id pub-id-type="pmid">15003376</pub-id></mixed-citation>
    </ref>
    <ref id="B99">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moon</surname><given-names>P.</given-names></name><name><surname>Spencer</surname><given-names>D. E.</given-names></name></person-group> (<year>1944</year>). <article-title>On the stiles-crawford effect.</article-title>
<source><italic>J. Opt. Soc. Am.</italic></source>
<volume>34</volume>:<issue>319</issue>. <pub-id pub-id-type="doi">10.1364/JOSA.34.000319</pub-id></mixed-citation>
    </ref>
    <ref id="B100">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morad</surname><given-names>Y.</given-names></name><name><surname>Lemberg</surname><given-names>H.</given-names></name><name><surname>Yofe</surname><given-names>N.</given-names></name><name><surname>Dagan</surname><given-names>Y.</given-names></name></person-group> (<year>2000</year>). <article-title>Pupillography as an objective indicator of fatigue.</article-title>
<source><italic>Curr. Eye Res.</italic></source>
<volume>21</volume>
<fpage>535</fpage>–<lpage>542</lpage>. <pub-id pub-id-type="doi">10.1076/0271-3683(200007)2111-ZFT535</pub-id><pub-id pub-id-type="pmid">11035533</pub-id></mixed-citation>
    </ref>
    <ref id="B101">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Morimoto</surname><given-names>C. H.</given-names></name><name><surname>Amir</surname><given-names>A.</given-names></name><name><surname>Flickner</surname><given-names>M.</given-names></name></person-group> (<year>2002</year>). “<article-title>Detecting eye position and gaze from a single camera and 2 light sources</article-title>,” in <source><italic>Proceedings of the International Conference on Pattern Recognition</italic></source>, <publisher-loc>Quebec City, QC</publisher-loc>, <fpage>314</fpage>–<lpage>317</lpage>. <pub-id pub-id-type="doi">10.1109/icpr.2002.1047459</pub-id></mixed-citation>
    </ref>
    <ref id="B102">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morimoto</surname><given-names>C. H.</given-names></name><name><surname>Koons</surname><given-names>D.</given-names></name><name><surname>Amir</surname><given-names>A.</given-names></name><name><surname>Flickner</surname><given-names>M.</given-names></name></person-group> (<year>2000</year>). <article-title>Pupil detection and tracking using multiple light sources.</article-title>
<source><italic>Image Vis. Comput.</italic></source>
<volume>18</volume>
<fpage>331</fpage>–<lpage>335</lpage>. <pub-id pub-id-type="doi">10.1016/S0262-8856(99)00053-0</pub-id></mixed-citation>
    </ref>
    <ref id="B103">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Münch</surname><given-names>M.</given-names></name><name><surname>Léon</surname><given-names>L.</given-names></name><name><surname>Crippa</surname><given-names>S. V.</given-names></name><name><surname>Kawasaki</surname><given-names>A.</given-names></name></person-group> (<year>2012</year>). <article-title>Circadian and wake-dependent effects on the pupil light reflex in response to narrow-bandwidth light pulses.</article-title>
<source><italic>Investig. Ophthalmol. Vis. Sci.</italic></source>
<volume>53</volume>
<fpage>4546</fpage>–<lpage>4555</lpage>. <pub-id pub-id-type="doi">10.1167/iovs.12-9494</pub-id>
<?supplied-pmid 22669721?><pub-id pub-id-type="pmid">22669721</pub-id></mixed-citation>
    </ref>
    <ref id="B104">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mure</surname><given-names>L. S.</given-names></name></person-group> (<year>2021</year>). <article-title>Intrinsically photosensitive retinal ganglion cells of the human retina.</article-title>
<source><italic>Front. Neurol.</italic></source>
<volume>12</volume>: 636330. <pub-id pub-id-type="doi">10.3389/fneur.2021.636330</pub-id>
<?supplied-pmid 33841306?><pub-id pub-id-type="pmid">33841306</pub-id></mixed-citation>
    </ref>
    <ref id="B105">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>P. R.</given-names></name><name><surname>Vandekerckhove</surname><given-names>J.</given-names></name><name><surname>Nieuwenhuis</surname><given-names>S.</given-names></name></person-group> (<year>2014</year>). <article-title>Pupil-linked arousal determines variability in perceptual decision making.</article-title>
<source><italic>PLoS Comput. Biol.</italic></source>
<volume>10</volume>: e1003854. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1003854</pub-id>
<?supplied-pmid 25232732?><pub-id pub-id-type="pmid">25232732</pub-id></mixed-citation>
    </ref>
    <ref id="B106">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>I. J.</given-names></name><name><surname>Kremers</surname><given-names>J.</given-names></name><name><surname>McKeefry</surname><given-names>D.</given-names></name><name><surname>Parry</surname><given-names>N. R. A.</given-names></name></person-group> (<year>2018</year>). <article-title>Paradoxical pupil responses to isolated M-cone increments.</article-title>
<source><italic>J. Opt. Soc. Am. A</italic></source>
<volume>35</volume>:<issue>B66</issue>. <pub-id pub-id-type="doi">10.1364/josaa.35.000b66</pub-id>
<?supplied-pmid 29603924?><pub-id pub-id-type="pmid">29603924</pub-id></mixed-citation>
    </ref>
    <ref id="B107">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>N. P.</given-names></name><name><surname>Hunfalvay</surname><given-names>M.</given-names></name><name><surname>Bolte</surname><given-names>T.</given-names></name></person-group> (<year>2017</year>). <article-title>The reliability, validity, and normative data of interpupillary distance and pupil diameter using eye-tracking technology.</article-title>
<source><italic>Transl. Vis. Sci. Technol.</italic></source>
<volume>6</volume>:<issue>2</issue>. <pub-id pub-id-type="doi">10.1167/tvst.6.4.2</pub-id>
<?supplied-pmid 28685104?><pub-id pub-id-type="pmid">28685104</pub-id></mixed-citation>
    </ref>
    <ref id="B108">
      <mixed-citation publication-type="journal"><collab>OpenCV</collab> (<year>2020</year>). <source><italic>OpenCV: Camera Calibration and 3D Reconstruction.</italic></source> Available online at: <ext-link ext-link-type="uri" xlink:href="https://docs.opencv.org/master/d9/d0c/group__calib3d.html">https://docs.opencv.org/master/d9/d0c/group__calib3d.html</ext-link>
<comment>(accessed November 16, 2020)</comment>.</mixed-citation>
    </ref>
    <ref id="B109">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ostrin</surname><given-names>L. A.</given-names></name><name><surname>Abbott</surname><given-names>K. S.</given-names></name><name><surname>Queener</surname><given-names>H. M.</given-names></name></person-group> (<year>2017</year>). <article-title>Attenuation of short wavelengths alters sleep and the ipRGC pupil response.</article-title>
<source><italic>Ophthalmic Physiol. Opt.</italic></source>
<volume>37</volume>
<fpage>440</fpage>–<lpage>450</lpage>. <pub-id pub-id-type="doi">10.1111/opo.12385</pub-id>
<?supplied-pmid 28656675?><pub-id pub-id-type="pmid">28656675</pub-id></mixed-citation>
    </ref>
    <ref id="B110">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedrotti</surname><given-names>M.</given-names></name><name><surname>Lei</surname><given-names>S.</given-names></name><name><surname>Dzaack</surname><given-names>J.</given-names></name><name><surname>Rötting</surname><given-names>M.</given-names></name></person-group> (<year>2011</year>). <article-title>A data-driven algorithm for offline pupil signal preprocessing and eyeblink detection in low-speed eye-tracking protocols.</article-title>
<source><italic>Behav. Res. Methods</italic></source>
<volume>43</volume>
<fpage>372</fpage>–<lpage>383</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-010-0055-7</pub-id>
<?supplied-pmid 21302023?><pub-id pub-id-type="pmid">21302023</pub-id></mixed-citation>
    </ref>
    <ref id="B111">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedrotti</surname><given-names>M.</given-names></name><name><surname>Mirzaei</surname><given-names>M. A.</given-names></name><name><surname>Tedesco</surname><given-names>A.</given-names></name><name><surname>Chardonnet</surname><given-names>J. R.</given-names></name><name><surname>Mérienne</surname><given-names>F.</given-names></name><name><surname>Benedetto</surname><given-names>S.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>Automatic stress classification with pupil diameter analysis.</article-title>
<source><italic>Int. J. Hum. Comput. Interact.</italic></source>
<volume>30</volume>
<fpage>220</fpage>–<lpage>236</lpage>. <pub-id pub-id-type="doi">10.1080/10447318.2013.848320</pub-id></mixed-citation>
    </ref>
    <ref id="B112">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pérez</surname><given-names>A.</given-names></name><name><surname>Córdoba</surname><given-names>M. L.</given-names></name><name><surname>García</surname><given-names>A.</given-names></name><name><surname>Méndez</surname><given-names>R.</given-names></name><name><surname>Muñoz</surname><given-names>M. L.</given-names></name><name><surname>Pedraza</surname><given-names>J. L.</given-names></name><etal/></person-group> (<year>2003</year>). <source><italic>A Precise Eye-Gaze Detection and Tracking System.</italic></source>
<publisher-loc>Plzen, Czech Republic</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="B113">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinheiro</surname><given-names>H. M.</given-names></name><name><surname>da Costa</surname><given-names>R. M.</given-names></name></person-group> (<year>2021</year>). <article-title>Pupillary light reflex as a diagnostic aid from computational viewpoint: a systematic literature review.</article-title>
<source><italic>J. Biomed. Inform.</italic></source>
<volume>117</volume>:<issue>103757</issue>. <pub-id pub-id-type="doi">10.1016/j.jbi.2021.103757</pub-id>
<?supplied-pmid 33826949?><pub-id pub-id-type="pmid">33826949</pub-id></mixed-citation>
    </ref>
    <ref id="B114">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Provencio</surname><given-names>I.</given-names></name><name><surname>Jiang</surname><given-names>G.</given-names></name><name><surname>De Grip</surname><given-names>W. J.</given-names></name><name><surname>Pär Hayes</surname><given-names>W.</given-names></name><name><surname>Rollag</surname><given-names>M. D.</given-names></name></person-group> (<year>1998</year>). <article-title>Melanopsin: an opsin in melanophores, brain, and eye.</article-title>
<source><italic>Proc. Natl. Acad. Sci. U.S.A.</italic></source>
<volume>95</volume>
<fpage>340</fpage>–<lpage>345</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.95.1.340</pub-id>
<?supplied-pmid 9419377?><pub-id pub-id-type="pmid">9419377</pub-id></mixed-citation>
    </ref>
    <ref id="B115">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Provencio</surname><given-names>I.</given-names></name><name><surname>Rodriguez</surname><given-names>I. R.</given-names></name><name><surname>Jiang</surname><given-names>G.</given-names></name><name><surname>Hayes</surname><given-names>W. P.</given-names></name><name><surname>Moreira</surname><given-names>E. F.</given-names></name><name><surname>Rollag</surname><given-names>M. D.</given-names></name></person-group> (<year>2000</year>). <article-title>A novel human opsin in the inner retina.</article-title>
<source><italic>J. Neurosci.</italic></source>
<volume>20</volume>
<fpage>600</fpage>–<lpage>605</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-02-00600.2000</pub-id>
<?supplied-pmid 10632589?><pub-id pub-id-type="pmid">10632589</pub-id></mixed-citation>
    </ref>
    <ref id="B116">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>F.</given-names></name><name><surname>Chan</surname><given-names>A. H. S.</given-names></name><name><surname>Zhu</surname><given-names>X. F.</given-names></name></person-group> (<year>2017</year>). <article-title>Effects of photopic and cirtopic illumination on steady state pupil sizes.</article-title>
<source><italic>Vision Res.</italic></source>
<volume>137</volume>
<fpage>24</fpage>–<lpage>28</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2017.02.010</pub-id>
<?supplied-pmid 28688906?><pub-id pub-id-type="pmid">28688906</pub-id></mixed-citation>
    </ref>
    <ref id="B117">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rea</surname><given-names>M. S.</given-names></name><name><surname>Figueiro</surname><given-names>M. G.</given-names></name></person-group> (<year>2018</year>). <article-title>Light as a circadian stimulus for architectural lighting.</article-title>
<source><italic>Light. Res. Technol.</italic></source>
<volume>50</volume>
<fpage>497</fpage>–<lpage>510</lpage>. <pub-id pub-id-type="doi">10.1177/1477153516682368</pub-id></mixed-citation>
    </ref>
    <ref id="B118">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reeves</surname><given-names>P.</given-names></name></person-group> (<year>1918</year>). <article-title>Rate of pupillary dilation and contraction.</article-title>
<source><italic>Psychol. Rev.</italic></source>
<volume>25</volume>
<fpage>330</fpage>–<lpage>340</lpage>. <pub-id pub-id-type="doi">10.1037/h0075293</pub-id></mixed-citation>
    </ref>
    <ref id="B119">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rote</surname><given-names>G.</given-names></name></person-group> (<year>1991</year>). <article-title>Computing the minimum Hausdorff distance between two point sets on a line under translation.</article-title>
<source><italic>Inf. Process. Lett.</italic></source>
<volume>38</volume>
<fpage>123</fpage>–<lpage>127</lpage>. <pub-id pub-id-type="doi">10.1016/0020-0190(91)90233-8</pub-id></mixed-citation>
    </ref>
    <ref id="B120">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruby</surname><given-names>N. F.</given-names></name><name><surname>Brennan</surname><given-names>T. J.</given-names></name><name><surname>Xie</surname><given-names>X.</given-names></name><name><surname>Cao</surname><given-names>V.</given-names></name><name><surname>Franken</surname><given-names>P.</given-names></name><name><surname>Heller</surname><given-names>H. C.</given-names></name><etal/></person-group> (<year>2002</year>). <article-title>Role of melanopsin in circadian responses to light.</article-title>
<source><italic>Science</italic></source>
<volume>298</volume>
<fpage>2211</fpage>–<lpage>2213</lpage>. <pub-id pub-id-type="doi">10.1126/science.1076701</pub-id>
<?supplied-pmid 12481140?><pub-id pub-id-type="pmid">12481140</pub-id></mixed-citation>
    </ref>
    <ref id="B121">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rukmini</surname><given-names>A. V.</given-names></name><name><surname>Milea</surname><given-names>D.</given-names></name><name><surname>Aung</surname><given-names>T.</given-names></name><name><surname>Gooley</surname><given-names>J. J.</given-names></name></person-group> (<year>2017</year>). <article-title>Pupillary responses to short-wavelength light are preserved in aging.</article-title>
<source><italic>Sci. Rep.</italic></source>
<volume>7</volume>
<fpage>1</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1038/srep43832</pub-id>
<?supplied-pmid 28266650?><pub-id pub-id-type="pmid">28127051</pub-id></mixed-citation>
    </ref>
    <ref id="B122">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sagawa</surname><given-names>K.</given-names></name></person-group> (<year>2006</year>). <article-title>Toward a CIE supplementary system of photometry: brightness at any level including mesopic vision.</article-title>
<source><italic>Ophthalmic Physiol. Opt.</italic></source>
<volume>26</volume>
<fpage>240</fpage>–<lpage>245</lpage>. <pub-id pub-id-type="doi">10.1111/j.1475-1313.2006.00357.x</pub-id>
<?supplied-pmid 16684150?><pub-id pub-id-type="pmid">16684150</pub-id></mixed-citation>
    </ref>
    <ref id="B123">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>San Agustin</surname><given-names>J.</given-names></name><name><surname>Skovsgaard</surname><given-names>H.</given-names></name><name><surname>Mollenbach</surname><given-names>E.</given-names></name><name><surname>Barret</surname><given-names>M.</given-names></name><name><surname>Tall</surname><given-names>M.</given-names></name><name><surname>Hansen</surname><given-names>D. W.</given-names></name><etal/></person-group> (<year>2010</year>). “<article-title>Evaluation of a low-cost open-source gaze tracker</article-title>,” in <source><italic>Proceedings of the 2010 Symposium on Eye-Tracking Research &amp; Applications</italic></source>, <publisher-loc>Austin, TX</publisher-loc>, <fpage>77</fpage>–<lpage>80</lpage>. <pub-id pub-id-type="doi">10.1145/1743666.1743685</pub-id></mixed-citation>
    </ref>
    <ref id="B124">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Santini</surname><given-names>T.</given-names></name><name><surname>Fuhl</surname><given-names>W.</given-names></name><name><surname>Geisler</surname><given-names>D.</given-names></name><name><surname>Kasneci</surname><given-names>E.</given-names></name></person-group> (<year>2017</year>). “<article-title>EyeRecToo: open-source software for real-time pervasive head-mounted eye tracking</article-title>,” in <source><italic>VISIGRAPP 2017 Proceedings of the 12th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</italic></source>, (<publisher-loc>Setúbal</publisher-loc>: <publisher-name>SciTePress</publisher-name>), <fpage>96</fpage>–<lpage>101</lpage>. <pub-id pub-id-type="doi">10.5220/0006224700960101</pub-id></mixed-citation>
    </ref>
    <ref id="B125">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santini</surname><given-names>T.</given-names></name><name><surname>Fuhl</surname><given-names>W.</given-names></name><name><surname>Kasneci</surname><given-names>E.</given-names></name></person-group> (<year>2018a</year>). <article-title>PuRe: robust pupil detection for real-time pervasive eye tracking.</article-title>
<source><italic>Comput. Vis. Image Underst.</italic></source>
<volume>170</volume>
<fpage>40</fpage>–<lpage>50</lpage>. <pub-id pub-id-type="doi">10.1016/j.cviu.2018.02.002</pub-id></mixed-citation>
    </ref>
    <ref id="B126">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Santini</surname><given-names>T.</given-names></name><name><surname>Fuhl</surname><given-names>W.</given-names></name><name><surname>Kasneci</surname><given-names>E.</given-names></name></person-group> (<year>2018b</year>). “<article-title>PuReST: robust pupil tracking for real-time pervasive eye tracking</article-title>,” in <source><italic>Proceedings of the 2018 ACM Symposium on Eye Tracking Research &amp; Applications (ETRA), 2018</italic></source>, <publisher-loc>Warsaw</publisher-loc>. <pub-id pub-id-type="doi">10.1145/3204493.3204578</pub-id></mixed-citation>
    </ref>
    <ref id="B127">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schluroff</surname><given-names>M.</given-names></name><name><surname>Zimmermann</surname><given-names>T. E.</given-names></name><name><surname>Freeman</surname><given-names>R. B.</given-names></name><name><surname>Hofmeister</surname><given-names>K.</given-names></name><name><surname>Lorscheid</surname><given-names>T.</given-names></name><name><surname>Weber</surname><given-names>A.</given-names></name></person-group> (<year>1986</year>). <article-title>Pupillary responses to syntactic ambiguity of sentences.</article-title>
<source><italic>Brain Lang.</italic></source>
<volume>27</volume>
<fpage>322</fpage>–<lpage>344</lpage>. <pub-id pub-id-type="doi">10.1016/0093-934X(86)90023-4</pub-id><pub-id pub-id-type="pmid">3513899</pub-id></mixed-citation>
    </ref>
    <ref id="B128">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>T. M.</given-names></name><name><surname>Alam</surname><given-names>N. M.</given-names></name><name><surname>Chen</surname><given-names>S.</given-names></name><name><surname>Kofuji</surname><given-names>P.</given-names></name><name><surname>Li</surname><given-names>W.</given-names></name><name><surname>Prusky</surname><given-names>G. T.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>A role for melanopsin in alpha retinal ganglion cells and contrast detection.</article-title>
<source><italic>Neuron</italic></source>
<volume>82</volume>
<fpage>781</fpage>–<lpage>788</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2014.03.022</pub-id>
<?supplied-pmid 24853938?><pub-id pub-id-type="pmid">24853938</pub-id></mixed-citation>
    </ref>
    <ref id="B129">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>M.</given-names></name><name><surname>Elbau</surname><given-names>I. G.</given-names></name><name><surname>Nantawisarakul</surname><given-names>T.</given-names></name><name><surname>Pöhlchen</surname><given-names>D.</given-names></name><name><surname>Brückl</surname><given-names>T.</given-names></name></person-group><collab>BeCOME Working</collab><etal/> (<year>2020</year>). <article-title>Pupil dilation during reward anticipation is correlated to depressive symptom load in patients with major depressive disorder.</article-title>
<source><italic>Brain Sci.</italic></source>
<volume>10</volume>:<issue>906</issue>. <pub-id pub-id-type="doi">10.3390/brainsci10120906</pub-id>
<?supplied-pmid 33255604?><pub-id pub-id-type="pmid">33255604</pub-id></mixed-citation>
    </ref>
    <ref id="B130">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwalm</surname><given-names>M.</given-names></name><name><surname>Jubal</surname><given-names>E. R.</given-names></name></person-group> (<year>2017</year>). <article-title>Back to pupillometry: how cortical network state fluctuations tracked by pupil dynamics could explain neural signal variability in human cognitive neuroscience.</article-title>
<source><italic>eNeuro</italic></source>
<volume>4</volume>: ENEURO.0293-16.2017. <pub-id pub-id-type="doi">10.1523/ENEURO.0293-16.2017</pub-id>
<?supplied-pmid 29379876?><pub-id pub-id-type="pmid">29379876</pub-id></mixed-citation>
    </ref>
    <ref id="B131">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schwarz</surname><given-names>L.</given-names></name><name><surname>Gamba</surname><given-names>H. R.</given-names></name><name><surname>Pacheco</surname><given-names>F. C.</given-names></name><name><surname>Ramos</surname><given-names>R. B.</given-names></name><name><surname>Sovierzoski</surname><given-names>M. A.</given-names></name></person-group> (<year>2012</year>). “<article-title>Pupil and iris detection in dynamic pupillometry using the OpenCV library</article-title>,” in <source><italic>Proceedings of the 2012 5th Int. Congr. Image Signal Process. CISP 2012</italic></source>, <publisher-loc>Chongqing</publisher-loc>, <fpage>211</fpage>–<lpage>215</lpage>. <pub-id pub-id-type="doi">10.1109/CISP.2012.6469846</pub-id></mixed-citation>
    </ref>
    <ref id="B132">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwiegerling</surname><given-names>J.</given-names></name></person-group> (<year>2000</year>). <article-title>Theoretical limits to visual performance.</article-title>
<source><italic>Surv. Ophthalmol.</italic></source>
<volume>45</volume>
<fpage>139</fpage>–<lpage>146</lpage>. <pub-id pub-id-type="doi">10.1016/S0039-6257(00)00145-4</pub-id><pub-id pub-id-type="pmid">11033040</pub-id></mixed-citation>
    </ref>
    <ref id="B133">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharpe</surname><given-names>L. T.</given-names></name><name><surname>Stockman</surname><given-names>A.</given-names></name><name><surname>Jagla</surname><given-names>W.</given-names></name><name><surname>Jaägle</surname><given-names>H.</given-names></name></person-group> (<year>2005</year>). <article-title>A luminous efficiency function, V<sup>∗</sup>(λ), for daylight adaptation.</article-title>
<source><italic>J. Vis.</italic></source>
<volume>5</volume>
<fpage>948</fpage>–<lpage>968</lpage>. <pub-id pub-id-type="doi">10.1167/5.11.3</pub-id><pub-id pub-id-type="pmid">16441195</pub-id></mixed-citation>
    </ref>
    <ref id="B134">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sibley</surname><given-names>C.</given-names></name><name><surname>Foroughi</surname><given-names>C. K.</given-names></name><name><surname>Brown</surname><given-names>N. L.</given-names></name><name><surname>Phillips</surname><given-names>H.</given-names></name><name><surname>Drollinger</surname><given-names>S.</given-names></name><name><surname>Eagle</surname><given-names>M.</given-names></name><etal/></person-group> (<year>2020</year>). <article-title>More than means: characterizing individual differences in pupillary dilations.</article-title>
<source><italic>Proc. Hum. Factors Ergon. Soc. Annu. Meet.</italic></source>
<volume>64</volume>
<fpage>57</fpage>–<lpage>61</lpage>. <pub-id pub-id-type="doi">10.1177/1071181320641017</pub-id></mixed-citation>
    </ref>
    <ref id="B135">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>V. C.</given-names></name><name><surname>Pokorny</surname><given-names>J.</given-names></name><name><surname>Lee</surname><given-names>B. B.</given-names></name><name><surname>Dacey</surname><given-names>D. M.</given-names></name></person-group> (<year>2008</year>). <article-title>Sequential processing in vision: the interaction of sensitivity regulation and temporal dynamics.</article-title>
<source><italic>Vision Res.</italic></source>
<volume>48</volume>
<fpage>2649</fpage>–<lpage>2656</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2008.05.002</pub-id>
<?supplied-pmid 18558416?><pub-id pub-id-type="pmid">18558416</pub-id></mixed-citation>
    </ref>
    <ref id="B136">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solomon</surname><given-names>S. G.</given-names></name><name><surname>Lennie</surname><given-names>P.</given-names></name></person-group> (<year>2007</year>). <article-title>The machinery of colour vision.</article-title>
<source><italic>Nat. Rev. Neurosci.</italic></source>
<volume>8</volume>
<fpage>276</fpage>–<lpage>286</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2094</pub-id>
<?supplied-pmid 17375040?><pub-id pub-id-type="pmid">17375040</pub-id></mixed-citation>
    </ref>
    <ref id="B137">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spitschan</surname><given-names>M.</given-names></name></person-group> (<year>2019a</year>). <article-title>Melanopsin contributions to non-visual and visual function.</article-title>
<source><italic>Curr. Opin. Behav. Sci.</italic></source>
<volume>30</volume>
<fpage>67</fpage>–<lpage>72</lpage>. <pub-id pub-id-type="doi">10.1016/j.cobeha.2019.06.004</pub-id>
<?supplied-pmid 31396546?><pub-id pub-id-type="pmid">31396546</pub-id></mixed-citation>
    </ref>
    <ref id="B138">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spitschan</surname><given-names>M.</given-names></name></person-group> (<year>2019b</year>). <article-title>Photoreceptor inputs to pupil control.</article-title>
<source><italic>J. Vis.</italic></source>
<volume>19</volume>:<issue>5</issue>. <pub-id pub-id-type="doi">10.1167/19.9.5</pub-id></mixed-citation>
    </ref>
    <ref id="B139">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spitschan</surname><given-names>M.</given-names></name><name><surname>Lazar</surname><given-names>R.</given-names></name><name><surname>Yetik</surname><given-names>E.</given-names></name><name><surname>Cajochen</surname><given-names>C.</given-names></name></person-group> (<year>2019</year>). <article-title>No evidence for an S cone contribution to acute neuroendocrine and alerting responses to light.</article-title>
<source><italic>Curr. Biol.</italic></source>
<volume>29</volume>
<fpage>R1297</fpage>–<lpage>R1298</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2019.11.031</pub-id>
<?supplied-pmid 31846672?><pub-id pub-id-type="pmid">31846672</pub-id></mixed-citation>
    </ref>
    <ref id="B140">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stanley</surname><given-names>P.</given-names></name><name><surname>Davies</surname><given-names>A.</given-names></name></person-group> (<year>1995</year>). <article-title>The effect of field of view size on steady-state pupil diameter.</article-title>
<source><italic>Ophthalmic Physiol. Opt.</italic></source>
<volume>15</volume>
<fpage>601</fpage>–<lpage>603</lpage>. <pub-id pub-id-type="doi">10.1016/0275-5408(94)00019-V</pub-id><pub-id pub-id-type="pmid">8594531</pub-id></mixed-citation>
    </ref>
    <ref id="B141">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stockman</surname><given-names>A.</given-names></name><name><surname>Sharpe</surname><given-names>L. T.</given-names></name></person-group> (<year>2000</year>). <article-title>The spectral sensitivities of the middle- and long-wavelength-sensitive cones derived from measurements in observers of known genotype.</article-title>
<source><italic>Vision Res.</italic></source>
<volume>40</volume>
<fpage>1711</fpage>–<lpage>1737</lpage>. <pub-id pub-id-type="doi">10.1016/S0042-6989(00)00021-3</pub-id><pub-id pub-id-type="pmid">10814758</pub-id></mixed-citation>
    </ref>
    <ref id="B142">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Świrski</surname><given-names>L.</given-names></name><name><surname>Bulling</surname><given-names>A.</given-names></name><name><surname>Dodgson</surname><given-names>N.</given-names></name></person-group> (<year>2012</year>). “<article-title>Robust real-time pupil tracking in highly off-axis images</article-title>,” in <source><italic>ETRA ’12: Proceedings of the Symposium on Eye Tracking Research and Applications</italic></source>, <publisher-loc>Santa Barbara, CA</publisher-loc>, <fpage>173</fpage>–<lpage>176</lpage>. <pub-id pub-id-type="doi">10.1145/2168556.2168585</pub-id></mixed-citation>
    </ref>
    <ref id="B143">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Świrski</surname><given-names>L.</given-names></name><name><surname>Dodgson</surname><given-names>N. A.</given-names></name></person-group> (<year>2013</year>). “<article-title>A fully-automatic, temporal approach to single camera, glint-free 3D eye model fitting</article-title>,” in <source><italic>Proceedings of the Pervasive Eye Track. Mob. Eye-Based Interact</italic></source>, <publisher-loc>Lund</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="B144">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tabashum</surname><given-names>T.</given-names></name><name><surname>Zaffer</surname><given-names>A.</given-names></name><name><surname>Yousefzai</surname><given-names>R.</given-names></name><name><surname>Colletta</surname><given-names>K.</given-names></name><name><surname>Jost</surname><given-names>M. B.</given-names></name><name><surname>Park</surname><given-names>Y.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>Detection of Parkinson’s disease through automated pupil tracking of the post-illumination pupillary response.</article-title>
<source><italic>Front. Med.</italic></source>
<volume>8</volume>:<issue>645293</issue>. <pub-id pub-id-type="doi">10.3389/fmed.2021.645293</pub-id>
<?supplied-pmid 33842509?><pub-id pub-id-type="pmid">33842509</pub-id></mixed-citation>
    </ref>
    <ref id="B145">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tähkämö</surname><given-names>L.</given-names></name><name><surname>Partonen</surname><given-names>T.</given-names></name><name><surname>Pesonen</surname><given-names>A. K.</given-names></name></person-group> (<year>2019</year>). <article-title>Systematic review of light exposure impact on human circadian rhythm.</article-title>
<source><italic>Chronobiol. Int.</italic></source>
<volume>36</volume>
<fpage>151</fpage>–<lpage>170</lpage>. <pub-id pub-id-type="doi">10.1080/07420528.2018.1527773</pub-id>
<?supplied-pmid 30311830?><pub-id pub-id-type="pmid">30311830</pub-id></mixed-citation>
    </ref>
    <ref id="B146">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thaler</surname><given-names>L.</given-names></name><name><surname>Schütz</surname><given-names>A. C.</given-names></name><name><surname>Goodale</surname><given-names>M. A.</given-names></name><name><surname>Gegenfurtner</surname><given-names>K. R.</given-names></name></person-group> (<year>2013</year>). <article-title>What is the best fixation target? The effect of target shape on stability of fixational eye movements.</article-title>
<source><italic>Vision Res.</italic></source>
<volume>76</volume>
<fpage>31</fpage>–<lpage>42</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2012.10.012</pub-id>
<?supplied-pmid 23099046?><pub-id pub-id-type="pmid">23099046</pub-id></mixed-citation>
    </ref>
    <ref id="B147">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thapan</surname><given-names>K.</given-names></name><name><surname>Arendt</surname><given-names>J.</given-names></name><name><surname>Skene</surname><given-names>D. J.</given-names></name></person-group> (<year>2001</year>). <article-title>An action spectrum for melatonin suppression: evidence for a novel non-rod, non-cone photoreceptor system in humans.</article-title>
<source><italic>J. Physiol.</italic></source>
<volume>535</volume>
<fpage>261</fpage>–<lpage>267</lpage>. <pub-id pub-id-type="doi">10.1111/j.1469-7793.2001.t01-1-00261.x</pub-id>
<?supplied-pmid 11507175?><pub-id pub-id-type="pmid">11507175</pub-id></mixed-citation>
    </ref>
    <ref id="B148">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Titz</surname><given-names>J.</given-names></name><name><surname>Scholz</surname><given-names>A.</given-names></name><name><surname>Sedlmeier</surname><given-names>P.</given-names></name></person-group> (<year>2018</year>). <article-title>Comparing eye trackers by correlating their eye-metric data.</article-title>
<source><italic>Behav. Res. Methods</italic></source>
<volume>50</volume>
<fpage>1853</fpage>–<lpage>1863</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-017-0954-y</pub-id>
<?supplied-pmid 28879442?><pub-id pub-id-type="pmid">28879442</pub-id></mixed-citation>
    </ref>
    <ref id="B149">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tkacz-Domb</surname><given-names>S.</given-names></name><name><surname>Yeshurun</surname><given-names>Y.</given-names></name></person-group> (<year>2018</year>). <article-title>The size of the attentional window when measured by the pupillary response to light.</article-title>
<source><italic>Sci. Rep.</italic></source>
<volume>8</volume>
<fpage>1</fpage>–<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1038/s41598-018-30343-7</pub-id>
<?supplied-pmid 30089801?><pub-id pub-id-type="pmid">29311619</pub-id></mixed-citation>
    </ref>
    <ref id="B150">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Topal</surname><given-names>C.</given-names></name><name><surname>Cakir</surname><given-names>H. I.</given-names></name><name><surname>Akinlar</surname><given-names>C.</given-names></name></person-group> (<year>2017</year>). <source><italic>APPD.</italic></source> Available online at: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1709.06366">http://arxiv.org/abs/1709.06366</ext-link>
<comment>(accessed 28th December, 2020)</comment>.</mixed-citation>
    </ref>
    <ref id="B151">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Truong</surname><given-names>W.</given-names></name><name><surname>Zandi</surname><given-names>B.</given-names></name><name><surname>Trinh</surname><given-names>V. Q.</given-names></name><name><surname>Khanh</surname><given-names>T. Q.</given-names></name></person-group> (<year>2020</year>). <article-title>Circadian metric – Computation of circadian stimulus using illuminance, correlated colour temperature and colour rendering index.</article-title>
<source><italic>Build. Environ.</italic></source>
<volume>184</volume>:<issue>107146</issue>. <pub-id pub-id-type="doi">10.1016/j.buildenv.2020.107146</pub-id></mixed-citation>
    </ref>
    <ref id="B152">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsukahara</surname><given-names>J. S.</given-names></name><name><surname>Harrison</surname><given-names>T. L.</given-names></name><name><surname>Engle</surname><given-names>R. W.</given-names></name></person-group> (<year>2016</year>). <article-title>The relationship between baseline pupil size and intelligence.</article-title>
<source><italic>Cogn. Psychol.</italic></source>
<volume>91</volume>
<fpage>109</fpage>–<lpage>123</lpage>. <pub-id pub-id-type="doi">10.1016/j.cogpsych.2016.10.001</pub-id>
<?supplied-pmid 27821254?><pub-id pub-id-type="pmid">27821254</pub-id></mixed-citation>
    </ref>
    <ref id="B153">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van der Stoep</surname><given-names>N.</given-names></name><name><surname>Van der Smagt</surname><given-names>M. J.</given-names></name><name><surname>Notaro</surname><given-names>C.</given-names></name><name><surname>Spock</surname><given-names>Z.</given-names></name><name><surname>Naber</surname><given-names>M.</given-names></name></person-group> (<year>2021</year>). <article-title>The additive nature of the human multisensory evoked pupil response.</article-title>
<source><italic>Sci. Rep.</italic></source>
<volume>11</volume>:<issue>707</issue>. <pub-id pub-id-type="doi">10.1038/s41598-020-80286-1</pub-id>
<?supplied-pmid 33436889?><pub-id pub-id-type="pmid">33436889</pub-id></mixed-citation>
    </ref>
    <ref id="B154">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Egroo</surname><given-names>M.</given-names></name><name><surname>Gaggioni</surname><given-names>G.</given-names></name><name><surname>Cespedes-Ortiz</surname><given-names>C.</given-names></name><name><surname>Ly</surname><given-names>J. Q. M.</given-names></name><name><surname>Vandewalle</surname><given-names>G.</given-names></name></person-group> (<year>2019</year>). <article-title>Steady-state pupil size varies with circadian phase and sleep homeostasis in healthy young men.</article-title>
<source><italic>Clocks Sleep</italic></source>
<volume>1</volume>
<fpage>240</fpage>–<lpage>258</lpage>. <pub-id pub-id-type="doi">10.3390/clockssleep1020021</pub-id>
<?supplied-pmid 33089167?><pub-id pub-id-type="pmid">33089167</pub-id></mixed-citation>
    </ref>
    <ref id="B155">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Meeteren</surname><given-names>A.</given-names></name></person-group> (<year>1978</year>). <article-title>On the detective quantum efficiency of the human eye.</article-title>
<source><italic>Vision Res.</italic></source>
<volume>18</volume>
<fpage>257</fpage>–<lpage>267</lpage>. <pub-id pub-id-type="doi">10.1016/0042-6989(78)90160-8</pub-id><pub-id pub-id-type="pmid">664301</pub-id></mixed-citation>
    </ref>
    <ref id="B156">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Rij</surname><given-names>J.</given-names></name><name><surname>Hendriks</surname><given-names>P.</given-names></name><name><surname>van Rijn</surname><given-names>H.</given-names></name><name><surname>Baayen</surname><given-names>R. H.</given-names></name><name><surname>Wood</surname><given-names>S. N.</given-names></name></person-group> (<year>2019</year>). <article-title>Analyzing the time course of pupillometric data.</article-title>
<source><italic>Trends Hear.</italic></source>
<volume>23</volume>
<fpage>1</fpage>–<lpage>22</lpage>. <pub-id pub-id-type="doi">10.1177/2331216519832483</pub-id>
<?supplied-pmid 31081486?><pub-id pub-id-type="pmid">31081486</pub-id></mixed-citation>
    </ref>
    <ref id="B157">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vera-Olmos</surname><given-names>F. J.</given-names></name><name><surname>Pardo</surname><given-names>E.</given-names></name><name><surname>Melero</surname><given-names>H.</given-names></name><name><surname>Malpica</surname><given-names>N.</given-names></name></person-group> (<year>2018</year>). <article-title>DeepEye: deep convolutional network for pupil detection in real environments.</article-title>
<source><italic>Integr. Comput. Aided. Eng.</italic></source>
<volume>26</volume>
<fpage>85</fpage>–<lpage>95</lpage>. <pub-id pub-id-type="doi">10.3233/ICA-180584</pub-id></mixed-citation>
    </ref>
    <ref id="B158">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>D.</given-names></name><name><surname>Mulvey</surname><given-names>F. B.</given-names></name><name><surname>Pelz</surname><given-names>J. B.</given-names></name><name><surname>Holmqvist</surname><given-names>K.</given-names></name></person-group> (<year>2017</year>). <article-title>A study of artificial eyes for the measurement of precision in eye-trackers.</article-title>
<source><italic>Behav. Res. Methods</italic></source>
<volume>49</volume>
<fpage>947</fpage>–<lpage>959</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-016-0755-8</pub-id>
<?supplied-pmid 27383751?><pub-id pub-id-type="pmid">27383751</pub-id></mixed-citation>
    </ref>
    <ref id="B159">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname><given-names>A. B.</given-names></name><name><surname>Yellott</surname><given-names>J. I.</given-names></name></person-group> (<year>2012</year>). <article-title>A unified formula for light-adapted pupil size.</article-title>
<source><italic>J. Vis.</italic></source>
<volume>12</volume>
<fpage>1</fpage>–<lpage>16</lpage>. <pub-id pub-id-type="doi">10.1167/12.10.12</pub-id></mixed-citation>
    </ref>
    <ref id="B160">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wildemeersch</surname><given-names>D.</given-names></name><name><surname>Baeten</surname><given-names>M.</given-names></name><name><surname>Peeters</surname><given-names>N.</given-names></name><name><surname>Saldien</surname><given-names>V.</given-names></name><name><surname>Vercauteren</surname><given-names>M.</given-names></name><name><surname>Hans</surname><given-names>G.</given-names></name></person-group> (<year>2018</year>). <article-title>Pupillary dilation reflex and pupillary pain index evaluation during general anaesthesia: a pilot study.</article-title>
<source><italic>Rom. J. Anaesth. Intensive Care</italic></source>
<volume>25</volume>
<fpage>19</fpage>–<lpage>23</lpage>. <pub-id pub-id-type="doi">10.21454/rjaic.7518.251.wil</pub-id>
<?supplied-pmid 29756058?><pub-id pub-id-type="pmid">29756058</pub-id></mixed-citation>
    </ref>
    <ref id="B161">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winn</surname><given-names>M. B.</given-names></name><name><surname>Wendt</surname><given-names>D.</given-names></name><name><surname>Koelewijn</surname><given-names>T.</given-names></name><name><surname>Kuchinsky</surname><given-names>S. E.</given-names></name></person-group> (<year>2018</year>). <article-title>Best practices and advice for using pupillometry to measure listening effort: an introduction for those who want to get started.</article-title>
<source><italic>Trends Hear.</italic></source>
<volume>22</volume>:<issue>233121651880086</issue>. <pub-id pub-id-type="doi">10.1177/2331216518800869</pub-id>
<?supplied-pmid 30261825?><pub-id pub-id-type="pmid">30261825</pub-id></mixed-citation>
    </ref>
    <ref id="B162">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Withouck</surname><given-names>M.</given-names></name><name><surname>Smet</surname><given-names>K. A. G.</given-names></name><name><surname>Ryckaert</surname><given-names>W. R.</given-names></name><name><surname>Pointer</surname><given-names>M. R.</given-names></name><name><surname>Deconinck</surname><given-names>G.</given-names></name><name><surname>Koenderink</surname><given-names>J.</given-names></name><etal/></person-group> (<year>2013</year>). <article-title>Brightness perception of unrelated self-luminous colors.</article-title>
<source><italic>J. Opt. Soc. Am. A</italic></source>
<volume>30</volume>:<issue>1248</issue>. <pub-id pub-id-type="doi">10.1364/JOSAA.30.001248</pub-id>
<?supplied-pmid 24323112?><pub-id pub-id-type="pmid">24323112</pub-id></mixed-citation>
    </ref>
    <ref id="B163">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woodhouse</surname><given-names>J. M.</given-names></name></person-group> (<year>1975</year>). <article-title>The effect of pupil size on grating detection at various contrast levels.</article-title>
<source><italic>Vision Res.</italic></source>
<volume>15</volume>
<fpage>645</fpage>–<lpage>648</lpage>. <pub-id pub-id-type="doi">10.1016/0042-6989(75)90278-3</pub-id><pub-id pub-id-type="pmid">1138478</pub-id></mixed-citation>
    </ref>
    <ref id="B164">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yiu</surname><given-names>Y. H.</given-names></name><name><surname>Aboulatta</surname><given-names>M.</given-names></name><name><surname>Raiser</surname><given-names>T.</given-names></name><name><surname>Ophey</surname><given-names>L.</given-names></name><name><surname>Flanagin</surname><given-names>V. L.</given-names></name><name><surname>Eulenburg</surname><given-names>P.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>DeepVOG: open-source pupil segmentation and gaze estimation in neuroscience using deep learning.</article-title>
<source><italic>J. Neurosci. Methods</italic></source>
<volume>324</volume>:<issue>108307</issue>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2019.05.016</pub-id>
<?supplied-pmid 31176683?><pub-id pub-id-type="pmid">31176683</pub-id></mixed-citation>
    </ref>
    <ref id="B165">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Young</surname><given-names>R. S. L.</given-names></name><name><surname>Kimura</surname><given-names>E.</given-names></name></person-group> (<year>2008</year>). <article-title>Pupillary correlates of light-evoked melanopsin activity in humans.</article-title>
<source><italic>Vision Res.</italic></source>
<volume>48</volume>
<fpage>862</fpage>–<lpage>871</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2007.12.016</pub-id>
<?supplied-pmid 18262584?><pub-id pub-id-type="pmid">18262584</pub-id></mixed-citation>
    </ref>
    <ref id="B166">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zandi</surname><given-names>B.</given-names></name><name><surname>Eissfeldt</surname><given-names>A.</given-names></name><name><surname>Herzog</surname><given-names>A.</given-names></name><name><surname>Khanh</surname><given-names>T. Q.</given-names></name></person-group> (<year>2021</year>). <article-title>Melanopic limits of metamer spectral optimisation in multi-channel smart lighting systems.</article-title>
<source><italic>Energies</italic></source>
<volume>14</volume>:<issue>527</issue>. <pub-id pub-id-type="doi">10.3390/en14030527</pub-id></mixed-citation>
    </ref>
    <ref id="B167">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zandi</surname><given-names>B.</given-names></name><name><surname>Guo</surname><given-names>X.</given-names></name><name><surname>Bodrogi</surname><given-names>P.</given-names></name><name><surname>Khanh</surname><given-names>T. Q.</given-names></name></person-group> (<year>2018</year>). “<article-title>EXPERIMENTAL EVALUATION OF DIFFERENT BRIGHTNESS PERCEPTION MODELS BASED ON HUMAN PUPIL LIGHT RESPONSES</article-title>,” in <source><italic>PROCEEDINGS OF CIE 2018 TOPICAL CONFERENCE ON SMART LIGHTING</italic></source>, (<publisher-loc>Taipei</publisher-loc>: <publisher-name>International Commission on Illumination, CIE</publisher-name>), <fpage>201</fpage>–<lpage>208</lpage>. <pub-id pub-id-type="doi">10.25039/x45.2018.OP34</pub-id></mixed-citation>
    </ref>
    <ref id="B168">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zandi</surname><given-names>B.</given-names></name><name><surname>Khanh</surname><given-names>T. Q.</given-names></name></person-group> (<year>2021</year>). <article-title>Deep learning-based pupil model predicts time and spectral dependent light responses.</article-title>
<source><italic>Sci. Rep.</italic></source>
<volume>11</volume>:<issue>841</issue>. <pub-id pub-id-type="doi">10.1038/s41598-020-79908-5</pub-id>
<?supplied-pmid 33436693?><pub-id pub-id-type="pmid">33436693</pub-id></mixed-citation>
    </ref>
    <ref id="B169">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zandi</surname><given-names>B.</given-names></name><name><surname>Klabes</surname><given-names>J.</given-names></name><name><surname>Khanh</surname><given-names>T. Q.</given-names></name></person-group> (<year>2020</year>). <article-title>Prediction accuracy of L- and M-cone based human pupil light models.</article-title>
<source><italic>Sci. Rep.</italic></source>
<volume>10</volume>:<issue>10988</issue>. <pub-id pub-id-type="doi">10.1038/s41598-020-67593-3</pub-id>
<?supplied-pmid 32620793?><pub-id pub-id-type="pmid">32620793</pub-id></mixed-citation>
    </ref>
    <ref id="B170">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zele</surname><given-names>A. J.</given-names></name><name><surname>Adhikari</surname><given-names>P.</given-names></name><name><surname>Cao</surname><given-names>D.</given-names></name><name><surname>Feigl</surname><given-names>B.</given-names></name></person-group> (<year>2019</year>). <article-title>Melanopsin and cone photoreceptor inputs to the afferent pupil light response.</article-title>
<source><italic>Front. Neurol.</italic></source>
<volume>10</volume>:<issue>529</issue>. <pub-id pub-id-type="doi">10.3389/fneur.2019.00529</pub-id>
<?supplied-pmid 31191431?><pub-id pub-id-type="pmid">31191431</pub-id></mixed-citation>
    </ref>
    <ref id="B171">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Z.</given-names></name></person-group> (<year>2000</year>). <article-title>A flexible new technique for camera calibration.</article-title>
<source><italic>IEEE Trans. Pattern Anal. Mach. Intell.</italic></source>
<volume>22</volume>
<fpage>1330</fpage>–<lpage>1334</lpage>. <pub-id pub-id-type="doi">10.1109/34.888718</pub-id></mixed-citation>
    </ref>
    <ref id="B172">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>D.</given-names></name><name><surname>Moore</surname><given-names>S. T.</given-names></name><name><surname>Raphan</surname><given-names>T.</given-names></name></person-group> (<year>1999</year>). <article-title>Robust pupil center detection using a curvature algorithm.</article-title>
<source><italic>Comput. Methods Programs Biomed.</italic></source>
<volume>59</volume>
<fpage>145</fpage>–<lpage>157</lpage>. <pub-id pub-id-type="doi">10.1016/S0169-2607(98)00105-9</pub-id><pub-id pub-id-type="pmid">10386764</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
