<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7355292</article-id>
    <article-id pub-id-type="pmid">32657386</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btaa479</article-id>
    <article-id pub-id-type="publisher-id">btaa479</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Studies of Phenotypes and Clinical Applications</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MHCAttnNet: predicting MHC-peptide bindings for MHC alleles classes I and II using an attention-based deep neural model</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Venkatesh</surname>
          <given-names>Gopalakrishnan</given-names>
        </name>
        <xref ref-type="corresp" rid="btaa479-cor1"/>
        <xref ref-type="author-notes" rid="btaa479-FM1"/>
        <xref ref-type="aff" rid="btaa479-aff1"/>
        <!--<email>Gopalakrishnan.V@iiitb.org</email>-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Grover</surname>
          <given-names>Aayush</given-names>
        </name>
        <xref ref-type="corresp" rid="btaa479-cor1"/>
        <xref ref-type="author-notes" rid="btaa479-FM1"/>
        <xref ref-type="aff" rid="btaa479-aff1"/>
        <!--<email>Aayush.Grover@iiitb.org</email>-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Srinivasaraghavan</surname>
          <given-names>G</given-names>
        </name>
        <xref ref-type="aff" rid="btaa479-aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Rao</surname>
          <given-names>Shrisha</given-names>
        </name>
        <xref ref-type="aff" rid="btaa479-aff1"/>
      </contrib>
    </contrib-group>
    <aff id="btaa479-aff1"><institution>International Institute of Information Technology Bangalore</institution>, Bangalore 560100, <country country="IN">India</country></aff>
    <author-notes>
      <fn id="btaa479-FM1">
        <p>The authors wish it to be known that, in their opinion, Gopalakrishnan Venkatesh and Aayush Grover should be regarded as Joint First Authors.</p>
      </fn>
      <corresp id="btaa479-cor1">To whom correspondence should be addressed. E-mail: <email>Gopalakrishnan.V@iiitb.org</email> or <email>Aayush.Grover@iiitb.org</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2020-07-13">
      <day>13</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>36</volume>
    <issue>Suppl 1</issue>
    <issue-title>ISMB 2020 Proceedings</issue-title>
    <fpage>i399</fpage>
    <lpage>i406</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btaa479.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Accurate prediction of binding between a major histocompatibility complex (MHC) allele and a peptide plays a major role in the synthesis of personalized cancer vaccines. The immune system struggles to distinguish between a cancerous and a healthy cell. In a patient suffering from cancer who has a particular MHC allele, only those peptides that bind with the MHC allele with high affinity, help the immune system recognize the cancerous cells.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>MHCAttnNet is a deep neural model that uses an attention mechanism to capture the relevant subsequences of the amino acid sequences of peptides and MHC alleles. It then uses this to accurately predict the MHC-peptide binding. MHCAttnNet achieves an AUC-PRC score of 94.18% with 161 class I MHC alleles, which outperforms the state-of-the-art models for this task. MHCAttnNet also achieves a better <italic>F</italic>1-score in comparison to the state-of-the-art models while covering a larger number of class II MHC alleles. The attention mechanism used by MHCAttnNet provides a heatmap over the amino acids thus indicating the important subsequences present in the amino acid sequence. This approach also allows us to focus on a much smaller number of relevant trigrams corresponding to the amino acid sequence of an MHC allele, from 9251 possible trigrams to about 258. This significantly reduces the number of amino acid subsequences that need to be clinically tested.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The data and source code are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/gopuvenkat/MHCAttnNet">https://github.com/gopuvenkat/MHCAttnNet</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Amazon AWS Machine Learning Research Award</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Major histocompatibility complex (MHC) classes I and II play a significant role in identifying the cancerous cells in one’s body. These sets of genes, when bonded with peptides, help the immune system by bringing the bonded complex to the surface of a cancerous cell, making it visible to T-cells which eventually destroy it. This antigen presentation to the T-cells is a crucial part of the training and development of the adaptive immune response. The formation of a peptide–MHC allele complex depends on several factors, including proteasome cleavage preference of a peptide and the MHC-peptide binding affinity, thereby making it a difficult task (<xref rid="btaa479-B45" ref-type="bibr">Zeng and Gifford, 2019a</xref>).</p>
    <p>There have been several works in the past that use computational methods to estimate the binding affinity between an MHC allele and a peptide. MHCflurry (<xref rid="btaa479-B28" ref-type="bibr">O’Donnell <italic>et al.</italic>, 2018</xref>) is considered to be one of the state-of-the-art methods for this task. MHCflurry is trained on 130 MHC alleles and works well only on MHC alleles on which it is trained. MHCflurry also limits the length of a peptide to 8–15 amino acids. Other widely used state-of-the-art models are NetMHC (<xref rid="btaa479-B4" ref-type="bibr">Andreatta and Nielsen, 2016</xref>; <xref rid="btaa479-B27" ref-type="bibr">Nielsen <italic>et al.</italic>, 2003</xref>) and NetMHCpan (<xref rid="btaa479-B15" ref-type="bibr">Hoof <italic>et al.</italic>, 2009</xref>; <xref rid="btaa479-B19" ref-type="bibr">Jurtz <italic>et al.</italic>, 2017</xref>; <xref rid="btaa479-B25" ref-type="bibr">Nielsen and Andreatta, 2016</xref>). NetMHC is allele-specific and thereby has a different predictive model for each allele. NetMHC-4.0 is trained on 118 MHC alleles. NetMHCpan is pan-specific and makes binding affinity prediction even for unseen alleles, as long as the allele’s amino acid sequence is known. NetMHCpan-4.0, on the other hand, is trained on 169 MHC alleles. NetMHC and NetMHCpan tools are not open-sourced and hence, the workflow of training their models are not clear. In convolutional neural network-based models, like PUFFIN (<xref rid="btaa479-B46" ref-type="bibr">Zeng and Gifford, 2019b</xref>), the architecture needs to work with a fixed-length input amino acid sequence. In these models, the variable-length sequences have to be made equal length by padding. The PUFFIN model predicts the expected affinity of an MHC-peptide binding, for both classes I and II alleles, as well as the uncertainty of its prediction. The MHCSeqNet (<xref rid="btaa479-B31" ref-type="bibr">Phloyphisut <italic>et al.</italic>, 2019</xref>) model considers the input as an equally-weighted linear amino acid chain and can handle 92 alleles of class I. It looks at all amino acid subsequences equally which is not necessary, as only certain subsequences contribute to determining the binding affinity. An artificial neural network, NN-Align (<xref rid="btaa479-B26" ref-type="bibr">Nielsen and Lund, 2009</xref>), focuses on class II alleles. It, however, handles only 14 class II alleles. A newer tool from NetMHCpan series, NetMHCII and NetMHCIIpan (<xref rid="btaa479-B18" ref-type="bibr">Jensen <italic>et al.</italic>, 2018</xref>), computes binding affinities on 36 different HLA-DR alleles.</p>
    <p>Despite the progress made so far, there is still a need for a more robust method for predicting binding affinity. There is a need to generalize the confidence of the predicted interactions between peptides and MHC alleles without having a significant drop in precision. MHCAttnNet overcomes the shortcomings of earlier methods by: (i) releasing the source code as an open-source package; (ii) enabling prediction even for variable-length peptides; (iii) focusing only on relevant subsequences of amino acids; and (iv) training and testing on a larger number of alleles.</p>
    <p>MHCAttnNet uses a bidirectional long short-term memory (Bi-LSTM) styled encoder to deal with variable-length peptide sequences. This permits the model to handle a large variety of peptides, and hence makes it more general. MHCAttnNet is trained and tested on the Immune Epitope Database (IEDB) (<xref rid="btaa479-B36" ref-type="bibr">Sahin <italic>et al.</italic>, 2017</xref>) (as of 2019) and is capable of working well with both class I and class II MHC alleles separately. This has been made possible with the help of the attention mechanism used in the neural network. The attention mechanism is used to identify relevant subsequences responsible for determining the binding affinity and thereby increase the weights of these relevant subsequences. This permits the model to focus on these important subsequences of the amino acid sequence, making it more targeted and informative. MHCAttnNet is trained on 161 different class I MHC alleles and 49 different class II alleles as only these many MHC alleles are presently available in the dataset. MHCAttnNet can compute binding predictions on other MHC alleles as well as long as their amino acid sequences are available. Even while handling a large variety of MHC alleles, our model outperforms the current state-of-the-art models for predicting binding between peptide and class I MHC alleles while at the same time, is competitive in case of class II MHC alleles.</p>
    <p>The structure of rest of this article is as follows: Section 2 presents the background about what personalized cancer vaccines are and how they work. The dataset, pre-processing steps, and the MHCAttnNet model are explained in Section 3. Section 4 describes the analysis of our results with respect to the state-of-the-art models. We summarize our approach and discuss some relevant aspects in Section 5.</p>
  </sec>
  <sec>
    <title>2 Background</title>
    <p>Personalized cancer vaccines have shown promising results in their early stages. To synthesize a personalized cancer vaccine, first the genomes of cancerous cells are collected, which helps in the identification of tumor-specific peptides called neoepitopes. Neoepitopes, when combined with adjuvants or other immune-stimulatory agents and injected into the patient, helps the immune system identify cancerous cells and kill them using the body’s own T-cells. This way, the human body learns to kill the cancerous cells on its own, without having the risk of autoimmune diseases (<xref rid="btaa479-B17" ref-type="bibr">Hu <italic>et al.</italic>, 2018b</xref>; <xref rid="btaa479-B29" ref-type="bibr">Ott <italic>et al.</italic>, 2017</xref>; <xref rid="btaa479-B34" ref-type="bibr">Reddy <italic>et al.</italic>, 2006</xref>; <xref rid="btaa479-B36" ref-type="bibr">Sahin <italic>et al.</italic>, 2017</xref>).</p>
    <p>For the identification of neoepitopes, next-generation sequencing data from tumor and healthy cells are compared with that of the human reference genome. RNA sequencing narrows the focus to mutations of expressed genes. The potential sequences are validated by using computational models that predict the binding affinity of neoepitopes with the individual’s MHC proteins that would present the neoepitopes to the surface. This filters the candidate neoepitopes for personalized vaccines, as shown in <xref ref-type="fig" rid="btaa479-F1">Figure 1</xref>.
</p>
    <fig id="btaa479-F1" orientation="portrait" position="float">
      <label>Fig. 1.</label>
      <caption>
        <p>Steps to synthesize personalized cancer vaccine</p>
      </caption>
      <graphic xlink:href="btaa479f1"/>
    </fig>
    <p>The MHC allele is referred to as HLA complex in humans. There are three types of MHC genes in humans, classes I, II and III. We only focus on class I and class II MHC alleles (there is no publicly available data for class III alleles). Class I MHC alleles present endogenous antigens that originate from the cytoplasm to cytotoxic T-cells (CD8+ T-cells). Class II MHC alleles present exogenous antigens that originate extra-cellularly from foreign bodies, such as bacteria to helper T-cells (CD4+ T-cells) (<xref rid="btaa479-B1" ref-type="bibr">Abbas <italic>et al.</italic>, 2014</xref>; <xref rid="btaa479-B9" ref-type="bibr">Delves <italic>et al.</italic>, 2017</xref>). Previous research (<xref rid="btaa479-B8" ref-type="bibr">Comber and Philip, 2014</xref>; <xref rid="btaa479-B13" ref-type="bibr">Garrido <italic>et al.</italic>, 1993</xref>) has shown that class I MHC alleles play a major role in identification of cancerous cells. Recent work (<xref rid="btaa479-B32" ref-type="bibr">Pyke <italic>et al.</italic>, 2018</xref>) however suggests that patient-specific variations in class II MHC alleles have as significant an effect on the mutations that arise in tumors, as that of class I MHC alleles.</p>
  </sec>
  <sec>
    <title>3 Implementation</title>
    <p>In this section, we discuss the dataset used and explain the MHCAttnNet model in detail.</p>
    <sec>
      <title>3.1 Dataset</title>
      <p>We use the IEDB (<xref rid="btaa479-B36" ref-type="bibr">Sahin <italic>et al.</italic>, 2017</xref>) (as of 2019) to prepare our training and testing data as done in previous works (<xref rid="btaa479-B16" ref-type="bibr">Hu <italic>et al.</italic>, 2018a</xref>; <xref rid="btaa479-B19" ref-type="bibr">Jurtz <italic>et al.</italic>, 2017</xref>; <xref rid="btaa479-B26" ref-type="bibr">Nielsen and Lund, 2009</xref>; <xref rid="btaa479-B31" ref-type="bibr">Phloyphisut <italic>et al.</italic>, 2019</xref>; <xref rid="btaa479-B46" ref-type="bibr">Zeng and Gifford, 2019b</xref>). We filter the data-points that correspond to ‘human’ or ‘homo-sapiens’ and the data-points that have MHC alleles belonging to classes I or II. We take into consideration the qualitative affinity measurements, which are labeled in the IEDB as ‘Positive’, ‘Positive High’, ‘Positive Intermediate’, ‘Positive Low’ and ‘Negative’. Based on <xref rid="btaa479-B47" ref-type="bibr">Zhao and Sher (2018)</xref> and our analysis of quantitative affinity measurements, the classes ‘Positive’, ‘Positive High’ and ‘Positive Intermediate’ have IC50 values of &lt;500 nM for class I and 1000 nM for class II alleles. Hence, all the data-points with these three classes are labeled as binding and the remaining two classes are labeled as non-binding.</p>
      <p>Our final dataset comprises of 491 018 class I MHC-allele data-points covering 161 different HLAs and 64 954 class II MHC-allele data-points covering 49 different HLAs. The peptide lengths range from 3 to 43 for both classes, while the lengths of amino acid sequences of HLAs range from 180 to 347 for class I and from 85 to 232 for class II.</p>
      <p>We get 451 484 HLA-A*, 39 424 HLA-B* and 110 HLA-C* class I alleles, out of which 379 783 are binding and 111 235 are non-binding. Similarly, we get 64 926 HLA-DRB1*, 22 HLA-DRB3*, 4 HLA-DRB4* and 2 HLA-DRB5* class II alleles out of which 36 035 are binding and 28 919 are non-binding.</p>
    </sec>
    <sec>
      <title>3.2 Model</title>
      <p>As shown in <xref ref-type="fig" rid="btaa479-F2">Figure 2</xref>, the embedding layers are used to encode the input amino acids into a low-dimensional vector. The Bi-LSTM encoder is used to extract abstract token-level features from these embeddings. The attention mechanism produces a weight vector, which is multiplied with the token-level features to build a sentence-level feature vector. The obtained weighted sentence-level feature vector is passed through the fully connected layers to perform the classification task.
</p>
      <fig id="btaa479-F2" orientation="portrait" position="float">
        <label>Fig. 2.</label>
        <caption>
          <p>An overview of the MHCAttnNet architecture</p>
        </caption>
        <graphic xlink:href="btaa479f2"/>
      </fig>
      <p>This architecture is capable of handling both class I and class II MHC alleles. The model weights are shared across all MHC alleles as opposed to building one model per MHC allele [as done elsewhere (<xref rid="btaa479-B28" ref-type="bibr">O’Donnell <italic>et al.</italic>, 2018</xref>)]. The Adam optimization algorithm (<xref rid="btaa479-B20" ref-type="bibr">Kingma and Ba, 2015</xref>) is used for the training with binary cross-entropy loss function. Our model accepts both the peptide and the MHC allele in the form of a sequence of amino acids.</p>
      <sec>
        <label>3.2.1</label>
        <title>Embedding</title>
        <p>The inputs to the model are peptides and MHC alleles, which are represented as sequences of amino acids. To encode these amino acid sequences, we use a continuous vector representation called embedding (<xref rid="btaa479-B7" ref-type="bibr">Collobert <italic>et al.</italic>, 2011</xref>). Embeddings, when used as the underlying input representations, have been shown to boost the performance of various Natural Language Processing (NLP) tasks as they capture the semantic meanings of words in sentences. The input, a sequence of amino acids, can be treated as a sentence where the individual words are the amino acids. The embedding representation for the tokens can be learned from a large corpus in an unsupervised manner, and can be later fine-tuned for the required upstream task. Given a sentence consisting of T words <inline-formula id="IE1"><mml:math id="IM1"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, every word <italic>e<sub>i</sub></italic> is converted into a real-valued vector <italic>x<sub>i</sub></italic>. The sentence S, represented through the real-valued vectors of words, is passed to the Bi-LSTM Encoder (Section 3.2.2).</p>
        <p>To pre-train the continuous vector embedding for the tokens, 1 or 3 consecutive amino acids (non-overlapping 1-g or 3-g) in an amino acid sequence, we use the Skip-Gram model (<xref rid="btaa479-B23" ref-type="bibr">Mikolov <italic>et al.</italic>, 2013</xref>). The lengths of amino acid sequences of peptides are much shorter than those of MHC alleles. Therefore, we train a 1-gram embedding for peptides, but use a 3-gram model called ProtVec for MHC alleles as suggested by an earlier study (<xref rid="btaa479-B5" ref-type="bibr">Asgari and Mofrad, 2015</xref>). We fix the embedding dimension at 100, which was reported as the optimal parameter (<xref rid="btaa479-B5" ref-type="bibr">Asgari and Mofrad, 2015</xref>).</p>
      </sec>
      <sec>
        <title>3.2.2 Bi-LSTM encoder</title>
        <p>A Bi-LSTM (<xref rid="btaa479-B38" ref-type="bibr">Schuster and Paliwal, 1997</xref>) was chosen to encode the sequence of amino acids as it is capable of processing sequences with variable lengths, unlike the fixed <italic>n-</italic>mer peptides, and generalizing the relationship in the amino acid sequence.</p>
        <p>The following notations and equations have been borrowed from <xref rid="btaa479-B48" ref-type="bibr">Zhou <italic>et al.</italic> (2016)</xref>. An LSTM cell consists of four components: one input gate <italic>i<sub>t</sub></italic> with the trainable weight matrices <italic>W<sub>xi</sub>, W<sub>hi</sub>, W<sub>ci</sub></italic> and bias <italic>b<sub>i</sub></italic>; one forget gate <italic>f<sub>t</sub></italic> with the trainable weight matrices <italic>W<sub>xf</sub>, W<sub>hf</sub>, W<sub>cf</sub></italic> and bias <italic>b<sub>f</sub></italic> and one output gate <italic>o<sub>t</sub></italic> with the trainable weight matrices <italic>W<sub>xo</sub>, W<sub>ho</sub>, W<sub>co</sub></italic> and bias b<sub><italic>o</italic></sub>. The current cell state, <italic>c<sub>t</sub></italic>, is generated by calculating the weighted sum using both the previous cell state and the current information generated by the cell.
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>tanh</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E4"><label>(4)</label><mml:math id="M4"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="E5"><label>(5)</label><mml:math id="M5"><mml:mrow><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E6"><label>(6)</label><mml:math id="M6"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mi>tanh</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>To learn the relationship within the amino acids sequences, it would be beneficial to have access to future as well as past context. Bidirectional LSTM (1997) networks permit this by extending the unidirectional LSTM networks by introducing a second layer. In this second layer, the hidden connections are aligned in the opposite direction. The model is therefore able to exploit information from both the past and the future context. The output of the every word is the concatenation of the forward and backward hidden states (6) for that word.</p>
      </sec>
      <sec>
        <label>3.2.3</label>
        <title>Attention</title>
        <p>Attentive neural networks (<xref rid="btaa479-B40" ref-type="bibr">Vaswani <italic>et al.</italic>, 2017</xref>) have recently demonstrated success over a wide range of fields ranging from NLP (<xref rid="btaa479-B6" ref-type="bibr">Bahdanau <italic>et al.</italic>, 2015</xref>) to Computer Vision (<xref rid="btaa479-B43" ref-type="bibr">Xu <italic>et al.</italic>, 2015</xref>). Here, we discuss the attention mechanism used for our classification task.
<disp-formula id="E7"><label>(7)</label><mml:math id="M7"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>tanh</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E8"><label>(8)</label><mml:math id="M8"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E9"><label>(9)</label><mml:math id="M9"><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="E10"><label>(10)</label><mml:math id="M10"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>The <italic>u</italic> vector is randomly initialized, which is learned during the training process, to weight the amino acid subsequences. <italic>u<sub>t</sub></italic> can be thought of as a non-linearity applied over the Bi-LSTM hidden state output, as in (6). <italic>v<sub>t</sub></italic> is the exponential of the dot-product of <italic>u<sub>t</sub></italic> with the <italic>u</italic> vector. Intuitively, the value of <italic>v<sub>t</sub></italic> is high if <italic>u</italic> and <italic>u<sub>t</sub></italic> are similar. We compute the weight of each token, <italic>α<sub>t</sub></italic>, by normalizing over the <italic>v<sub>t</sub></italic> vectors and thereby compute the final sentence pair-representation used for classification by taking the weighted sum of the Bi-LSTM hidden vectors <italic>h<sub>t</sub></italic> (6).</p>
      </sec>
      <sec>
        <label>3.2.4</label>
        <title>Fully connected layers</title>
        <p>The neurons are fully pairwise connected across adjacent linear layers, but the neurons within a single layer do not share connections. In the MHCAttnNet model, a non-linear activation function is applied in every fully connected layer. The Rectified Linear Unit (ReLU) (<xref rid="btaa479-B24" ref-type="bibr">Nair and Hinton, 2010</xref>) activation function is used which computes the function <inline-formula id="IE2"><mml:math id="IM2"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. This forces the activation to be thresholded at zero.</p>
        <p>The attended vectors, (10), from the peptide and MHC-allele branch of the network are concatenated before being passed onto another fully connected layer. The output of the model is the probability of the input being in a particular class which is obtained from the softmax function (<xref rid="btaa479-B14" ref-type="bibr">Goodfellow <italic>et al.</italic>, 2016</xref>), defined as in (11).
<disp-formula id="E11"><label>(11)</label><mml:math id="M11"><mml:mrow><mml:mi>σ</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>The softmax function squeezes the outputs for each class between 0 and 1, by dividing each class output by the sum of the outputs of all classes.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>4 Results</title>
    <p>MHCAttnNet produces competitive results in terms of predicting MHC-peptide bindings for both MHC classes I and II. The metrics used to compare the models are described in Section 4.1. Comparison with the state-of-the-art models is done in Sections 4.2 and 4.3. Attention in MHCAttnNet provides some interesting insights into which subsequences of amino acids of peptides and MHC alleles play major roles in determining the MHC-peptide bindings. These insights are explained in detail in Section 4.4. In Section 4.5, we define sequence reduction, a list of trigrams of amino acid sequences of MHC alleles, which captures all the information, relevant for predicting binding, corresponding to a particular amino acid. This list consists of just about 2.8% of the total possible trigrams, and would greatly reduce the number of possible clinical trials needed for vaccine development without major loss of information. This means that only 2.8% of the total trigrams have any significance in predicting the binding between a peptide amino acid and an MHC allele.</p>
    <sec>
      <title>4.1 Model performance</title>
      <p>To test the performance of MHCAttnNet, we look at the following metrics:
</p>
      <list list-type="bullet">
        <list-item>
          <p>Precision or positive predictive value (PPV) (<xref rid="btaa479-B3" ref-type="bibr">Alpaydin, 2014</xref>), which denotes the fraction of results which are relevant (12).</p>
        </list-item>
        <list-item>
          <p>Recall or sensitivity (<xref rid="btaa479-B39" ref-type="bibr">Trevethan, 2017</xref>), which denotes the fraction of relevant instances that the model was able to retrieve (13).</p>
        </list-item>
        <list-item>
          <p>Accuracy (<xref rid="btaa479-B3" ref-type="bibr">Alpaydin, 2014</xref>) is the percentage of correctly classified instances (14).</p>
        </list-item>
        <list-item>
          <p><italic>F</italic>1-score (<xref rid="btaa479-B3" ref-type="bibr">Alpaydin, 2014</xref>), which gives a good balance between precision and recall (15).</p>
        </list-item>
        <list-item>
          <p>Area under receiver operating characteristics (AUC-ROC) (<xref rid="btaa479-B12" ref-type="bibr">Fawcett, 2006</xref>), which is the area under the curve plotted when true positive rate is plotted against false positive rate.</p>
        </list-item>
        <list-item>
          <p>Area under precision-recall curve (AUC-PRC) (<xref rid="btaa479-B37" ref-type="bibr">Saito and Rehmsmeier, 2015</xref>), which is defined as the area under the curve plotted when Precision is plotted against Sensitivity.</p>
        </list-item>
        <list-item>
          <p>Pearson correlation coefficient (PCC) (<xref rid="btaa479-B21" ref-type="bibr">Kirch, 2008</xref>), which is a linear correlation between two normally distributed continuous variables.</p>
        </list-item>
      </list>
      <p>AUC-PRC is an appropriate metric to compare the results of models as there is a large class imbalance in the dataset, especially for class I MHC alleles. Precision-recall plots provide a good estimate of future classification performance as they evaluate the fraction of true positives among the positive predictions (<xref rid="btaa479-B37" ref-type="bibr">Saito and Rehmsmeier, 2015</xref>).</p>
      <p>In the following expressions, TP, TN, FP and FN denote true positive, true negative, false positive and false negative, respectively.
<disp-formula id="E12"><label>(12)</label><mml:math id="M12"><mml:mrow><mml:mtext>PPV</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="E13"><label>(13)</label><mml:math id="M13"><mml:mrow><mml:mtext>Sensitivity</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="E14"><label>(14)</label><mml:math id="M14"><mml:mrow><mml:mtext>Accuracy</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>TN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="E15"><label>(15)</label><mml:math id="M15"><mml:mrow><mml:mtext>F1</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mtext>PPV</mml:mtext><mml:mo>×</mml:mo><mml:mtext>Sensitivity</mml:mtext></mml:mrow><mml:mrow><mml:mtext>PPV</mml:mtext><mml:mo>+</mml:mo><mml:mtext>Sensitivity</mml:mtext></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>The performance of MHCAttnNet in class I MHC alleles is measured using AUC-ROC, AUC-PRC, PCC, PPV, <italic>F</italic>1-score and Sensitivity. In class II alleles, the class imbalance is not significant, and hence to evaluate the performance of MHCAttnNet on class II MHC alleles, accuracy, AUC-ROC, PCC, Sensitivity, PPV and <italic>F</italic>1-score are used.</p>
      <p>These metrics are computed by us on a 5-fold cross-validation data (<xref rid="btaa479-B35" ref-type="bibr">Refaeilzadeh <italic>et al.</italic>, 2009</xref>), which is derived from IEDB (<xref rid="btaa479-B41" ref-type="bibr">Vita <italic>et al.</italic>, 2015</xref>) and processed as mentioned in Section 3.1. The 5-fold cross-validation data are different for class I and class II MHC alleles. All experiments are run on these 5-fold cross-validation data. Each of the 5 test files for class I MHC alleles consists of 97 000 data-points, while for class II MHC alleles, there are around 13 000 data-points each.</p>
    </sec>
    <sec>
      <title>4.2 Performance on class I MHC alleles</title>
      <p>MHCAttnNet is the first model that takes into consideration 161 different class I MHC alleles and yet achieves a 5-fold cross-validation AUC-PRC score of 94.18%. The model also attains a high cross-validation <italic>F</italic>1-score of 94.22%. Four different hyper-parameter configurations score higher than 94% for AUC-PRC, as seen in <xref rid="btaa479-T1" ref-type="table">Table 1</xref>. MHCAttnNet, therefore, has high accuracy with low variance. The scores for the best model configuration are shown in bold in <xref rid="btaa479-T1" ref-type="table">Table 1</xref>. This best model configuration was used for all the experiments.
</p>
      <table-wrap id="btaa479-T1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>The 5-fold cross-validation performance of MHCAttnNet with different hyper-parameters on class I MHC alleles</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Bi-LSTM</th>
              <th align="center" rowspan="1" colspan="1">Number of layers</th>
              <th align="center" rowspan="1" colspan="1">Number of layers</th>
              <th align="center" rowspan="1" colspan="1">Context</th>
              <th align="center" rowspan="1" colspan="1">AUC-PRC</th>
              <th align="center" rowspan="1" colspan="1">AUC-ROC</th>
              <th align="center" rowspan="1" colspan="1"><italic>F</italic>1-</th>
            </tr>
            <tr>
              <th align="center" rowspan="1" colspan="1">hidden dimension</th>
              <th align="center" rowspan="1" colspan="1">in peptide Bi-LSTM</th>
              <th align="center" rowspan="1" colspan="1">in MHC Bi-LSTM</th>
              <th align="center" rowspan="1" colspan="1">dimension</th>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
              <th align="center" rowspan="1" colspan="1">score</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">64</td>
              <td rowspan="1" colspan="1">3</td>
              <td rowspan="1" colspan="1">3</td>
              <td rowspan="1" colspan="1">16</td>
              <td rowspan="1" colspan="1">
                <bold>0.9418</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.8893</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.9422</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">64</td>
              <td rowspan="1" colspan="1">3</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">16</td>
              <td rowspan="1" colspan="1">0.9418</td>
              <td rowspan="1" colspan="1">0.8893</td>
              <td rowspan="1" colspan="1">0.9416</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">64</td>
              <td rowspan="1" colspan="1">3</td>
              <td rowspan="1" colspan="1">3</td>
              <td rowspan="1" colspan="1">32</td>
              <td rowspan="1" colspan="1">0.9409</td>
              <td rowspan="1" colspan="1">0.8874</td>
              <td rowspan="1" colspan="1">0.9398</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">64</td>
              <td rowspan="1" colspan="1">3</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">32</td>
              <td rowspan="1" colspan="1">0.9416</td>
              <td rowspan="1" colspan="1">0.8887</td>
              <td rowspan="1" colspan="1">0.9404</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic>Note</italic>: The best performance is indicated in bold. The hyper-parameter setting, corresponding to this, was used to run all the experiments.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>MHCAttnNet is compared with the widely used state-of-the-art models, MHCflurry (<xref rid="btaa479-B28" ref-type="bibr">O’Donnell <italic>et al.</italic>, 2018</xref>), NetMHC-4.0 (<xref rid="btaa479-B4" ref-type="bibr">Andreatta and Nielsen, 2016</xref>) and PUFFIN (<xref rid="btaa479-B46" ref-type="bibr">Zeng and Gifford, 2019b</xref>).</p>
      <p>We tested MHCflurry on our 5-fold cross-validation data using mhctools (<ext-link ext-link-type="uri" xlink:href="https://github.com/openvax/mhctools">https://github.com/openvax/mhctools</ext-link>). MHCflurry takes into consideration 130 class I MHC alleles as compared to 161 for MHCAttnNet. The predictions were computed for all the data-points in the test data, except for those data-points that had MHC alleles, which are not supported by MHCflurry. The performance of MHCflurry is computed without those data-points. MHCflurry outperforms MHCAttnNet on (PPV) by only 0.1% while MHCAttnNet achieves better results on the remaining metrics as seen in <xref rid="btaa479-T2" ref-type="table">Table 2</xref>. MHCAttnNet achieves AUC-ROC score of 88.93% as compared to 82.34% of MHCflurry, which is an improvement of about 6.5%. The AUC-PRC score of MHCAttnNet is 94.18% while it is 90.74% for MHCflurry, an improvement of 3.5%. Moreover, MHCAttnNet shows a significant improvement in terms of Sensitivity (around 21.5%), PCC (around 19.5%) and <italic>F</italic>1-score (around 12%).
</p>
      <table-wrap id="btaa479-T2" orientation="portrait" position="float">
        <label>Table 2.</label>
        <caption>
          <p>Comparison of performance of MHCAttnNet with the state-of-the-art methods for class I MHC alleles</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th align="center" rowspan="1" colspan="1">AUC-</th>
              <th align="center" rowspan="1" colspan="1">AUC-</th>
              <th align="center" rowspan="1" colspan="1">PCC</th>
              <th align="center" rowspan="1" colspan="1">PPV</th>
              <th align="center" rowspan="1" colspan="1"><italic>F</italic>1-</th>
              <th align="center" rowspan="1" colspan="1">Sensitivity</th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th align="center" rowspan="1" colspan="1">ROC</th>
              <th align="center" rowspan="1" colspan="1">PRC</th>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
              <th align="center" rowspan="1" colspan="1">score</th>
              <th rowspan="1" colspan="1"/>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">NetMHC4.0</td>
              <td rowspan="1" colspan="1">0.8237</td>
              <td rowspan="1" colspan="1">0.9048</td>
              <td rowspan="1" colspan="1">0.5738</td>
              <td rowspan="1" colspan="1">0.9486</td>
              <td rowspan="1" colspan="1">0.8531</td>
              <td rowspan="1" colspan="1">0.7757</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MHCflurry</td>
              <td rowspan="1" colspan="1">0.8234</td>
              <td rowspan="1" colspan="1">0.9074</td>
              <td rowspan="1" colspan="1">0.5624</td>
              <td rowspan="1" colspan="1">
                <bold>0.9695</bold>
              </td>
              <td rowspan="1" colspan="1">0.8230</td>
              <td rowspan="1" colspan="1">0.7149</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PUFFIN</td>
              <td rowspan="1" colspan="1">0.8185</td>
              <td rowspan="1" colspan="1">0.9129</td>
              <td rowspan="1" colspan="1">0.5398</td>
              <td rowspan="1" colspan="1">0.9668</td>
              <td rowspan="1" colspan="1">0.8264</td>
              <td rowspan="1" colspan="1">0.7215</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MHCAttnNet (no attention)</td>
              <td rowspan="1" colspan="1">0.8822</td>
              <td rowspan="1" colspan="1">0.9380</td>
              <td rowspan="1" colspan="1">0.7463</td>
              <td rowspan="1" colspan="1">0.9643</td>
              <td rowspan="1" colspan="1">0.9402</td>
              <td rowspan="1" colspan="1">0.9303</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MHCAttnNet</td>
              <td rowspan="1" colspan="1">
                <bold>0.8893</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.9418</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.7570</bold>
              </td>
              <td rowspan="1" colspan="1">0.9683</td>
              <td rowspan="1" colspan="1">
                <bold>0.9422</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.9304</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <p><italic>Note</italic>: The best performances are indicated in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>We tested NetMHC-4.0 using the publicly available prediction software (2016). NetMHC-4.0 is trained on 118 class I MHC alleles and hence, there are some class I MHC alleles in our test data, which are not covered by NetMHC-4.0. Those data-points were removed from the test data on which the performance of NetMHC-4.0 is computed. <xref rid="btaa479-T2" ref-type="table">Table 2</xref> shows that MHCAttnNet outperforms NetMHC-4.0 on all the metrics. MHCAttnNet achieves an AUC-ROC score of 88.93% and shows a gain of more than 6% over NetMHC-4.0. Similarly MHCAttnNet is able to achieve AUC-PRC and PPV scores of 94.18 and 96.83%, respectively, while NetMHC4.0 achieves 90.48 and 94.86%, respectively. MHCAttnNet shows a significant gain of around 15% in both, Sensitivity and PCC.</p>
      <p>We tested the pre-trained PUFFIN model using the provided official implementation (2019) on our test data. <xref rid="btaa479-T2" ref-type="table">Table 2</xref> also shows that MHCAttnNet scores better than PUFFIN on all the metrics for class I MHC alleles. MHCAttnNet outperforms PUFFIN by about 7% on AUC-ROC metric and about 3% on AUC-PRC metric. PUFFIN achieves the PPV score of 96.68%, which is just about 0.15% shy of MHCAttnNet’s score. MHCAttnNet shows improved performances on PCC, <italic>F</italic>1-score and Sensitivity than PUFFIN by about 22, 11.5 and 21%, respectively.</p>
      <p>We also tested the performance of MHCAttnNet without the Attention Module to show the importance of attention. The comparison of performances with and without attention is shown in <xref rid="btaa479-T2" ref-type="table">Table 2</xref>. It is clear that having attention produces better results. The obtained attention weights are also useful to understand how the model is weighting the amino acids while predicting.</p>
    </sec>
    <sec>
      <title>4.3 Performance on class II MHC alleles</title>
      <p>For class II MHC alleles, MHCAttnNet reaches as high as 75.79% on the AUC-ROC metric with 5-fold cross-validation on 49 different class II MHC alleles. NN-Align (<xref rid="btaa479-B26" ref-type="bibr">Nielsen and Lund, 2009</xref>) is considered as one of the state-of-the-art methods to compute binding predictions between peptides and class II MHC alleles. NN-Align handles only 14 alleles as compared to 49 alleles for MHCAttnNet. NN-Align is also allele-specific and hence, has different predictive models for each allele. On the contrary, MHCAttnNet is pan-allele and hence, has only one model for the task of peptide and class II MHC-allele binding. We were unable to do a class-wise analysis on the benchmark <xref rid="btaa479-B42" ref-type="bibr">Wang <italic>et al.</italic> (2008)</xref> dataset, which is used for analysis of the NN-Align model, as the number of data–points, there are too low for our model to learn. We however compared MHCAttnNet to a newer model for class II, NetMHCIIpan-3.2 (<xref rid="btaa479-B18" ref-type="bibr">Jensen <italic>et al.</italic>, 2018</xref>). We tested NetMHCIIpan using the software that has been made publicly available. The performance is computed on our 5-fold cross-validation data after removing the data-points that correspond to MHC alleles which are not handled by NetMHCIIpan. The results are compared on different metrics in <xref rid="btaa479-T3" ref-type="table">Table 3</xref>. MHCAttnNet outperforms NetMHCIIpan on all metrics by around 7%–9%.
</p>
      <table-wrap id="btaa479-T3" orientation="portrait" position="float">
        <label>Table 3.</label>
        <caption>
          <p>Comparison of performance of MHCAttnNet with the state-of-the-art methods for class II MHC alleles</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th align="center" rowspan="1" colspan="1">Accuracy</th>
              <th align="center" rowspan="1" colspan="1">AUC-ROC</th>
              <th align="center" rowspan="1" colspan="1">PCC</th>
              <th align="center" rowspan="1" colspan="1">Sensitivity</th>
              <th align="center" rowspan="1" colspan="1">PPV</th>
              <th align="center" rowspan="1" colspan="1"><italic>F</italic>1-score</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">NetMHCIIpan</td>
              <td rowspan="1" colspan="1">0.6822</td>
              <td rowspan="1" colspan="1">0.6751</td>
              <td rowspan="1" colspan="1">0.4297</td>
              <td rowspan="1" colspan="1">0.3888</td>
              <td rowspan="1" colspan="1">0.9039</td>
              <td rowspan="1" colspan="1">0.5435</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PUFFIN</td>
              <td rowspan="1" colspan="1">
                <bold>0.7657</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.7756</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.5520</bold>
              </td>
              <td rowspan="1" colspan="1">0.6853</td>
              <td rowspan="1" colspan="1">0.8644</td>
              <td rowspan="1" colspan="1">0.7645</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MHCAttnNet (no attention)</td>
              <td rowspan="1" colspan="1">0.7503</td>
              <td rowspan="1" colspan="1">0.7530</td>
              <td rowspan="1" colspan="1">0.5032</td>
              <td rowspan="1" colspan="1">0.7275</td>
              <td rowspan="1" colspan="1">0.8732</td>
              <td rowspan="1" colspan="1">0.7636</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MHCAttnNet</td>
              <td rowspan="1" colspan="1">0.7549</td>
              <td rowspan="1" colspan="1">0.7579</td>
              <td rowspan="1" colspan="1">0.5130</td>
              <td rowspan="1" colspan="1">
                <bold>0.7298</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.8769</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.7675</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn3">
            <p><italic>Note</italic>: The best performances are indicated in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>We also compare MHCAttnNet’s performance with PUFFIN. We run the pre-trained PUFFIN model, as given by <xref rid="btaa479-B46" ref-type="bibr">Zeng and Gifford (2019b</xref>), on our test data for class II MHC alleles. PUFFIN scores better than MHCAttnNet on three metrics–AUC-ROC, Accuracy and PCC by ∼1.5, 1 and 4%, respectively. On the other hand, MHCAttnNet scores better on Sensitivity, PPV and <italic>F</italic>1-score. It achieves 72.98% on Sensitivity, 87.69% on PPV and 76.75% on <italic>F</italic>1–score, which is more than PUFFIN by ∼4.5, 1.3 and 0.3%, respectively. Overall, MHCAttnNet’s performance on class II MHC alleles is comparable to the PUFFIN model as seen in <xref rid="btaa479-T3" ref-type="table">Table 3</xref>. Although the task of prediction is difficult on class II MHC alleles (<xref rid="btaa479-B11" ref-type="bibr">Dimitrov <italic>et al.</italic>, 2010</xref>), MHCAttnNet outperforms or does as well as the state-of-the-art models for class II MHC alleles.</p>
      <p>This shows that the overall performance of MHCAttnNet is better than the previous state-of-the-art methods. It may be noted that MHCAttnNet has a single architecture that works for both class I and class II MHC alleles, and that even without making any changes to the layers (<xref ref-type="fig" rid="btaa479-F2">Fig. 2</xref>) of MHCAttnNet, it predicts the binding of peptides with both class I and class II MHC alleles accurately.</p>
    </sec>
    <sec>
      <title>4.4 Analysis of attention weights</title>
      <p>Attention mechanisms play a crucial role in NLP (<xref rid="btaa479-B6" ref-type="bibr">Bahdanau <italic>et al.</italic>, 2015</xref>), where they focus on the sensitive parts of the input during output generation. Such mechanisms are most important in long sentences as it is not required to encode the full source sentence into a fixed-length vector. The encoded vector, produced by the attention layer, depends on the weighted combination of all the hidden states of the Bi-LSTM layer and not just the last state. Hence, such mechanisms can boost the contributions of the key features. The placed attention mechanism is effective as it results in higher scores, over the model without attention, as seen in <xref rid="btaa479-T2" ref-type="table">Tables 2</xref> and <xref rid="btaa479-T3" ref-type="table">3</xref>.</p>
      <p>One of the most widely used approaches is interpretation by visualizing the weights of the model. In long sequence domains, like amino acid sequences, attention is visualized using text heatmaps (<xref rid="btaa479-B44" ref-type="bibr">Yang and Zhang, 2018</xref>). A heatmap is an efficient representation that uses a system of color codes to graphically show the importance of different values of the input. As shown in <xref ref-type="fig" rid="btaa479-F3">Figures 3</xref> and <xref ref-type="fig" rid="btaa479-F4">4</xref>, the 1D heatmaps are superimposed on input text, showing the aggregate attention, which provides an overview of consequential portions of input. The input MHC allele and peptide, as sequences of amino acids, are tokenized as indicated in Section 3. The intensity of the background color is the magnitude of the attention weight placed at that token.
</p>
      <fig id="btaa479-F3" orientation="portrait" position="float">
        <label>Fig. 3.</label>
        <caption>
          <p>Attention weights (2018) are highlighted on class I MHC allele (HLA-A*02:01) and peptide. respectively</p>
        </caption>
        <graphic xlink:href="btaa479f3"/>
      </fig>
      <fig id="btaa479-F4" orientation="portrait" position="float">
        <label>Fig. 4.</label>
        <caption>
          <p>Attention weights (2018) are highlighted on class II MHC allele (HLA-DRB1*04:01) and peptide, respectively</p>
        </caption>
        <graphic xlink:href="btaa479f4"/>
      </fig>
      <p>A big advantage of attention mechanism is that it gives us the ability to reason and visualize what the model is doing. The attention mechanism makes it easier to interpret the results by giving us better insights about which subsequences of the amino acid sequence are more relevant (for the upstream classification task)—the visualized attention weights give insights into how the model made the prediction. The understanding of actual binding process between an MHC allele and a peptide is still not very clear (<xref rid="btaa479-B33" ref-type="bibr">Rajapakse <italic>et al.</italic>, 2007</xref>) and hence, the attention weights can help the researchers to focus their studies on the particular subsequences of amino acids to get a better understanding of the binding mechanism. The significant amino acids, based on the attention values placed over them, may serve as a good starting point for the clinicians but the reliability of this is subject to clinical validation.</p>
    </sec>
    <sec>
      <title>4.5 Sequence reduction</title>
      <p>We analyze the cumulative information coverage obtained by going through the important trigrams corresponding to a particular amino acid, with attention weights as given by (10) being &gt;0.001, from most important to least. Out of the 9261 possible trigrams of amino acid sequences of MHC alleles, we find that only about 258 of these trigrams contain all the information about a particular amino acid present in a peptide, as seen in <xref ref-type="fig" rid="btaa479-F5">Figure 5</xref>. This reduced list of trigrams we call <italic>sequence reduction</italic>.
</p>
      <fig id="btaa479-F5" orientation="portrait" position="float">
        <label>Fig. 5.</label>
        <caption>
          <p>Number of trigrams needed to collectively convey 100% of information. The average number of trigrams needed to convey the complete information is 258 and is indicated with the dotted line</p>
        </caption>
        <graphic xlink:href="btaa479f5"/>
      </fig>
      <p>The threshold of 0.001 was chosen because we found it to be the most appropriate value without any significant loss of information. We ran the experiments for different thresholds. We noticed that when we selected a higher threshold value (say 0.01), our reduced list had around 211 trigrams, hence lost on some significantly informative trigrams whereas when we selected a lower threshold value (say 0.0001), our sequence reduction list had 301 trigrams, which contained a large number of low-contributing trigrams. In <xref ref-type="fig" rid="btaa479-F6">Figure 6</xref>, which has been plotted with a threshold of 0.001, we can see that for almost all the amino acids, the number of trigrams that cover most of the information is around 220. A similar curve profile is noticed for other thresholds. The biological significance of this threshold can be empirically tested by conducting clinical trials. The sequence reduction gives us a correlation between an amino acid of a peptide with a trigram of an MHC allele, while helping in significantly reducing the number of amino acid sequences that need to be clinically tested.
</p>
      <fig id="btaa479-F6" orientation="portrait" position="float">
        <label>Fig. 6.</label>
        <caption>
          <p>Relevance of particular trigrams of MHC alleles for some amino acids in peptides</p>
        </caption>
        <graphic xlink:href="btaa479f6"/>
      </fig>
      <p>The Pareto Principle (<xref rid="btaa479-B22" ref-type="bibr">Lipovetsky, 2009</xref>) suggests that a small fraction (typically given as 20%) of items in a set has a disproportionate impact (typically given as 80%). While analyzing sequence reduction values, we found that the Pareto Principle holds for the amino acid <italic>E</italic>. As shown in <xref ref-type="fig" rid="btaa479-F6">Figure 6</xref>, 20% of the top relevant trigrams contribute to 77.27% of the information provided by all the important trigrams for the amino acid <italic>E</italic>. On the other hand, for other amino acids, the top 20% of the attended trigrams contribute for an average 60% of the relevant information, with a standard deviation of &lt;10% of the mean. This shows that even in the sequence reduction list, it is only a small percentage of trigrams that play a significant role in the prediction of binding. While conducting clinical trials, this reduced list can help determine which peptides are likely to have a higher impact on a particular allele.</p>
    </sec>
  </sec>
  <sec>
    <title>5 Conclusion</title>
    <p>MHCAttnNet shows improved results on a larger variety of peptides and MHC alleles, including both class I and class II MHC alleles. The model handles variable-length peptides because of the use of Bi-LSTMs. It is important to note that unlike many of the previous approaches, MHCAttnNet is a pan-allele method. MHCAttnNet has only one model architecture that works well with both class I and class II MHC alleles. The model weights are different for class I and class II MHC alleles but the architecture remains the same. The improvement in results for MHCAttnNet is mainly due to the use of the ProtVec Embedding (<xref rid="btaa479-B5" ref-type="bibr">Asgari and Mofrad, 2015</xref>) and the use of Bi-LSTMs to encode the amino acid information. The attention layers help improve the results marginally, but the most important benefit of the attention layers is to help illustrate the importance given to any subsequence of the amino acid sequence (either peptide or MHC allele). The use of an attention mechanism helps MHCAttnNet produce more focused and insightful results.</p>
    <p>The bio-medicine community can gain from such a deep learning model that can not only predict with higher precision, but also gives an insight into relevant subsequences of amino acids of MHC alleles and their correlations with particular amino acids of peptides. Therefore, MHCAttnNet can contribute to improved processes for the design and manufacture of personalized cancer vaccines.</p>
    <p>In MHCAttnNet, the input sequence of amino acids is encoded using the ProtVec embedding [trained using the Skip-Gram approach of the Word2Vec (<xref rid="btaa479-B23" ref-type="bibr">Mikolov <italic>et al.</italic>, 2013</xref>) algorithm]. Further work could be to use a contextualized word embedding, like ELMo (<xref rid="btaa479-B30" ref-type="bibr">Peters <italic>et al.</italic>, 2018</xref>), Flair (<xref rid="btaa479-B2" ref-type="bibr">Akbik <italic>et al.</italic>, 2018</xref>) or BERT (<xref rid="btaa479-B10" ref-type="bibr">Devlin <italic>et al.</italic>, 2019</xref>), to encode the input sequence. Such a contextualized word embedding could capture the structural aspects of amino acid sequences by looking into their contexts and hence, would further improve the binding predictions. It would also be interesting to see if the contextualized embedding can lead us to look at the amino acid sequences differently. The focused subsequences (as seen in Section 4.4) can lead to a better understanding of the mechanism of binding between alleles and peptides.</p>
    <p>Our use of sequence reduction, a reduced relevant list of trigrams of amino acids obtained using the attention mechanism, opens the door to further research in this area. Sequence reduction reduces the search space significantly, making clinical trials easier. Understanding where the binding takes place in peptides and MHC alleles is an interesting and important problem. Further work on sequence reduction can lead to better development of cancer vaccines.</p>
  </sec>
  <sec>
    <title>Acknowledgement</title>
    <p>This work was supported by an Amazon AWS Machine Learning Research Award.</p>
    <p><italic>Conflict of Interest</italic>: none declared.</p>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btaa479-B1">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Abbas</surname><given-names>A.K.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) <source>Cellular and Molecular Immunology E-Book</source>. 
<publisher-name>Elsevier Health Sciences</publisher-name> Philadelphia, USA. <ext-link ext-link-type="uri" xlink:href="https://www.clinicalkey.com/dura/browse/bookChapter/3-s2.0-C20150023565">https://www.clinicalkey.com/dura/browse/bookChapter/3-s2.0-C20150023565</ext-link>.</mixed-citation>
    </ref>
    <ref id="btaa479-B2">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Akbik</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) Contextual string embeddings for sequence labeling. In: <italic>Proceedings of the 27th International Conference on Computational Linguistics</italic> pp. <fpage>1638</fpage>–<lpage>1649</lpage>. Association for Computational Linguistics ACL, Santa Fe, NM, USA. <ext-link ext-link-type="uri" xlink:href="https://www.aclweb.org/anthology/C18-1139">https://www.aclweb.org/anthology/C18-1139</ext-link>.</mixed-citation>
    </ref>
    <ref id="btaa479-B3">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Alpaydin</surname><given-names>E.</given-names></name></person-group> (<year>2014</year>) <source>Introduction to Machine Learning</source>. <edition>3rd edn.</edition><publisher-name>MIT Press, Cambridge, Massachusetts, USA</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btaa479-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Andreatta</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Nielsen</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>) 
<article-title>Gapped sequence alignment using artificial neural networks: application to the MHC class I system</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>511</fpage>–<lpage>517</lpage>.<pub-id pub-id-type="pmid">26515819</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Asgari</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Mofrad</surname><given-names>M.R.</given-names></name></person-group> (<year>2015</year>) 
<article-title>Continuous distributed representation of biological sequences for deep proteomics and genomics</article-title>. <source>PLoS One</source>, <volume>10</volume>, <fpage>e0141287</fpage>.<pub-id pub-id-type="pmid">26555596</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B6">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Bahdanau</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) Neural machine translation by jointly learning to align and translate. In: <italic>Published as a conference paper at International Conference on Learning Representations, ICLR</italic>. San Diego, California, USA.</mixed-citation>
    </ref>
    <ref id="btaa479-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Collobert</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Natural language processing (almost) from scratch</article-title>. <source>J. Mach. Learn. Res</source>., <volume>12</volume>, <fpage>2493</fpage>–<lpage>2537</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa479-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Comber</surname><given-names>J.D.</given-names></name>, <name name-style="western"><surname>Philip</surname><given-names>R.</given-names></name></person-group> (<year>2014</year>) 
<article-title>MHC class I antigen presentation and implications for developing a new generation of therapeutic vaccines</article-title>. <source>Ther. Adv. Vaccines Immunother</source>., <volume>2</volume>, <fpage>77</fpage>–<lpage>89</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa479-B9">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Delves</surname><given-names>P.J.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) <source>Essential Immunology</source>. 
<publisher-name>John Wiley &amp; Sons, Massachusetts, USA</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btaa479-B10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Devlin</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) BERT: pre-training of deep bidirectional transformers for language understanding. In: <italic>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol. 1 (Long and Short Papers)</italic> pp. <fpage>4171</fpage>–<lpage>4186</lpage>. Association for Computational Linguistics, Minneapolis, MN, USA.</mixed-citation>
    </ref>
    <ref id="btaa479-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dimitrov</surname><given-names>I.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>MHC class II binding prediction’a little help from a friend</article-title>. <source>BioMed Res. Int</source>., <volume>2010</volume>, <fpage>1</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa479-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fawcett</surname><given-names>T.</given-names></name></person-group> (<year>2006</year>) 
<article-title>An introduction to ROC analysis</article-title>. <source>Pattern Recognit. Lett</source>., <volume>27</volume>, <fpage>861</fpage>–<lpage>874</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa479-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Garrido</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>1993</year>) 
<article-title>Natural history of HLA expression during tumour development</article-title>. <source>Immunol. Today</source>, <volume>14</volume>, <fpage>491</fpage>–<lpage>499</lpage>. <pub-id pub-id-type="doi">10.1016/0167-5699(93)90264-L</pub-id>.<pub-id pub-id-type="pmid">8274189</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B14">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Goodfellow</surname><given-names>I.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) <source>Deep Learning</source>. 
<publisher-name>MIT press, Cambridge, Massachusetts, USA</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btaa479-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hoof</surname><given-names>I.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>NetMHCpan, a method for MHC class I binding prediction beyond humans</article-title>. <source>Immunogenetics</source>, <volume>61</volume>, <fpage>1</fpage>–<lpage>13</lpage>.<pub-id pub-id-type="pmid">19002680</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>a) 
<article-title>ACME: pan-specific peptide-MHC class I binding prediction through attention-based deep neural networks</article-title>. <italic>Bioinformatics</italic>, 35, 4946-4954<source>.</source></mixed-citation>
    </ref>
    <ref id="btaa479-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>Z.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>b) 
<article-title>Towards personalized, tumour-specific, therapeutic vaccines for cancer</article-title>. <source>Nat. Rev. Immunol</source>., <volume>18</volume>, <fpage>168</fpage>–<lpage>182</lpage>.<pub-id pub-id-type="pmid">29226910</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jensen</surname><given-names>K.K.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Improved methods for predicting peptide binding affinity to MHC class II molecules</article-title>. <source>Immunology</source>, <volume>154</volume>, <fpage>394</fpage>–<lpage>406</lpage>.<pub-id pub-id-type="pmid">29315598</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jurtz</surname><given-names>V.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>NetMHCpan-4.0: improved peptide–MHC class I interaction predictions integrating eluted ligand and peptide binding affinity data</article-title>. <source>J. Immunol</source>., <volume>199</volume>, <fpage>3360</fpage>–<lpage>3368</lpage>.<pub-id pub-id-type="pmid">28978689</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B20">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Kingma</surname><given-names>D.P.</given-names></name>, <name name-style="western"><surname>Ba</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>) Adam: a method for stochastic optimization. In: <italic>Published as a conference paper at International Conference on Learning Representations, ICLR</italic>.</mixed-citation>
    </ref>
    <ref id="btaa479-B21">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Kirch</surname><given-names>W.</given-names></name></person-group> (<year>2008</year>) <chapter-title>Pearson’s correlation coefficient</chapter-title> In: <source>Encyclopedia of Public Health</source>. 
<publisher-name>Springer</publisher-name>, 
<publisher-loc>Dordrecht, The Netherlands</publisher-loc>, pp. <fpage>1090</fpage>–<lpage>2013</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa479-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lipovetsky</surname><given-names>S.</given-names></name></person-group> (<year>2009</year>) 
<article-title>Pareto 80/20 law: derivation via random partitioning</article-title>. <source>Int. J. Math. Educ. Sci. Technol</source>., <volume>40</volume>, <fpage>271</fpage>–<lpage>277</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa479-B23">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Mikolov</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) Distributed representations of words and phrases and their compositionality. In: <italic>Advances in Neural Information Processing Systems NIPS</italic> pp. <fpage>3111</fpage>–<lpage>3119</lpage>. Lake Tahoe, USA. <ext-link ext-link-type="uri" xlink:href="http://dl.acm.org/citation.cfm?id=2999792.2999959">http://dl.acm.org/citation.cfm? id=2999792.2999959</ext-link>.</mixed-citation>
    </ref>
    <ref id="btaa479-B24">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Nair</surname><given-names>V.</given-names></name>, <name name-style="western"><surname>Hinton</surname><given-names>G.E.</given-names></name></person-group> (<year>2010</year>) Rectified linear units improve restricted Boltzmann machines. In: <italic>ICML’10 Proceedings of the 27th International Conference on Machine Learning</italic> pp. <fpage>807</fpage>–<lpage>814</lpage>. Omnipress, USA. <ext-link ext-link-type="uri" xlink:href="http://dl.acm.org/citation.cfm?id=3104322.3104425">http://dl.acm.org/citation.cfm?id=3104322.3104425</ext-link>.</mixed-citation>
    </ref>
    <ref id="btaa479-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nielsen</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Andreatta</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>) 
<article-title>NetMHCpan-3.0; improved prediction of binding to MHC class I molecules integrating information from multiple receptor and peptide length datasets</article-title>. <source>Genome Med</source>., <volume>8</volume>, <fpage>33</fpage>.<pub-id pub-id-type="pmid">27029192</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nielsen</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Lund</surname><given-names>O.</given-names></name></person-group> (<year>2009</year>) 
<article-title>NN-align. An artificial neural network-based alignment algorithm for MHC class II peptide binding prediction</article-title>. <source>BMC Bioinformatics</source>, <volume>10</volume>, <fpage>296</fpage>.<pub-id pub-id-type="pmid">19765293</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nielsen</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2003</year>) 
<article-title>Reliable prediction of T-cell epitopes using neural networks with novel sequence representations</article-title>. <source>Protein Sci</source>., <volume>12</volume>, <fpage>1007</fpage>–<lpage>1017</lpage>.<pub-id pub-id-type="pmid">12717023</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>O’Donnell</surname><given-names>T.J.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>MHCflurry: open-source class I MHC binding affinity prediction</article-title>. <source>Cell Syst</source>., <volume>7</volume>, <fpage>129</fpage>–<lpage>132</lpage>.<pub-id pub-id-type="pmid">29960884</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ott</surname><given-names>P.A.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>An immunogenic personal neoantigen vaccine for patients with melanoma</article-title>. <source>Nature</source>, <volume>547</volume>, <fpage>217</fpage>–<lpage>221</lpage>.<pub-id pub-id-type="pmid">28678778</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B30">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Peters</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) Deep contextualized word representations. In: <italic>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL): Human Language Technologies, Vol. 1 (Long Papers)</italic>. New Orleans, Louisiana, USA. doi:<pub-id pub-id-type="doi">10.18653/v1/n18-1202</pub-id>.</mixed-citation>
    </ref>
    <ref id="btaa479-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Phloyphisut</surname><given-names>P.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>MHCSeqNet: a deep neural network model for universal MHC binding prediction</article-title>. <source>BMC Bioinformatics</source>, <volume>20</volume>, <fpage>270</fpage>.<pub-id pub-id-type="pmid">31138107</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pyke</surname><given-names>R.M.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Evolutionary pressure against MHC class II binding cancer mutations</article-title>. <source>Cell</source>, <volume>175</volume>, <fpage>416</fpage>–<lpage>428</lpage>.<pub-id pub-id-type="pmid">30245014</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rajapakse</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) 
<article-title>Predicting peptides binding to MHC class II molecules using multi-objective evolutionary algorithms</article-title>. <source>BMC Bioinformatics</source>, <volume>8</volume>, <fpage>459</fpage>.<pub-id pub-id-type="pmid">18031584</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Reddy</surname><given-names>S.T.</given-names></name></person-group><etal>et al</etal> (<year>2006</year>) 
<article-title>Targeting dendritic cells with biomaterials: developing the next generation of vaccines</article-title>. <source>Trends Immunol</source>., <volume>27</volume>, <fpage>573</fpage>–<lpage>579</lpage>.<pub-id pub-id-type="pmid">17049307</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B35">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Refaeilzadeh</surname><given-names>P.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) <source>Cross-Validation</source>. 
<publisher-name>Springer</publisher-name>, 
<publisher-loc>Boston, MA, USA</publisher-loc>, pp. <fpage>532</fpage>–<lpage>538</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa479-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sahin</surname><given-names>U.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Personalized RNA mutanome vaccines mobilize poly-specific therapeutic immunity against cancer</article-title>. <source>Nature</source>, <volume>547</volume>, <fpage>222</fpage>–<lpage>226</lpage>.<pub-id pub-id-type="pmid">28678784</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Saito</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Rehmsmeier</surname><given-names>M.</given-names></name></person-group> (<year>2015</year>) 
<article-title>The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets</article-title>. <source>PLoS One</source>, <volume>10</volume>, <fpage>e0118432</fpage>.<pub-id pub-id-type="pmid">25738806</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schuster</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Paliwal</surname><given-names>K.</given-names></name></person-group> (<year>1997</year>) 
<article-title>Bidirectional recurrent neural networks</article-title>. <source>Trans. Sig. Proc</source>., <volume>45</volume>, <fpage>2673</fpage>–<lpage>2681</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa479-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Trevethan</surname><given-names>R.</given-names></name></person-group> (<year>2017</year>) 
<article-title>Sensitivity, specificity, and predictive values: foundations, pliabilities, and pitfalls in research and practice</article-title>. <source>Front. Public Health</source>, <volume>5</volume>, <fpage>307</fpage>.<pub-id pub-id-type="pmid">29209603</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B40">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) <chapter-title>Attention is all you need</chapter-title> In: <person-group person-group-type="editor"><name name-style="western"><surname>Guyon</surname><given-names>I.</given-names></name></person-group><etal>et al</etal> (eds) <source>Advances in Neural Information Processing Systems 30</source>. 
<publisher-name>Curran Associates, Inc</publisher-name>, pp. <fpage>5998</fpage>–<lpage>6008</lpage>. Neural Information Processing Systems (NIPS 2017), California, USA. <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="btaa479-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vita</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>The immune epitope database (IEDB) 3.0</article-title>. <source>Nucleic Acids Res</source>., <volume>43</volume>, <fpage>D405</fpage>–<lpage>D412</lpage>.<pub-id pub-id-type="pmid">25300482</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>P.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>A systematic assessment of MHC class II peptide binding predictions and evaluation of a consensus approach</article-title>. <source>PLoS Comput. Biol</source>., <volume>4</volume>, <fpage>e1000048</fpage>.<pub-id pub-id-type="pmid">18389056</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B43">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) Show, attend and tell: neural image caption generation with visual attention. In: <italic>International Conference on Machine Learning (ICML)</italic>, pp. <fpage>2048</fpage>–<lpage>2057</lpage>. Lille, France.</mixed-citation>
    </ref>
    <ref id="btaa479-B44">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group> (<year>2018</year>) NCRF++: an open-source neural sequence labeling toolkit. In: <italic>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL).</italic> Melbourne, Australia. <ext-link ext-link-type="uri" xlink:href="http://aclweb.org/anthology/P18-4013">http://aclweb.org/anthology/P18-4013</ext-link>.</mixed-citation>
    </ref>
    <ref id="btaa479-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zeng</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Gifford</surname><given-names>D.K.</given-names></name></person-group> (<year>2019</year>a) 
<article-title>DeepLigand: accurate prediction of MHC class I ligands using peptide embedding</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>i278</fpage>–<lpage>i283</lpage>.<pub-id pub-id-type="pmid">31510651</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zeng</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Gifford</surname><given-names>D.K.</given-names></name></person-group> (<year>2019</year>b) 
<article-title>Quantification of uncertainty in peptide-MHC binding prediction improves high-affinity peptide selection for therapeutic design</article-title>. <source>Cell Syst</source>., <volume>9</volume>, <fpage>159</fpage>–<lpage>166</lpage>.<pub-id pub-id-type="pmid">31176619</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>W.</given-names></name>, <name name-style="western"><surname>Sher</surname><given-names>X.</given-names></name></person-group> (<year>2018</year>) 
<article-title>Systematically benchmarking peptide-MHC binding predictors: from synthetic to naturally processed epitopes</article-title>. <source>PLoS Comput. Biol</source>., <volume>14</volume>, <fpage>e1006457</fpage>.<pub-id pub-id-type="pmid">30408041</pub-id></mixed-citation>
    </ref>
    <ref id="btaa479-B48">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>P.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) Attention-based bidirectional long short-term memory networks for relation classification. In: <italic>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Vol. 2: Short Papers)</italic> pp. <fpage>207</fpage>–<lpage>212</lpage>. Association for Computational Linguistics, Berlin, Germany.</mixed-citation>
    </ref>
  </ref-list>
</back>
