<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5521087</article-id>
    <article-id pub-id-type="publisher-id">1757</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-017-1757-y</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methodology Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>A deep convolutional neural network approach to single-particle recognition in cryo-electron microscopy</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Zhu</surname>
          <given-names>Yanan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ouyang</surname>
          <given-names>Qi</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9302-2257</contrib-id>
        <name>
          <surname>Mao</surname>
          <given-names>Youdong</given-names>
        </name>
        <address>
          <phone>+1 617 632 4358</phone>
          <email>youdong_mao@dfci.harvard.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2256 9319</institution-id><institution-id institution-id-type="GRID">grid.11135.37</institution-id><institution>Center for Quantitative Biology, </institution><institution>Peking University, </institution></institution-wrap>Beijing, 100871 China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2256 9319</institution-id><institution-id institution-id-type="GRID">grid.11135.37</institution-id><institution>State Key Laboratory for Artificial Microstructure and Mesoscopic Physics, </institution><institution>Peking University, Institute of Condensed Matter Physics, School of Physics, </institution></institution-wrap>Beijing, 100871 China </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2256 9319</institution-id><institution-id institution-id-type="GRID">grid.11135.37</institution-id><institution>Peking-Tsinghua Center for Life Sciences, </institution><institution>Peking University, </institution></institution-wrap>Beijing, 100871 China </aff>
      <aff id="Aff4"><label>4</label>Intel Parallel Computing Center for Structural Biology, Department of Microbiology and Immunobiology, Dana-Farber Cancer Institute, Harvard Medical School, Boston, MA 02115 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>21</day>
      <month>7</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>21</day>
      <month>7</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2017</year>
    </pub-date>
    <volume>18</volume>
    <elocation-id>348</elocation-id>
    <history>
      <date date-type="received">
        <day>16</day>
        <month>11</month>
        <year>2016</year>
      </date>
      <date date-type="accepted">
        <day>13</day>
        <month>7</month>
        <year>2017</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s). 2017</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Single-particle cryo-electron microscopy (cryo-EM) has become a mainstream tool for the structural determination of biological macromolecular complexes. However, high-resolution cryo-EM reconstruction often requires hundreds of thousands of single-particle images. Particle extraction from experimental micrographs thus can be laborious and presents a major practical bottleneck in cryo-EM structural determination. Existing computational methods for particle picking often use low-resolution templates for particle matching, making them susceptible to reference-dependent bias. It is critical to develop a highly efficient template-free method for the automatic recognition of particle images from cryo-EM micrographs.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We developed a deep learning-based algorithmic framework, DeepEM, for single-particle recognition from noisy cryo-EM micrographs, enabling automated particle picking, selection and verification in an integrated fashion. The kernel of DeepEM is built upon a convolutional neural network (CNN) composed of eight layers, which can be recursively trained to be highly “knowledgeable”. Our approach exhibits an improved performance and accuracy when tested on the standard KLH dataset. Application of DeepEM to several challenging experimental cryo-EM datasets demonstrated its ability to avoid the selection of un-wanted particles and non-particles even when true particles contain fewer features.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">The DeepEM methodology, derived from a deep CNN, allows automated particle extraction from raw cryo-EM micrographs in the absence of a template. It demonstrates an improved performance, objectivity and accuracy. Application of this novel method is expected to free the labor involved in single-particle verification, significantly improving the efficiency of cryo-EM data processing.</p>
      </sec>
      <sec>
        <title>Electronic supplementary material</title>
        <p>The online version of this article (doi:10.1186/s12859-017-1757-y) contains supplementary material, which is available to authorized users.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Cryo-EM</kwd>
      <kwd>Particle recognition</kwd>
      <kwd>Convolutional neural network</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Single-particle reconstruction</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id>
            <institution>National Natural Science Foundation of China</institution>
          </institution-wrap>
        </funding-source>
        <award-id>91530321</award-id>
        <principal-award-recipient>
          <name>
            <surname>Mao</surname>
            <given-names>Youdong</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2017</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par9">Single-particle cryo-EM images suffer from heavy background noise and low contrast, due to the limited electron dose used in imaging in order to reduce radiation damage to the biomolecules of interest [<xref ref-type="bibr" rid="CR1">1</xref>]. Hence, a large number of single-particle images, extracted from cryo-EM micrographs, is required to perform a reliable 3D reconstruction of the underlying structure. Particle recognition thus represents the first bottleneck in the practice of cryo-EM structure determination. During the past decades, many computational methods have been proposed for automated particle recognition, mostly based on template matching, edge detection, feature extraction or neural networks [<xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR15">15</xref>]. The template matching methods depend on a local cross-correlation that is sensitive to noise, and a substantial fraction of false positives may result from false correlation peaks [<xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR8">8</xref>]. Similarly, both the edge-based [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR10">10</xref>] and feature-based methods [<xref ref-type="bibr" rid="CR11">11</xref>–<xref ref-type="bibr" rid="CR13">13</xref>] suffer from a dramatical reduction of performance with lower contrast of the micrographs. In a different approach, a method based on a three-layer pyramidal-type artificial neural network was developed [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR15">15</xref>]. However, there is only one hidden layer in the designed neutral network, which is insufficient to extract rich features from single-particle images. A common problem for these automated particle recognition algorithms lies in the fact that they cannot distinguish “good particles” from “bad” ones, including overlapped particles, local aggregates, background noise fluctuations, ice contamination and carbon-rich areas. Thus, additional steps comprising unsupervised image classification or manual verification and selection are necessary to sort out “good particles” after initial automated particle picking. For example, TMaCS uses the support vector machine (SVM) algorithm to classify the particles initially picked by a template-matching method to remove false positives [<xref ref-type="bibr" rid="CR16">16</xref>].</p>
    <p id="Par10">Deep learning is a type of machine learning that focuses on learning from multiple levels of feature representation, and can be used to make sense of multi-dimensional data such as images, sound and text [<xref ref-type="bibr" rid="CR17">17</xref>–<xref ref-type="bibr" rid="CR22">22</xref>]. It is a process of layered feature extraction. In other words, features in greater detail can be extracted by moving the hidden layer down to a deeper level using multiple non-linear transformations [<xref ref-type="bibr" rid="CR22">22</xref>]. Convolutional neural network (CNN) is a biologically inspired deep, feed-forward neural network that has demonstrated an outstanding performance in speech recognition [<xref ref-type="bibr" rid="CR23">23</xref>] and image processing, such as handwriting recognition [<xref ref-type="bibr" rid="CR24">24</xref>], facial detection [<xref ref-type="bibr" rid="CR25">25</xref>] and cellular image classification [<xref ref-type="bibr" rid="CR26">26</xref>]. Its unique advantage lies in the fact that the special structure of shared local weights reduces the complexity of the network [<xref ref-type="bibr" rid="CR27">27</xref>, <xref ref-type="bibr" rid="CR28">28</xref>]. Multidimensional images can be directly used as inputs of the network, which avoids the complexities of feature extraction in the reconstructed data [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR27">27</xref>].</p>
    <p id="Par11">The particle recognition problem in cryo-EM is fundamentally a binary classification problem, and is based on the features of single-particle images. We devised a novel automated particle recognition approach based on deep CNN learning [<xref ref-type="bibr" rid="CR27">27</xref>]. Our algorithm, named DeepEM, is built upon an eight-layer CNN, including an input layer, three convolutional layers, three subsampling layers, and an output layer (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). In this study, we applied this deep-learning approach to tackle the problem of automated template-free particle recognition. The DeepEM algorithm was examined through the task of detecting “good particles” from cryo-EM micrographs taken in a variety of situations, and demonstrated improved accuracy over other template-matching methods.<fig id="Fig1"><label>Fig. 1</label><caption><p>The architecture of the convolutional neural network used in DeepEM. The convolutional layer and the subsampling layer are abbreviated as C and S, respectively. C1:6@222×222 means that it is a convolutional layer and is the first layer of the network. This layer is comprised of six feature maps, each of which has a size of 222 × 222 pixels. The <italic>symbols</italic> and <italic>numbers</italic> above the feature maps of other layers have the equivalent corresponding meaning</p></caption><graphic xlink:href="12859_2017_1757_Fig1_HTML" id="MO1"/></fig>
</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>Design of the DeepEM algorithm</title>
      <p id="Par12">The DeepEM algorithm is based on a convolutional neural network, a multilayered neural network with local connections. It contains convolutional layers, subsampling layers and fully connected layers, in addition to the input and output layers (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). The convolutional and subsampling layers produce feature maps through repeated application of the activation function across sub-regions of the images, which represent low-frequency features extracted from the previous layer (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S1).</p>
      <p id="Par13">In the convolutional layer, which is the core building block of a CNN, the connections are local, but expand throughout the entire input image. Such a network architecture ensures that the outputs of the convolutional layer are effectively activated in response to the detection of meaningful input spatial features. The feature maps from the previous layer are convoluted by a learnable kernel. All convolution operation outputs are then transformed by a nonlinear activation function. We used the sigmoid function (<xref rid="Equ1" ref-type="">1</xref>) as the nonlinear activation function.<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ sigmoid(x)=1/\left(1+{e}^{- x}\right) $$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mtext mathvariant="italic">sigmoid</mml:mtext><mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">/</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2017_1757_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>
</p>
      <p id="Par14">The convolution operations in the same convolutional layer share the same connectivity weights with the previous layer, so that:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {X}_j^{\left[ l\right]}= sigmoid\left(\sum_{i\in {M}_j}{X_i^{\left[ l-1\right]}}^{\ast }{W}_{i j}^{\left[ l\right]}+{B}^{\left[ l\right]}\right), $$\end{document}</tex-math><mml:math id="M4" display="block"><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mfenced close="]" open="["><mml:mi>l</mml:mi></mml:mfenced></mml:msubsup><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">sigmoid</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msup><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:msubsup><mml:mo>∗</mml:mo></mml:msup><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow><mml:mfenced close="]" open="["><mml:mi>l</mml:mi></mml:mfenced></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mfenced close="]" open="["><mml:mi>l</mml:mi></mml:mfenced></mml:msup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2017_1757_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>
</p>
      <p id="Par15">where <italic>l</italic> represents the convolutional layer; <italic>W</italic> represents the shared weights; <italic>M</italic> represents different feature maps from the previous layer; <italic>j</italic> represents one of the output feature maps; <italic>B</italic> represents the bias in the layer; and the star symbol (*) represents the convolution operation.</p>
      <p id="Par16">Subsampling is another important concept in CNNs. A subsampling layer is designed to subsample the input data to progressively decrease the spatial size of the representation and reduce the number of parameters and computational cost in the network, thus reducing potential over-fitting [<xref ref-type="bibr" rid="CR29">29</xref>]. We computed the subsampling averages after each convolutional layer using the following expression:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {X}_{ij}^{\left[ l\right]}=\frac{1}{ M N}{\sum}_m^M{\sum}_n^N{X}_{iM+ m, jN+ n}^{\left[ l-1\right]} $$\end{document}</tex-math><mml:math id="M6" display="block"><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow><mml:mfenced close="]" open="["><mml:mi>l</mml:mi></mml:mfenced></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi mathvariant="italic">MN</mml:mi></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:msubsup><mml:msubsup><mml:mo>∑</mml:mo><mml:mi>n</mml:mi><mml:mi>N</mml:mi></mml:msubsup><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">iM</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="italic">jN</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:msubsup></mml:math><graphic xlink:href="12859_2017_1757_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <italic>i</italic> and <italic>j</italic> represent the position of the output map; <italic>M</italic> and <italic>N</italic> represent the subsampling size in two orthogonal dimensions.</p>
      <p id="Par17">The basic network architecture of DeepEM contains three convolutional layers (the first, third, and fifth layers) and three subsampling layers (the second, fourth and sixth layers). The last layer is fully connected to the previous layer, which outputs a prediction for the classification of the input image by the weight matrix and the activation function (Fig. <xref rid="Fig1" ref-type="fig">1</xref>).</p>
    </sec>
    <sec id="Sec4">
      <title>Training of the DeepEM network</title>
      <p id="Par18">Prior to the application of DeepEM for automated particle recognition, the CNN needs to be trained with a manually assembled dataset, sampling both true particle images (positive training data) and non-particle images (negative training data) (Examples in Fig. <xref rid="Fig3" ref-type="fig">3a</xref>, <xref rid="Fig3" ref-type="fig">b</xref>). Only a well-trained CNN should be used to recognize particles from raw micrographs. We used the error back-propagation method [<xref ref-type="bibr" rid="CR30">30</xref>] to train the network, which produces an output of “1” for the true particle images and “0” for the non-particle images. The weights and biases in the CNN model are initialized with a random number between 0 and 1, and are then updated in the training process. We used the squared-error loss function [<xref ref-type="bibr" rid="CR30">30</xref>] as the objective function in our model. For a training dataset with the number of <italic>N</italic>, it is defined as:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {E}_N=\frac{1}{2 N}{\sum}_{n=1}^N{\left\Vert {t}_n-{y}_n\right\Vert}^2, $$\end{document}</tex-math><mml:math id="M8" display="block"><mml:msub><mml:mi>E</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msup><mml:mfenced close="‖" open="‖"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2017_1757_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>
</p>
      <p id="Par19">where <italic>t</italic>
<sub><italic>n</italic></sub> is the target of the <italic>n</italic>th training image, and <italic>y</italic>
<sub><italic>n</italic></sub> is the value of the output layer in response to the <italic>n</italic>th input training image. During the process of training, the objective function is minimized using an error back-propagation algorithm [<xref ref-type="bibr" rid="CR30">30</xref>], which performs a gradient-based update as follows:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \omega \left( t+1\right)=\omega (t)-\frac{\eta}{N}{\sum}_{k=1}^N{\varepsilon}_n\frac{\partial {\varepsilon}_n}{\partial \omega} $$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mi>ω</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>ω</mml:mi><mml:mfenced close=")" open="("><mml:mi>t</mml:mi></mml:mfenced><mml:mo>−</mml:mo><mml:mfrac><mml:mi>η</mml:mi><mml:mi>N</mml:mi></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mi>ε</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="12859_2017_1757_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <italic>ε</italic>
<sub><italic>n</italic></sub> = ‖<italic>t</italic>
<sub><italic>n</italic></sub> − <italic>y</italic>
<sub><italic>n</italic></sub>‖; <italic>ω</italic>(<italic>t</italic>) and <italic>ω</italic>(<italic>t</italic> + 1) represent the parameters before and after the update of an iteration, respectively; <italic>η</italic> is the learning rate and was set to 1 in this study.</p>
      <p id="Par20">The data augmentation technique has shown a certain improvement in the accuracy of CNN training with a large number of parameters [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR26">26</xref>]. During our DeepEM training, each original particle image in the training dataset was rotated by 90°, 180° and 270°, in order to augment the size of data sampling by a factor of four. The intensity of each pixel from an original or rotated image was then used as the input of a neuron of the input layer. The desired output was set to 1 for the positive data and 0 for the negative data in the error back-propagation procedure.</p>
      <p id="Par21">The experimental cryo-EM micrographs may contain heterogeneous objects, such as protein impurities, ice contamination, carbon-rich areas, overlapping particles and local aggregates. Moreover, since the molecules in the single-particle images assume random orientations, significantly different projection structures of the same macromolecule may coexist in a micrograph. These factors make it difficult to assemble a relatively balanced training dataset at the beginning, which must include representative positive and negative particle images. The initially trained CNN is prone to missing some target particles in certain views or recognizing some unwanted particles whose appearances are similar to the target. The training dataset can be optimized by adding a greater number of representative particle images to the original training dataset after testing on a separate set of micrographs that are independent of the ones used for assembling the original training dataset, and then re-training the network following the workflow chart shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. After a sufficient number of iterations of training, the CNN becomes more “knowledgeable” in differentiating positive particles from negative ones.<fig id="Fig2"><label>Fig. 2</label><caption><p>The workflow diagram of the DeepEM algorithm. The <italic>dashed box on the left</italic> represents the learning process; the <italic>dashed box on the right</italic> represents the recognition process</p></caption><graphic xlink:href="12859_2017_1757_Fig2_HTML" id="MO2"/></fig>
</p>
      <p id="Par22">Since the input particle images size may vary in different datasets, one can set different hyper-parameters for each case, including the number of feature maps, the kernel size of the convolutional layers and the pooling region size of the pooling layers. We empirically initialized these hyper-parameters and fine-tuned them during the training process (Fig. <xref rid="Fig2" ref-type="fig">2</xref>). The details of the hyper-parameters used in this study are shown in Table <xref rid="Tab1" ref-type="table">1</xref>. In general, the output dimension of the convolutional layer is chosen as 70–90% of its input dimension, and the output dimension of the subsampling layer is scaled to about half its input dimension. We implemented the DeepEM algorithm based on the DeepLearnToolbox [<xref ref-type="bibr" rid="CR31">31</xref>], a toolbox for the development of deep learning algorithms, in conjunction with Matlab.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Hyper-parameters used in different datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Dataset</th><th rowspan="2">Particle size</th><th colspan="6">Corresponding layer in DeepEM</th></tr><tr><th>C1</th><th>S2</th><th>C3</th><th>S4</th><th>C5</th><th>S6</th></tr></thead><tbody><tr><td>KLH</td><td>272 × 272</td><td>6@222X222</td><td>6@74X74</td><td>12@54X54</td><td>12@27X27</td><td>12@18X18</td><td>12@9X9</td></tr><tr><td>19S</td><td>160 × 160</td><td>6@141X141</td><td>6@47X47</td><td>12@38X38</td><td>12@19X19</td><td>12@16X16</td><td>12@8X8</td></tr><tr><td>26S</td><td>150 × 150</td><td>6@120X120</td><td>6@60X60</td><td>12@46X46</td><td>12@23X23</td><td>12@14X14</td><td>12@7X7</td></tr><tr><td>Inflammasome</td><td>112 × 112</td><td>6@98X98</td><td>6@49X49</td><td>12@40X40</td><td>12@20X20</td><td>12@14X14</td><td>12@7X7</td></tr></tbody></table></table-wrap>
</p>
    </sec>
    <sec id="Sec5">
      <title>Particle recognition and selection in the DeepEM model</title>
      <p id="Par23">When a well-trained CNN is used to recognize particles, a square box of pixels is taken as the CNN input. Each input image boxed out of a testing micrograph is rotated incrementally, to generate three additional copies of the input image with rotations of 90°, 180° and 270°, relative to the original. Each copy is used as a separate input to generate a CNN output. The final expectation value of each input image is taken as the average of its four output values from the non-rotated and rotated copies. The boxed area is initially placed into a corner of the testing micrograph, and is raster-scanned across the whole micrograph to generate an array of CNN outputs.</p>
      <p id="Par24">We used two criteria to select particles. First, a threshold score must be defined. The boxed image is identified as a candidate if the CNN output score of the particle is above the threshold score. Those particles whose CNN scores are below the threshold are rejected. We used the <italic>F</italic>-measure [<xref ref-type="bibr" rid="CR32">32</xref>], which is a measure of the accuracy of a test that combines both precision and recall for binary classification problems, to determine the threshold score in our approach, which is defined as.<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {F}_{\beta}={\left(1+{\beta}^2\right)}^{\ast}\frac{precision^{\ast } recall}{\left({\beta^2}^{\ast } precision+ recall\right)}, $$\end{document}</tex-math><mml:math id="M12" display="block"><mml:msub><mml:mi>F</mml:mi><mml:mi>β</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfenced><mml:mo>∗</mml:mo></mml:msup><mml:mfrac><mml:mrow><mml:msup><mml:mtext mathvariant="italic">precision</mml:mtext><mml:mo>∗</mml:mo></mml:msup><mml:mtext mathvariant="italic">recall</mml:mtext></mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:msup><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>∗</mml:mo></mml:msup><mml:mtext mathvariant="italic">precision</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">recall</mml:mtext></mml:mrow></mml:mfenced></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2017_1757_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>
</p>
      <p id="Par25">where <italic>β</italic> is a coefficient weighting the importance of precision and recall. In our method, we used the <italic>F</italic>
<sub>2</sub> score, which weights the recall higher than the precision. The <italic>F</italic>
<sub>2</sub>-score reaches its best value at 1 and its worst at 0. We defined the cutoff threshold at the highest value of the <italic>F</italic>
<sub>2</sub>-score.</p>
      <p id="Par26">Secondly, candidate images were further selected based on the standard deviation of the pixel intensities. There are often carbon-rich areas or contaminants in raw micrographs where the initially detected particles may not be good choices for downstream single-particle analysis. The pixels belonging to the “particles” in these areas usually have higher or lower standard deviations compared with those in other areas with clean amorphous ice. We therefore set a narrow range of the pixel standard deviation to remove the candidate particles that are initially picked from these unwanted areas [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR16">16</xref>] (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S2).</p>
    </sec>
    <sec id="Sec6">
      <title>DeepEM algorithm workflow</title>
      <sec id="Sec7">
        <title>Learning process</title>
        <p id="Par27"><bold>Input</bold>: Training dataset.</p>
        <p id="Par28"><bold>Output</bold>: Trained CNN parameters (weights and biases)<list list-type="order"><list-item><p id="Par29">Rotate each input particle image three times, each with a 90° increment;</p></list-item><list-item><p id="Par30">Set the output of the positive data as 1, and the output of the negative data as 0;</p></list-item><list-item><p id="Par31">Initialize the hyper-parameters;</p></list-item><list-item><p id="Par32">Randomly initialize the weights and biases in each convolutional layer;</p></list-item><list-item><p id="Par33">While (Learning error &gt; Defined error), do<list list-type="alpha-lower"><list-item><p id="Par34">Tune the hyper-parameters or optimize the training dataset by adding more representative positive and negative particles from a new set of micrographs, which are independent of those used in the previous iterations, to the training dataset;</p></list-item><list-item><p id="Par35">Train weights and biases via the error back-propagation algorithm;</p></list-item><list-item><p id="Par36">Apply the trained CNN to an independent testing dataset to measure the learning error</p></list-item></list>
</p></list-item><list-item><p id="Par37">End while</p></list-item></list></p>
      </sec>
      <sec id="Sec8">
        <title>Recognition process</title>
        <p id="Par38"><bold>Input</bold>: Micrographs and trained CNN.</p>
        <p id="Par39"><bold>Output</bold>: Box files of selected particles in the EMAN2 [<xref ref-type="bibr" rid="CR33">33</xref>] format for each micrograph<list list-type="order"><list-item><p id="Par40">Iterate the following steps (a-c) until the whole micrograph has been raster-scanned;<list list-type="alpha-lower"><list-item><p id="Par41">Extract a square the size of a particle, starting from a corner of the input micrograph;</p></list-item><list-item><p id="Par42">Rotate the boxed image three times, each with a 90-degree increment;</p></list-item><list-item><p id="Par43">Use the trained CNN to process four copies of the boxed image, including the non-rotated and rotated copies, and average the resulting output scores of the four images;</p></list-item></list>
</p></list-item><list-item><p id="Par44">Pick the particle candidates based on scores that are not only local maxima but also above the threshold score;</p></list-item><list-item><p id="Par45">Select particle images based on their standard deviations;</p></list-item><list-item><p id="Par46">Write the coordinates of the selected particle images into the box file.</p></list-item></list></p>
      </sec>
    </sec>
    <sec id="Sec9">
      <title>Performance evaluation</title>
      <p id="Par47">We evaluated the performance of the method based on the precision-recall curve [<xref ref-type="bibr" rid="CR34">34</xref>], which is one of the most popular metrics for the performance evaluation of various particle-selection algorithms. The precision and recall are defined by Eqs. (<xref rid="Equ7" ref-type="">7</xref>) and (<xref rid="Equ8" ref-type="">8</xref>), respectively.<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{Precision}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}} $$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mtext>Precision</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mi>TP</mml:mi><mml:mrow><mml:mi>TP</mml:mi><mml:mo>+</mml:mo><mml:mi>FP</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="0.25em"/></mml:math><graphic xlink:href="12859_2017_1757_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{Recall}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}} $$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mtext>Recall</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mi>TP</mml:mi><mml:mrow><mml:mi>TP</mml:mi><mml:mo>+</mml:mo><mml:mi>FN</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="12859_2017_1757_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>
</p>
      <p id="Par48">The precision represents the fraction of true positives (TP) among the total particle images selected (TP + FP), and the recall represents the fraction of true particle images selected among all the true particle images (TP + FN) contained in the micrographs. The precision-recall curve is generated from the algorithm by varying the threshold score used in the particle recognition procedure. When the threshold increases, the precision would increase and the recall would decrease accordingly. Thus, the threshold is manifested as a balance between the precision and the recall. For a good performance in particle selection, both the precision and the recall are expected to achieve higher values at a certain threshold.</p>
    </sec>
    <sec id="Sec10">
      <title>DeepEM training on the keyhole limpet Hemocyanin (KLH) dataset</title>
      <p id="Par49">The KLH dataset was acquired from the US National Resource for Automated Molecular Microscopy (<ext-link ext-link-type="uri" xlink:href="http://nramm.scripps.edu">nramm.scripps.edu</ext-link>). KLH is ~8 MDa protein particle with a size of ~40 nm. It consists of 82 micrographs at 2.2 Å/pixel that were acquired on a Philips CM200 microscope at 120 kV. The size of the micrograph is 2048 by 2048 pixels. There are two main types of projection views of the KLH complex, the side view and the top view. We boxed the particle images with a dimension of 272 pixels. 800 particle images were manually selected for the positive training dataset. The same number of randomly selected non-particle images from the first fifty micrographs was used as a negative dataset (Fig. <xref rid="Fig3" ref-type="fig">3a</xref>). Each original image in the training dataset was rotated at 90° increments to create three additional images to augment the training data. We also selected some particle images as a testing dataset containing positive and negative data that were not used in the prior training step. The testing dataset was used to test the intermediately trained CNN model (Fig. <xref rid="Fig2" ref-type="fig">2</xref>). The accuracy or error of the CNN learning output from the testing dataset was used as a feedback parameter to tune the hyper-parameters, including the number of feature maps, kernel size of the convolutional layers, and subsampling size of the subsampling layers in the network. Throughout the training-testing cycles, we tuned the hyper-parameters and updated the training dataset until the accuracy of the CNN learning reached a satisfactory level. The acceptable value was often set as ~95% at the threshold of 0.5 (Fig. <xref rid="Fig2" ref-type="fig">2</xref>).<fig id="Fig3"><label>Fig. 3</label><caption><p>The DeepEM results for the KLH and 19S regulatory particle datasets. <bold>a</bold> and <bold>b</bold> Examples of positive and negative particle images selected for the CNN training in conjunction with the KLH and 19S datasets, respectively. <bold>c</bold> and <bold>d</bold> Typical micrographs from the KLH and 19S datasets, respectively. The <italic>white square boxes</italic> indicate the positive particle images selected by DeepEM. The <italic>boxes with a triangle</italic> inside indicate that a false-positive particle image was picked. The <italic>star marks</italic> one example of a false negative, a true particle missed by the recognition program. <bold>e</bold> The <italic>F</italic>
<sub>2</sub>-score curves provide different thresholds for particle recognition in the KLH and 19S datasets, the arrows indicate the peaks of each curve, where the cutoff threshold value is defined. <bold>f</bold> The precision-recall curves plotted against a manually selected list of particle images</p></caption><graphic xlink:href="12859_2017_1757_Fig3_HTML" id="MO3"/></fig>
</p>
    </sec>
    <sec id="Sec11">
      <title>Application to experimental cryo-EM data</title>
      <p id="Par50">The original sizes of the micrographs of the inflammasome, 19S regulatory particle and 26S proteasome were 7420 by 7676, 3710 by 3838 and 7420 by 7676 pixels, respectively. The pixel sizes of the inflammasome, 19S regulatory particle and proteasome holoenzyme were 0.86, 0.98 and 0.86 Å/pixel, respectively. For the inflammasome and 26S proteasome, the micrographs were binned 4 times. Therefore, the pixel size used for the inflammasome and proteasome holoenzyme was 3.44 Å/pixel. For the 19S regulatory particle, the micrographs were binned 2 times, resulting in a pixel size of 1.96 Å/pixel. Thus, the resulting sizes of the micrographs used in our tests were all 1855 by 1919 pixels; the dimension of the particle images of the inflammasome, 19S and 26S complexes were 112, 160 and 150 pixels, respectively. These experimental cryo-EM datasets were acquired using a FEI Tecnai Arctica microscope (FEI, USA) at 200 kV, equipped with a Gatan K2 Summit direct electron detector. Finally, we applied the DeepEM algorithm to these cryo-EM datasets. The hyper-parameters tuned for these datasets are shown in Table <xref rid="Tab1" ref-type="table">1</xref>. Different from the training for the KLH dataset, we added true positive and false positive data, which were manually verified on a separate set of micrographs independent of the testing dataset used for tuning the hyper-parameters, to optimize the training dataset and to train the network recursively for the low-contrast datasets (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S3).</p>
    </sec>
  </sec>
  <sec id="Sec12">
    <title>Results</title>
    <sec id="Sec13">
      <title>Experiments on the KLH dataset</title>
      <p id="Par51">We first tested our DeepEM algorithm on the Keyhole Limpet Hemocyanin (KLH) dataset [<xref ref-type="bibr" rid="CR35">35</xref>] that was previously used as a standard testing dataset to benchmark various particle selection methods [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR11">11</xref>–<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR16">16</xref>]. For the KLH dataset, the recall and the precision both reached ~90% at the same time in the precision-recall curve (Fig. <xref rid="Fig3" ref-type="fig">3f</xref>) plotted against a manually selected set of particle images from 32 micrographs that did not include any particle images used in the training dataset. Our approach achieved a higher precision over all the particle images selected, whereas the recall was kept at a high value, indicating that fewer false-negative particle images were missed among the micrographs. In a typical KLH micrograph (Fig. <xref rid="Fig3" ref-type="fig">3c</xref>), all true particle images were automatically recognized by our method with a threshold of 0.84, as determined by the <italic>F</italic>
<sub>2</sub>-score (see Methods and Eq. 6) (Fig. <xref rid="Fig3" ref-type="fig">3e</xref>). A comparison of the precision-recall curves between DeepEM, RELION [<xref ref-type="bibr" rid="CR36">36</xref>] and TMACS [<xref ref-type="bibr" rid="CR16">16</xref>] suggests that DeepEM outperforms these two template-matching based methods (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S4).</p>
      <p id="Par52">To understand the impact of the number of training particles on algorithm performance, we varied the particle number in the KLH training dataset from 100 to 1200, and plotted the corresponding precision-recall curves (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). In each testing case, the number of positive particles was kept equal to that of the negative particles. Although there was clear improvement in the precision-call curve when the training particle number was increased from 100 to 400, there was little improvement with a further increase of the training dataset size. The best result was obtained in the training run with 800 positive particle images.<fig id="Fig4"><label>Fig. 4</label><caption><p>Impact of the training image number on the precision-recall curve. The <italic>black</italic>, <italic>blue</italic>, <italic>red</italic> and <italic>green</italic> curves were obtained with the training datasets including 100, 400, 800 and 1200 positive or negative images, respectively</p></caption><graphic xlink:href="12859_2017_1757_Fig4_HTML" id="MO4"/></fig>
</p>
    </sec>
    <sec id="Sec14">
      <title>Experiments on cryo-EM datasets</title>
      <p id="Par53">We also applied our method to several challenging cryo-EM datasets collected using a direct electron detector, including the 19S regulatory particle, 26S proteasome and NLRC4/NAIP2 inflammasome [<xref ref-type="bibr" rid="CR37">37</xref>]. Figure <xref rid="Fig3" ref-type="fig">3d</xref> shows a typical micrograph of the 19S regulatory particle, in which DeepEM selected almost all true particle images contained in the micrograph. At the same time, it avoided selecting non-particles from areas containing aggregates and carbon film. The precision-recall curve resulting from the test on the 19S dataset is shown in Fig. <xref rid="Fig3" ref-type="fig">3f</xref>. The precision and recall both reach ~80% at the same time. The picked particles were approximately as well-centered as the manually boxed ones. To further verify that the selected particle images are correct, we performed unsupervised 2D classification. The resulting reference-free class averages from about 100 micrographs were consistent with different views of the protein samples (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S5).</p>
      <p id="Par54">Two difficult cases from the inflammasome dataset were examined. Figure <xref rid="Fig5" ref-type="fig">5a</xref> shows a micrograph with a high particle density that contains excessively overlapped particles and ice contamination. Most methods based on template matching were incapable of avoiding particle picking from overlapped particles and ice contaminants in this case. Figure <xref rid="Fig5" ref-type="fig">5b</xref> presents another difficult situation, in which the side views of the inflammasome display a lower SNR, lack low-frequency features, and are dispersed with a very low spatial density. In both cases, DeepEM still performed quite well in particle recognition, while avoiding the selection of overlapping particles and non-particles. Further tests on similar cases from other protein samples suggested that this observation had a good reproducibility (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S6). Most importantly, DeepEM was able to determine the structure of the human 26S proteasome [<xref ref-type="bibr" rid="CR38">38</xref>].<fig id="Fig5"><label>Fig. 5</label><caption><p>Two challenging examples of automated particle recognition. <bold>a</bold> A typical micrograph showing high-density top views of the inflammasome complex. Considerable ice contaminants and overlapping particles are present. <bold>b</bold> A typical micrograph of the side views of the inflammasome showing both a paucity of features and a low density of objects. The <italic>white square boxes</italic> indicate the positive particle images selected by DeepEM. The <italic>boxes with a triangle</italic> inside indicate that false-positive particle images were picked. The <italic>boxes with a star</italic> inside indicate the omitted particle images. <bold>c</bold> The precision-recall curves corresponding to the cases shown in (<bold>a</bold>) and (<bold>b</bold>)</p></caption><graphic xlink:href="12859_2017_1757_Fig5_HTML" id="MO5"/></fig>
</p>
    </sec>
    <sec id="Sec15">
      <title>Computational efficiency</title>
      <p id="Par55">The DeepEM algorithm was first tested on a Macintosh with a 3.3 GHz Intel Core i5 and 32 GB memory, running Matlab 2014b. When the size of the particle images increases, the parameter space increases substantially, so that it costs more computational time for each micrograph. We usually binned the original micrographs 2 or 4 times to reduce the size of the particle images. For the KLH dataset, it took about 7300 s per micrograph with a micrograph size of 2048 by 2048 pixels and particle image size of 272 by 272 pixels. For the 19S regulatory particle, inflammasome and 26S proteasome datasets, it took about 790, 560, and 1160 s per micrograph with a binned micrograph size of 1855 by 1919 pixels and particle image sizes of 112 by 112, 160 by 160, and 150 by 150 pixels, respectively. To speed up the calculations, multiple instances of the code were run in parallel. We also implemented a Graphic Processing Unit (GPU)-accelerated version of DeepEM in Matlab. We tested it on a desktop computer with 4.0 GHz Intel Core i7-6700 k, 64GB memory and Geforce GTX 970, running Matlab 2016a and CUDA 8.0. It only took about 190, 50, 40, and 60 s per micrograph for the KLH, 19S regulatory particle, inflammasome and 26S proteasome datasets, respectively. The GPU-accelerated DeepEM version therefore speeds up the computation by at least an order of magnitude.</p>
    </sec>
  </sec>
  <sec id="Sec16">
    <title>Discussion</title>
    <p id="Par56">Based on the principles of deep CNN, we have developed the DeepEM algorithm for single-particle recognition in cryo-EM. The method allows automated particle extraction from raw cryo-EM micrographs, thus improving the efficiency of cryo-EM data processing. In our current scheme, a new dataset containing particles of significantly different features may render the previously trained hyper-parameters suboptimal. Readers are directed to Table <xref rid="Tab1" ref-type="table">1</xref> as references for the hyper-parameter tuning for specific cases. Indeed, finding a set of fine-tuned hyper-parameters leading to optimized learning results on new datasets therefore demands additional user intervention in CNN training. In the above-described examples, we screened several combinations of hyper-parameters to empirically pinpoint an optimal setting. This procedure may be inefficient and can be laborious in certain cases. An automated method for the systemic tuning of hyper-parameters could be developed in the future to address this issue.</p>
    <p id="Par57">The execution of the DeepEM algorithm requires users to first label several hundreds of ‘good particles’ and ‘bad particles’ for CNN training purpose, which can be readily assembled from several micrographs. Further processing of these raw particle images is not needed. By contrast, in the traditional template-matching methods [<xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR36">36</xref>], users need to first obtain many high-quality class averages or an initial 3D model, which involves multiple steps of single-particle analysis significantly more laborious than the single step of manual particle labeling required by our DeepEM approach. If the template is based on a 3D model, it is usually not trivial to determine a high-quality initial model from new samples, which involves a complete procedure of the ab initio 3D structure determination at low resolution [<xref ref-type="bibr" rid="CR1">1</xref>]. If the template is based on a set of 2D class averages, users still have to first manually pick thousands of particles and then perform 2D image clustering to generate high-quality 2D classes. Moreover, the number of the reference images are often very limited and hardly include all kinds of orientations, potentially introducing orientation bias in particle picking through template matching. Thus, the preparation step of DeepEM is considerably easier than those of template-matching methods.</p>
    <p id="Par58">Although there are unlimited possibilities for the design of deep CNNs, we made some explorations that helped us understand the optimal use of CNNs for our single-particle recognition problem. First, we examined the noise tolerance of the algorithm with simulated datasets. When the SNR is decreased to 0.005, the DeepEM can still recognize particle images after proper training (Fig. <xref rid="Fig6" ref-type="fig">6</xref>). Second, we replaced the sigmoid activation function with a rectified linear unit (ReLU) function. Our results indicate that the ReLU function gives rise to a slightly inferior accuracy in particle recognition than the sigmoid function (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S7). Third, we attempted to design a six-layer CNN, but found that it failed to produce a better or equivalent performance (data not shown). Thus, it is likely that the eight-layer CNN we designed possesses the minimum depth suited to our problem. A deeper CNN might enable greater capacities in these tasks and awaits further investigation. Finally, from the experiments on the inflammasome dataset, we noticed that DeepEM is more effective for feature-rich data. It exhibits a reduced performance when tested on the side views as compared to the top views of the inflammasome (Fig. <xref rid="Fig4" ref-type="fig">4c</xref>), because the side views exhibit significantly less low-frequency features than the top views. Thus, the richness of low-frequency particle features is positively correlated with the achievable performance of CNNs.<fig id="Fig6"><label>Fig. 6</label><caption><p>Effect of the signal-to-noise ratio (SNR) on the precision-recall curves. Three synthetic datasets were generated through computational simulation of micrographs containing single-particle images with SNRs of 0.01, 0.008, 0.005, 0.003, 0.002 and 0.001. For each case, the CNN was first trained on the synthetic dataset of a given SNR and then used to examine the precision-recall relationship using another synthetic dataset with the same SNR. All synthetic datasets used the 70S ribosome as the single-particle model</p></caption><graphic xlink:href="12859_2017_1757_Fig6_HTML" id="MO6"/></fig>
</p>
    <p id="Par59">Our DeepEM algorithm framework exhibits several advantages. First, with sufficient training, DeepEM can select true particles without picking non-particles in a single, integrative step of particle recognition. In fact, it performs as well as a human worker. Similar performance was previously only made possible by combining several steps, encompassing automated particle picking, unsupervised classification and manual curation. Second, DeepEM features traits representative of other artificial intelligence (AI) or machine learning systems. The more it is trained or learned, the better it performs. We found that with iterative updating or optimization of the training dataset, the particle recognition performance of DeepEM can be further improved, which was not possible for conventional particle-recognition algorithms developed so far. Therefore, the performance of earlier algorithms was intricately bound by their mathematics and control parameters, and DeepEM overcomes these limitations.</p>
  </sec>
  <sec id="Sec17">
    <title>Conclusion</title>
    <p id="Par60">DeepEM, which is derived from deep CNNs, has proved to be a very useful tool for particle extraction from noisy micrographs in the absence of templates. This approach gives rise to improved “precision-recall” performance in particle recognition, and demonstrates a higher tolerance to much lower SNRs in the micrographs than was possible with older methods based on template-matching. Thus, it enables automated particle picking, selection and verification in an integrated fashion, with a quality comparable to that of a human worker. We expect that this development will broaden the applications of modern AI technology in expediting cryo-EM structure determination. Related AI technologies may be developed in the near future to address key challenges in this area, such as deep classification of highly heterogeneous cryo-EM datasets.</p>
  </sec>
</body>
<back>
  <app-group>
    <app id="App1">
      <sec id="Sec18">
        <title>Additional file</title>
        <p id="Par66">
          <media position="anchor" xlink:href="12859_2017_1757_MOESM1_ESM.pdf" id="MOESM1">
            <label>Additional file 1: Figure S1.</label>
            <caption>
              <p>The feature maps of the convolutional and subsampling layers from a typical particle image of KLH learned by our CNN. <bold>Figure S2.</bold> (a) and (b) show a comparison of the results obtained before and after additional selection using standard deviation of the KLH dataset, respectively. (c) and (d) show a comparison of the results obtained before and after additional selection using standard deviation of the 19S, respectively. <bold>Figure S3.</bold> (a) and (b) show a comparison of the results obtained before and after optimization of the training dataset, respectively. <bold>Figure S4.</bold> Comparison of DeepEM with TMACS and RELION using the KLH dataset as benchmark. The curves of TMACS [<xref ref-type="bibr" rid="CR16">16</xref>] and RELION [<xref ref-type="bibr" rid="CR36">36</xref>] were directly obtained from published data. <bold>Figure S5.</bold> Reference-free 2D classification of 19S proteasomes recognized by DeepEM. <bold>Figure S6.</bold> Results of the recognition of the side view of the 26S proteasome by DeepEM. <bold>Figure S7.</bold> A comparison of the results of different activation functions tested on the KLH dataset (PDF 477 kb)</p>
            </caption>
          </media>
        </p>
      </sec>
    </app>
  </app-group>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>AI</term>
        <def>
          <p id="Par4">Artificial intelligence</p>
        </def>
      </def-item>
      <def-item>
        <term>CNN</term>
        <def>
          <p id="Par5">Convolutional neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>Cryo-EM</term>
        <def>
          <p id="Par6">Cryo-electron microscopy</p>
        </def>
      </def-item>
      <def-item>
        <term>KLH</term>
        <def>
          <p id="Par7">Keyhole Limpet Hemocyanin</p>
        </def>
      </def-item>
      <def-item>
        <term>SNR</term>
        <def>
          <p id="Par8">Signal-to-noise ratio</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Electronic supplementary material</bold>
      </p>
      <p>The online version of this article (doi:10.1186/s12859-017-1757-y) contains supplementary material, which is available to authorized users.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors thank H. Liu, Y. Xu, M. Lin, D. Yu, Y. Wang, J. Wu and S. Chen for helpful discussions, as well as S. Zhang for assistance in the code adaptation for GPU-based acceleration. The computation was performed in part using the high-performance computational platform at the Peking-Tsinghua Center for Life Science at Peking University, Beijing, China.</p>
    <sec id="FPar1">
      <title>Funding</title>
      <p id="Par61">The cryo-EM experiments were performed in part at the Center for Nanoscale Systems at Harvard University, Cambridge, MA, USA, a member of the National Nanotechnology Coordinated Infrastructure Network (NNCI), which is supported by the National Science Foundation of the USA, under NSF award no. 1541959. This work was funded by a grant of the Thousand Talents Plan of China (Y.M.), by grants from the National Natural Science Foundation of China No. 11434001 and No. 91530321 (Y.M., Q.O.), and by the Intel Parallel Computing Center program (Y.M.).</p>
    </sec>
    <sec id="FPar2">
      <title>Availability of data and materials</title>
      <p id="Par62">Our software implementation in Matlab is freely available at <ext-link ext-link-type="uri" xlink:href="http://ipccsb.dfci.harvard.edu/deepem">http://ipccsb.dfci.harvard.edu/deepem</ext-link>. The experimental micrograph data are freely available at the Electron Microscopy Pilot Image Archive (<ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pdbe/emdb/empiar/)">https://www.ebi.ac.uk/pdbe/emdb/empiar/)</ext-link> under the accession codes EMPIAR-10063 and EMPIAR-10072.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>Conceived and designed the experiments: YZ QO YM. Performed the experiments: YZ. Analyzed the data: YZ YM. Contributed reagents/materials/analysis tools: QO YM. Wrote the manuscript: ZY YM. All authors have read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="COI-statement">
    <sec id="FPar3">
      <title>Ethics approval and consent to participate</title>
      <p id="Par63">Not applicable.</p>
    </sec>
    <sec id="FPar4">
      <title>Consent for publication</title>
      <p id="Par64">Not applicable.</p>
    </sec>
    <sec id="FPar5">
      <title>Competing interests</title>
      <p id="Par65">The authors declare that they have no competing interests.</p>
    </sec>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Frank</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>Three-dimensional electron microscopy of macromolecular assemblies</source>
        <year>2006</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Oxford U. Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <mixed-citation publication-type="other">Roseman, A M. Particle finding in electron micrographs using a fast local correlation algorithm. Ultramicroscopy 2003;94:225–236.</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Application of template matching technique to particle detection in electron micrographs</article-title>
        <source>J Struct Biol</source>
        <year>2004</year>
        <volume>145</volume>
        <fpage>29</fpage>
        <lpage>40</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jsb.2003.11.004</pub-id>
        <?supplied-pmid 15065671?>
        <pub-id pub-id-type="pmid">15065671</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Roseman, A M. FindEM- a fast, efficient program for automatic selection of particles from micrographs. J Struct Biol 2004<italic>;</italic>145:91–99.</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rath</surname>
            <given-names>BK</given-names>
          </name>
          <name>
            <surname>Frank</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Fast automatic particle picking from cryo-electron micrographs using a locally normalized cross-correlation function: a case study</article-title>
        <source>J Struct Biol</source>
        <year>2004</year>
        <volume>145</volume>
        <fpage>84</fpage>
        <lpage>90</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jsb.2003.11.015</pub-id>
        <?supplied-pmid 15065676?>
        <pub-id pub-id-type="pmid">15065676</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>JZ</given-names>
          </name>
          <name>
            <surname>Grigorieff</surname>
            <given-names>N</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SIGNATURE: a single-particle selection system for molecular electron microscopy</article-title>
        <source>J Struct Biol</source>
        <year>2007</year>
        <volume>157</volume>
        <fpage>168</fpage>
        <lpage>173</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jsb.2006.06.001</pub-id>
        <?supplied-pmid 16870473?>
        <pub-id pub-id-type="pmid">16870473</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Langlois</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automated particle picking for low-contrast macromolecules in cryo-electron microscopy</article-title>
        <source>J Struct Biol</source>
        <year>2014</year>
        <volume>186</volume>
        <fpage>1</fpage>
        <lpage>7</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jsb.2014.03.001</pub-id>
        <?supplied-pmid 24607413?>
        <pub-id pub-id-type="pmid">24607413</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Scheres</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>RELION: implementation of a Bayesian approach to cryo-EM structure determination</article-title>
        <source>J Struct Biol</source>
        <year>2015</year>
        <volume>180</volume>
        <fpage>519</fpage>
        <lpage>530</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jsb.2012.09.006</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Adiga</surname>
            <given-names>U</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Particle picking by segmentation: a comparative study with SPIDER-based manual particle picking</article-title>
        <source>J Struct Biol</source>
        <year>2005</year>
        <volume>152</volume>
        <fpage>211</fpage>
        <lpage>220</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jsb.2005.09.007</pub-id>
        <?supplied-pmid 16330229?>
        <pub-id pub-id-type="pmid">16330229</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Woolford</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SwarmPS: rapid, semi-automated single particle selection software</article-title>
        <source>J Struct Biol</source>
        <year>2007</year>
        <volume>157</volume>
        <fpage>174</fpage>
        <lpage>188</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jsb.2006.04.006</pub-id>
        <?supplied-pmid 16774837?>
        <pub-id pub-id-type="pmid">16774837</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Detecting circular and rectangular particles based on geometric feature detection in electron micrographs</article-title>
        <source>J Struct Biol</source>
        <year>2004</year>
        <volume>145</volume>
        <fpage>168</fpage>
        <lpage>180</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jsb.2003.10.027</pub-id>
        <?supplied-pmid 15065684?>
        <pub-id pub-id-type="pmid">15065684</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mallick</surname>
            <given-names>SP</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Detecting particles in cryo-EM micrographs using learned features</article-title>
        <source>J Struct Biol</source>
        <year>2004</year>
        <volume>145</volume>
        <fpage>52</fpage>
        <lpage>62</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jsb.2003.11.005</pub-id>
        <?supplied-pmid 15065673?>
        <pub-id pub-id-type="pmid">15065673</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sorzano</surname>
            <given-names>COS</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automatic particle selection from electron micrographs using machine learning techniques</article-title>
        <source>J Struct Biol</source>
        <year>2009</year>
        <volume>167</volume>
        <fpage>252</fpage>
        <lpage>260</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jsb.2009.06.011</pub-id>
        <?supplied-pmid 19555764?>
        <pub-id pub-id-type="pmid">19555764</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ogura</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sato</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>An automatic particle pickup method using a neural network applicable to low-contrast electron micrographs</article-title>
        <source>J Struct Biol</source>
        <year>2001</year>
        <volume>136</volume>
        <fpage>227</fpage>
        <lpage>238</lpage>
        <pub-id pub-id-type="doi">10.1006/jsbi.2002.4442</pub-id>
        <?supplied-pmid 12051902?>
        <pub-id pub-id-type="pmid">12051902</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ogura</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sato</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Automatic particle pickup method using a neural network has high accuracy by applying an initial weight derived from eigenimages: a new reference free method for single-particle analysis</article-title>
        <source>J Struct Biol</source>
        <year>2004</year>
        <volume>145</volume>
        <fpage>63</fpage>
        <lpage>75</lpage>
        <pub-id pub-id-type="doi">10.1016/S1047-8477(03)00139-4</pub-id>
        <?supplied-pmid 15065674?>
        <pub-id pub-id-type="pmid">15065674</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>TMaCS: a hybrid template matching and classification system for partially-automated particle selection</article-title>
        <source>J Struct Biol</source>
        <year>2013</year>
        <volume>181</volume>
        <fpage>234</fpage>
        <lpage>242</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jsb.2012.12.010</pub-id>
        <?supplied-pmid 23333657?>
        <pub-id pub-id-type="pmid">23333657</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lecun</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep Learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
        <?supplied-pmid 26017442?>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hinton</surname>
            <given-names>GE</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Reducing the dimensionality of data with neural networks</article-title>
        <source>Science</source>
        <year>2006</year>
        <volume>313</volume>
        <fpage>504</fpage>
        <lpage>507</lpage>
        <pub-id pub-id-type="doi">10.1126/science.1127647</pub-id>
        <?supplied-pmid 16873662?>
        <pub-id pub-id-type="pmid">16873662</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hinton</surname>
            <given-names>GE</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A fast learning algorithm for deep belief nets</article-title>
        <source>Neural Comput</source>
        <year>2006</year>
        <volume>18</volume>
        <fpage>1527</fpage>
        <lpage>1554</lpage>
        <pub-id pub-id-type="doi">10.1162/neco.2006.18.7.1527</pub-id>
        <?supplied-pmid 16764513?>
        <pub-id pub-id-type="pmid">16764513</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">LeCun Y, et al. Handwritten digit recognition with a back-propagation network. In Proc. Adv Neural Inf Proces Syst. 1990:396–404.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Medsker LR, et al. Recurrent neural networks design and application. CRC Press. 2001;</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Deng, L. et al. Deep learning: Methods and Applications. Foundations and Trends in Signal Processing. 2013;7, Nos. 3–4 197–387.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Waibel</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Phoneme recognition using time-delay neural network</article-title>
        <source>IEEE Trans Acoustics Speech Signal Process</source>
        <year>1989</year>
        <volume>37</volume>
        <fpage>328</fpage>
        <lpage>339</lpage>
        <pub-id pub-id-type="doi">10.1109/29.21701</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Semard D, et al. Best practices for convolutional neural network. In Proc Doc Anal Recognit. 2003:985–63.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lawrence</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Face recognition: a convolutional neural-network approach</article-title>
        <source>IEEE Trans Neural Netw</source>
        <year>1997</year>
        <volume>8</volume>
        <fpage>98</fpage>
        <lpage>113</lpage>
        <pub-id pub-id-type="doi">10.1109/72.554195</pub-id>
        <?supplied-pmid 18255614?>
        <pub-id pub-id-type="pmid">18255614</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Gao Z, et al. HEp-2 cell image classification with deep convolutional neural networks. IEEE J Biomed Health Inform. 2016;</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Krizhevsky A, et al. ImageNet classification with deep convolutional neural networks. Advances in Neural Information Processing System 25. 2012;1106–1114.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mallats</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Understanding deep convolutional networks</article-title>
        <source>Phil Trans R Soc A</source>
        <year>2016</year>
        <volume>374</volume>
        <fpage>20150203</fpage>
        <pub-id pub-id-type="doi">10.1098/rsta.2015.0203</pub-id>
        <pub-id pub-id-type="pmid">26953183</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Andrew Ng. et al. Feature extraction using convolution. <ext-link ext-link-type="uri" xlink:href="http://ufldl.stanford.edu/tutorial">http://ufldl.stanford.edu/tutorial/</ext-link> supervised/FeatureExtractionUsingConvolution/. 2015.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rumelhart</surname>
            <given-names>DE</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>learning representations by back-propagating errors</article-title>
        <source>Nature</source>
        <year>1986</year>
        <volume>323</volume>
        <fpage>533</fpage>
        <lpage>536</lpage>
        <pub-id pub-id-type="doi">10.1038/323533a0</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Palm, R. Prediction as a candidate for learning deep hierarchical models of data. 2012;IMM2012–06284.</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Powers</surname>
            <given-names>DMW</given-names>
          </name>
        </person-group>
        <article-title>Evaluation: from precision, recall and F-measure to ROC, informedness, markedness &amp; correlation</article-title>
        <source>J Mach Learn Technol</source>
        <year>2011</year>
        <volume>2</volume>
        <issue>1</issue>
        <fpage>37</fpage>
        <lpage>63</lpage>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tang</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>EMAN2: an extensible image processing suite for electron microscopy</article-title>
        <source>J Struct Biol</source>
        <year>2007</year>
        <volume>157</volume>
        <issue>1</issue>
        <fpage>38</fpage>
        <lpage>46</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jsb.2006.05.009</pub-id>
        <?supplied-pmid 16859925?>
        <pub-id pub-id-type="pmid">16859925</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Langlois</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A clarification of the terms used in comparing semi-automated particle selection algorithms in Cryo-EM</article-title>
        <source>J Struct Biol</source>
        <year>2011</year>
        <volume>175</volume>
        <fpage>348</fpage>
        <lpage>352</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jsb.2011.03.009</pub-id>
        <?supplied-pmid 21420497?>
        <pub-id pub-id-type="pmid">21420497</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automatic particle detection through efficient Hough transforms</article-title>
        <source>IEEE Trans Med Imaging</source>
        <year>2003</year>
        <volume>22</volume>
        <fpage>1053</fpage>
        <lpage>1062</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2003.816947</pub-id>
        <?supplied-pmid 12956261?>
        <pub-id pub-id-type="pmid">12956261</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Scheres</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Semi-automated selection of cryo-EM particles in RELION-1.3</article-title>
        <source>J. Struct. Biol</source>
        <year>2015</year>
        <volume>189</volume>
        <fpage>114</fpage>
        <lpage>122</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jsb.2014.11.010</pub-id>
        <?supplied-pmid 25486611?>
        <pub-id pub-id-type="pmid">25486611</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Cryo-EM structure of the activated NAIP2-NLRC4 inflammasome reveals nucleated polymerization</article-title>
        <source>Science</source>
        <year>2015</year>
        <volume>350</volume>
        <issue>6259</issue>
        <fpage>404</fpage>
        <lpage>409</lpage>
        <pub-id pub-id-type="doi">10.1126/science.aac5789</pub-id>
        <?supplied-pmid 26449474?>
        <pub-id pub-id-type="pmid">26449474</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Chen S, et al. Structural basis for dynamic regulation of the human 26S proteasome. Proc Natl Acad Sci U S A. 2016; doi:10.1073/pnas.1614614113.</mixed-citation>
    </ref>
  </ref-list>
</back>
