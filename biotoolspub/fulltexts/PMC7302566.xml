<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?covid-19-tdm?>
<?noissn?>
<front>
  <journal-meta>
    <journal-id journal-id-type="publisher-id">978-3-030-50426-7</journal-id>
    <journal-id journal-id-type="doi">10.1007/978-3-030-50426-7</journal-id>
    <journal-id journal-id-type="nlm-ta">Computational Science – ICCS 2020</journal-id>
    <journal-title-group>
      <journal-title>Computational Science – ICCS 2020</journal-title>
      <journal-subtitle>20th International Conference, Amsterdam, The Netherlands, June 3–5, 2020, Proceedings, Part V</journal-subtitle>
    </journal-title-group>
    <isbn publication-format="print">978-3-030-50425-0</isbn>
    <isbn publication-format="electronic">978-3-030-50426-7</isbn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7302566</article-id>
    <article-id pub-id-type="publisher-id">33</article-id>
    <article-id pub-id-type="doi">10.1007/978-3-030-50426-7_33</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>OpenGraphGym: A Parallel Reinforcement Learning Framework for Graph Optimization Problems</article-title>
    </title-group>
    <contrib-group content-type="book editors">
      <contrib contrib-type="editor">
        <name>
          <surname>Krzhizhanovskaya</surname>
          <given-names>Valeria V.</given-names>
        </name>
        <address>
          <email>V.Krzhizhanovskaya@uva.nl</email>
        </address>
        <xref ref-type="aff" rid="Aff8">8</xref>
      </contrib>
      <contrib contrib-type="editor">
        <name>
          <surname>Závodszky</surname>
          <given-names>Gábor</given-names>
        </name>
        <address>
          <email>G.Zavodszky@uva.nl</email>
        </address>
        <xref ref-type="aff" rid="Aff9">9</xref>
      </contrib>
      <contrib contrib-type="editor">
        <name>
          <surname>Lees</surname>
          <given-names>Michael H.</given-names>
        </name>
        <address>
          <email>m.h.lees@uva.nl</email>
        </address>
        <xref ref-type="aff" rid="Aff10">10</xref>
      </contrib>
      <contrib contrib-type="editor">
        <name>
          <surname>Dongarra</surname>
          <given-names>Jack J.</given-names>
        </name>
        <address>
          <email>dongarra@icl.utk.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff11">11</xref>
      </contrib>
      <contrib contrib-type="editor">
        <name>
          <surname>Sloot</surname>
          <given-names>Peter M. A.</given-names>
        </name>
        <address>
          <email>p.m.a.sloot@uva.nl</email>
        </address>
        <xref ref-type="aff" rid="Aff12">12</xref>
      </contrib>
      <contrib contrib-type="editor">
        <name>
          <surname>Brissos</surname>
          <given-names>Sérgio</given-names>
        </name>
        <address>
          <email>sergio.brissos@intellegibilis.com</email>
        </address>
        <xref ref-type="aff" rid="Aff13">13</xref>
      </contrib>
      <contrib contrib-type="editor">
        <name>
          <surname>Teixeira</surname>
          <given-names>João</given-names>
        </name>
        <address>
          <email>joao.teixeira@intellegibilis.com</email>
        </address>
        <xref ref-type="aff" rid="Aff14">14</xref>
      </contrib>
      <aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="GRID">grid.7177.6</institution-id><institution-id institution-id-type="ISNI">0000000084992262</institution-id><institution>University of Amsterdam, </institution></institution-wrap>Amsterdam, The Netherlands </aff>
      <aff id="Aff9"><label>9</label><institution-wrap><institution-id institution-id-type="GRID">grid.7177.6</institution-id><institution-id institution-id-type="ISNI">0000000084992262</institution-id><institution>University of Amsterdam, </institution></institution-wrap>Amsterdam, The Netherlands </aff>
      <aff id="Aff10"><label>10</label><institution-wrap><institution-id institution-id-type="GRID">grid.7177.6</institution-id><institution-id institution-id-type="ISNI">0000000084992262</institution-id><institution>University of Amsterdam, </institution></institution-wrap>Amsterdam, The Netherlands </aff>
      <aff id="Aff11"><label>11</label><institution-wrap><institution-id institution-id-type="GRID">grid.411461.7</institution-id><institution-id institution-id-type="ISNI">0000 0001 2315 1184</institution-id><institution>University of Tennesee, </institution></institution-wrap>Knoxville, TN USA </aff>
      <aff id="Aff12"><label>12</label><institution-wrap><institution-id institution-id-type="GRID">grid.7177.6</institution-id><institution-id institution-id-type="ISNI">0000000084992262</institution-id><institution>University of Amsterdam, </institution></institution-wrap>Amsterdam, The Netherlands </aff>
      <aff id="Aff13"><label>13</label>Intellegibilis, Setúbal, Portugal </aff>
      <aff id="Aff14"><label>14</label>Intellegibilis, Setúbal, Portugal </aff>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2791-0031</contrib-id>
        <name>
          <surname>Zheng</surname>
          <given-names>Weijian</given-names>
        </name>
        <address>
          <email>zheng273@purdue.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff15">15</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6806-5108</contrib-id>
        <name>
          <surname>Wang</surname>
          <given-names>Dali</given-names>
        </name>
        <address>
          <email>wangd@ornl.gov</email>
        </address>
        <xref ref-type="aff" rid="Aff16">16</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7382-093X</contrib-id>
        <name>
          <surname>Song</surname>
          <given-names>Fengguang</given-names>
        </name>
        <address>
          <email>fgsong@iupui.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff15">15</xref>
      </contrib>
      <aff id="Aff15"><label>15</label><institution-wrap><institution-id institution-id-type="GRID">grid.257413.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2287 3919</institution-id><institution>Indiana University-Purdue University, </institution></institution-wrap>Indianapolis, IN 46202 USA </aff>
      <aff id="Aff16"><label>16</label><institution-wrap><institution-id institution-id-type="GRID">grid.135519.a</institution-id><institution-id institution-id-type="ISNI">0000 0004 0446 2659</institution-id><institution>Oak Ridge National Laboratory, </institution></institution-wrap>Oak Ridge, TN 37831 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>25</day>
      <month>05</month>
      <year>2020</year>
    </pub-date>
    <volume>12141</volume>
    <fpage>439</fpage>
    <lpage>452</lpage>
    <permissions>
      <copyright-statement>© This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply 2020</copyright-statement>
      <license>
        <license-p>This article is made available via the PMC Open Access Subset for unrestricted research re-use and secondary analysis in any form or by any means with acknowledgement of the original source. These permissions are granted for the duration of the World Health Organization (WHO) declaration of COVID-19 as a global pandemic.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">This paper presents an open-source, parallel AI environment (named <italic>OpenGraphGym</italic>) to facilitate the application of reinforcement learning (RL) algorithms to address combinatorial graph optimization problems. This environment incorporates a basic deep reinforcement learning method, and several graph embeddings to capture graph features, it also allows users to rapidly plug in and test new RL algorithms and graph embeddings for graph optimization problems. This new open-source RL framework is targeted at achieving both high performance and high quality of the computed graph solutions. This RL framework forms the foundation of several ongoing research directions, including 1) benchmark works on different RL algorithms and embedding methods for classic graph problems; 2) advanced parallel strategies for extreme-scale graph computations, as well as 3) performance evaluation on real-world graph solutions.</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Reinforcement learning</kwd>
      <kwd>Graph optimization problems</kwd>
      <kwd>Distributed GPU computing</kwd>
      <kwd>Open AI software environment</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© Springer Nature Switzerland AG 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">Solving graph optimization problems effectively is critical in many important domains, including social networks, telecommunications, marketing, security, transportation, power grid, bioinformatics, traffic planning, scheduling, and emergency preparedness. However, many of the graph optimization problems are in the class of NP-hard problems, and require exponential time algorithms to search for optimal solutions. Due to the exact graph algorithms’ exponential time complexity, practical approaches most often use either <italic>approximation algorithms</italic> or <italic>heuristic algorithms</italic> to tackle big graphs. The approximation algorithms are of polynomial time (if they do exist), but in theory can be several times worse than the optimal solutions. The heuristic algorithms are fast, but do not have the same guaranteed solution quality as that of approximation algorithms. Also, heuristic algorithms typically require experts’ knowledge, insights, and repeated redesigns to create efficient heuristics.</p>
    <p id="Par3">Instead of devising different heuristics for different graph problems and distinct graph datasets, we aim to utilize machine learning techniques to “learn” effective heuristics automatically. Since 2016, a few researchers have started to design reinforcement learning and deep learning methods to solve combinatorial optimization problems [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR11">11</xref>–<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR22">22</xref>]. The rational behind it is that graphs from the same application domain or similar types are not totally different from each other; they may have similar structures and are often solved repeatedly. Hence, it can be beneficial to use machine learning to generalize the methods or heuristics to find near optimal solutions.</p>
    <p id="Par4">To investigate different deep reinforcement learning methods, and design new domain-specific graph embeddings to capture graph features, we design and implement an open source AI environment to allow users to rapidly plug in and test new RL algorithms and graph embeddings for graph optimization problems. The new open source RL framework, named <italic>OpenGraphGym</italic>, is targeted at achieving both high performance and high quality of the computed graph solutions. Our work has the following contributions. 1) We design and create an extensible framework for generic graph problems. A suit of NP-hard graph problems and graph embedding methods can be added into our framework conveniently. Our framework can also be used to benchmark several RL algorithms for graph optimization problems. 2) Our distributed RL framework can utilize multiple GPUs. 3) Case study shows that our framework can help to provide better solutions for Minimum Vertex Cover problems (a classic NP-hard graph problems).</p>
    <p id="Par5">In the remainder of the paper, we will first introduce the related work, then describe how to convert (or map) conventional graph problems to RL problems in Sect. <xref rid="Sec3" ref-type="sec">3</xref>. In Sect. <xref rid="Sec7" ref-type="sec">4</xref>, we will present the <italic>OpenGraphGym</italic> framework design and implementation details. A case study of using <italic>OpenGraphGym</italic> to solve the Minimum Vertex Cover problem with different types of graphs will be shown in Sect. <xref rid="Sec11" ref-type="sec">5</xref>. Finally, Sect. <xref rid="Sec16" ref-type="sec">6</xref> will present our conclusions and future work.</p>
  </sec>
  <sec id="Sec2">
    <title>Related Work</title>
    <p id="Par6">Reinforcement learning (RL) was commonly used in the field of playing games [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR20">20</xref>, <xref ref-type="bibr" rid="CR21">21</xref>]. Recently, researchers started to investigate if RL can be used to help solve NP-hard graph problems. Based on the observation that knowledge learned from some problem instances can be applied to a similar type of problem instances, Dai et al. [<xref ref-type="bibr" rid="CR11">11</xref>] created an end-to-end RL model that combines graph embedding and the objective <italic>Q</italic> function to tackle NP-hard graph problems. In their work, the solution is built by incrementally adding vertices. They studied the Minimum Vertex Cover, the Maximum Cut, and the Traveling Salesman problems by applying the Q-learning algorithm. Meanwhile, their results also proved that the strategy learned by the smaller size of graphs could be applied to the larger size of graphs. Bello et al. [<xref ref-type="bibr" rid="CR4">4</xref>] also applied RL to graph optimization problems. However, they focused on euclidean Travelling Salesman Problems (TSP), and their methods cannot be applied to other graph problems conveniently.</p>
    <p id="Par7">Besides RL methods, researchers have also employed supervised machine learning methods to solve graph problems. Li et al. [<xref ref-type="bibr" rid="CR12">12</xref>, <xref ref-type="bibr" rid="CR14">14</xref>] applied the Graph Convolution Network (GCN) to find multiple solutions in one step, then used a tree search model to select the best solution. The labeled SATLIB dataset [<xref ref-type="bibr" rid="CR10">10</xref>] was used to train their GCN model. Another similar work was done by Mittal et al., who also used GCN to generate multiple solutions [<xref ref-type="bibr" rid="CR16">16</xref>]. However, instead of using tree search, they took advantage of RL to select the best solution. In addition, Vinyals et al. applied a neural network architecture called <italic>pointer network</italic> to address graph combinatorial optimization problems [<xref ref-type="bibr" rid="CR22">22</xref>]. The following study by Kool et al. modified the pointer network by introducing an attention-based encoder-decoder model and applied it to the TSP problem [<xref ref-type="bibr" rid="CR13">13</xref>].</p>
    <p id="Par8">Compared to the existing work, our project targets creating an open AI framework that is optimized for solving big graph optimization problems. The major differences are as follows. First, our <italic>OpenGraphGym</italic> framework is an open environment, in which additional graph embedding methods, different RL algorithms, and new graph problems can be plugged in and tested rapidly. Second, <italic>OpenGraphGym</italic> is designed to be a high performance computing solution that can support distributed GPU systems. By contrast, the existing work is either constrained to a very small subset of graph problems, or works on shared-memory systems only. Third, the end-to-end learning approach realized in <italic>OpenGraphGym</italic> follows the line of research done by Dai et al. [<xref ref-type="bibr" rid="CR11">11</xref>], but we extend it with new parallel GPU computing algorithms and a distinct software design and implementation using Tensorflow [<xref ref-type="bibr" rid="CR1">1</xref>] and Horovod [<xref ref-type="bibr" rid="CR19">19</xref>].</p>
  </sec>
  <sec id="Sec3">
    <title>Methodology</title>
    <p id="Par9">In this section, we describe how to apply RL to solve graph optimization problems, which involves processing input graphs, reducing graph problems to RL problems, and executing RL training and testing.</p>
    <sec id="Sec4">
      <title>Graph Processing for Reinforcement Learning</title>
      <p id="Par10">In conventional RL applications such as Atari games [<xref ref-type="bibr" rid="CR17">17</xref>], input data are typically represented as matrices. For instance, pixel images may be taken as input to train deep neural network (DNN) models.</p>
      <p id="Par11">To handle graphs, an intuitive way is to feed a graph’s corresponding adjacency matrix to DNN models. It is feasible. However, there are two major issues: 1) It requires a lot of memory space to train a DNN model due to graphs’ large dimensions; 2) The successfully trained model only works for the graphs that have the same number of vertices as that of the training graph. To solve the issues, we use the technique of graph embedding, which is currently an active research area [<xref ref-type="bibr" rid="CR5">5</xref>]. In brief, graph embedding can take a graph or vertex as input, then produce a <italic>p</italic> dimension vector that represents the useful information of the graph or vertex. Here, the dimension of <italic>p</italic> is predefined by users.</p>
      <p id="Par12">In our current implementation, we support two graph embedding of <italic>structure2vec</italic> [<xref ref-type="bibr" rid="CR6">6</xref>] and <italic>node2vec</italic> [<xref ref-type="bibr" rid="CR8">8</xref>]. Other graph embedding methods can be added to <italic>OpenGraphGym</italic> by extending certain classes. In Sect. <xref rid="Sec10" ref-type="sec">4.3</xref>, we explain how to add a new graph embedding method to <italic>OpenGraphGym</italic>.</p>
    </sec>
    <sec id="Sec5">
      <title>Reinforcement Learning Formulation</title>
      <p id="Par13">In reinforcement learning, an agent and an environment interact with each other repeatedly in every <italic>step</italic>. For each <italic>step</italic>, the agent will take an <italic>action</italic>, then the environment will provide the agent with a <italic>reward</italic> and the old and new <italic>states</italic>. Eventually, the RL process will stop at a special “finished” state, which is called the <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$terminal\ state$$\end{document}</tex-math><inline-graphic xlink:href="500805_1_En_33_Chapter_IEq1.gif"/></alternatives></inline-formula>. The above sequence of <italic>steps</italic> until <italic>terminal</italic>
<italic>state</italic> is called an <italic>episode</italic>.<fig id="Fig1"><label>Fig. 1.</label><caption><p>The OpenGraphGym framework architecture.</p></caption><graphic xlink:href="500805_1_En_33_Fig1_HTML" id="MO1"/></fig>
</p>
      <p id="Par14">Figure <xref rid="Fig1" ref-type="fig">1</xref> shows the architecture of our <italic>OpenGraphGym</italic> framework. In the framework, the <italic>Graph</italic>
<italic>Learning</italic>
<italic>Agent</italic> takes an <italic>action</italic> by selecting and adding the “best” node to the graph problem’s partial solution. Then, the <italic>Graph</italic>
<italic>Environment</italic> returns the <italic>reward</italic>. The <italic>reward</italic> is used to justify the quality of a solution. It varies for different graph problems. More details of the framework will be introduced in Sect. <xref rid="Sec8" ref-type="sec">4.1</xref>.</p>
      <p id="Par15">For different types of graph problems, there are different formulations for the graph problem’s RL algorithm. For instance, an RL algorithm for a distinct graph problem may have a new representation of <italic>state</italic>, a problem-specific <italic>action</italic>, and a redefined <italic>reward</italic>.</p>
      <p id="Par16">As an example, Table <xref rid="Tab1" ref-type="table">1</xref> shows two graph problems’ <italic>states</italic>, <italic>actions</italic>, <italic>termination</italic>
<italic>states</italic>, and <italic>rewards</italic>. The graph problems of the Minimum Vertex Cover (MVC) and the Maximum Cut (MAX) are defined briefly as follows:<list list-type="bullet"><list-item><p id="Par17"><bold>Minimum Vertex Cover (MVC):</bold> Given an undirected graph, find the smallest subset of nodes to cover all the edges.</p></list-item><list-item><p id="Par18"><bold>Maximum Cut (MAX):</bold> Given an undirected graph, a subset of nodes <italic>S</italic>, assume the cut set is the set of edges that only has one end in <italic>S</italic>, find <italic>S</italic> with the largest weight of the cut set.</p></list-item></list>
</p>
      <p id="Par19">In Table <xref rid="Tab1" ref-type="table">1</xref>, we can observe that the <italic>state</italic> and the <italic>action</italic> for MVC and MAX are same. However, the <italic>reward</italic> and the <italic>termination</italic> varies. As to MVC, the <italic>reward</italic> and the <italic>termination</italic> are related to the number of edges. As to MAX, the <italic>reward</italic> and <italic>termination</italic> are related to the cut set weight. Note that although we pick two NP-hard graph problems, our framework can be extended to solve more graph optimization problems.<table-wrap id="Tab1"><label>Table 1.</label><caption><p>Examples of NP-hard graph problems that are defined in RL algorithms</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Problem</th><th align="left">State</th><th align="left">Action</th><th align="left">Reward</th><th align="left">Termination</th></tr></thead><tbody><tr><td align="left">MVC</td><td align="left">Subset of nodes selected as the partial solution</td><td align="left">Add a new node to the partial solution</td><td align="left">Number of nodes used to cover all edges at the end of the episode</td><td align="left">All edges are covered</td></tr><tr><td align="left">MAX</td><td align="left">Subset of nodes selected as the partial solution</td><td align="left">Add a new node to the partial solution</td><td align="left">Cut set weight at the end of the episode</td><td align="left">Cut set weight cannot be improved</td></tr></tbody></table></table-wrap>
</p>
    </sec>
    <sec id="Sec6">
      <title>Graph-RL Training and Testing Algorithms</title>
      <p id="Par20">
        <graphic position="anchor" xlink:href="500805_1_En_33_Figa_HTML" id="MO2"/>
      </p>
      <p id="Par21">In the previous Sect. <xref rid="Sec5" ref-type="sec">3.2</xref>, we have introduced how to formulate a graph RL algorithm. The next step is to train and test the model. In this section, we will summarize the algorithm of training and testing.</p>
      <p id="Par22">As shown in Algorithm 1, we will first initialize the experience replay memory buffer and the objective Q function (lines 2–4). Then, for each episode, we will select a random graph from the distribution <italic>D</italic> (line 6). One distribution of graphs include graphs generated using the same model and parameters. Next, we will initialize three sets of vertices (lines 8 and 9). Two of them (<inline-formula id="IEq2"><alternatives><tex-math id="M2">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{new}^{}$$\end{document}</tex-math><inline-graphic xlink:href="500805_1_En_33_Chapter_IEq2.gif"/></alternatives></inline-formula> and <inline-formula id="IEq3"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{old}^{}$$\end{document}</tex-math><inline-graphic xlink:href="500805_1_En_33_Chapter_IEq3.gif"/></alternatives></inline-formula>) are for the solution. <inline-formula id="IEq4"><alternatives><tex-math id="M4">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{new}^{}$$\end{document}</tex-math><inline-graphic xlink:href="500805_1_En_33_Chapter_IEq4.gif"/></alternatives></inline-formula> is a set of vertices that includes all the nodes which have been selected as the solution in the current step. <inline-formula id="IEq5"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{old}^{}$$\end{document}</tex-math><inline-graphic xlink:href="500805_1_En_33_Chapter_IEq5.gif"/></alternatives></inline-formula> is a set of vertices that includes all the nodes which have been selected as the solution in the previous step. Another one (<italic>C</italic>) is for the candidate nodes. A temporary replay buffer is also initialized (line 10). At each step, the agent will either randomly or according to a policy to select a node <inline-formula id="IEq6"><alternatives><tex-math id="M6">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$v_{t}^{}$$\end{document}</tex-math><inline-graphic xlink:href="500805_1_En_33_Chapter_IEq6.gif"/></alternatives></inline-formula> from the candidate nodes set <italic>C</italic> (line 12). Then, we will update the solution sets <inline-formula id="IEq7"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{old}^{}$$\end{document}</tex-math><inline-graphic xlink:href="500805_1_En_33_Chapter_IEq7.gif"/></alternatives></inline-formula> and <inline-formula id="IEq8"><alternatives><tex-math id="M8">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{new}^{}$$\end{document}</tex-math><inline-graphic xlink:href="500805_1_En_33_Chapter_IEq8.gif"/></alternatives></inline-formula> (lines 13–16). Meanwhile, <inline-formula id="IEq9"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$v_{t}^{}$$\end{document}</tex-math><inline-graphic xlink:href="500805_1_En_33_Chapter_IEq9.gif"/></alternatives></inline-formula> will also be removed from the candidate nodes set <italic>C</italic>. <inline-formula id="IEq10"><alternatives><tex-math id="M10">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{old}^{}$$\end{document}</tex-math><inline-graphic xlink:href="500805_1_En_33_Chapter_IEq10.gif"/></alternatives></inline-formula>, <inline-formula id="IEq11"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{new}^{}$$\end{document}</tex-math><inline-graphic xlink:href="500805_1_En_33_Chapter_IEq11.gif"/></alternatives></inline-formula> and the selected node <inline-formula id="IEq12"><alternatives><tex-math id="M12">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$v_{t}^{}$$\end{document}</tex-math><inline-graphic xlink:href="500805_1_En_33_Chapter_IEq12.gif"/></alternatives></inline-formula> will be combined and be added to the temporary replay buffer <inline-formula id="IEq13"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R\_temp$$\end{document}</tex-math><inline-graphic xlink:href="500805_1_En_33_Chapter_IEq13.gif"/></alternatives></inline-formula>. Tuples in the temporary buffer will be pushed to the replay buffer <italic>R</italic> when we finish one episode (lines 19–20). At each step, we will also update the objective function <italic>Q</italic> by sampling a batch of tuples from the replay buffer <italic>R</italic>.</p>
      <p id="Par23">After we have trained an RL agent successfully, we can utilize the trained agent to find solutions to a set of new unseen graphs afterwards. Such an algorithm is called an RL Testing algorithm. The RL Testing algorithm is nearly the same as the training algorithm Algorithm 1 except for two differences: 1) Only the best candidate node will be selected every step (in line 12), and 2) the RL agent will not update the objective function (in line 17).</p>
    </sec>
  </sec>
  <sec id="Sec7">
    <title>Design and Implementation of OpenGraphGym</title>
    <p id="Par24">This section presents 1) the main components of our framework, 2) how we design the framework to support parallel computing on multiple GPUs, and 3) how to extend the framework to support new graph optimization problems and graph embedding methods. Our code can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/zwj3652011/OpenGraphGym.git">https://github.com/zwj3652011/OpenGraphGym.git</ext-link>.</p>
    <sec id="Sec8">
      <title>Main Software Components</title>
      <p id="Par25">As shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, <italic>OpenGraphGym</italic> has five main components, which are described as follows:<list list-type="bullet"><list-item><p id="Par26"><bold>Graph Learning Agent:</bold> It is the agent that is responsible for reinforcement learning for graph problems. It constantly receives input from three other components: the <italic>Neural</italic>
<italic>Network</italic>
<italic>Model</italic>, the <italic>Agent</italic>
<italic>Helper</italic>
<italic>Functions</italic>, and the <italic>Graph</italic>
<italic>Environment</italic>. The <italic>Neural</italic>
<italic>Network</italic>
<italic>Model</italic> provides DNN model output (e.g., Q values), the <italic>Agent</italic>
<italic>Helper</italic>
<italic>Functions</italic> provides actions, and the <italic>Graph</italic>
<italic>Environment</italic> provides states and rewards. On the other hand, the <italic>Graph Learning Agent</italic> also sends information to the three components constantly.</p></list-item><list-item><p id="Par27"><bold>Neural Network Model:</bold> It defines the graph embedding function and the RL agent’s DNN model. During RL training, the <italic>Neural Network Model</italic> takes a graph state as input and produces a <italic>Q value</italic>. The <italic>Q</italic> value will be sent to the <italic>Agent</italic> for making decisions. In the current implementation of <italic>OpenGraphGym</italic>, we use the deep network Q-learning (DQN) method. Our next work will add the support of other RL methods such as A2C and A3C.</p></list-item><list-item><p id="Par28"><bold>Agent Helper Functions:</bold> It is a set of functions that are used by the RL agent to compute its appropriate <italic>action</italic>. By receiving the parameters of <italic>states</italic>, <italic>models</italic>
<italic>outputs</italic> and <italic>graph</italic>
<italic>problem</italic>
<italic>types</italic>, the helper functions computes the <italic>action</italic> needs to be taken by checking the <italic>model</italic>
<italic>output</italic> and the <italic>graph</italic>
<italic>problem</italic>
<italic>type</italic>. Please note that the <italic>Agent</italic>
<italic>Helper</italic>
<italic>Functions</italic> varies for different graph problems. In our framework, we include the <italic>Agent</italic>
<italic>Helper</italic>
<italic>Functions</italic> for some graph problems.</p></list-item><list-item><p id="Par29"><bold>Graph Environment:</bold> The <italic>Graph</italic>
<italic>Environment</italic> is the interface between the <italic>Graph</italic> and the <italic>Graph</italic>
<italic>Learning</italic>
<italic>Agent</italic>. <italic>Action</italic>, <italic>reward</italic> and <italic>state</italic> will be transferred through it. For example, when the <italic>action</italic> is received from the <italic>Graph</italic>
<italic>Learning</italic>
<italic>Agent</italic>, the <italic>Graph</italic>
<italic>Environment</italic> will call the function <italic>step</italic> to send the <italic>action</italic> to the component <italic>Graph</italic>. Then, it will receive the <italic>state</italic> and the <italic>reward</italic> from the component <italic>Graph</italic>. Finally, it will push the <italic>state</italic> and the <italic>reward</italic> to the <italic>Graph</italic>
<italic>Learning</italic>
<italic>Agent</italic>.</p></list-item><list-item><p id="Par30"><bold>Graph:</bold> It is a graph object implemented by our framework. Each graph object stores a set of graph-related information (e.g., number of nodes, number of vertices). Currently, our framework supports two types of graph objects. In the <italic>Basic</italic>
<italic>Graph</italic>, we store the node lists, edge lists, number of nodes, and other basic information of a graph. Another one is defined by the networkX graph library [<xref ref-type="bibr" rid="CR9">9</xref>]. NetworkX will read graph objects from edgelist files. The <italic>Basic</italic>
<italic>Graph</italic> object is more flexible and can be extended by the user. If users cannot find a metric of graphs in networkX or other graph libraries, they can define and add their metrics to the <italic>Basic</italic>
<italic>Graph</italic> object.</p></list-item></list>
</p>
      <p id="Par31">In general, the above components can be classified into two categories. The first category is designed to support for the agent part in RL, which includes the first three components. The <italic>Graph</italic>
<italic>Learning</italic>
<italic>Agent</italic> works as the interface between them. The second category is designed to support the environment part, for which the <italic>Graph</italic>
<italic>Environment</italic> is the interface of them.</p>
      <p id="Par32">Furthermore, our framework is designed to be modular. By modifying a couple of components, it can be extended to support new graph embedding methods, RL algorithms, and graph optimization problems. For example, if a user desires to study another graph problem, the user needs to modify the <italic>Graph</italic> component to do it. In Sect. <xref rid="Sec10" ref-type="sec">4.3</xref>, we will show more details about it.</p>
    </sec>
    <sec id="Sec9">
      <title>Parallel Implementation Using Multiple GPUs</title>
      <p id="Par33">The <italic>OpenGraphGym</italic> framework is able to support RL training on multiple GPUs. The following content describes how we distribute the workload and compute the graph RL in parallel among multiple GPUs.</p>
      <p id="Par34">
        <list list-type="bullet">
          <list-item>
            <p id="Par35"><bold>Parallel Setup and Initialization:</bold> Our framework will launch <italic>n</italic> processes given a number of <italic>n</italic> available GPUs. One CPU and one GPU will be mapped to one process. Inside each process, we create an instance of <italic>Graph</italic>
<italic>Learning</italic>
<italic>Agent</italic> and an instance of <italic>Graph</italic>
<italic>Environment</italic>. Each <italic>Graph</italic>
<italic>Learning</italic>
<italic>Agent</italic> has its own copy of the global DNN model (i.e., a single model but duplicated multiple times on multiple GPUs), as well as a private RL replay buffer. At the beginning of the parallel execution, we use the distributed deep learning framework Horovod [<xref ref-type="bibr" rid="CR19">19</xref>] to ensure each agent’s DNN model will be initialized with the same weights.</p>
          </list-item>
          <list-item>
            <p id="Par36"><bold>Exploring Graphs in Parallel:</bold> We use an asynchronous algorithm to let each process explore training on different graphs in parallel. At the start of each <italic>episode</italic>, every process will select a random graph from all the training graphs based on their unique random seeds. At the end of the <italic>episode</italic>, each process will then push its experience tuples to its own replay buffer. Note that all the training graphs are generated automatically by our framework.</p>
          </list-item>
          <list-item>
            <p id="Par37"><bold>Computing Gradients:</bold> In the previous step, each process has started to explore graphs asynchronously. Then, at the end of every step, as shown in Algorithm 1, line 16, each process needs to sample some tuples from their replay memory buffer and compute the gradients. Assume the batch size is <italic>b</italic> and we have <italic>n</italic> processes, each process will sample <italic>b</italic>/<italic>n</italic> tuples and compute the averaged gradients of <italic>b</italic>/<italic>n</italic> tuples. Thus, each process will have one gradient. Finally, all processes’ gradients will be averaged. We use the distributed deep learning framework Horovod [<xref ref-type="bibr" rid="CR19">19</xref>] to finish the gradient computing. Horovod will accomplish two major tasks in this step: 1) add a barrier to wait for all processes to finish the gradient computing, and 2) average all processes’ gradients and broadcasts it to them.</p>
          </list-item>
          <list-item>
            <p id="Par38"><bold>Updating Model:</bold> In the previous step, all processes have received the averaged gradients. Then, each process needs to update their DNN model using the new gradient, as shown in Algorithm 1, line 17. Please note that all processes’ DNN models are still the same after updating for the following two reasons: 1) DNN models are initialized to be the same, and 2) the gradients used to update the DNN models are identical for all processes.</p>
          </list-item>
        </list>
      </p>
      <p id="Par39">Please note that the above operations are not executed in the same frequency. As shown in the Algorithm 1, the agent’s DNN model will be updated in each <italic>step</italic>. Hence, the operations of <bold>Computing Gradients</bold> and <bold>Updating Model</bold> will be called every <italic>step</italic>. Moreover, in each <italic>episode</italic>, a new graph will be explored. Therefore, the operation of <bold>Exploring Graphs</bold> will be called every episode.</p>
    </sec>
    <sec id="Sec10">
      <title>Framework Extensibility</title>
      <p id="Par40">In this section, we demonstrate the extensibility of our framework from two perspectives: 1) how to support other graph optimization problems, and 2) how to add new graph embedding methods.</p>
      <p id="Par41">Now, we use an example to show to extend <italic>OpenGraphGym</italic> to support other graph optimization problems. In the case of adding the Max Cut problem (MAX), we need to modify the <italic>Graph</italic> component as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. More specifically, two functions will be modified, which are: 1) the environment constructor (function <italic>__init__</italic> in graph.py), and 2) the step function (function <italic>update_state</italic> in graph.py). In the environment constructor, we need to add a new local variable to represent the cut set weight inside the environment initialization function. In the step function, we then calculate the MAX-specific cut set weight to decide the termination state and the corresponding reward. Table <xref rid="Tab1" ref-type="table">1</xref> shows the definitions of the reward and termination state for the MAX graph problem.</p>
      <p id="Par42">To demonstrate how to add the new graph embedding method, we will use an example of adding the node2vec embedding method. Node2vec is a graph embedding method aiming at preserving each node’s neighborhood information [<xref ref-type="bibr" rid="CR8">8</xref>]. We use the open-source node2vec library Node2Vec in our framework [<xref ref-type="bibr" rid="CR18">18</xref>]. Two functions needs to be modified in the <italic>Graph</italic> component are 1) the environment constructor (function <italic>__init__</italic> in graph.py), and 2) the step function (function <italic>update_state</italic> in graph.py). As to the environment constructor, we need to set up the node2vec model using the APIs provided by the Node2Vec library. As to the step function, we need to reset and update the node2vec model when the graph is modified.</p>
    </sec>
  </sec>
  <sec id="Sec11">
    <title>A Case Study on Minimum Vertex Cover (MVC)</title>
    <p id="Par43">To evaluate the performance and accuracy of the <italic>OpenGraphGym</italic> framework, we compare the solution found by our RL framework with that of other classical solvers on the MVC problem. In addition, we did experiments to show the improved convergence rate by utilizing multiple GPUs.</p>
    <sec id="Sec12">
      <title>Experimental Setting</title>
      <p id="Par44">The software and hardware configurations for all our experiments are provided as follows.</p>
      <p id="Par45"><bold>Software:</bold> To implement <italic>OpenGraphGym</italic>, we use Horovod version 0.16.4 to compute DNN gradients in a distributed setting, as mentioned in Sect. <xref rid="Sec9" ref-type="sec">4.2</xref>. Horovod can help with our distributed training by doing the following three tasks: 1) initialize each agent’s DNN models to be the same, 2) add a barrier for multiple processes gradient computing, and 3) average the gradients from all processes and broadcast it to them. We also use Tensorflow version 1.12.0 [<xref ref-type="bibr" rid="CR1">1</xref>], graph library networkX version 2.4 [<xref ref-type="bibr" rid="CR9">9</xref>], and the HPC environment toolkit Docker version 18.09.2 [<xref ref-type="bibr" rid="CR15">15</xref>]. Tensorflow is responsible for the agent’s neural network training and inference. As for networkX, we use it to generate, manipulate, and evaluate different graphs. As to Docker, we create an environment using Docker and install all required libraries of our framework. Then, we can deploy the environment conveniently to a new HPC system. Docker helps us to manage the software and libraries used in our framework.</p>
      <p id="Par46"><bold>Graph Datasets Used:</bold> Our graph datasets contain two types of graphs, which are generated by two different distributions or random graph generation models. In the following content, we will present how graphs are generated using them. We will also present the parameter values we set for each model.</p>
      <p id="Par47"><list list-type="bullet"><list-item><p id="Par48"><italic>Erdős-Rényi (ER) Graphs:</italic> The Erdős-Rényi model will generate a random graph with the graph size <italic>m</italic> and the edge possibility <italic>r</italic> [<xref ref-type="bibr" rid="CR7">7</xref>]. We use the function <italic>erdos_renyi_graph</italic> in networkX to generate different sizes of ER graphs. The edge probability is set to 0.15, which means every possible edge has the possibility of 0.15 to exist.</p></list-item><list-item><p id="Par49"><italic>Barabási-Albert (BA) Graphs:</italic> The Barabási-Albert model generate the random graph based on the graph size <italic>m</italic> and the edge density <italic>d</italic> [<xref ref-type="bibr" rid="CR2">2</xref>]. Edge density <italic>d</italic> is equal to the number of edges from a new node to the existing nodes. We use the function <italic>barabasi_albert_graph</italic> in networkX with the edge density of 4 to generate BA graphs.</p></list-item></list><bold>Computer System:</bold> We use an Nvidia DGX workstation to do all experiments, which consists of 40 CPU cores and 4 Volta V100 GPUs. More details of the system is provided in Table <xref rid="Tab2" ref-type="table">2</xref>.</p>
      <p id="Par50"><bold>Others:</bold> In addition, we use the learning rate
<inline-graphic xlink:href="500805_1_En_33_Figb_HTML.gif" id="d30e1625"/>
and batch size 128 to train our DNN model. The RL parameter exploration rate <inline-formula id="IEq14"><alternatives><tex-math id="M14">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\epsilon $$\end{document}</tex-math><inline-graphic xlink:href="500805_1_En_33_Chapter_IEq14.gif"/></alternatives></inline-formula> is set to 0.1. The size of the RL replay buffer is set to store up to 50,000 experience tuples.</p>
    </sec>
    <sec id="Sec13">
      <title>Quality of Graph Problem Solutions</title>
      <p id="Par51">To evaluate the quality of our graph solutions, we compare the results generated by five different MVC solvers:<list list-type="bullet"><list-item><p id="Par52"><bold>Graph-RL:</bold> This is our RL framework of <italic>OpenGraphGym</italic>.</p></list-item><list-item><p id="Par53"><bold>Random:</bold> The Random solver finds a solution by randomly taking a node from the graph in each step.</p></list-item><list-item><p id="Par54"><bold>2-OPT:</bold> The 2-OPT approximation algorithm takes both endpoints of an edge from the graph in each step. Its solution is guaranteed to be less than twice of the optimal solution [<xref ref-type="bibr" rid="CR3">3</xref>].</p></list-item><list-item><p id="Par55"><bold>Greedy:</bold> The Greedy algorithm builds the solution by simply adding the node with the largest degree in each step.</p></list-item><list-item><p id="Par56"><bold>Exhaustive Search:</bold> For the verification purpose, we also implement a “brute-force” random search MVC solver. We let this program continuously search for large numbers of solutions until no better solution could be found in one hour.</p></list-item></list>
<table-wrap id="Tab2"><label>Table 2.</label><caption><p>An Nvidia DGX System.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">CPU</th><th align="left">Intel Xeon CPU E5-2698 v4 2.20 GHz</th></tr></thead><tbody><tr><td align="left">GPU</td><td align="left">Nvidia Volta V100</td></tr><tr><td align="left">Number of CPU Cores</td><td align="left">40</td></tr><tr><td align="left">Number of GPUs</td><td align="left">4</td></tr><tr><td align="left">Memory per GPU</td><td align="left">16 GB</td></tr><tr><td align="left">Operating System</td><td align="left">Ubuntu</td></tr><tr><td align="left">OS Version</td><td align="left">16.04.6</td></tr></tbody></table></table-wrap>
</p>
      <p id="Par57">In our validation experiment, we use 40 graphs to train the RL agent, and use 10 graphs to test the agent. The 10 test graphs has never been seen by the agent during training. Note that the training graphs and test graphs belong to the same type of graphs: either ER or BA type.</p>
      <p id="Par58">Table <xref rid="Tab3" ref-type="table">3</xref> shows four different datasets used in our experiments (one dataset per row), each with different average number of nodes and edges in the ER or BA type. In the table, the graph type, average number of vertices and edges are shown in the first three columns. The best two solutions are also highlighted in a bold font.</p>
      <p id="Par59">As to the ER graph dataset with 20 nodes, the exhaustive search algorithm obtains the best solution with 9.4. Our Graph-RL algorithm is the second-best with 10.1. As to the BA graph dataset with 20 nodes, our solver is the best one with MVC size 11.4. The second-best is the exhaustive search algorithm with the best MVC size 12. For the rest of the solvers, the 2-OPT and the Greedy are similar, whose solutions are around 15 and 16. Finally, the random method produces the worst solutions for both ER and BA graphs.</p>
      <p id="Par60">We also use ER and BA graph datasets that have 50 nodes for training and testing. As shown in the two rows at the bottom of Table <xref rid="Tab3" ref-type="table">3</xref>, our Graph-RL solver always found the best MVC solutions.</p>
      <p id="Par61">Based on the above comparison, we can say that our Graph-RL solver and the exhaustive search solver are constantly better than the other three solvers. In addition, the quality of the Graph-RL solutions is comparable to that of the long-time exhaustive searching algorithm.<table-wrap id="Tab3"><label>Table 3.</label><caption><p>Experiments with MVC on both ER and BA graphs. All MVC solutions shown here are averaged over 10 testing graphs. Five solvers (Graph-RL, Random, 2-OPT, Greedy, Exhaustive Search) are compared. The best two solutions are highlighted in bold for each dataset. Graph-RL is the solution obtained by our OpenGraphGym framework.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Graphs</th><th align="left">Avg#nodes</th><th align="left">Avg#edges</th><th align="left">Graph-RL</th><th align="left">Random</th><th align="left">2-OPT</th><th align="left">Greedy</th><th align="left">Exhaustive search</th></tr></thead><tbody><tr><td align="left">ER</td><td align="left">20</td><td align="left">30.1</td><td align="left"><bold>10.1</bold></td><td align="left">17.8</td><td align="left">15.2</td><td align="left">16.3</td><td align="left"><bold>9.4</bold></td></tr><tr><td align="left">BA</td><td align="left">20</td><td align="left">64</td><td align="left"><bold>11.4</bold></td><td align="left">18</td><td align="left">16.2</td><td align="left">16</td><td align="left"><bold>12</bold></td></tr><tr><td align="left">ER</td><td align="left">50</td><td align="left">190.4</td><td align="left"><bold>33.2</bold></td><td align="left">48</td><td align="left">45</td><td align="left">47.4</td><td align="left"><bold>36.4</bold></td></tr><tr><td align="left">BA</td><td align="left">50</td><td align="left">184</td><td align="left"><bold>28.8</bold></td><td align="left">48</td><td align="left">41.8</td><td align="left">46</td><td align="left"><bold>34.3</bold></td></tr></tbody></table></table-wrap>
</p>
    </sec>
    <sec id="Sec14">
      <title>Deploying an Agent Trained by Small Graphs to Test Bigger Graphs</title>
      <p id="Par62">The DNN model implemented by our framework can support graphs with various sizes. This feature enables us to train and test graphs with distinct sizes. To test the generalization performance of our model, we train the model on smaller size of graphs first. Then, we test the learned model using larger size of graphs.</p>
      <p id="Par63">The new experimental results are shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. For this experiment, we use a dataset with 40 ER graphs that have an average number of 20 nodes for training. However, we use a dataset of 10 ER graphs that have a number of 50 nodes for testing. For every 20 episodes of training, we use the 50-node testing dataset to test the model’s solution quality. From Fig. <xref rid="Fig2" ref-type="fig">2</xref>, we can observe that after around 75 episodes, the average number of nodes to cover the test graph dataset reaches 34.8, which is close to the exhaustive search algorithm’s solution. This result demonstrates that an RL agent trained from small graphs can be generalized to solve larger size graphs.<fig id="Fig2"><label>Fig. 2.</label><caption><p>Generalization ability test. 40 graphs with 20 nodes are used to train the model. At every 20 episodes, the model will be tested using 10 graphs with 50 nodes.</p></caption><graphic xlink:href="500805_1_En_33_Fig2_HTML" id="MO4"/></fig>
<fig id="Fig3"><label>Fig. 3.</label><caption><p>In this set of experiments, we use multiple GPUs for training on ER graphs with 20 nodes. The orange line is for the testing results with four GPUs. The blue line is for the results with one GPU. (Color figure online)</p></caption><graphic xlink:href="500805_1_En_33_Fig3_HTML" id="MO5"/></fig>
</p>
    </sec>
    <sec id="Sec15">
      <title>Effect of Using Multiple GPUs</title>
      <p id="Par64">Finally, we use multiple GPUs to accelerate the RL training process with our <italic>OpenGraphGym</italic> framework. In the experiment, the training dataset has 40 ER graphs and the test dataset has 10 ER graphs. All the training and testing graphs have an average number of 20 nodes. Also, we evaluate the trained RL model’s solution with the test graph dataset in every episode.</p>
      <p id="Par65">As shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>, the blue line represents the MVC solutions computed by a single GPU. The orange line represents the solutions computed by four GPUs. From the figure, we can observe that when we use one GPU, our framework can find the best solutions after 80 episodes. By contrast, the framework takes only 62 episodes to find the best solutions when using four GPUs. This experiment shows that our RL framework is able to find the best solution of a problem by taking fewer episodes (i.e., converging faster) when more GPUs are used.</p>
    </sec>
  </sec>
  <sec id="Sec16">
    <title>Conclusion</title>
    <p id="Par66">In this work, we design and implement a parallel reinforcement learning framework OpenGraphGym for graph optimization problems. Then, we use the MVC as the test case to demonstrate that the solution provided by our framework is better than some classical MVC solvers. This work focuses on three research directions: 1) We aim to use the open framework to benchmark various new RL algorithms and embedding methods. 2) Many real-world graphs are extreme-scales. We will add the support of extreme-scale graphs to our framework. 3) Currently, we only support a few basic parallel strategies. To better utilize the high performance computing resources, we will extend the <italic>OpenGraphGym</italic> framework to design more advanced and efficient parallel strategies.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>This research was funded by the U.S. Department of Energy, Office of Science, Advanced Scientific Computing Research (Interoperable Design of Extreme-scale Application Software).</p>
    </fn>
  </fn-group>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <mixed-citation publication-type="other">Abadi, M., et al.: TensorFlow: a system for large-scale machine learning. In: 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pp. 265–283 (2016)</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Albert</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Barabási</surname>
            <given-names>AL</given-names>
          </name>
        </person-group>
        <article-title>Statistical mechanics of complex networks</article-title>
        <source>Rev. Mod. Phys.</source>
        <year>2002</year>
        <volume>74</volume>
        <issue>1</issue>
        <fpage>47</fpage>
        <pub-id pub-id-type="doi">10.1103/RevModPhys.74.47</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <mixed-citation publication-type="other">Bar-Yehuda, R., Even, S.: A local-ratio theorem for approximating the weighted vertex cover problem. Technical report, Computer Science Department, Technion (1983)</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Bello, I., Pham, H., Le, Q.V., Norouzi, M., Bengio, S.: Neural combinatorial optimization with reinforcement learning. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1611.09940">arXiv:1611.09940</ext-link> (2016)</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cai</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>VW</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>KCC</given-names>
          </name>
        </person-group>
        <article-title>A comprehensive survey of graph embedding: problems, techniques, and applications</article-title>
        <source>IEEE Trans. Knowl. Data Eng.</source>
        <year>2018</year>
        <volume>30</volume>
        <issue>9</issue>
        <fpage>1616</fpage>
        <lpage>1637</lpage>
        <pub-id pub-id-type="doi">10.1109/TKDE.2018.2807452</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Dai, H., Dai, B., Song, L.: Discriminative embeddings of latent variable models for structured data. In: International Conference on Machine Learning, pp. 2702–2711 (2016)</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Erdős</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Rényi</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>On the evolution of random graphs</article-title>
        <source>Publ. Math. Inst. Hung. Acad. Sci.</source>
        <year>1960</year>
        <volume>5</volume>
        <issue>1</issue>
        <fpage>17</fpage>
        <lpage>60</lpage>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Grover, A., Leskovec, J.: node2vec: scalable feature learning for networks. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 855–864. ACM (2016)</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Hagberg, A., Swart, P., Chult, D.S.: Exploring network structure, dynamics, and function using NetworkX. Technical report, Los Alamos National Lab. (LANL), Los Alamos, NM (United States) (2008)</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Hoos, H.H., Stützle, T.: SATLIB: an online resource for research on SAT. In: SAT 2000, pp. 283–292 (2000)</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Khalil, E., Dai, H., Zhang, Y., Dilkina, B., Song, L.: Learning combinatorial optimization algorithms over graphs. In: Advances in Neural Information Processing Systems, pp. 6348–6358 (2017)</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional networks. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1609.02907">arXiv:1609.02907</ext-link> (2016)</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kool</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>van Hoof</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Welling</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Attention solves your TSP, approximately</article-title>
        <source>Statistics</source>
        <year>2018</year>
        <volume>1050</volume>
        <fpage>22</fpage>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Li, Z., Chen, Q., Koltun, V.: Combinatorial optimization with graph convolutional networks and guided tree search. In: Advances in Neural Information Processing Systems, pp. 539–548 (2018)</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Merkel</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Docker: lightweight linux containers for consistent development and deployment</article-title>
        <source>Linux J.</source>
        <year>2014</year>
        <volume>2014</volume>
        <issue>239</issue>
        <fpage>2</fpage>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Mittal, A., Dhawan, A., Medya, S., Ranu, S., Singh, A.: Learning heuristics over large graphs via deep reinforcement learning. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1903.03332">arXiv:1903.03332</ext-link> (2019)</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mnih</surname>
            <given-names>V</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Human-level control through deep reinforcement learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>518</volume>
        <issue>7540</issue>
        <fpage>529</fpage>
        <pub-id pub-id-type="doi">10.1038/nature14236</pub-id>
        <pub-id pub-id-type="pmid">25719670</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Node2Vec (2019). <ext-link ext-link-type="uri" xlink:href="https://github.com/eliorc/node2vec">https://github.com/eliorc/node2vec</ext-link></mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Sergeev, A., Del Balso, M.: Horovod: fast and easy distributed deep learning in TensorFlow. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1802.05799">arXiv:1802.05799</ext-link> (2018)</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Silver</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Mastering the game of go with deep neural networks and tree search</article-title>
        <source>Nature</source>
        <year>2016</year>
        <volume>529</volume>
        <issue>7587</issue>
        <fpage>484</fpage>
        <pub-id pub-id-type="doi">10.1038/nature16961</pub-id>
        <pub-id pub-id-type="pmid">26819042</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Silver</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Mastering the game of go without human knowledge</article-title>
        <source>Nature</source>
        <year>2017</year>
        <volume>550</volume>
        <issue>7676</issue>
        <fpage>354</fpage>
        <pub-id pub-id-type="doi">10.1038/nature24270</pub-id>
        <pub-id pub-id-type="pmid">29052630</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Vinyals, O., Fortunato, M., Jaitly, N.: Pointer networks. In: Advances in Neural Information Processing Systems, pp. 2692–2700 (2015)</mixed-citation>
    </ref>
  </ref-list>
</back>
