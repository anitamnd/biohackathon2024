<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Vision (Basel)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Vision (Basel)</journal-id>
    <journal-id journal-id-type="publisher-id">vision</journal-id>
    <journal-title-group>
      <journal-title>Vision</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2411-5150</issn>
    <publisher>
      <publisher-name>MDPI</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6969935</article-id>
    <article-id pub-id-type="doi">10.3390/vision3040055</article-id>
    <article-id pub-id-type="publisher-id">vision-03-00055</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Development of Open-source Software and Gaze Data Repositories for Performance Evaluation of Eye Tracking Systems</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2543-1697</contrib-id>
        <name>
          <surname>Kar</surname>
          <given-names>Anuradha</given-names>
        </name>
        <xref rid="c1-vision-03-00055" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1670-4793</contrib-id>
        <name>
          <surname>Corcoran</surname>
          <given-names>Peter</given-names>
        </name>
      </contrib>
    </contrib-group>
    <aff id="af1-vision-03-00055">Department of Electrical &amp; Electronic Engineering, National University of Ireland, H91 TK33 Galway, Ireland; <email>peter.corcoran@nuigalway.ie</email></aff>
    <author-notes>
      <corresp id="c1-vision-03-00055"><label>*</label>Correspondence: <email>a.kar2@nuigalway.ie</email>; Tel.: +353-8343-16560</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>22</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <volume>3</volume>
    <issue>4</issue>
    <elocation-id>55</elocation-id>
    <history>
      <date date-type="received">
        <day>30</day>
        <month>6</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>15</day>
        <month>10</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2019 by the authors.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="open-access">
        <license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p>In this paper, a range of open-source tools, datasets, and software that have been developed for quantitative and in-depth evaluation of eye gaze data quality are presented. Eye tracking systems in contemporary vision research and applications face major challenges due to variable operating conditions such as user distance, head pose, and movements of the eye tracker platform. However, there is a lack of open-source tools and datasets that could be used for quantitatively evaluating an eye tracker’s data quality, comparing performance of multiple trackers, or studying the impact of various operating conditions on a tracker’s accuracy. To address these issues, an open-source code repository named GazeVisual-Lib is developed that contains a number of algorithms, visualizations, and software tools for detailed and quantitative analysis of an eye tracker’s performance and data quality. In addition, a new labelled eye gaze dataset that is collected from multiple user platforms and operating conditions is presented in an open data repository for benchmark comparison of gaze data from different eye tracking systems. The paper presents the concept, development, and organization of these two repositories that are envisioned to improve the performance analysis and reliability of eye tracking systems.</p>
    </abstract>
    <kwd-group>
      <kwd>eye gaze</kwd>
      <kwd>eye trackers</kwd>
      <kwd>fixations</kwd>
      <kwd>data quality</kwd>
      <kwd>performance evaluation</kwd>
      <kwd>code repository</kwd>
      <kwd>gaze dataset</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1-vision-03-00055">
    <title>1. Introduction</title>
    <p>Gaze data quality refers to the validity of the gaze data measured and reported by an eye tracker [<xref rid="B1-vision-03-00055" ref-type="bibr">1</xref>]. The most common method of representing gaze data quality is by specifying gaze estimation accuracy, which refers to the difference between the true and the measured gaze positions [<xref rid="B2-vision-03-00055" ref-type="bibr">2</xref>]. There currently exists significant diversity in gaze accuracy measures as described in reference [<xref rid="B3-vision-03-00055" ref-type="bibr">3</xref>], which leads to ambiguity in interpretation of the quality of gaze data from different eye tracking systems and difficulty in comparison of two or more eye trackers. Moreover, with the growing applications of gaze information in consumer devices like augmented and virtual reality, smartphones, and smart TVs [<xref rid="B4-vision-03-00055" ref-type="bibr">4</xref>,<xref rid="B5-vision-03-00055" ref-type="bibr">5</xref>,<xref rid="B6-vision-03-00055" ref-type="bibr">6</xref>,<xref rid="B7-vision-03-00055" ref-type="bibr">7</xref>] the eye trackers used in such applications need to be thoroughly evaluated to ensure the high quality and consistency of their gaze data outputs. This calls for the development and adoption of homogeneous metrics for reporting gaze accuracy and a consistent set of methods for complete characterization of eye trackers’ data under different operating conditions [<xref rid="B8-vision-03-00055" ref-type="bibr">8</xref>]. There are several software tools [<xref rid="B9-vision-03-00055" ref-type="bibr">9</xref>,<xref rid="B10-vision-03-00055" ref-type="bibr">10</xref>,<xref rid="B11-vision-03-00055" ref-type="bibr">11</xref>,<xref rid="B12-vision-03-00055" ref-type="bibr">12</xref>,<xref rid="B13-vision-03-00055" ref-type="bibr">13</xref>] that have been developed over the years by gaze researchers as well as eye tracker manufacturers for gaze data analysis. The general focus of these software is toward determining eye movement characteristics (i.e., fixations, scanpath, saccades) and studying eye movement relationships with human cognitive process, such as creation of attention maps, understanding regions of user interests, and visual search patterns. Also, a range of gaze datasets have been developed so far by gaze researchers, which are either aimed at building of new gaze estimation algorithms or toward cognitive studies, visual saliency research, and scanpath analysis. However, gaze datasets that contain gaze and ground truth data collected under different operating conditions of an eye tracker, from multiple user platforms, are not yet publicly available.</p>
    <p>In this paper, two open-source code and data repositories are presented that are targeted specifically toward in-depth analysis and comparison of eye gaze data quality from generic eye trackers. These repositories are (1) the GazeVisual-Lib repository of software resources hosted on GitHub and (2) the NUIG_EyeGaze01 gaze data repository hosted on Mendeley data. This paper describes the creation, organization and usage of these two repositories that are aimed towards standardized evaluation of the performance of generic eye-trackers. These repositories can benefit gaze researchers, developers of gaze-based systems and applications, and generic users by providing them easy-to-use methods for quantitatively evaluating gaze data outputs from an eye tracker, compare quality of two or more trackers or user platforms. The key features of these two repositories are summarized in <xref ref-type="sec" rid="sec1dot1-vision-03-00055">Section 1.1</xref> and <xref ref-type="sec" rid="sec1dot2-vision-03-00055">Section 1.2</xref> below.</p>
    <p>The motivation behind developing the GazeVisual-Lib software repository is that it could be used by gaze researchers to analyze gaze data and answer critical questions related to gaze data quality. For example, what are the performance limits and tolerances of a given eye tracker? How much is an eye tracker’s accuracy affected when operating under non ideal operating conditions? Which operating conditions affect the tracker’s performance in a particular use-case? How can two gaze datasets, or the performance of two eye tracking systems, be compared quantitatively? What are the performance bottlenecks of individual algorithms? How can gaze error patterns be detected and predicted? The software resources provided in the GazeVisual-Lib repository can help any generic user or an eye gaze researcher to find answer to these questions with minimal programming effort. </p>
    <p>The motivation for developing the NUIG_EyeGaze01 data repository is to present gaze datasets collected under unique and challenging operating conditions which are not usually available to gaze researchers. The gaze data within the repository has been collected from a high-resolution eye tracker under carefully designed operating conditions so that best- and worst-case performance characteristics of an eye tracker under the influence of these conditions may be studied. These gaze datasets can help researchers to compare the variation in the data quality of multiple eye trackers, determine anomalous gaze data types, and study a tracker’s reliability and system characteristics under unconstrained operating conditions. </p>
    <sec id="sec1dot1-vision-03-00055">
      <title>1.1. GazeVisual-Lib: An Open Software Repository for Eye Tracker Data Evaluation</title>
      <p>This paper describes the GitHub code repository named GazeVisual-Lib that contains the source codes for a complete GUI application tool and a range of numerical and visualization methods for quantitative and visual exploration of eye gaze data quality. A major component of the GazeVisual-Lib repository is the source code of a desktop GUI software application named GazeVisual, which takes in raw eye gaze data and implements several accuracy metrics and a range of visualizations to study gaze data quality. It also has methods to interface the GUI with an eye tracker for live gaze data collection [<xref rid="B14-vision-03-00055" ref-type="bibr">14</xref>]. Multiple videos are provided in the repository that show how to use the software to upload gaze data and derive results and visualizations. Apart from this, in the repository, there are codes in different sub-directories that could be used for (a) estimating gaze accuracy in angular resolutions as the difference between input gaze and ground truth data coordinates, (b) metrics and visualizations for exploration of gaze data quality [<xref rid="B8-vision-03-00055" ref-type="bibr">8</xref>], (c) de-noising and outlier removal from gaze data, and (d) augmentation of a gaze (fixation/scanpath) dataset by seven different methods. The GazeVisual-Lib repository is hosted on GitHub and accompanies full documentation and guidance on the use of individual repository components. The GitHub repository can be found at github.com/anuradhakar49/GazeVisual-Lib.</p>
      <p>This paper provides details on how to use the GazeVisual-Lib repository, installation of the dependencies i.e., Python libraries (<uri xlink:href="www.python.org">www.python.org</uri>) required for running the codes from the repository, and practical illustrative examples that would guide a user to run the GazeVisual GUI tool. Also, all the Python codes are made available as Jupyter notebooks within the GitHub repository, so that any user can run these and adapt these resources easily. </p>
    </sec>
    <sec id="sec1dot2-vision-03-00055">
      <title>1.2. NUIG_EyeGaze01: An Open Gaze Data Repository</title>
      <p>In addition to the open coding resources, a new eye gaze dataset is presented in this paper, named NUIG_EyeGaze01 (Labelled eye gaze dataset). This dataset is created through dedicated experiments, using data from a high resolution eye tracker while it operated on three different eye tracking platforms—a desktop, a laptop, and a tablet—under a wide range of operating conditions such as variable user head poses, user distances, screen resolutions, and platform poses. The gaze data files are made available publicly and could be useful to gaze researchers for benchmark comparison of performance of other eye trackers, for building advanced gaze data evaluation metrics, and also for understanding gaze error patterns caused by the different operating conditions mentioned above. The NUIG_EyeGaze01 dataset is hosted on Mendeley Data, which is an open data repository and may be found in the following link: <uri xlink:href="https://data.mendeley.com/datasets/cfm4d9y7bh/1">https://data.mendeley.com/datasets/cfm4d9y7bh/1</uri>.</p>
      <p>In this paper, details on the data collection process for creation of the NUIG_EyeGaze01 dataset and the dataset organization is provided. The contents of the collected gaze data files are discussed along with sample data presentation from the various experiments done for the data collections. Also, a sample Python code snippet is provided in this paper that may be used to read from the CSV data files in the open dataset, so that researchers can readily use these datasets and extract and manipulate the information in them. Finally, the utility and significance of the dataset and the coding resources toward gaze research are discussed. </p>
    </sec>
    <sec id="sec1dot3-vision-03-00055">
      <title>1.3. Scope and Organization of the Paper</title>
      <p>The scope of this paper is focused around discussing the organization and contents of the two code and data repositories described in <xref ref-type="sec" rid="sec1dot1-vision-03-00055">Section 1.1</xref> and <xref ref-type="sec" rid="sec1dot2-vision-03-00055">Section 1.2</xref>. This paper describes the components of the GazeVisual-Lib repository along with detailed instructions on how these resources maybe used with minimum programing effort. This is done so that readers can understand the purpose, contents, and implementation of the GazeVisual-Lib repository, and it can be readily useful to the interdisciplinary gaze research community for evaluation of gaze data quality. It may be noted that the mathematical derivation of the metrics, visualizations, and concept of the GazeVisual GUI application (present in the GitHub repository) have been discussed in details in our previous papers [<xref rid="B8-vision-03-00055" ref-type="bibr">8</xref>] and [<xref rid="B14-vision-03-00055" ref-type="bibr">14</xref>], which provide the scientific background for the coding resources presented in the repository. In a similar way, this paper describes the content and structure of the NUIG_EyeGaze01 data repository with details on each data file, their columns and file naming conventions and sample usage. These would ensure that the collected datasets may be easily used by vision researchers. The philosophy behind the gaze data collection process has been discussed in [<xref rid="B8-vision-03-00055" ref-type="bibr">8</xref>].</p>
      <p>The paper is organized as follows: <xref ref-type="sec" rid="sec2-vision-03-00055">Section 2</xref> presents a literature review on contemporary gaze data evaluation software and publicly available gaze datasets. <xref ref-type="sec" rid="sec3-vision-03-00055">Section 3</xref> describes the structure and contents of the GazeVisual-Lib repository, and <xref ref-type="sec" rid="sec4-vision-03-00055">Section 4</xref> presents the details of the NUIG_EyeGaze01 data repository. Sub-<xref ref-type="sec" rid="sec4dot4-vision-03-00055">Section 4.4</xref> presents discussions and analysis of the collected datasets. <xref ref-type="app" rid="app2-vision-03-00055">Appendix A</xref> presents the installation instructions for the various libraries required to run the GazeVisual-Lib coding resources, and <xref ref-type="app" rid="app3-vision-03-00055">Appendix B</xref> contains a series of gaze data plots created using data from the NUIG_EyeGaze01 repository.</p>
    </sec>
  </sec>
  <sec id="sec2-vision-03-00055">
    <title>2. Previous Works on Open-Source Gaze Data Analysis Software and Gaze Datasets </title>
    <p>Eye-tracking has found applications in fields such as neuro and cognitive sciences, psychology, human-computer interactions, consumer electronics, and assistive technologies [<xref rid="B15-vision-03-00055" ref-type="bibr">15</xref>]. Performance of eye trackers are judged based on their accuracy (in degrees of angular resolution) and it is affected by physiological limitations, tracker setup geometry, or due to type of calibration techniques used in them [<xref rid="B16-vision-03-00055" ref-type="bibr">16</xref>]. Works such as references [<xref rid="B2-vision-03-00055" ref-type="bibr">2</xref>] discuss the evaluation and comparison of several commercial eye trackers. In reference [<xref rid="B5-vision-03-00055" ref-type="bibr">5</xref>], an open-source Matlab toolkit is presented that can be interfaced with a commercial (Tobii EyeX) tracker. The paper evaluates and reports the eye tracker’s performance in terms of angular accuracy, precision, latency, and sampling frequency. In reference [<xref rid="B6-vision-03-00055" ref-type="bibr">6</xref>], the performances of three wearable trackers, from Pupil Labs (120 Hz), SMI, and the TobiiPro, are compared in terms of their accuracy under multiple viewing conditions. In reference [<xref rid="B2-vision-03-00055" ref-type="bibr">2</xref>], the accuracy and precision the Eye Tribe tracker is compared with the SMI tracker. The work concluded that the selection of software to record and process the data are significant in obtaining high accuracy results from an eye tracker. </p>
    <p>There are several open-source software packages and toolboxes that have been developed for recording and analyzing gaze data, for example ETCAL [<xref rid="B7-vision-03-00055" ref-type="bibr">7</xref>], Pygaze [<xref rid="B10-vision-03-00055" ref-type="bibr">10</xref>], GazeParser [<xref rid="B17-vision-03-00055" ref-type="bibr">17</xref>], EyeMMV [<xref rid="B18-vision-03-00055" ref-type="bibr">18</xref>], and GazeAlyse [<xref rid="B19-vision-03-00055" ref-type="bibr">19</xref>] to name a few. PyGaze is an open-source software package in Python language which is built for creating eye tracking experiments, e.g., for presentation of visual stimulus and collection of user response via keyboard or mouse. It also allows online detection of eye movements and supports a wide range of commercial eye trackers. Another Python based open-source library is GazeParser which was developed for low-cost eye tracking, gaze data recording and analysis. It captures images from a camera to record eye position and subsequently performs calibration, synchronization of stimulus presentation along with recording and analysis of eye movements. Eye Movements Metrics &amp; Visualizations (EyeMMV) is a toolbox built using MATLAB for eye movement analysis. It contains functions for identifying fixations, heatmap, and scanpath visualizations and region of interest analysis. Another Matlab-based toolbox is GazeAlyze, which does analysis of eye movement data, e.g., detecting and filtering artefacts, generating regions of interest, and visualizations such as path plots and fixation heat maps. There are also functions for correcting eye movement data due to the head position changes. The EMA toolbox [<xref rid="B20-vision-03-00055" ref-type="bibr">20</xref>] is implemented in Matlab for eye movement analysis and can parse gaze data from eye trackers like SR Research, SMI (RED 250), and Tobii EyeX. This toolbox allows for data conversion from normalized to pixel to degrees, determination of saccades and their kinematics, and creating saliency maps. Another toolkit named Pytrack [<xref rid="B21-vision-03-00055" ref-type="bibr">21</xref>] is built for analyzing and visualizing eye tracking data, feature extraction with respect to blinks, fixations, saccades, micro-saccades and pupil diameter, generate gaze heat map, micro-saccade position, and velocity plots.</p>
    <p>ETCAL [<xref rid="B7-vision-03-00055" ref-type="bibr">7</xref>] is a recent development among open-source gaze research tools, and it is a library that provides a common platform to implement a range of calibration procedures to determine gaze points from raw eye movement recordings. The library contains algorithms for preparation and optimization of calibration models and automatic detection of gaze targets for implicit calibration scenarios. ETCAL is a useful tool for researchers who work with different calibration scenarios or want to build their own eye trackers, compare different calibration algorithms and data quality.</p>
    <p>It may be observed that most of software developed so far for eye trackers aim towards exploration of eye movement characteristics (detecting fixations, scanpath, saccades, eye movement speed, direction, duration), studying eye movement and their relationships with human behavior (such as building attention maps), deriving regions and sequence of interests, and analyzing cognitive processes. However, only a few software tools (for example, ETCAL [<xref rid="B7-vision-03-00055" ref-type="bibr">7</xref>]) exist that are designed for quantitative evaluation and visualization of gaze error characteristics, e.g., for estimation of gaze error statistics and distributions and comparison of gaze errors collected under different operating conditions (or error sources). Therefore, in this paper, a new open-source repository of Python-based software tools is presented that can be used for the in-depth analysis of the gaze error characteristics that is collected from any eye tracker, irrespective of the tracking platform, hardware, or algorithm.</p>
    <p>With respect to eye gaze datasets, there currently exists a multitude of them, and more are being developed by researchers to cater to individual research problems. A survey of gaze datasets was made, and it was observed that existing gaze datasets can be broadly classified into two types: the ones used for building and testing gaze estimation algorithms, and the others that are used for modelling and validating user attention patterns and cognitive processes. <xref rid="vision-03-00055-t001" ref-type="table">Table 1</xref> shows the results of this survey and presents the details of several datasets that have been developed for building and testing gaze estimation algorithms. <xref rid="vision-03-00055-t002" ref-type="table">Table 2</xref> presents the datasets developed for saliency and cognitive studies. These datasets have been developed with users looking at a series of images while their eye movements/images/videos are recorded. The collected eye movement data is then used for building and validating cognitive studies, visual attention patterns, saliency models, etc.</p>
    <p>It is observed that typically gaze datasets include eye images/video, eye corners, iris, blink rate, eye closure, fixation or smooth pursuit data. Some include head pose information, while datasets are captured under “free-head motion,” i.e., the exact angular positions of the user head are not known. Some datasets include conditions such as users with/without glasses, change in illumination and background, varying race, age, etc. In this work, a new eye tracking dataset is built comprising of gaze data from three different user platforms, specifically for benchmark evaluation of eye trackers operating under unconstrained operational scenarios and is described in <xref ref-type="sec" rid="sec4-vision-03-00055">Section 4</xref> of this paper.</p>
  </sec>
  <sec id="sec3-vision-03-00055">
    <title>3. Description of the GazeVisual-Lib Code Repository</title>
    <sec id="sec3dot1-vision-03-00055">
      <title>3.1. Organization of the Repository</title>
      <p>The GazeVisual-Lib repository is hosted on GitHub and contains numerical and visual methods implemented as Python codes that can be used to evaluate and compare data quality and characteristics of generic eye trackers. The methods require gaze data samples from an eye tracker, ground truth locations, and values of setup variables like user-tracker distance, size, and resolution of the display screen where gaze was tracked. <xref rid="vision-03-00055-t003" ref-type="table">Table 3</xref> presents an overview of the repository.</p>
    </sec>
    <sec id="sec3dot2-vision-03-00055">
      <title>3.2. Functionalities of the GazeVisual-Lib Repository Components</title>
      <p>The GazeVisual-Lib repository provides easy-to-use gaze data evaluation resources for free use, modification, and upgradation by eye gaze researchers and engineers. As shown in <xref ref-type="fig" rid="vision-03-00055-f001">Figure 1</xref>, the contents of the repository are organized into multiple sub-directories, each containing a set of codes written in the Python language. The Python codes and supporting information for using the numerical methods and visualizations are included in the repository in different folders (<xref ref-type="fig" rid="vision-03-00055-f002">Figure 2</xref>). The details about the contents of these folders and their functionalities are provided below. For running these codes, a user must have Python 2.7 along with libraries like Python libraries like Numpy, Matplotlib, Tkinter, Pygame, Statsmodels, and Seaborn installed.</p>
      <sec id="sec3dot2dot1-vision-03-00055">
        <title>3.2.1. “Gaze Data Pre-Processing” Folder</title>
        <p>In this folder, there are three Python or .py files, also combined in a Jupyter Notebook or .ipynb file (named: Data pre-processing.ipynb), which are meant to perform the following functions: <list list-type="simple"><list-item><label>(1)</label><p>Raw data conversion and calculation of accuracy: The main_proc.py file in this folder estimates gaze angular and gaze yaw and pitch accuracies from raw gaze data samples and ground truth data. The output is a CSV file (user_data_proc.csv), which contains several gaze angular variables (gaze yaw, pitch, frontal angle) and gaze accuracy values which are the angular differences between estimated gaze locations and stimuli locations [<xref rid="B8-vision-03-00055" ref-type="bibr">8</xref>].</p></list-item><list-item><label>(2)</label><p>Outlier removal: Gaze data is almost always corrupted with outliers and it is impossible to observe any error patterns until outliers are removed. The outlier_removal.py file implements three different outlier detection and removal strategies which are 1D Median filtering, median absolute deviation and interquartile range method [<xref rid="B8-vision-03-00055" ref-type="bibr">8</xref>].</p></list-item><list-item><label>(3)</label><p>Data augmentation: Six different methods for synthesizing gaze angular data variables from an original gaze data file are presented in the code named data_augmentation.py. The methods include addition of white and colored noise, data interpolation, time-shifting, data convolution with cosine kernels, and a combination of these. Augmented gaze datasets may be used for purposes like the use of machine learning algorithms [<xref rid="B48-vision-03-00055" ref-type="bibr">48</xref>] to model gaze data patterns of a tracker, modelling of scanpath and search patterns. Sample output plots from the main_proc.py and the outlier_removal.py codes are in <xref ref-type="fig" rid="vision-03-00055-f003">Figure 3</xref>a,b.</p></list-item></list></p>
      </sec>
      <sec id="sec3dot2dot2-vision-03-00055">
        <title>3.2.2. “Gaze Accuracy Metrics” Folder</title>
        <p>In this folder, there is a sample gaze data file (user_data_proc.csv, which is produced from the main_proc.py program) and three files (data_statistics.py, data_similarity.py, spatial_density.py) that may be used to compute gaze data statistics, similarity between gaze datasets, and gaze error spatial density on the display screen where gaze was tracked. The data_statistics.py file calculates mean gaze angular error (over the number of gaze data points), standard and median absolute deviation, confidence intervals, and Z-score of the gaze angular error from an input gaze dataset. The file data_similarity.py computes the similarity between gaze data, e.g., from different eye trackers or experiments. The similarity calculation is based on correlation, intersection, and Bhattacharya distance [<xref rid="B8-vision-03-00055" ref-type="bibr">8</xref>] computed on histograms of two gaze datasets. The scatter_density.py file helps to create a gaze data density plot in which raw gaze data is plotted as data point clusters and color-mapped according to point densities, which helps to study gaze data patterns and detect anomaly. All these are combined in the Jupyter Notebook named “Gaze accuracy metrics.ipynb” kept within the folder.</p>
      </sec>
      <sec id="sec3dot2dot3-vision-03-00055">
        <title>3.2.3. “Gaze Data Visualizations” Folder</title>
        <p>In this folder there is a sample gaze data file (user_data_proc.csv, output of the main_proc.py) and 4 python codes (combined in the Gaze data visualizations.ipynb file) that implement various visualizations [<xref rid="B8-vision-03-00055" ref-type="bibr">8</xref>]. The file 3D_plot.py creates a 2D grid of on-screen locations and produces a 3D plot of the magnitude of gaze errors (along <italic>Z</italic>-axis) as a function of X and Y dimensions of the display screen. These plots help to diagnose gaze error levels over the display area.</p>
        <p>The eccentricity.py file plots create a 2D surface plot of gaze error levels mapped with respect to visual angle values on the display screen, using data from the DIFF GZ” and “YAW DATA” and “PITCH DATA” columns, respectively, from the user_data_proc.csv file. This plot program may be used to study how gaze errors vary with visual angles, especially if user distance from the tracker is increased or decreased. For shorter distances, gaze errors are usually more sensitive to visual eccentricities, whereas gaze errors for long distances (e.g., at 80 cm) show less sensitivity to eccentricities [<xref rid="B8-vision-03-00055" ref-type="bibr">8</xref>]. The file 3D_histogram.py plots stacked 3D data distributions using data from two or more trackers or experiments. It helps to understand and compare data patterns and gain insight into data characteristics, e.g., where error values are concentrated or presence of data extremes. Sample output plots from the eccentricity.py and 3D_plot.py are shown in <xref ref-type="fig" rid="vision-03-00055-f004">Figure 4</xref>a,b.</p>
      </sec>
      <sec id="sec3dot2dot4-vision-03-00055">
        <title>3.2.4. “GazeVisual GUI Tool” Folder</title>
        <p>This folder contains the source code (contained in the files gazevisual_v101.py, gazevisual_v101.ipynb) for the GazeVisual GUI application software. This software is designed for quick, easy, and in-depth evaluation of eye tracker data through a suite of statistical and visualization functions incorporated in it. GazeVisual comes in the form of a graphical user interface (GUI) and contains a range of functions to input and process gaze data files and produce various gaze accuracy metric results and visualizations. It can generate visual stimuli and can also be directly interfaced with an eye tracker to collect gaze data samples. It is entirely built in Python language using several data analysis and graphics libraries. The architecture of GazeVisual software is shown in <xref ref-type="fig" rid="vision-03-00055-f005">Figure 5</xref>a and views of the software are in <xref ref-type="fig" rid="vision-03-00055-f006">Figure 6</xref> and <xref ref-type="fig" rid="vision-03-00055-f007">Figure 7</xref>.</p>
        <p>The GazeVisual software is comprised of four independent windows containing a range of functions incorporated in it that are aimed toward gaze data evaluation. Input data format for the GazeVisual software is shown in <xref ref-type="fig" rid="vision-03-00055-f005">Figure 5</xref>b. It is comprised of columns for raw gaze and ground truth data coordinates and input variables like display screen resolution and pixel pitch of the display (µ) [<xref rid="B8-vision-03-00055" ref-type="bibr">8</xref>] and user distance from the tracker. Two sample gaze data files that can be input to the GazeVisual software are provided in the GitHub folder. The GazeVisual software can be compiled as a generic Python program/IPython Notebook to produce the GUI application. Following this, the input gaze data files can be uploaded to the software using the “Upload csv file” button, and then the rest of the software functionalities may be implemented using the other GUI buttons. Outputs of the software include gaze accuracy values, error statistics, plots of error numerical and spatial distributions, and comparison of two gaze datasets [<xref rid="B14-vision-03-00055" ref-type="bibr">14</xref>].</p>
        <p>The “Data Analysis” window provides functions for estimation of gaze angular variables, accuracy (in degrees), gaze error statistics, and distributions using a single gaze data file and allows comparison of these parameters for two gaze data files. The “Visualizations” window can be used to plot gaze error histograms and 3D spatial distributions from a gaze data file. It can also take in two gaze data CSV files and compare their characteristics by creating correlation/regression/box plots. The Test UI and LiveTracking window can create static and dynamic stimuli for data collection from an eye tracker and also interface GazeVisual with an eye tracker for direct data collection.</p>
        <p>The GazeVisual GUI application has been tested with data from two remote eye trackers and a head mounted eye tracker and is seen to produce consistent results [<xref rid="B14-vision-03-00055" ref-type="bibr">14</xref>]. Its input requirements are (a) input gaze data (x, y coordinates) and corresponding ground truth data coordinates, (b) input gaze and ground truth data coordinates should have their origin at display center, (c) input data is arranged in the format shown in <xref ref-type="fig" rid="vision-03-00055-f005">Figure 5</xref>b, (d) the data is free from non-numeric or NAN values, and (e) gaze and ground truth data must have same lengths (number of data rows). Sample input data files for testing this software may be found in the GitHub folder (named “usr1_45_gazedata.csv”). GazeVisual can be compiled and run as a desktop application on any operating system having Python 2.7 with libraries such as Tkinter, Pygame, Statsmodels, and Seaborn installed.</p>
        <p>The GazeVisual GUI tool could be extended by gaze researchers in various ways. For example, in <xref ref-type="fig" rid="vision-03-00055-f007">Figure 7</xref>, a single dataset is used to plot the figure, and the utility of this is to study gaze data characteristics from single person or experiments. However, functions for plotting the mean values of gaze errors and error patterns for different persons could be a valuable feature that could be added to the software in future. Although this feature is not present in the current version of GazeVisual, it is always possible to extend the software features to include more functions such as for uploading multiple datasets, numerical functions e.g., for data filtering and outlier removal. Also, common APIs could be developed to interface the GUI will multiple eye trackers. This is the utility of making the software and codes open resources that are freely available for modification.</p>
      </sec>
      <sec id="sec3dot2dot5-vision-03-00055">
        <title>3.2.5. Steps for Using the Gaze Data Evaluation Methods in GazeVisual-Lib for Analysing Gaze Data</title>
        <p>The GazeVisual-lib is hosted on GitHub and can be found in the following web-address: github.com/anuradhakar49/GazeVisual-Lib. It is released under the GNU-GPL v3.0 license, which allows the users to run, share, and modify the software and the source codes in the repository.</p>
        <p>An experimental setting for using this code repository and its components is schematically shown in <xref ref-type="fig" rid="vision-03-00055-f008">Figure 8</xref>. For using the gaze data evaluation methods, first of all, a sample of gaze data from one or more participants is required [<xref rid="B8-vision-03-00055" ref-type="bibr">8</xref>]. To collect gaze data, (a) a user has to sit in front of the eye tracker under test, which is mounted on a computer screen, and the user eyes are calibrated [<xref rid="B49-vision-03-00055" ref-type="bibr">49</xref>]. (b) The user is presented with visual stimuli, and the eye tracker under test should record the gaze coordinates of the user as the user gazes at the stimuli points. (c) The gaze data from the eye tracker is to be saved in CSV format. The ground truth data comprising the screen coordinates of the visual stimuli points appearing during gaze data collection are also to be saved in a separate CSV file.</p>
        <p>The raw gaze and ground truth data collected in the above manner can then be used as inputs to the codes in the repository and the GazeVisual software application. Details on how the gaze and ground truth data files may be used with the repository codes to estimate gaze accuracy, and successively implementing other gaze data evaluation methods is provided within the repository.</p>
        <p>To run the codes in the GazeVisual-lib code repository, users must have Python 2.7 and the different Python libraries installed (installation commands may be found in <xref ref-type="app" rid="app2-vision-03-00055">Appendix A</xref>). All the codes are in the “Code repository” folder (<xref ref-type="fig" rid="vision-03-00055-f002">Figure 2</xref> above), which has several “README” files in various sub-folders that provide details about how to format an input data file and use it with the codes. The README file of the root folder is the main documentation for the repository, which has details about how the codes may be run and the current repository version. Researchers should check this main README file to learn about current and subsequent version updates. Also examples showing the workings of the GazeVisual GUI software may be found in the videos S1 and S2, mentioned in the “<xref ref-type="app" rid="app1-vision-03-00055">Supplementary Materials</xref>” section of this paper below. Links to the videos are <uri xlink:href="https://www.youtube.com/watch?v=mPGlw711BCA">https://www.youtube.com/watch?v=mPGlw711BCA</uri> (Video 1, showing Visualization functions) <uri xlink:href="https://www.youtube.com/watch?v=sir_qZmvGME">https://www.youtube.com/watch?v=sir_qZmvGME</uri> (Video 2, showing Data Analysis functions).</p>
        <p>Gaze data from any source (e.g., eye tracking device, application, or algorithm) must be formatted as per the instructions in the README files and used first with the main_proc.py in the “data pre-processing” sub-folder to produce an output CSV file. The rest of the code functions are based on this output CSV file. Similarly, to use the GazeVisual GUI tool, users should copy and save the GazeVisual_v101.py file into any directory of their computer, run it as regular python codes, and make sure all the imported libraries are pre-installed. Sample CSV files to test the codes and understand the data format are in each sub-folder of the repository. Links to sample videos showing the operation of the GazeVisual GUI tool are in the “sample videos” file of the GUI folder.</p>
        <p>The gaze data evaluation methods in GazeVisual-Lib are based on calculations using data from both eyes and centralized gaze coordinates. If any eye tracker provides only monocular data or only centralized gaze coordinates without left/right eye position values, then the repository codes can still be used for gaze angle and accuracy calculations after minor changes. The metrics and visualizations in the repository will still work, but the results may vary from the case when binocular data is used.</p>
      </sec>
      <sec id="sec3dot2dot6-vision-03-00055">
        <title>3.2.6. Installing the GazeVisual-Lib Components</title>
        <p>For using the GazeVisual-Lib code repository, the repository has to be downloaded from GitHub manually or cloned using git. For cloning a Git bash shell (on Window) or terminal (OSX, Linux) and the repository is to be cloned by the command git clone <uri xlink:href="https://github.com/anuradhakar49/GazeVisual-Lib.git">https://github.com/anuradhakar49/GazeVisual-Lib.git</uri>.</p>
        <p>Prior to using the repository components, Python 2.7 must be installed on the computer, which can be easily done through the Anaconda distribution for Windows, MAC, and Linux operating systems (<uri xlink:href="https://www.anaconda.com/distribution/">https://www.anaconda.com/distribution/</uri>). After installing Python 2.7 through the Anaconda installer, the Anaconda prompt—which is a terminal application for installing Python libraries—is to be opened, and the libraries required for running the GazeVisual codes are to be installed. <xref rid="vision-03-00055-t0A1" ref-type="table">Table A1</xref> in <xref ref-type="app" rid="app2-vision-03-00055">Appendix A</xref> lists the names of the special libraries used in the repository codes (libraries like math, csv, numpy, os, sys, etc. are pre-installed in Python) and the commands that have to be entered to the Anaconda Prompt to install them in their latest versions. For persons using pip for Python to install the libraries, the corresponding commands are also provided.</p>
        <p>In addition to the individual Python scripts, all the methods within the repository have been made into Jupyter notebooks, in which the users do not need to install the libraries separately. A Jupyter notebook is a web-based interactive environment for the development of Python codes, which can be used for data analysis and visualization over a browser. This makes it very simple for users to run, view the outputs, and share the GitHub repository codes. All the codes in the GazeViusal-Lib repository can thus be run as Python scripts or as IPython notebooks. Jupyter can be opened by typing on Anaconda prompt (or terminal) the following: “jupyter notebook”.</p>
      </sec>
      <sec id="sec3dot2dot7-vision-03-00055">
        <title>3.2.7. Running the GazeVisual-Lib Codes and the GazeVisual GUI Application</title>
        <p>Running the codes of the GazeVisual-Lib repository is a three-step process (<xref ref-type="fig" rid="vision-03-00055-f009">Figure 9</xref>a). First, the repository has to be cloned or downloaded to a user’s computer. After this, the necessary libraries need to be installed as described above. Finally, the Python (.py) files or Jupyter notebooks (.ipynb files) are to be opened, and within, them the path variables (also mentioned in comments within the codes) have to be changed so that they refer to the actual physical location of the downloaded repository folder and its sub-folders on the user’s computer. After this, the repository codes can be run as normal Python script files or Jupyter notebooks. The CSV data files in the downloaded repository need to be checked to ensure that they contain column-wise data as shown on page, or they need to be formatted into columns using the “text to columns” function of MS Excel.</p>
        <p>An illustrative example of running the Jupyter notebook for the GazeVisual GUI application tool is described here. To open up the Jupyter application, the steps described in the last section should be followed and thereafter the Jupyter Notebook Dashboard will open up in the web browser address: <uri xlink:href="http://localhost:8888">http://localhost:8888</uri> as shown in <xref ref-type="fig" rid="vision-03-00055-f009">Figure 9</xref>b. On the Dashboard, the Jupyter Notebook App can access the files within its start-up folder, so a user has to navigate and find the folder where the .ipynb files are stored. As in <xref ref-type="fig" rid="vision-03-00055-f009">Figure 9</xref>b, the main repository folder downloaded from GitHub named GazeVisual-Lib-master is seen under “Files” tab, which is where all the .ipynb files can be accessed.</p>
        <p>The user has to click on “GazeVisual-Lib-master,” then click on the folder “Code repository” within it, and then click the folder name “GazeVisual GUI tool” to reach the GazeVisual_GUI.ipynb file. Then, to run this file, the user has to click on this file, upon which the notebook containing the code for the GUI application will open up in a new browser tab, as shown in <xref ref-type="fig" rid="vision-03-00055-f009">Figure 9</xref>b. Next, the path addresses “initialdir” and “imgpath” have to be changed so that they point to the physical location of the “GazeVisual-Lib-master” folder on the user computer. Finally, to run the GUI code, the whole notebook has to be run by clicking on the menu Cell → Run All (<xref ref-type="fig" rid="vision-03-00055-f009">Figure 9</xref>c). To restart the kernel and run the code afresh, the user has to click on the menu Kernel and then click Restart.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec4-vision-03-00055">
    <title>4. Description of the NUIG_EyeGaze01 Gaze Data Repository</title>
    <p>There are currently no publicly available eye gaze datasets that allow benchmark comparison and analysis of gaze data quality from two or more eye trackers. Also, there are no datasets that can be used to study gaze error patterns caused by various external operating conditions (or error sources) like head poses, user distances, or platform poses. While there exist plenty of eye gaze datasets containing eye images and videos or fixations and scanpath, none of them contain fixations and corresponding ground truth data collected from more than one eye tracking platform on different display resolutions. Without high-quality gaze data collected under measured variations of different such operating conditions (or error sources), no objective or practical comparisons of performance of new and/or existing gaze tracking systems can be made.</p>
    <p>Considering these factors, a rich and diverse gaze dataset, using the eye tracking data collected through dedicated eye tracking experiments conducted under wide range of operating conditions, is therefore built and presented in an open data repository. The dataset is named NUIG_EyeGaze01 (Labelled eye gaze dataset) and is hosted in the Mendeley open data repository with the doi:10.17632/cfm4d9y7bh.1. The link to the dataset is provided in <xref ref-type="sec" rid="sec1dot2-vision-03-00055">Section 1.2</xref>. This is a new kind of gaze data set, collected from three user platforms (desktop, laptop, tablet) under the influence of one condition at a time. Using this dataset, the impact of different operating conditions may be observed and quantitatively compared. The conditions include fifteen different head poses, four user distances, nine different platform poses, and three display screen size and resolutions. Each gaze data file is labelled with the type of operating conditions under which it was collected.</p>
    <sec id="sec4dot1-vision-03-00055">
      <title>4.1. Description of the Gaze Data Collection Process</title>
      <p>The gaze data collection setup for creating the NUIG_EyeGaze01 dataset included a commercial eye tracker mounted on a desktop computer, a laptop and a tablet whose specifications are provided in <xref rid="vision-03-00055-t004" ref-type="table">Table 4</xref>. A Tobii EyeX eye tracker was used for gaze estimation, and an Eyetribe tracker was used for pilot data collection. Participants were seated in front of the tracker-screen setup, and their chin was fixed with a chin rest (<xref ref-type="fig" rid="vision-03-00055-f010">Figure 10</xref>a). Prior to each data collection session, the eye tracker was calibrated with its own calibration software (six-point calibration). After calibration, a visual stimulus interface (<xref ref-type="fig" rid="vision-03-00055-f010">Figure 10</xref>b) was presented to the participants [<xref rid="B4-vision-03-00055" ref-type="bibr">4</xref>], and they were asked to gaze at the specific stimuli targets that appeared on the display screen as their gaze was recorded by the eye tracker. For each experiment session, the following gaze data parameters were estimated for each user: (a) gaze positions data vs ground truth data (locations of stimuli) in pixels and millimeter (b) gaze yaw, pitch angles vs time, and corresponding ground truth yaw, pitch angles vs time (ms);(c) gaze primary angular error, yaw error, and pitch error for each stimuli position and time point.</p>
      <p>For gaze data collection under variable operating conditions of the eye tracker, a series of gaze data collection experiments were done on the desktop, laptop and tablet platforms using the eye tracker. These experiments included (a) user distance experiments where users were seated at 50, 60, 70, or 80 cm from the tracker. This was done for the desktop, laptop and the tablet platforms (b) head pose experiments where a user had to position their head at certain fixed head pose angles while their gaze data was collected (<xref ref-type="fig" rid="vision-03-00055-f010">Figure 10</xref>c). This was done only for the desktop platform (c) platform pose experiments, where the eye tracking platform or tablet was oriented at certain fixed tablet pose angles <xref ref-type="fig" rid="vision-03-00055-f010">Figure 10</xref>d while user gaze data was collected. This was done only for the tablet platform. Further details about the participants, experimental setup and variables may be found in <xref rid="vision-03-00055-t004" ref-type="table">Table 4</xref>. <xref rid="vision-03-00055-t005" ref-type="table">Table 5</xref> provides the details about the contents of each CSV data file contained within the repository and a description of the data columns.</p>
      <p>In <xref ref-type="fig" rid="vision-03-00055-f011">Figure 11</xref>, <xref ref-type="fig" rid="vision-03-00055-f012">Figure 12</xref> and <xref ref-type="fig" rid="vision-03-00055-f013">Figure 13</xref>, samples of eye gaze data overlapped on ground truth locations of the stimuli from each of these experiments are provided. Gaze data is in black and ground truth data are in blue. It is seen that data from different experiments look consistent but are affected by variable levels of outliers, which is why the outlier removal methods are provided in the GazeVisual-Lib repository. All these data, along with time stamps, were written in comma separated values (CSV) format for each user and each experiment session. Gaze data plots from multiple participants for the different operating conditions may be found in <xref ref-type="app" rid="app3-vision-03-00055">Appendix B</xref> of this paper.</p>
    </sec>
    <sec id="sec4dot2-vision-03-00055">
      <title>4.2. Organization of the NUIG_EyeGaze01 Gaze Dataset on Mendeley Data</title>
      <p>The NUIG_EyeGaze01 dataset hosted on Mendeley is shown in <xref ref-type="fig" rid="vision-03-00055-f014">Figure 14</xref>. It contains gaze and ground truth data in CSV files distributed under multiple folders and subfolders which are depicted in <xref ref-type="fig" rid="vision-03-00055-f015">Figure 15</xref>. Each CSV file in the dataset contains 21 columns (<xref ref-type="fig" rid="vision-03-00055-f016">Figure 16</xref>) with multiple gaze data variables estimated from the raw gaze coordinates. The variables are computed from the raw gaze data using the methods described in reference [<xref rid="B4-vision-03-00055" ref-type="bibr">4</xref>]. Other than the raw gaze data, inputs for calculating the variables are resolution, pixel pitch of the display where gaze was tracked, and user distance from the tracker.</p>
      <p>Within the NUIG_EyeGaze01(Labelled eye gaze dataset) data repository, the data CSV file names are labelled with the participant number, platform name and operating condition. Name of each gaze data file has the convention: USERNUMBER_CONDITION_PLATFORM.CSV (e.g., us01_80_desk.csv). The data files can be downloaded, and respective column values can be read to directly use or visualize them using Python or any CSV reading program. A detailed documentation of the data is also provided within the repository.</p>
      <p>The NUIG_EyeGaze01 data repository is published under CC BY-NC 3.0 license. According to this license, Licensees may copy and distribute the material if they give the licensor the credits (attribution). Licensees may distribute derivative works only under a license identical to the license that governs the original work. The license also specifies that Licensees may use the data only for non-commercial purposes and there is also the condition that Licensees may copy, distribute, display, and perform only verbatim copies of the work, not derivative works of it.</p>
      <p>There remain possibilities for extending this gaze dataset by collecting gaze data under other challenging conditions. For example, calibration could be done with a fixed head pose, and then gaze data be collected from the subject in another head pose. Then, this data could be compared with that from fixed head pose and its specific calibration. Another scenario could be collecting gaze data when head pose and eye tracker pose change together, e.g., in an automotive environment.</p>
    </sec>
    <sec id="sec4dot3-vision-03-00055">
      <title>4.3. Using Data from the NUIG_EyeGaze01 Repository</title>
      <p>Users can read gaze data and other variables from any of the CSV data files present in the NUIG_EyeGaze01 repository on Mendeley Data using Python and the Pandas library (after downloading the files to their computer). <xref ref-type="fig" rid="vision-03-00055-f017">Figure 17</xref> shows such a code snippet that can be used for reading data from a gaze data CSV file and plotting the gaze error variable as a function of time.</p>
    </sec>
    <sec id="sec4dot4-vision-03-00055">
      <title>4.4. Analysing Gaze Data from the NUIG_EyeGaze01 Repository</title>
      <p>In order to study the characteristics of gaze data collected from the different eye tracker platforms (desktop, tablet) and under different operating conditions, statistical analysis is done on the datasets and their results are provided below. <xref rid="vision-03-00055-t006" ref-type="table">Table 6</xref> and <xref rid="vision-03-00055-t007" ref-type="table">Table 7</xref> below present the gaze error statistical values (mean, median absolute deviation, interquartile range, and 95% confidence intervals) from desktop and tablet experiments respectively. The methods for calculating gaze errors and estimating statistical metrics on gaze error values is provided in our previous paper [<xref rid="B4-vision-03-00055" ref-type="bibr">4</xref>]. It may be noted that the gaze data used for this analysis is available in the NUIG_EyeGaze01 data repository, and the software codes used for the gaze data analysis are provided in the GazeVisual-Lib GitHub repository.</p>
      <p>In <xref rid="vision-03-00055-t006" ref-type="table">Table 6</xref> and <xref ref-type="fig" rid="vision-03-00055-f018">Figure 18</xref>a, the terms UD 50, UD60, UD70, and UD80 correspond to gaze data from different user-distance experiments done on the desktop platform and R20, Y20, and P20 correspond to gaze data from head pose roll pitch yaw angle (20 degrees for each) experiments. All value-fields in the table have units in degrees of angular resolution. It is seen that gaze error levels are higher at low user distances and error reduces as user-tracker distance increases. Errors due to head yaw are seen to have the highest magnitude and errors due to head pitch have the highest inter-quartile range (or variability) in error magnitudes. Also, error levels due to various head poses are quite higher compared to when head pose is neutral (UD60 values in <xref rid="vision-03-00055-t006" ref-type="table">Table 6</xref>).</p>
      <p>In <xref rid="vision-03-00055-t007" ref-type="table">Table 7</xref> and <xref ref-type="fig" rid="vision-03-00055-f018">Figure 18</xref>b, UD 50, UD60, UD70, and UD80 correspond to gaze data from different user-distance experiments done on the tablet platform and R20, Y20, and P20 represent data from the tablet pose roll pitch yaw angles (20 degrees for each) experiments. It is seen that magnitudes of gaze angular errors due to tablet pose are high, and the highest error is caused due to platform roll variations. The error characteristics from tablet data are quite different than those from the desktop platform, and error magnitudes are lower for tablet for all user distances. Also, magnitudes of errors due to different platform poses (<xref ref-type="fig" rid="vision-03-00055-f018">Figure 18</xref>b) are higher than errors due to head poses (<xref ref-type="fig" rid="vision-03-00055-f018">Figure 18</xref>a).</p>
      <p><xref ref-type="fig" rid="vision-03-00055-f019">Figure 19</xref>a,b below show gaze error distributions for the data (after outlier removal) from desktop user distance and head pose experiments. The gaze error distributions are estimated using Kernel Density Estimate [<xref rid="B10-vision-03-00055" ref-type="bibr">10</xref>] on gaze error values corresponding to different operating conditions, using Gaussian Kernel and a bandwidth value of 0.2. It is seen that each operating condition leaves a definite signature on the gaze error distributions. Distinction exists between patterns of gaze errors for different user distances and head poses as the error distribution shifts toward higher, average, or lower error values for different conditions. Similar observations are made for tablet data for different conditions (<xref ref-type="fig" rid="vision-03-00055-f019">Figure 19</xref>c,d). The error distributions are seen to be non-Gaussian and also do not resemble any known statistical distribution.</p>
    </sec>
  </sec>
  <sec id="sec5-vision-03-00055">
    <title>5. Utility and Impact of Open Resources toward Eye Gaze Research</title>
    <p>The GazeVisual-lib repository described in this paper provides a set of open and standardized methods for gaze data evaluation to the interdisciplinary eye gaze research community so that gaze data from a variety of eye trackers, dynamic applications [<xref rid="B50-vision-03-00055" ref-type="bibr">50</xref>,<xref rid="B51-vision-03-00055" ref-type="bibr">51</xref>,<xref rid="B52-vision-03-00055" ref-type="bibr">52</xref>,<xref rid="B53-vision-03-00055" ref-type="bibr">53</xref>], or user platforms may be evaluated and compared under a unified framework. While using the repository, users can fully understand the sequence of development of the data evaluation codes, starting from raw gaze data, making these methods adaptable to gaze data from any source. With these methods, the practical limits and capabilities of any eye tracking system may be studied and compared quantitatively and can also be upgraded by researchers to adapt to their individual research problems.</p>
    <p>Since knowing the quality of gaze data is essential for ensuring the reliability of any gaze-based application or research, the evaluation routines of the repository can be used to constantly monitor the data quality of any eye tracker, especially during real-life operations that accompany variable setup and user conditions. Using the GUI application tool, users can perform in depth gaze data evaluation without the need for any detailed programming knowledge owing to its simple interface. This is particularly important due to the inter-disciplinary nature of gaze research where eye trackers are used widely by people from non-technological fields. The intended user group of the GazeVisual-Lib code repository is therefore quite diverse, ranging from developers of gaze estimation algorithms to users from fields like human-computer interaction, psychology and cognitive studies. Incidentally, gaze data quality is a critical aspect that affects all the stages of any gaze data–based research or application, and the open-source codes for gaze data evaluation are therefore expected to be highly useful in this respect.</p>
    <p>The experiments described in this paper have helped to develop and introduce an accessible, diverse, and benchmark eye gaze dataset that can aid in identifying the capabilities and limits of different eye tracking systems. Such labelled gaze datasets containing signatures of different operating conditions that frequently affect gaze data quality on different user platforms do not exist yet, and keeping this in mind, the NUIG-Eyegaze01 dataset has been made publicly available. The data can be put to a wide range of uses, including modelling and comparing error patterns [<xref rid="B54-vision-03-00055" ref-type="bibr">54</xref>], development and testing of gaze anomaly detection algorithms, or gaze error compensation algorithms, to name a few. These are all sparsely explored areas in gaze research, which could benefit from our diverse and open data repository. Further, the datasets may also be augmented using the data augmentation routines in the GazeVisual-Lib repository. The code and data repositories are therefore complementary to each other. A major utility of presenting the data and code repositories as open resources is that they are meant to encourage research toward practical and realistic performance evaluation of eye trackers, standardization of gaze research results, and building of more open-source tools for these purposes.</p>
  </sec>
  <sec sec-type="conclusions" id="sec6-vision-03-00055">
    <title>6. Conclusions</title>
    <p>The open-source gaze data evaluation methods of GazeVisual-Lib could be useful for researchers, engineers, and developers working with gaze estimation systems for the thorough assessment of their gaze data quality. The methods could be especially beneficial for eye trackers that operate under variable operating conditions where gaze data quality frequently becomes unreliable. Also, the GUI application GazeVisual may be used to perform prompt and in-depth gaze data evaluation without the need for any detailed programming knowledge. This could be particularly useful for the inter-disciplinary gaze research community where eye trackers are used widely in non-technological fields. The potential user group of GazeVisual-Lib is therefore quite diverse, ranging from gaze tracking system developers, researchers using eye trackers in virtual/augmented reality, human–computer interactions, cognitive sciences, and generic users having any consumer-grade eye tracker or gaze-based application.</p>
    <p>The new eye gaze database NUIG_EyeGaze01 presented in this paper could be beneficial to designers of gaze-based systems for benchmark comparison of their system performances under challenging operating conditions such as variations of head pose, user distance, and tracker orientations. As can be observed from the gaze data analysis results presented in <xref ref-type="sec" rid="sec4dot4-vision-03-00055">Section 4.4</xref>, possible future directions of research using these gaze datasets (in conjunction with the coding resources of GazeVisual-Lib) include comparison of gaze error patterns from multiple eye trackers, modelling of gaze error patterns induced by different operating conditions, studying gaze error distributions, or the development of gaze error pattern detection algorithms. These would depend on how gaze researchers, statisticians, and researchers working with machine learning models would prefer to use these datasets. The open resources presented in the paper are envisioned to foster collaborative development and adoption of even better resources toward standardized gaze data evaluation, which ultimately can strengthen the usability and reliability of gaze estimation systems in their wide range of applications.</p>
  </sec>
</body>
<back>
  <app-group>
    <app id="app1-vision-03-00055">
      <title>Supplementary Materials</title>
      <p>The following are available online, Video S1: <uri xlink:href="https://www.youtube.com/watch?v=mPGlw711BCA">https://www.youtube.com/watch?v=mPGlw711BCA</uri>. Video S2: <uri xlink:href="https://www.youtube.com/watch?v=sir_qZmvGME">https://www.youtube.com/watch?v=sir_qZmvGME</uri>.</p>
    </app>
  </app-group>
  <notes>
    <title>Author Contributions</title>
    <p>A.K. conceived and designed the experiments, prepared the gaze tracking setups and ran the experiments for the collection of data. A.K. wrote the draft, and both authors discussed the contents of the manuscript. P.C. contributed to the research idea, did supervision of the work, provided feedback on the work, corrected the draft, and approved the final version. Conceptualization: A.K.; Data curation, A.K.; Formal analysis, A.K.; Funding acquisition, P.C.; Investigation, A.K.; Methodology, A.K.; Project administration, P.C.; Software, A.K.; Supervision, P.C.; Visualization, A.K.; Writing—original draft, A.K.; Writing—review &amp; editing, P.C.</p>
  </notes>
  <notes>
    <title>Funding</title>
    <p>The research work presented here was funded under the Strategic Partnership Program of Science Foundation Ireland (SFI) and co-funded by FotoNation Ltd. Project ID: 13/SPP/I2868 on “Next Generation Imaging for Smartphone and Embedded Platforms.”</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Conflicts of Interest</title>
    <p>The authors declare no conflict of interest.</p>
  </notes>
  <app-group>
    <app id="app2-vision-03-00055">
      <title>Appendix A</title>
      <table-wrap id="vision-03-00055-t0A1" orientation="portrait" position="anchor">
        <object-id pub-id-type="pii">vision-03-00055-t0A1_Table A1</object-id>
        <label>Table A1</label>
        <caption>
          <p>Python libraries to be installed for using GazeVisual-Lib and their commands.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">No.</th>
              <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Python Library Name</th>
              <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Anaconda/Pip Command</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">matplotlib</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">conda install -c conda-forge matplotlib pip install matplotlib </td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PIL</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">conda install -c anaconda pil, pip install Pillow==2.2.2</td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ttk</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">conda install -c conda-forge pyttk, pip install pyttk</td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">pandas</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">conda install -c anaconda pandas, pip install pandas</td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">statsmodels</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">conda install -c anaconda statsmodels pip install statsmodels</td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">seaborn</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">conda install -c anaconda seaborn pip install git+<uri xlink:href="https://github.com/mwaskom/seaborn.git#egg=seaborn">https://github.com/mwaskom/seaborn.git#egg=seaborn</uri></td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">pygame</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">conda install -c cogsci pygame pip install pygame</td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">scipy</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">conda install -c anaconda scipy pip install scipy</td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">sklearn</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">conda install -c anaconda scikit-learn pip install -U scikit-learn</td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">itertools</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">conda install -c conda-forge more-itertools pip install more-itertools</td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Git</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">conda install -c anaconda git, pip install python-git</td>
            </tr>
            <tr>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Jupyter notebook (comes by default with Anaconda distribution)</td>
              <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">pip install jupyter</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </app>
    <app id="app3-vision-03-00055">
      <title>Appendix B</title>
      <p>Gaze data (on-screen x, y coordinates) of five participants (P1–P5) belonging to different desktop and tablet-based experiments are presented below along with ground truth data (GT, in black). By ground truth we mean the screen location (x, y coordinates) of the stimuli dots as they appear on the screen while the user looks at them. The gaze data collection procedure has been discussed in detail above in <xref ref-type="sec" rid="sec4dot1-vision-03-00055">Section 4.1</xref>. During experiments, a visual stimulus in the form of a black moving dot sequentially moves over a grid of (5 × 3) locations over the display screen (of desktop or tablet). The screen locations traced by the stimulus dot are shown as the black lines in plots B.1 and B.2 which is our ground truth data. The gaze data comprises of a participant’s gaze coordinates on the display as they follow the stimuli dots. It may be noted that our Tobii tracker operates at 30 fps and therefore cannot register rapid eye movements like drifts, saccades. and micro-saccades.</p>
      <sec id="secBdot1-vision-03-00055">
        <title>Appendix B.1. Gaze Data from Desktop Experiments</title>
        <fig id="vision-03-00055-f0A1" orientation="portrait" position="anchor">
          <label>Figure A1</label>
          <caption>
            <p>Gaze data from desktop-based user distance and head pose experiments.</p>
          </caption>
          <graphic xlink:href="vision-03-00055-g0A1"/>
        </fig>
      </sec>
      <sec id="secBdot2-vision-03-00055">
        <title>Appendix B.2. Data from Tablet Experiments</title>
        <fig id="vision-03-00055-f0A2" orientation="portrait" position="anchor">
          <label>Figure A2</label>
          <caption>
            <p>Gaze data from tablet-based user distance and tablet pose experiments.</p>
          </caption>
          <graphic xlink:href="vision-03-00055-g0A2"/>
        </fig>
      </sec>
    </app>
  </app-group>
  <ref-list>
    <title>References</title>
    <ref id="B1-vision-03-00055">
      <label>1.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Holmqvist</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Nyström</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Mulvey</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Eye tracker data quality: What it is and how to measure it</article-title>
        <source>Proceedings of the ETRA’12</source>
        <conf-loc>Santa Barbara, CA, USA</conf-loc>
        <conf-date>28–30 March 2012</conf-date>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2012</year>
        <fpage>45</fpage>
        <lpage>52</lpage>
      </element-citation>
    </ref>
    <ref id="B2-vision-03-00055">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ooms</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Lapon</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Dupont</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Popelka</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Accuracy and precision of fixation locations recorded with the low-cost Eye Tribe tracker in different experimental set-ups</article-title>
        <source>J. Eye Mov. Res.</source>
        <year>2015</year>
        <volume>8</volume>
        <fpage>1</fpage>
        <lpage>24</lpage>
      </element-citation>
    </ref>
    <ref id="B3-vision-03-00055">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kar</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Corcoran</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>A Review and Analysis of Eye-Gaze Estimation Systems, Algorithms and Performance Evaluation Methods in Consumer Platforms</article-title>
        <source>IEEE Access</source>
        <year>2017</year>
        <volume>5</volume>
        <fpage>16495</fpage>
        <lpage>16519</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2017.2735633</pub-id>
      </element-citation>
    </ref>
    <ref id="B4-vision-03-00055">
      <label>4.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Funke</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Greenlee</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Carter</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Dukes</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Brown</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Menke</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Which eye tracker is right for your research? Performance evaluation of several cost variant eye trackers</article-title>
        <source>Proceedings of the Human Factors and Ergonomics Society 2016 Annual Meeting</source>
        <conf-loc>Washington, DC, USA</conf-loc>
        <conf-date>19–23 September 2016</conf-date>
        <fpage>1239</fpage>
        <lpage>1243</lpage>
      </element-citation>
    </ref>
    <ref id="B5-vision-03-00055">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gibaldi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Vanegas</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Bex</surname>
            <given-names>P.J.</given-names>
          </name>
          <name>
            <surname>Maiello</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Evaluation of the Tobii EyeX Eye tracking controller and Matlab toolkit for research</article-title>
        <source>Behav. Res. Methods</source>
        <year>2017</year>
        <volume>49</volume>
        <fpage>923</fpage>
        <lpage>946</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-016-0762-9</pub-id>
        <?supplied-pmid 27401169?>
        <pub-id pub-id-type="pmid">27401169</pub-id>
      </element-citation>
    </ref>
    <ref id="B6-vision-03-00055">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>MacInnes</surname>
            <given-names>J.J.</given-names>
          </name>
          <name>
            <surname>Iqbal</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Pearson</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>E.N.</given-names>
          </name>
        </person-group>
        <article-title>Wearable Eye-tracking for Research: Automated dynamic gaze mapping and accuracy/precision comparisons across devices</article-title>
        <source>Neuroscience</source>
        <year>2018</year>
        <fpage>299925</fpage>
        <pub-id pub-id-type="doi">10.1101/299925</pub-id>
      </element-citation>
    </ref>
    <ref id="B7-vision-03-00055">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kasprowski</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Harezlak</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>SoftwareX ETCAL—A versatile and extendable library for eye tracker calibration</article-title>
        <source>Digit. Signal Process.</source>
        <year>2018</year>
        <volume>8</volume>
        <fpage>71</fpage>
        <lpage>76</lpage>
      </element-citation>
    </ref>
    <ref id="B8-vision-03-00055">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kar</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Corcoran</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Performance Evaluation Strategies for Eye Gaze Estimation Systems with Quantitative Metrics and Visualizations</article-title>
        <source>Sensors</source>
        <year>2018</year>
        <volume>18</volume>
        <elocation-id>3151</elocation-id>
        <pub-id pub-id-type="doi">10.3390/s18093151</pub-id>
      </element-citation>
    </ref>
    <ref id="B9-vision-03-00055">
      <label>9.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Špakov</surname>
            <given-names>O.</given-names>
          </name>
        </person-group>
        <article-title>iComponent-Device-Independent Platform for Analyzing Eye Movement Data and Developing Eye-based Applications. Dissert</article-title>
        <source>Interactive Technology</source>
        <publisher-name>University of Tampere</publisher-name>
        <publisher-loc>Tampere, Finland</publisher-loc>
        <year>2008</year>
        <volume>Volume 9</volume>
      </element-citation>
    </ref>
    <ref id="B10-vision-03-00055">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dalmaijer</surname>
            <given-names>E.S.</given-names>
          </name>
          <name>
            <surname>Mathôt</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Van der Stigchel</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>PyGaze: An opensource, cross-platform toolbox for minimal-effort programming of eye tracking experiments</article-title>
        <source>Behav. Res. Methods</source>
        <year>2014</year>
        <volume>46</volume>
        <fpage>913</fpage>
        <lpage>921</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-013-0422-2</pub-id>
        <pub-id pub-id-type="pmid">24258321</pub-id>
      </element-citation>
    </ref>
    <ref id="B11-vision-03-00055">
      <label>11.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Tula</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Kurauchi</surname>
            <given-names>A.T.N.</given-names>
          </name>
          <name>
            <surname>Coutinho</surname>
            <given-names>F.L.</given-names>
          </name>
          <name>
            <surname>Morimoto</surname>
            <given-names>C.H.</given-names>
          </name>
        </person-group>
        <article-title>Heatmap Explorer: An interactive gaze data visualization tool for the evaluation of computer interfaces</article-title>
        <source>Proceedings of the ACM IHC ’16</source>
        <conf-loc>New York, NY, USA</conf-loc>
        <conf-date>4–7 October 2016</conf-date>
        <comment>Article 24</comment>
        <fpage>1</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="B12-vision-03-00055">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van Renswoude</surname>
            <given-names>D.R.</given-names>
          </name>
          <name>
            <surname>Raijmakers</surname>
            <given-names>M.E.J.</given-names>
          </name>
          <name>
            <surname>Koornneef</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>S.P.</given-names>
          </name>
          <name>
            <surname>Hunnius</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Visser</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <article-title>Gazepath: An eye-tracking analysis tool that accounts for individual differences and data quality</article-title>
        <source>Behav. Res. Methods</source>
        <year>2018</year>
        <volume>50</volume>
        <fpage>834</fpage>
        <lpage>852</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-017-0909-3</pub-id>
        <pub-id pub-id-type="pmid">28593606</pub-id>
      </element-citation>
    </ref>
    <ref id="B13-vision-03-00055">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Voßkühler</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Nordmeier</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Kuchinke</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Jacobs</surname>
            <given-names>A.M.</given-names>
          </name>
        </person-group>
        <article-title>OGAMA (Open Gaze and Mouse Analyzer): Open-source software designed to analyze eye and mouse movements in slideshow study designs</article-title>
        <source>Behav. Res. Methods</source>
        <year>2008</year>
        <volume>40</volume>
        <fpage>1150</fpage>
        <lpage>1162</lpage>
        <pub-id pub-id-type="doi">10.3758/BRM.40.4.1150</pub-id>
        <pub-id pub-id-type="pmid">19001407</pub-id>
      </element-citation>
    </ref>
    <ref id="B14-vision-03-00055">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kar</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Corcoran</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>GazeVisual—A Practical Software Tool and Web Application for Performance Evaluation of Eye Tracking Systems</article-title>
        <source>IEEE Trans. Consum. Electron.</source>
        <year>2019</year>
        <volume>65</volume>
        <fpage>293</fpage>
        <lpage>302</lpage>
        <pub-id pub-id-type="doi">10.1109/TCE.2019.2912802</pub-id>
      </element-citation>
    </ref>
    <ref id="B15-vision-03-00055">
      <label>15.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Canessa</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gibaldi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Chessa</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Paolo</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <source>The Perspective Geometry of the Eye: Toward Image-Based Eye-Tracking</source>
        <publisher-name>IntechOpen</publisher-name>
        <publisher-loc>London, UK</publisher-loc>
        <year>2012</year>
        <fpage>1</fpage>
        <lpage>30</lpage>
      </element-citation>
    </ref>
    <ref id="B16-vision-03-00055">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Balasubramanyam</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Hanna</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Pavan</surname>
            <given-names>K.B.N.</given-names>
          </name>
          <name>
            <surname>Chai</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Calibration Techniques and Gaze Accuracy Estimation in Pupil Labs Eye Tracker</article-title>
        <source>TECHART J. Arts Imaging Sci.</source>
        <year>2018</year>
        <volume>5</volume>
        <fpage>38</fpage>
        <lpage>41</lpage>
      </element-citation>
    </ref>
    <ref id="B17-vision-03-00055">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sogo</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>GazeParser: An open-source and multiplatform library for low-cost eye tracking and analysis</article-title>
        <source>Behav. Res. Methods</source>
        <year>2013</year>
        <volume>45</volume>
        <fpage>684</fpage>
        <lpage>695</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-012-0286-x</pub-id>
        <?supplied-pmid 23239074?>
        <pub-id pub-id-type="pmid">23239074</pub-id>
      </element-citation>
    </ref>
    <ref id="B18-vision-03-00055">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Krassanakis</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Filippakopoulou</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Nakos</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>EyeMMV toolbox: An eye movement post-analysis tool based on a two-step spatial dispersion threshold for fixation identification</article-title>
        <source>J. Eye Mov. Res.</source>
        <year>2007</year>
        <volume>7</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="B19-vision-03-00055">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Berger</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Winkels</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Lischke</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Hoeppner</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>GazeAlyze: A MATLAB toolbox for the analysis of eye movement data</article-title>
        <source>Behav. Res. Method</source>
        <year>2012</year>
        <volume>44</volume>
        <fpage>404</fpage>
        <lpage>419</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-011-0149-x</pub-id>
      </element-citation>
    </ref>
    <ref id="B20-vision-03-00055">
      <label>20.</label>
      <element-citation publication-type="web">
        <article-title>EMA Toolbox</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://sourceforge.net/projects/ema-toolbox/">https://sourceforge.net/projects/ema-toolbox/</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2019-06-28">(accessed on 28 June 2019)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B21-vision-03-00055">
      <label>21.</label>
      <element-citation publication-type="web">
        <article-title>PyTrack</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://github.com/titoghose/PyTrack">https://github.com/titoghose/PyTrack</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2019-06-28">(accessed on 28 June 2019)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B22-vision-03-00055">
      <label>22.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Smith</surname>
            <given-names>B.A.</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Feiner</surname>
            <given-names>S.K.</given-names>
          </name>
          <name>
            <surname>Nayar</surname>
            <given-names>S.K.</given-names>
          </name>
        </person-group>
        <article-title>Gaze locking: Passive eye contact detection for human-object interaction</article-title>
        <source>Proceedings of the 26th annual ACM UIST symposium on User interface software and technology</source>
        <conf-loc>St. Andrews, UK</conf-loc>
        <conf-date>8–11 October 2013</conf-date>
        <fpage>271</fpage>
        <lpage>280</lpage>
      </element-citation>
    </ref>
    <ref id="B23-vision-03-00055">
      <label>23.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Weidenbacher</surname>
            <given-names>U.</given-names>
          </name>
          <name>
            <surname>Layher</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Strauss</surname>
            <given-names>P.-M.</given-names>
          </name>
          <name>
            <surname>Neumann</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>A comprehensive head pose and gaze database</article-title>
        <source>Proceedings of the 3rd IET International Conference on Intelligent Environments (IE 07)</source>
        <conf-loc>Ulm, Germany</conf-loc>
        <conf-date>24–25 September 2007</conf-date>
        <fpage>455</fpage>
        <lpage>458</lpage>
      </element-citation>
    </ref>
    <ref id="B24-vision-03-00055">
      <label>24.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>McMurrough</surname>
            <given-names>C.D.</given-names>
          </name>
          <name>
            <surname>Metsis</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Rich</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Makedon</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>An eye tracking dataset for point of gaze detection</article-title>
        <source>Proceedings of the Symposium on SDN Research—SOSR’16</source>
        <conf-loc>Santa Barbara, CA, USA</conf-loc>
        <conf-date>28–30 March 2012</conf-date>
        <fpage>305</fpage>
      </element-citation>
    </ref>
    <ref id="B25-vision-03-00055">
      <label>25.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Sugano</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Matsushita</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Sato</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Matsushita</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Sato</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Learning-by-Synthesis for Appearance-Based 3D Gaze Estimation</article-title>
        <source>Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition, Institute of Electrical and Electronics Engineers (IEEE)</source>
        <conf-loc>Washington, DC, USA</conf-loc>
        <conf-date>23–28 June 2014</conf-date>
        <fpage>1821</fpage>
        <lpage>1828</lpage>
      </element-citation>
    </ref>
    <ref id="B26-vision-03-00055">
      <label>26.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Sugano</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Fritz</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Bulling</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Appearance-based gaze estimation in the wild</article-title>
        <source>Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Institute of Electrical and Electronics Engineers (IEEE)</source>
        <conf-loc>Boston, MA, USA</conf-loc>
        <conf-date>7–12 June 2015</conf-date>
        <fpage>4511</fpage>
        <lpage>4520</lpage>
      </element-citation>
    </ref>
    <ref id="B27-vision-03-00055">
      <label>27.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Hong</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Chai</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Holappa</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Pietikäinen</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>OMEG: Oulu Multi-Pose Eye Gaze Dataset</article-title>
        <source>Lecture Notes in Computer Science</source>
        <comment>Image Analysis, SCIA</comment>
        <person-group person-group-type="editor">
          <name>
            <surname>Paulsen</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Pedersen</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Cham, Switzerland</publisher-loc>
        <year>2015</year>
        <volume>Volume 9127</volume>
      </element-citation>
    </ref>
    <ref id="B28-vision-03-00055">
      <label>28.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Busso</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Evaluating the robustness of an appearance-based gaze estimation method for multimodal interfaces</article-title>
        <source>Proceedings of the 15th ACM on International conference on multimodal interaction (ICMI ’13)</source>
        <conf-loc>New York, NY, USA</conf-loc>
        <conf-date>9–13 December 2013</conf-date>
        <fpage>91</fpage>
        <lpage>98</lpage>
      </element-citation>
    </ref>
    <ref id="B29-vision-03-00055">
      <label>29.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Funes Mora</surname>
            <given-names>K.A.</given-names>
          </name>
          <name>
            <surname>Monay</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Odobez</surname>
            <given-names>J.M.</given-names>
          </name>
        </person-group>
        <article-title>EYEDIAP: A Database for the Development and Evaluation of Gaze Estimation Algorithms from RGB and RGB-16 T</article-title>
        <source>ACM Symposium on Eye Tracking Research and Applications</source>
        <person-group person-group-type="editor">
          <name>
            <surname>Fischer</surname>
            <given-names>H.J.</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Demiris</surname>
            <given-names>D.C.</given-names>
          </name>
        </person-group>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>Safety Harbor, FL, USA</publisher-loc>
        <year>2014</year>
        <fpage>255</fpage>
        <lpage>258</lpage>
      </element-citation>
    </ref>
    <ref id="B30-vision-03-00055">
      <label>30.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Erdogmus</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Marcel</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Spoofing in 2D face recognition with 3D masks and anti-spoofing with Kinect</article-title>
        <source>Proceedings of the 2013 IEEE Sixth International Conference on Biometrics: Theory, Applications and Systems (BTAS), Institute of Electrical and Electronics Engineers (IEEE)</source>
        <conf-loc>Arlington, VA, USA</conf-loc>
        <conf-date>29 September–2 October 2013</conf-date>
        <fpage>1</fpage>
        <lpage>6</lpage>
      </element-citation>
    </ref>
    <ref id="B31-vision-03-00055">
      <label>31.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Asteriadis</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Soufleros</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Karpouzis</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Kollias</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>A natural head pose and eye gaze dataset</article-title>
        <source>Proceedings of the International Workshop on Affective Aware Virtual Agents and Social Robots AFFINE 09</source>
        <conf-loc>Boston, MA, USA</conf-loc>
        <conf-date>6 November 2009</conf-date>
        <publisher-name>ACM Press</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2009</year>
        <fpage>1</fpage>
        <lpage>4</lpage>
      </element-citation>
    </ref>
    <ref id="B32-vision-03-00055">
      <label>32.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Martinikorena</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Cabeza</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Villanueva</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Porta</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Introducing I2head database</article-title>
        <source>Proceedings of the ACM 7th Workshop on Pervasive Eye Tracking and Mobile Eye-Based Interaction (PETMEI ’18)</source>
        <conf-loc>New York, NY, USA</conf-loc>
        <conf-date>15–16 June 2018</conf-date>
        <comment>Article 1</comment>
        <fpage>7</fpage>
      </element-citation>
    </ref>
    <ref id="B33-vision-03-00055">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hadizadeh</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Enriquez</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Bajic</surname>
            <given-names>I.V.</given-names>
          </name>
        </person-group>
        <article-title>Eye-Tracking Database for a Set of Standard Video Sequences</article-title>
        <source>IEEE Trans. Image Process.</source>
        <year>2012</year>
        <volume>21</volume>
        <fpage>898</fpage>
        <lpage>903</lpage>
        <pub-id pub-id-type="doi">10.1109/TIP.2011.2165292</pub-id>
        <pub-id pub-id-type="pmid">21859619</pub-id>
      </element-citation>
    </ref>
    <ref id="B34-vision-03-00055">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bovik</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Cormack</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Van Der Linde</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Rajashekar</surname>
            <given-names>U.</given-names>
          </name>
        </person-group>
        <article-title>DOVES: A database of visual eye movements</article-title>
        <source>Spat. Vis.</source>
        <year>2009</year>
        <volume>22</volume>
        <fpage>161</fpage>
        <lpage>177</lpage>
        <pub-id pub-id-type="doi">10.1163/156856809787465636</pub-id>
        <pub-id pub-id-type="pmid">19228456</pub-id>
      </element-citation>
    </ref>
    <ref id="B35-vision-03-00055">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hickman</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Firestone</surname>
            <given-names>A.R.</given-names>
          </name>
          <name>
            <surname>Beck</surname>
            <given-names>F.M.</given-names>
          </name>
          <name>
            <surname>Speer</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Eye fixations when viewing faces</article-title>
        <source>J. Am. Dent. Assoc.</source>
        <year>2010</year>
        <volume>141</volume>
        <fpage>40</fpage>
        <lpage>46</lpage>
        <pub-id pub-id-type="doi">10.14219/jada.archive.2010.0019</pub-id>
        <pub-id pub-id-type="pmid">20045820</pub-id>
      </element-citation>
    </ref>
    <ref id="B36-vision-03-00055">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kootstra</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>De Boer</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Schomaker</surname>
            <given-names>L.R.B.</given-names>
          </name>
        </person-group>
        <article-title>Predicting Eye Fixations on Complex Visual Stimuli Using Local Symmetry</article-title>
        <source>Cogn. Comput.</source>
        <year>2011</year>
        <volume>3</volume>
        <fpage>223</fpage>
        <lpage>240</lpage>
        <pub-id pub-id-type="doi">10.1007/s12559-010-9089-5</pub-id>
      </element-citation>
    </ref>
    <ref id="B37-vision-03-00055">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Levine</surname>
            <given-names>M.D.</given-names>
          </name>
          <name>
            <surname>An</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Visual Saliency Based on Scale-Space Analysis in the Frequency Domain</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2013</year>
        <volume>35</volume>
        <fpage>996</fpage>
        <lpage>1010</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2012.147</pub-id>
        <pub-id pub-id-type="pmid">22802112</pub-id>
      </element-citation>
    </ref>
    <ref id="B38-vision-03-00055">
      <label>38.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Judd</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Ehinger</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Durand</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Torralba</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Learning to predict where humans look</article-title>
        <source>Proceedings of the IEEE 12th International Conference on Computer Vision</source>
        <conf-loc>Kyoto, Japan</conf-loc>
        <conf-date>29 September–2 October 2009</conf-date>
        <fpage>2106</fpage>
        <lpage>2113</lpage>
      </element-citation>
    </ref>
    <ref id="B39-vision-03-00055">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Judd</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Durand</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Torralba</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Fixations on low-resolution images</article-title>
        <source>J. Vis.</source>
        <year>2011</year>
        <volume>11</volume>
        <fpage>14</fpage>
        <pub-id pub-id-type="doi">10.1167/11.4.14</pub-id>
      </element-citation>
    </ref>
    <ref id="B40-vision-03-00055">
      <label>40.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Ramanathan</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Katti</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Sebe</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Kankanhalli</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Chua</surname>
            <given-names>T.S.</given-names>
          </name>
        </person-group>
        <article-title>An Eye Fixation Database for Saliency Detection in Images</article-title>
        <source>Computer Vision–ECCV 2010</source>
        <comment>ECCV 2010</comment>
        <person-group person-group-type="editor">
          <name>
            <surname>Daniilidis</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Maragos</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Paragios</surname>
            <given-names>N.</given-names>
          </name>
          <collab>Lecture Notes in Computer Science</collab>
        </person-group>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
        <year>2010</year>
        <volume>Volume 6314</volume>
      </element-citation>
    </ref>
    <ref id="B41-vision-03-00055">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bruce</surname>
            <given-names>N.D.B.</given-names>
          </name>
          <name>
            <surname>Tsotsos</surname>
            <given-names>J.K.</given-names>
          </name>
        </person-group>
        <article-title>Saliency, attention, and visual search: An information theoretic approach</article-title>
        <source>J. Vis.</source>
        <year>2009</year>
        <volume>9</volume>
        <fpage>5</fpage>
        <pub-id pub-id-type="doi">10.1167/9.3.5</pub-id>
        <?supplied-pmid 19757944?>
        <pub-id pub-id-type="pmid">19757944</pub-id>
      </element-citation>
    </ref>
    <ref id="B42-vision-03-00055">
      <label>42.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Engelke</surname>
            <given-names>U.</given-names>
          </name>
          <name>
            <surname>Maeder</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Zepernick</surname>
            <given-names>H.-J.</given-names>
          </name>
        </person-group>
        <article-title>Visual attention modelling for subjective image quality databases</article-title>
        <source>Proceedings of the 2009 IEEE International Workshop on Multimedia Signal Processing, Institute of Electrical and Electronics Engineers (IEEE)</source>
        <conf-loc>Rio De Janeiro, Brazil</conf-loc>
        <conf-date>5–7 October 2009</conf-date>
        <fpage>1</fpage>
        <lpage>6</lpage>
      </element-citation>
    </ref>
    <ref id="B43-vision-03-00055">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mathe</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sminchisescu</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Actions in the Eye: Dynamic Gaze Datasets and Learnt Saliency Models for Visual Recognition</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2015</year>
        <volume>37</volume>
        <fpage>1408</fpage>
        <lpage>1424</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2014.2366154</pub-id>
        <?supplied-pmid 26352449?>
        <pub-id pub-id-type="pmid">26352449</pub-id>
      </element-citation>
    </ref>
    <ref id="B44-vision-03-00055">
      <label>44.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Duan</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <article-title>SALICON: Saliency in Context</article-title>
        <source>Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
        <conf-loc>Boston, MA, USA</conf-loc>
        <conf-date>7–12 June 2015</conf-date>
        <fpage>1072</fpage>
        <lpage>1080</lpage>
      </element-citation>
    </ref>
    <ref id="B45-vision-03-00055">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Palazzi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Abati</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Calderara</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Solera</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Cucchiara</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Predicting the Driver’s Focus of Attention: The DR (eye) VE Project</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2019</year>
        <volume>41</volume>
        <fpage>1720</fpage>
        <lpage>1733</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2018.2845370</pub-id>
        <?supplied-pmid 29994193?>
        <pub-id pub-id-type="pmid">29994193</pub-id>
      </element-citation>
    </ref>
    <ref id="B46-vision-03-00055">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Kankanhalli</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <article-title>Predicting Human Gaze Beyond Pixels</article-title>
        <source>J. Vis.</source>
        <year>2014</year>
        <volume>14</volume>
        <fpage>1</fpage>
        <lpage>20</lpage>
        <pub-id pub-id-type="doi">10.1167/14.1.28</pub-id>
      </element-citation>
    </ref>
    <ref id="B47-vision-03-00055">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leborán</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>García-Díaz</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Fdez-Vidal</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Pardo</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Dynamic Whitening Saliency</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2017</year>
        <volume>39</volume>
        <fpage>893</fpage>
        <lpage>907</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2016.2567391</pub-id>
        <pub-id pub-id-type="pmid">27187946</pub-id>
      </element-citation>
    </ref>
    <ref id="B48-vision-03-00055">
      <label>48.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <name>
            <surname>Bednarik</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Busjahn</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Gibaldi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Sharif</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Bielikova</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Tvarozek</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>The EMIP Dataset; Technical Report</article-title>
        <year>2018</year>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://emipws.org/emip_dataset/">http://emipws.org/emip_dataset/</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2019-06-28">(accessed on 28 June 2019)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B49-vision-03-00055">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Harezlak</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Kasprowski</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Stasch</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Towards Accurate Eye Tracker Calibration–Methods and Procedures</article-title>
        <source>Procedia Comput. Sci.</source>
        <year>2014</year>
        <volume>35</volume>
        <fpage>1073</fpage>
        <lpage>1081</lpage>
        <pub-id pub-id-type="doi">10.1016/j.procs.2014.08.194</pub-id>
      </element-citation>
    </ref>
    <ref id="B50-vision-03-00055">
      <label>50.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Piumsomboon</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Lindeman</surname>
            <given-names>R.W.</given-names>
          </name>
          <name>
            <surname>Billinghurst</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Exploring natural eye-gaze-based interaction for immersive virtual reality</article-title>
        <source>Proceedings of the 2017 IEEE Symposium on 3D User Interfaces (3DUI), Institute of Electrical and Electronics Engineers (IEEE)</source>
        <conf-loc>Los Angeles, CA, USA</conf-loc>
        <conf-date>18–19 March 2017</conf-date>
        <fpage>36</fpage>
        <lpage>39</lpage>
      </element-citation>
    </ref>
    <ref id="B51-vision-03-00055">
      <label>51.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Design and Implementation of an Augmented Reality System Using Gaze Interaction</article-title>
        <source>Proceedings of the 2011 International Conference on Information Science and Applications</source>
        <conf-loc>Jeju Island, Korea</conf-loc>
        <conf-date>26–29 April 2011</conf-date>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="B52-vision-03-00055">
      <label>52.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Biedert</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Dengel</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Buscher</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Vartan</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Reading and estimating gaze on smart phones</article-title>
        <source>Proceedings of the ACM ETRA ‘12</source>
        <conf-loc>New York, NY, USA</conf-loc>
        <conf-date>28–30 March 2012</conf-date>
        <person-group person-group-type="editor">
          <name>
            <surname>Stephen</surname>
            <given-names>N.S.</given-names>
          </name>
        </person-group>
        <year>2012</year>
        <fpage>385</fpage>
        <lpage>388</lpage>
      </element-citation>
    </ref>
    <ref id="B53-vision-03-00055">
      <label>53.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>Y.-L.</given-names>
          </name>
          <name>
            <surname>Chiang</surname>
            <given-names>C.-Y.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>C.-W.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>W.-C.</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>S.-M.</given-names>
          </name>
        </person-group>
        <article-title>Real-time eye tracking and event identification techniques for smart TV applications</article-title>
        <source>Proceedings of the 2014 IEEE International Conference on Consumer Electronics-Taiwan, Institute of Electrical and Electronics Engineers (IEEE)</source>
        <conf-loc>Taipei, Taiwan</conf-loc>
        <conf-date>26–28 May 2014</conf-date>
        <fpage>63</fpage>
        <lpage>64</lpage>
      </element-citation>
    </ref>
    <ref id="B54-vision-03-00055">
      <label>54.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Ou</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Research on machine learning algorithms and feature extraction for time series</article-title>
        <source>Proceedings of the 2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC), Institute of Electrical and Electronics Engineers (IEEE)</source>
        <conf-loc>Montreal, QC, Canada</conf-loc>
        <conf-date>8–13 October 2017</conf-date>
        <fpage>1</fpage>
        <lpage>5</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="vision-03-00055-f001" orientation="portrait" position="float">
    <label>Figure 1</label>
    <caption>
      <p>Organization of the GazeVisual-Lib code repository on GitHub.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g001"/>
  </fig>
  <fig id="vision-03-00055-f002" orientation="portrait" position="float">
    <label>Figure 2</label>
    <caption>
      <p>View of the GazeVisual-Lib code repository on GitHub.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g002"/>
  </fig>
  <fig id="vision-03-00055-f003" orientation="portrait" position="float">
    <label>Figure 3</label>
    <caption>
      <p>Outputs from using GazeVisual-Lib codes on eye gaze data (<bold>a</bold>) gaze angle, yaw, pitch values as a function of time (<bold>b</bold>) outliers removed from gaze data using IQR method.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g003"/>
  </fig>
  <fig id="vision-03-00055-f004" orientation="portrait" position="float">
    <label>Figure 4</label>
    <caption>
      <p>Outputs from using GazeVisual-Lib codes: (<bold>a</bold>) 2D surface distribution of gaze errors as a function of visual angles; (<bold>b</bold>) 3D plot of gaze errors as a function of display dimensions in pixels.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g004"/>
  </fig>
  <fig id="vision-03-00055-f005" orientation="portrait" position="float">
    <label>Figure 5</label>
    <caption>
      <p>(<bold>a</bold>) Architecture of the GazeVisual software; (<bold>b</bold>) Input data format (CSV) for the software.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g005"/>
  </fig>
  <fig id="vision-03-00055-f006" orientation="portrait" position="float">
    <label>Figure 6</label>
    <caption>
      <p>View of the “Data Analysis” window of the GazeVisual software.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g006"/>
  </fig>
  <fig id="vision-03-00055-f007" orientation="portrait" position="float">
    <label>Figure 7</label>
    <caption>
      <p>View of the “Visualizations” window of the GazeVisual software.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g007"/>
  </fig>
  <fig id="vision-03-00055-f008" orientation="portrait" position="float">
    <label>Figure 8</label>
    <caption>
      <p>Workflow to implement gaze data evaluation methods of the GazeVisual-Lib repository.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g008"/>
  </fig>
  <fig id="vision-03-00055-f009" orientation="portrait" position="float">
    <label>Figure 9</label>
    <caption>
      <p>(<bold>a</bold>) Flowchart for running the GazeVisual-Lib codes. (<bold>b</bold>) Opening the Jupyter Dashboard to locate the downloaded GazeVisual-Lib folder. (<bold>c</bold>) Running the GazeVisual GUI code on Jupyter (arrow shows the paths which have to be changed). (<bold>d</bold>) View of the Jupyter notebook for the Gaze accuracy metrics folder.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g009a"/>
    <graphic xlink:href="vision-03-00055-g009b"/>
  </fig>
  <fig id="vision-03-00055-f010" orientation="portrait" position="float">
    <label>Figure 10</label>
    <caption>
      <p>(<bold>a</bold>) Positioning of a participant for an experiment session (<bold>b</bold>) layout of the stimuli points for data collection. AOI stands for area of interest. Different (<bold>c</bold>) head poses and (<bold>d</bold>) tablet poses under which gaze data was collected.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g010"/>
  </fig>
  <fig id="vision-03-00055-f011" orientation="portrait" position="float">
    <label>Figure 11</label>
    <caption>
      <p>Data from user distance experiments: Desktop (<bold>a</bold>) 50 cm, (<bold>b</bold>) 60 cm, (<bold>c</bold>) 70 cm, and (<bold>d</bold>) 80 cm.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g011"/>
  </fig>
  <fig id="vision-03-00055-f012" orientation="portrait" position="float">
    <label>Figure 12</label>
    <caption>
      <p>Data from head pose experiments: (<bold>a</bold>) neutral, (<bold>b</bold>) roll +30, (<bold>c</bold>) pitch +20, and (<bold>d</bold>) yaw +30 degrees.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g012"/>
  </fig>
  <fig id="vision-03-00055-f013" orientation="portrait" position="float">
    <label>Figure 13</label>
    <caption>
      <p>Tablet pose experiment data: (<bold>a</bold>) neutral, (<bold>b</bold>) roll +20, (<bold>c</bold>) pitch +20, and (<bold>d</bold>) yaw +20 degrees.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g013"/>
  </fig>
  <fig id="vision-03-00055-f014" orientation="portrait" position="float">
    <label>Figure 14</label>
    <caption>
      <p>Snapshot of the NUIG_EyeGaze01 repository on Mendeley data.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g014"/>
  </fig>
  <fig id="vision-03-00055-f015" orientation="portrait" position="float">
    <label>Figure 15</label>
    <caption>
      <p>Dataset organization in the NUIG_EyeGaze01 repository on Mendeley data.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g015"/>
  </fig>
  <fig id="vision-03-00055-f016" orientation="portrait" position="float">
    <label>Figure 16</label>
    <caption>
      <p>A screenshot of the data format in each comma separated values (CSV) file in the gaze dataset.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g016"/>
  </fig>
  <fig id="vision-03-00055-f017" orientation="portrait" position="float">
    <label>Figure 17</label>
    <caption>
      <p>Python code snippet to read data from a gaze data file from NUIG_EyeGaze01 repository.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g017"/>
  </fig>
  <fig id="vision-03-00055-f018" orientation="portrait" position="float">
    <label>Figure 18</label>
    <caption>
      <p>Gaze error statistics (box plots) from (<bold>a</bold>) desktop experiments and (<bold>b</bold>) tablet experiments. <italic>Y</italic>-axes of the plots represents gaze error in degrees. <italic>X</italic>-axis represents the different experiments from which data was used for plotting.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g018"/>
  </fig>
  <fig id="vision-03-00055-f019" orientation="portrait" position="float">
    <label>Figure 19</label>
    <caption>
      <p>Gaze error distribution due to (<bold>a</bold>) user distance–desktop, (<bold>b</bold>) head pose–desktop, (<bold>c</bold>) user distance–tablet, and (<bold>d</bold>) platform pose–tablet.</p>
    </caption>
    <graphic xlink:href="vision-03-00055-g019"/>
  </fig>
  <table-wrap id="vision-03-00055-t001" orientation="portrait" position="float">
    <object-id pub-id-type="pii">vision-03-00055-t001_Table 1</object-id>
    <label>Table 1</label>
    <caption>
      <p>Eye gaze datasets for building gaze estimation algorithms.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset Name</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Type</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Purpose of the Dataset</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CAVE [<xref rid="B22-vision-03-00055" ref-type="bibr">22</xref>]</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">images</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dataset built to train a detector to sense eye contact in an image. Can be used for gaze estimation or tracking</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56 subjects (32 male, 24 female). 5880 images, acquired for combinations of five calibrated head poses. Dataset includes users wearing glasses. </td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Weidenbacher et al. [<xref rid="B23-vision-03-00055" ref-type="bibr">23</xref>]</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">images</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Evaluations of computational methods for head pose and eye gaze estimation. Benchmark dataset for Point of Gaze (PoG) detection algorithms</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20 subjects, 2220 color images. Various head poses, nine different gaze conditions for each head pose. Participants with glasses included.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">McMurrough et al. [<xref rid="B24-vision-03-00055" ref-type="bibr">24</xref>]</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">videos</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Training and testing data for appearance-based gaze estimation methods.</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20 subjects. Videos recorded as subjects followed a set of predefined points on a screen. Participants didn’t wear spectacles, free head motion. </td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UT Multiview [<xref rid="B25-vision-03-00055" ref-type="bibr">25</xref>]</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">images</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Building and testing 3D gaze estimation algorithms</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50 (15 female and 35 male) subjects. 160 gaze directions per person were acquired using eight cameras. 64,000 eye images, 8000 3D eye shape models, 1,152,000 gaze directions. </td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MPII Gaze [<xref rid="B26-vision-03-00055" ref-type="bibr">26</xref>]</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">images</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">For appearance-based gaze estimation in the wild.</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15 participants. 213,659 images. some samples manually annotated with 6 facial landmarks and pupil centers. Free head motion, uncontrolled illumination.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OMEG: Oulu Multi-Pose Eye Gaze Dataset [<xref rid="B27-vision-03-00055" ref-type="bibr">27</xref>]</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">images</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Evaluating and comparing gaze tracking algorithms.</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50 subjects. Over 40,000 images captured under fixed and free head poses. Five landmark labels and gaze angles are provided as ground truth.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MSP Gaze corpus [<xref rid="B28-vision-03-00055" ref-type="bibr">28</xref>]</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">videos</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">For appearance-based, user dependent, or independent gaze estimators.</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">46 subjects. Videos with/without head movement, different user distance, free head motion. </td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EYEDIAP [<xref rid="B29-vision-03-00055" ref-type="bibr">29</xref>]</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">videos</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">For training and evaluation of gaze estimation algorithms from RGB and RGB-D data.</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16 subjects (12 male, four female). 94 session recordings, each with different characteristics of ambient conditions and types of targets.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3D mask attack dataset [<xref rid="B30-vision-03-00055" ref-type="bibr">30</xref>]</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">videos</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Testing biometric face spoofing attacks.</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">17 subjects. 76,500 frames recorded using Kinect for. Eye-positions manually labelled in each video.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HPEG [<xref rid="B31-vision-03-00055" ref-type="bibr">31</xref>]</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">videos</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">For testing head pose and eye gaze estimation algorithm.</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10 subjects (two female, eight male). Free movement of subjects, 20 color video sequences.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">I2Head [<xref rid="B32-vision-03-00055" ref-type="bibr">32</xref>]</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">videos</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reference dataset for low-cost gaze estimation.</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12 subjects. Dataset contains head pose, gaze and user face models. Webcam is and head pose sensors used.</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="vision-03-00055-t002" orientation="portrait" position="float">
    <object-id pub-id-type="pii">vision-03-00055-t002_Table 2</object-id>
    <label>Table 2</label>
    <caption>
      <p>Eye gaze datasets for saliency/cognitive studies.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset Name</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hadizadeh et al. [<xref rid="B33-vision-03-00055" ref-type="bibr">33</xref>] </td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Video database for computational models of visual attention. Twelve video sequences with eye-tracking data. Gaze fixations recorded using a head-mounted eye-tracker 15 participants (two women and 13 men).</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DOVES [<xref rid="B34-vision-03-00055" ref-type="bibr">34</xref>]</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Benchmark for testing gaze modelling algorithms. Contains fixation coordinates and eye movement trajectories of 29 observers as they viewed 101 natural calibrated images and 30,000 fixation points.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fixations in Faces (FiFA) [<xref rid="B35-vision-03-00055" ref-type="bibr">35</xref>] </td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recorded to demonstrate that faces attract significant visual attention while viewing images through free-viewing, search, and memory tasks.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KTH Eye-tracking Dataset [<xref rid="B36-vision-03-00055" ref-type="bibr">36</xref>]</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Comprises of complex photographic images and was used to validate a saliency model predicting interesting image regions. The study concluded that early eye fixations are observed in symmetrical image areas.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">McGill ImgSal [<xref rid="B37-vision-03-00055" ref-type="bibr">37</xref>]</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Aims to validate a frequency domain-based saliency detector incorporating scale-space analysis.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIT CSAIL Saliency [<xref rid="B38-vision-03-00055" ref-type="bibr">38</xref>]</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Publicly available, large-scale eye movement database to aid natural image-related visual attention studies. Used to validate a supervised saliency model combining top-down/ bottom-up cues.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIT Low-Resolution Saliency [<xref rid="B39-vision-03-00055" ref-type="bibr">39</xref>]</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Compiled to study how image resolution affects consistency in eye fixations across observers. The study noted that eye fixations are biased towards the image center for all resolutions.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NUS Eye Fixation (NUSEF) [<xref rid="B40-vision-03-00055" ref-type="bibr">40</xref>]</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Contains a repository of eye fixations to study viewing patterns on semantically rich and diverse images, including faces, portraits, indoor/outdoor scenes, and affective content.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Toronto Dataset [<xref rid="B41-vision-03-00055" ref-type="bibr">41</xref>]</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Contains eye movement recordings while viewing natural scenes to validate a visual saliency model based on the principle of maximizing scene information.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visual Attention for Image Quality-VAIQ [<xref rid="B42-vision-03-00055" ref-type="bibr">42</xref>] </td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Provides eye-tracking data for reference images from three image quality databases to validate the hypothesis that salient image regions should contribute more to objective image quality metrics.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Actions in the Eye Dataset [<xref rid="B43-vision-03-00055" ref-type="bibr">43</xref>] </td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Two subject groups—an active group of 12 subjects performed action recognition, while a second group of four subjects free-viewed the videos. Fixation patterns of free and active.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SALICON [<xref rid="B44-vision-03-00055" ref-type="bibr">44</xref>] </td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Saliency in Context eye tracking dataset, 1000 images with eye-tracking data in 80 image classes.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DR(EYE)VE [<xref rid="B45-vision-03-00055" ref-type="bibr">45</xref>]</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Seventy-four video sequences of 5 min each, captured and annotated more than 500,000 frames. The labeling contains drivers’ gaze fixations and their temporal integration.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OSIE [<xref rid="B46-vision-03-00055" ref-type="bibr">46</xref>]</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Seven hundred images, 5551 segmented objects, eye tracking data.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CITIUS [<xref rid="B47-vision-03-00055" ref-type="bibr">47</xref>] </td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A database of 72 videos with eye-tracking data to evaluate dynamic saliency visual models. </td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Eye Movements in Programming (EMIP) [<xref rid="B48-vision-03-00055" ref-type="bibr">48</xref>] </td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">The EMP dataset contains eye movement data during program comprehension data on eye movement parameters such as horizontal and vertical pupil positions, pupil center, diameter, corneal reflex positions, gaze vector, and point of regard, along with programming experience of participants in various languages.</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="vision-03-00055-t003" orientation="portrait" position="float">
    <object-id pub-id-type="pii">vision-03-00055-t003_Table 3</object-id>
    <label>Table 3</label>
    <caption>
      <p>Features of GazeVisual-Lib repository.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Repository Parameters</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">File types in repository</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">py and .IPYNB (Python scripts and Jupyter notebooks)</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Link to repository </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <uri xlink:href="https://github.com/anuradhakar49/GazeVisual-Lib">https://github.com/anuradhakar49/GazeVisual-Lib</uri>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Legal Code License</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">GNU General Public License v3.0</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Software languages used</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">Python</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Operating environments &amp; dependencies</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">Operating environment: Python 2.7; Dependencies: Python libraries Tkinter, Pygame, Statsmodels, Seaborn, CSV, Pandas, Sklearn. Scipy, Numpy, Matplotlib, PIL</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Latest version date</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">July 2019</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="vision-03-00055-t004" orientation="portrait" position="float">
    <object-id pub-id-type="pii">vision-03-00055-t004_Table 4</object-id>
    <label>Table 4</label>
    <caption>
      <p>Features of the gaze data collection process for building the NUIG_EyeGaze01 dataset.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Data type</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fixations</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Data file type</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CSV</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Eye tracker </td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tobii EyeX 4C with specified accuracy of 0.5 degrees.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Data collection platforms</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Desktop, tablet, laptop</td>
        </tr>
        <tr>
          <td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Screen sizes and resolutions</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Desktop: 22 inch diagonal, 1680 × 1050 pixels </td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Laptop: 14 inch diagonal, 1366 × 768 pixels </td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tablet: 10.1 inch diagonal, 1920 × 800 pixels</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visual stimuli details</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Moving dot stopping for 3 s at 15 locations on display screen. Stimulus dot size 10 pixels, color black.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of Participants</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Twenty for all experiments, 15 male, five female, no glasses.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ambient illumination</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Constant, 60 Lux, Indoor setup.</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Chin rest</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Used for all sessions, Head poses measured using webcam with real time head pose model.</td>
        </tr>
        <tr>
          <td rowspan="7" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Desktop data details</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gaze data for four user distances: 50, 60, 70, 80 cm </td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Gaze data for following head poses: Neutral, roll plus (10, 20, 30 degree),</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">roll minus (10, 20, 30 degree)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neutral, pitch plus (10, 20 degree), </td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">pitch minus (10, 20 degree)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neutral, yaw plus (10, 20, 30 degree),</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">yaw minus (10, 20, 30 degree)</td>
        </tr>
        <tr>
          <td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Tablet data details</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gaze data for four user distances: 50,60, 70, 80 cm</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gaze data for following platform poses: </td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neutral, roll plus 20 degree, minus 20 degree</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neutral, pitch plus 10 degree, plus 20 degree</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neutral, yaw plus 20 degree, minus 20 degree</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Laptop data details</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gaze data for four user distances: 50, 60, 70, 80 cm</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="vision-03-00055-t005" orientation="portrait" position="float">
    <object-id pub-id-type="pii">vision-03-00055-t005_Table 5</object-id>
    <label>Table 5</label>
    <caption>
      <p>Columns in a gaze data file from the NUIG_EyeGaze01dataset and their meaning.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Description</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">“TIM REL”</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Relative time stamp for each gaze data point in the file (measured during data collection)</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">“GTX”, “GTY”</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Ground truth x, y positions of stimuli in pixels</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">“XRAW”, “YRAW”</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Raw gaze data x, y coordinates in pixels</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">“GT Xmm”, “GT Ymm”</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Ground truth x, y positions in mm, converted using the pixel pitch value</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">“Xmm”, “Ymm”</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Gaze x, y positions in mm, converted using the pixel pitch value</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">“YAW GT”, “YAW DATA”</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Ground truth and estimated yaw angles from input gaze data</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">“PITCH GT”, “PITCH DATA”</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Ground truth and estimated pitch angles from input gaze data</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">“GAZE GT”, ”GAZE ANG”</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Ground truth and estimated gaze primary angles from input gaze data</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">“DIFF GZ”</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Difference between ground truth and estimated gaze primary angles, i.e., Gaze angular accuracy, Index of each stimulus point</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">“AOI_X”, ”AOI_Y”</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Index of each stimulus position X, Y coordinates of each stimulus position</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">“MEAN_ERR”, “STD ERR”</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean and standard deviation of gaze estimation error at each stimulus position</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="vision-03-00055-t006" orientation="portrait" position="float">
    <object-id pub-id-type="pii">vision-03-00055-t006_Table 6</object-id>
    <label>Table 6</label>
    <caption>
      <p>Gaze error statistics from desktop experiments (table values in degrees).</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UD50</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UD60</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UD70</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UD80</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Roll 20</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Yaw 20</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Pitch 20</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Mean</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.37</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.04</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.21</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.02</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.7</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">8.51</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.15</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">MAD</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.49</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.77</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.82</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.66</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.63</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">10.0</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.90</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">IQR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.13</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.77</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.76</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.79</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.21</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.49</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.59</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95% interval</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.15–3.59</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.90–2.18</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.15–1.26</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.16–1.24</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.30–4.09</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.60–9.43</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.83–3.47</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="vision-03-00055-t007" orientation="portrait" position="float">
    <object-id pub-id-type="pii">vision-03-00055-t007_Table 7</object-id>
    <label>Table 7</label>
    <caption>
      <p>Gaze error statistics from tablet experiments (table values in degrees).</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UD50</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UD60</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UD70</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UD80</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Roll 20</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Yaw 20</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Pitch 20</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Mean</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.68</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.46</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.59</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.55</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">7.74</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4.25</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.45</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">MAD</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.38</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.42</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.29</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.24</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.77</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.60</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.46</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">IQR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.39</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.54</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.33</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.22</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.75</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.53</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.23</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95% interval</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.65–2.71</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.43–2.48</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.57–0.61</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.53–1.57</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.69–7.80</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.22–4.29</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.41–2.49</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
