<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr SoftwareX?>
<?submitter-system nihms?>
<?submitter-canonical-name Elsevier?>
<?submitter-canonical-id ELSEVIERAM?>
<?submitter-userid 8068823?>
<?submitter-authority myNCBI?>
<?submitter-login elsevieram?>
<?submitter-name Elsevier Author Support?>
<?domain nihpa?>
<?ppubseason Jul-Dec?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">101660267</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">43949</journal-id>
    <journal-id journal-id-type="nlm-ta">SoftwareX</journal-id>
    <journal-id journal-id-type="iso-abbrev">SoftwareX</journal-id>
    <journal-title-group>
      <journal-title>SoftwareX</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2352-7110</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7453838</article-id>
    <article-id pub-id-type="doi">10.1016/j.softx.2019.100333</article-id>
    <article-id pub-id-type="manuscript">nihpa1546289</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CameraTransform: A Python package for perspective corrections and image mapping</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Gerum</surname>
          <given-names>Richard C.</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
        <xref rid="CR1" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Richter</surname>
          <given-names>Sebastian</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
        <xref ref-type="aff" rid="A2">b</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Winterl</surname>
          <given-names>Alexander</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
        <xref ref-type="aff" rid="A2">b</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mark</surname>
          <given-names>Christoph</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fabry</surname>
          <given-names>Ben</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bohec</surname>
          <given-names>Céline Le</given-names>
        </name>
        <xref ref-type="aff" rid="A3">c</xref>
        <xref ref-type="aff" rid="A4">d</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zitterbart</surname>
          <given-names>Daniel P.</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
        <xref ref-type="aff" rid="A2">b</xref>
      </contrib>
    </contrib-group>
    <aff id="A1"><label>a</label>Biophysics Group, Department of Physics, University of Erlangen-Nürnberg, Germany</aff>
    <aff id="A2"><label>b</label>Applied Ocean Physics and Engineering, Woods Hole Oceanographic Institution, Woods Hole, MA, USA</aff>
    <aff id="A3"><label>c</label>Centre Scientifique de Monaco, Département de Biologie Polaire, Principality of Monaco, Monaco</aff>
    <aff id="A4"><label>d</label>Université de Strasbourg, CNRS, IPHC, UMR 7178, Strasbourg, France</aff>
    <author-notes>
      <corresp id="CR1"><label>*</label>Corresponding author. <email>richard.gerum@fau.de</email> (R.C. Gerum).</corresp>
      <fn fn-type="con" id="FN1">
        <p id="P1">CRediT authorship contribution statement</p>
        <p id="P2"><bold>Richard C. Gerum:</bold> Conceptualization, Formal analysis, Methodology, Software, Visualisation, Vailidation, Writing - original draft, Writing - review &amp; editing. <bold>Sebastian Richter:</bold> Investigation, Software, Data curation, Validation, Writing - review &amp; editing. <bold>Alexander Winterl:</bold> Investigation, Software, Data curation, Validation. <bold>Christoph Mark:</bold> Software, Writing - review &amp; editing. <bold>Ben Fabry:</bold> Project administration, Supervision, Writing - original draft, Writing - review &amp; editing, Funding acquistion. <bold>Céline Le Bohec:</bold> Investigation, Writing - review &amp; editing, Funding acquistion. <bold>Daniel P. Zitterbart:</bold> Investigation, Supervision, Writing - original draft, Writing - review &amp; editing, Funding acquistion.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>31</day>
      <month>1</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>3</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <season>Jul-Dec</season>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>28</day>
      <month>8</month>
      <year>2020</year>
    </pub-date>
    <volume>10</volume>
    <elocation-id>100333</elocation-id>
    <permissions>
      <license license-type="open-access">
        <license-p>This is an open access article under the CC BY license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="https://www.sciencedirect.com/science/article/pii/S2352711019302018?via%3Dihub"/>
    <abstract id="ABS1">
      <p id="P3">Camera images and video recordings are simple and non-invasive tools to investigate animals in their natural habitat. Quantitative evaluations, however, often require an exact reconstruction of object positions, sizes, and distances in the image. Here, we provide an open source software package to perform such calculations. Our approach allows the user to correct for perspective distortion, transform images to “bird’s-eye" view projections, or transform image-coordinates to real-world coordinates and vice versa. The extrinsic camera parameters that are necessary to perform such image corrections and transformations (elevation, tilt/roll angle, and heading of the camera) are obtained from the image using contextual information such as a visible horizon, GPS coordinates of landmarks, known object sizes, or images of the same object obtained from different viewing angles. All mathematical operations are implemented in the Python package <italic>CameraTransform</italic>. The performance of the implementation is evaluated using computer-generated synthetic images with known camera parameters. Moreover, we test our algorithm on images of emperor penguin colonies, and demonstrate that the camera tilt and roll angles can be estimated with an error of less than one degree, and the camera elevation with an error of less than 5%. The <italic>CameraTransform</italic> software package simplifies camera matrix-based image transformations and the extraction of quantitative image information. An extensive documentation and usage examples in an ecological context are provided at <ext-link ext-link-type="uri" xlink:href="http://cameratransform.readthedocs.io/">http://cameratransform.readthedocs.io</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>Perspective projection</kwd>
      <kwd>Quantitative image analysis</kwd>
      <kwd>Geo-referencing</kwd>
      <kwd>Camera lens distortions</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <table-wrap id="T1" position="anchor" orientation="portrait">
    <table frame="below" rules="none">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="top" style="border-bottom: solid 1px" rowspan="1" colspan="1">Code metadata</th>
          <th align="left" valign="top" style="border-bottom: solid 1px" rowspan="1" colspan="1"/>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1"> Current code version</td>
          <td align="left" valign="top" rowspan="1" colspan="1">v1.1</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1"> Permanent link to code/repository used for this code version</td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <ext-link ext-link-type="uri" xlink:href="https://github.com/ElsevierSoftwareX/SOFTX_2019_198">https://github.com/ElsevierSoftwareX/SOFTX_2019_198</ext-link>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1"> Legal Code License</td>
          <td align="left" valign="top" rowspan="1" colspan="1">MIT</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1"> Code versioning system used</td>
          <td align="left" valign="top" rowspan="1" colspan="1">git</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1"> Software code languages, tools, and services used</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Python, Matplotlib, Pylustrator, SciPy, Numpy, Pandas</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1"> Compilation requirements, operating environments &amp; dependencies</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Python v3.4.x and higher</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1"> If available Link to developer documentation/manual</td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <ext-link ext-link-type="uri" xlink:href="http://cameratransform.readthedocs.org/">http://cameratransform.readthedocs.org/</ext-link>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1"> Support email for questions</td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <email>richard.gerum@fau.de</email>
          </td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <sec id="S1">
    <label>1.</label>
    <title>Introduction</title>
    <p id="P4">Camera traps, time-lapse recordings, or video recordings are widely used tools in ecology research [<xref rid="R1" ref-type="bibr">1</xref>,<xref rid="R2" ref-type="bibr">2</xref>], for example for estimating abundance [<xref rid="R3" ref-type="bibr">3</xref>], or for behavioral studies [<xref rid="R4" ref-type="bibr">4</xref>,<xref rid="R5" ref-type="bibr">5</xref>]. However, such images inherently contain perspective distortions that need to be accounted for when accurate positions and distances need to be measured. To correct for perspective distortions and to map image points to real-world positions, it is paramount to know the intrinsic and extrinsic camera parameters [<xref rid="R6" ref-type="bibr">6</xref>,<xref rid="R7" ref-type="bibr">7</xref>]. Intrinsic parameters are the sensor and lens properties. Extrinsic parameters are the geographic camera position relative to landmarks in the scene, the camera elevation, tilt/roll angle, and heading. Some of the extrinsic parameters, however, are often difficult or impractical to measure in the field at the time of the recording. Here we present methods to reconstruct unknown extrinsic camera parameters from features in the image. The mathematical procedure behind this reconstruction is based on linear algebra and is implemented in the Python package <italic>CameraTransform</italic>. In addition to reconstructing extrinsic camera parameters, <italic>CameraTransform</italic> provides a toolbox to transform point coordinates in the image to geographic coordinates. In the following, we explain the mathematical details, estimate the reconstruction uncertainties, and describe practical applications.</p>
  </sec>
  <sec id="S2">
    <label>2.</label>
    <title>Software</title>
    <p id="P5">The complete software <italic>CameraTransform</italic>, released under the MIT license, is implemented in Python 3.6 [<xref rid="R8" ref-type="bibr">8</xref>], an interpreted programming language optimized for scientific purposes. For maximum efficiency, several open-source libraries are used. For numerical operations, such as matrix operations, we use the Numpy library [<xref rid="R9" ref-type="bibr">9</xref>]. Statistical distributions are implemented using the SciPy package [<xref rid="R10" ref-type="bibr">10</xref>]. The data are visualized using the Matplotlib [<xref rid="R11" ref-type="bibr">11</xref>] and Pylustrator [<xref rid="R12" ref-type="bibr">12</xref>] libraries, and are stored using the Pandas library [<xref rid="R13" ref-type="bibr">13</xref>].</p>
  </sec>
  <sec id="S3">
    <label>3.</label>
    <title>Camera matrix</title>
    <p id="P6">All information required for mapping real-world points (x, y, z coordinates in meters) to image points are stored in the camera matrix. The camera matrix is expressed in projective coordinates, and is split into two parts – the intrinsic matrix and the extrinsic matrix that correspond to the intrinsic and extrinsic camera parameters, respectively [<xref rid="R14" ref-type="bibr">14</xref>].</p>
    <sec id="S4">
      <label>3.1.</label>
      <title>Intrinsic parameters</title>
      <p id="P7">The intrinsic matrix entries contain information about the focal length f of the camera in mm, the effective sensor dimensions (<italic>w</italic><sub>sensor</sub> × <italic>h</italic><sub>sensor</sub>) in mm, and the image dimensions (<italic>w</italic><sub>image</sub> × <italic>h</italic><sub>image</sub>) in pixels (see <xref rid="F1" ref-type="fig">Fig. 1a</xref>,<xref rid="F1" ref-type="fig">b</xref>). Specifically, the intrinsic matrix entries are the effective focal length <italic>f</italic><sub>pix</sub> and the center of the sensor (<italic>w</italic><sub>image</sub>/2, <italic>h</italic><sub>image</sub>/2)
<disp-formula id="FD1"><label>(1)</label><mml:math display="block" id="M1"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mtext>intr</mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mtext>pix</mml:mtext></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mtext>image</mml:mtext></mml:msub><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mtext>pix</mml:mtext></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mtext>image</mml:mtext></mml:msub><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD2"><label>(2)</label><mml:math display="block" id="M2"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mtext>pix</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext>image</mml:mtext></mml:msub><mml:mo stretchy="false">∕</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext>sensor</mml:mtext></mml:msub></mml:mrow></mml:math></disp-formula></p>
      <p id="P8">The diagonal elements account for the re-scaling from pixels in the image to a position in mm on the sensor. The off-diagonal elements account for the offset between image and sensor coordinates, whereby the origin of the image is at the top left corner, and the origin of the sensor coordinates is at the center.</p>
      <p id="P9"><xref rid="FD1" ref-type="disp-formula">Eq. (1)</xref> only applies for the rectilinear projection of a pinhole camera (camera obscura). The <italic>CameraTransform</italic> package also supports cylindrical or equirectangular projections, which cannot be expressed in matrix notation. These are commonly used for panoramic images (see <xref rid="SD1" ref-type="supplementary-material">Supplementary Information</xref>). The package also supports corrections for radial lens distortions (see <xref rid="SD1" ref-type="supplementary-material">Supplementary Information</xref>).</p>
    </sec>
    <sec id="S5">
      <label>3.2.</label>
      <title>Extrinsic parameters</title>
      <p id="P10">The extrinsic matrix consists of the offset (x, y, z) of the camera relative to an arbitrary fixed real-world reference point (0,0,0). Customarily, (x = 0, y = 0) is the position of the camera, and the z-coordinate of the reference point is the ground level so that z is the elevation of the camera. The x, y plane of the coordinate system is usually the horizontal plane. Furthermore, the extrinsic matrix contains the tilt angle <italic>α</italic><sub>tilt</sub>, the heading angle <italic>α</italic><sub>heading</sub>, and the roll angle <italic>α</italic><sub>roll</sub> of the camera (see <xref rid="F1" ref-type="fig">Fig. 1c</xref>-<xref rid="F1" ref-type="fig">e</xref> and <xref rid="SD1" ref-type="supplementary-material">Supplementary Information</xref>).</p>
      <p id="P11">The extrinsic parameters are used to rotate and translate the intrinsic camera matrix, with the aim to map or project 3-D points from real-world coordinates to 2-D image coordinates. The backprojection from a 2-D image point to 3-D real-world coordinates, however, is inherently underdetermined due to the lack of depth information in the image, and therefore requires one additional constraint, e.g. the z-coordinate of that real-world point or its distance to another real-world point. <xref rid="SD1" ref-type="supplementary-material">Supplementary Information</xref> explains several strategies to perform the rectilinear, cylindrical, or equirectangular backprojection.</p>
    </sec>
  </sec>
  <sec id="S6">
    <label>4.</label>
    <title>Fitting of the extrinsic camera parameters</title>
    <p id="P12">While the intrinsic camera parameters describing camera and lens properties are usually well known, this is often not the case for the extrinsic parameters that define the orientation of the camera. Below, we start with the simplest case where the heading and position of the camera is irrelevant and thus can be set to arbitrary values (e.g. 0). This leaves only three free parameters <italic>elevation</italic>, <italic>tilt</italic> and <italic>roll</italic>, unless the camera was properly horizontally aligned, in which case <italic>roll</italic> is approximately zero. The more complicated case where knowledge of camera heading and position is important, e.g. for multi-camera setups, or if the image needs to be geographically mapped, is described further below.</p>
    <p id="P13">The <italic>CameraTransform</italic> package provides several fitting routines that allow the user to infer the extrinsic parameters from characteristic features in the image.</p>
    <sec id="S7">
      <label>4.1.</label>
      <title>Fitting by object height</title>
      <p id="P14">If the true height of objects in the image is known, for example for a group of animals, or more generally if distances between points seen in the image are known, the camera parameters can be fitted. This works especially well for the tilt angle as it most sensitively affects the apparent object height (see <xref rid="SD1" ref-type="supplementary-material">Supplementary Information</xref>, Fig. B.5b). To evaluate the fit parameter uncertainties, we use Metropolis Monte-Carlo sampling [<xref rid="R16" ref-type="bibr">16</xref>,<xref rid="R17" ref-type="bibr">17</xref>]. The input for this sampling process is a list of base (foot) and top (head) positions of the objects. Optionally, also the position of the horizon, landmarks, or reference objects such as rulers or survey poles can be provided to improve the algorithm (for details see below). The algorithm projects the foot positions from the image to real-world coordinates, using the constraint z = 0, then projects the head positions from the image to real-world coordinates, using as a constraint the x position of the projected foot points. The distance between these pairs of points is the estimated height of the object. This height is assigned a probability according to a known height distribution (e.g. the user does not need to know the exact height but can instead provide an estimate for the mean and standard deviation, or any non-Gaussian distributions of the expected object heights).</p>
      <p id="P15">The Metropolis algorithm starts with arbitrarily chosen parameter values (for elevation, tilt, and roll) and evaluates the probability <italic>p</italic><sub>0</sub> assigned to these parameter values. Optionally, the user can provide starting values, but usually the algorithm converges well from a random initialization. Subsequently, small random numbers are added to the parameter values, and the corresponding probability <italic>p</italic><sub>1</sub> is re-evaluated. If the probability increases, the new parameter sample is saved, if the probability decreases, the new sample is only saved with a probability of <italic>p</italic><sub>1</sub>/<italic>p</italic><sub>0</sub>, otherwise discarded. After many such iterations (typically 10 000 in our case, which takes roughly 0.5 min on a standard desktop PC), the saved set of parameter samples represents the distribution of the fitted parameters (elevation, tilt, and roll). From these parameter samples, one can finally compute the mean value, denoting the best-fit parameter values, and the standard deviation or credible intervals, denoting the uncertainty of the parameters.</p>
      <p id="P16">Optionally, if a horizon is visible in the image, <italic>CameraTransform</italic> uses the horizon line as an additional constraint to determine the extrinsic camera parameters. Based on the elevation, tilt and roll, the astronomical horizon line of a perfectly spherical earth is projected onto the image, and its distance to the user-provided horizon is minimized.</p>
      <p id="P17">To evaluate this method, an artificial image (<xref rid="F2" ref-type="fig">Fig. 2a</xref>) is created using the <italic>CameraTransform</italic> package. A set of rectangles with a width of 30 cm and a height of 0.75m are randomly placed at distances ranging from 50m to 150 m, and subsequently projected to the image plane using the following camera parameters: focal length 14 mm, sensor size 17.3 × 9.7 mm with 4608 × 2592 px, camera height 16.1 m, tilt angle 85.3°, and roll angle 0.3°. Using the software <italic>ClickPoints</italic> [<xref rid="R18" ref-type="bibr">18</xref>], we mark the base and top positions of these rectangles in the image (<xref rid="F2" ref-type="fig">Fig. 2c</xref>) and provide them as input for the sampling routine. We then investigate how the estimated distribution of elevation, tilt angle, and roll angle vary with the number of provided objects. The analysis is performed with and without a horizon, for 5, 10, 25, and 50 randomly chosen objects. The results show that by including a larger number <italic>n</italic> of objects, the uncertainty of the parameter estimate (as indicated by the width of the distribution) decreases roughly as <italic>n</italic><sup>−0.5</sup> (<xref rid="F2" ref-type="fig">Fig. 2</xref>). Moreover, we find that both the camera elevation and the tilt angle can be fitted with considerably less uncertainty if a horizon is provided (<xref rid="F2" ref-type="fig">Fig. 2g</xref>,<xref rid="F2" ref-type="fig">h</xref>,<xref rid="F2" ref-type="fig">i</xref>), compared to parameter estimates without horizon (<xref rid="F2" ref-type="fig">Fig. 2d</xref>,<xref rid="F2" ref-type="fig">e</xref>,<xref rid="F2" ref-type="fig">f</xref>).</p>
      <p id="P18">Furthermore, the uncertainty of the parameter estimates depends on the position of the objects in the image (see <xref rid="SD1" ref-type="supplementary-material">Supplementary Information</xref>). Objects that are evenly distributed throughout the image provide better estimates compared to objects that are clustered in the front or in the back of the image.</p>
      <p id="P19">To demonstrate the fitting procedure, we analyze an image (<xref rid="F3" ref-type="fig">Fig. 3a</xref>) from a wide-angle camera overseeing an Emperor penguin (<italic>Aptenodytes forsteri</italic>) colony at Pointe Géologie, Antarctica (micrObs system, see [<xref rid="R15" ref-type="bibr">15</xref>]). The camera was positioned on an island with an elevation above sea ice level of 19m as measured by differential GPS. We estimate the extrinsic camera parameters by providing the feet and head positions of 50 animals, assuming an average height of 0.75m with an uncertainty that is left as a free parameter. Furthermore, we assume that the z-position of all animals is exactly equal as they are standing on the frozen sea ice, which we assume to be flat. <xref rid="F3" ref-type="fig">Fig. 3b</xref> shows the projected top view based on the extracted extrinsic camera parameters. The estimated extrinsic parameters are: elevation 18.9 ± 0.7 m, tilt 84.6 ± 0.42°, and roll −0.2 ± 0.41°. The size variation of the penguins, which was left as a free parameter for the metropolis algorithm to sample, was estimated to be 0.059 m. The obtained parameters can now be used e.g. to estimate the area of huddles in the image. For this purpose, the user paints the region occupied by the heads of huddling penguins (pink line in <xref rid="F3" ref-type="fig">Fig. 3a</xref>), and the circumference of this region is transformed to a top-view projection (pink line in <xref rid="F3" ref-type="fig">Fig. 3d</xref>,<xref rid="F3" ref-type="fig">f</xref>) and moved down by a distance of 0.75m (one penguin height) to indicate the huddle area (green line in <xref rid="F3" ref-type="fig">Fig. 3d</xref>,<xref rid="F3" ref-type="fig">f</xref>).</p>
    </sec>
    <sec id="S8">
      <label>4.2.</label>
      <title>Fitting by geo-referencing</title>
      <p id="P20">For small tilt angles, e.g. images taken from a helicopter (<xref rid="F4" ref-type="fig">Fig. 4a</xref>), the size of the objects in the image does not vary sufficiently over the range of y-positions, and hence fitting by object height fails. If in addition there is no visible horizon, such images require a different method. If an accurate map or a high-resolution satellite image of the area of interest is available, point correspondences between the image and the map can be used instead to estimate the camera parameters using an image registration approach.</p>
      <p id="P21">In the example shown in <xref rid="F4" ref-type="fig">Fig. 4</xref>, photographs of a King penguin (<italic>Aptenodytes patagonicus</italic>) colony at the Baie du Marin (Possession Island, Crozet Archipelago) have been taken from a helicopter flying approximately 300m above ground. We choose eight distinct points that are recognizable in both the camera image and the satellite image provided by Google Earth (<xref rid="F4" ref-type="fig">Fig. 4a</xref>,<xref rid="F4" ref-type="fig">c</xref>). To calculate the cost function for image registration, we project the image points (blue points in <xref rid="F4" ref-type="fig">Fig. 4a</xref>) onto the satellite image (blue points in <xref rid="F4" ref-type="fig">Fig. 4c</xref>) and calculate the distance to the target points (red points in <xref rid="F4" ref-type="fig">Fig. 4b</xref>,<xref rid="F4" ref-type="fig">c</xref>). The fit routine of <italic>CameraTransform</italic> then computes the <italic>height</italic> and <italic>tilt</italic> of the camera as well as the camera’s x,y-position and heading. The example in <xref rid="F4" ref-type="fig">Fig. 4</xref> with an rms error of 0.76m between transformed image and target points demonstrates that the fit routine matches all except one point, which is the branch point of a river that has shifted from the time the satellite image was taken (<xref rid="F4" ref-type="fig">Fig. 4c</xref>).</p>
    </sec>
  </sec>
  <sec id="S9">
    <label>5.</label>
    <title>Stereo images</title>
    <p id="P22">If object sizes are unknown, a stereo camera setup can be used to determine the size of multiple objects that are recorded from two different positions and angles, provided that corresponding points in both images can be unambiguously marked, and that either the distance between the two cameras or the absolute size of a single object in the image is known.</p>
    <p id="P23"><italic>CameraTransform</italic> estimates the relative orientation of the cameras by minimizing the back-projected distance of a marked point in one image to the epipolar line of the corresponding point in the other image (the epipolar line is the line in image B that corresponds to a single point in image A), and vice versa (see Supplementary Fig. C7). We found that a minimization of the back-projection error in pixels is preferable to a more direct minimization of the distance between the corresponding rays of the points in the world space, due to scaling issues.</p>
    <p id="P24">In contrast to existing stereo fitting methods (e.g. OpenCV), our method is based on the Monte Carlo approach and provides complete distributions of the estimated parameters to assess the uncertainty of the estimates. Furthermore, our method directly provides the relative orientation of the two cameras, whereas the commonly employed Eight-Point Algorithm [<xref rid="R19" ref-type="bibr">19</xref>] only yields the fundamental matrix, from which the relative orientation cannot be unambiguously extracted.</p>
  </sec>
  <sec id="S10">
    <label>6.</label>
    <title>Impact</title>
    <p id="P25">We present the Python package <italic>CameraTransform</italic> for estimating extrinsic camera parameters based on image features, satellite images, and images obtained from multiple viewing angles. Moreover, <italic>CameraTransform</italic> allows users to geo-reference images or to correct images for perspective distortions (e.g. to obtain top-view or “bird’s-eye” view projections). The package has been previously applied for studying Emperor and King penguin colonies [<xref rid="R5" ref-type="bibr">5</xref>,<xref rid="R15" ref-type="bibr">15</xref>,<xref rid="R20" ref-type="bibr">20</xref>], but is generally applicable for other quantitative image analysis where the extrinsic camera parameters were not or could not be measured in the field at the time of the recording. The package is published under the GPLv3 open source license to allow for continuous use and application in science. The documentation is hosted on <ext-link ext-link-type="uri" xlink:href="http://cameratransform.readthedocs.io/">http://cameratransform.readthedocs.io</ext-link> and contains further details on how to install the package. The documentation also provides numerous usage examples.</p>
  </sec>
  <sec sec-type="supplementary-material" id="SM1">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="SD1">
      <label>1</label>
      <media xlink:href="NIHMS1546289-supplement-1.pdf" orientation="portrait" id="d39e798" position="anchor"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="S11">
    <title>Acknowledgments</title>
    <p id="P26">This work was supported by the National Institutes of Health, USA (HL120839), the Institut Polaire Françis Paul-Emile Victor, France (IPEV, Program no. 137 to CLB), the RTPI NUTRESS (Réseau Thématique Pluridisciplinaire International “NUTrition et RESistance aux Stress environnementaux” - CSM/CNRS/Unistra), Monaco, and the Deutsche Forschungsgemeinschaft (DFG), Germany grant FA336/5-1 and ZI1525/3-1 in the framework of the priority program “Antarctic research with comparative investigations in Arctic ice areas”.</p>
  </ack>
  <fn-group>
    <fn fn-type="COI-statement" id="FN2">
      <p id="P27">Declaration of competing interest</p>
      <p id="P28">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
    </fn>
    <fn id="FN3">
      <p id="P29">Appendix A. Supplementary data</p>
      <p id="P30">Supplementary material related to this article can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.softx.2019.100333">https://doi.org/10.1016/j.softx.2019.100333</ext-link>.</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>[1]</label>
      <mixed-citation publication-type="journal"><name><surname>Gregory</surname><given-names>T</given-names></name>, <name><surname>Carrasco Rueda</surname><given-names>F</given-names></name>, <name><surname>Deichmann</surname><given-names>J</given-names></name>, <name><surname>Kolowski</surname><given-names>J</given-names></name>, <name><surname>Alonso</surname><given-names>A</given-names></name>. <article-title>Arboreal camera trapping: taking a proven method to new heights</article-title>. <source>Methods Ecol Evol</source><year>2014</year>;<volume>5</volume>(<issue>5</issue>):<fpage>443</fpage>–<lpage>51</lpage>. <pub-id pub-id-type="doi">10.1111/2041-210X.12177</pub-id>, URL <comment><ext-link ext-link-type="uri" xlink:href="http://doi.wiley.com/10.1111/2041-210X.12177">http://doi.wiley.com/10.1111/2041-210X.12177</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R2">
      <label>[2]</label>
      <mixed-citation publication-type="journal"><name><surname>Cutler</surname><given-names>TL</given-names></name>, <name><surname>Swann</surname><given-names>DE</given-names></name>. <article-title>Using remote photography in wildlife ecology: A review</article-title>. <source>Wildl Soc Bull</source><year>1999</year>;<volume>27</volume>(<issue>3</issue>):<fpage>571</fpage>–<lpage>81</lpage>, URL <comment><ext-link ext-link-type="uri" xlink:href="http://www.jstor.org/stable/3784076">http://www.jstor.org/stable/3784076</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R3">
      <label>[3]</label>
      <mixed-citation publication-type="journal"><name><surname>Lynch</surname><given-names>TP</given-names></name>, <name><surname>Alderman</surname><given-names>R</given-names></name>, <name><surname>Hobday</surname><given-names>AJ</given-names></name>. <article-title>A high-resolution panorama camera system for monitoring colony-wide seabird nesting behaviour</article-title>. In: <name><surname>Schielzeth</surname><given-names>H</given-names></name>, editor. <source>Methods Ecol Evol</source><year>2015</year>;<volume>6</volume>(<issue>5</issue>):<fpage>491</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1111/2041-210X.12339</pub-id>, URL <comment><ext-link ext-link-type="uri" xlink:href="http://doi.wiley.com/10.1111/2041-210X.12339">http://doi.wiley.com/10.1111/2041-210X.12339</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R4">
      <label>[4]</label>
      <mixed-citation publication-type="journal"><name><surname>Zitterbart</surname><given-names>DP</given-names></name>, <name><surname>Wienecke</surname><given-names>B</given-names></name>, <name><surname>Butler</surname><given-names>JP</given-names></name>, <name><surname>Fabry</surname><given-names>B</given-names></name>. <article-title>Coordinated movements prevent jamming in an emperor penguin huddle</article-title>. <source>PLoS One</source><year>2011</year>;<volume>6</volume>(<issue>6</issue>). <fpage>e20260</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0020260</pub-id>, URL <comment><ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3106014{&amp;}tool=pmcentrez{&amp;}rendertype=abstract">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3106014{&amp;}tool=pmcentrez{&amp;}rendertype=abstract</ext-link>.</comment><pub-id pub-id-type="pmid">21673816</pub-id></mixed-citation>
    </ref>
    <ref id="R5">
      <label>[5]</label>
      <mixed-citation publication-type="journal"><name><surname>Richter</surname><given-names>S</given-names></name>, <name><surname>Gerum</surname><given-names>RC</given-names></name>, <name><surname>Schneider</surname><given-names>W</given-names></name>, <name><surname>Fabry</surname><given-names>B</given-names></name>, <name><surname>Le Bohec</surname><given-names>C</given-names></name>, <name><surname>Zitterbart</surname><given-names>DP</given-names></name>. <article-title>A remote-controlled observatory for behavioural and ecological research: A case study on emperor penguins</article-title>. <source>Methods Ecol Evol</source><year>2018</year>;<volume>9</volume>(<issue>5</issue>). <pub-id pub-id-type="doi">10.1111/2041-210X.12971</pub-id>, URL <comment><ext-link ext-link-type="uri" xlink:href="http://doi.wiley.com/10.1111/2041-210X.12971">http://doi.wiley.com/10.1111/2041-210X.12971</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R6">
      <label>[6]</label>
      <mixed-citation publication-type="journal"><name><surname>Ito</surname><given-names>M</given-names></name><article-title>Robot vision modelling–camera modelling and camera calibration</article-title>. <source>Adv Robot</source><year>1990</year>;<volume>5</volume>(<issue>3</issue>):<fpage>321</fpage>–<lpage>35</lpage>. <pub-id pub-id-type="doi">10.1163/156855391X00232</pub-id>, URL <comment><ext-link ext-link-type="uri" xlink:href="http://www.tandfonline.com/doi/abs/10.1163/156855391X00232">http://www.tandfonline.com/doi/abs/10.1163/156855391X00232</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R7">
      <label>[7]</label>
      <mixed-citation publication-type="journal"><name><surname>Pollefeys</surname><given-names>M</given-names></name>, <name><surname>Koch</surname><given-names>R</given-names></name>, <name><surname>Van Gool</surname><given-names>L</given-names></name>. <article-title>Self-calibration and metric reconstruction inspite of varying and unknown intrinsic camera parameters</article-title>. <source>Int J Comput Vis</source><year>1999</year>;<volume>32</volume>(<issue>1</issue>):<fpage>7</fpage>–<lpage>25</lpage>. <pub-id pub-id-type="doi">10.1023/A:1008109111715</pub-id>, URL <comment><ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.1023/A:1008109111715">http://link.springer.com/10.1023/A:1008109111715</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R8">
      <label>[8]</label>
      <mixed-citation publication-type="book"><name><surname>Van Rossum</surname><given-names>G</given-names></name>, <name><surname>Drake</surname><given-names>FL</given-names><suffix>Jr</suffix></name>. <source>Python tutorial</source>. <publisher-loc>The Netherlands</publisher-loc>: <publisher-name>Centrum voor Wiskunde en Informatica Amsterdam</publisher-name>; <year>1995</year>.</mixed-citation>
    </ref>
    <ref id="R9">
      <label>[9]</label>
      <mixed-citation publication-type="journal"><name><surname>Van Der Walt</surname><given-names>S</given-names></name>, <name><surname>Colbert</surname><given-names>SC</given-names></name>, <name><surname>Varoquaux</surname><given-names>G</given-names></name>. <article-title>The NumPy array: A structure for efficient numerical computation</article-title>. <source>Comput. Sci. Eng</source>. <year>2011</year>;<volume>13</volume>(<issue>2</issue>):<fpage>22</fpage>–<lpage>30</lpage>. <pub-id pub-id-type="doi">10.1109/MCSE.2011.37</pub-id>, arXiv:1102.1523, URL <comment><ext-link ext-link-type="uri" xlink:href="http://ieeexplore.ieee.org/document/5725236/">http://ieeexplore.ieee.org/document/5725236/</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R10">
      <label>[10]</label>
      <mixed-citation publication-type="journal"><name><surname>Oliphant</surname><given-names>TE</given-names></name>. <article-title>Python for scientific computing</article-title>. <source>Comput Sci Eng</source><year>2007</year>;<volume>9</volume>(<issue>3</issue>):<fpage>10</fpage>–<lpage>20</lpage>. <pub-id pub-id-type="doi">10.1109/MCSE.2007.58</pub-id>, URL <comment><ext-link ext-link-type="uri" xlink:href="https://ieeexplore.ieee.org/abstract/document/4160250/">https://ieeexplore.ieee.org/abstract/document/4160250/</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R11">
      <label>[11]</label>
      <mixed-citation publication-type="journal"><name><surname>Hunter</surname><given-names>JD</given-names></name>. <article-title>Matplotlib: A 2D graphics environment</article-title>. <source>Comput Sci Eng</source><year>2007</year>;<volume>9</volume>(<issue>3</issue>):<fpage>90</fpage>–<lpage>5</lpage>. <pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R12">
      <label>[12]</label>
      <mixed-citation publication-type="journal"><name><surname>Gerum</surname><given-names>R</given-names></name><article-title>Pylustrator: code generation for reproducible figures for publication</article-title>. <source>arXiv e-prints</source><year>2019</year>. arXiv:1910.00279.</mixed-citation>
    </ref>
    <ref id="R13">
      <label>[13]</label>
      <mixed-citation publication-type="confproc"><name><surname>McKinney</surname><given-names>W</given-names></name><source>Data Structures for Statistical Computing in Python</source>, in: <conf-name>Proc. 9th python sci. conf. (SCIPY 2010)</conf-name>, <year>2010</year>, p. <fpage>51</fpage>–<lpage>6</lpage>, <pub-id pub-id-type="doi">10.1016/S0168-0102(02)00204-3</pub-id>, URL <comment><ext-link ext-link-type="uri" xlink:href="https://pdfs.semanticscholar.org/f6da/c1c52d3b07c993fe52513b8964f86e8fe381.pdf">https://pdfs.semanticscholar.org/f6da/c1c52d3b07c993fe52513b8964f86e8fe381.pdf</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R14">
      <label>[14]</label>
      <mixed-citation publication-type="book"><name><surname>Hartley</surname><given-names>R</given-names></name>, <name><surname>Zisserman</surname><given-names>A</given-names></name>. <source>Multiple view geometry in computer vision</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>; <year>2003</year>, p. <fpage>657</fpage>.</mixed-citation>
    </ref>
    <ref id="R15">
      <label>[15]</label>
      <mixed-citation publication-type="journal"><name><surname>Richter</surname><given-names>S</given-names></name>, <name><surname>Gerum</surname><given-names>RC</given-names></name>, <name><surname>Winterl</surname><given-names>A</given-names></name>, <name><surname>Houstin</surname><given-names>A</given-names></name>, <name><surname>Seifert</surname><given-names>MA</given-names></name>, <name><surname>Peschel</surname><given-names>J</given-names></name>, <name><surname>Fabry</surname><given-names>B</given-names></name>, <name><surname>Le Bohec</surname><given-names>C</given-names></name>, <name><surname>Zitterbart</surname><given-names>DP</given-names></name>. <article-title>Phase transitions in huddling emperor penguins</article-title>. <source>J Phys D Appl Phys</source><year>2018</year>;<volume>51</volume>(<issue>21</issue>):<fpage>214002</fpage><pub-id pub-id-type="doi">10.1088/1361-6463/aabb8e</pub-id>, URL <comment><ext-link ext-link-type="uri" xlink:href="http://stacks.iop.org/0022-3727/51/i=21/a=214002?key=crossref.b52459c985c989270a3ae1d74f2e8477http://iopscience.iop.org/article/10.1088/1361-6463/aabb8e">http://stacks.iop.org/0022-3727/51/i=21/a=214002?key=crossref.b52459c985c989270a3ae1d74f2e8477http://iopscience.iop.org/article/10.1088/1361-6463/aabb8e</ext-link>.</comment><pub-id pub-id-type="pmid">30416209</pub-id></mixed-citation>
    </ref>
    <ref id="R16">
      <label>[16]</label>
      <mixed-citation publication-type="journal"><name><surname>Hastings</surname><given-names>W</given-names></name><article-title>Monte Carlo sampling methods using Markov chains and their applications</article-title>. <source>Biometrika</source><year>1970</year>;<volume>51</volume>(<issue>1</issue>):<fpage>97</fpage>–<lpage>109</lpage>, URL <comment><ext-link ext-link-type="uri" xlink:href="https://academic.oup.com/biomet/article-abstract/57/1/97/284580">https://academic.oup.com/biomet/article-abstract/57/1/97/284580</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R17">
      <label>[17]</label>
      <mixed-citation publication-type="journal"><name><surname>Metropolis</surname><given-names>N</given-names></name>, <name><surname>Rosenbluth</surname><given-names>AW</given-names></name>, <name><surname>Rosenbluth</surname><given-names>MN</given-names></name>, <name><surname>Teller</surname><given-names>AH</given-names></name>, <name><surname>Teller</surname><given-names>E</given-names></name>. <article-title>Equation of state calculations by fast computing machines</article-title>. <source>J Chem Phys</source><year>1953</year>;<volume>21</volume>(<issue>6</issue>):<fpage>1087</fpage>–<lpage>92</lpage>. <pub-id pub-id-type="doi">10.1063/1.1699114</pub-id>, arXiv:5744249209, URL <comment><ext-link ext-link-type="uri" xlink:href="http://aip.scitation.org/doi/10.1063/1.1699114">http://aip.scitation.org/doi/10.1063/1.1699114</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R18">
      <label>[18]</label>
      <mixed-citation publication-type="journal"><name><surname>Gerum</surname><given-names>RC</given-names></name>, <name><surname>Richter</surname><given-names>S</given-names></name>, <name><surname>Fabry</surname><given-names>B</given-names></name>, <name><surname>Zitterbart</surname><given-names>DP</given-names></name>. <article-title>ClickPoints: an expandable toolbox for scientific image annotation and analysis</article-title>. <source>Methods Ecol Evol</source><year>2016</year>;<volume>8</volume>(<issue>6</issue>):<fpage>750</fpage>–<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1111/2041-210X.12702</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R19">
      <label>[19]</label>
      <mixed-citation publication-type="journal"><name><surname>Longuet-Higgins</surname><given-names>HC</given-names></name>. <article-title>A computer algorithm for reconstructing a scene from two projections</article-title>. <source>Nature</source><year>1981</year>;<volume>293</volume>(<issue>5828</issue>):<fpage>133</fpage>–<lpage>5</lpage>. <pub-id pub-id-type="doi">10.1038/293133a0</pub-id>, URL <comment><ext-link ext-link-type="uri" xlink:href="http://www.nature.com/articles/293133a0">http://www.nature.com/articles/293133a0</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R20">
      <label>[20]</label>
      <mixed-citation publication-type="journal"><name><surname>Gerum</surname><given-names>RC</given-names></name>, <name><surname>Richter</surname><given-names>S</given-names></name>, <name><surname>Fabry</surname><given-names>B</given-names></name>, <name><surname>Le Bohec</surname><given-names>C</given-names></name>, <name><surname>Bonadonna</surname><given-names>F</given-names></name>, <name><surname>Nesterova</surname><given-names>A</given-names></name>, <name><surname>Zitterbart</surname><given-names>DP</given-names></name>. <article-title>Structural organisation and dynamics in king penguin colonies</article-title>. <source>J Phys D Appl Phys</source><year>2018</year>;<volume>51</volume>(<issue>16</issue>):<fpage>164004</fpage><pub-id pub-id-type="doi">10.1088/1361-6463/AAB46B</pub-id>, URL <comment><ext-link ext-link-type="uri" xlink:href="http://iopscience.iop.org/article/10.1088/1361-6463/aab46b">http://iopscience.iop.org/article/10.1088/1361-6463/aab46b</ext-link>.</comment><pub-id pub-id-type="pmid">30319146</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="F1" orientation="portrait" position="float">
    <label>Fig. 1.</label>
    <caption>
      <p id="P31">Camera parameters. Intrinsic parameters: (a) the dimensions of the image in pixels <italic>w</italic><sub>image</sub> × <italic>h</italic><sub>image</sub>, (b) the size of the sensor in mm (<italic>w</italic><sub>sensor</sub> × <italic>h</italic><sub>sensor</sub>) and the focal length <italic>f</italic> in mm. Extrinsic parameters: (c) side view: the <italic>elevation</italic> specifies the height of the camera above a reference altitude, e.g. above ground, the <italic>tilt</italic> specifies the angle between the vertical direction and the viewing direction (sensor normal). (d) top view: the <italic>offset</italic> (x, y) specifies the x, y coordinates of the camera relative to a reference position (<italic>x</italic> = 0, <italic>y</italic> = 0). and the <italic>heading</italic> specifies the angle between the <italic>x</italic>-direction and the viewing direction (sensor normal). (e) Image sensor: the <italic>roll</italic> specifies the angle between the lower sensor edge and the horizontal direction.</p>
    </caption>
    <graphic xlink:href="nihms-1546289-f0001"/>
  </fig>
  <fig id="F2" orientation="portrait" position="float">
    <label>Fig. 2.</label>
    <caption>
      <p id="P32">Influence of number of objects to determine camera elevation, tilt angle and roll angle. (a) Artificial image with randomly placed objects and horizon. (b) 3 points on the horizon and (c) foot and head points of 50 objects are manually selected. Metropolis-sampled uncertainty of the obtained parameters (elevation, tilt, and roll) for a fit without using horizon points (d)–(f) and with horizon points (g)–(i), for different number of randomly selected (without replacing) objects (N = 5, blue line; N = 10, orange line; N = 25, green line; N = 50 red line; true value, black line). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</p>
    </caption>
    <graphic xlink:href="nihms-1546289-f0002"/>
  </fig>
  <fig id="F3" orientation="portrait" position="float">
    <label>Fig. 3.</label>
    <caption>
      <p id="P33">Fit of camera parameters using image objects. (a) Image taken with the micrObs system [<xref rid="R15" ref-type="bibr">15</xref>] of a penguin colony. The feet (green) and head (yellow) positions of 50 penguins were manually marked (shown in inset (b)) and the circumference of a huddle was marked (purple, shown in inset (c)). The head and feet positions are used to estimate the camera perspective (estimated head positions are shown as red squares) for projecting the image to a top view (d). Inset (e) shows the uncertainty of the projected penguin positions and inset (f) shows the projected huddle circumference.</p>
    </caption>
    <graphic xlink:href="nihms-1546289-f0003"/>
  </fig>
  <fig id="F4" orientation="portrait" position="float">
    <label>Fig. 4.</label>
    <caption>
      <p id="P34">Fit of camera parameters by image registration. (a) Camera image from a helicopter flight at the Baie du Marin colony at the Crozet islands (Dec 09, 2014) [<xref rid="R20" ref-type="bibr">20</xref>]. (b) Satellite image provided by Google Earth. (c) Image fitted over points in the image (blue) with points in the map image (red). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</p>
    </caption>
    <graphic xlink:href="nihms-1546289-f0004"/>
  </fig>
</floats-group>
