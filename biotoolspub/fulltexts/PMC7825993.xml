<?DTDIdentifier.IdentifierValue http://null/schema/dtds/document/fulltext/xcr/xocs-article.xsd?>
<?DTDIdentifier.IdentifierType schema?>
<?SourceDTD.DTDName xocs-article.xsd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName ftrr2jats.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Online Soc Netw Media</journal-id>
    <journal-id journal-id-type="iso-abbrev">Online Soc Netw Media</journal-id>
    <journal-title-group>
      <journal-title>Online Social Networks and Media</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2468-6964</issn>
    <publisher>
      <publisher-name>Elsevier B.V.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7825993</article-id>
    <article-id pub-id-type="pii">S2468-6964(21)00007-0</article-id>
    <article-id pub-id-type="doi">10.1016/j.osnem.2021.100123</article-id>
    <article-id pub-id-type="publisher-id">100123</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CoVerifi: A COVID-19 news verification system</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au0001">
        <name>
          <surname>Kolluri</surname>
          <given-names>Nikhil L.</given-names>
        </name>
        <xref rid="aff0001" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au0002">
        <name>
          <surname>Murthy</surname>
          <given-names>Dhiraj</given-names>
        </name>
        <xref rid="aff0002" ref-type="aff">b</xref>
        <xref rid="cor0001" ref-type="corresp">⁎</xref>
      </contrib>
      <aff id="aff0001"><label>a</label>Department of Electrical and Computer Engineering, University of Texas, Austin, TX 78712, United States</aff>
      <aff id="aff0002"><label>b</label>School of Journalism and Media, Moody College of Communication and Department of Sociology, University of Texas at Austin, Austin, TX 78712, United States</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor0001"><label>⁎</label>Corresponding author.</corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>23</day>
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="ppub">
      <month>3</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>23</day>
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <volume>22</volume>
    <fpage>100123</fpage>
    <lpage>100123</lpage>
    <history>
      <date date-type="received">
        <day>27</day>
        <month>7</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>7</day>
        <month>1</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>18</day>
        <month>1</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 Elsevier B.V. All rights reserved.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Elsevier B.V.</copyright-holder>
      <license>
        <license-p>Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.</license-p>
      </license>
    </permissions>
    <abstract id="abs0002">
      <p>There is an abundance of misinformation, disinformation, and “fake news” related to COVID-19, leading the director-general of the World Health Organization to term this an ‘infodemic’. Given the high volume of COVID-19 content on the Internet, many find it difficult to evaluate veracity. Vulnerable and marginalized groups are being misinformed and subject to high levels of stress. Riots and panic buying have also taken place due to “fake news”. However, individual research-led websites can make a major difference in terms of providing accurate information. For example, the Johns Hopkins Coronavirus Resource Center website has over 81 million entries linked to it on Google. With the outbreak of COVID-19 and the knowledge that deceptive news has the potential to measurably affect the beliefs of the public, new strategies are needed to prevent the spread of misinformation. This study seeks to make a timely intervention to the information landscape through a COVID-19 “fake news”, misinformation, and disinformation website. In this article, we introduce CoVerifi, a web application which combines both the power of machine learning and the power of human feedback to assess the credibility of news. By allowing users the ability to “vote” on news content, the CoVerifi platform will allow us to release labelled data as open source, which will enable further research on preventing the spread of COVID-19-related misinformation. We discuss the development of CoVerifi and the potential utility of deploying the system at scale for combating the COVID-19 “infodemic”.</p>
    </abstract>
    <kwd-group id="keys0001">
      <title>Keywords</title>
      <kwd>Infodemic</kwd>
      <kwd>Misinformation</kwd>
      <kwd>Media diet</kwd>
      <kwd>Machine learning</kwd>
      <kwd>COVID-19</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec0001">
    <label>1</label>
    <title>Introduction</title>
    <p id="para0006">Coronavirus (COVID-19), declared a Public Health Emergency of International Concern (PHEIC), is a virus which originated in Wuhan, China in December 2019 <xref rid="bib0001" ref-type="bibr">[1]</xref>. As of November 22, 2020, COVID-19 has spread to 220 countries, areas, or territories, infected over 57.8 million people, and killed over 1.3 million people <xref rid="bib0002" ref-type="bibr">[2]</xref>. In February 2020, World Health Organization (WHO) Director-General Tedros Adhanom Ghebreyesus said “We're not just fighting an epidemic; we're fighting an infodemic. Fake news spreads faster and more easily than this virus, and is just as dangerous.” <xref rid="bib0001" ref-type="bibr">[1]</xref>. Sylvie Briand, architect of the WHO's strategy to counter the infodemic risk argues that “with social media […] this phenomenon is amplified, it goes faster and further, like the viruses that travel with people and go faster and further” <xref rid="bib0001" ref-type="bibr">[1]</xref>. Previous work examining COVID-19 tweets found that twice as much false information as evidence-based information was tweeted, though this trend did not apply to retweets <xref rid="bib0003" ref-type="bibr">[3]</xref>. Among the Twitter posts regarding COVID-19 in their sample, most posts (59%) rated false by their fact-checkers remained up at the time of publishing their article <xref rid="bib0004" ref-type="bibr">[4]</xref>.</p>
    <p id="para0007">This infodemic has proven its ability to accelerate the epidemic process, increase violence against certain groups, and cause bodily harm. COVID-19 misinformation can directly threaten lives. There are harmful “cures” being suggested such as drinking fish tank additives, bleach, or cow urine <xref rid="bib0005" ref-type="bibr">[5]</xref>. Furthermore, there exists a threat of COVID-19 misinformation increasing resistance to a vaccine. There is already a growing anti-vaccination community related to COVID-19, which according to some, is better positioned for growth than the pro-vaccination community <xref rid="bib0005" ref-type="bibr">[5]</xref>. Additionally, false rumors that people with dark skin may be immune to COVID-19 have been spreading on social media since late January 2020, and have potentially contributed to the over-representation of some minorities as victims. In the US, as of early April 2020, approximately 70% of fatalities in Chicago and Louisiana were African Americans, who only make up roughly 30% of the population <xref rid="bib0005" ref-type="bibr">[5]</xref>, <xref rid="bib0006" ref-type="bibr">[6]</xref>, <xref rid="bib0007" ref-type="bibr">[7]</xref>. APM Research Lab found that Black, Indigenous, and Latino Americans all had a COVID-19 death rate of triple or more White Americans <xref rid="bib0008" ref-type="bibr">[8]</xref>. Moreover, some malicious COVID-19 narratives have been linked to offline anti-Asian violence [<xref rid="bib0005" ref-type="bibr">5</xref>,<xref rid="bib0009" ref-type="bibr">9</xref>,<xref rid="bib0010" ref-type="bibr">10</xref>].</p>
    <p id="para0008">To help address the continuing and major impacts associated with COVID-19-related misinformation, this article introduces CoVerifi, our web application to assess the credibility of COVID-19 news. CoVerifi retrieves a selection of news articles, tweets, and Reddit posts and displays credibility ratings produced by machine learning models and a credibility rating obtained from the “votes” of other CoVerifi users, and for tweets, the Botometer API's [<xref rid="bib0011" ref-type="bibr">11</xref>,<xref rid="bib0012" ref-type="bibr">12</xref>] bot score for the tweet poster. As the platform is used, we plan to release the labelled data we collect from these user votes as an open source dataset, which will enable further research on preventing the spread of COVID-19-related misinformation. As CoVerifi is open source, we also enable important future research, including the expansion of CoVerifi to other news-related platforms, machine learning models, and data collection opportunities. We discuss the development of CoVerifi and the potential utility of deploying the system at scale for combating the COVID-19 infodemic.</p>
    <sec id="sec0002">
      <label>1.1</label>
      <title>Fake news, misinformation, and disinformation</title>
      <p id="para0009">Lazer et al. <xref rid="bib0013" ref-type="bibr">[13]</xref> define “fake news” “to be fabricated information that mimics news media content in form but not in organizational process or intent”. As Tandoc et al. <xref rid="bib0014" ref-type="bibr">[14]</xref> highlight, the term remains contentious, yet important to engage with; hence, our inclusion of quotation marks. Gelbert <xref rid="bib0015" ref-type="bibr">[15]</xref> acknowledges the term “fake news” as new and rapidly evolving but argues in favor of only using it to refer to misleading claims which are misleading by design. To best address the scope of our research, our use of “fake news” includes inaccurate news, low-quality news, and imposter news. Moreover as Lazer et al. <xref rid="bib0013" ref-type="bibr">[13]</xref> observe ‘fake news overlaps with other information disorders, such as misinformation (false or misleading information) and disinformation (false information that is purposely spread to deceive people)’. Tandoc et al. <xref rid="bib0014" ref-type="bibr">[14]</xref> add that “fake news” also encompasses ‘viral posts based on fictitious accounts made to look like news reports’.</p>
      <p id="para0010">These types of “fake news” can be split into two categories: “fake news” with an implicit understanding by the reader that the content is false (such as parody and satire), and “fake news” where readers are unaware that the information is false. A recent study found that 38% of COVID-19-related misinformation in their sample was completely fabricated, 59% of the misinformation involved reconfiguration, and only 3% was satire/parody <xref rid="bib0004" ref-type="bibr">[4]</xref>. This suggests that amidst COVID-19, “fake news” which the reader is unaware is fake is present at alarming rates.</p>
      <p id="para0011">This category can be further broken into two groups: misinformation, which refers to the “inadvertent sharing of false information” [<xref rid="bib0016" ref-type="bibr">16</xref>,<xref rid="bib0017" ref-type="bibr">17</xref>] and disinformation, which refers to “the deliberate creation and sharing of information known to be false” [<xref rid="bib0016" ref-type="bibr">16</xref>,<xref rid="bib0017" ref-type="bibr">17</xref>]. The spread of misleading information on the web and social media “poses a major risk to society” <xref rid="bib0018" ref-type="bibr">[18]</xref> and “is overloading the exchange of ideas upon which democracies depend” <xref rid="bib0019" ref-type="bibr">[19]</xref>. With the presence of algorithms which personalize online experiences and hinder exposure to ideologically diverse sources of information, some argue that echo chambers emerge which make it harder to encounter ideologically diverse types of information [<xref rid="bib0019" ref-type="bibr">19</xref>,<xref rid="bib0020" ref-type="bibr">20</xref>]. Since social media and digital platforms are capable of substantially fragmenting the public's opinions and decreasing challenges against untrue information, we must very seriously consider strategies to combat misinformation which traverses these platforms. Moreover, for misinformation on social media and digital platforms, factual corrections are often ineffective, slow, and rarely reaching the people originally influenced by the misinformation <xref rid="bib0019" ref-type="bibr">[19]</xref>. Since social media and digital platforms are characterized by providing a massive quantity of information without the ability to provide factual corrections on content that has already been consumed, analysis of the way consumers address future media content, the types of media content provided to users, and the potential to foster organic responses to misinformation is increasingly important. Disruptions in the information landscape caused by COVID-19 warrant even greater consideration of these lines of research, since the veracity of content that individuals are consuming has measurable health impacts [<xref rid="bib0021" ref-type="bibr">21</xref>,<xref rid="bib0005" ref-type="bibr">5</xref>].</p>
    </sec>
    <sec id="sec0003">
      <label>1.2</label>
      <title>Pre-COVID-19 misinformation research</title>
      <p id="para0012">Prior to the outbreak of COVID-19, there were several attempts at addressing the more general problem of combating “fake news” and misinformation via a semi-automated or fully-automated approach. Past research reached a conclusion that social media is systematically exploited to manipulate and alter the public opinion <xref rid="bib0022" ref-type="bibr">[22]</xref>. Since these attacks are often orchestrated using bots, previous work has used machine learning to separate humans from bots, which can be used to determine if a social media account is part of a nefarious campaign [<xref rid="bib0022" ref-type="bibr">22</xref>,<xref rid="bib0023" ref-type="bibr">23</xref>]. Furthermore, there are several works surrounding computer-aided strategies to combat “fake news”, including stance-detection machine learning models, neural “fake news” detection models, claim identification pipelines, and “fake news” datasets. Collectively, these address “fake news” from news articles, Reddit posts, Facebook posts, and Twitter tweets. The inclusion of information from a variety of platforms is important because it parallels the multiplatform nature of the media diet of the average person. The notion that media habits are best characterized by diverse media consumption patterns has been discussed for decades, with early work involving ‘time diaries’ which qualitatively document media habits <xref rid="bib0024" ref-type="bibr">[24]</xref>. Today, people continue to consume information from diverse sources and prioritize their time on content from certain sources and even on specific topics. In the case of Twitter, users tend to consume information primarily on one or two specific topics of their interest, but the Twitter recommendation system mitigates imbalances in users’ consumed diets <xref rid="bib0025" ref-type="bibr">[25]</xref>. Users therefore can be presented with information that is unbalanced in terms of coverage of news stories and different from what other users are presented with <xref rid="bib0025" ref-type="bibr">[25]</xref>. Since it is typical for news consumers to draw from a range of sources, an approach which leverages a variety of information sources is crucial.</p>
      <p id="para0013">Social media has been shown to play a very important role in the media diet of digital native voters. Research has indicated that a digital media environment may socialize young voters into polarized information environments which in turn increases their involvement in elections <xref rid="bib0026" ref-type="bibr">[26]</xref>. Common in misinformation research prior to the outbreak of COVID-19 was the use of machine learning, a subset of artificial intelligence “that involves building and adapting models, which allow programs to ‘learn’ through experience” <xref rid="bib0027" ref-type="bibr">[27]</xref>. A ‘model’ represents something which takes in data as an input, performs some computation on the data, and produces some information as an output. Furthermore, when discussing machine learning models for news verification purposes, the models can often be described as performing natural language processing (NLP), a type of artificial intelligence tasked with comprehending, deciphering, and even reproducing human languages.</p>
      <sec id="sec0004">
        <label>1.2.1</label>
        <title>Existing models and approaches</title>
        <p id="para0014">There has been substantial work in leveraging machine learning and artificial intelligence to differentiate between fake and real information. Based on the types of machine learning models and web tools which have already been developed, it appears difficult to create a robust, entirely feature-based NLP model which includes no external information. Even seemingly performant natural language processing models have shown significantly reduced accuracy when presented with reconfigured news (changing small amounts of information to make the information false) as part of an adversarial attack <xref rid="bib0028" ref-type="bibr">[28]</xref>. Therefore, much of the prior work seems to be on seemingly peripheral tasks, such as stance detection, neural “fake news” detection, bot detection, and multi-step approaches involving the inclusion of external information.</p>
        <p id="para0015">An important step in developing the notion that “fake news” detection is not best addressed as a singular, isolated machine learning model was the “fake news” Challenge Stance Detection Task (FNC1). Notably, the competition focused on the task of stance detection (i.e., evaluating whether the headline agrees with the claim) rather than the task of labeling a claim as true or false <xref rid="bib0029" ref-type="bibr">[29]</xref>. The FNC-1 creators found that “truth labeling” is very difficult in practice and preferred a reliable semi-automated tool over a fully-automated system, which they felt would inevitably fall far short of 100% accuracy <xref rid="bib0029" ref-type="bibr">[29]</xref>. The results of FNC-1 encourage future work toward tools which aid journalists and fact checkers, both because fact checkers and journalists have acknowledged semi-automated tools as valuable and because machine learning in a vacuum may not be able solve the truth labeling problem.</p>
        <p id="para0016">With the presence of text generation models such as Google's BERT <xref rid="bib0030" ref-type="bibr">[30]</xref> which do a good job at mimicking real speech patterns, neural “fake news” generated by robots became reality. Since neural “fake news” algorithms cause the resulting generated texts to have some similar traits, achieving high accuracy with a neural “fake news” detection model is possible. As a response, several models have emerged to detect neural “fake news”. These include Grover and GPT-2. The former is a “fake news” generator which can also spot “fake news” generated by other AI models. In a setting with a limited access to neural “fake news” articles, Grover obtained over 92% accuracy at differentiating between human-written and machine-written news <xref rid="bib0031" ref-type="bibr">[31]</xref>. GPT-2, a successor to GPT, was trained to predict the next word in internet text, which allows it to generate synthetic text <xref rid="bib0032" ref-type="bibr">[32]</xref>. OpenAI also released a GPT-2 output dataset, which was used to train a corresponding fake text detector model capable of labeling text as “real” or “fake” with a confidence percentage <xref rid="bib0033" ref-type="bibr">[33]</xref>. An important note is that this fake text detector model will likely perform best on text generated by GPT-2, though our intuition is that it may help identify fake text generated by other models which use similar text-generation algorithms. The GPT-2 output detector should be seen as a tool for predicting whether content was generated by a machine or by a human. It is not capable of directly predicting veracity. For a more in-depth discussion of this important distinction, see <xref rid="sec0016" ref-type="sec">Section 2.2.3</xref>.</p>
        <p id="para0017">Other work tangential to neural “fake news” detection includes bot detection. The research surrounding bot detection is highly relevant to “fake news” since it can suggest that “fake news” is often propagated as a result of orchestrated, malicious campaigns. BotOrNot, now renamed Botometer, was a system to evaluate social bots which has served more than one million requests via their website. While BotOrNot provides effective functionality for samples up to thousands of accounts, it cannot scale much more extensively than that due to its reliance on the rate-limited Twitter API <xref rid="bib0022" ref-type="bibr">[22]</xref>. Other work utilized a mixture of machine learning techniques and cognitive heuristics for bot detection <xref rid="bib0022" ref-type="bibr">[22]</xref>. Ferrara et al. <xref rid="bib0022" ref-type="bibr">[22]</xref> found that bots that existed during the 2016 U.S. Presidential election campaign to support alt-right narratives went dark after November 8, 2016, and were used again in the days prior to the 2017 French presidential election. Moreover, it is reasonable to conclude that their content would be overrepresented in relation to the amount of users orchestrating the campaign, given that bots can produce far more content than humans in short timescales. Therefore, addressing these intentional, bot-driven, malicious campaigns remains important to the literature due to (1) the relative ease of identifying neural “fake news” and bot-created news compared to identifying a diverse range of “fake news” types and (2) the ability to identify large volumes of “fake news” at once if the presence of a bot is detected. Given this discussion of the utility of models identifying bot-generated news in combating misinformation at large, we chose to use a form of neural “fake news” detection (a text classification model trained on the outputs of GPT-2) in our CoVerifi platform. For tweets, we also include the bot score provided by Botometer's API for the poster's account.</p>
        <p id="para0018">However, it should be noted that automated approaches to assess the validity of a piece of news content have had some success, though currently reported accuracy has not yet reached acceptable levels. This research included the Fact Extraction and VERification (FEVER) Shared Task, which challenged participants to classify whether human-written factoid claims could be supported or refuted by using evidence retrieved from Wikipedia. The best performing system achieved a FEVER score of 64.21% <xref rid="bib0034" ref-type="bibr">[34]</xref>. This line of research demonstrates that while creating a fully automated machine-learning based approach for assessing the validity of a piece of news may indeed be possible, it seems to be a difficult, computationally intensive process with potentially marginal gains. This information strengthens the claim that pursuing tools that make fact checking easier and more effective could potentially be a more rewarding research area than an entirely automated approach.</p>
      </sec>
      <sec id="sec0005">
        <label>1.2.2</label>
        <title>Existing web tools</title>
        <p id="para0019">Toward the end of creating tools that make fact checking easier rather than producing a single, insular, machine-learning based approach, there are several web-based tools which have made substantial progress. Among these are ClaimBuster, Google Fact Check, and GLTR. ClaimBuster offers a near-complete fact-checking system, whereas Google Fact Check allows users to check specific claims and GLTR allows a visualization of the likelihood that text is machine-generated. Each provides specific and unique value, all advancing the goal of mitigating the harm caused by the spread of misinformation.</p>
        <p id="para0020">ClaimBuster <xref rid="bib0035" ref-type="bibr">[35]</xref> monitors live discourses (interviews, speeches, and debates), social media, and news to identify factual claims, detect matches with a professionally-verified repository of fact-checks, and instantly deliver the result to the audience <xref rid="bib0036" ref-type="bibr">[36]</xref>. For new, unchecked claims, ClaimBuster translates them into queries against knowledge databases and reports the result <xref rid="bib0036" ref-type="bibr">[36]</xref>. If humans must be brought into the loop for claims, it provides tools to help lay people to understand and vet claims <xref rid="bib0036" ref-type="bibr">[36]</xref>. This decision to provide tools rather than a classification for cases in which humans must be brought into the loop reveals that the authors were aware that an insulated, entirely automated approach may not be sufficient. While the decision-making process behind ClaimBuster is a great step in a positive direction, the system has limitations. It appears to focus on U.S. Presidential Election information, the ability to match claims against existing knowledge bases, and tools to check tweets. Therefore, ClaimBuster has limitations in terms of being able to address a wide range of types, formats, and quantities of misinformation, especially since more subtle forms of misinformation may not be possible to query against existing knowledge bases, and may occur on platforms which the authors are not aware of. Moreover, while a targeted approach is powerful and may work for many types of misinformation, it does not provide a solution which can change and expand at the same rate as the quantity and type of misinformation expands. Ultimately, such approaches could be complemented by less-perfect, but more-scalable approaches which can grow at the same rate as misinformation grows.</p>
        <p id="para0021">Other web tools which address similar goals include the Google Fact Check Explorer and GLTR. The former is a resource which allows searching for a claim and receiving information from fact checking sites which have rated the claim as likely true or likely false. For example, searching “inhale steam to kill coronavirus” will return several results from similar claims which were fact checked (in this case, rated “False”) by different sources <xref rid="bib0037" ref-type="bibr">[37]</xref>. There is an associated Google FactCheck Claim Search API which can be used to query the same set of fact check results as the Fact Check Explorer. While the Google Fact Check Explorer is useful, it can only provide help when there are specific claims which need to be checked. It is not designed to evaluate a verbatim new article, tweet, or post. Thus, Google Fact Check Explorer seems to be best used as a tool for types of “fake news” which involve specific, previously manually-checked claims, but is not well-suited to evaluate large quantities of misinformation in varying formats. GLTR, a tool to detect computer generated text <xref rid="bib0038" ref-type="bibr">[38]</xref>, has access to the GPT-2 and BERT language models and, given a textual input, can analyze what the models would have predicted as the next word for any position <xref rid="bib0038" ref-type="bibr">[38]</xref>. Using this knowledge, GLTR can help visualize the likelihood that a text passage is fake by using different colors to show which words were within the top words that these language models would have predicted [<xref rid="bib0039" ref-type="bibr">39</xref>,<xref rid="bib0038" ref-type="bibr">38</xref>]. GLTR is similar to Google Fact Check Explorer in that it also serves a useful, but specific and limited function. While it could help a user to determine if a body of text is machine generated, it seems to require manually submitting information to the tool every time it is used, which again limits its scalability.</p>
        <p id="para0022">Additional projects working toward the goal of partially-automated or fully-automated misinformation detection include those partnered with SOMA [<xref rid="bib0040" ref-type="bibr">40</xref>,<xref rid="bib0041" ref-type="bibr">41</xref>,<xref rid="bib0042" ref-type="bibr">42</xref>]. The European Union's SOMA project contains an Observatory, which aims to support experts in their work against disinformation by providing them with cyberinfrastructure and a network of people working on misinformation detection [<xref rid="bib0040" ref-type="bibr">40</xref>,<xref rid="bib0041" ref-type="bibr">41</xref>,<xref rid="bib0042" ref-type="bibr">42</xref>]. Members of the Observatory have access to existing verification platforms as well as new tools and algorithms <xref rid="bib0040" ref-type="bibr">[40]</xref>. All members of the SOMA project are given access to EUNOMIA, a platform for analyzing the source, modification history, and trustworthiness of a piece of content which includes blockchain-based infrastructure, a digital companion which uses AI to analyze content and context, and the ability to vote on the trustworthiness of social media posts [<xref rid="bib0040" ref-type="bibr">40</xref>,<xref rid="bib0043" ref-type="bibr">43</xref>]. The platform requires the consent of the user to have the posts in their social media analyzed for trustworthiness [<xref rid="bib0044" ref-type="bibr">44</xref>,<xref rid="bib0043" ref-type="bibr">43</xref>]. SOMA members are also given access to SocialTruth, which provides individuals with access to “fake news” detection based on AI technology and content verification trust and integrity based on blockchain technology [<xref rid="bib0045" ref-type="bibr">45</xref>,<xref rid="bib0046" ref-type="bibr">46</xref>]. SocialTruth integrates its content verification with various platforms such as web search, journalist tools, content development, and a browser add-on [<xref rid="bib0045" ref-type="bibr">45</xref>,<xref rid="bib0046" ref-type="bibr">46</xref>]. WeVerify aims to address content verification challenges through participatory verification, open source algorithms, human-in-the-loop machine learning, and visualizations [<xref rid="bib0047" ref-type="bibr">47</xref>,<xref rid="bib0048" ref-type="bibr">48</xref>]. The project is designed for collaborative, decentralized content verification, tracking, and debunking [<xref rid="bib0047" ref-type="bibr">47</xref>,<xref rid="bib0048" ref-type="bibr">48</xref>] . WeVerify has specific elements designed for journalists, such as tools for detecting the spread of misinformation. Their inVID plugin, for example, is particularly useful for fact checking content, including video <xref rid="bib0049" ref-type="bibr">[49]</xref>. The Provenance project seeks to develop an intermediary-free solution for digital content verification <xref rid="bib0050" ref-type="bibr">[50]</xref>. The project claims that its solutions will make it easier for consumers to evaluate online information by providing a graphical guide that will clarify the source and history of a piece of content <xref rid="bib0050" ref-type="bibr">[50]</xref>. Moreover, work by the Provenance team found that “countermeasures which encourage citizens to reflect on the information they consume and choose to share is likely to be more effective than authoritative corrections” <xref rid="bib0051" ref-type="bibr">[51]</xref>.</p>
      </sec>
      <sec id="sec0006">
        <label>1.2.3</label>
        <title>Existing datasets</title>
        <p id="para0023">For the goal of enabling research on strategies to combat misinformation, there exists several potentially useful datasets. The LIAR dataset consists of 12,836 manually labeled short statements from politifact.com ranked as barely true, false, half true, mostly true, or pants on fire <xref rid="bib0052" ref-type="bibr">[52]</xref>. Other well known datasets includes the ISOT dataset, which consists of 21,417 real news articles and 23,481 “fake news” articles [<xref rid="bib0053" ref-type="bibr">53</xref>,<xref rid="bib0054" ref-type="bibr">54</xref>] and a dataset with 1000 news articles, evenly split between fake and legitimate news <xref rid="bib0055" ref-type="bibr">[55]</xref>.<xref rid="txtfn2" ref-type="fn">1</xref>
The presence of multiple datasets with misinformation content in multiple formats is useful, since the types of “fake news” experienced amidst the COVID-19 infodemic are broad and span multiple formats. One Twitter-specific dataset is CREDBANK, a crowdsourced dataset of accuracy assessments for events in Twitter, and another is PHEME, a dataset of potential rumors in Twitter and journalistic assessments of their accuracy <xref rid="bib0056" ref-type="bibr">[56]</xref>. Furthermore, BuzzFeed created a “fake news” dataset consisting of Facebook news, but their dataset has been used by Buntain et al. to extract parallel “fake news” data from Twitter [<xref rid="bib0039" ref-type="bibr">39</xref>,<xref rid="bib0040" ref-type="bibr">40</xref>]. Interestingly, one study found that models trained against the crowdsourced workers dataset (CREDBANK) outperformed models trained against the journalists’ assessment dataset (PHEME) when tested on Twitter data sourced from the BuzzFeed “fake news” dataset <xref rid="bib0039" ref-type="bibr">[39]</xref>. This is significant as it indicates that crowd-sourced data may be useful. Specifically, if crowd-sourced information is effective, platforms leveraging this approach could be powerful tools for rapidly developing user interfaces that allow crowd-sourced information collection across multiple platforms. Furthermore, prior analysis has found that certain datasets overrepresented some topics and underrepresented others <xref rid="bib0057" ref-type="bibr">[57]</xref>. When a model is trained on an unbalanced dataset and exposed to a new type of data, it is liable to arbitrarily place the new data in the wrong class <xref rid="bib0057" ref-type="bibr">[57]</xref>. Moreover, there have been calls to action to create a greater volume of reliably labelled “fake news” articles <xref rid="bib0057" ref-type="bibr">[57]</xref>.</p>
      </sec>
    </sec>
    <sec id="sec0007">
      <label>1.3</label>
      <title>COVID-19 misinformation research</title>
      <p id="para0024">After the outbreak of COVID-19, rich literature has emerged surrounding the amount of misinformation, the type of misinformation, and approaches to combat misinformation. This work has examined global trends on Twitter by country, analyzing tweet volume according to specific themes in coronavirus-related queries, posts related to specific myths surrounding the virus, and the number of tweets containing items deemed myths <xref rid="bib0058" ref-type="bibr">[58]</xref>. Other work attempts to find warning signs that a country will experience an infodemic <xref rid="bib0058" ref-type="bibr">[58]</xref>. Furthermore, studies have shown that hateful content is rapidly evolving and becoming increasingly coherent as time continues <xref rid="bib0058" ref-type="bibr">[58]</xref>.</p>
      <sec id="sec0008">
        <label>1.3.1</label>
        <title>Results of research characterizing the infodemic</title>
        <p id="para0025">A significant finding supporting the existence of an infodemic surrounding COVID-19 is that fact checkers are overburdened. One study found that after the outbreak of COVID-19, the number of English-language fact checks increased by 900% from January to March, 2020 <xref rid="bib0004" ref-type="bibr">[4]</xref>. Despite this increase in the number of fact checks, fact checkers have limited resources and cannot check all problematic content <xref rid="bib0004" ref-type="bibr">[4]</xref>. In addition to research characterizing the types and quantities of information shared as news, there has been similar research characterizing traits of COVID-19 information shared on social media platforms, such as Twitter. One finding is that some keywords are correlated with misinformation. A study of 673 tweets over 14 hashtags and keywords related to the COVID-19 epidemic found that 24.8% of the sample included misinformation, 17.4% included unverifiable information, and tweets from unverified Twitter accounts contained more misinformation <xref rid="bib0059" ref-type="bibr">[59]</xref>. Other work indicates that machine learning can be used to get information about the key phrases used by people discussing the pandemic, as well as the emotional sentiment among phrases in these groups <xref rid="bib0060" ref-type="bibr">[60]</xref>. The knowledge that machine learning can be used for sentiment analysis of tweets is significant since it would potentially allow research discussing the relationship between hateful COVID-19 misinformation and the sentiment of the content.</p>
      </sec>
      <sec id="sec0009">
        <label>1.3.2</label>
        <title>Research on combating COVID-19 misinformation</title>
        <p id="para0026">At the heart of the explosion of misinformation present in the COVID-19 infodemic is the question of what makes people share misinformation. A study found that people shared false claims related to COVID-19 partly because they didn't think sufficiently about whether or not the content was accurate before deciding what to share <xref rid="bib0061" ref-type="bibr">[61]</xref>. When participants were primed to think about accuracy at the beginning of a study, their level of truth discernment in whether they intended to share the COVID-19- related articles was more than doubled <xref rid="bib0061" ref-type="bibr">[61]</xref>. The conclusion was that priming individuals to think that accuracy is important to consider can significantly affect their truth discernment <xref rid="bib0061" ref-type="bibr">[61]</xref>. This work indicates that “truth nudging”, or priming individuals to think about accuracy of news content, could serve as a highly effective form of misinformation treatment while requiring virtually no computational power and potentially outperforming machines for certain types of misinformation. It also means that research on automated approaches should be carefully examined to ensure that the automated approach does not give a false illusion of certainty. If an automated approach claims that a piece of “fake news” content is real, it may make the user less likely to critically examine it, which in turn could decrease the user's truth discernment.</p>
        <p id="para0027">Other work explores the impact of providing a news feed that has been vetted for factuality to users. WashKaro uses NLP approaches, machine learning, and m-Health to provide authentic sources of information with daily news in Hindi and English, along with performing other functions such as contact tracing <xref rid="bib0062" ref-type="bibr">[62]</xref>. It provides authentic news by checking for similarity between new news articles and existing news articles in their dataset that were clustered according to which WHO guidelines they are similar to; when new guidelines are added, news articles with the 10 highest similarity ratings are provided to the user <xref rid="bib0062" ref-type="bibr">[62]</xref>. WashKaro provides information related to health guidelines, rather than COVID-19 information holistically. For example, their application does not seem to apply to racially-targeted hateful misinformation.</p>
        <p id="para0028">Other work examined malicious COVID-19 content and found that hateful COVID-19 content mobilizes and accumulates on platforms which allow specific community features (such as Facebook Pages / VKontakte groups), but then makes its way back to the mainstream <xref rid="bib0009" ref-type="bibr">[9]</xref>. The study also found that real, racially motivated violence occurred after the outbreak of COVID-19, suggesting potential implications of these hateful communities <xref rid="bib0009" ref-type="bibr">[9]</xref>. While malicious activity can appear isolated and largely eradicated on a given platform, it has likely just moved to another platform <xref rid="bib0009" ref-type="bibr">[9]</xref>. One possible research area this line of work opens up is on the effectiveness of utilizing the “truth nudging” strategy on a given news or media platform to let users know that when they leave one platform to visit another platform, there is a risk of encountering hateful or malicious content. Another potential research area, given the presence of hateful COVID-19 information, is determining the effectiveness of implementing sentiment detection (determining the predominant emotion in a text) using natural language processing to identify potentially hateful content. Finally, since malicious COVID-19 information is heavily decentralized and regulations on a single platform may simply push the information to other platforms, research questions evaluating whether an open-source, multi-platform misinformation detection tool could be useful become important. Perhaps one response to the proliferation of a variety of deregulated, open-source media platforms is a symmetric open-source, multi-platform misinformation detection tool which could be altered to allow usage on a wide variety of platforms.</p>
        <p id="para0029">Other approaches acknowledge that the massive volume of new online material surrounding COVID-19 makes manual analysis non-viable, opening the door to automated machine learning approaches to combat misinformation through counter-messaging <xref rid="bib0005" ref-type="bibr">[5]</xref>. It is not yet known whether overt targeted ads presenting counter-information would be as effective as a more holistic truth-nudging approach, which has been shown to double truth discernment in social media content sharing decisions <xref rid="bib0061" ref-type="bibr">[61]</xref>. Ultimately, it is clear that it is virtually impossible to address all of “fake news” in the form of a singular, generalizable solution. However, targeted approaches, like WashKaro's use of the WHO's guidelines to decide which news to share, have downsides in their ability to address new types of information. One way to address this is creating a tool which could be used to do a good enough job at detecting “fake news” on a wide variety of platforms, performs truth nudging to encourage users to critically reflect on the veracity of what they have read, and warns users of the dangers of leaving one platform for another.</p>
      </sec>
    </sec>
    <sec id="sec0010">
      <label>1.4</label>
      <title>Additional noteworthy COVID-19 misinformation research</title>
      <p id="para0030">There have been several studies that focused on the role played by Facebook in online discussions surrounding COVID-19. Research studying rumors claiming that the rollout of 5G technology was related to the outbreak of the COVID-19 pandemic found that conspiracy theories can start as ideas that move from fringe beliefs to entering mainstream discourse <xref rid="bib0063" ref-type="bibr">[63]</xref>. Therefore, understanding typical processes of idea spread is an important method for gaining a clearer picture of key points at which dissemination may be slowed or halted <xref rid="bib0063" ref-type="bibr">[63]</xref>. Other work which examined the role of Facebook advertisements in facilitating coronavirus-related conversation found instances of possible misinformation ranging from bioweapons conspiracy theories to unverifiable claims by politicians <xref rid="bib0064" ref-type="bibr">[64]</xref>. Non-English-language work using over 1.5 million Italian-language posts on Facebook related to COVID-19 found that sources of ‘supposedly’ reliable information experienced higher engagement compared to websites sharing unreliable content with a “small-world effect” observed in the sharing of URLs <xref rid="bib0065" ref-type="bibr">[65]</xref>. Moreover, users who navigate a limited set of pages/groups can be exposed to a wide range of content, ranging from extreme propaganda to verified information <xref rid="bib0065" ref-type="bibr">[65]</xref>. A study of alternative news media content on Facebook used computational content analysis to evaluate the validity of the claim that alternative news media outlets spread societal confusion and potentially dangerous “fake news” <xref rid="bib0066" ref-type="bibr">[66]</xref>. The authors found that while alternative news media outlets do not tend to spread obvious lies, they do predominantly share critical messages, including anti-establishment views (which oppose mainstream news media and the political establishment) that can contribute to worldviews based on mistrust <xref rid="bib0066" ref-type="bibr">[66]</xref>.</p>
      <p id="para0031">The literature studying the role of Twitter in online COVID-19 conversations is considerable. A study of two competing COVID-19 misinformation communities - misinformed users (who are actively posting misinformation) and informed users (who are spreading true information) - concluded that COVID-19 misinformed communities are denser and more organized than informed communities <xref rid="bib0067" ref-type="bibr">[67]</xref>. A significant volume of the misinformation they studied likely originated in disinformation campaigns, a large majority of misinformed users may be 'anti-vaxxers' (i.e., people who doubt the safety of vaccines or believe vaccination infringes on their human rights), and informed users tend to use more narratives than misinformed users <xref rid="bib0067" ref-type="bibr">[67]</xref>. A study of 43.3 million English-language tweets related to COVID-19 found evidence of the presence of bots in the COVID-19 discussion on Twitter <xref rid="bib0068" ref-type="bibr">[68]</xref>. High bot score accounts were found to use COVID-19-related content and hashtags to promote visibility of ideological hashtags that are typically associated with the alt-right in the United States and human users, on the other hand, are predominantly concerned with public health and welfare <xref rid="bib0068" ref-type="bibr">[68]</xref>. Research exploring high and low quality URLs shared on Twitter in the context of COVID-19 found that more tweets contained URLs from low quality misinformation websites compared to high quality health information websites, but both are present at a much lower rate compared to news sources <xref rid="bib0069" ref-type="bibr">[69]</xref>. While some high and low quality sites are connected, connections to and from news sources are more common <xref rid="bib0069" ref-type="bibr">[69]</xref>. The authors’ findings suggest that despite low quality URLs not being extensively shared in the COVID-19 Twitter conversation, there is “a well connected community of low quality COVID-19 related information” which has connections to both health and news sources <xref rid="bib0069" ref-type="bibr">[69]</xref>. Using a dataset of 67 million tweets from 12 million users, other work found that the majority of influential tweets were posted by news media, government officials, and individual news reporters, but the most influential tweets were posted by average, everyday users <xref rid="bib0070" ref-type="bibr">[70]</xref>. They observed that average users were also more likely to spread noncredible tweets, but that “many of these regular users appear to be bots” <xref rid="bib0070" ref-type="bibr">[70]</xref>.</p>
    </sec>
  </sec>
  <sec id="sec0011">
    <label>2</label>
    <title>Evaluation and development</title>
    <sec id="sec0012">
      <label>2.1</label>
      <title>Development considerations</title>
      <p id="para0032">We sought to rapidly and affordably develop a web tool leveraging natural language processing and highly accessible human feedback to provide an experience which has an accessible, easy-to-use experience and draws from a variety of sources to match a realistic COVID-19-related media diet. The project's development occurred from May-July 2020. Our tool was designed to be able to aid in contributing to the body of labelled data surrounding “fake news” and especially COVID-19 “fake news”. These key decision-making factors are described below:</p>
      <p id="para0033"><italic>Combining NLP and Human Feedback.</italic> As demonstrated in the analysis on prior models, it is difficult to create a completely accurate, robust, and fully-automated machine learning approach to “fake news” detection. Thus, while we decided to use machine learning, we also felt that the combination of machine learning with human feedback was very valuable, since it allows for misclassifications by the ML model to be detected by humans and for “fake news” which fools humans to be detected by ML models. This allows us to leverage ML without creating an illusion of absoluteness behind the results and enables our service to remain useful even for ML model “edge cases,” such as mostly true news with a few key words replaced to make the claims false.</p>
      <p id="para0034"><italic>Accessible and Realistic News Feed.</italic> Prior approaches are effective at helping users identify if a claim or body of text are rated true or false by machine learning models, but are often either for specific types of textual input or would be cumbersome to include in one's typical news consumption process. Therefore, through attempting to create an accessible news feed which is representative of the typical “media diet” of an average person, we aim to increase the likelihood that our fact checking tools will actually be used. Creating a dynamically generated feed from multiple sources avoids requiring users to input individual bodies of text to our checker, instead automatically processing and detecting user votes for every news item. Rather than creating an entirely academic, infrequently used tool for news generation, we aim to offer a step toward integrating “fake news” detection within a typical news consumption routine, which could lead to wider adoption.</p>
      <p id="para0035"><italic>High-Quality Labelled Data Collection.</italic> A goal of our project was the ability to help collect high-quality labelled misinformation data. Through crowdsourcing credibility information from human feedback, we provide an automated data generation system. If several hundred human voters ranked an article as true, that information could in some cases be more useful than a single data labeler's assessment, especially when considering the decision-making fatigue which could result from labeling large amounts of data, as is often needed for machine learning model training. We sought to create a tool with results which could either be used directly to train machine learning models, or with “mostly accurate” labelled data which could accelerate the task of manual data labeling.</p>
    </sec>
    <sec id="sec0013">
      <label>2.2</label>
      <title>Evaluation of tools and frameworks</title>
      <sec id="sec0014">
        <label>2.2.1</label>
        <title>Frontend/Backend language and hosting</title>
        <p id="para0036">The CoVerifi frontend is a web app written in React.js, a JavaScript library created by Facebook. We used a sample React Twitter Feed web app, which was available on GitHub with a MIT license, allowing modification, distribution, and commercial use <xref rid="bib0075" ref-type="bibr">[75]</xref>. The frontend uses Firebase Hosting, which allows free hosting of React.js projects. Since we developed and host our own ML model, we used a Python-based architecture with Flask, a framework for developing web services that allows for the creation of API endpoints for communication between the backend and frontend. The backend is hosted by Heroku, which is free for our use case. Prior to deciding on Heroku, we attempted to host our service on AWS EC2 and AWS Elastic Beanstalk, but both had costs and complexities. A limitation, however, is that it sleeps after 30 min of inactivity <xref rid="bib0076" ref-type="bibr">[76]</xref>, which means that the first API call within a 30-minute period will take 30 s, whereas subsequent calls will be quick.</p>
      </sec>
      <sec id="sec0015">
        <label>2.2.2</label>
        <title>News API decision process</title>
        <p id="para0037">Since the Google News Search API is deprecated, we chose from other existing options detailed in <xref rid="tbl0001" ref-type="table">Table 1</xref>
. Though we initially used newsapi.org due to their free tier and cheaper per-request cost, the Bing News Search API, has a free, 1000 requests/month option, and another option where every 1000 requests costs $4 (<xref rid="tbl0001" ref-type="table">Table 1</xref> provides a detailed breakdown of APIs and their advantages/disadvantages). While there exists seemingly cheaper options, such as ContextualWeb News API, we erred on the side of choosing a more widely-known tool, Bing News Search, since we plan on performing labelled data collection at a scale of between 1000 and 10,000 requests.<table-wrap position="float" id="tbl0001"><label>Table 1</label><caption><p>Comparison of News APIs.</p></caption><alt-text id="alt0004">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Service Name</th><th valign="top">Free requests</th><th valign="top">Paid tier pricing</th><th valign="top">Additional Info</th></tr></thead><tbody><tr><td valign="top">NewsAPI.org <xref rid="bib0071" ref-type="bibr">[71]</xref></td><td valign="top">500/day</td><td valign="top">$449 for 250,000, then $44.90 per 25,000 calls</td><td valign="top">Cannot make requests from browser in free tier</td></tr><tr><td valign="top">Bing News Search <xref rid="bib0072" ref-type="bibr">[72]</xref></td><td valign="top">1000/month</td><td valign="top">$4 per 1000 transactions</td><td valign="top">Ability to set budgets. If 1000/month is exceeded, no charge is incurred.</td></tr><tr><td valign="top">Currents API <xref rid="bib0073" ref-type="bibr">[73]</xref></td><td valign="top">600/day</td><td valign="top">$150 for 300,000 requests, then $25 per 25,000 requests</td><td valign="top">Free tier says “No Access to Articles”, while paid tier says “Access to Articles”</td></tr><tr><td valign="top">ContextualWeb News API <xref rid="bib0074" ref-type="bibr">[74]</xref></td><td valign="top">10,000/month</td><td valign="top">$0.5 for 1000 requests after 10,000 exceeded</td><td valign="top">Not very well known. Potential for overage charges: “Depending on your plan's specification, you will either incur overage charges or be suspended.”</td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="sec0016">
        <label>2.2.3</label>
        <title>Machine learning model</title>
        <p id="para0038">As a starting point, we implemented a text classification model trained on the outputs of OpenAI's GPT-2 <xref rid="bib0032" ref-type="bibr">[32]</xref> model for generation of neural fake text, hosted by Hugging Face<xref rid="cit_1" ref-type="fn">2</xref>
as a part of their hosted inference API <xref rid="bib0077" ref-type="bibr">[77]</xref>. This free solution labels a piece of text as “fake” (meaning machine-generated) or “real” (meaning human-generated) with an associated confidence. Specific API endpoint information is provided in this paper's GitHub repository at <ext-link ext-link-type="uri" xlink:href="https://github.com/nlkolluri/CoVerifi" id="interref0001">https://github.com/nlkolluri/CoVerifi</ext-link>. While we believe this model provides a valuable signal of whether text might be machine generated due to the intuition that text generated by different models may have similar characteristics, it is important to note that its accuracy is highest on fake text generated by GPT-2 versus other models. When discussing the GPT-2 output detector, the phrases “fake text” and “real text” only refer to whether the text was generated by a machine or by a human and do not in any way refer to whether the text's content is considered true or false. As such, the GPT-2 output detector does not make a direct claim about the veracity of a piece of content. There are cases in which a language model produces a true sentence, and there are also cases in which a human writes a false statement. We chose to include the GPT-2 output detector in our platform because we believe that, while being machine generated is not the same as being false, knowing whether a piece of content is likely to be machine generated may help gage credibility.</p>
        <p id="para0040">CoVerifi also features a machine-learning model which we developed and trained on a COVID-19 specific misinformation dataset, CoAID <xref rid="bib0078" ref-type="bibr">[78]</xref>. Moreover, our model and code usage samples are all available open source in order for other research teams to extend and develop our work. We trained a Bidirectional LSTM on 1257 pieces of news content from CoAID and internally validated it on 419 pieces of news content from CoAID, as part of a 75% train and 25% test split. When using a weighted average across the two labels and rounding to the nearest hundredth, our F1-score was 0.93, with equal precision, recall, and accuracy. To assess generalizability to new types of COVID-19-specific misinformation, we created a dataset containing approximately 7000 pieces of COVID-19-specific misinformation content. This dataset includes “fake news” labeled content (produced by Poynter) and verified news (for which we inherit news source credibility). Since the Poynter news contained several different labels, we assigned a label of 1 to all content with the label of “TRUE” and a label of 0 to all other content. As such, there may be a very small amount of mislabelled content present. We then tested our model trained on CoAID on this new dataset. When using a weighted average across the two labels and rounding to the nearest hundredth, our F1-score was 0.75, with equal precision, recall, and accuracy. While inheriting news source credibility may not be as reliable as manual labeling, this is less of an issue in an external validation set: it still shows that our model can perform better than random on new data sources. Furthermore, the F1- score for the “false” label, which came from Poynter, is 0.79. This is better than our F1-score of 0.70 for the “true” label.</p>
        <p id="para0041">Since our false-labelled news came from an established fact checking organization, Poynter, and our true-labelled news was obtained through the less-reliable method of inheriting news source credibility, it is reasonable to conclude that the false news F1-score is more indicative of the performance we would have with a perfectly-labelled dataset. This leads us to believe that rigorous manual labeling for each label in the external validation set could potentially increase accuracy.</p>
      </sec>
      <sec id="sec0017">
        <label>2.2.4</label>
        <title>Additional tools decision process</title>
        <p id="para0042">Since News APIs typically only provide a brief subsection of the full article text along with a URL, we use the news-please <xref rid="bib0079" ref-type="bibr">[79]</xref> news crawler. To make the Twitter API easier to access, the Tweepy library was used <xref rid="bib0080" ref-type="bibr">[80]</xref>. We use Google Firebase's Cloud Firestore, which allows for a document-based database.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec0018">
    <label>3</label>
    <title>System design</title>
    <p id="para0043">As illustrated in <xref rid="fig0001" ref-type="fig">Fig. 1</xref>
, CoVerifi can best be understood as 3 separate parts, (1) a frontend, public-facing web app written in React.js, and (2) a backend Python service which can be accessed through simple API calls from the front end with JSON input data and JSON output data, and (3) a database storing user vote information. Because they are separate parts, we hosted them using distinct services as detailed in <xref rid="sec0011" ref-type="sec">Section 2</xref>.<fig id="fig0001"><label>Fig. 1</label><caption><p>System Interactions of CoVerifi.</p></caption><alt-text id="alt0001">Fig. 1</alt-text><graphic xlink:href="gr1"/></fig></p>
    <sec id="sec0019">
      <label>3.1</label>
      <title>CoVerifi frontend</title>
      <p id="para0044">CoVerifi has 5 different news options: COVID-19 news from the Bing News Search API, Breaking News from the Bing News Search API, Reddit's news subreddit, news from Twitter, and the option to “search” for a specific query in the Bing News Search API by using the search bar. For each of these options, a “feed” is created with several entries, each corresponding to a news article, Reddit post, or tweet and information about the piece of news content, the machine learning model assessments of the content's credibility, the credibility of the content as assigned by the user voters, a button to rate the news as “credible”, and a button to rate the news as “fake” is displayed (see <xref rid="fig0002" ref-type="fig">Fig. 2</xref>, <xref rid="fig0003" ref-type="fig">Fig. 3</xref>
).<fig id="fig0002"><label>Fig. 2</label><caption><p>COVID-19 News from Bing News Search API.</p></caption><alt-text id="alt0002">Fig. 2</alt-text><graphic xlink:href="gr2"/></fig><fig id="fig0003"><label>Fig. 3</label><caption><p>Tweets from Twitter API.</p></caption><alt-text id="alt0003">Fig. 3</alt-text><graphic xlink:href="gr3"/></fig></p>
      <sec id="sec0020">
        <label>3.1.1</label>
        <title>Bing news search API and Reddit news</title>
        <p id="para0045">For the Bing News Search API and the Reddit news API, the respective API is called with the query information. This leaves limited information about the article, since the Bing News Search API and Reddit API only load very limited content from the article/post itself. Thus, for each entry, the URL of each piece of content is sent to the backend API in JSON format. The backend returns the full-text of the article, the GPT-2 output detector model's classification of how likely the piece of content is real or fake, and our machine learning model's classification of the title of the news content in JSON format. The pieces of content are updated one-by-one as the backend finishes processing them. Given that the Heroku backend “sleeps” (causes a 30 second response time after 30 min of inactivity), constructing the frontend in this way allowed us to mimic the responsive behavior of our code at production. The Bing News API requests can eventually originate from the backend.</p>
      </sec>
      <sec id="sec0021">
        <label>3.1.2</label>
        <title>Twitter news</title>
        <p id="para0046">For Twitter, a different format is followed wherein the frontend makes a call to the backend with the Twitter query (i.e., ‘COVID-19′ or ‘Coronavirus’). The backend returns a JSON object with the information to be displayed about several tweets. Articles were not classified in the prior step. Then, each tweet is individually passed to the backend to perform classification by the machine learning models.</p>
      </sec>
      <sec id="sec0022">
        <label>3.1.3</label>
        <title>User credibility ratings</title>
        <p id="para0047">To display the user credibility ratings on the initial load of the page, the database is checked for each entry, using the expanded text of the article or tweet as the database key. Once a user votes on a particular entry, their vote is recorded and the updated state of the database is retrieved. This means that if a second user voted between the time that the page loaded and when the first user voted, the second user's vote is still displayed. Displaying the most recent state of the database without needing to refresh the page thus functions as an incentive for the user to vote, which in turn allows us to collect more labelled data with respect to the credibility of news content.</p>
        <p id="para0048">To combat concerns about abuse of crowd-sourced credibility ratings, we have implemented IP-based abuse prevention. There can only be one vote per piece of content per IP address. The IP-based vote limiting is accomplished by retrieving the IP address on the client side using ipapi.co <xref rid="bib0081" ref-type="bibr">[81]</xref> and only allowing that IP address to cast one vote per piece of content. It is important to note that the current implementation of our crowd-based labeling in CoVerifi would benefit from improvements in security (see <xref rid="sec0028" ref-type="sec">Section 5</xref> for a detailed discussion of limitations and areas for improvement).</p>
      </sec>
    </sec>
    <sec id="sec0023">
      <label>3.2</label>
      <title>CoVerifi backend</title>
      <p id="para0049">Our design consists of three endpoints: one for getting tweets, one for classifying tweets, and one for getting article expanded text and processing the article, all of which can be accessed through a HTTP POST request.</p>
      <p id="para0050"><italic>Collecting and Processing Tweets.</italic> Once the information queried from the Twitter API via the Tweepy package is received, tweets are filtered to ensure that they are not retweets. A custom JSON object is created which includes selected information from every tweet returned by Tweepy. Then, the JSON object is returned as an HTTP response and used by the frontend. The endpoint for processing tweets is separate from the collecting tweets endpoint so that individual tweets can be processed one by one, rather than delaying the display of tweets until all tweets have been processed. The expanded text of the tweet is sent to the endpoint with an HTTP POST request in the form of a JSON object. Then, a classifier trained on outputs from OpenAI's GPT-2 for detection of neural “fake news” is used to process the tweet, which in practice means submitting a HTTP request to an API provided by Hugging Face <xref rid="bib0077" ref-type="bibr">[77]</xref>. At this step, we also collect the Botometer score for the tweet's poster. These metrics for determining if the tweet is credible, wrapped in a JSON object, are returned as an HTTP response.</p>
      <p id="para0051"><italic>Get Article Text and Process</italic>. The endpoint for getting article expanded text and processing article expanded text is accessed through an HTTP POST request containing a JSON object with the URL of the piece of news content. The URL is passed to the news-please library, which retrieves the entire text of the news article being processed. A 300-character subsection of the expanded text of the article is then passed into the GPT-2 output detector model API endpoint for neural “fake news” detection. We also pass the title of the article into our machine learning model, which outputs a credibility score.</p>
      <p id="para0052">The expanded text of the article, the credibility rating outputted by the GPT-2 output detector model, and the output of our machine learning model are packaged together in a JSON object and returned as an HTTP Response. The frontend uses the updated expanded text to retrieve information from the database, as well as to display more information from the article where relevant.</p>
      <p id="para0053"><italic>Database</italic>. CoVerifi's Cloud Firestore database consists of a collection of documents, where the key of each document is a subsection of the expanded text of a piece of news content, which allows each document to be uniquely identified in the database. An alternative keying strategy could be using the URL of the piece of news content, but the current strategy allows our web app to be expanded to display types of news content which can be collected via API call but may not have an associated link, or content with a non-constant URL.</p>
    </sec>
  </sec>
  <sec id="sec0024">
    <label>4</label>
    <title>Discussion and implications for future work</title>
    <p id="para0054">CoVerifi enables many future research directions as its code is provided as open source. The source code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/nlkolluri/CoVerifi" id="interref0002">https://github.com/nlkolluri/CoVerifi</ext-link> and the website is publicly available at <ext-link ext-link-type="uri" xlink:href="https://coverifi.web.app/" id="interref0003">https://coverifi.web.app/</ext-link>. This section reflects on the utility of CoVerifi and what lines of research the platform opens up for future work.</p>
    <sec id="sec0025">
      <label>4.1</label>
      <title>Misinformation hypothesis testing</title>
      <p id="para0055">A survey of 2501 respondents from Singapore found that most social media users simply ignore “fake news” posts they come across on social media, only offering corrections when the issue is strongly relevant to them and to people with whom they share a strong personal relationship <xref rid="bib0082" ref-type="bibr">[82]</xref>. An active call for intervention is potentially useful because it would allow researchers to frame the COVID-19 infodemic as something strongly relevant to media consumers, which in turn could allow organic checks on misinformation. CoVerifi fulfills this function by actively asking users to provide their feedback on the credibility of news content which is displayed on CoVerifi. Second, by providing users an abundance of potentially false news information which needs classification, CoVerifi reinforces the notion that “fake news” is an issue which requires intervention on behalf of anyone who encounters it. This would enable researchers to compare the truth discernment on CoVerifi, where an active call for intervention is requested, with the truth discernment of users deciding what to share on social media. Additionally, research could be done comparing a group who first used CoVerifi and then used social media with a control group who only used social media. It is possible that prior usage of CoVerifi could have a priming effect and result in media consumers being more likely to intervene against misinformation when they encounter it.</p>
      <p id="para0056">Given that when participants were primed to think about accuracy at the beginning of a study, their level of truth discernment in whether they intended to share the COVID-19-related articles was more than doubled <xref rid="bib0061" ref-type="bibr">[61]</xref>, a future direction for use with our platform is quantifying the increase in effectiveness at identifying “fake news” which users experience when using our platform. Perhaps asking users to vote on individual pieces of news will implicitly prime users to consider accuracy in their responses. It could also be examined whether using CoVerifi, combined with explicit truth nudging, yields any benefit over using just CoVerifi. Evaluating the efficacy of either subtly or overtly nudging users to think about the accuracy of content is another potential use case of CoVerifi.</p>
      <p id="para0057">Because users on mainstream media platforms are often at most a few clicks away from malicious, hateful content <xref rid="bib0009" ref-type="bibr">[9]</xref>, one line of research could be to evaluate the effectiveness of navigation warnings at preventing users from ending up on a page with malicious, hateful misinformation. Due to the demonstrated effectiveness at truth-nudging <xref rid="bib0061" ref-type="bibr">[61]</xref>, it is possible that giving a warning prior to a user leaving CoVerifi for another, potentially unreliable site could increase user awareness of unreliable content.</p>
      <p id="para0058">With the presence of malicious COVID-19 narratives being potentially linked to offline anti-Asian violence [<xref rid="bib0005" ref-type="bibr">5</xref>,<xref rid="bib0009" ref-type="bibr">9</xref>,<xref rid="bib0010" ref-type="bibr">10</xref>], hateful misinformation is a real issue. Perhaps an approach which combines traditional credibility detection strategies with sentiment detection strategies to identify hateful or aggressive sentiment could be useful in helping prevent the spread of hateful misinformation. In the context of our web app, a sentiment rating could be displayed below the computed credibility rating. This would allow research on whether angry or aggressive sentiment is correlated with malicious misinformation.</p>
    </sec>
    <sec id="sec0026">
      <label>4.2</label>
      <title>Optimization and scaling</title>
      <p id="para0059">Through moving to use a more scalable hosting set-up, a different machine learning model, and different news API payment tier (or implementing a query caching system), we aim to increase the scalability of CoVerifi. This would allow seamless integration of fact verification with the average media diet and news consumption routines, allowing us to help reduce the spread of COVID-related misinformation.</p>
      <p id="para0060">In the context of our project, we could continually update the machine learning model used on our CoVerifi as we collect more data. Through combining machine learning with human feedback, we can minimize the errors present in either approach. Due to the fact that “fake news” often includes a confusing mix of “true” news and misinformation, humans can often be inaccurate at identifying “fake news” on their own, achieving only 50–63% success at identifying “fake news” <xref rid="bib0083" ref-type="bibr">[83]</xref>. Another study found that while humans were better at identifying “fake news” content in the Celebrity category than the automated system, their system outperformed humans while detecting “fake news” in more serious and diverse news sources <xref rid="bib0055" ref-type="bibr">[55]</xref>. Due to human error in “fake news” identification, it is possible that our display of a machine learning model's classification prior to a user voting may minimize the rate of incorrect human classification. The result may be that our crowdsourced data is higher quality than the initial labelled dataset, and this hypothesis could be tested by an iterative approach of continually retraining as more data is introduced. Moreover, due to the established presence of hateful and malicious information on a variety of platforms, there remains a need for ways to check the validity of content on new platforms. CoVerifi could be used to rapidly develop new content consumption streams by simply swapping the API from Reddit/Twitter to an API which collects content from a new information sharing platform.</p>
    </sec>
    <sec id="sec0027">
      <label>4.3</label>
      <title>Data collection and analysis</title>
      <p id="para0061">Given that models trained against a crowdsourced dataset (CREDBANK) outperformed models trained against the journalists’ assessed dataset (PHEME) when tested on a set of credibility-labelled tweets <xref rid="bib0056" ref-type="bibr">[56]</xref>, crowdsourcing data may have considerable merit. Our web app can thus function as an easy-to-use interface to allow crowdsourcing of factuality data, which could in turn help create high-quality labelled datasets. While we believe crowdsourced credibility assessments are a useful feature of the CoVerifi platform, this approach is not new. A decade ago, Ratkiewicz et al. <xref rid="bib0084" ref-type="bibr">[84]</xref> used crowdsourced judgements to label memes as “Truthy” or “Legitimate”.</p>
      <p id="para0062">Since CoVerifi leverages multiple platforms (Bing News Search, Reddit, Twitter) to provide a diverse set of news providers mimicking a typical media diet, our platform could be used to perform comparative analysis on how well machine learning models respond to the various platforms. Additionally, our tool could allow comparative research on how users perceive the accuracy of platforms.</p>
      <p id="para0063">We evaluated whether training on only COVID-19- specific “fake news”, on only general “fake news”, or on a combination of COVID-19-specific “fake news” and general “fake news” would yield the best results when tested on a new COVID-19-specific dataset. We used a COVID-19-specific “fake news” dataset from CoAID and a general “fake news” dataset from FakeNewsNet to evaluate generalizability to our dataset discussed in <xref rid="sec0016" ref-type="sec">Section 2.2.3</xref>. For 7 different combinations of data sources including COVID-19 “fake news”, general “fake news”, or both, we trained implementations of a Support Vector Machine (SVM) classifier, a Logistic Regression classifier, and a Bernoulli Naive Bayes (Bernoulli NB) model. We found that the inclusion of CoAID maintained or improved performance compared to not including it, confirming that including COVID-19-specific misinformation content improves model performance.</p>
    </sec>
  </sec>
  <sec id="sec0028">
    <label>5</label>
    <title>Limitations</title>
    <p id="para0064">Our project does have limitations as many design decisions were significantly influenced by prioritizing speed and cost. Due to CoVerifi's lack of set-up costs, it is not yet suited for large-scale usage. This limits the scale of research questions that can be answered using our tool in its present state. Specifically, Heroku's hosting will only allow us to have our server “awake” for 550 h/month, sleeping after 30 min of activity. Additionally, we are currently using the Bing News API in the 1000 requests/month setting, but in the future, we will either upgrade to a setting allowing additional requests at $4 per 1000 requests or perform caching of search results to decrease the number of API calls. Moreover, a limitation of our model choice is that it only detects robot-generated “fake news”, though this opens up future directions for model development that we discuss in <xref rid="sec0024" ref-type="sec">Section 4</xref>. Lastly, CoVerifi's current news inputs reflect predominantly Western preferences; however, other, more international API endpoints can be added as needed by others.</p>
    <p id="para0065">While CoVerifi cannot prevent certain platforms from eventually restricting access to their content, it appears that the presence of multiple avenues for obtaining content (i.e., news sources pulled from Bing News Search, Reddit news, and Twitter) will prevent CoVerifi from becoming obsolete. We also believe that CoVerifi, through providing multiple credibility metrics in a single unified location, minimizes the amount of time required by the user. However, there is a necessity of user interpretation, since prior truth nudging research <xref rid="bib0061" ref-type="bibr">[61]</xref> indicates that critically thinking about accuracy is beneficial for truth discernment.</p>
    <p id="para0066">A significant limitation of CoVerifi is that our crowd-based labeling is not currently at a fully secure stage. Specifically, CoVerifi could be vulnerable to a coordinated attack aimed at deceptively enhancing the credibility of a specific news feed. While CoVerifi protects against a single user repeatedly voting on a piece of content through our website from the same IP address, it does not protect against more sophisticated approaches involving programmatically accessing our database, coordinated attacks performed by several users, or a user voting from several different IP addresses. One important direction for increasing the security of our crowd-based labeling is handling all database access on the backend rather than the frontend to decrease the vulnerability of our data to programmatic manipulation. Additionally, IP-based activity monitoring and analysis should be used to detect suspicious behavior. For example, if sets of IP addresses always tend to promote the same content or if sets of IP addresses always disagree with our machine learning models, these IPs could be flagged for manual review. Such approaches have the potential to mitigate the risk associated with both coordinated attacks performed by several users as well as attacks performed by single users across several IP addresses. The IP-based activity monitoring functionality might also be used to cross-check all votes. If a vote has been recorded in the database without a valid IP address from which it originated, this could reveal vulnerabilities in the crowd-based labeling mechanism and indicate that the integrity of the crowd-based data has been compromised. Another option for improving the security of our system is to incorporate user authentication and require users to be logged in to vote. This could be combined with IP checking and the blocking of suspicious user behavior. Given that our crowd-based labeling is not completely secure, it is important that future researchers are mindful of limitations and consider how to incorporate a level of security appropriate for their use case.</p>
    <p id="para0067">Furthermore, there is a concern highlighted in the literature that a labelled news-feed could backfire by making users too dependent on the credibility labels, unable to discern truth for themselves. For example, in the case of detecting images related to “fake news”, misinformation, disinformation, credibility labels were found to be “supplanting reasoned deliberation with mechanistic verification” <xref rid="bib0085" ref-type="bibr">[85]</xref>. CoVerifi addresses this concern by providing (1) a disclaimer section and (2) a multi-faceted automated credibility detection approach. The disclaimer section of CoVerifi provides accuracy metrics for our machine learning model in order to establish that the possibility of error is very real and we are transparent in this disclosure. In our disclaimer, we have included text to indicate that readers should be critical of the news content they read. This is drawn from other empirical work which shows that truth-nudging increases the accuracy of a user's own critical judgements <xref rid="bib0061" ref-type="bibr">[61]</xref>.</p>
    <p id="para0068">We also utilize a multi-faceted automated credibility detection approach. For news content, the neural “fake news” detector can give a signal that the text may have been machine-generated and the machine learning model we trained can provide a signal that the news article title is similar to that of “fake news” articles. For Twitter, the neural detector can produce a signal that the text itself may have been machine-generated, while the Botometer API score can produce a signal that the account posting the tweets may be a bot. Through providing multiple credibility scores which often disagree with each other, we highlight the active role of the user to decide the credibility of the piece of news content they are viewing. CoVerifi critically provides additional information and adds a data-driven layer to a user's ability to discern credibility, rather than asserting a definitive credibility score. A user can employ this information to help decide whether a seemingly-robotic piece of content may have actually been generated by a robot, whether a hyperbolic title bears similarity to “fake news”, or whether other viewers of the site think the content is false.</p>
    <p id="para0069">The literature also raises concerns that credibility labels could simply be ineffective. While Gao et al. <xref rid="bib0086" ref-type="bibr">[86]</xref> “did not find that credibility labels had any effect on people's perception of fake news”, they found that “credibility labels mitigate selective exposure bias, especially for users with liberal stances”, which suggests that “credibility labels could marginally decrease people's level of agreement on news articles on their own side, which may lead to a more moderate opinion space”. However, Gao et al.’s <xref rid="bib0086" ref-type="bibr">[86]</xref> study only contained content on two topics (gun control and U.S. President Donald Trump) from 14 articles (8 of which were labelled for credibility) and acknowledges the benefits of more long-term studies on the impact of labeling. The limitations of their study, the lack of significant negative effects found from credibility labeling, and the impact of credibility labeling on mitigating selective exposure bias indicates that further work which evaluates credibility labeling remains worthwhile.</p>
    <p id="para0070">Other work indicates that repeatedly labeling a claim as false can give the illusion that the claim is true by increasing familiarity with the content <xref rid="bib0087" ref-type="bibr">[87]</xref>. This is supported by studies that have demonstrated that exposing people to claims increases the perceived truth of the claim when it is seen again later <xref rid="bib0087" ref-type="bibr">[87]</xref>, <xref rid="bib0088" ref-type="bibr">[88]</xref>, <xref rid="bib0089" ref-type="bibr">[89]</xref>. This is the case even for statements that are explicitly identified as false on initial presentation [<xref rid="bib0087" ref-type="bibr">87</xref>,<xref rid="bib0090" ref-type="bibr">90</xref>,<xref rid="bib0091" ref-type="bibr">91</xref>]. As Polage, argues, news source credibility has been found to be directly affected by repeated exposure <xref rid="bib0092" ref-type="bibr">[92]</xref>. Specifically, “people will believe information to be true if it is repeated, if it does not contradict previously stored knowledge, and if the source has not been discredited” <xref rid="bib0092" ref-type="bibr">[92]</xref>. CoVerifi mitigates this issue due to the many unique pieces of content displayed on the platform at any given time. Since we derive content from the APIs of Bing News, Reddit, and Twitter dynamically, the volume and variety of content means that the risk of familiarity with content is very low.</p>
  </sec>
  <sec id="sec0029">
    <title>Conclusion</title>
    <p id="para0071">The explosion of misinformation, disinformation and hate news associated with the COVID-19 infodemic has left fact checkers overburdened. Given the massive quantity of “fake news”, automated approaches are imminently important as a way of minimizing the damage of the infodemic. We introduce CoVerifi, a solution which combines the power of truth-nudging, human feedback, and machine learning in a highly platform- and information-agnostic manner. CoVerifi provides a multi-channel credibility check for news, which means that “fake news” that has failed to be detected by the automated approaches can be detected by user feedback and vice versa. The presence of multiple platforms means that CoVerifi can reflect diverse media diets. Moreover, this enables CoVerifi to be used to analyze new types of content on more regionally specific platforms. Our open sourced code enables others to host CoVerifi, including deploying it with paid services for greater scalability. CoVerifi's code could then be connected to a different classification model or a different news API to analyze different types of data. Furthermore, the querying feature present in the current version allows virtually any news article to be fact checked, since Bing News Search contains articles from an expansive range of news sources. It is our intention that CoVerifi provides a starting point for new research directions, allowing researchers to rapidly create accessible services to address a wide range of misinformation concerns and research questions across a broad spectrum of platforms and disciplines.</p>
  </sec>
  <sec id="sec0030">
    <title>Author statement</title>
    <p id="para0072">Nikhil L. Kolluri: Software, Conceptualization, Methodology, Writing- Original draft preparation, Writing- Reviewing and Editing. Dhiraj Murthy: Conceptualization, Methodology, Writing- Original draft preparation, Writing- Reviewing and Editing, Supervision.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="para0073">The authors declare that there are no competing interests.</p>
  </sec>
</body>
<back>
  <ref-list id="cebibl1">
    <title>References</title>
    <ref id="bib0001">
      <label>1</label>
      <element-citation publication-type="journal" id="sbref0001">
        <person-group person-group-type="author">
          <name>
            <surname>Hua</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Shaw</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Corona virus (covid-19) “infodemic” and emerging issues through a data lens: the case of china</article-title>
        <source>Int. J. Environ. Res. Public Health</source>
        <volume>17</volume>
        <issue>7</issue>
        <year>2020</year>
        <fpage>2309</fpage>
        <pub-id pub-id-type="doi">10.3390/ijerph17072309</pub-id>
        <comment>URL http://dx.doi.org/10.3390/ijerph17072309</comment>
        <pub-id pub-id-type="pmid">32235433</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0002">
      <label>2</label>
      <mixed-citation publication-type="other" id="sbref0002">World Health Organization, Coronavirus disease (COVID-19) Pandemic (2020 (accessed November 23, 2020)). URL https://www.who.int/emergencies/diseases/novel-coronavirus-2019.</mixed-citation>
    </ref>
    <ref id="bib0003">
      <label>3</label>
      <element-citation publication-type="journal" id="sbref0003">
        <person-group person-group-type="author">
          <name>
            <surname>Pulido</surname>
            <given-names>C.M.</given-names>
          </name>
          <name>
            <surname>Villarejo-Carballido</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Redondo-Sama</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Gómez</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Covid-19 infodemic: more retweets for science-based information on coronavirus than for false information</article-title>
        <source>Int. Sociol.</source>
        <volume>35</volume>
        <issue>4</issue>
        <year>2020</year>
        <fpage>377</fpage>
        <lpage>392</lpage>
        <pub-id pub-id-type="doi">10.1177/0268580920914755</pub-id>
        <comment>arXiv: https://doi.org/10.1177/0268580920914755URL https://doi.org/10.1177/0268580920914755</comment>
      </element-citation>
    </ref>
    <ref id="bib0004">
      <label>4</label>
      <element-citation publication-type="book" id="sbref0004">
        <person-group person-group-type="author">
          <name>
            <surname>Brennen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Simon</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Howard</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Nielsen</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <part-title>Types, Sources, and Claims of COVID-19 Misinformation</part-title>
        <year>2020</year>
        <comment>04</comment>
      </element-citation>
    </ref>
    <ref id="bib0005">
      <label>5</label>
      <element-citation publication-type="journal" id="sbref0005">
        <person-group person-group-type="author">
          <name>
            <surname>Sear</surname>
            <given-names>R.F.</given-names>
          </name>
          <name>
            <surname>Velásquez</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Leahy</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Restrepo</surname>
            <given-names>N.J.</given-names>
          </name>
          <name>
            <surname>Oud</surname>
            <given-names>S.E.</given-names>
          </name>
          <name>
            <surname>Gabriel</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Lupu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>N.F.</given-names>
          </name>
        </person-group>
        <article-title>Quantifying covid-19 content in the online health opinion war using machine learning</article-title>
        <source>IEEE Access</source>
        <volume>8</volume>
        <year>2020</year>
        <fpage>91886</fpage>
        <lpage>91893</lpage>
        <pub-id pub-id-type="pmid">34192099</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0006">
      <label>6</label>
      <mixed-citation publication-type="other" id="sbref0006">S. Almasy, H. Yan, M. Holcombe, Coronavirus pandemic hitting some African-American communities extremely hard (2020 (accessed July 18, 2020)). URL <ext-link ext-link-type="uri" xlink:href="https://www.cnn.com/2020/04/06/health/us-coronavirus-updates-monday/index.html" id="interref0029e">https://www.cnn.com/2020/04/06/health/us-coronavirus-updates-monday/index.html</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0007">
      <label>7</label>
      <mixed-citation publication-type="other" id="sbref0007">A. Maqbool, Coronavirus: why has the virus hit African Americans so hard? (2020 (accessed July 18, 2020)). URL <ext-link ext-link-type="uri" xlink:href="https://www.bbc.com/news/world-us-canada-52245690" id="interref00219">https://www.bbc.com/news/world-us-canada-52245690</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0008">
      <label>8</label>
      <element-citation publication-type="book" id="sbref0008">
        <person-group person-group-type="author">
          <name>
            <surname>Lab</surname>
            <given-names>A.R.</given-names>
          </name>
        </person-group>
        <part-title>APM Research Lab</part-title>
        <year>2020</year>
        <comment>(Accessed November 23, 2020)). URL https://www.apmresearchlab.org/</comment>
      </element-citation>
    </ref>
    <ref id="bib0009">
      <label>9</label>
      <element-citation publication-type="book" id="sbref0009">
        <person-group person-group-type="author">
          <name>
            <surname>Velásquez</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Leahy</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Restrepo</surname>
            <given-names>N.J.</given-names>
          </name>
          <name>
            <surname>Lupu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Sear</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Gabriel</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Jha</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Goldberg</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>N.F.</given-names>
          </name>
        </person-group>
        <part-title>Hate Multiverse Spreads Malicious Covid-19 Content Online Beyond Individual Platform Control</part-title>
        <year>2020</year>
        <comment>arXiv:2004.00673</comment>
      </element-citation>
    </ref>
    <ref id="bib0010">
      <label>10</label>
      <mixed-citation publication-type="other" id="sbref0010">H. Yan, N. Chen, D. Naresh, What's spreading faster than coronavirus in the US? Racist assaults and ignorant attacks against Asians (2020 (accessed July 18, 2020)). URL <ext-link ext-link-type="uri" xlink:href="https://www.cnn.com/2020/02/20/us/coronavirus-racist-attacks-against-asian-americans/index.html" id="interref00229">https://www.cnn.com/2020/02/20/us/coronavirus-racist-attacks-against-asian-americans/index.html</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0011">
      <label>11</label>
      <element-citation publication-type="book" id="sbref0011">
        <person-group person-group-type="author">
          <name>
            <surname>Sayyadiharikandeh</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Varol</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>K.-.C.</given-names>
          </name>
          <name>
            <surname>Flammini</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Menczer</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <part-title>Detection of Novel Social Bots by Ensembles of Specialized Classifiers</part-title>
        <year>2020</year>
        <comment>arXiv:2006.06867</comment>
      </element-citation>
    </ref>
    <ref id="bib0012">
      <label>12</label>
      <mixed-citation publication-type="other" id="sbref0012">IUNetSci, Botometer Python API (2020 (Accessed November 15, 2020)). URL <ext-link ext-link-type="uri" xlink:href="https://github.com/IUNetSci/botometer-python" id="interref0029f">https://github.com/IUNetSci/botometer-python</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0013">
      <label>13</label>
      <element-citation publication-type="journal" id="sbref0013">
        <person-group person-group-type="author">
          <name>
            <surname>Lazer</surname>
            <given-names>D.M.J.</given-names>
          </name>
          <name>
            <surname>Baum</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Benkler</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Berinsky</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Greenhill</surname>
            <given-names>K.M.</given-names>
          </name>
          <name>
            <surname>Menczer</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Metzger</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Nyhan</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Pennycook</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Rothschild</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Schudson</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Sloman</surname>
            <given-names>S.A.</given-names>
          </name>
          <name>
            <surname>Sunstein</surname>
            <given-names>C.R.</given-names>
          </name>
          <name>
            <surname>Thorson</surname>
            <given-names>E.A.</given-names>
          </name>
          <name>
            <surname>Watts</surname>
            <given-names>D.J.</given-names>
          </name>
          <name>
            <surname>Zittrain</surname>
            <given-names>J.L.</given-names>
          </name>
        </person-group>
        <article-title>The science of fake news</article-title>
        <source>Science</source>
        <volume>359</volume>
        <issue>6380</issue>
        <year>2018</year>
        <fpage>1094</fpage>
        <lpage>1096</lpage>
        <pub-id pub-id-type="doi">10.1126/science.aao2998</pub-id>
        <comment>arXiv:https://science.sciencemag.org/content/359/6380/1094.full.pdf,URL https://science.sciencemag.org/content/359/6380/1094</comment>
        <pub-id pub-id-type="pmid">29590025</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0014">
      <label>14</label>
      <element-citation publication-type="journal" id="sbref0014">
        <person-group person-group-type="author">
          <name>
            <surname>Tandoc</surname>
            <given-names>E.C.</given-names>
            <suffix>Jr.</suffix>
          </name>
          <name>
            <surname>Lim</surname>
            <given-names>Z.W.</given-names>
          </name>
          <name>
            <surname>Ling</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Defining “fake news</article-title>
        <source>Digital J.</source>
        <volume>6</volume>
        <issue>2</issue>
        <year>2018</year>
        <fpage>137</fpage>
        <lpage>153</lpage>
        <pub-id pub-id-type="doi">10.1080/21670811.2017.1360143</pub-id>
        <comment>arXiv:https: //doi.org/10.1080/21670811.2017.1360143URL https://doi.org/10.1080/21670811.2017.1360143</comment>
      </element-citation>
    </ref>
    <ref id="bib0015">
      <label>15</label>
      <element-citation publication-type="journal" id="sbref0015">
        <person-group person-group-type="author">
          <name>
            <surname>Gelfert</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Fake news: a definition</article-title>
        <source>Inf. Logic</source>
        <volume>38</volume>
        <issue>1</issue>
        <year>2018</year>
        <fpage>84</fpage>
        <lpage>117</lpage>
        <comment>doi: https://doi.org/10.22329/il.v38i1.5068</comment>
      </element-citation>
    </ref>
    <ref id="bib0016">
      <label>16</label>
      <element-citation publication-type="book" id="sbref0016">
        <person-group person-group-type="author">
          <name>
            <surname>Dalkir</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Katz</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <part-title>Navigating fake news, alternative facts, and misinformation in a post-truth world</part-title>
        <source>Advances in Media, Entertainment, and the Arts</source>
        <year>2020</year>
        <publisher-name>IGI Global</publisher-name>
        <comment>URL https://books.google.com/books?id=RLzTDwAAQBAJ</comment>
      </element-citation>
    </ref>
    <ref id="bib0017">
      <label>17</label>
      <mixed-citation publication-type="other" id="sbref0017">C. Wardle, Fake news. It's complicated. (2017 (accessed July 18, 2020)). URL <ext-link ext-link-type="uri" xlink:href="https://medium.com/1st-draft/fake-news-its-complicated-d0f773766c79" id="interref0029n">https://medium.com/1st-draft/fake-news-its-complicated-d0f773766c79</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0018">
      <label>18</label>
      <element-citation publication-type="journal" id="sbref0018">
        <person-group person-group-type="author">
          <name>
            <surname>Ciampaglia</surname>
            <given-names>G.L.</given-names>
          </name>
        </person-group>
        <article-title>Fighting fake news: a role for computational social science in the fight against digital misinformation</article-title>
        <source>J. Comput. Soc. Sci.</source>
        <year>2018</year>
        <fpage>147</fpage>
        <lpage>153</lpage>
        <pub-id pub-id-type="doi">10.1007/s42001-017-0005-6</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0019">
      <label>19</label>
      <element-citation publication-type="journal" id="sbref0019">
        <person-group person-group-type="author">
          <name>
            <surname>Ciampaglia</surname>
            <given-names>G.L.</given-names>
          </name>
          <name>
            <surname>Mantzarlis</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Maus</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Menczer</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Research challenges of digital misinformation: toward a trustworthy web</article-title>
        <source>AI Mag.</source>
        <volume>39</volume>
        <issue>1</issue>
        <year>2018</year>
        <fpage>65</fpage>
        <lpage>74</lpage>
        <pub-id pub-id-type="doi">10.1609/aimag.v39i1.2783</pub-id>
        <comment>URL https://www.aaai.org/ojs/index.php/aimagazine/article/view/2783</comment>
      </element-citation>
    </ref>
    <ref id="bib0020">
      <label>20</label>
      <element-citation publication-type="journal" id="sbref0020">
        <person-group person-group-type="author">
          <name>
            <surname>Nikolov</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Oliveira</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Flammini</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Menczer</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Measuring online social bubbles</article-title>
        <source>Peer J. Comput. Sci.</source>
        <volume>1</volume>
        <year>2015</year>
        <pub-id pub-id-type="doi">10.7717/peerj-cs.38</pub-id>
        <comment>02</comment>
      </element-citation>
    </ref>
    <ref id="bib0021">
      <label>21</label>
      <element-citation publication-type="book" id="sbref0021">
        <person-group person-group-type="author">
          <name>
            <surname>Cinelli</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Quattrociocchi</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Galeazzi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Valensise</surname>
            <given-names>C.M.</given-names>
          </name>
          <name>
            <surname>Brugnoli</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Schmidt</surname>
            <given-names>A.L.</given-names>
          </name>
          <name>
            <surname>Zola</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Zollo</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Scala</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>The covid-19 Social Media Infodemic</part-title>
        <year>2020</year>
        <comment>arXiv:2003.05004</comment>
      </element-citation>
    </ref>
    <ref id="bib0022">
      <label>22</label>
      <element-citation publication-type="journal" id="sbref0022">
        <person-group person-group-type="author">
          <name>
            <surname>Ferrara</surname>
            <given-names>E.</given-names>
          </name>
        </person-group>
        <article-title>Disinformation and social bot operations in the run up to the 2017 French presidential election</article-title>
        <source>First Monday</source>
        <volume>22</volume>
        <issue>8</issue>
        <year>Jul 2017</year>
        <pub-id pub-id-type="doi">10.5210/fm.v22i8.8005</pub-id>
        <comment>URL http://dx.doi.org/10.5210/fm.v22i8.8005</comment>
      </element-citation>
    </ref>
    <ref id="bib0023">
      <label>23</label>
      <element-citation publication-type="book" id="sbref0023">
        <person-group person-group-type="author">
          <name>
            <surname>Davis</surname>
            <given-names>C.A.</given-names>
          </name>
          <name>
            <surname>Varol</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Ferrara</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Flammini</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Menczer</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <part-title>Botornot: a system to evaluate social bots</part-title>
        <source>Proceedings of the 25th International Conference Companion on World Wide Web, WWW ’16 Companion, International World Wide Web Conferences Steering Committee</source>
        <conf-name>CHE</conf-name>
        <year>2016</year>
        <publisher-name>Republic and Canton of Geneva</publisher-name>
        <fpage>273</fpage>
        <lpage>274</lpage>
        <pub-id pub-id-type="doi">10.1145/2872518.2889302</pub-id>
        <comment>URL https://doi.org/10.1145/2872518.2889302</comment>
      </element-citation>
    </ref>
    <ref id="bib0024">
      <label>24</label>
      <element-citation publication-type="book" id="sbref0024">
        <person-group person-group-type="author">
          <name>
            <surname>Robinson</surname>
            <given-names>J.P.</given-names>
          </name>
        </person-group>
        <part-title>How Americans Use Time: A Social-Psychological Analysis of Everyday Behavior</part-title>
        <year>1977</year>
        <publisher-name>Praeger scientific</publisher-name>
        <publisher-loc>Praeger</publisher-loc>
        <comment>URL https://books.google.com/books?id=UL3ZAAAAMAAJ</comment>
      </element-citation>
    </ref>
    <ref id="bib0025">
      <label>25</label>
      <element-citation publication-type="book" id="sbref0025">
        <person-group person-group-type="author">
          <name>
            <surname>Kulshrestha</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zafar</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Noboa</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Gummadi</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Ghosh</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <part-title>Characterizing Information Diets of Social Media Users</part-title>
        <year>2015</year>
        <comment>URL</comment>
        <ext-link ext-link-type="uri" xlink:href="https://www.aaai.org/ocs/index.php/ICWSM/ICWSM15/paper/view/10595/10505" id="interref0004">https://www.aaai.org/ocs/index.php/ICWSM/ICWSM15/paper/view/10595/10505</ext-link>
      </element-citation>
    </ref>
    <ref id="bib0026">
      <label>26</label>
      <element-citation publication-type="journal" id="sbref0026">
        <person-group person-group-type="author">
          <name>
            <surname>Ohme</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>When digital natives enter the electorate: political social media use among first-time voters and its effects on campaign participation</article-title>
        <source>J. Inf. Technol. Polit.</source>
        <volume>16</volume>
        <issue>2</issue>
        <year>2019</year>
        <fpage>119</fpage>
        <lpage>136</lpage>
        <pub-id pub-id-type="doi">10.1080/19331681.2019.1613279</pub-id>
        <comment>arXiv: https://doi.org/10.1080/19331681.2019.1613279URL https://doi.org/10.1080/19331681.2019.1613279</comment>
      </element-citation>
    </ref>
    <ref id="bib0027">
      <label>27</label>
      <mixed-citation publication-type="other" id="sbref0027">DeepAI, Machine Learning (n.d. (Accessed 7/18/2020)). URL <ext-link ext-link-type="uri" xlink:href="https://deepai.org/machinelearning-glossary-and-terms/machinelearning" id="interref0005">https://deepai.org/machinelearning-glossary-and-terms/machinelearning</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0028">
      <label>28</label>
      <element-citation publication-type="book" id="sbref0028">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Guan</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Bhat</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Hsu</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>Fake news detection via nlp is vulnerable to adversarial attacks</part-title>
        <source>Proceedings of the 11th International Conference on Agents and Artificial Intelligence</source>
        <year>2019</year>
        <pub-id pub-id-type="doi">10.5220/0007566307940800</pub-id>
        <comment>URL http://dx.doi.org/10.5220/0007566307940800</comment>
      </element-citation>
    </ref>
    <ref id="bib0029">
      <label>29</label>
      <mixed-citation publication-type="other" id="sbref0029">Fake News Challenge, Frequently asked questions (2017 (Accessed July 18, 2020)). URL <ext-link ext-link-type="uri" xlink:href="http://www.fakenewschallenge.org/" id="interref0006">http://www.fakenewschallenge.org/</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0030">
      <label>30</label>
      <mixed-citation publication-type="other" id="sbref0030">I. Turc, M.-.W. Chang, K. Lee, K. Toutanova, Well-read students learn better: on the importance of pre-training compact models (2019). arXiv:1908.08962.</mixed-citation>
    </ref>
    <ref id="bib0031">
      <label>31</label>
      <mixed-citation publication-type="other" id="sbref0031">R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner, Y. Choi, Defending against neural fake news (2019). arXiv: 1905.12616.</mixed-citation>
    </ref>
    <ref id="bib0032">
      <label>32</label>
      <mixed-citation publication-type="other" id="sbref0032">A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Language models are unsupervised multitask learners, 2019.</mixed-citation>
    </ref>
    <ref id="bib0033">
      <label>33</label>
      <mixed-citation publication-type="other" id="sbref0033">OpenAI, GPT-2 output detector (2019 (accessed July 18, 2020)). URL <ext-link ext-link-type="uri" xlink:href="https://github.com/openai/gpt-2-output-dataset/tree/master/detector" id="interref0007">https://github.com/openai/gpt-2-output-dataset/tree/master/detector</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0034">
      <label>34</label>
      <element-citation publication-type="book" id="sbref0034">
        <person-group person-group-type="author">
          <name>
            <surname>Thorne</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Vlachos</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Cocarascu</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Christodoulopoulos</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Mittal</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>The fact extraction and VERification (FEVER) shared task</part-title>
        <source>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</source>
        <year>2018</year>
        <publisher-name>Association for Computational Linguistics</publisher-name>
        <publisher-loc>Brussels, Belgium</publisher-loc>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.18653/v1/W18-5501</pub-id>
        <comment>URL</comment>
        <ext-link ext-link-type="uri" xlink:href="https://www.aclweb.org/anthology/W18-5501" id="interref0008">https://www.aclweb.org/anthology/W18-5501</ext-link>
      </element-citation>
    </ref>
    <ref id="bib0035">
      <label>35</label>
      <mixed-citation publication-type="other" id="sbref0035">ClaimBuster, Fact checker (n.d. (accessed July 18, 2020)). URL <ext-link ext-link-type="uri" xlink:href="https://idir.uta.edu/claimbuster/factchecker/" id="interref0009">https://idir.uta.edu/claimbuster/factchecker/</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0036">
      <label>36</label>
      <element-citation publication-type="journal" id="sbref0036">
        <person-group person-group-type="author">
          <name>
            <surname>Hassan</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Arslan</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Caraballo</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Jimenez</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Gawsane</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Hasan</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Joseph</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kulkarni</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Nayak</surname>
            <given-names>A.K.</given-names>
          </name>
          <name>
            <surname>Sable</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Tremayne</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Claimbuster: the first-ever end-to-end fact-checking system</article-title>
        <source>Proc. VLDB Endow</source>
        <volume>10</volume>
        <issue>12</issue>
        <year>2017</year>
        <fpage>1945</fpage>
        <lpage>1948</lpage>
        <pub-id pub-id-type="doi">10.14778/3137765.3137815</pub-id>
        <comment>URL https://doi.org/10.14778/3137765.3137815</comment>
      </element-citation>
    </ref>
    <ref id="bib0037">
      <label>37</label>
      <element-citation publication-type="book" id="sbref0037">
        <person-group person-group-type="author">
          <collab>Google</collab>
        </person-group>
        <part-title>Fact Check Explorer</part-title>
        <year>2020</year>
        <comment>(n.d. (accessed July 18URL</comment>
        <ext-link ext-link-type="uri" xlink:href="https://toolbox.google.com/factcheck/explorer" id="interref0010">https://toolbox.google.com/factcheck/explorer</ext-link>
      </element-citation>
    </ref>
    <ref id="bib0038">
      <label>38</label>
      <element-citation publication-type="book" id="sbref0038">
        <person-group person-group-type="author">
          <name>
            <surname>Gehrmann</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Strobelt</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Rush</surname>
            <given-names>A.M.</given-names>
          </name>
        </person-group>
        <part-title>Gltr: Statistical Detection and Visualization of Generated Text</part-title>
        <year>2019</year>
        <comment>arXiv:1906.04043</comment>
      </element-citation>
    </ref>
    <ref id="bib0039">
      <label>39</label>
      <element-citation publication-type="book" id="sbref0039">
        <person-group person-group-type="author">
          <name>
            <surname>Strobelt</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Gehrmann</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <part-title>Catching a Unicorn with GLTR: A tool to Detect Automatically Generated Text</part-title>
        <year>2019</year>
        <comment>(Accessed July 18, 2020)). URL</comment>
        <ext-link ext-link-type="uri" xlink:href="http://gltr.io/" id="interref0011">http://gltr.io/</ext-link>
      </element-citation>
    </ref>
    <ref id="bib0040">
      <label>40</label>
      <mixed-citation publication-type="other" id="sbref0040">S.O. for Disinformation, S.M. Analysis, Homepage, SOMA Disinfobservatory (2020 (Accessed November 15, 2020)). URL <ext-link ext-link-type="uri" xlink:href="https://www.disinfobservatory.org/" id="interref0012">https://www.disinfobservatory.org/</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0041">
      <label>41</label>
      <element-citation publication-type="journal" id="sbref0041">
        <person-group person-group-type="author">
          <name>
            <surname>Guarino</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Trino</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Celestini</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Chessa</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Riotta</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Characterizing networks of propaganda on twitter: a case study</article-title>
        <source>Appl. Netw. Sci.</source>
        <volume>5</volume>
        <issue>1</issue>
        <year>Sep 2020</year>
        <pub-id pub-id-type="doi">10.1007/s41109-020-00286-y</pub-id>
        <comment>URL http://dx.doi.org/10.1007/s41109-020-00286-y</comment>
      </element-citation>
    </ref>
    <ref id="bib0042">
      <label>42</label>
      <mixed-citation publication-type="other" id="sbref0042">S. Guarino, N. Trino, A. Chessa, G. Riotta, Beyond fact-checking: network analysis tools for monitoring disinformation in social media, 2019, pp. 436–447. doi:10.1007/978-3-030- 36687-2_36.</mixed-citation>
    </ref>
    <ref id="bib0043">
      <label>43</label>
      <mixed-citation publication-type="other" id="sbref0043">L. Toumanidis, R. Heartfield, P. Kasnesis, G. Loukas, C. Patrikakis, A prototype framework for assessing information provenance in decentralised social media: the EUNOMIA concept, 2020, pp. 196–208. doi:10.1007/978-3-030-37545-4_13.</mixed-citation>
    </ref>
    <ref id="bib0044">
      <label>44</label>
      <mixed-citation publication-type="other" id="sbref0044">EUNOMIA, The project, Eunomia (2020 (Accessed November 15, 2020)). URL <ext-link ext-link-type="uri" xlink:href="https://www.eunomia.social/project" id="interref0013">https://www.eunomia.social/project</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0045">
      <label>45</label>
      <mixed-citation publication-type="other" id="sbref0045">SocialTruth, The social truth project (2020 (Accessed November 15, 2020)). URL <ext-link ext-link-type="uri" xlink:href="http://www.socialtruth.eu/" id="interref0014">http://www.socialtruth.eu/</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0046">
      <label>46</label>
      <element-citation publication-type="book" id="sbref0046">
        <person-group person-group-type="author">
          <name>
            <surname>Choras</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Pawlicki</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kozik</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Demestichas</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Kosmides</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Gupta</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <part-title>Socialtruth project approach to online disinformation (fake news) detection and mitigation</part-title>
        <source>Proceedings of the 14th International Conference on Availability, Reliability and Security, ARES ’19</source>
        <year>2019</year>
        <publisher-name>Association for Computing Machinery</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <pub-id pub-id-type="doi">10.1145/3339252.3341497</pub-id>
        <comment>URL https://doi.org/10.1145/3339252.3341497</comment>
      </element-citation>
    </ref>
    <ref id="bib0047">
      <label>47</label>
      <mixed-citation publication-type="other" id="sbref0047">WeVerify, About us - WeVerify (2020 (Accessed November 15, 2020)). URL <ext-link ext-link-type="uri" xlink:href="https://weverify.eu/about/" id="interref0015">https://weverify.eu/about/</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0048">
      <label>48</label>
      <element-citation publication-type="book" id="sbref0048">
        <person-group person-group-type="author">
          <name>
            <surname>Marinova</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Spangenberg</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Teyssou</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Papadopoulos</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sarris</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Alaphilippe</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bontcheva</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <part-title>Weverify: wider and enhanced verification for you project overview and tools</part-title>
        <source>2020 IEEE International Conference on Multimedia Expo Workshops (ICMEW)</source>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>4</lpage>
        <pub-id pub-id-type="doi">10.1109/ICMEW46912.2020.9106056</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0049">
      <label>49</label>
      <mixed-citation publication-type="other" id="sbref0049">WeVerify, Verification plugin - WeVerify (2020 (Accessed November 15, 2020)). URL <ext-link ext-link-type="uri" xlink:href="https://weverify.eu/verificationplugin/" id="interref0016">https://weverify.eu/verificationplugin/</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0050">
      <label>50</label>
      <mixed-citation publication-type="other" id="sbref0050">Provenance, About provenance (2020 (Accessed November 15, 2020)). URL <ext-link ext-link-type="uri" xlink:href="https://www.provenanceh2020.eu/about/about-provenance" id="interref00259">https://www.provenanceh2020.eu/about/about-provenance</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0051">
      <label>51</label>
      <element-citation publication-type="book" id="sbref0051">
        <person-group person-group-type="author">
          <name>
            <surname>Culloty</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Suiter</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>Beyond fact-checking: countering the spread of political disinformation</part-title>
        <source>The European Consortium for Political Research</source>
        <year>2019</year>
        <publisher-name>Wroc law</publisher-name>
        <publisher-loc>Poland</publisher-loc>
        <comment>URL</comment>
        <ext-link ext-link-type="uri" xlink:href="https://ecpr.eu/Events/Event/PaperDetails/46889" id="interref0017">https://ecpr.eu/Events/Event/PaperDetails/46889</ext-link>
      </element-citation>
    </ref>
    <ref id="bib0052">
      <label>52</label>
      <element-citation publication-type="book" id="sbref0052">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>W.Y.</given-names>
          </name>
        </person-group>
        <part-title>liar, liar pants on fire”: a new benchmark dataset for fake news detection</part-title>
        <source>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</source>
        <year>2017</year>
        <publisher-name>Association for Computational Linguistics</publisher-name>
        <publisher-loc>Vancouver, Canada</publisher-loc>
        <fpage>422</fpage>
        <lpage>426</lpage>
        <pub-id pub-id-type="doi">10.18653/v1/P17-2067</pub-id>
        <comment>URL</comment>
        <ext-link ext-link-type="uri" xlink:href="https://www.aclweb.org/anthology/P17-2067" id="interref0018">https://www.aclweb.org/anthology/P17-2067</ext-link>
      </element-citation>
    </ref>
    <ref id="bib0053">
      <label>53</label>
      <element-citation publication-type="journal" id="sbref0053">
        <person-group person-group-type="author">
          <name>
            <surname>Ahmed</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Traore</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Saad</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Detecting opinion spams and fake news using text classification</article-title>
        <source>Security and Privacy</source>
        <volume>1</volume>
        <issue>1</issue>
        <year>2018</year>
        <fpage>e9</fpage>
        <pub-id pub-id-type="doi">10.1002/spy2.9</pub-id>
        <comment>arXiv: https://onlinelibrary.wiley.com/doi/pdf/10.1002/spy2.9URL https://onlinelibrary.wiley.com/doi/abs/10.1002/spy2.9</comment>
      </element-citation>
    </ref>
    <ref id="bib0054">
      <label>54</label>
      <element-citation publication-type="book" id="sbref0054">
        <person-group person-group-type="author">
          <name>
            <surname>Traore</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Saad</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <part-title>Detection of online fake news using n-gram analysis and machine learning techniques</part-title>
        <source>Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments</source>
        <year>2017</year>
        <fpage>127</fpage>
        <lpage>138</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-319-69155-8_9</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0055">
      <label>55</label>
      <element-citation publication-type="book" id="sbref0055">
        <person-group person-group-type="author">
          <name>
            <surname>Pérez-Rosas</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Kleinberg</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Lefevre</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mihalcea</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <part-title>Automatic detection of fake news</part-title>
        <source>Proceedings of the 27th International Conference on Computational Linguistics</source>
        <conf-name>Santa Fe, New Mexico, USA</conf-name>
        <year>2018</year>
        <publisher-name>Association for Computational Linguistics</publisher-name>
        <fpage>3391</fpage>
        <lpage>3401</lpage>
        <comment>URL</comment>
        <ext-link ext-link-type="uri" xlink:href="https://www.aclweb.org/anthology/C18-1287" id="interref0019">https://www.aclweb.org/anthology/C18-1287</ext-link>
      </element-citation>
    </ref>
    <ref id="bib0056">
      <label>56</label>
      <element-citation publication-type="book" id="sbref0056">
        <person-group person-group-type="author">
          <name>
            <surname>Buntain</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Golbeck</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>Automatically identifying fake news in popular twitter threads</part-title>
        <source>2017 IEEE International Conference on Smart Cloud (SmartCloud)</source>
        <year>2017</year>
        <fpage>208</fpage>
        <lpage>215</lpage>
      </element-citation>
    </ref>
    <ref id="bib0057">
      <label>57</label>
      <element-citation publication-type="journal" id="sbref0057">
        <person-group person-group-type="author">
          <name>
            <surname>Asr</surname>
            <given-names>F.T.</given-names>
          </name>
          <name>
            <surname>Taboada</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Big data and quality data for fake news and misinformation detection</article-title>
        <source>Big Data Soc.</source>
        <volume>6</volume>
        <issue>1</issue>
        <year>2019</year>
        <object-id pub-id-type="publisher-id">2053951719843310</object-id>
        <pub-id pub-id-type="doi">10.1177/2053951719843310</pub-id>
        <comment>arXiv: https://doi.org/10.1177/2053951719843310URL https://doi.org/10.1177/2053951719843310</comment>
      </element-citation>
    </ref>
    <ref id="bib0058">
      <label>58</label>
      <element-citation publication-type="book" id="sbref0058">
        <person-group person-group-type="author">
          <name>
            <surname>Bullock</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Luccioni</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Pham</surname>
            <given-names>K.H.</given-names>
          </name>
          <name>
            <surname>Lam</surname>
            <given-names>C.S.N.</given-names>
          </name>
          <name>
            <surname>Luengo-Oroz</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <part-title>Mapping the Landscape of Artificial Intelligence Applications Against Covid-19</part-title>
        <year>2020</year>
        <comment>arXiv:2003.11336</comment>
      </element-citation>
    </ref>
    <ref id="bib0059">
      <label>59</label>
      <element-citation publication-type="journal" id="sbref0059">
        <person-group person-group-type="author">
          <name>
            <surname>Kouzy</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Jaoude</surname>
            <given-names>J.A.</given-names>
          </name>
          <name>
            <surname>Kraitem</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Alam</surname>
            <given-names>M.B.E.</given-names>
          </name>
          <name>
            <surname>Karam</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Adib</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Zarka</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Traboulsi</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Akl</surname>
            <given-names>E.W.</given-names>
          </name>
          <name>
            <surname>Baddour</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Coronavirus goes viral: quantifying the covid-19 misinformation epidemic on twitter</article-title>
        <source>Cureus</source>
        <volume>12</volume>
        <issue>03</issue>
        <year>2020</year>
        <pub-id pub-id-type="doi">10.7759/cureus.7255</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0060">
      <label>60</label>
      <mixed-citation publication-type="other" id="sbref0060">J. Xue, J. Chen, R. Hu, C. Chen, C. Zheng, X. Liu, T. Zhu, Twitter discussions and emotions about Covid-19 pandemic: a machine learning approach (2020). arXiv:2005.12830.</mixed-citation>
    </ref>
    <ref id="bib0061">
      <label>61</label>
      <element-citation publication-type="journal" id="sbref0061">
        <person-group person-group-type="author">
          <name>
            <surname>Pennycook</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>McPhetres</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>J.G.</given-names>
          </name>
          <name>
            <surname>Rand</surname>
            <given-names>D.G.</given-names>
          </name>
        </person-group>
        <article-title>Fighting covid-19 misinformation on social media: experimental evidence for a scalable accuracy-nudge intervention</article-title>
        <source>Psychol. Sci.</source>
        <volume>31</volume>
        <issue>7</issue>
        <year>2020</year>
        <fpage>770</fpage>
        <lpage>780</lpage>
        <pub-id pub-id-type="doi">10.1177/0956797620939054</pub-id>
        <comment>pMID: 32603243. arXiv: https://doi.org/10.1177/0956797620939054URL https://doi.org/10.1177/0956797620939054</comment>
        <pub-id pub-id-type="pmid">32603243</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0062">
      <label>62</label>
      <mixed-citation publication-type="other" id="sbref0062">R. Pandey, V. Gautam, C. Jain, P. Syal, H. Sharma, K. Bhagat, R. Pal, L.S. Dhingra, Arushi, L.Patel, M. Agarwal, S. Agrawal, M. Arora, B. Rana, P. Kumaraguru, T. Sethi, A machine learning application for raising wash awareness in the times of Covid-19 pandemic (2020). arXiv:2003.07074.</mixed-citation>
    </ref>
    <ref id="bib0063">
      <label>63</label>
      <element-citation publication-type="journal" id="sbref0063">
        <person-group person-group-type="author">
          <name>
            <surname>Bruns</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Harrington</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Hurcombe</surname>
            <given-names>E.</given-names>
          </name>
        </person-group>
        <article-title>corona? 5 g? or both?’: the dynamics of covid-19/5g conspiracy theories on facebook</article-title>
        <source>Media Int. Aust.</source>
        <volume>177</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>12</fpage>
        <lpage>29</lpage>
        <pub-id pub-id-type="doi">10.1177/1329878X20946113</pub-id>
        <comment>arXiv: https://doi.org/10.1177/1329878X20946113URL https://doi.org/10.1177/1329878X20946113</comment>
      </element-citation>
    </ref>
    <ref id="bib0064">
      <label>64</label>
      <element-citation publication-type="book" id="sbref0064">
        <person-group person-group-type="author">
          <name>
            <surname>Mejova</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Kalimeri</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <part-title>Covid-19 on facebook ads: competing agendas around a public health crisis</part-title>
        <source>Proceedings of the 3rd ACM SIGCAS Conference on Computing and Sustainable Societies, COMPASS ’20</source>
        <conf-name>New York, NY, USA</conf-name>
        <year>2020</year>
        <publisher-name>Association for Computing Machinery</publisher-name>
        <fpage>22</fpage>
        <lpage>31</lpage>
        <pub-id pub-id-type="doi">10.1145/3378393.3402241</pub-id>
        <comment>URL https://doi.org/10.1145/3378393.3402241</comment>
      </element-citation>
    </ref>
    <ref id="bib0065">
      <label>65</label>
      <mixed-citation publication-type="other" id="sbref0065">A. Celestini, M.D. Giovanni, S. Guarino, F. Pierri, Information disorders on Italian Facebook during Covid-19 infodemic (2020). arXiv: 2007.11302.</mixed-citation>
    </ref>
    <ref id="bib0066">
      <label>66</label>
      <mixed-citation publication-type="other" id="sbref0066">S. Boberg, T. Quandt, T. Schatto-Eckrodt, L. Frischlich, Pandemic populism: Facebook pages of alternative news media and the corona crisis – a computational content analysis (2020). arXiv:2004.02566.</mixed-citation>
    </ref>
    <ref id="bib0067">
      <label>67</label>
      <mixed-citation publication-type="other" id="sbref0067">S.A. Memon, K.M. Carley, Characterizing Covid-19 misinformation communities using a novel Twitter dataset (2020). arXiv:2008.00791.</mixed-citation>
    </ref>
    <ref id="bib0068">
      <label>68</label>
      <element-citation publication-type="book" id="sbref0068">
        <person-group person-group-type="author">
          <name>
            <surname>Ferrara</surname>
            <given-names>E.</given-names>
          </name>
        </person-group>
        <part-title>What Types of Covid-19 Conspiracies are Populated by Twitter Bots?</part-title>
        <year>May 2020</year>
        <publisher-name>First Monday</publisher-name>
        <pub-id pub-id-type="doi">10.5210/fm.v25i6.10633</pub-id>
        <comment>URL http://dx.doi.org/10.5210/fm.v25i6.10633</comment>
      </element-citation>
    </ref>
    <ref id="bib0069">
      <label>69</label>
      <element-citation publication-type="journal" id="sbref0069">
        <person-group person-group-type="author">
          <name>
            <surname>Singh</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Bode</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Budak</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Kawintiranon</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Padden</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Vraga</surname>
            <given-names>E.</given-names>
          </name>
        </person-group>
        <article-title>Understanding high-and low-quality url sharing on covid-19 twitter streams</article-title>
        <source>J. Comput. Soc. Sci.</source>
        <volume>3</volume>
        <year>2020</year>
        <fpage>343</fpage>
        <lpage>366</lpage>
        <pub-id pub-id-type="doi">10.1007/s42001-020-00093-6</pub-id>
        <pub-id pub-id-type="pmid">33263092</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0070">
      <label>70</label>
      <mixed-citation publication-type="other" id="sbref0070">B. Huang, K.M. Carley, Disinformation and misinformation on Twitter during the novel coronavirus outbreak (2020). arXiv:2006.04278.</mixed-citation>
    </ref>
    <ref id="bib0071">
      <label>71</label>
      <mixed-citation publication-type="other" id="sbref0071">NewsAPI, Pricing (n.d. (accessed July 18, 2020)). URL <ext-link ext-link-type="uri" xlink:href="https://newsapi.org/pricing" id="interref0020">https://newsapi.org/pricing</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0072">
      <label>72</label>
      <element-citation publication-type="book" id="sbref0072">
        <person-group person-group-type="author">
          <collab>Microsoft</collab>
        </person-group>
        <part-title>Cognitive Services Pricing – Bing Search API</part-title>
        <year>2020</year>
        <comment>(n.d. (Accessed July 18URL https://azure.microsoft.com/en-us/pricing/details/cognitive-services/search-api/</comment>
      </element-citation>
    </ref>
    <ref id="bib0073">
      <label>73</label>
      <mixed-citation publication-type="other" id="sbref0073">CurrentsAPI, Pricing overview (n.d. (accessed July 18, 2020)). URL <ext-link ext-link-type="uri" xlink:href="https://currentsapi.services/en/product/price" id="interref0021">https://currentsapi.services/en/product/price</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0074">
      <label>74</label>
      <element-citation publication-type="book" id="sbref0074">
        <person-group person-group-type="author">
          <collab>contextualwebsearch</collab>
        </person-group>
        <part-title>Contextual Web Search Pricing”)</part-title>
        <year>2020</year>
        <comment>(accessed June 28, 2020)) URL</comment>
        <ext-link ext-link-type="uri" xlink:href="https://rapidapi.com/contextualwebsearch/api/web-search/pricing" id="interref0022">https://rapidapi.com/contextualwebsearch/api/web-search/pricing</ext-link>
      </element-citation>
    </ref>
    <ref id="bib0075">
      <label>75</label>
      <mixed-citation publication-type="other" id="sbref0075">Kakaly, twitter-feed (2019 (accessed July 18, 2020)). URL <ext-link ext-link-type="uri" xlink:href="https://github.com/kakaly/twitterfeed" id="interref0023">https://github.com/kakaly/twitterfeed</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0076">
      <label>76</label>
      <mixed-citation publication-type="other" id="sbref0076">Heroku, Heroku pricing (n.d. (Accessed July 18, 2020)). URL <ext-link ext-link-type="uri" xlink:href="https://www.heroku.com/pricing" id="interref0024">https://www.heroku.com/pricing</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0077">
      <label>77</label>
      <mixed-citation publication-type="other" id="sbref0077">HuggingFace, roberta-base-openai-detector (2020 (Accessed December 28, 2020)). URL <ext-link ext-link-type="uri" xlink:href="https://huggingface.co/robertabase-openai-detector" id="interref0025">https://huggingface.co/robertabase-openai-detector</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0078">
      <label>78</label>
      <element-citation publication-type="book" id="sbref0078">
        <person-group person-group-type="author">
          <name>
            <surname>Cui</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <part-title>Coaid: Covid-19 Healthcare Misinformation Dataset</part-title>
        <year>2020</year>
        <comment>arXiv:2006.00885</comment>
      </element-citation>
    </ref>
    <ref id="bib0079">
      <label>79</label>
      <element-citation publication-type="book" id="sbref0079">
        <person-group person-group-type="author">
          <name>
            <surname>Hamborg</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Meuschke</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Breitinger</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Gipp</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <part-title>news-please: a generic news crawler and extractor</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Gaede</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Trkulja</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Petra</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <source>Proceedings of the 15th International Symposium of Information Science</source>
        <year>2017</year>
        <fpage>218</fpage>
        <lpage>223</lpage>
      </element-citation>
    </ref>
    <ref id="bib0080">
      <label>80</label>
      <element-citation publication-type="book" id="sbref0080">
        <person-group person-group-type="author">
          <name>
            <surname>Tweepy</surname>
          </name>
        </person-group>
        <part-title>Tweepy: Twitter for Python!</part-title>
        <year>2020</year>
        <comment>(Accessed July 18, 2020)). URL</comment>
        <ext-link ext-link-type="uri" xlink:href="https://github.com/tweepy/tweepy" id="interref0026">https://github.com/tweepy/tweepy</ext-link>
      </element-citation>
    </ref>
    <ref id="bib0081">
      <label>81</label>
      <mixed-citation publication-type="other" id="sbref0081">ipapi, ipapi - IP address lookup and geolocation API (2020 (Accessed November 16, 2020)). URL <ext-link ext-link-type="uri" xlink:href="https://ipapi.co/" id="interref0027">https://ipapi.co/</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0082">
      <label>82</label>
      <element-citation publication-type="journal" id="sbref0082">
        <person-group person-group-type="author">
          <name>
            <surname>Tandoc</surname>
            <given-names>E.C.</given-names>
            <suffix>Jr.</suffix>
          </name>
          <name>
            <surname>Lim</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Ling</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Diffusion of disinformation: how social media users respond to fake news and why</article-title>
        <source>Journalism</source>
        <volume>21</volume>
        <issue>3</issue>
        <year>2020</year>
        <fpage>381</fpage>
        <lpage>398</lpage>
        <pub-id pub-id-type="doi">10.1177/1464884919868325</pub-id>
        <comment>arXiv: https://doi.org/10.1177/1464884919868325URL https://doi.org/10.1177/1464884919868325</comment>
      </element-citation>
    </ref>
    <ref id="bib0083">
      <label>83</label>
      <element-citation publication-type="book" id="sbref0083">
        <person-group person-group-type="author">
          <name>
            <surname>Rasool</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Butt</surname>
            <given-names>W.H.</given-names>
          </name>
          <name>
            <surname>Shaukat</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Akram</surname>
            <given-names>M.U.</given-names>
          </name>
        </person-group>
        <part-title>Multi-label fake news detection using multi-layered supervised learning</part-title>
        <source>Proceedings of the 2019 11th International Conference on Computer and Automation Engineering, ICCAE 2019</source>
        <conf-name>New York, NY, USA</conf-name>
        <year>2019</year>
        <publisher-name>Association for Computing Machinery</publisher-name>
        <fpage>73</fpage>
        <lpage>77</lpage>
        <pub-id pub-id-type="doi">10.1145/3313991.3314008</pub-id>
        <comment>URL https://doi.org/10.1145/3313991.3314008</comment>
      </element-citation>
    </ref>
    <ref id="bib0084">
      <label>84</label>
      <mixed-citation publication-type="other" id="sbref0084">J. Ratkiewicz, M. Conover, M. Meiss, B. Goncalves, A. Flammini, F. Menczer, Detecting and tracking political abuse in social media (2011). URL <ext-link ext-link-type="uri" xlink:href="https://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/view/2850/3274" id="interref0028">https://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/view/2850/3274</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0085">
      <label>85</label>
      <element-citation publication-type="book" id="sbref0085">
        <person-group person-group-type="author">
          <name>
            <surname>Giotta</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <part-title>Ways of seeing. What you want: flexible visuality and image politics in the post-truth era</part-title>
        <source>Fake News: Understanding Media and Misinformation in the Digital Age</source>
        <year>2020</year>
        <publisher-name>The MIT Press</publisher-name>
        <fpage>29</fpage>
        <lpage>44</lpage>
        <pub-id pub-id-type="doi">10.7551/mitpress/11807.003.0005</pub-id>
        <comment>arXiv: https://direct.mit.edu/book/chapter-pdf/271929/9780262357388_cak.pdfURL https://doi.org/10.7551/mitpress/11807.003.0005</comment>
      </element-citation>
    </ref>
    <ref id="bib0086">
      <label>86</label>
      <element-citation publication-type="book" id="sbref0086">
        <person-group person-group-type="author">
          <name>
            <surname>Gao</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Karahalios</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>W.-.T.</given-names>
          </name>
        </person-group>
        <part-title>To label or not to label: the effect of stance and credibility labels on readers’ selection and perception of news articles</part-title>
        <source>Proceeding of the ACM on Human - Computer Interaction 2 (CSCW)</source>
        <year>Nov. 2018</year>
        <pub-id pub-id-type="doi">10.1145/3274324</pub-id>
        <comment>URL https://doi.org/10.1145/3274324</comment>
      </element-citation>
    </ref>
    <ref id="bib0087">
      <label>87</label>
      <element-citation publication-type="journal" id="sbref0087">
        <person-group person-group-type="author">
          <name>
            <surname>Skurnik</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Yoon</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>D.C.</given-names>
          </name>
          <name>
            <surname>Schwarz</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <article-title>How warnings about false claims become recommendations</article-title>
        <source>J. Consum. Res.</source>
        <volume>31</volume>
        <issue>4</issue>
        <year>2005</year>
        <fpage>713</fpage>
        <lpage>724</lpage>
        <pub-id pub-id-type="doi">10.1086/426605</pub-id>
        <comment>URL https://doi.org/10.1086/426605</comment>
      </element-citation>
    </ref>
    <ref id="bib0088">
      <label>88</label>
      <element-citation publication-type="journal" id="sbref0088">
        <person-group person-group-type="author">
          <name>
            <surname>Hasher</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Goldstein</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Toppino</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Frequency and the conference of referential validity</article-title>
        <source>J. Verbal Learn. Verbal Behav.</source>
        <volume>16</volume>
        <issue>1</issue>
        <year>1977</year>
        <fpage>107</fpage>
        <lpage>112</lpage>
        <comment>https://doi.org/10.1016/S0022-5371(77)80012-1. URL</comment>
        <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S0022537177800121" id="interref0029">http://www.sciencedirect.com/science/article/pii/S0022537177800121</ext-link>
      </element-citation>
    </ref>
    <ref id="bib0089">
      <label>89</label>
      <element-citation publication-type="journal" id="sbref0089">
        <person-group person-group-type="author">
          <name>
            <surname>Hawkins</surname>
            <given-names>S.A.</given-names>
          </name>
          <name>
            <surname>Hoch</surname>
            <given-names>S.J.</given-names>
          </name>
        </person-group>
        <article-title>Low-involvement learning: memory without evaluation</article-title>
        <source>J. Consum. Res.</source>
        <volume>19</volume>
        <issue>2</issue>
        <year>1992</year>
        <fpage>212</fpage>
        <lpage>225</lpage>
        <pub-id pub-id-type="doi">10.1086/209297</pub-id>
        <comment>arXiv: https://academic.oup.com/jcr/article-pdf/19/2/212/5438798/19-2-212.pdfURL https://doi.org/10.1086/209297</comment>
      </element-citation>
    </ref>
    <ref id="bib0090">
      <label>90</label>
      <element-citation publication-type="journal" id="sbref0090">
        <person-group person-group-type="author">
          <name>
            <surname>Begg</surname>
            <given-names>I.M.</given-names>
          </name>
          <name>
            <surname>Anas</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Farinacci</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Dissociation of processes in belief: source recollection, statement familiarity, and the illusion of truth</article-title>
        <source>J. Experiment. Psychol.: Gen.</source>
        <volume>121</volume>
        <issue>4</issue>
        <year>1992</year>
        <fpage>446</fpage>
        <lpage>458</lpage>
        <pub-id pub-id-type="doi">10.1037/0096-3445.121.4.446</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0091">
      <label>91</label>
      <element-citation publication-type="journal" id="sbref0091">
        <person-group person-group-type="author">
          <name>
            <surname>Gilbert</surname>
            <given-names>D.T.</given-names>
          </name>
          <name>
            <surname>Krull</surname>
            <given-names>D.S.</given-names>
          </name>
          <name>
            <surname>Malone</surname>
            <given-names>P.S.</given-names>
          </name>
        </person-group>
        <article-title>Unbelieving the unbelievable: some problems in the rejection of false information</article-title>
        <source>J. Pers. Soc. Psychol.</source>
        <volume>59</volume>
        <issue>4</issue>
        <year>1990</year>
        <fpage>601</fpage>
        <lpage>613</lpage>
      </element-citation>
    </ref>
    <ref id="bib0092">
      <label>92</label>
      <element-citation publication-type="book" id="sbref0092">
        <person-group person-group-type="author">
          <name>
            <surname>Polage</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <part-title>Source credibility and belief in fake news: i'll believe you if you agree with me</part-title>
        <source>Fake News: Understanding Media and Misinformation in the Digital Age</source>
        <year>2020</year>
        <publisher-name>The MIT Press</publisher-name>
        <fpage>235</fpage>
        <lpage>244</lpage>
        <pub-id pub-id-type="doi">10.7551/mitpress/11807.003.0025</pub-id>
        <comment>arXiv: https://direct.mit.edu/book/chapter-pdf/271956/9780262357388_ceb.pdfURL https://doi.org/10.7551/mitpress/11807.003.0025</comment>
      </element-citation>
    </ref>
  </ref-list>
  <ack id="ack0001">
    <title>Acknowledgments</title>
    <p id="para0074">The work presented in this paper was partially funded by the Moody College of Communication, <funding-source id="gs0001"><institution-wrap><institution-id institution-id-type="doi">10.13039/100005688</institution-id><institution>University of Texas at Austin</institution></institution-wrap></funding-source>.</p>
  </ack>
  <fn-group>
    <fn id="txtfn2">
      <label>1</label>
      <p id="ntparahnkJZIP68Sa">The ISOT dataset is available at: <ext-link ext-link-type="uri" xlink:href="http://web.eecs.umich.edu/~mihalcea/downloads/fakeNewsDatasets.zip" id="interref0029s">http://web.eecs.umich.edu/~mihalcea/downloads/fakeNewsDatasets.zip</ext-link></p>
    </fn>
    <fn id="cit_1">
      <label>2</label>
      <p id="notep0001">We contacted Hugging Face about making calls to their API in our website and were told that this is permissible for academic use.</p>
    </fn>
  </fn-group>
</back>
