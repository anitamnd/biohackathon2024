<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Transl Vis Sci Technol</journal-id>
    <journal-id journal-id-type="iso-abbrev">Transl Vis Sci Technol</journal-id>
    <journal-id journal-id-type="publisher-id">TVST</journal-id>
    <journal-title-group>
      <journal-title>Translational Vision Science &amp; Technology</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2164-2591</issn>
    <publisher>
      <publisher-name>The Association for Research in Vision and Ophthalmology</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9290317</article-id>
    <article-id pub-id-type="pmid">35833885</article-id>
    <article-id pub-id-type="doi">10.1167/tvst.11.7.12</article-id>
    <article-id pub-id-type="publisher-id">TVST-22-4423</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Artificial Intelligence</subject>
      </subj-group>
      <subj-group subj-group-type="category">
        <subject>Artificial Intelligence</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>AutoMorph: Automated Retinal Vascular Morphology Quantification Via a Deep Learning Pipeline</article-title>
      <alt-title alt-title-type="runhead">AutoMorph: Automated Retinal Vascular Morphology Quantification</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Zhou</surname>
          <given-names>Yukun</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wagner</surname>
          <given-names>Siegfried K.</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chia</surname>
          <given-names>Mark A.</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhao</surname>
          <given-names>An</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Woodward-Court</surname>
          <given-names>Peter</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff5" ref-type="aff">
          <sup>5</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Xu</surname>
          <given-names>Moucheng</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Struyven</surname>
          <given-names>Robbert</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Alexander</surname>
          <given-names>Daniel C.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <xref rid="afn1" ref-type="author-notes">*</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Keane</surname>
          <given-names>Pearse A.</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="afn1" ref-type="author-notes">*</xref>
      </contrib>
      <aff id="aff1"><label>1</label>Centre for Medical Image Computing, University College London, London, UK</aff>
      <aff id="aff2"><label>2</label>NIHR Biomedical Research Centre for Ophthalmology, Moorfields Eye Hospital NHS Foundation Trust and UCL Institute of Ophthalmology, London, UK</aff>
      <aff id="aff3"><label>3</label>Department of Computer Science, University College London, London, UK</aff>
      <aff id="aff4"><label>4</label>Department of Medical Physics and Biomedical Engineering, University College London, London, UK</aff>
      <aff id="aff5"><label>5</label>Institute of Health Informatics, University College London, London, UK</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><bold>Correspondence:</bold> Pearse A. Keane, NIHR Biomedical Research Centre for Ophthalmology, Moorfields Eye Hospital NHS Foundation Trust and UCL Institute of Ophthalmology, 162 City Road, London EC1V 2PD, UK. e-mail: <email>pearse.keane1@nhs.net</email></corresp>
      <fn id="afn1">
        <label>*</label>
        <p>DCA and PAK contributed equally to this work.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>14</day>
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <volume>11</volume>
    <issue>7</issue>
    <elocation-id>12</elocation-id>
    <history>
      <date date-type="accepted">
        <day>06</day>
        <month>6</month>
        <year>2022</year>
      </date>
      <date date-type="received">
        <day>28</day>
        <month>1</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright 2022 The Authors</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This work is licensed under a Creative Commons Attribution 4.0 International License.</license-p>
      </license>
    </permissions>
    <self-uri xlink:title="pdf" xlink:href="tvst-11-7-12.pdf"/>
    <abstract>
      <sec>
        <title>Purpose</title>
        <p>To externally validate a deep learning pipeline (AutoMorph) for automated analysis of retinal vascular morphology on fundus photographs. AutoMorph has been made publicly available, facilitating widespread research in ophthalmic and systemic diseases.</p>
      </sec>
      <sec>
        <title>Methods</title>
        <p>AutoMorph consists of four functional modules: image preprocessing, image quality grading, anatomical segmentation (including binary vessel, artery/vein, and optic disc/cup segmentation), and vascular morphology feature measurement. Image quality grading and anatomical segmentation use the most recent deep learning techniques. We employ a model ensemble strategy to achieve robust results and analyze the prediction confidence to rectify false gradable cases in image quality grading. We externally validate the performance of each module on several independent publicly available datasets.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>The EfficientNet-b4 architecture used in the image grading module achieves performance comparable to that of the state of the art for EyePACS-Q, with an <italic toggle="yes">F</italic><sub>1</sub>-score of 0.86. The confidence analysis reduces the number of images incorrectly assessed as gradable by 76%. Binary vessel segmentation achieves an <italic toggle="yes">F</italic><sub>1</sub>-score of 0.73 on AV-WIDE and 0.78 on DR HAGIS. Artery/vein scores are 0.66 on IOSTAR-AV, and disc segmentation achieves 0.94 in IDRID. Vascular morphology features measured from the AutoMorph segmentation map and expert annotation show good to excellent agreement.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p>AutoMorph modules perform well even when external validation data show domain differences from training data (e.g., with different imaging devices). This fully automated pipeline can thus allow detailed, efficient, and comprehensive analysis of retinal vascular morphology on color fundus photographs.</p>
      </sec>
      <sec>
        <title>Translational Relevance</title>
        <p>By making AutoMorph publicly available and open source, we hope to facilitate ophthalmic and systemic disease research, particularly in the emerging field of oculomics.</p>
      </sec>
    </abstract>
    <kwd-group>
      <kwd>retinal fundus photograph</kwd>
      <kwd>vascular analysis</kwd>
      <kwd>deep learning</kwd>
      <kwd>oculomics</kwd>
      <kwd>external validation</kwd>
    </kwd-group>
    <counts>
      <page-count count="14"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1">
    <title>Introduction</title>
    <p>The widespread availability of rapid, non-invasive retinal imaging has been one of the most notable developments within ophthalmology in recent decades. The significance of the retinal vasculature for assessing ophthalmic disease is well known; however, there is also growing interest in its capacity to provide valuable insights into systemic disease, a field that has been termed “oculomics.”<xref rid="bib1" ref-type="bibr"><sup>1</sup></xref><sup>–</sup><xref rid="bib4" ref-type="bibr"><sup>4</sup></xref> Narrowing of the retinal arteries is associated with hypertension and atherosclerosis,<xref rid="bib5" ref-type="bibr"><sup>5</sup></xref><sup>–</sup><xref rid="bib8" ref-type="bibr"><sup>8</sup></xref> and dilation of the retinal veins is linked with diabetic retinopathy.<xref rid="bib9" ref-type="bibr"><sup>9</sup></xref><sup>–</sup><xref rid="bib11" ref-type="bibr"><sup>11</sup></xref> Increased tortuosity of the retinal arteries is also associated with hypercholesterolemia and hypertension.<xref rid="bib12" ref-type="bibr"><sup>12</sup></xref><sup>–</sup><xref rid="bib14" ref-type="bibr"><sup>14</sup></xref> Considering that manual vessel segmentation and feature extraction can be extremely time consuming, as well as poorly reproducible,<xref rid="bib15" ref-type="bibr"><sup>15</sup></xref> there has been growing interest in the development of tools that can extract retinal vascular features in a fully automated manner.</p>
    <p>In recent decades, a large body of technical work has focused on retinal vessel map segmentation. Performance has improved dramatically by employing a range of techniques, from unsupervised graph- and feature-based methods<xref rid="bib16" ref-type="bibr"><sup>16</sup></xref><sup>–</sup><xref rid="bib20" ref-type="bibr"><sup>20</sup></xref> to supervised deep learning models.<xref rid="bib21" ref-type="bibr"><sup>21</sup></xref> Despite this progress, the widespread use of these techniques in clinical research has been limited by a number of factors. First, technical papers<xref rid="bib21" ref-type="bibr"><sup>21</sup></xref><sup>–</sup><xref rid="bib25" ref-type="bibr"><sup>25</sup></xref> often focus on performing a single function while ignoring upstream and downstream tasks, such as preprocessing<xref rid="bib24" ref-type="bibr"><sup>24</sup></xref><sup>,</sup><xref rid="bib25" ref-type="bibr"><sup>25</sup></xref> and feature measurement.<xref rid="bib21" ref-type="bibr"><sup>21</sup></xref><sup>–</sup><xref rid="bib23" ref-type="bibr"><sup>23</sup></xref> Second, existing techniques often perform poorly when applied to real-world clinical settings limited by poor generalizability outside of the environment in which they were developed.<xref rid="bib26" ref-type="bibr"><sup>26</sup></xref><sup>,</sup><xref rid="bib27" ref-type="bibr"><sup>27</sup></xref></p>
    <p>Although some software has been utilized for clinical research, most of it is only semi-automated, requiring human intervention for correcting vessel segmentation and artery/vein identification.<xref rid="bib6" ref-type="bibr"><sup>6</sup></xref><sup>,</sup><xref rid="bib24" ref-type="bibr"><sup>24</sup></xref><sup>,</sup><xref rid="bib25" ref-type="bibr"><sup>25</sup></xref><sup>,</sup><xref rid="bib28" ref-type="bibr"><sup>28</sup></xref><sup>,</sup><xref rid="bib29" ref-type="bibr"><sup>29</sup></xref> This limits process efficiency and introduces subjective bias, potentially influencing the final outcomes. Further, most existing software has not integrated the crucial functions required for such a pipeline—namely, image cropping, quality assessment, segmentation, and vascular feature measurement. For example, poor-quality images in research cohorts often must be manually filtered by physicians, which generates a considerable workload. There is also the potential to improve the performance of underlying segmentation algorithms by employing the most recent advances in machine learning, thus enhancing the accuracy of vascular feature measurements.</p>
    <p>In this study, we explored the feasibility of a deep learning pipeline providing automated analysis of retinal vascular morphology from color fundus photographs. We highlight three unique advantages of the proposed AutoMorph pipeline:<list list-type="simple"><list-item><label>•</label><p>AutoMorph consists of four functional modules, including (1) retinal image preprocessing; (2) image quality grading; (3) anatomical segmentation (binary vessel segmentation, artery/vein segmentation, and optic disc segmentation); and (4) morphological feature measurement.</p></list-item><list-item><label>•</label><p>AutoMorph alleviates the need for physician intervention by addressing two key areas. First, we employ an ensemble technique with confidence analysis to reduce the number of ungradable images that are incorrectly classified as being gradable (false gradable images). Second, accurate binary vessel segmentation and artery/vein identification reduce the need for manual rectification.</p></list-item><list-item><label>•</label><p>AutoMorph generates a diverse catalog of retinal feature measurements that previous work indicates has the potential to be used for the exploration of ocular biomarkers for systemic disease.</p></list-item></list></p>
    <p>Perhaps most importantly, we made AutoMorph publicly available with a view to stimulating breakthroughs in the emerging field of oculomics.</p>
  </sec>
  <sec sec-type="methods" id="sec2">
    <title>Methods</title>
    <p>The AutoMorph pipeline consists of four modules: (1) image preprocessing, (2) image quality grading, (3) anatomical segmentation, and (4) metric measurement (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). Source code for this pipeline is available from <ext-link xlink:href="https://github.com/rmaphoh/AutoMorph" ext-link-type="uri">https://github.com/rmaphoh/AutoMorph</ext-link>.</p>
    <fig position="float" id="fig1">
      <label>Figure 1.</label>
      <caption>
        <p>Diagram of the proposed AutoMorph pipeline. The input is color fundus photography, and the final output is the measured vascular morphology features. Image quality grading and anatomical segmentation modules use deep learning models. Confidence analysis decreases false gradable images in the image quality grading module.</p>
      </caption>
      <graphic xlink:href="tvst-11-7-12-f001" position="float"/>
    </fig>
    <sec id="sec2-1">
      <title>Datasets</title>
      <p>The datasets used for development and external validation of the deep learning models described in this work are summarized in <xref rid="tbl1" ref-type="table">Table 1</xref> and <xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">Supplementary Material S1</xref>. For model training, we chose publicly available datasets that contain a large quantity of annotated images.<xref rid="bib30" ref-type="bibr"><sup>30</sup></xref> Importantly, a diverse combination of public datasets was used in order to enhance external generalizability. Some image examples are shown in <xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">Supplementary Figure S1</xref>. To validate the models, we externally evaluated the performance of those trained models on datasets distinct from those on which they were trained (e.g., imaging devices, countries of origin, types of pathology). All of the datasets provide the retinal fundus photographs and the corresponding expert annotation. For image quality grading datasets (using EyePACS-Q as an example), two experts grade each image into three categories: good, usable, and reject quality, determined by image illumination, artifacts, and the diagnosability of the general eye diseases to the experts. For anatomical segmentation datasets, such as the Digital Retinal Images for Vessel Extraction (DRIVE) dataset for the binary vessel segmentation task, two experts annotate each pixel as vessel or background, thus generating a ground-truth map with the same size of the retinal fundus photographs, where a white color indicates vessel pixels and a black color the background. More details can be found in <xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">Supplementary Material S1</xref>.</p>
      <table-wrap position="float" content-type="5col" id="tbl1">
        <label>Table 1.</label>
        <caption>
          <p>Characteristics of the Training and External Validation Data</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col align="left" span="1"/>
            <col align="center" span="1"/>
            <col align="center" span="1"/>
            <col align="center" span="1"/>
            <col align="left" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Type of Data</th>
              <th align="center" rowspan="1" colspan="1">Dataset Name</th>
              <th align="center" rowspan="1" colspan="1">Country of Origin</th>
              <th align="center" rowspan="1" colspan="1">Image Quantity<xref rid="tb1fn1" ref-type="table-fn"><sup>a</sup></xref></th>
              <th align="center" rowspan="1" colspan="1">Device (Manufacturer)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" colspan="5" rowspan="1">Image Quality Grading</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Training data</td>
              <td rowspan="1" colspan="1">EyePACS-Q-train<xref rid="bib30" ref-type="bibr"><sup>30</sup></xref><sup>,</sup><xref rid="bib31" ref-type="bibr"><sup>31</sup></xref></td>
              <td rowspan="1" colspan="1">USA</td>
              <td rowspan="1" colspan="1">12,543 (NR, more than 99%)</td>
              <td rowspan="1" colspan="1">A variety of imaging devices, including DRS (CenterVue, Padova, Italy); iCam (Optovue, Fremont, CA); CR1/DGi/CR2 (Canon, Tokyo, Japan); Topcon NW 8 (Topcon, Tokyo, Japan)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Internal validation data</td>
              <td rowspan="1" colspan="1">EyePACS-Q-test<xref rid="bib30" ref-type="bibr"><sup>30</sup></xref><sup>,</sup><xref rid="bib31" ref-type="bibr"><sup>31</sup></xref></td>
              <td rowspan="1" colspan="1">USA</td>
              <td rowspan="1" colspan="1">16,249 (NR, more than 99%)</td>
              <td align="center" rowspan="1" colspan="1">—</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">External validation data</td>
              <td rowspan="1" colspan="1">DDR test<xref rid="bib32" ref-type="bibr"><sup>32</sup></xref></td>
              <td rowspan="1" colspan="1">China</td>
              <td rowspan="1" colspan="1">4,105 (100%)</td>
              <td rowspan="1" colspan="1">42 types of fundus cameras, mainly Topcon D7000, Topcon TRC NW48, D5200 (Nikon, Tokyo, Japan), and Canon CR 2 cameras</td>
            </tr>
            <tr>
              <td align="left" colspan="5" rowspan="1"/>
            </tr>
            <tr>
              <td align="left" colspan="5" rowspan="1">Binary Vessel Segmentation</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Training data</td>
              <td rowspan="1" colspan="1">DRIVE<xref rid="bib33" ref-type="bibr"><sup>33</sup></xref></td>
              <td rowspan="1" colspan="1">Netherlands</td>
              <td rowspan="1" colspan="1">40 (100%)</td>
              <td rowspan="1" colspan="1">CR5 non-mydriatic 3CCD camera (Canon)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">STARE<xref rid="bib34" ref-type="bibr"><sup>34</sup></xref></td>
              <td rowspan="1" colspan="1">USA</td>
              <td rowspan="1" colspan="1">20 (100%)</td>
              <td rowspan="1" colspan="1">TRV-50 fundus camera (Topcon)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">CHASEDB1<xref rid="bib35" ref-type="bibr"><sup>35</sup></xref></td>
              <td rowspan="1" colspan="1">UK</td>
              <td rowspan="1" colspan="1">28 (0%)</td>
              <td rowspan="1" colspan="1">NM-200D handheld fundus camera (Nidek, Aichi, Japan)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">HRF<xref rid="bib36" ref-type="bibr"><sup>36</sup></xref></td>
              <td rowspan="1" colspan="1">Germany and Czech Republic</td>
              <td rowspan="1" colspan="1">45 (100%)</td>
              <td rowspan="1" colspan="1">CF-60UVi camera (Canon)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">IOSTAR<xref rid="bib37" ref-type="bibr"><sup>37</sup></xref></td>
              <td rowspan="1" colspan="1">Netherlands and China</td>
              <td rowspan="1" colspan="1">30 (53.3%)</td>
              <td rowspan="1" colspan="1">EasyScan camera (i-Optics, Rijswijk, Netherlands)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">LES-AV<xref rid="bib38" ref-type="bibr"><sup>38</sup></xref></td>
              <td rowspan="1" colspan="1">NR</td>
              <td rowspan="1" colspan="1">22 (0%)</td>
              <td rowspan="1" colspan="1">Visucam Pro NM fundus camera (Carl Zeiss Meditec, Jena, Germany)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">External validation data<xref rid="tb1fn2" ref-type="table-fn"><sup>b</sup></xref></td>
              <td rowspan="1" colspan="1">AV-WIDE<xref rid="bib19" ref-type="bibr"><sup>19</sup></xref><sup>,</sup><xref rid="bib39" ref-type="bibr"><sup>39</sup></xref></td>
              <td rowspan="1" colspan="1">USA</td>
              <td rowspan="1" colspan="1">30 (100%)</td>
              <td rowspan="1" colspan="1">200Tx Ultra-widefield Imaging Device (Optos, Dunfermline, UK)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">DR HAGIS<xref rid="bib40" ref-type="bibr"><sup>40</sup></xref></td>
              <td rowspan="1" colspan="1">UK</td>
              <td rowspan="1" colspan="1">39 (100%)</td>
              <td rowspan="1" colspan="1">TRC-NW6s (Topcon), TRC-NW8 (Topcon), or CR-DGi fundus camera (Canon)</td>
            </tr>
            <tr>
              <td align="left" colspan="5" rowspan="1"/>
            </tr>
            <tr>
              <td align="left" colspan="5" rowspan="1">Artery/Vein Segmentation</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Training data</td>
              <td rowspan="1" colspan="1">DRIVE-AV<xref rid="bib33" ref-type="bibr"><sup>33</sup></xref><sup>,</sup><xref rid="bib41" ref-type="bibr"><sup>41</sup></xref></td>
              <td rowspan="1" colspan="1">Netherlands</td>
              <td rowspan="1" colspan="1">40 (100%)</td>
              <td rowspan="1" colspan="1">CR5 non-mydriatic 3CCD camera (Canon)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">HRF-AV<xref rid="bib36" ref-type="bibr"><sup>36</sup></xref><sup>,</sup><xref rid="bib42" ref-type="bibr"><sup>42</sup></xref></td>
              <td rowspan="1" colspan="1">Germany and Czech Republic</td>
              <td rowspan="1" colspan="1">45 (100%)</td>
              <td rowspan="1" colspan="1">CF-60UVi camera (Canon)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">LES-AV<xref rid="bib38" ref-type="bibr"><sup>38</sup></xref></td>
              <td rowspan="1" colspan="1">Nauru</td>
              <td rowspan="1" colspan="1">22 (9%)</td>
              <td rowspan="1" colspan="1">Visucam Pro NM fundus camera (Zeiss)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">External validation data</td>
              <td rowspan="1" colspan="1">IOSTAR-AV<xref rid="bib37" ref-type="bibr"><sup>37</sup></xref><sup>,</sup><xref rid="bib43" ref-type="bibr"><sup>43</sup></xref></td>
              <td rowspan="1" colspan="1">Netherlands and China</td>
              <td rowspan="1" colspan="1">30 (53.3%)</td>
              <td rowspan="1" colspan="1">EasyScan camera (i-Optics)</td>
            </tr>
            <tr>
              <td align="left" colspan="5" rowspan="1"/>
            </tr>
            <tr>
              <td align="left" colspan="5" rowspan="1">Optic Disc Segmentation</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Training data</td>
              <td rowspan="1" colspan="1">REFUGE<xref rid="bib44" ref-type="bibr"><sup>44</sup></xref></td>
              <td rowspan="1" colspan="1">China</td>
              <td rowspan="1" colspan="1">800 (100%)</td>
              <td rowspan="1" colspan="1">Visucam 500 fundus camera (Zeiss) and CR-2 camera (Canon)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">GAMMA<xref rid="bib45" ref-type="bibr"><sup>45</sup></xref><sup>,</sup><xref rid="bib46" ref-type="bibr"><sup>46</sup></xref></td>
              <td rowspan="1" colspan="1">China</td>
              <td rowspan="1" colspan="1">100 (100%)</td>
              <td align="center" rowspan="1" colspan="1">—</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">External validation data<xref rid="tb1fn3" ref-type="table-fn"><sup>c</sup></xref></td>
              <td rowspan="1" colspan="1">IDRID<xref rid="bib47" ref-type="bibr"><sup>47</sup></xref></td>
              <td rowspan="1" colspan="1">India</td>
              <td rowspan="1" colspan="1">81 (100%)</td>
              <td rowspan="1" colspan="1">VX-10α digital fundus camera (Kowa, Las Vegas, NV)</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tb1fn1">
            <p>External validation data are unseen for model training and were purely used to evaluate the trained model performance on out-of-distribution data with different countries of origin and imaging devices. EyePACS-Q is a subset of EyePACS with image quality grading annotation. NR, not reported.</p>
          </fn>
          <fn id="tb1fn2">
            <label>a</label>
            <p>Image quantity indicates the image number used in this work and the parentheses show the proportion of macula-centered images.</p>
          </fn>
          <fn id="tb1fn3">
            <label>b</label>
            <p>Although we have evaluated the binary vessel segmentation model on the ultra-widefield retinal fundus dataset AV-WIDE, we recommend using AutoMorph on retinal fundus photographs with a 25° to 60° FOV, as all of the deep learning models are trained using images with FOV equals to 25° to 60°, and the preprocessing step is tailored for images with this FOV.</p>
          </fn>
          <fn id="tb1fn4">
            <label>c</label>
            <p>Evaluated on disc due to no cup annotation.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec id="sec2-2">
      <title>Modules</title>
      <sec id="sec2-2-1">
        <title>Image Preprocessing</title>
        <p>Retinal fundus photographs often contain superfluous background, resulting in dimensions that deviate from a geometric square. To account for this, we employed a technique that combines thresholding, morphological image operations, and cropping<xref rid="bib31" ref-type="bibr"><sup>31</sup></xref> to remove the background so that the resulting image conforms to a geometric square (examples are shown in <xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">Supplementary Fig. S2</xref>).</p>
      </sec>
      <sec id="sec2-2-2">
        <title>Image Quality Grading</title>
        <p>To filter out ungradable images that often fail in segmentation and measurement modules, AutoMorph incorporates a classification model to identify ungradable images. The model classifies each image as good, usable, or reject quality. In our study, good and usable images were considered to be gradable; however, this decision may be modified in scenarios with sufficient data to include only good-quality images. We employed EfficientNet-B4<xref rid="bib48" ref-type="bibr"><sup>48</sup></xref> as the model architecture and performed transfer learning on EyePACS-Q. Further details are outlined in <xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">Supplementary Material S2</xref> and <xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">Supplementary Figure S3</xref>.</p>
      </sec>
      <sec id="sec2-2-3">
        <title>Anatomical Segmentation</title>
        <p>Vascular structure is thin and elusive especially against low-contrast backgrounds. To enhance binary vessel segmentation performance, AutoMorph uses an adversarial segmentation network.<xref rid="bib23" ref-type="bibr"><sup>23</sup></xref> Six public datasets were used for model training (<xref rid="tbl1" ref-type="table">Table 1</xref>). Accurate artery/vein segmentation is a long-standing challenge. To address this, we employed an information fusion network<xref rid="bib22" ref-type="bibr"><sup>22</sup></xref> tailored for artery/vein segmentation. Three datasets were used for training. Parapapillary atrophic changes, which can be a hallmark of myopia or glaucoma, can cause large errors in disc localization and segmentation. To counter this, AutoMorph employs a coarse-to-fine deep learning network,<xref rid="bib49" ref-type="bibr"><sup>49</sup></xref> which achieved first place for disc segmentation in the MICCAI 2021 GAMMA challenge.<xref rid="bib45" ref-type="bibr"><sup>45</sup></xref><sup>,</sup><xref rid="bib46" ref-type="bibr"><sup>46</sup></xref> Two public datasets were utilized in model training. Further detailed information is provided in <xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">Supplementary Material S3</xref>.</p>
      </sec>
      <sec id="sec2-2-4">
        <title>Vascular Morphology Feature Measurement</title>
        <p>AutoMorph measures a series of clinically relevant vascular features, as summarized in <xref rid="fig2" ref-type="fig">Figure 2</xref> (comprehensive list in <xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">Supplementary Fig. S13</xref>). Three different calculation methods for vessel tortuosity are provided, including distance measurement tortuosity, squared curvature tortuosity,<xref rid="bib50" ref-type="bibr"><sup>50</sup></xref> and tortuosity density.<xref rid="bib51" ref-type="bibr"><sup>51</sup></xref> The fractal dimension value (Minkowski–Bouligand dimension)<xref rid="bib52" ref-type="bibr"><sup>52</sup></xref> provides a measurement of vessel complexity. The vessel density indicates the ratio between the area of vessels to the whole image. For vessel caliber, AutoMorph calculates the central retinal arteriolar equivalent (CRAE) and central retinal venular equivalent (CRVE), as well as the arteriolar–venular ratio (AVR).<xref rid="bib53" ref-type="bibr"><sup>53</sup></xref><sup>–</sup><xref rid="bib55" ref-type="bibr"><sup>55</sup></xref> AutoMorph measures the features in standard regions, including Zone B (the annulus 0.5–1 optic disc diameter from the disc margin) and Zone C (the annulus 0.5–2 optic disc diameter from the disc margin).<xref rid="bib29" ref-type="bibr"><sup>29</sup></xref> Considering that Zone B and Zone C of macular-centered images may be out of the circular fundus, the features for the whole image are also measured.</p>
        <fig position="float" id="fig2">
          <label>Figure 2.</label>
          <caption>
            <p>Features measured by AutoMorph, including tortuosity, vessel caliber, disc-to-cup ratio, and others. For each image, the optic disc/cup information is measured, including the height and width, as well as cup-to-disc ratio. For binary vessels, the tortuosity, fractal dimension, vessel density, and average width are measured. In addition to these features, arteries/veins are also used for measuring the caliber features CRAE, CRVE, and AVR by Hubbard and Knudtson methods.</p>
          </caption>
          <graphic xlink:href="tvst-11-7-12-f002" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec id="sec2-3">
      <title>Ensemble and Confidence Analysis</title>
      <p>In model training, 80% of the training data is used for model training and 20% is used to tune the training hyperparameters, such as scheduling the learning rate. In retinal image grading, we ensemble the output from eight trained models with different subsets of training data, as it generally gives a more robust result.<xref rid="bib56" ref-type="bibr"><sup>56</sup></xref> Moreover, the average value and standard deviation (SD) of the eight possibilities are calculated for confidence analysis. Average probability indicates the average confidence of prediction. Low average cases are prone to false predictions, such as <xref rid="fig3" ref-type="fig">Figure 3</xref>c. Meanwhile, SD represents the inconsistency between models. High inconsistency likely corresponds to a false prediction, as shown in <xref rid="fig3" ref-type="fig">Figure 3</xref>d. The images with either low average probability or high SD are automatically recognized as low-confidence images and rectified as ungradable. False gradable images can fail the anatomical segmentation module, thus generating a large error in vascular feature measurement. The confidence analysis economizes physician intervention and increases the reliability of AutoMorph by filtering these potential errors. To our knowledge, this is the first report of a confidence analysis combined with the model ensemble integrated within the vessel analysis pipeline. An average threshold corresponds to a change of operating point and SD threshold involved in uncertainty theory. In this work, we set an average threshold of 0.75 and a SD threshold of 0.1 to filter out false gradable images. Specifically, the average probability lower than 0.75 or SD larger than 0.1 were rectified as ungradable images. The rationale for selecting these threshold values is based on the probability distribution histogram on tuning data. More details are described in <xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">Supplementary Material S2</xref> and <xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">Supplementary Figure S4</xref>.</p>
      <fig position="float" id="fig3">
        <label>Figure 3.</label>
        <caption>
          <p>Confidence analysis for image quality grading. M1 to M8 represent the eight ensemble models. For each image, the predicted category is transferred as gradable or ungradable (good and usable are as gradable, reject as ungradable). The average probability and SD are calculated for the predicted category. (a, b) Two image cases with high confidence in prediction. The case shown in (c) is classified as gradable quality with low average probability of 0.619, and the case in (d) has a high SD of 0.191, which are defined as low-confidence images in our work. Although (c) and (d) are preliminarily classified as gradable, the final classification is rectified as ungradable with the confidence threshold.</p>
        </caption>
        <graphic xlink:href="tvst-11-7-12-f003" position="float"/>
      </fig>
    </sec>
    <sec sec-type="methods" id="sec2-4">
      <title>Statistical Analyses and Compared Methods</title>
      <p>For deep learning functional modules, the well-established expert annotation is used as a reference standard to quantitatively evaluate the module performance. We calculated sensitivity, specificity, positive predictive value (precision), accuracy, area under the receiver operating characteristic (AUC-ROC) curve, <italic toggle="yes">F</italic><sub>1</sub>-score, and intersection of union (IoU) metrics to verify the model performance. These metric definitions are
<disp-formula id="ueq1"><mml:math id="math-ueq1" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="ueq2"><mml:math id="math-ueq2" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="ueq3"><mml:math id="math-ueq3" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="ueq4"><mml:math id="math-ueq4" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="ueq5"><mml:math id="math-ueq5" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mspace width="4pt"/><mml:mo>×</mml:mo><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mspace width="4pt"/><mml:mo>+</mml:mo><mml:mspace width="4pt"/><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic toggle="yes">TP</italic>, <italic toggle="yes">TN</italic>, <italic toggle="yes">FP</italic>, and <italic toggle="yes">FN</italic> indicate true positive, true negative, false positive, and false negative, respectively. AUC-ROC curve is a performance measurement for classification problems at various threshold settings; it tells how much the model is capable of distinguishing between classes. In segmentation tasks, IoU measures the overlap degree between ground-truth maps and segmentation maps. Following the same setting,<xref rid="bib31" ref-type="bibr"><sup>31</sup></xref><sup>,</sup><xref rid="bib39" ref-type="bibr"><sup>39</sup></xref><sup>,</sup><xref rid="bib57" ref-type="bibr"><sup>57</sup></xref><sup>–</sup><xref rid="bib59" ref-type="bibr"><sup>59</sup></xref> we set the ungradable images as the positive class in image quality grading. The probability of the ungradable category equals that of reject quality, and the probability of the gradable category is the sum of good quality and usable quality. As introduced in the discussion on confidence analysis, we used a mean value of 0.75 and SD of 0.1 as thresholds to obtain the final rectified gradable and ungradable categories. For binary vessel segmentation, each pixel of the retinal fundus photograph corresponds to a binary classification task. The vessel pixel is positive class and the background pixel is negative. The probability range for each pixel is from 0 to 1, where a larger value indicates a higher probability of being a vessel pixel. We thresholded the segmentation map with 0.5, which is a standard threshold for binary medical image segmentation tasks. Optic disc segmentation is similar to binary vessel segmentation, but the difference is that the positive class is the optic disc pixel. For artery/vein segmentation, each pixel has a four-class probability of artery, vein, uncertain pixel, and background. Following standard settings for multiclass segmentation tasks, the category with the largest probability across the four classes is the thresholded pixel category. More information is listed in <xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">Supplementary Material S3</xref>.</p>
      <p>We conducted the quantitative comparison to other competitive methods to characterize the generalizability of AutoMorph using external validation. We used internal validation results from other published work to provide a benchmark for a well-performing model. These methods used a reasonable proportion of data for model training and the remainder for internal validation (e.g., fivefold validation that means 80% of images are used for training and tuning and 20% are used for validating the trained model), and claimed that they have achieved state-of-the-art performance. As introduced in <xref rid="tbl1" ref-type="table">Table 1</xref>, the models of AutoMorph are trained on several public datasets and externally validated on separate datasets, whereas the compared methods<xref rid="bib39" ref-type="bibr"><sup>39</sup></xref><sup>,</sup><xref rid="bib57" ref-type="bibr"><sup>57</sup></xref><sup>–</sup><xref rid="bib59" ref-type="bibr"><sup>59</sup></xref> are trained in the same domain data as the validation data but with fewer training images. The goal of the comparison was not to prove the technical strengths of AutoMorph over recent methods, as this has already been verified in previously published work.<xref rid="bib22" ref-type="bibr"><sup>22</sup></xref><sup>,</sup><xref rid="bib23" ref-type="bibr"><sup>23</sup></xref><sup>,</sup><xref rid="bib47" ref-type="bibr"><sup>47</sup></xref><sup>,</sup><xref rid="bib48" ref-type="bibr"><sup>48</sup></xref> Rather, we aimed to demonstrate that, due to the diversity of its training data, AutoMorph performs well on external datasets, even when these datasets include pathology and show large domain differences from the training data. Additionally, to demonstrate the technical superiority of this method, we have provided the internal validation of AutoMorph in <xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">Supplementary Table S1</xref>.</p>
      <p>Considering that we employ standard formulas<xref rid="bib29" ref-type="bibr"><sup>29</sup></xref><sup>,</sup><xref rid="bib50" ref-type="bibr"><sup>50</sup></xref><sup>–</sup><xref rid="bib52" ref-type="bibr"><sup>52</sup></xref> to measure vascular morphology features, the measurement error only comes from inaccuracy of anatomical segmentation. In order to evaluate measurement error that occurs as a result of vessel segmentation, we respectively measure the vascular features based on AutoMorph segmentation and expert vessel annotation, and then we draw Bland–Altman plots. Following the same evaluation,<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref><sup>,</sup><xref rid="bib60" ref-type="bibr"><sup>60</sup></xref> intraclass correlation coefficients (ICCs) are calculated to quantitatively show agreement. Additionally, the boxplots of differences between the vascular features from AutoMorph segmentation and expert annotation are shown in <xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">Supplementary Figures S9</xref>–<xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">S11</xref>.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="sec3">
    <title>Results</title>
    <p>Results for external validation of AutoMorph are summarized in <xref rid="tbl2" ref-type="table">Table 2</xref>.</p>
    <table-wrap position="float" content-type="14col" id="tbl2">
      <label>Table 2.</label>
      <caption>
        <p>Validation of Functional Modules and Comparison With Other Methods</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col align="left" span="1"/>
          <col align="center" span="1"/>
          <col align="center" span="1"/>
          <col align="center" span="1"/>
          <col align="center" span="1"/>
          <col align="center" span="1"/>
          <col align="char" char="." span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th rowspan="1" colspan="1"/>
            <th align="center" colspan="4" rowspan="1">Image Quality Grading</th>
            <th align="center" colspan="2" rowspan="1">Artery/Vein Segmentation</th>
          </tr>
          <tr>
            <th rowspan="1" colspan="1"/>
            <th align="center" colspan="2" rowspan="1">EyePACS-Q Test</th>
            <th align="center" colspan="2" rowspan="1">DDR Test</th>
            <th align="center" colspan="2" rowspan="1">IOSTAR-AV</th>
          </tr>
          <tr>
            <th rowspan="1" colspan="1"/>
            <th align="center" rowspan="1" colspan="1">AutoMorph (Internal)</th>
            <th align="center" rowspan="1" colspan="1">Comparison<xref rid="bib31" ref-type="bibr"><sup>31</sup></xref> (Internal)</th>
            <th align="center" rowspan="1" colspan="1">AutoMorph (External)</th>
            <th align="center" rowspan="1" colspan="1">Comparison<sup>a</sup> (Internal)</th>
            <th align="center" rowspan="1" colspan="1">AutoMorph (External)</th>
            <th align="center" rowspan="1" colspan="1">Comparison<xref rid="bib58" ref-type="bibr"><sup>58</sup></xref> (Internal)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="1" colspan="1">Sensitivity</td>
            <td rowspan="1" colspan="1">0.85</td>
            <td rowspan="1" colspan="1">0.85</td>
            <td rowspan="1" colspan="1">1</td>
            <td rowspan="1" colspan="1">0.93</td>
            <td rowspan="1" colspan="1">0.64</td>
            <td rowspan="1" colspan="1">0.79</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Specificity</td>
            <td rowspan="1" colspan="1">0.93</td>
            <td align="center" rowspan="1" colspan="1">NR</td>
            <td rowspan="1" colspan="1">0.89</td>
            <td rowspan="1" colspan="1">0.97</td>
            <td rowspan="1" colspan="1">0.98</td>
            <td rowspan="1" colspan="1">0.76</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Precision</td>
            <td rowspan="1" colspan="1">0.87</td>
            <td rowspan="1" colspan="1">0.87</td>
            <td rowspan="1" colspan="1">0.6</td>
            <td rowspan="1" colspan="1">0.73</td>
            <td rowspan="1" colspan="1">0.68</td>
            <td align="center" rowspan="1" colspan="1">NR</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Accuracy</td>
            <td rowspan="1" colspan="1">0.92</td>
            <td rowspan="1" colspan="1">0.92</td>
            <td rowspan="1" colspan="1">0.91</td>
            <td rowspan="1" colspan="1">0.99</td>
            <td rowspan="1" colspan="1">0.96</td>
            <td rowspan="1" colspan="1">0.78</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">AUC-ROC</td>
            <td rowspan="1" colspan="1">0.97</td>
            <td align="center" rowspan="1" colspan="1">NR</td>
            <td rowspan="1" colspan="1">0.99</td>
            <td rowspan="1" colspan="1">0.99</td>
            <td rowspan="1" colspan="1">0.95</td>
            <td align="center" rowspan="1" colspan="1">NR</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1"><italic toggle="yes">F</italic><sub>1</sub>-score</td>
            <td rowspan="1" colspan="1">0.86</td>
            <td rowspan="1" colspan="1">0.86</td>
            <td rowspan="1" colspan="1">0.75</td>
            <td rowspan="1" colspan="1">0.82</td>
            <td rowspan="1" colspan="1">0.66</td>
            <td align="center" rowspan="1" colspan="1">NR</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">IoU</td>
            <td align="center" rowspan="1" colspan="1">—</td>
            <td align="center" rowspan="1" colspan="1">—</td>
            <td align="center" rowspan="1" colspan="1">—</td>
            <td align="center" rowspan="1" colspan="1">—</td>
            <td rowspan="1" colspan="1">0.53</td>
            <td align="center" rowspan="1" colspan="1">NR</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1"/>
            <td align="center" colspan="4" rowspan="1">Binary Vessel Segmentation</td>
            <td align="center" colspan="2" rowspan="1">Optic Disc</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1"/>
            <td align="center" colspan="2" rowspan="1">Ultra-widefield: AV-WIDE</td>
            <td align="center" colspan="2" rowspan="1">Standard Field: DR HAGIS</td>
            <td align="center" colspan="2" rowspan="1">IDRID</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1"/>
            <td align="center" rowspan="1" colspan="1">AutoMorph (External)</td>
            <td align="center" rowspan="1" colspan="1">Comparison<xref rid="bib39" ref-type="bibr"><sup>39</sup></xref> (Internal)</td>
            <td align="center" rowspan="1" colspan="1">AutoMorph (External)</td>
            <td align="center" rowspan="1" colspan="1">Comparison<xref rid="bib57" ref-type="bibr"><sup>57</sup></xref> (Internal)</td>
            <td align="center" rowspan="1" colspan="1">AutoMorph (External)</td>
            <td align="center" rowspan="1" colspan="1">Comparison<xref rid="bib59" ref-type="bibr"><sup>59</sup></xref> (Internal)</td>
          </tr>
          <tr>
            <td align="center" colspan="7" rowspan="1"/>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Sensitivity</td>
            <td rowspan="1" colspan="1">0.71</td>
            <td rowspan="1" colspan="1">0.78</td>
            <td rowspan="1" colspan="1">0.84</td>
            <td rowspan="1" colspan="1">0.67</td>
            <td rowspan="1" colspan="1">0.9</td>
            <td rowspan="1" colspan="1">0.9</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Specificity</td>
            <td rowspan="1" colspan="1">0.98</td>
            <td align="center" rowspan="1" colspan="1">NR</td>
            <td rowspan="1" colspan="1">0.98</td>
            <td rowspan="1" colspan="1">0.98</td>
            <td rowspan="1" colspan="1">0.95</td>
            <td align="center" rowspan="1" colspan="1">NR</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Precision</td>
            <td rowspan="1" colspan="1">0.75</td>
            <td rowspan="1" colspan="1">0.82</td>
            <td rowspan="1" colspan="1">0.73</td>
            <td align="center" rowspan="1" colspan="1">NR</td>
            <td rowspan="1" colspan="1">0.94</td>
            <td align="center" rowspan="1" colspan="1">NR</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Accuracy</td>
            <td rowspan="1" colspan="1">0.96</td>
            <td rowspan="1" colspan="1">0.97</td>
            <td rowspan="1" colspan="1">0.97</td>
            <td rowspan="1" colspan="1">0.97</td>
            <td rowspan="1" colspan="1">0.99</td>
            <td rowspan="1" colspan="1">0.99</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">AUC-ROC</td>
            <td rowspan="1" colspan="1">0.96</td>
            <td align="center" rowspan="1" colspan="1">NR</td>
            <td rowspan="1" colspan="1">0.98</td>
            <td align="center" rowspan="1" colspan="1">NR</td>
            <td rowspan="1" colspan="1">0.95</td>
            <td align="center" rowspan="1" colspan="1">NR</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1"><italic toggle="yes">F</italic><sub>1</sub>-score</td>
            <td rowspan="1" colspan="1">0.73</td>
            <td rowspan="1" colspan="1">0.8</td>
            <td rowspan="1" colspan="1">0.78</td>
            <td rowspan="1" colspan="1">0.71</td>
            <td rowspan="1" colspan="1">0.94</td>
            <td align="center" rowspan="1" colspan="1">NR</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">IoU</td>
            <td rowspan="1" colspan="1">0.57</td>
            <td align="center" rowspan="1" colspan="1">NR</td>
            <td rowspan="1" colspan="1">0.64</td>
            <td align="center" rowspan="1" colspan="1">NR</td>
            <td rowspan="1" colspan="1">0.91</td>
            <td rowspan="1" colspan="1">0.85</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="tb2fn1">
          <p>“Internal” indicates that the validation and training data are from the same dataset but isolated. “External” means that validation data are from external datasets. The comparisons are with competitive methods of image quality grading,<xref rid="bib31" ref-type="bibr"><sup>31</sup></xref> binary vessel segmentation,<xref rid="bib39" ref-type="bibr"><sup>39</sup></xref><sup>,</sup><xref rid="bib57" ref-type="bibr"><sup>57</sup></xref> artery/vein segmentation,<xref rid="bib58" ref-type="bibr"><sup>58</sup></xref> and optic disc segmentation.<xref rid="bib59" ref-type="bibr"><sup>59</sup></xref> NR, not reported.</p>
        </fn>
        <fn id="tb2fn2">
          <p><sup>a</sup>Due to no comparison method on the DDR test, we compared AutoMorph (external) to the same architecture, EfficientNet-b4, that is trained with DDR train data (internal).</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <sec id="sec3-1">
      <title>Image Quality Grading</title>
      <p>The internal validation is on EyePACS-Q test data. For fair comparison,<xref rid="bib31" ref-type="bibr"><sup>31</sup></xref> we evaluated the image quality grading performance of categorizing good, usable, and reject quality. The quantitative results are listed in <xref rid="tbl2" ref-type="table">Table 2</xref>. The classification <italic toggle="yes">F</italic><sub>1</sub>-score achieved 0.86, on par with the state-of-the-art method with a <italic toggle="yes">F</italic><sub>1</sub>-score of 0.86.<xref rid="bib31" ref-type="bibr"><sup>31</sup></xref> The prediction was transferred to gradable (good and usable quality) and ungradable (reject quality), and the resulting confusion matrix of validation on the EyePACS-Q test is shown in <xref rid="fig4" ref-type="fig">Figure 4</xref>. We learned that confidence thresholding brings a trade-off in performance metrics, suppressing false gradable ratio but simultaneously increasing false negative. False gradable images are prone to fail the anatomical segmentation module and generate large errors and outliers in vascular feature measurement. Although this thresholding filters out some adequate quality images, it maintains the reliability of AutoMorph.</p>
      <fig position="float" id="fig4">
        <label>Figure 4.</label>
        <caption>
          <p>The confusion matrix of the grading results on EyePACS-Q test data. (a) The results before confidence thresholding; (b) the results after thresholding. The value is normalized in rows. The diagonal includes the correct classification ratio. The <italic toggle="yes">red box</italic> indicates false gradable (i.e., ungradable images are wrongly classified as gradable), and the <italic toggle="yes">green box</italic> shows the percentage of false ungradable (i.e., gradable images are wrongly categorized as ungradable). The false gradable of (b) is reduced by 76.2% compared with that of (a), but the false ungradable increases in (b).</p>
        </caption>
        <graphic xlink:href="tvst-11-7-12-f004" position="float"/>
      </fig>
      <p>The external validation is on the general-purpose diabetic retinopathy dataset (DDR) test data. As DDR includes only two categories in image quality annotation (gradable and ungradable), we first transferred the AutoMorph prediction of good and usable quality as gradable and reject quality as ungradable and then evaluated the quantitative results. Although the difference in the annotation might underestimate the AutoMorph image quality grading capability, the performance was satisfactory compared to the internal group, as shown in <xref rid="tbl2" ref-type="table">Table 2</xref>. The confusion matrix and AUC-ROC curve are shown in <xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">Supplementary Figure S5</xref>. All ungradable images were correctly identified, which is significant with regard to the reliability of AutoMorph.</p>
    </sec>
    <sec id="sec3-2">
      <title>Anatomical Segmentation</title>
      <p>Visualization results are presented in <xref rid="fig5" ref-type="fig">Figure 5</xref>, and quantitative results are listed in <xref rid="tbl2" ref-type="table">Table 2</xref>. For binary vessel segmentation, the two public datasets AV-WIDE and the diabetic retinopathy, hypertension, age-related macular degeneration, and glaucoma image set (DR HAGIS) are employed in model validation. The binary vessel segmentation model works comparably to SOTA performance on the fundus photography data (DR HAGIS) and moderately so on ultra-widefield data (AV-WIDE). For artery/vein segmentation, the performance is validated on the IOSTAR-AV dataset. Compared with the most recent method,<xref rid="bib58" ref-type="bibr"><sup>58</sup></xref> AutoMorph achieves lower sensitivity but much higher specificity. The visualization results of two challenging cases from Moorfields Eye Hospital and the Online Retinal Fundus Image Dataset for Glaucoma Analysis and Research (ORIGA) are shown in <xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">Supplementary Figure S6</xref>. For optic disc segmentation, we validated the performance on the dataset Indian Diabetic Retinopathy Image Dataset (IDRID). The performance is on the par with the compared method,<xref rid="bib59" ref-type="bibr"><sup>59</sup></xref> and the <italic toggle="yes">F</italic><sub>1</sub>-score is slightly higher. Although pathology disturbs, the segmentation disc shows robustness.</p>
      <fig position="float" id="fig5">
        <label>Figure 5.</label>
        <caption>
          <p>Visualization results of anatomical segmentation, including binary vessel (first two columns), artery/vein (third column), and optic disc (final column).</p>
        </caption>
        <graphic xlink:href="tvst-11-7-12-f005" position="float"/>
      </fig>
    </sec>
    <sec id="sec3-3">
      <title>Vascular Feature Measurement</title>
      <p>The ICCs between AutoMorph features and expert features are listed in <xref rid="tbl3" ref-type="table">Table 3</xref>. For binary vessel morphology, the fractal dimension, vessel density, and average width metrics all achieve excellent reliability (ICC &gt; 0.9). The other metrics show good consistency. Bland–Altman plots for Zone B are shown in <xref rid="fig6" ref-type="fig">Figure 6</xref>. All features show agreement. For the fractal dimension, the mean difference (MD) is –0.01, with 95% limits of agreement (LOA) of –0.05 to 0.03; for vessel density, the MD is 0.001, with 95% LOA of 0 to 0.002; for the average width, the MD is 1.32 pixels, with 95% LOA of 0.44 to 2.19; for distance tortuosity, the MD is 0.02, with 95% LOA of –2.18 to 2.22; for squared curvature tortuosity, the MD is –1.02, with 95% LOA of –14.59 to 12.56; for tortuosity density, the MD is 0.02, with 95% LOA of –0.09 to 0.13; for CRAE Hubbard, the MD is –0.13, with 95% LOA of –2.49 to 2.24; for CRVE Hubbard, the MD is 0, with 95% LOA of –2.9 to 2.9; and for AVR Hubbard, the MD is –0.03, with 95% LOA of –0.17 to 0.11. The results at Zone C and the whole image are provided in <xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">Supplementary Figures S7</xref> and <xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">S8</xref>. Note that for the metrics CRAE, CRVE, and average width, measurements are presented in pixels, as resolution information is unknown. Some images with large errors are listed in <xref rid="tvst-11-7-12_s001" ref-type="supplementary-material">Supplementary Figure S12</xref>.</p>
      <table-wrap position="float" content-type="4col" id="tbl3">
        <label>Table 3.</label>
        <caption>
          <p>Agreement Calculation of Measured Vascular Features Between AutoMorph and Expert Annotation</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col align="left" span="1"/>
            <col align="char" char="(" span="1"/>
            <col align="char" char="(" span="1"/>
            <col align="char" char="(" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th align="center" colspan="3" rowspan="1">ICC (95% Confidence Interval)</th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th align="center" rowspan="1" colspan="1">Zone B</th>
              <th align="center" rowspan="1" colspan="1">Zone C</th>
              <th align="center" rowspan="1" colspan="1">Whole Image</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">DR HAGIS</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> Fractal dimension</td>
              <td rowspan="1" colspan="1">0.94 (0.88–0.97)</td>
              <td rowspan="1" colspan="1">0.98 (0.95–0.99)</td>
              <td rowspan="1" colspan="1">0.94 (0.88–0.97)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> Vessel density</td>
              <td rowspan="1" colspan="1">0.98 (0.96–0.99)</td>
              <td rowspan="1" colspan="1">0.97 (0.94–0.99)</td>
              <td rowspan="1" colspan="1">0.94 (0.88–0.97)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> Average width</td>
              <td rowspan="1" colspan="1">0.95 (0.89–0.98)</td>
              <td rowspan="1" colspan="1">0.96 (0.93–0.98)</td>
              <td rowspan="1" colspan="1">0.97 (0.95–0.99)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> Distance tortuosity</td>
              <td rowspan="1" colspan="1">0.80 (0.59–0.91)</td>
              <td rowspan="1" colspan="1">0.85 (0.69–0.93)</td>
              <td rowspan="1" colspan="1">0.86 (0.73–0.93)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> Squared curvature tortuosity</td>
              <td rowspan="1" colspan="1">0.68 (0.34–0.85)</td>
              <td rowspan="1" colspan="1">0.88 (0.75–0.94)</td>
              <td rowspan="1" colspan="1">0.84 (0.68–0.92)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> Tortuosity density</td>
              <td rowspan="1" colspan="1">0.89 (0.77–0.95)</td>
              <td rowspan="1" colspan="1">0.70 (0.38–0.86)</td>
              <td rowspan="1" colspan="1">0.87 (0.74–0.93)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">IOSTAR-AV</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> CRAE (Hubbard)</td>
              <td rowspan="1" colspan="1">0.81 (0.56–0.92)</td>
              <td rowspan="1" colspan="1">0.82 (0.57–0.91)</td>
              <td align="center" rowspan="1" colspan="1">—</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> CRVE (Hubbard)</td>
              <td rowspan="1" colspan="1">0.8 (0.54–0.91)</td>
              <td rowspan="1" colspan="1">0.78 (0.52–0.89)</td>
              <td align="center" rowspan="1" colspan="1">—</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> AVR (Hubbard)</td>
              <td rowspan="1" colspan="1">0.87 (0.69–0.94)</td>
              <td rowspan="1" colspan="1">0.81 (0.66–0.92)</td>
              <td align="center" rowspan="1" colspan="1">—</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> CRAE (Knudtson)</td>
              <td rowspan="1" colspan="1">0.76 (0.45–0.9)</td>
              <td rowspan="1" colspan="1">0.75 (0.44–0.89)</td>
              <td align="center" rowspan="1" colspan="1">—</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> CRVE (Knudtson)</td>
              <td rowspan="1" colspan="1">0.85 (0.67–0.94)</td>
              <td rowspan="1" colspan="1">0.86 (0.58–0.9)</td>
              <td align="center" rowspan="1" colspan="1">—</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"> AVR (Knudtson)</td>
              <td rowspan="1" colspan="1">0.85 (0.66–0.94)</td>
              <td rowspan="1" colspan="1">0.82 (0.51–0.91)</td>
              <td align="center" rowspan="1" colspan="1">—</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tb3fn1">
            <p>The agreement of vessel caliber was validated on the IOSTAR-AV dataset, other metrics with the DR HAGIS dataset. Because caliber features rely on the six largest arteries and veins in Zones B and C, there is no caliber feature for the whole image level.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <fig position="float" id="fig6">
        <label>Figure 6.</label>
        <caption>
          <p>Bland–Altman plots of vascular feature agreement between expert annotation and AutoMorph segmentation at Zone B. The first two row features (e.g., tortuosity, fractal dimension) were calculated with the binary vessel segmentation map from DR HAGIS; the last row features (caliber) were measured with the artery/vein segmentation map from IOSTAR-AV. In each subplot, the <italic toggle="yes">central line</italic> indicates the mean difference and two <italic toggle="yes">dash</italic><italic toggle="yes">ed</italic>
<italic toggle="yes">lines</italic> represent 95% limits of agreement. The unit of average width, CRAE, and CRVE is the pixel, as resolution was unknown.</p>
        </caption>
        <graphic xlink:href="tvst-11-7-12-f006" position="float"/>
      </fig>
    </sec>
    <sec id="sec3-4">
      <title>Running Efficiency and Interface</title>
      <p>The average running time for one image is about 20 seconds using a single graphics processing unit (GPU) Tesla T4 graphic card, from preprocessing to feature measurement. To ensure accessibility for researchers without coding experience, we have made AutoMorph compatible with Google Colaboratory (free GPU) (<xref rid="fig7" ref-type="fig">Fig. 7</xref>). The process involves placing images in a specified folder and then clicking the “run” command. All results will be stored, including segmentation maps and a file containing all measured features.</p>
      <fig position="float" id="fig7">
        <label>Figure 7.</label>
        <caption>
          <p>Interface of AutoMorph on Google Colaboratory. After uploading images and clicking the “run” button, all processes are executed and results stored, requiring no human intervention. The <italic toggle="yes">left side</italic> shows the files directory, and the <italic toggle="yes">right bottom</italic> lists five examples with parts of features.</p>
        </caption>
        <graphic xlink:href="tvst-11-7-12-f007" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion" id="sec4">
    <title>Discussion</title>
    <p>In this report, the four functional modules of the AutoMorph pipeline achieved comparable, or better, performance compared with the state of the art for both image quality grading and anatomical segmentation. Furthermore, our approach to confidence analysis decreased the number of false gradable images by 76%, greatly enhancing the reliability of our pipeline. Hence, we have learned that, by using a tailored combination of deep learning techniques, it is practical to accurately analyze the retinal vascular morphology in a fully automated way. Although we have evaluated the binary vessel segmentation model on the ultra-widefield retinal fundus dataset AV-WIDE, we recommend using AutoMorph on retinal fundus photographs with a 25° to 60° field of view (FOV), as all of the deep learning models are trained using images with FOVs equal to 25° to 60°, and the preprocessing step is tailored for images with this FOV.</p>
    <p>AutoMorph maintains computation transparency despite the use of deep learning techniques. Recently, similar systems have used deep learning models to skip intermediary steps and instead directly predict morphology features. For example, the Singapore I vessel assessment (SIVA) deep learning system (DLS) predicts vessel caliber from retinal fundus images without optic disc localization or artery/vein segmentation.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref> Another work directly predicts CVD factors from retinal fundus images in an end-to-end manner.<xref rid="bib61" ref-type="bibr"><sup>61</sup></xref> Although these designs provide some insight into the applications of deep learning to ophthalmology, the end-to-end pipeline sacrifices transparency and raises interpretability concerns, representing a potential barrier to clinical implementation.<xref rid="bib62" ref-type="bibr"><sup>62</sup></xref><sup>,</sup><xref rid="bib63" ref-type="bibr"><sup>63</sup></xref> Specifically, considering that some formulas are empirically defined (e.g., CRAE and CRVE are calculated based on the six widest arteries and veins), it is difficult to verify whether a model can learn this type of derivation. In contrast, the AutoMorph pipeline maintains transparency, as the individual processes can be decomposed. Models are initially employed for anatomical segmentation before vascular features are measured with traditional formulas. This process is consistent with the typical pipeline of human computation, thus improving the credibility of feature measurements.</p>
    <p>The study cohort is selected by the image quality grading module. In this work, being different from previous work with only good-quality images, we tried to explore the effectiveness of usable images. Although purely including good-quality images can avoid potentially challenging cases for anatomic segmentation models (e.g., images with gloomy illumination), it filters out usable images that can contribute to a more general conclusion with a larger study cohort. Also, in clinical practice, a considerable number of images are usable quality but may not qualify as perfectly good quality. The pipeline developed in an environment similar to clinical reality is more prone to be deployed in the clinic. In image quality grading, the confidence analysis has recognized a considerable proportion of false gradable images and corrected them as reject quality by thresholding, as shown in <xref rid="fig3" ref-type="fig">Figures 3</xref> and <xref rid="fig4" ref-type="fig">4</xref>. This avoids some reject quality images failing the anatomical segmentation and then generating large errors in feature measurement. Although this thresholding increased the false ungradable cases (<xref rid="fig4" ref-type="fig">Fig. 4</xref>b, green box), the priority of recognizing the false gradable images is secured. Of course, it is acceptable to include only the good-quality images in the research cohorts, the same as previous work, when the quantity of good-quality images is large.</p>
    <p>Although this work demonstrates the effectiveness of a deep learning pipeline for analyzing retinal vascular morphology, there are some challenges remaining regarding technique and standardization. First, annotating retinal image quality is subjective and lacks strict guidelines; therefore, it is difficult to benchmark external validation performance. Second, there is still room for improving anatomical segmentation, especially for artery/vein segmentation. Third, considering that the agreement varies across various vascular features (<xref rid="tbl3" ref-type="table">Table 3</xref>), it is necessary to compare the robustness of these features and understand the pros and cons of each one. Finally, a uniform protocol for validating retinal analysis pipelines is required, because existing software (e.g., RA<xref rid="bib28" ref-type="bibr"><sup>28</sup></xref>, IVAN,<xref rid="bib6" ref-type="bibr"><sup>6</sup></xref> SIVA,<xref rid="bib29" ref-type="bibr"><sup>29</sup></xref> VAMPIRE<xref rid="bib25" ref-type="bibr"><sup>25</sup></xref>) shows high variation in feature measurement.<xref rid="bib64" ref-type="bibr"><sup>64</sup></xref><sup>,</sup><xref rid="bib65" ref-type="bibr"><sup>65</sup></xref> These four challenges exist in the field of oculomics, presenting an impediment to more extensive research.</p>
    <p>We have made AutoMorph publicly available to benefit research in the field of oculomics, which studies the association between ocular biomarkers and systemic disease. We designed the AutoMorph interface using Google Colaboratory to facilitate its use by clinicians without coding experience. In future work, we will investigate solutions dedicated to the above challenges in oculomics research. Also, the feasibility of automatic pipeline can be extended to other modalities, such as optical coherence tomography (OCT) and OCT angiography.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="tvst-11-7-12_s001" position="float" content-type="local-data">
      <caption>
        <title>Supplement 1</title>
      </caption>
      <media xlink:href="tvst-11-7-12_s001.docx" id="d64e1846" position="anchor"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgments</title>
    <p>Supported by grants from the Engineering and Physical Sciences Research Council (EP/M020533/1, EP/R014019/1, and EP/V034537/1); by the National Institute for Health and Care Research Biomedical Research Centre; by an MRC Clinical Research Training Fellowship (MR/TR000953/1 to SKW); by a Moorfields Eye Charity Career Development Award (R190028A to PAK); and by a UK Research &amp; Innovation Future Leaders Fellowship (MR/T019050/1 to PAK).</p>
    <p content-type="COI-statement">Disclosure: <bold>Y. Zhou</bold>, None; <bold>S.K. Wagner</bold>, None; <bold>M.A. Chia</bold>, None; <bold>A. Zhao</bold>, None; <bold>P. Woodward-Court</bold>, None; <bold>M. Xu</bold>, None; <bold>R. Struyven</bold>, None; <bold>D.C. Alexander</bold>, None; <bold>P.A. Keane</bold>, DeepMind (C), Roche (C), Novartis (C), Apellis (C), BitFount (C), Big Picture Medical (I), Heidelberg Engineering (F), Topcon (F), Allergan (F), Bayer (F)</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="bib1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wagner</surname><given-names>SK</given-names></string-name>, <string-name><surname>Fu</surname><given-names>DJ</given-names></string-name>, <string-name><surname>Faes</surname><given-names>L</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>Insights into systemic disease through retinal imaging-based oculomics</article-title>. <source><italic toggle="yes">Transl Vis Sci Technol</italic></source>. <year>2020</year>; <volume>9</volume>: <fpage>6</fpage>.</mixed-citation>
    </ref>
    <ref id="bib2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rizzoni</surname><given-names>D</given-names></string-name>, <string-name><surname>Muiesan</surname><given-names>ML</given-names></string-name></person-group>. <article-title>Retinal vascular caliber and the development of hypertension: a meta-analysis of individual participant data</article-title>. <source><italic toggle="yes">J Hypertens</italic></source>. <year>2014</year>; <volume>32</volume>: <fpage>225</fpage>–<lpage>227</lpage>.<pub-id pub-id-type="pmid">24430117</pub-id></mixed-citation>
    </ref>
    <ref id="bib3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cheung</surname><given-names>CY</given-names></string-name>, <string-name><surname>Xu</surname><given-names>D</given-names></string-name>, <string-name><surname>Cheng</surname><given-names>C-Y</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>A deep-learning system for the assessment of cardiovascular disease risk via the measurement of retinal-vessel calibre</article-title>. <source><italic toggle="yes">Nat Biomed Eng</italic></source>. <year>2021</year>; <volume>5</volume>: <fpage>498</fpage>–<lpage>508</lpage>.<pub-id pub-id-type="pmid">33046867</pub-id></mixed-citation>
    </ref>
    <ref id="bib4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wong</surname><given-names>TY</given-names></string-name>, <string-name><surname>Mitchell</surname><given-names>P</given-names></string-name></person-group>. <article-title>Hypertensive retinopathy</article-title>. <source><italic toggle="yes">N Engl J Med</italic></source>. <year>2004</year>; <volume>351</volume>: <fpage>2310</fpage>–<lpage>2317</lpage>.<pub-id pub-id-type="pmid">15564546</pub-id></mixed-citation>
    </ref>
    <ref id="bib5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cheung</surname><given-names>N</given-names></string-name>, <string-name><surname>Bluemke</surname><given-names>DA</given-names></string-name>, <string-name><surname>Klein</surname><given-names>R</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>Retinal arteriolar narrowing and left ventricular remodeling: the multi-ethnic study of atherosclerosis</article-title>. <source><italic toggle="yes">J Am Coll Cardiol</italic></source>. <year>2007</year>; <volume>50</volume>: <fpage>48</fpage>–<lpage>55</lpage>.<pub-id pub-id-type="pmid">17601545</pub-id></mixed-citation>
    </ref>
    <ref id="bib6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wong</surname><given-names>TY</given-names></string-name>, <string-name><surname>Islam</surname><given-names>FMA</given-names></string-name>, <string-name><surname>Klein</surname><given-names>R</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>Retinal vascular caliber, cardiovascular risk factors, and inflammation: the Multi-Ethnic Study of Atherosclerosis (MESA)</article-title>. <source><italic toggle="yes">Invest Ophthalmol Vis Sci</italic></source>. <year>2006</year>; <volume>47</volume>: <fpage>2341</fpage>.<pub-id pub-id-type="pmid">16723443</pub-id></mixed-citation>
    </ref>
    <ref id="bib7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wong</surname><given-names>TY</given-names></string-name>, <string-name><surname>Klein</surname><given-names>R</given-names></string-name>, <string-name><surname>Sharrett</surname><given-names>AR</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>Retinal arteriolar diameter and risk for hypertension</article-title>. <source><italic toggle="yes">Ann Intern Med</italic></source>. <year>2004</year>; <volume>140</volume>: <fpage>248</fpage>–<lpage>255</lpage>.<pub-id pub-id-type="pmid">14970147</pub-id></mixed-citation>
    </ref>
    <ref id="bib8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wong</surname><given-names>TY</given-names></string-name>, <string-name><surname>Shankar</surname><given-names>A</given-names></string-name>, <string-name><surname>Klein</surname><given-names>R</given-names></string-name>, <string-name><surname>Klein</surname><given-names>BEK</given-names></string-name>, <string-name><surname>Hubbard</surname><given-names>LD</given-names></string-name></person-group>. <article-title>Prospective cohort study of retinal vessel diameters and risk of hypertension</article-title>. <source><italic toggle="yes">BMJ</italic></source>. <year>2004</year>; <volume>329</volume>: <fpage>79</fpage>.<pub-id pub-id-type="pmid">15175230</pub-id></mixed-citation>
    </ref>
    <ref id="bib9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jaulim</surname><given-names>A</given-names></string-name>, <string-name><surname>Ahmed</surname><given-names>B</given-names></string-name>, <string-name><surname>Khanam</surname><given-names>T</given-names></string-name>, <string-name><surname>Chatziralli</surname><given-names>IP</given-names></string-name></person-group>. <article-title>Branch retinal vein occlusion: epidemiology, pathogenesis, risk factors, clinical features, diagnosis, and complications. An update of the literature</article-title>. <source><italic toggle="yes">Retina</italic></source>. <year>2013</year>; <volume>33</volume>: <fpage>901</fpage>–<lpage>910</lpage>.<pub-id pub-id-type="pmid">23609064</pub-id></mixed-citation>
    </ref>
    <ref id="bib10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yau</surname><given-names>JWY</given-names></string-name>, <string-name><surname>Lee</surname><given-names>P</given-names></string-name>, <string-name><surname>Wong</surname><given-names>TY</given-names></string-name>, <string-name><surname>Best</surname><given-names>J</given-names></string-name>, <string-name><surname>Jenkins</surname><given-names>A</given-names></string-name></person-group>. <article-title>Retinal vein occlusion: an approach to diagnosis, systemic risk factors and management</article-title>. <source><italic toggle="yes">Intern Med J</italic></source>. <year>2008</year>; <volume>38</volume>: <fpage>904</fpage>–<lpage>910</lpage>.<pub-id pub-id-type="pmid">19120547</pub-id></mixed-citation>
    </ref>
    <ref id="bib11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wong</surname><given-names>TY</given-names></string-name></person-group>. <article-title>Retinal vessel diameter as a clinical predictor of diabetic retinopathy progression: time to take out the measuring tape</article-title>. <source><italic toggle="yes">Arch Ophthalmol</italic></source>. <year>2011</year>; <volume>129</volume>: <fpage>95</fpage>–<lpage>96</lpage>.<pub-id pub-id-type="pmid">21220635</pub-id></mixed-citation>
    </ref>
    <ref id="bib12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Owen</surname><given-names>CG</given-names></string-name>, <string-name><surname>Rudnicka</surname><given-names>AR</given-names></string-name>, <string-name><surname>Nightingale</surname><given-names>CM</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>Retinal arteriolar tortuosity and cardiovascular risk factors in a multi-ethnic population study of 10-year-old children; the Child Heart and Health Study in England (CHASE)</article-title>. <source><italic toggle="yes">Arterioscler Thromb Vasc Biol</italic></source>. <year>2011</year>; <volume>31</volume>: <fpage>1933</fpage>–<lpage>1938</lpage>.<pub-id pub-id-type="pmid">21659645</pub-id></mixed-citation>
    </ref>
    <ref id="bib13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cheung</surname><given-names>CY-L</given-names></string-name>, <string-name><surname>Zheng</surname><given-names>Y</given-names></string-name>, <string-name><surname>Hsu</surname><given-names>W</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>Retinal vascular tortuosity, blood pressure, and cardiovascular risk factors</article-title>. <source><italic toggle="yes">Ophthalmology</italic></source>. <year>2011</year>; <volume>118</volume>: <fpage>812</fpage>–<lpage>818</lpage>.<pub-id pub-id-type="pmid">21146228</pub-id></mixed-citation>
    </ref>
    <ref id="bib14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Owen</surname><given-names>CG</given-names></string-name>, <string-name><surname>Rudnicka</surname><given-names>AR</given-names></string-name>, <string-name><surname>Mullen</surname><given-names>R</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>Measuring retinal vessel tortuosity in 10-year-old children: validation of the Computer-Assisted Image Analysis of the Retina (CAIAR) program</article-title>. <source><italic toggle="yes">Invest Ophthalmol Vis Sci</italic></source>. <year>2009</year>; <volume>50</volume>: <fpage>2004</fpage>–<lpage>2010</lpage>.<pub-id pub-id-type="pmid">19324866</pub-id></mixed-citation>
    </ref>
    <ref id="bib15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Couper</surname><given-names>DJ</given-names></string-name>, <string-name><surname>Klein</surname><given-names>R</given-names></string-name>, <string-name><surname>Hubbard</surname><given-names>LD</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>Reliability of retinal photography in the assessment of retinal microvascular characteristics: the Atherosclerosis Risk in Communities Study</article-title>. <source><italic toggle="yes">Am J Ophthalmol</italic></source>. <year>2002</year>; <volume>133</volume>: <fpage>78</fpage>–<lpage>88</lpage>.<pub-id pub-id-type="pmid">11755842</pub-id></mixed-citation>
    </ref>
    <ref id="bib16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname><given-names>F</given-names></string-name>, <string-name><surname>Dashtbozorg</surname><given-names>B</given-names></string-name>, <string-name><surname>ter Haar Romeny</surname><given-names>BM</given-names></string-name></person-group>. <article-title>Artery/vein classification using reflection features in retina fundus images</article-title>. <source><italic toggle="yes">Mach Vis Appl</italic></source>. <year>2018</year>; <volume>29</volume>: <fpage>23</fpage>–<lpage>34</lpage>.</mixed-citation>
    </ref>
    <ref id="bib17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mirsharif</surname><given-names>Q</given-names></string-name>, <string-name><surname>Tajeripour</surname><given-names>F</given-names></string-name>, <string-name><surname>Pourreza</surname><given-names>H</given-names></string-name></person-group>. <article-title>Automated characterization of blood vessels as arteries and veins in retinal images</article-title>. <source><italic toggle="yes">Comput Med Imaging Graph</italic></source>. <year>2013</year>; <volume>37</volume>: <fpage>607</fpage>–<lpage>617</lpage>.<pub-id pub-id-type="pmid">23849699</pub-id></mixed-citation>
    </ref>
    <ref id="bib18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dashtbozorg</surname><given-names>B</given-names></string-name>, <string-name><surname>Mendonça</surname><given-names>AM</given-names></string-name>, <string-name><surname>Campilho</surname><given-names>A</given-names></string-name></person-group>. <article-title>An automatic graph-based approach for artery/vein classification in retinal images</article-title>. <source><italic toggle="yes">IEEE Trans Image Process</italic></source>. <year>2014</year>; <volume>23</volume>: <fpage>1073</fpage>–<lpage>1083</lpage>.<pub-id pub-id-type="pmid">23693131</pub-id></mixed-citation>
    </ref>
    <ref id="bib19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Estrada</surname><given-names>R</given-names></string-name>, <string-name><surname>Allingham</surname><given-names>MJ</given-names></string-name>, <string-name><surname>Mettu</surname><given-names>PS</given-names></string-name>, <string-name><surname>Cousins</surname><given-names>SW</given-names></string-name>, <string-name><surname>Tomasi</surname><given-names>C</given-names></string-name>, <string-name><surname>Farsiu</surname><given-names>S</given-names></string-name></person-group>. <article-title>Retinal artery-vein classification via topology estimation</article-title>. <source><italic toggle="yes">IEEE Trans Med Imaging</italic></source>. <year>2015</year>; <volume>34</volume>: <fpage>2518</fpage>–<lpage>2534</lpage>.<pub-id pub-id-type="pmid">26068204</pub-id></mixed-citation>
    </ref>
    <ref id="bib20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Srinidhi</surname><given-names>CL</given-names></string-name>, <string-name><surname>Aparna</surname><given-names>P</given-names></string-name>, <string-name><surname>Rajan</surname><given-names>J</given-names></string-name></person-group>. <article-title>Automated method for retinal artery/vein separation via graph search metaheuristic approach [published online ahead of print January 1, 2019]</article-title>. <source><italic toggle="yes">IEEE Trans Image Process</italic></source>, <pub-id pub-id-type="doi">10.1109/TIP.2018.2889534</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib21">
      <label>21.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ronneberger</surname><given-names>O</given-names></string-name>, <string-name><surname>Fischer</surname><given-names>P</given-names></string-name>, <string-name><surname>Brox</surname><given-names>T</given-names></string-name></person-group>. <article-title>U-Net: convolutional networks for biomedical image segmentation</article-title>. In: <person-group person-group-type="editor"><string-name><surname>Navab</surname><given-names>N</given-names></string-name>, <string-name><surname>Hornegger</surname><given-names>J</given-names></string-name>, <string-name><surname>Wells</surname><given-names>WM</given-names></string-name>, <string-name><surname>Frangi</surname><given-names>A</given-names></string-name></person-group>, eds. <source><italic toggle="yes">Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015</italic></source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>; <year>2015</year>: <fpage>234</fpage>–<lpage>241</lpage>.</mixed-citation>
    </ref>
    <ref id="bib22">
      <label>22.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Zhou</surname><given-names>Y</given-names></string-name>, <string-name><surname>Xu</surname><given-names>M</given-names></string-name>, <string-name><surname>Hu</surname><given-names>Y</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>Learning to address intra-segment misclassification in retinal imaging</article-title>. In: <person-group person-group-type="editor"><string-name><surname>de Bruijne</surname><given-names>M</given-names></string-name>, <string-name><surname>Cattin</surname><given-names>PC</given-names></string-name>, <string-name><surname>Cotin</surname><given-names>S</given-names></string-name>, <etal>et al</etal>.</person-group>, eds. <source><italic toggle="yes">Medical Image Computing and Computer Assisted Intervention–MICCAI 2021</italic></source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>; <year>2021</year>: <fpage>482</fpage>–<lpage>492</lpage>.</mixed-citation>
    </ref>
    <ref id="bib23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname><given-names>Y</given-names></string-name>, <string-name><surname>Chen</surname><given-names>Z</given-names></string-name>, <string-name><surname>Shen</surname><given-names>H</given-names></string-name>, <string-name><surname>Zheng</surname><given-names>X</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>R</given-names></string-name>, <string-name><surname>Duan</surname><given-names>X</given-names></string-name></person-group>. <article-title>A refined equilibrium generative adversarial network for retinal vessel segmentation</article-title>. <source><italic toggle="yes">Neurocomputing</italic></source>. <year>2021</year>; <volume>437</volume>: <fpage>118</fpage>–<lpage>130</lpage>.</mixed-citation>
    </ref>
    <ref id="bib24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fraz</surname><given-names>MM</given-names></string-name>, <string-name><surname>Welikala</surname><given-names>RA</given-names></string-name>, <string-name><surname>Rudnicka</surname><given-names>AR</given-names></string-name>, <string-name><surname>Owen</surname><given-names>CG</given-names></string-name>, <string-name><surname>Strachan</surname><given-names>DP</given-names></string-name>, <string-name><surname>Barman</surname><given-names>SA</given-names></string-name></person-group>. <article-title>QUARTZ: quantitative analysis of retinal vessel topology and size – an automated system for quantification of retinal vessels morphology</article-title>. <source><italic toggle="yes">Expert Syst Appl</italic></source>. <year>2015</year>; <volume>42</volume>: <fpage>7221</fpage>–<lpage>7234</lpage>.</mixed-citation>
    </ref>
    <ref id="bib25">
      <label>25.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Perez-Rovira</surname><given-names>A</given-names></string-name>, <string-name><surname>MacGillivray</surname><given-names>T</given-names></string-name>, <string-name><surname>Trucco</surname><given-names>E</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>VAMPIRE: vessel assessment and measurement platform for images of the retina</article-title>. In: <source><italic toggle="yes">Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</italic></source>. <publisher-loc>Piscataway, NJ</publisher-loc>: <publisher-name>Institute of Electrical and Electronics Engineers</publisher-name>; <year>2011</year>: <fpage>3391</fpage>–<lpage>3394</lpage>.</mixed-citation>
    </ref>
    <ref id="bib26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Futoma</surname><given-names>J</given-names></string-name>, <string-name><surname>Simons</surname><given-names>M</given-names></string-name>, <string-name><surname>Panch</surname><given-names>T</given-names></string-name>, <string-name><surname>Doshi-Velez</surname><given-names>F</given-names></string-name>, <string-name><surname>Celi</surname><given-names>LA</given-names></string-name></person-group>. <article-title>The myth of generalisability in clinical research and machine learning in health care</article-title>. <source><italic toggle="yes">Lancet Digit Health</italic></source>. <year>2020</year>; <volume>2</volume>: <fpage>e489</fpage>–<lpage>e492</lpage>.<pub-id pub-id-type="pmid">32864600</pub-id></mixed-citation>
    </ref>
    <ref id="bib27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mårtensson</surname><given-names>G</given-names></string-name>, <string-name><surname>Ferreira</surname><given-names>D</given-names></string-name>, <string-name><surname>Granberg</surname><given-names>T</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>The reliability of a deep learning model in clinical out-of-distribution MRI data: a multicohort study</article-title>. <source><italic toggle="yes">Med Image Anal</italic></source>. <year>2020</year>; <volume>66</volume>: <fpage>101714</fpage>.<pub-id pub-id-type="pmid">33007638</pub-id></mixed-citation>
    </ref>
    <ref id="bib28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wong</surname><given-names>TY</given-names></string-name>, <string-name><surname>Shankar</surname><given-names>A</given-names></string-name>, <string-name><surname>Klein</surname><given-names>R</given-names></string-name>, <string-name><surname>Klein</surname><given-names>BEK</given-names></string-name></person-group>. <article-title>Retinal vessel diameters and the incidence of gross proteinuria and renal insufficiency in people with type 1 diabetes</article-title>. <source><italic toggle="yes">Diabetes</italic></source>. <year>2004</year>; <volume>53</volume>: <fpage>179</fpage>–<lpage>184</lpage>.<pub-id pub-id-type="pmid">14693713</pub-id></mixed-citation>
    </ref>
    <ref id="bib29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cheung</surname><given-names>CY</given-names></string-name>, <string-name><surname>Tay</surname><given-names>WT</given-names></string-name>, <string-name><surname>Mitchell</surname><given-names>P</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>Quantitative and qualitative retinal microvascular characteristics and blood pressure</article-title>. <source><italic toggle="yes">J Hypertens</italic></source>. <year>2011</year>; <volume>29</volume>: <fpage>1380</fpage>–<lpage>1391</lpage>.<pub-id pub-id-type="pmid">21558958</pub-id></mixed-citation>
    </ref>
    <ref id="bib30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khan</surname><given-names>SM</given-names></string-name>, <string-name><surname>Liu</surname><given-names>X</given-names></string-name>, <string-name><surname>Nath</surname><given-names>S</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>A global review of publicly available datasets for ophthalmological imaging: barriers to access, usability, and generalisability</article-title>. <source><italic toggle="yes">Lancet Digit Health</italic></source>. <year>2021</year>; <volume>3</volume>: <fpage>e51</fpage>–<lpage>e66</lpage>.<pub-id pub-id-type="pmid">33735069</pub-id></mixed-citation>
    </ref>
    <ref id="bib31">
      <label>31.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Fu</surname><given-names>H</given-names></string-name>, <string-name><surname>Wang</surname><given-names>B</given-names></string-name>, <string-name><surname>Shen</surname><given-names>J</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>Evaluation of retinal image quality assessment networks in different color-spaces</article-title>. In: <source><italic toggle="yes">International Conference on Medical Image Computing and Computer-Assisted Intervention</italic></source>. <publisher-loc>Cham</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2019</year>: <fpage>48</fpage>–<lpage>56</lpage>.</mixed-citation>
    </ref>
    <ref id="bib32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>T</given-names></string-name>, <string-name><surname>Gao</surname><given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname><given-names>K</given-names></string-name>, <string-name><surname>Guo</surname><given-names>S</given-names></string-name>, <string-name><surname>Liu</surname><given-names>H</given-names></string-name>, <string-name><surname>Kang</surname><given-names>H</given-names></string-name></person-group>. <article-title>Diagnostic assessment of deep learning algorithms for diabetic retinopathy screening</article-title>. <source><italic toggle="yes">Inform Sci</italic></source>. <year>2019</year>; <volume>501</volume>: <fpage>511</fpage>–<lpage>522</lpage>.</mixed-citation>
    </ref>
    <ref id="bib33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Staal</surname><given-names>J</given-names></string-name>, <string-name><surname>Abramoff</surname><given-names>MD</given-names></string-name>, <string-name><surname>Niemeijer</surname><given-names>M</given-names></string-name>, <string-name><surname>Viergever</surname><given-names>MA</given-names></string-name>, <string-name><surname>van Ginneken</surname><given-names>B</given-names></string-name></person-group>. <article-title>Ridge-based vessel segmentation in color images of the retina</article-title>. <source><italic toggle="yes">IEEE Trans Med Imaging</italic></source>. <year>2004</year>; <volume>23</volume>: <fpage>501</fpage>–<lpage>509</lpage>.<pub-id pub-id-type="pmid">15084075</pub-id></mixed-citation>
    </ref>
    <ref id="bib34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoover</surname><given-names>A</given-names></string-name>, <string-name><surname>Kouznetsova</surname><given-names>V</given-names></string-name>, <string-name><surname>Goldbaum</surname><given-names>M</given-names></string-name></person-group>. <article-title>Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response</article-title>. <source><italic toggle="yes">IEEE Trans Med Imaging</italic></source>. <year>2000</year>; <volume>19</volume>: <fpage>203</fpage>–<lpage>210</lpage>.<pub-id pub-id-type="pmid">10875704</pub-id></mixed-citation>
    </ref>
    <ref id="bib35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fraz</surname><given-names>MM</given-names></string-name>, <string-name><surname>Remagnino</surname><given-names>P</given-names></string-name>, <string-name><surname>Hoppe</surname><given-names>A</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>An ensemble classification-based approach applied to retinal blood vessel segmentation</article-title>. <source><italic toggle="yes">IEEE Trans Biomed Eng</italic></source>. <year>2012</year>; <volume>59</volume>: <fpage>2538</fpage>–<lpage>2548</lpage>.<pub-id pub-id-type="pmid">22736688</pub-id></mixed-citation>
    </ref>
    <ref id="bib36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Budai</surname><given-names>A</given-names></string-name>, <string-name><surname>Bock</surname><given-names>R</given-names></string-name>, <string-name><surname>Maier</surname><given-names>A</given-names></string-name>, <string-name><surname>Hornegger</surname><given-names>J</given-names></string-name>, <string-name><surname>Michelson</surname><given-names>G</given-names></string-name></person-group>. <article-title>Robust vessel segmentation in fundus images</article-title>. <source><italic toggle="yes">Int J Biomed Imaging</italic></source>. <year>2013</year>; <volume>2013</volume>: <fpage>154860</fpage>.<pub-id pub-id-type="pmid">24416040</pub-id></mixed-citation>
    </ref>
    <ref id="bib37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>J</given-names></string-name>, <string-name><surname>Dashtbozorg</surname><given-names>B</given-names></string-name>, <string-name><surname>Bekkers</surname><given-names>E</given-names></string-name>, <string-name><surname>Pluim</surname><given-names>JPW</given-names></string-name>, <string-name><surname>Duits</surname><given-names>R</given-names></string-name>, <string-name><surname>Ter Haar Romeny</surname><given-names>BM</given-names></string-name></person-group>. <article-title>Robust retinal vessel segmentation via locally adaptive derivative frames in orientation scores</article-title>. <source><italic toggle="yes">IEEE Trans Med Imaging</italic></source>. <year>2016</year>; <volume>35</volume>: <fpage>2631</fpage>–<lpage>2644</lpage>.<pub-id pub-id-type="pmid">27514039</pub-id></mixed-citation>
    </ref>
    <ref id="bib38">
      <label>38.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Orlando</surname><given-names>JI</given-names></string-name>, <string-name><surname>Breda</surname><given-names>JB</given-names></string-name>, <string-name><surname>van Keer</surname><given-names>K</given-names></string-name>, <string-name><surname>Blaschko</surname><given-names>MB</given-names></string-name>, <string-name><surname>Blanco</surname><given-names>PJ</given-names></string-name>, <string-name><surname>Bulant</surname><given-names>CA</given-names></string-name></person-group>. <article-title>Towards a glaucoma risk index based on simulated hemodynamics from fundus images</article-title>. In: <source><italic toggle="yes">Medical Image Computing and Computer Assisted Intervention–MICCAI 2018</italic></source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>; <year>2018</year>: <fpage>65</fpage>–<lpage>73</lpage>.</mixed-citation>
    </ref>
    <ref id="bib39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khanal</surname><given-names>A</given-names></string-name>, <string-name><surname>Estrada</surname><given-names>R</given-names></string-name></person-group>. <article-title>Dynamic deep networks for retinal vessel segmentation</article-title>. <source><italic toggle="yes">Front Comput Sci</italic></source>. <year>2020</year>; <volume>2</volume>: <fpage>35</fpage>.</mixed-citation>
    </ref>
    <ref id="bib40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Holm</surname><given-names>S</given-names></string-name>, <string-name><surname>Russell</surname><given-names>G</given-names></string-name>, <string-name><surname>Nourrit</surname><given-names>V</given-names></string-name>, <string-name><surname>McLoughlin</surname><given-names>N</given-names></string-name></person-group>. <article-title>DR HAGIS-a fundus image database for the automatic extraction of retinal surface vessels from diabetic patients</article-title>. <source><italic toggle="yes">J Med Imaging (Bellingham)</italic></source>. <year>2017</year>; <volume>4</volume>: <fpage>014503</fpage>.<pub-id pub-id-type="pmid">28217714</pub-id></mixed-citation>
    </ref>
    <ref id="bib41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hu</surname><given-names>Q</given-names></string-name>, <string-name><surname>Abràmoff</surname><given-names>MD</given-names></string-name>, <string-name><surname>Garvin</surname><given-names>MK</given-names></string-name></person-group>. <article-title>Automated separation of binary overlapping trees in low-contrast color retinal images</article-title>. <source><italic toggle="yes">Med Image Comput Comput Assist Interv</italic></source>. <year>2013</year>; <volume>16</volume>: <fpage>436</fpage>–<lpage>443</lpage>.</mixed-citation>
    </ref>
    <ref id="bib42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hemelings</surname><given-names>R</given-names></string-name>, <string-name><surname>Elen</surname><given-names>B</given-names></string-name>, <string-name><surname>Stalmans</surname><given-names>I</given-names></string-name>, <string-name><surname>Van Keer</surname><given-names>K</given-names></string-name>, <string-name><surname>De Boever</surname><given-names>P</given-names></string-name>, <string-name><surname>Blaschko</surname><given-names>MB</given-names></string-name></person-group>. <article-title>Artery-vein segmentation in fundus images using a fully convolutional network</article-title>. <source><italic toggle="yes">Comput Med Imaging Graph</italic></source>. <year>2019</year>; <volume>76</volume>: <fpage>101636</fpage>.<pub-id pub-id-type="pmid">31288217</pub-id></mixed-citation>
    </ref>
    <ref id="bib43">
      <label>43.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Abbasi-Sureshjani</surname><given-names>S</given-names></string-name>, <string-name><surname>Smit-Ockeloen</surname><given-names>I</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>J</given-names></string-name>, <string-name><surname>Ter Haar Romeny</surname><given-names>B</given-names></string-name></person-group>. <article-title>Biologically-inspired supervised vasculature segmentation in SLO retinal fundus images</article-title>. In: <person-group person-group-type="editor"><string-name><surname>Kamel</surname><given-names>M</given-names></string-name>, <string-name><surname>Campilho</surname><given-names>A</given-names></string-name></person-group>, eds. <source>International Conference Image Analysis and Recognition</source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2015</year>: <fpage>325</fpage>–<lpage>334</lpage>.</mixed-citation>
    </ref>
    <ref id="bib44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Orlando</surname><given-names>JI</given-names></string-name>, <string-name><surname>Fu</surname><given-names>H</given-names></string-name>, <string-name><surname>Breda</surname><given-names>JB</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>REFUGE Challenge: a unified framework for evaluating automated methods for glaucoma assessment from fundus photographs</article-title>. <source><italic toggle="yes">Med Image Anal</italic></source>. <year>2020</year>; <volume>59</volume>: <fpage>101570</fpage>.<pub-id pub-id-type="pmid">31630011</pub-id></mixed-citation>
    </ref>
    <ref id="bib45">
      <label>45.</label>
      <mixed-citation publication-type="other"><collab>OMIA</collab>. <article-title>OMIA8: 8th MICCAI Workshop on Ophthalmic Medical Image Analysis</article-title>. Available at: <ext-link xlink:href="https://sites.google.com/view/omia8" ext-link-type="uri">https://sites.google.com/view/omia8</ext-link>. Accessed July 1, 2022.</mixed-citation>
    </ref>
    <ref id="bib46">
      <label>46.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Wu</surname><given-names>J</given-names></string-name>, <string-name><surname>Fang</surname><given-names>H</given-names></string-name>, <string-name><surname>Li</surname><given-names>F</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>GAMMA Challenge:Glaucoma grAding from Multi-Modality imAges</article-title>. arXiv. <year>2022</year>, <pub-id pub-id-type="doi">10.48550/arXiv.2202.06511</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib47">
      <label>47.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Porwal</surname><given-names>P</given-names></string-name>, <string-name><surname>Pachade</surname><given-names>S</given-names></string-name>, <string-name><surname>Kamble</surname><given-names>R</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>Indian Diabetic Retinopathy Image Dataset (IDRiD): a database for diabetic retinopathy screening research</article-title>. <source><italic toggle="yes">Data</italic></source>. <year>2018</year>; <volume>3</volume>: <fpage>25</fpage>.</mixed-citation>
    </ref>
    <ref id="bib48">
      <label>48.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Tan</surname><given-names>M</given-names></string-name>, <string-name><surname>Le</surname><given-names>Q</given-names></string-name></person-group>. <article-title>Efficientnet: rethinking model scaling for convolutional neural networks</article-title>. In: <person-group person-group-type="editor"><string-name><surname>Chaudhuri</surname><given-names>K</given-names></string-name>, <string-name><surname>Salakhutdinov</surname><given-names>R</given-names></string-name></person-group>, eds. <source><italic toggle="yes">Thirty-Sixth International Conference on Machine Learning</italic></source>. <publisher-loc>San Diego, CA</publisher-loc>: <publisher-name>ICML</publisher-name>; <year>2019</year>: <fpage>6105</fpage>–<lpage>6114</lpage>.</mixed-citation>
    </ref>
    <ref id="bib49">
      <label>49.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Galdran</surname><given-names>A</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>The little W-Net that could: state-of-the-art retinal vessel segmentation with minimalistic models</article-title>. arXiv. <year>2020</year>, <pub-id pub-id-type="doi">10.48550/arXiv.2009.01907</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib50">
      <label>50.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hart</surname><given-names>WE</given-names></string-name>, <string-name><surname>Goldbaum</surname><given-names>M</given-names></string-name>, <string-name><surname>Côté</surname><given-names>B</given-names></string-name>, <string-name><surname>Kube</surname><given-names>P</given-names></string-name>, <string-name><surname>Nelson</surname><given-names>MR</given-names></string-name></person-group>. <article-title>Measurement and classification of retinal vascular tortuosity</article-title>. <source><italic toggle="yes">Int J Med Inform</italic></source>. <year>1999</year>; <volume>53</volume>: <fpage>239</fpage>–<lpage>252</lpage>.<pub-id pub-id-type="pmid">10193892</pub-id></mixed-citation>
    </ref>
    <ref id="bib51">
      <label>51.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grisan</surname><given-names>E</given-names></string-name>, <string-name><surname>Foracchia</surname><given-names>M</given-names></string-name>, <string-name><surname>Ruggeri</surname><given-names>A</given-names></string-name></person-group>. <article-title>A novel method for the automatic grading of retinal vessel tortuosity</article-title>. <source><italic toggle="yes">IEEE Trans Med Imaging</italic></source>. <year>2008</year>; <volume>27</volume>: <fpage>310</fpage>–<lpage>319</lpage>.<pub-id pub-id-type="pmid">18334427</pub-id></mixed-citation>
    </ref>
    <ref id="bib52">
      <label>52.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Falconer</surname><given-names>K</given-names></string-name></person-group>. <source><italic toggle="yes">Fractal Geometry: Mathematical Foundations and Applications</italic></source>. <publisher-loc>New York</publisher-loc>: <publisher-name>John Wiley &amp; Sons</publisher-name>; <year>2004</year>.</mixed-citation>
    </ref>
    <ref id="bib53">
      <label>53.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wong</surname><given-names>TY</given-names></string-name>, <string-name><surname>Klein</surname><given-names>R</given-names></string-name>, <string-name><surname>Klein</surname><given-names>BEK</given-names></string-name>, <string-name><surname>Meuer</surname><given-names>SM</given-names></string-name>, <string-name><surname>Hubbard</surname><given-names>LD</given-names></string-name></person-group>. <article-title>Retinal vessel diameters and their associations with age and blood pressure</article-title>. <source><italic toggle="yes">Invest Ophthalmol Vis Sci</italic></source>. <year>2003</year>; <volume>44</volume>: <fpage>4644</fpage>–<lpage>4650</lpage>.<pub-id pub-id-type="pmid">14578380</pub-id></mixed-citation>
    </ref>
    <ref id="bib54">
      <label>54.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parr</surname><given-names>JC</given-names></string-name>, <string-name><surname>Spears</surname><given-names>GF</given-names></string-name></person-group>. <article-title>General caliber of the retinal arteries expressed as the equivalent width of the central retinal artery</article-title>. <source><italic toggle="yes">Am J Ophthalmol</italic></source>. <year>1974</year>; <volume>77</volume>: <fpage>472</fpage>–<lpage>477</lpage>.<pub-id pub-id-type="pmid">4819451</pub-id></mixed-citation>
    </ref>
    <ref id="bib55">
      <label>55.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parr</surname><given-names>JC</given-names></string-name>, <string-name><surname>Spears</surname><given-names>GFS</given-names></string-name></person-group>. <article-title>Mathematic relationships between the width of a retinal artery and the widths of its branches</article-title>. <source><italic toggle="yes">Am J Ophthalmol</italic></source>. <year>1974</year>; <volume>77</volume>: <fpage>478</fpage>–<lpage>483</lpage>.<pub-id pub-id-type="pmid">4819452</pub-id></mixed-citation>
    </ref>
    <ref id="bib56">
      <label>56.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hansen</surname><given-names>LK</given-names></string-name>, <string-name><surname>Salamon</surname><given-names>P</given-names></string-name></person-group>. <article-title>Neural network ensembles</article-title>. <source><italic toggle="yes">IEEE Trans Pattern Anal Mach Intell</italic></source>. <year>1990</year>; <volume>12</volume>: <fpage>993</fpage>–<lpage>1001</lpage>.</mixed-citation>
    </ref>
    <ref id="bib57">
      <label>57.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sarhan</surname><given-names>A</given-names></string-name>, <string-name><surname>Rokne</surname><given-names>J</given-names></string-name>, <string-name><surname>Alhajj</surname><given-names>R</given-names></string-name>, <string-name><surname>Crichton</surname><given-names>A</given-names></string-name></person-group>. <article-title>Transfer learning through weighted loss function and group normalization for vessel segmentation from retinal images</article-title>. In: <source><italic toggle="yes">Proceedings of ICPR 2020: 25th International Conference on Pattern Recognition (ICPR)</italic></source>. <publisher-loc>Piscataway, NJ</publisher-loc>: <publisher-name>Institute of Electrical and Electronics Engineers</publisher-name>; <year>2021</year>.</mixed-citation>
    </ref>
    <ref id="bib58">
      <label>58.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shin</surname><given-names>SY</given-names></string-name>, <string-name><surname>Lee</surname><given-names>S</given-names></string-name>, <string-name><surname>Yun</surname><given-names>ID</given-names></string-name>, <string-name><surname>Lee</surname><given-names>KM</given-names></string-name></person-group>. <article-title>Topology-aware retinal artery–vein classification via deep vascular connectivity prediction</article-title>. <source><italic toggle="yes">Appl Sci</italic></source>. <year>2020</year>; <volume>11</volume>: <fpage>320</fpage>.</mixed-citation>
    </ref>
    <ref id="bib59">
      <label>59.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hasan</surname><given-names>MK</given-names></string-name>, <string-name><surname>Alam</surname><given-names>MA</given-names></string-name>, <string-name><surname>Elahi</surname><given-names>MTE</given-names></string-name>, <string-name><surname>Roy</surname><given-names>S</given-names></string-name>, <string-name><surname>Martí</surname><given-names>R</given-names></string-name></person-group>. <article-title>DRNet: segmentation and localization of optic disc and fovea from diabetic retinopathy image</article-title>. <source><italic toggle="yes">Artif Intell Med</italic></source>. <year>2021</year>; <volume>111</volume>: <fpage>102001</fpage>.<pub-id pub-id-type="pmid">33461693</pub-id></mixed-citation>
    </ref>
    <ref id="bib60">
      <label>60.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cheung</surname><given-names>CY-L</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>A new method to measure peripheral retinal vascular caliber over an extended area</article-title>. <source><italic toggle="yes">Microcirculation</italic></source>. <year>2010</year>; <volume>17</volume>: <fpage>495</fpage>–<lpage>503</lpage>.<pub-id pub-id-type="pmid">21040115</pub-id></mixed-citation>
    </ref>
    <ref id="bib61">
      <label>61.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Poplin</surname><given-names>R</given-names></string-name>, <string-name><surname>Varadarajan</surname><given-names>AV</given-names></string-name>, <string-name><surname>Blumer</surname><given-names>K</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning</article-title>. <source><italic toggle="yes">Nat Biomed Eng</italic></source>. <year>2018</year>; <volume>2</volume>: <fpage>158</fpage>–<lpage>164</lpage>.<pub-id pub-id-type="pmid">31015713</pub-id></mixed-citation>
    </ref>
    <ref id="bib62">
      <label>62.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kelly</surname><given-names>CJ</given-names></string-name>, <string-name><surname>Karthikesalingam</surname><given-names>A</given-names></string-name>, <string-name><surname>Suleyman</surname><given-names>M</given-names></string-name>, <string-name><surname>Corrado</surname><given-names>G</given-names></string-name>, <string-name><surname>King</surname><given-names>D</given-names></string-name></person-group>. <article-title>Key challenges for delivering clinical impact with artificial intelligence</article-title>. <source><italic toggle="yes">BMC Med</italic></source><italic toggle="yes">.</italic><year>2019</year>; <volume>17</volume>: <fpage>195</fpage>.<pub-id pub-id-type="pmid">31665002</pub-id></mixed-citation>
    </ref>
    <ref id="bib63">
      <label>63.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Singh</surname><given-names>RP</given-names></string-name>, <string-name><surname>Hom</surname><given-names>GL</given-names></string-name>, <string-name><surname>Abramoff</surname><given-names>MD</given-names></string-name>, <string-name><surname>Campbell</surname><given-names>JP</given-names></string-name>, <string-name><surname>Chiang</surname><given-names>MF</given-names></string-name></person-group>, <collab>AOO Task Force on Artificial Intelligence</collab>. <article-title>Current challenges and barriers to real-world artificial intelligence adoption for the healthcare system, provider, and the patient</article-title>. <source><italic toggle="yes">Transl Vis Sci Technol</italic></source>. <year>2020</year>; <volume>9</volume>: <fpage>45</fpage>.</mixed-citation>
    </ref>
    <ref id="bib64">
      <label>64.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yip</surname><given-names>W</given-names></string-name>, <string-name><surname>Tham</surname><given-names>YC</given-names></string-name>, <string-name><surname>Hsu</surname><given-names>W</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>Comparison of common retinal vessel caliber measurement software and a conversion algorithm</article-title>. <source><italic toggle="yes">Transl Vis Sci Technol</italic></source>. <year>2016</year>; <volume>5</volume>: <fpage>11</fpage>.</mixed-citation>
    </ref>
    <ref id="bib65">
      <label>65.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McGrory</surname><given-names>S</given-names></string-name>, <string-name><surname>Taylor</surname><given-names>AM</given-names></string-name>, <string-name><surname>Pellegrini</surname><given-names>E</given-names></string-name>, <etal>et al</etal>.</person-group><article-title>Towards standardization of quantitative retinal vascular parameters: comparison of SIVA and VAMPIRE measurements in the Lothian Birth Cohort 1936</article-title>. <source><italic toggle="yes">Transl Vis Sci Technol</italic></source>. <year>2018</year>; <volume>7</volume>: <fpage>12</fpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
