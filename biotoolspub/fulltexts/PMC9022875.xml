<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr SoftwareX?>
<?submitter-system nihms?>
<?submitter-canonical-name Elsevier?>
<?submitter-canonical-id ELSEVIERAM?>
<?submitter-userid 8068823?>
<?submitter-authority myNCBI?>
<?submitter-login elsevieram?>
<?submitter-name Elsevier Author Support?>
<?domain nihpa?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">101660267</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">43949</journal-id>
    <journal-id journal-id-type="nlm-ta">SoftwareX</journal-id>
    <journal-id journal-id-type="iso-abbrev">SoftwareX</journal-id>
    <journal-title-group>
      <journal-title>SoftwareX</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2352-7110</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9022875</article-id>
    <article-id pub-id-type="pmid">35465173</article-id>
    <article-id pub-id-type="doi">10.1016/j.softx.2021.100953</article-id>
    <article-id pub-id-type="manuscript">nihpa1784836</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Open community platform for hearing aid algorithm research: open Master Hearing Aid (openMHA)</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Kayser</surname>
          <given-names>Hendrik</given-names>
        </name>
        <xref rid="A1" ref-type="aff">a</xref>
        <xref rid="A2" ref-type="aff">b</xref>
        <xref rid="A3" ref-type="aff">c</xref>
        <xref rid="CR1" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Herzke</surname>
          <given-names>Tobias</given-names>
        </name>
        <xref rid="A2" ref-type="aff">b</xref>
        <xref rid="A3" ref-type="aff">c</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Maanen</surname>
          <given-names>Paul</given-names>
        </name>
        <xref rid="A2" ref-type="aff">b</xref>
        <xref rid="A3" ref-type="aff">c</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zimmermann</surname>
          <given-names>Max</given-names>
        </name>
        <xref rid="A2" ref-type="aff">b</xref>
        <xref rid="A3" ref-type="aff">c</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Grimm</surname>
          <given-names>Giso</given-names>
        </name>
        <xref rid="A1" ref-type="aff">a</xref>
        <xref rid="A2" ref-type="aff">b</xref>
        <xref rid="A3" ref-type="aff">c</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hohmann</surname>
          <given-names>Volker</given-names>
        </name>
        <xref rid="A1" ref-type="aff">a</xref>
        <xref rid="A2" ref-type="aff">b</xref>
        <xref rid="A3" ref-type="aff">c</xref>
      </contrib>
    </contrib-group>
    <aff id="A1"><label>a</label>Carl von Ossietzky Universität Oldenburg, Department of Medical Physics and Acoustics - Auditory Signal Processing and Hearing Devices, D-26111 Oldenburg, Germany</aff>
    <aff id="A2"><label>b</label>Hörzentrum Oldenburg gGmbH, Marie-Curie-Str. 2, 26129 Oldenburg, Germany</aff>
    <aff id="A3"><label>c</label>Cluster of Excellence “Hearing4all”, Germany</aff>
    <author-notes>
      <corresp id="CR1"><label>*</label>Corresponding author at: Carl von Ossietzky Universität Oldenburg, Department of Medical Physics and Acoustics - Auditory Signal Processing and Hearing Devices, D-26111 Oldenburg, Germany. <email>hendrik.kayser@uni-oldenburg.de</email> (Hendrik Kayser).</corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>2</day>
      <month>4</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>1</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>30</day>
      <month>12</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>21</day>
      <month>4</month>
      <year>2022</year>
    </pub-date>
    <volume>17</volume>
    <elocation-id>100953</elocation-id>
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY-NC-ND license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P1"><italic toggle="yes">open Master Hearing Aid</italic> (openMHA) was developed and provided to the hearing aid research community as an open-source software platform with the aim to support sustainable and reproducible research towards improvement and new types of assistive hearing systems not limited by proprietary software. The software offers a flexible framework that allows the users to conduct hearing aid research using tools and a number of signal processing plugins provided with the software as well as the implementation of own methods. The openMHA software is independent of a specific hardware and supports Linux, macOS and Windows operating systems as well as 32-bit and 64-bit ARM-based architectures such as used in small portable integrated systems. <ext-link xlink:href="http://www.openmha.org/" ext-link-type="uri">www.openmha.org</ext-link></p>
    </abstract>
    <kwd-group>
      <kwd>Hearing aids</kwd>
      <kwd>Real-time audio signal processing</kwd>
      <kwd>Audiological research</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <table-wrap position="anchor" id="T1">
    <table frame="hsides" rules="none">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">Code metadata</th>
          <th align="left" valign="top" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <th colspan="2" align="left" valign="top" rowspan="1">
            <hr/>
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Current code version</td>
          <td align="left" valign="top" rowspan="1" colspan="1">4.16.1</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Permanent link to code/repository used for this code version</td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <ext-link xlink:href="https://github.com/ElsevierSoftwareX/SOFTX-D-21-00049" ext-link-type="uri">https://github.com/ElsevierSoftwareX/SOFTX-D-21-00049</ext-link>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Legal Code License</td>
          <td align="left" valign="top" rowspan="1" colspan="1">AGPL-3.0</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Code versioning system used</td>
          <td align="left" valign="top" rowspan="1" colspan="1">git</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Software code languages, tools, and services used</td>
          <td align="left" valign="top" rowspan="1" colspan="1">C++, C, Matlab/Octave, TeX, Node-RED, Python, Java</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Compilation requirements, operating environments &amp; dependencies</td>
          <td align="left" valign="top" rowspan="1" colspan="1">See <ext-link xlink:href="https://github.com/HoerTech-gGmbH/openMHA/blob/master/COMPILATION.md" ext-link-type="uri">https://github.com/HoerTech-gGmbH/openMHA/blob/master/COMPILATION.md</ext-link></td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Link to developer documentation/manual</td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <ext-link xlink:href="http://www.openmha.org/documentation/" ext-link-type="uri">http://www.openmha.org/documentation/</ext-link>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Support email for questions</td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <ext-link xlink:href="http://info@openmha.org" ext-link-type="uri">info@openmha.org</ext-link>
          </td>
        </tr>
        <tr>
          <td colspan="2" align="left" valign="top" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">Software metadata</th>
          <th align="left" valign="top" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td colspan="2" align="left" valign="top" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Current software version</td>
          <td align="left" valign="top" rowspan="1" colspan="1">4.16.1</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Permanent link to s of this version</td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <ext-link xlink:href="https://github.com/HoerTech-gGmbH/openMHA/releases/tag/v4.16.1" ext-link-type="uri">https://github.com/HoerTech-gGmbH/openMHA/releases/tag/v4.16.1</ext-link>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Legal Software License</td>
          <td align="left" valign="top" rowspan="1" colspan="1">AGPL-3.0</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Computing platforms/Operating Systems</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Linux, macOS, Microsoft Windows, Armv7, AArch64</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Installation requirements &amp; dependencies</td>
          <td align="left" valign="top" rowspan="1" colspan="1">See <ext-link xlink:href="https://github.com/HoerTech-gGmbH/openMHA/blob/master/INSTALLATION.md" ext-link-type="uri">https://github.com/HoerTech-gGmbH/openMHA/blob/master/INSTALLATION.md</ext-link></td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Link to user manuals</td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <ext-link xlink:href="http://www.openmha.org/documentation/" ext-link-type="uri">http://www.openmha.org/documentation/</ext-link>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Support email for questions</td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <ext-link xlink:href="http://info@openmha.org" ext-link-type="uri">info@openmha.org</ext-link>
          </td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <sec id="S1">
    <label>1.</label>
    <title>Motivation and significance</title>
    <p id="P2">The development of hearing aid signal processing methods and new algorithms for assistive listening devices is aimed at providing a benefit for the communication abilities for users in real-world scenarios. In today’s hearing aids, digital audio signal processing is employed to tackle the various requirements to be met by these devices [<xref rid="R1" ref-type="bibr">1</xref>]. From the audiological point of view, hearing loss compensation and the related recruitment phenomenon are important items that need to be addressed to improve speech intelligibility in people with hearing impairment. A variety of algorithms, in particular automatic gain control, dynamic range compression, noise reduction as well as spatial filtering such as directional microphones of acoustic beamformers are employed to tackle this task. Further, the suppression of acoustic feedback plays an important role as well as algorithms for acoustic scene analysis and classification to adjust processing parameters or select a program in the hearing aid to provide optimal processing dependent on the communication scenario. Research in this field is carried out at universities as well as by hearing aid companies and audio equipment manufacturers. The two latter groups provide end-user devices that are developed on proprietary systems, which are not accessible to the research community and which are subject to commercial constraints. Open tools for hearing device research are a way to foster transfer of research outcomes into the design of hearing aids, cochlear implants and consumer devices for the target group’s benefit. Furthermore, open tools may accelerate relevant studies with novel algorithms, lower the barriers for hardware and software development in research environments, facilitate collaborative research efforts and enable more reproducibility in hearing research. The open community platform for hearing aid algorithm research <italic toggle="yes">open Master Hearing Aid</italic> (openMHA, [<xref rid="R2" ref-type="bibr">2</xref>,<xref rid="R3" ref-type="bibr">3</xref>]) is such a tool, which has reached the level of maturity to be used in a wide research community for hearing device research and for studies with hearing impaired subjects [<xref rid="R4" ref-type="bibr">4</xref>–<xref rid="R30" ref-type="bibr">30</xref>], see <xref rid="S2" ref-type="sec">Section 1.1</xref> for an overview.</p>
    <p id="P3">An important part of the development of new methods is their evaluation. The first step in the evaluation of novel methods is usually simulations that are carried out in an offline manner, i.e., all audio data is processed in a batch and causality is not necessarily met. This first step typically tests the performance of a method in isolation from other operations that are part of the signal processing chain in a real device. The outcome of such a simulation is an important result in terms of the capabilities and the potential performance of the method itself. However, it is not a good predictor of the benefit the method can provide for a user in real-life conditions. For this, some important further aspects need to be taken into account: The system needs to be considered as a whole, i.e., the signal processing algorithm needs to work in combination with all other components of the system to provide an estimate of benefit in a realistic usage scenario. This “holistic” approach should include essential hearing aid functionality such as amplification and dynamic range compression to compensate for hearing loss. Feedback suppression techniques to minimize howling caused by acoustic feedback loops between hearing aid loudspeaker and microphone also play a role to allow for sufficiently high amplification of the incoming audio signal. The coupling of a hearing device to the ear affects the lower limit of sound level that can be present at the eardrum and also determines the ratio between unprocessed sound signals arriving directly and sound processed by the hearing device. In addition, signal enhancement methods are used to increase speech intelligibility, e.g., directional microphones, single- and multi-channel noise reduction algorithms. All these processing steps underlie two important technical constraints: They must be accomplished in (a) real-time<sup><xref rid="FN2" ref-type="fn">1</xref></sup> and (b) with a low latency<sup><xref rid="FN3" ref-type="fn">2</xref></sup> between input signal at the hearing aid microphone and playback at the hearing aid loudspeaker. This is required to avoid an echo effect and to provide natural perception of the user’s own voice and synchronicity between visual cues such as lip movements and incoming sound signal, thus enabling natural communication in real-world scenarios. A latency of 10 ms is a value that is typically defined as the upper limit for the latency [<xref rid="R32" ref-type="bibr">32</xref>].</p>
    <p id="P4">All the aspects listed so far should be taken into account in advanced simulations of the hearing aid processing chain. This means, the processing chain needs to be run live on suitable hardware.</p>
    <p id="P5">Furthermore, to enable differently-skilled target groups to use the software tools for research, several approaches to realize and modify hearing aid processing are required. These include the implementation of new signal processing algorithms using an established programming language, the possibility to set up and configure hearing aid processing chains based on the available components without knowledge of a programming language as well as the possibility to control available setups using high-level user interfaces and tools without detailed knowledge of the software itself.</p>
    <p id="P6">openMHA is a comprehensive software package that meets these requirements and provides a modular framework to set up a hearing system with all required processing steps. It originates from the commercial closed-source software <italic toggle="yes">Master Hearing Aid</italic> [<xref rid="R31" ref-type="bibr">31</xref>] that was made available and is further developed under open-source license. It features mechanisms to ensure real-time-safe audio signal processing,<sup><xref rid="FN4" ref-type="fn">3</xref></sup> real-time-safe configuration changes at runtime,<sup><xref rid="FN5" ref-type="fn">4</xref></sup> and monitoring of variables during processing such that a hearing aid algorithm chain under development and test can be easily monitored and validated. In addition, beyond providing the fundamental structure for hearing aid processing, the software package includes a large and growing set of signal processing plugins that can be used to create a multitude of configurations without requiring knowledge of the C/C++ programming language. New developments can be integrated using openMHA’s algorithm development framework using C/C++ or the Matlab Coder [<xref rid="R33" ref-type="bibr">33</xref>].</p>
    <p id="P7">A scientific problem that is tackled with openMHA is the realization of more ecologically valid studies in the context of hearing aid research with the aim to increase the degree to which laboratory-based research findings reflect real-life hearing-related function. The crucial aspect is that researchers are enabled to investigate, implement and test their developments in a realistic hearing aid processing chain using realistic acoustic signal scenarios.</p>
    <sec id="S2">
      <label>1.1.</label>
      <title>Usage of openMHA in the research community</title>
      <p id="P8">Since its first release in 2017 openMHA is utilized in research related to hearing aids, evaluation of signal processing methods and for the setup of hearing aid processing in real-time systems. The software has been used for various purposes such as fitting of a multi-band dynamic range compressor to the hearing loss of hearing-impaired subjects a) in order to model speech intelligibility in cochlear implant users [<xref rid="R4" ref-type="bibr">4</xref>], predicting speech recognition performance in aided subjects [<xref rid="R5" ref-type="bibr">5</xref>] also including a directional microphone available with openMHA, and evaluation of near-end listening enhancement [<xref rid="R6" ref-type="bibr">6</xref>] with individual hearing loss compensation. In that context the openMHA fitting tool (see <xref rid="F2" ref-type="fig">Fig. 2</xref>) and the gain prescription rules implemented therein were employed to realize the hearing aid fitting procedure. Perceived sound quality for music-listening and speech with hearing aids and preferred gain settings were evaluated [<xref rid="R7" ref-type="bibr">7</xref>–<xref rid="R9" ref-type="bibr">9</xref>]. A study on the impact of movement behavior of human subjects on the potential benefit provided by different hearing aid processing methods [<xref rid="R10" ref-type="bibr">10</xref>,<xref rid="R11" ref-type="bibr">11</xref>] used a set of reference implementations of algorithms available with openMHA for a post-analysis that incorporates movement trajectories of the head and eyes in the dynamic virtual acoustic scenes. This work lead to an open-access database [<xref rid="R12" ref-type="bibr">12</xref>] that contains the measured trajectories and parameters of the scenes. A software for rendering of acoustic scenes [<xref rid="R13" ref-type="bibr">13</xref>] that supports the integration and control of openMHA from within the software is used together with openMHA to enable the reproduction of these virtual scenes by the user.</p>
      <p id="P9">openMHA was used to realize real-time signal processing for an ear-level device for hearing aid research [<xref rid="R14" ref-type="bibr">14</xref>], which was compared to commercially available devices in terms of a technical [<xref rid="R15" ref-type="bibr">15</xref>] and a perceptual [<xref rid="R16" ref-type="bibr">16</xref>] evaluations.</p>
      <p id="P10">Several studies used signal processing algorithms implemented in openMHA as references in the evaluation of new developments or to complement other processing methods. A recurrent neural network (RNN) was used to perform noise reduction and dereverberation and compared to the available single-channel noise reduction (SCNR, [<xref rid="R34" ref-type="bibr">34</xref>]) algorithm [<xref rid="R17" ref-type="bibr">17</xref>]. Furthermore the author implemented the RNN-based processing as an openMHA plugin in order to use it on portable hardware. In a different study, the same SCNR algorithm was found to be superior compared to a statistical-model-based noise reduction algorithm for stationary background noise and at poor SNRs [<xref rid="R18" ref-type="bibr">18</xref>]. Furthermore, the combination of our SCNR method with additional envelope enhancement turned out to be beneficial. In subject experiments with normal hearing and unilateral implanted CI listeners that evaluated directional microphone algorithms for speech enhancement in these target groups [<xref rid="R19" ref-type="bibr">19</xref>], openMHA was used to conduct the hearing loss compensation and the spatial filtering. The multi-band dynamic compressor and filterbanks provided with our software were integrated in a bone conduction hearing aid [<xref rid="R20" ref-type="bibr">20</xref>]. The <italic toggle="yes">Clarity Challenge</italic> project [<xref rid="R21" ref-type="bibr">21</xref>], which organizes machine learning challenges for hearing devices on speech enhancement with hearing aid signal processing and perception models of speech intelligibility, uses an openMHA configuration as a baseline hearing aid. It features adaptive differential microphones [<xref rid="R35" ref-type="bibr">35</xref>] and provides hearing loss compensation based on the Camfit compressive gain prescription rule [<xref rid="R36" ref-type="bibr">36</xref>]. openMHA was integrated with the Julia programming language [<xref rid="R22" ref-type="bibr">22</xref>] to implement Bayesian machine learning techniques for real-time hearing aid processing. The real-time capabilities of openMHA were also used in an computer vision application for real-time feature extraction and classification of posture and gesture [<xref rid="R23" ref-type="bibr">23</xref>].</p>
      <p id="P11">openMHA is also suitable to conduct hearing aid signal processing on portable integrated systems that are enabled by the availability of small hardware with sufficient computational power such as single-board computers (SBC) or systems on a chip (SoC). Such hardware platforms are of particular interest to conduct hearing aid research outside the laboratory. For that purpose, openMHA was ported to an open-source SoC Field Programmable Gate Array (FPGA) platform [<xref rid="R24" ref-type="bibr">24</xref>–<xref rid="R26" ref-type="bibr">26</xref>]. Integrated portable platforms that feature ear-level devices such as behind-the-ear devices are of particular interest. In combination with a software suite that provides signal processing capabilities as well as measurement applications for hearing research, such as openMHA, a complete field test system is available. See <xref rid="S22" ref-type="sec">Section 3</xref> for an example of such a field test setup that is tailored to hearing device research with openMHA [<xref rid="R27" ref-type="bibr">27</xref>]. It was found to have “performed on a comparable level as the commercial hearing aids for the assessed metrics and thus can be viewed as a fully functional hearing aid for research purposes” [<xref rid="R28" ref-type="bibr">28</xref>]. In a recent study [<xref rid="R29" ref-type="bibr">29</xref>], this portable platform was extended with a mobile EEG acquisition system aiming at utilizing brain signals for hearing aid processing. Another portable hearing aid research platform [<xref rid="R30" ref-type="bibr">30</xref>] has been shown to be capable of running openMHA, which was included in the software suite for its processing board [<xref rid="R37" ref-type="bibr">37</xref>]. In summary, the large amount of studies performed at different research institutions by researchers with different background using the openMHA for different research questions shows that the openMHA software package in its current version is fully appropriate.</p>
    </sec>
    <sec id="S3">
      <label>1.2.</label>
      <title>User groups</title>
      <p id="P12">To optimally support the community, we divided the user group into three different levels that we address in the documentation and tools that are provided with the software package.</p>
      <sec id="S4">
        <title>Audiological researcher.</title>
        <p id="P13">This user group carries out audiological research studies with hearing-impaired and normal-hearing subjects. For this purpose configurations and graphical user interfaces (GUI) are provided as tools with the openMHA software. Examples for these tools are a GUI for hearing aid fitting that runs under Matlab and Octave and a webserver-based GUI.</p>
      </sec>
      <sec id="S5">
        <title>Application engineer.</title>
        <p id="P14">The group of application engineers builds setups for experiments and studies to be performed by audiological researchers. This group uses the features and tools that are provided with the openMHA software package. It contains a number of signal processing plugins that can be combined to complete hearing aid signal processing chains using a text-based configuration language. GUIs or other control applications can be created, for instance, in Matlab or Octave, in Python, or in Node-RED [<xref rid="R38" ref-type="bibr">38</xref>].</p>
      </sec>
      <sec id="S6">
        <title>Plugin developer.</title>
        <p id="P15">In case that a functionality needed for a particular setup is not available in its current version such as a newly developed signal processing algorithm, the software can be extended with own plugins. A developer uses C/C++ programming language to implement new features into the framework. A large library of signal processing methods to draw from is included with the openMHA software development framework. Furthermore, the Matlab Coder [<xref rid="R33" ref-type="bibr">33</xref>] can be used to implement new openMHA plugins or to compile a signal processing library that can be loaded by a specific wrapper plugin in openMHA. In the latter case, new plugins can be developed and implemented without any knowledge of C++ and without having an openMHA development environment available for compilation.</p>
        <p id="P16">Manuals and examples of documented source code are provided, see end of <xref rid="S20" ref-type="sec">Section 2.3.3</xref> for more details.</p>
      </sec>
    </sec>
    <sec id="S7">
      <label>1.3.</label>
      <title>Available algorithms</title>
      <p id="P17">The software package includes plugins for basic operations such as calibration, filtering, resampling, amplification and an overlap-add Fourier analysis and synthesis framework for signal processing in the spectral domain. Furthermore a number of plugins are included that cover processing methods of the following types: multi-band dynamic range compression [<xref rid="R39" ref-type="bibr">39</xref>], adaptive feedback cancellation [<xref rid="R40" ref-type="bibr">40</xref>], directional microphones [<xref rid="R35" ref-type="bibr">35</xref>], binaural noise and feedback reduction [<xref rid="R41" ref-type="bibr">41</xref>], binaural beamforming [<xref rid="R42" ref-type="bibr">42</xref>–<xref rid="R44" ref-type="bibr">44</xref>], single-channel noise reduction [<xref rid="R34" ref-type="bibr">34</xref>], and sound source localization [<xref rid="R45" ref-type="bibr">45</xref>]. In some cases ([<xref rid="R34" ref-type="bibr">34</xref>,<xref rid="R35" ref-type="bibr">35</xref>,<xref rid="R41" ref-type="bibr">41</xref>,<xref rid="R42" ref-type="bibr">42</xref>], adaptive beamforming [<xref rid="R44" ref-type="bibr">44</xref>], and a delay-and-subtract beamformer), reference configurations are provided that have been used the same way in a research study with the aim to enable reproducibility of the experimental setup by other researchers.</p>
    </sec>
  </sec>
  <sec id="S8">
    <label>2.</label>
    <title>Software description</title>
    <p id="P18">This section presents functionalities of openMHA (<xref rid="S9" ref-type="sec">2.1</xref>), software architecture (<xref rid="S10" ref-type="sec">2.2</xref>), code examples (<xref rid="S17" ref-type="sec">2.3</xref>) for different user scenarios, and our practices and tools used for the software development (<xref rid="S21" ref-type="sec">2.4</xref>).</p>
    <sec id="S9">
      <label>2.1.</label>
      <title>Software functionalities</title>
      <p id="P19">openMHA processes audio signals with hearing aid signal processing algorithms implemented as plugins. Using the configuration language, different plugins can be combined and configured to form the desired complete signal processing chain.</p>
      <p id="P20">openMHA can process either live audio signal from a sound card, or it can process audio input signal from input sound files and produce output sound files. When processing a live audio signal, the configuration of individual plugins can be altered without interrupting the audio processing, making it possible to subjectively compare different algorithm configurations.</p>
      <p id="P21">Different signal processing plugins are available that import and export data to and from files in different formats and over the network (open sound control (OSC, [<xref rid="R46" ref-type="bibr">46</xref>]), lab streaming layer (LSL, [<xref rid="R47" ref-type="bibr">47</xref>])). The latter is useful to take information from other sensors – such as head- and eye motion and EEG data – into account in audio signal processing methods. To this end, plugins can be implemented that analyze such data in order to control signal processing in a certain way. Examples are the exploitation of head- and eye movement to steer directional filtering, EEG data for auditory attention decoding or listening effort.</p>
    </sec>
    <sec id="S10">
      <label>2.2.</label>
      <title>Software architecture</title>
      <p id="P22">The overall software architecture is depicted in <xref rid="F1" ref-type="fig">Fig. 1</xref>. In the following, the single components are described in detail.</p>
      <sec id="S11">
        <label>2.2.1.</label>
        <title>openMHA server</title>
        <p id="P23">openMHA is mainly implemented in C++ as a command line application mha (MHA_Server in <xref rid="F1" ref-type="fig">Fig. 1</xref>) without a graphical user interface. The command line application processes its command line parameters, then opens a TCP port (TCP_Port) and listens for incoming configuration commands. The MHA_Server mha creates one instance of the openMHA framework (MHA_Framework) class which functions as the main event dispatcher of openMHA. The openMHA framework reacts to configuration command events by either</p>
        <list list-type="order" id="L1">
          <list-item>
            <p id="P24">interpreting framework configuration commands itself with the help of the MHA configuration language parser, or</p>
          </list-item>
          <list-item>
            <p id="P25">forwarding configuration commands affecting the audio IO plugin to the respective IO plugin, if one is loaded, or</p>
          </list-item>
          <list-item>
            <p id="P26">forwarding configuration commands affecting one of the signal processing commands to the processing plugin loader.</p>
          </list-item>
        </list>
        <p id="P27">The openMHA framework can load a single IO plugin (IO_Plugin) and a single signal processing plugin (Processing_ Plugin) with the help of the respective plugin loaders, the IO plugin loader (IO_Plugin_Loader) or the signal processing plugin loader (Processing_Plugin_Loader).</p>
      </sec>
      <sec id="S12">
        <label>2.2.2.</label>
        <title>Plugin_Loaders and plugins</title>
        <p id="P28">The plugin loaders search for MHA plugins in a configurable set of file system directories and load the first matching openMHA plugin into the process. Newly loaded plugins insert their configurable settings into a tree of configuration settings, and they are also inserted into the signal processing path.</p>
        <p id="P29">IO plugins as well as signal processing plugins are compiled into shared libraries which export a short list of functions, i.e., their interface. The respective interface’s shared libraries are loaded into the running MHA_Server application mha, which in turn are used by the plugin loaders to integrate the plugin into the mha.</p>
        <p id="P30">Classes and functions shared between the MHA_Server application mha and the plugins are compiled into a shared library, the openMHA Toolbox Library libopenmha.</p>
        <sec id="S13">
          <title>IO plugins.</title>
          <p id="P31">IO plugins connect the openMHA to audio data sources/sinks like sound cards or sound files. openMHA does not impose any restrictions on audio sampling rates, number of audio channels, etc.</p>
        </sec>
        <sec id="S14">
          <title>Signal processing plugins.</title>
          <p id="P32">Signal processing plugins implement hearing aid signal processing algorithms. Some of the existing signal processing plugins contain processing plugin loader(s) themselves and can load other signal processing plugin(s), creating a tree of processing plugins.</p>
        </sec>
      </sec>
      <sec id="S15">
        <label>2.2.3.</label>
        <title>Configuration language interpreter</title>
        <p id="P33">The MHA configuration language interpreter is used by the openMHA framework, the IO plugins, the signal processing plugin loader and the signal processing plugins. It is contained in the openMHA Toolbox library, a shared library linked by the MHA server as well as by the individual plugins. It provides a text-based interface to a tree of configuration variables of different data types. Configuration variables are declared by the framework, the plugins, etc. during their respective initialization, registered with the configuration language tree and made accessible by the openMHA server mha to command line arguments and to commands received on the TCP port. The configuration language interpreter can invoke registered C++ callbacks when a configuration variable is read from or written to, thereby enabling configuration changes before and during signal processing.</p>
        <p id="P34">The design decision to develop a custom configuration language was taken to allow for a lightweight language interpreter, as required for minimal portable implementations, which still allows for the flexibility of extensions through pre- and post-assignment hooks for seamless interaction between a configuration process and the dropout-free real-time signal processing.<sup><xref rid="FN6" ref-type="fn">5</xref></sup> The MHA configuration language is similar to Matlab/Octave language. A detailed description can be found in [<xref rid="R31" ref-type="bibr">31</xref>] and in the <ext-link xlink:href="http://www.openmha.org/docs/openMHA_application_manual.pdf" ext-link-type="uri">openMHA Application Manual</ext-link>, an example is provided in <xref rid="S19" ref-type="sec">Section 2.3.2</xref>.</p>
      </sec>
      <sec id="S16">
        <label>2.2.4.</label>
        <title>Configuration tools</title>
        <p id="P35">Independent software tools can be used to interact with the openMHA using the TCP interface. We provide Matlab/Octave tools for interacting with a running openMHA server over the TCP connection ranging from functions for retrieving and setting MHA configuration variables to graphical user interfaces (GUI) for different tasks, e.g. calibration or adapting a dynamic compressor to individual hearing losses. These tools can execute on the same computer as the openMHA server mha or can also execute on a different computer and communicate with the openMHA server over a wired or wireless network. For embedded systems and smartphones, Matlab or Octave are not a feasible solution to control an openMHA instance on the same or a remote device. In such a case, an embedded webserver that runs on the processing platform itself and provides a GUI for any connected browser-enabled device is a universal approach. An example that uses Node-RED [<xref rid="R38" ref-type="bibr">38</xref>] is shown in <xref rid="F4" ref-type="fig">Fig. 4</xref> in <xref rid="S22" ref-type="sec">Section 3</xref>.</p>
      </sec>
    </sec>
    <sec id="S17">
      <label>2.3.</label>
      <title>Sample code snippets analysis</title>
      <p id="P36">In the following, one exemplary code sample is shown for each user group described in <xref rid="S3" ref-type="sec">Section 1.2</xref>. <xref rid="S18" ref-type="sec">Section 2.3.1</xref> depicts how to start an available openMHA configuration file, which does not require any interaction with the configuration language or the source code itself. In <xref rid="S19" ref-type="sec">Section 2.3.2</xref> the structure of the configuration used in <xref rid="S18" ref-type="sec">Section 2.3.1</xref> is described in detail. Understanding such a configuration file enables the user to modify plugins and parameters in available configurations and create own configurations using the plugins that are available with openMHA. <xref rid="S20" ref-type="sec">Section 2.3.3</xref> describes the development of a simple signal processing plugin on source-code level. These examples and many more can be found in the openMHA software package. In combination with the manuals and tutorials available with the <ext-link xlink:href="http://www.openmha.org/documentation/" ext-link-type="uri">openMHA documentation</ext-link>, the examples presented here can be used as a basis for the users’ own developments.</p>
      <sec id="S18">
        <label>2.3.1.</label>
        <title>Invocation and control</title>
        <p id="P37">Audiologists can invoke openMHA with existing configurations to perform experiments. The example configuration from the following subsection – a realization of multi-band dynamic range compressor – can be invoked from the command line, e.g., as</p>
        <preformat position="float" xml:space="preserve">mha ?read:examples/14-dc-simple/example_dc_simple_live.cfg cmd=start</preformat>
        <p id="P38">To control an experiment from Matlab or Octave, openMHA can also be started with the same configuration from Octave:</p>
        <preformat position="float" xml:space="preserve">mha = mha_start(); % start openMHA process
mha_query(mha,’read’,’examples/14-dc-simple/example_dc_simple_live.cfg’);
mha_set(mha,’cmd’,’start’);
mhacontrol(mha); % start Control and Fitting GUI</preformat>
        <p id="P39">The last command will open a graphical user interface with access to level meters, calibration dialog, profiling tools as well as an audiogram database and a fitting user interface as shown in <xref rid="F2" ref-type="fig">Fig. 2</xref> to apply different gain prescription rules and to modify gain parameters in the running instance of openMHA. A detailed description of this tool is available in the <ext-link xlink:href="http://www.openmha.org/docs/openMHA_gui_manual.pdf" ext-link-type="uri">Fitting GUI manual</ext-link>. It provides a number of gain prescription rules and also offers the possibility to implement own gain rules in Octave/Matlab based on a template available with the software.</p>
        <p id="P40">openMHA can also be used to batch-preprocess sound files with hearing aid dynamic compression for specific hearing losses:</p>
        <preformat position="float" xml:space="preserve">offline_fitting_tool(); % starts a wizard that guides users</preformat>
        <p id="P41">The command above opens a GUI-based Matlab/Octave tool for processing a large number of audio files with hearing aid signal enhancement adapted to custom hearing losses. The tool guides the user through the selection of sound files, audiogram, and a fitting rule with the same interface.</p>
      </sec>
      <sec id="S19">
        <label>2.3.2.</label>
        <title>Configuration file example</title>
        <p id="P42">Application engineers can use a simple text-based configuration language to describe which plugins to load, how to arrange and configure them. The following example – the configuration invoked in <xref rid="S18" ref-type="sec">Section 2.3.1</xref> – performs multi-band dynamic compression of a binaural input signal, where the compression is performed in the Short-Time Fourier Transform (STFT) domain:</p>
        <preformat position="float" xml:space="preserve">1 # Number of input audio channels
2 nchannels_in = 2
3 # Signal fragment size in samples
4 fragsize = 64
5 # Sampling rate
6 srate = 44100
7 # MHA library name
8 mhalib = transducers
9 # IO plugin library name
10 iolib = MHAIOJack
11 # Select audio input and output ports
12 io.con_in = [system:capture_1 system:capture_2]
13 io.con_out = [system:playback_1 system:playback_2]
14 # Load STFT framework plugin
15 mha.plugin_name = overlapadd
16 # Calibration settings
17 mha.calib_in.peaklevel = [116 116]
18 mha.calib_out.peaklevel = [114 114]
19 # STFT parameters
20 mha.overlapadd.fftlen = 256
21 mha.overlapadd.wnd.len = 128
22 # Load mhachain for spectraldomain processing
23 mha.overlapadd.plugin_name = mhachain
24 # load plugins. dc_simple performs dynamic compression.
25 mha.overlapadd.mhachain.algos = [fftfilter-bank dc_simple combinechannels]
26 # Frequency bands
27 mha.overlapadd.mhachain.fftfilterbank.f = [250 1000 4000]
28 # Threshold of noise gate in dB SPL
29 mha.overlapadd.mhachain.dc_simple.expansion_threshold = [20 20 20 20 20 20]
30 # Slope of level mapping below noise gate
31 mha.overlapadd.mhachain.dc_simple.expansion_slope = [4 4 4 4 4 4]
32 # Gain at 50 dB SPL
33 mha.overlapadd.mhachain.dc_simple.g50 = [10 25 40 11 31 55]
34 # Gain at 80 dB SPL
35 mha.overlapadd.mhachain.dc_simple.g80 = [5 15 10 5 21 19]
36 # Limiter threshold, a.k.a maximum possible out-put level, in dB SPL
37 mha.overlapadd.mhachain.dc_simple.limiter_threshold = [120 120 120 120 120 120]
38 # attack time constant in s
39 mha.overlapadd.mhachain.dc_simple.tau_attack = [0.02]
40 # decay time constant in s
41 mha.overlapadd.mhachain.dc_simple.tau_decay = [0.1]
42 # Name of fftfilterbank plugin. Used to extract frequency information.
43 mha.overlapadd.mhachain.dc_simple.filterbank = fftfilterbank
44 # Number of audio channels for re-combination after filterbank
45 mha.overlapadd.mhachain.combinechannels.outchannels = 2</preformat>
        <p id="P43">This configuration is available in the openMHA example 14, “dc_simple”. It implements a simple multi-band dynamic range compression scheme. In lines 1–13 the basic parameters such as number of audio channels, blocksize in samples, sampling rate, and hardware-dependent connections are set. The Jack Audio Connection Kit (JACK) [<xref rid="R48" ref-type="bibr">48</xref>] is employed for audio in- and output and the according library is loaded. As the first plugin (mhalib) the transducers plugin is loaded that handles calibration. Lines 14–21 set up the STFT framework (overlapadd) parameters and values that affect the calibration in openMHA which is crucial for dynamic compression. In lines 22–45 the actual processing chain for the compressor is configured including a filterbank that splits the signal into the target frequency bands, the gain values for the compressor and time constants. See comments in the code given above for more details.</p>
        <p id="P44">General information on the configuration language can be found in the <ext-link xlink:href="http://www.openmha.org/docs/openMHA_application_manual.pdf" ext-link-type="uri">openMHA Application Manual</ext-link>.</p>
      </sec>
      <sec id="S20">
        <label>2.3.3.</label>
        <title>Plugin development example</title>
        <p id="P45">Algorithm developers can develop a new plugin by inheriting from the openMHA class MHAPlugin::plugin_t<italic toggle="yes">&lt;&gt;</italic> and implementing a few methods. The following code implements a plugin that attenuates sound by 20 dB:</p>
        <preformat position="float" xml:space="preserve">#include “ mha_plugin.hh”
class attenuate20_t : public MHAPlugin::plugin_t&lt;int&gt; { public:
 attenuate20_t(algo_comm_t &amp; ac,
	  const std::string &amp; chain_name,
	  const std::string &amp; algo_name)
    : MHAPlugin::plugin_t&lt;int&gt;(“ This plugin attenuates by 20dB”
,ac)
 {}
 void release(void) override
 {}
 void prepare(mhaconfig_t &amp; signal_info) override
 {
  if (signal_info.domain != MHA_WAVEFORM)
    throw MHA_Error(__FILE__, __LINE__,”
can only process waveform” );
 }
 mha_wave_t * process(mha_wave_t * signal)
 {
  //−20dB = factor 0.1
  MHASignal::for_each(signal,[](mha_real_t sample){return sample * 0.1f;});
  return signal;
 }
};
MHAPLUGIN_CALLBACKS(attenuate20,attenuate20_t,wave,wave)
MHAPLUGIN_DOCUMENTATION(attenuate20, “
example level-modification”,
 “ Plugin attenuate20 attenuates the input signal by 20dB.” )</preformat>
        <p id="P46">The new plugin class needs to implement the constructor and the methods prepare(), process(), and release(). The <ext-link xlink:href="http://www.openmha.org/docs/openMHA_developer_manual.pdf" ext-link-type="uri">Plugin development guide</ext-link> contains a <ext-link xlink:href="http://mha.hoertech.de/doc/master/group__example__tut.html" ext-link-type="uri">plugin development tutorial</ext-link> as well as a reference of classes and methods of the openMHA toolbox library that can be used while developing signal processing algorithms with the openMHA. This example plugin is available in openMHA as “attenuate20”.</p>
        <p id="P47">Alternatively, the Matlab Coder [<xref rid="R33" ref-type="bibr">33</xref>] software is supported in openMHA. This functionality enables the conversion of Matlab code into openMHA-compatible C/C++ code or libraries that can be loaded into a processing chain using a wrapper plugin such that knowledge of C/C++ is not required if a user wants to implement an own Matlab-based signal processing method in openMHA. Researchers can follow and adapt <ext-link xlink:href="https://github.com/HoerTech-gGmbH/openMHA/tree/master/examples/23-matlab-coder" ext-link-type="uri">a complete example for producing an openMHA plugin from Matlab code</ext-link>, general documentation is available in the <ext-link xlink:href="http://www.openmha.org/docs/openMHA_matlab_coder_integration.pdf" ext-link-type="uri">Matlab Coder Integration manual</ext-link>. When following this procedure, it is also possible to split the process by performing the Matlab-to-C++ conversion on a different computer (with Matlab Coder available) than the compilation of the generated C++ code into an MHA plugin (with the compilers for the desired target platform available).</p>
      </sec>
    </sec>
    <sec id="S21">
      <label>2.4.</label>
      <title>Development procedure and practices</title>
      <p id="P48">Users can submit issues, feature requests and pull requests through the openMHA GitHub repository [<xref rid="R49" ref-type="bibr">49</xref>]. Users can post questions about openMHA usage in the openMHA forum [<xref rid="R50" ref-type="bibr">50</xref>] where they are usually answered within a few days by other users or by the openMHA development team.</p>
      <p id="P49">The openMHA development team uses a non-public Phabricator [<xref rid="R51" ref-type="bibr">51</xref>] instance for internal issue tracking, feature requests, peer code reviews, democratic decisions, and documentation of development processes.</p>
      <p id="P50">We use git [<xref rid="R52" ref-type="bibr">52</xref>] for version control. Features and bug fixes are developed in dedicated branches and merged into the public development branch only after peer code reviews. We are using unit tests written with Googletest/Googlemock [<xref rid="R53" ref-type="bibr">53</xref>] to test details of our openMHA application, our toolbox library, or individual openMHA plugins. Additionally, we use system tests written in Matlab/Octave to set up complete instances of openMHA processing configurations and to test the effects of signal processing by individual openMHA plugins or by combinations of several openMHA plugins against expectations. These tests are integrated into our build process which uses GNU Make [<xref rid="R54" ref-type="bibr">54</xref>] and can be invoked with “make unit-tests” for the unit tests or “make test” for the system tests, respectively. Additionally, we use a Jenkins [<xref rid="R55" ref-type="bibr">55</xref>] server to automate recompilation and execution of the aforementioned unit tests and system tests on all openMHA target platforms (Windows, macOS, Linux for PC, Linux for ARM) after every update of the development branch, and also for every suggested code review request. Code review requests will only be sent to reviewers once all builds and tests have succeeded on all openMHA target platforms. We use the GNU GCC compilers on Linux [<xref rid="R56" ref-type="bibr">56</xref>] and Windows [<xref rid="R57" ref-type="bibr">57</xref>] and the Clang compiler on macOS [<xref rid="R58" ref-type="bibr">58</xref>] to compile openMHA, and have configured these compilers to treat all warnings as errors (-Wall -Werror) in order to force ourselves to fix all code smells detected by modern C++ compilers. The minimum C++ standard that openMHA supports is C++17.</p>
      <p id="P51">To release new openMHA versions, the openMHA development branch on git is merged into the master branch on git, but only after additional live audio processing tests have been performed by the openMHA development team and which cannot be automated.</p>
    </sec>
  </sec>
  <sec id="S22">
    <label>3.</label>
    <title>Illustrative example</title>
    <p id="P52">One example of how openMHA can be used in the field is the “Portable Hearing Laboratory” (PHL, [<xref rid="R27" ref-type="bibr">27</xref>]), see <xref rid="F3" ref-type="fig">Fig. 3</xref>. The PHL consist of a <italic toggle="yes">BeagleBone Black wireless</italic> single-board computer that was extended with a multi-channel audio board based on the open-source audio interface Cape4all [<xref rid="R59" ref-type="bibr">59</xref>]. In addition to the processing board and audio interface the integrated system includes a battery and a set of binaural behind-the-ear (BTE) hearing aids that comprise two microphones on each side to capture the sound field and receivers in the canal (RIC) for the playback of processed audio signals. The light-weight system can be worn by a user using a neckband, the BTE-RIC devices are connected with the processing box via flexible cables. An optimized Linux operating system - <italic toggle="yes">MAHALIA</italic> [<xref rid="R60" ref-type="bibr">60</xref>] - is available that is tailored to performing low-latency signal processing on the processing board.</p>
    <p id="P53">A typical use case for the PHL is a field test of a new hearing aid fitting strategy or gain prescription rule. As it is not possible to implement a new fitting rule on the hearing aid of a hard-of-hearing subject their device can be replaced by the PHL for the experiment. Input-level-dependent gain values can be pre-computed according to the prescription rule under test taking into account audiometric measures such as hearing threshold and loudness perception or directly modified at runtime on the device using the fitting tool shown in <xref rid="F2" ref-type="fig">Fig. 2</xref>. The resulting values are effective in the multi-band dynamic range compressor plugin which is available in openMHA. This way, the subject has an individualized hearing aid that can be worn and tested in every-day situations. For the evaluation, e.g., questionnaires can be used or feedback can be obtained in interviews with the participant. Furthermore, the subject may be given access to certain parameters of the processing to adjust the device in different situations or compare different processing modes. Therefore, a smartphone (or any other browser-enabled device with wireless network functionality) can be connected to the PHL device through a wireless network connection. A webserver running on the PHL can be used to provide a GUI as shown in <xref rid="F4" ref-type="fig">Fig. 4</xref>.</p>
    <p id="P54">This particular GUI provides control over a basic hearing aid configuration that is provided with the MAHALIA operating system sd-card image (available from <ext-link xlink:href="https://mahalia.openmha.org/" ext-link-type="uri">https://mahalia.openmha.org/</ext-link>) and also available in the openMHA configuration examples as example 17 “PHL-generic-hearing-aid”. The configuration provides a hearing aid processing chain as sketched in <xref rid="F5" ref-type="fig">Fig. 5</xref>. It uses the two microphone signals from each of the BTE devices (<italic toggle="yes">Lin</italic> and <italic toggle="yes">Rin</italic>) as input, followed by the microphone calibration stage. For noise reduction it features directional filtering with bilateral adaptive differential microphones (ADM). The resulting binaural signal is further processed in the spectral domain with a binaural coherence filter for noise suppression and feedback reduction, a 5-band dynamic range compressor with selectable compression ratio, and a band-limited frequency shifter for feedback reduction. Finally, the signals are transformed back into the time domain, calibrated to account for receiver and ear-canal frequency response, and played back through the left and right hearing aid RIC (<italic toggle="yes">Lout</italic> and <italic toggle="yes">Rout</italic>). In addition to the BTE-RIC devices, the 3.5 mm jack connectors of the PHL can be used for audio input (left (pink) jack connector in <xref rid="F3" ref-type="fig">Fig. 3</xref>) and audio output (rigth (green) jack connector in <xref rid="F3" ref-type="fig">Fig. 3</xref>). The latency between input and output of the processed audio amounts to 10 ms, the audio sampling rate is 24 kHz.</p>
    <p id="P55">The signal processing with this configuration uses 50% of the CPU capacity of the system with all processing stages activated. This leaves room for modifications of the setup in several ways, e.g., a larger number of frequency bands in the compressor, optimization towards lower latency, or extension with other signal enhancement approaches.</p>
  </sec>
  <sec id="S23">
    <label>4.</label>
    <title>Impact</title>
    <p id="P56">openMHA provides a modular framework that enables researchers to set up a flexible defined hearing system that provides full control and access to the processed signal at each processing step. This allows researchers to assess the benefit of hearing aid processing methods in several ways, ranging from offline simulations based on a set of audio files, over simulated realistic virtual audio-visual environments, to field studies in uncontrolled real-world environments. During the whole process the same software and configurations can be used with only minimal adaptation required such as switching from processing of audio files to processing of microphone input. In particular, due to the real-time capabilities of openMHA, hearing aid processing with live sound input and output at low latency can be achieved. In general, this does not require the re-implementation of the processing on a different hardware/software system, i.e., rapid prototyping with tests in realistic test scenarios become possible with openMHA. This versatility facilitates the development and evaluation of new methods with a focus of ecological validity, i.e., to increase the level to which research findings reflect real-life hearing-related function [<xref rid="R61" ref-type="bibr">61</xref>]. Furthermore, openMHA facilitates the investigation of interactions among a hearing aid processing strategy and the user needs.</p>
    <p id="P57">We have identified several publications (see <xref rid="S2" ref-type="sec">Section 1.1</xref>) originating from studies that actively use openMHA in the context of</p>
    <list list-type="bullet" id="L2">
      <list-item>
        <p id="P58">hearing aid fitting</p>
      </list-item>
      <list-item>
        <p id="P59">modeling and prediction of hearing aid benefit</p>
      </list-item>
      <list-item>
        <p id="P60">signal processing for assistive hearing devices</p>
      </list-item>
      <list-item>
        <p id="P61">speech enhancement methods including deep machine learning</p>
      </list-item>
      <list-item>
        <p id="P62">psychoacoustic experiments on hearing aid processing</p>
      </list-item>
      <list-item>
        <p id="P63">assessment of user behavior in subject-in-the loop experiments</p>
      </list-item>
    </list>
    <p id="P64">Reproducible and accessible science is another important factor that is supported with the openMHA software. openMHA software setups that have been used in hearing studies can be made available for direct reproduction by other researchers. Some of them have already been included in the openMHA distribution, e.g., the openMHA configurations used by [<xref rid="R11" ref-type="bibr">11</xref>,<xref rid="R44" ref-type="bibr">44</xref>]. Setups can be passed between different research sites or multi-center studies can be carried out using a common code basis such as is the case in the <italic toggle="yes">Clarity Challenge</italic> [<xref rid="R21" ref-type="bibr">21</xref>] on machine learning for hearing devices and prediction of hearing aid benefit. In summary, the already large impact of the openMHA software on hearing-aid research is clearly demonstrated by large amount of published studies that used openMHA.</p>
  </sec>
  <sec id="S24">
    <label>5.</label>
    <title>Conclusions</title>
    <p id="P65">In this publication we introduced <italic toggle="yes">open Master Hearing Aid</italic> (openMHA), an open-source software platform for hearing aid research. The software enables sustainable and reproducible research for hearing instruments and supports studies that aim for a high level of ecological validity, i.e., that increase the degree to which research findings reflect real-life hearing-related function. It offers several services for hearing aid researchers such as fundamental functionality for hearing aid signal processing like acoustic calibration and filterbanks, a library for common signal processing tasks and a complete set of hearing aid signal processing reference algorithms, capabilities of offline-processing as well as real-time signal processing with a reliable low acoustic latency. A versatile tool set enables researchers to conduct audiological studies to investigate hearing aid processing methods 1) based on available reference configurations and control tools 2) based on custom hearing aid signal processing chains that can be configured based on a large set of available algorithms and 3) using novel algorithms that can be implemented by a user using C/C++ programming language or based on Matlab code. The software can be used on a wide range of hardware including small portable devices and under all established operating systems.</p>
    <p id="P66">It has demonstrated its usability in a number of studies from different laboratories that used openMHA.</p>
  </sec>
</body>
<back>
  <ack id="S25">
    <title>Acknowledgments</title>
    <p id="P67">Research reported in this publication was supported by the National Institute On Deafness And Other Communication Disorders of the National Institutes of Health, USA under Award Number R01DC015429. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.</p>
  </ack>
  <fn-group>
    <fn id="FN1">
      <p id="P68">Declaration of competing interest</p>
      <p id="P69">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
    </fn>
    <fn id="FN2">
      <label>1</label>
      <p id="P70">In the context of audio signal processing in openMHA, <italic toggle="yes">real-time</italic> processing means that a fragment of the incoming audio signal can be processed within the temporal duration of the respective fragment.</p>
    </fn>
    <fn id="FN3">
      <label>2</label>
      <p id="P71"><italic toggle="yes">Latency</italic> refers to the time which passes between the input of an incoming audio sample and the output of that sample after processing. Latency is influenced by the properties of the audio hardware, the size of the signal fragments used for processing as well as signal buffering as required, e.g., in STFT frameworks, see [<xref rid="R31" ref-type="bibr">31</xref>] for more details. Note that algorithmic delay such as, e.g., the group delay of filters in the signal processing chain, add to the latency defined here.</p>
    </fn>
    <fn id="FN4">
      <label>3</label>
      <p id="P72">Audio signal processing can be performed in dedicated threads with high priority settings. Furthermore, the audio signal processing implemented in openMHA avoids that can introduce unpredictable delays (e.g., memory allocation, file system access).</p>
    </fn>
    <fn id="FN5">
      <label>4</label>
      <p id="P73">Changes of configuration parameters of signal processing algorithms while signal processing is continuously ongoing will not induce additional adaptation operations within the signal processing thread, and therefore do not increase the computation time required for signal processing. This is achieved by performing all required preparations for parameter changes outside the real-time signal processing thread, typically with lower priority settings.</p>
    </fn>
    <fn id="FN6">
      <label>5</label>
      <p id="P74">High-level languages such as Python, lua or ruby were considered for the configuration language. However, since primarily only low-level configuration functionality is required in openMHA, they would exceed the requirements of openMHA by far.</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>[1]</label>
      <mixed-citation publication-type="journal"><name><surname>Hamacher</surname><given-names>V</given-names></name>, <name><surname>Chalupper</surname><given-names>J</given-names></name>, <name><surname>Eggers</surname><given-names>E</given-names></name>, <name><surname>Kornagel</surname><given-names>U</given-names></name>, <name><surname>Puder</surname><given-names>H</given-names></name>, <etal/><article-title>Signal processing in high-end hearing aids: State of the art, challenges, and future trends</article-title>. <source>EURASIP J. Adv. Signal Process</source>. <year>2005</year>;<volume>2005</volume>(<issue>18</issue>):152674. <pub-id pub-id-type="doi">10.1155/ASP.2005.2915</pub-id>.</mixed-citation>
    </ref>
    <ref id="R2">
      <label>[2]</label>
      <mixed-citation publication-type="webpage"><source>Open community platform for hearing aid algorithm research</source>, <pub-id pub-id-type="doi">10.5281/zenodo.5517741</pub-id>.</mixed-citation>
    </ref>
    <ref id="R3">
      <label>[3]</label>
      <mixed-citation publication-type="confproc"><name><surname>Herzke</surname><given-names>T</given-names></name>, <name><surname>Kayser</surname><given-names>H</given-names></name>, <name><surname>Loshaj</surname><given-names>F</given-names></name>, <name><surname>Grimm</surname><given-names>G</given-names></name>, <name><surname>Hohmann</surname><given-names>V</given-names></name>. <source>Open signal processing software platform for hearing aid research (openMHA)</source>. In: <conf-name>Linux Audio Conference</conf-name>. <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="R4">
      <label>[4]</label>
      <mixed-citation publication-type="journal"><name><surname>Zedan</surname><given-names>A</given-names></name>, <name><surname>Williges</surname><given-names>B</given-names></name>, <name><surname>Jürgens</surname><given-names>T</given-names></name>. <article-title>Modeling speech intelligibility of simulated bimodal and single-sided deaf cochlear implant users</article-title>. <source>Acta Acust. United Acust</source>. <year>2018</year>;<volume>104</volume>:<fpage>918</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.3813/AAA.919256</pub-id>.</mixed-citation>
    </ref>
    <ref id="R5">
      <label>[5]</label>
      <mixed-citation publication-type="journal"><name><surname>Schädler</surname><given-names>MR</given-names></name>, <name><surname>Hülsmeier</surname><given-names>D</given-names></name>, <name><surname>Warzybok</surname><given-names>A</given-names></name>, <name><surname>Kollmeier</surname><given-names>B</given-names></name>. <article-title>Individual aided speech-recognition performance and predictions of benefit for listeners with impaired hearing employing FADE</article-title>. <source>Trends Hear</source><year>2020</year>;<volume>24</volume>:2331216520938929. <pub-id pub-id-type="doi">10.1177/2331216520938929</pub-id>.</mixed-citation>
    </ref>
    <ref id="R6">
      <label>[6]</label>
      <mixed-citation publication-type="confproc"><name><surname>Chermaz</surname><given-names>C</given-names></name>, <name><surname>Vormann</surname><given-names>M</given-names></name>, <name><surname>Wagener</surname><given-names>K</given-names></name>, <name><surname>Hohmann</surname><given-names>V</given-names></name>. <source>Measuring the benefit of NELE algorithms for hearing aid users in realistic scenarios with the AFC-MHA platform</source>. In: <conf-name>Speech In Noise - SPIN 2020</conf-name>, <conf-loc>Toulouse, France</conf-loc>. <year>2020</year>.</mixed-citation>
    </ref>
    <ref id="R7">
      <label>[7]</label>
      <mixed-citation publication-type="confproc"><name><surname>Vaisberg</surname><given-names>J</given-names></name>, <name><surname>Beaulac</surname><given-names>S</given-names></name>, <name><surname>Glista</surname><given-names>D</given-names></name>, <name><surname>Eeckhoutte</surname><given-names>MV</given-names></name>, <name><surname>Macpherson</surname><given-names>E</given-names></name>, <name><surname>Scollie</surname><given-names>S</given-names></name>. <source>C3-O-6: Preferred hearing aid gain settings for music-listening using a 3D modified simplex procedure implemented with the Open Source Master Hearing Aid platform</source>. In: <conf-name>International hearing aid research conference</conf-name>. <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="R8">
      <label>[8]</label>
      <mixed-citation publication-type="thesis"><name><surname>Vaisberg</surname><given-names>JM</given-names></name>. <source>Understanding hearing aid sound quality for music-listening</source>. (<comment>Ph.D. thesis</comment>), <collab>The University of Western Ontario</collab>; <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="R9">
      <label>[9]</label>
      <mixed-citation publication-type="journal"><name><surname>Vaisberg</surname><given-names>JM</given-names></name>, <name><surname>Beaulac</surname><given-names>S</given-names></name>, <name><surname>Glista</surname><given-names>D</given-names></name>, <name><surname>Macpherson</surname><given-names>EA</given-names></name>, <name><surname>Scollie</surname><given-names>SD</given-names></name>. <article-title>Perceived sound quality dimensions influencing frequency-gain shaping preferences for hearing aid-amplified speech and music</article-title>. <source>Trends Hear</source><year>2021</year>;<volume>25</volume>. <pub-id pub-id-type="doi">10.1177/2331216521989900</pub-id>.</mixed-citation>
    </ref>
    <ref id="R10">
      <label>[10]</label>
      <mixed-citation publication-type="confproc"><name><surname>Hendrikse</surname><given-names>MME</given-names></name>, <name><surname>Llorach</surname><given-names>G</given-names></name>, <name><surname>Grimm</surname><given-names>G</given-names></name>, <name><surname>Hohmann</surname><given-names>V</given-names></name>. <source>Realistic audiovisual listening environments in the lab: analysis of movement behavior and consequences for hearing aids</source>. In: <name><surname>Ochmann</surname><given-names>M</given-names></name>, <name><surname>Vorländer</surname><given-names>M</given-names></name>, <name><surname>Fels</surname><given-names>J</given-names></name>, editors. <conf-name>Proceedings of the 23rd International Congress on Acoustics: Integrating 4th EAA Euroregio in Aachen</conf-name>, <conf-loc>Germany. Berlin</conf-loc>, <publisher-name>Germany: Deutsche Gesellschaft für Akustik</publisher-name>; <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="R11">
      <label>[11]</label>
      <mixed-citation publication-type="journal"><name><surname>Hendrikse</surname><given-names>MME</given-names></name>, <name><surname>Grimm</surname><given-names>G</given-names></name>, <name><surname>Hohmann</surname><given-names>V</given-names></name>. <article-title>Evaluation of the influence of head movement on hearing aid algorithm performance using acoustic simulations</article-title>. <source>Trends Hear</source><year>2020</year>;<volume>24</volume>. <pub-id pub-id-type="doi">10.1177/2331216520916682</pub-id>.</mixed-citation>
    </ref>
    <ref id="R12">
      <label>[12]</label>
      <mixed-citation publication-type="journal"><name><surname>Hendrikse</surname><given-names>MME</given-names></name>, <name><surname>Schwarte</surname><given-names>K</given-names></name>, <name><surname>Grimm</surname><given-names>G</given-names></name>, <name><surname>Hohmann</surname><given-names>V</given-names></name>. <article-title>Generating hearing aid microphone recordings including head movement in virtual acoustic environments resembling everyday life</article-title>. <source>Zenodo</source>; <year>2020</year>, <pub-id pub-id-type="doi">10.5281/zenodo.3905920</pub-id>.</mixed-citation>
    </ref>
    <ref id="R13">
      <label>[13]</label>
      <mixed-citation publication-type="journal"><name><surname>Grimm</surname><given-names>G</given-names></name>, <name><surname>Luberadzka</surname><given-names>J</given-names></name>, <name><surname>Hohmann</surname><given-names>V</given-names></name>. <article-title>A toolbox for rendering virtual acoustic environments in the context of audiology</article-title>. <source>Acta Acust. United Acust</source>. <year>2019</year>;<volume>105</volume>(<issue>3</issue>):<fpage>566</fpage>–<lpage>78</lpage>. <pub-id pub-id-type="doi">10.3813/AAA.919337</pub-id>.</mixed-citation>
    </ref>
    <ref id="R14">
      <label>[14]</label>
      <mixed-citation publication-type="journal"><name><surname>Denk</surname><given-names>F</given-names></name>, <name><surname>Lettau</surname><given-names>M</given-names></name>, <name><surname>Schepker</surname><given-names>H</given-names></name>, <name><surname>Doclo</surname><given-names>S</given-names></name>, <name><surname>Roden</surname><given-names>R</given-names></name>, <name><surname>Blau</surname><given-names>M</given-names></name>, <etal/><article-title>A one-size-fits-all earpiece with multiple microphones and drivers for hearing device research</article-title>. <source>J Audio Eng Soc</source><year>2019</year>.</mixed-citation>
    </ref>
    <ref id="R15">
      <label>[15]</label>
      <mixed-citation publication-type="journal"><name><surname>Denk</surname><given-names>F</given-names></name>, <name><surname>Schepker</surname><given-names>H</given-names></name>, <name><surname>Doclo</surname><given-names>S</given-names></name>, <name><surname>Kollmeier</surname><given-names>B</given-names></name>. <article-title>Acoustic transparency in hearables — technical evaluation</article-title>. <source>J Audio Eng Soc</source><year>2020</year>;<volume>68</volume>(<issue>7/8</issue>):<fpage>508</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.17743/jaes.2020.0042</pub-id>.</mixed-citation>
    </ref>
    <ref id="R16">
      <label>[16]</label>
      <mixed-citation publication-type="journal"><name><surname>Schepker</surname><given-names>H</given-names></name>, <name><surname>Denk</surname><given-names>F</given-names></name>, <name><surname>Kollmeier</surname><given-names>B</given-names></name>, <name><surname>Doclo</surname><given-names>S</given-names></name>. <article-title>Acoustic transparency in hearables - perceptual sound quality evaluations</article-title>. <source>J Audio Eng Soc</source><year>2020</year>;<volume>68</volume>:<fpage>495</fpage>–<lpage>507</lpage>. <pub-id pub-id-type="doi">10.17743/jaes.2020.0045</pub-id>.</mixed-citation>
    </ref>
    <ref id="R17">
      <label>[17]</label>
      <mixed-citation publication-type="thesis"><name><surname>Parameswaran</surname><given-names>K</given-names></name>. <source>Objective assessment of machine learning algorithms for speech enhancement in hearing aids</source>. (<comment>Ph.D. thesis</comment>), <collab>The University of Western Ontario</collab>; <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="R18">
      <label>[18]</label>
      <mixed-citation publication-type="journal"><name><surname>Moshgelani</surname><given-names>F</given-names></name>, <name><surname>Parsa</surname><given-names>V</given-names></name>, <name><surname>Allan</surname><given-names>C</given-names></name>, <name><surname>Veeranna</surname><given-names>SA</given-names></name>, <name><surname>Allen</surname><given-names>P</given-names></name>. <article-title>Perceptual and objective assessment of envelope enhancement for children with auditory processing disorder</article-title>. <source>IEEE Trans. Neural Syst. Rehabil. Eng</source>. <year>2020</year>;<volume>28</volume>(<issue>1</issue>):<fpage>143</fpage>–<lpage>51</lpage>. <pub-id pub-id-type="doi">10.1109/TNSRE.2019.2957230</pub-id>.<pub-id pub-id-type="pmid">31804940</pub-id></mixed-citation>
    </ref>
    <ref id="R19">
      <label>[19]</label>
      <mixed-citation publication-type="book"><name><surname>Kahl</surname><given-names>S</given-names></name>. <source>Evaluation of directional microphone algorithms for speech enhancement with normal hearing and unilateral implanted CI listeners in different speech-in-noise scenarios</source>. <publisher-name>Universität zu Lübeck</publisher-name>; <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="R20">
      <label>[20]</label>
      <mixed-citation publication-type="confproc"><name><surname>Riyani</surname><given-names>A</given-names></name>, <name><surname>Handayani</surname><given-names>A</given-names></name>, <name><surname>Mengko</surname><given-names>TLR</given-names></name>. <source>Bone conduction hearing aid for microtia patient using open MHA library</source>. In: <conf-name>2020 International Seminar on Intelligent Technology and its Applications</conf-name>. <year>2020</year>. p. <fpage>152</fpage>–<lpage>157</lpage>.</mixed-citation>
    </ref>
    <ref id="R21">
      <label>[21]</label>
      <mixed-citation publication-type="confproc"><name><surname>Graetzer</surname><given-names>S</given-names></name>, <name><surname>Barker</surname><given-names>J</given-names></name>, <name><surname>Cox</surname><given-names>TJ</given-names></name>, <name><surname>Akeroyd</surname><given-names>M</given-names></name>, <name><surname>Culling</surname><given-names>JF</given-names></name>, <name><surname>Naylor</surname><given-names>G</given-names></name>, <etal/><source>Clarity-2021 challenges: Machine learning challenges for advancing hearing aid processing</source>. In: <conf-name>Proc. of the Annual Conference of the International Speech Communication Association - INTERSPEECH</conf-name>. <year>2021</year>.</mixed-citation>
    </ref>
    <ref id="R22">
      <label>[22]</label>
      <mixed-citation publication-type="book"><name><surname>Villescas</surname><given-names>MR</given-names></name>, <name><surname>de Vries</surname><given-names>B</given-names></name>, <name><surname>Stuijk</surname><given-names>S</given-names></name>, <name><surname>Corporaal</surname><given-names>H</given-names></name>. <part-title>Real-time audio processing for hearing aids using a model-based Bayesian inference framework</part-title>. In: <source>SCOPES</source>, vol. <volume>20</volume>, <publisher-loc>New York, NY, USA</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>; <year>2020</year>, p. <fpage>82</fpage>–<lpage>5</lpage>. <pub-id pub-id-type="doi">10.1145/3378678.3397528</pub-id>.</mixed-citation>
    </ref>
    <ref id="R23">
      <label>[23]</label>
      <mixed-citation publication-type="confproc"><name><surname>Grimm</surname><given-names>G</given-names></name>, <name><surname>Luberadzka</surname><given-names>J</given-names></name>, <name><surname>Hohmann</surname><given-names>V</given-names></name>. <source>Multi-user posture and gesture classification for ‘subject-in-the-loop’ applications</source>. In: <conf-name>Linux Audio Conference</conf-name>. <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="R24">
      <label>[24]</label>
      <mixed-citation publication-type="confproc"><name><surname>Vannoy</surname><given-names>T</given-names></name>, <name><surname>Davis</surname><given-names>T</given-names></name>, <name><surname>Dack</surname><given-names>C</given-names></name>, <name><surname>Sobrero</surname><given-names>D</given-names></name>, <name><surname>Snider</surname><given-names>R</given-names></name>. <source>An open audio processing platform using SoC FPGAs and model-based development</source>. In: <conf-name>147th Audio Engineering Society Convention</conf-name>. <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="R25">
      <label>[25]</label>
      <mixed-citation publication-type="journal"><name><surname>Snider</surname><given-names>RK</given-names></name>, <name><surname>Vannoy</surname><given-names>T</given-names></name>, <name><surname>Eaton</surname><given-names>J</given-names></name>, <name><surname>Blunt</surname><given-names>M</given-names></name>, <name><surname>Galacci</surname><given-names>EB</given-names></name>, <name><surname>Williams</surname><given-names>J</given-names></name>, <etal/><article-title>Real-time audio signal processing using system-on-chip field programmable gate arrays</article-title>. <source>J. Acoust. Soc. Am</source>. <year>2019</year>;<volume>146</volume>(<issue>4</issue>):<fpage>2879</fpage>. <pub-id pub-id-type="doi">10.1121/1.5136987</pub-id>.</mixed-citation>
    </ref>
    <ref id="R26">
      <label>[26]</label>
      <mixed-citation publication-type="journal"><name><surname>Snider</surname><given-names>R</given-names></name>, <name><surname>Blunt</surname><given-names>M</given-names></name>, <name><surname>Vannoy</surname><given-names>T</given-names></name>, <name><surname>Sobrero</surname><given-names>D</given-names></name>, <name><surname>Wickham</surname><given-names>D</given-names></name>, <name><surname>Davis</surname><given-names>T</given-names></name>. <article-title>Implementing the open master hearing aid on a system-on-chip field programmable gate array</article-title>. <source>J. Acoust. Soc. Am</source>. <year>2020</year>;<volume>148</volume>(<issue>4</issue>):<fpage>2508</fpage>. <pub-id pub-id-type="doi">10.1121/1.5146971</pub-id>.</mixed-citation>
    </ref>
    <ref id="R27">
      <label>[27]</label>
      <mixed-citation publication-type="confproc"><name><surname>Pavlovic</surname><given-names>C</given-names></name>, <name><surname>Kayser</surname><given-names>H</given-names></name>, <name><surname>Maanen</surname><given-names>P</given-names></name>, <name><surname>Herzke</surname><given-names>T</given-names></name>, <name><surname>Hohmann</surname><given-names>V</given-names></name>, <name><surname>Prakash</surname><given-names>S</given-names></name>, <etal/><source>Open portable platform for hearing aid research</source>. In: <conf-name>Annual meeting of the American Auditory Society</conf-name>. <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="R28">
      <label>[28]</label>
      <mixed-citation publication-type="confproc"><name><surname>Jürgensen</surname><given-names>L</given-names></name>, <name><surname>Husstedt</surname><given-names>H</given-names></name>, <name><surname>Denk</surname><given-names>F</given-names></name>. <source>Comparison of an open-source hearing aid prototype with commercially available hearing aids</source>. In: <conf-name>8th International Symposium on Auditory and Audiological Research</conf-name>. <year>2021</year>.</mixed-citation>
    </ref>
    <ref id="R29">
      <label>[29]</label>
      <mixed-citation publication-type="journal"><name><surname>Dasenbrock</surname><given-names>S</given-names></name>, <name><surname>Blum</surname><given-names>S</given-names></name>, <name><surname>Debener</surname><given-names>S</given-names></name>, <name><surname>Hohmann</surname><given-names>V</given-names></name>, <name><surname>Kayser</surname><given-names>H</given-names></name>. <article-title>A step towards neuro-steered hearing aids: Integrated portable setup for time-synchronized acoustic stimuli presentation and EEG recording</article-title>. <source>Curr Dir Biomed Eng</source><year>2021</year>;<volume>7</volume>(<issue>2</issue>):<fpage>855</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="R30">
      <label>[30]</label>
      <mixed-citation publication-type="journal"><name><surname>Pisha</surname><given-names>L</given-names></name>, <name><surname>Warchall</surname><given-names>J</given-names></name>, <name><surname>Zubatiy</surname><given-names>T</given-names></name>, <name><surname>Hamilton</surname><given-names>S</given-names></name>, <name><surname>Lee</surname><given-names>CH</given-names></name>, <name><surname>Chockalingam</surname><given-names>G</given-names></name>, <etal/><article-title>A wearable, extensible, open-source platform for hearing healthcare research</article-title>. <source>IEEE Access</source><year>2019</year>;<volume>7</volume>:162083–101. <pub-id pub-id-type="doi">10.1109/ACCESS.2019.2951145</pub-id>.</mixed-citation>
    </ref>
    <ref id="R31">
      <label>[31]</label>
      <mixed-citation publication-type="journal"><name><surname>Grimm</surname><given-names>G</given-names></name>, <name><surname>Herzke</surname><given-names>T</given-names></name>, <name><surname>Berg</surname><given-names>D</given-names></name>, <name><surname>Hohmann</surname><given-names>V</given-names></name>. <article-title>The master hearing aid: a PC-based platform for algorithm development and evaluation</article-title>. <source>Acta Acust. United Acust</source>. <year>2006</year>;<volume>92</volume>:<fpage>618</fpage>–<lpage>28</lpage>.</mixed-citation>
    </ref>
    <ref id="R32">
      <label>[32]</label>
      <mixed-citation publication-type="journal"><name><surname>Stone</surname><given-names>MA</given-names></name>, <name><surname>Moore</surname><given-names>BC</given-names></name>, <name><surname>Meisenbacher</surname><given-names>K</given-names></name>, <name><surname>Derleth</surname><given-names>R</given-names></name>. <article-title>Tolerable hearing aid delays. V. Estimation of limits for open canal fittings</article-title>. <source>Ear Hear</source><year>2008</year>;<volume>29</volume>. <pub-id pub-id-type="doi">10.1097/AUD.0b013e3181734ef2</pub-id>.</mixed-citation>
    </ref>
    <ref id="R33">
      <label>[33]</label>
      <mixed-citation publication-type="webpage"><collab>MathWorks</collab>. <source>MATLAB Coder</source>, URL <comment><ext-link xlink:href="https://www.mathworks.com/products/matlab-coder.html" ext-link-type="uri">https://www.mathworks.com/products/matlab-coder.html</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R34">
      <label>[34]</label>
      <mixed-citation publication-type="confproc"><name><surname>Breithaupt</surname><given-names>C</given-names></name>, <name><surname>Gerkmann</surname><given-names>T</given-names></name>, <name><surname>Martin</surname><given-names>R</given-names></name>. <source>A novel a priori SNR estimation approach based on selective cepstro-temporal smoothing</source>. In: <conf-name>2008 IEEE International Conference on Acoustics, Speech and Signal Processing</conf-name>. <year>2008</year>, p. <fpage>4897</fpage>–<lpage>900</lpage>. <pub-id pub-id-type="doi">10.1109/ICASSP.2008.4518755</pub-id>.</mixed-citation>
    </ref>
    <ref id="R35">
      <label>[35]</label>
      <mixed-citation publication-type="confproc"><name><surname>Elko</surname><given-names>GW</given-names></name>, <name><surname>Pong</surname><given-names>A-TN</given-names></name>. <source>A simple adaptive first-order differential microphone</source>. In: <conf-name>Proceedings of 1995 Workshop on Applications of Signal Processing to Audio and Accoustics</conf-name>. <year>1995</year>. p. <fpage>169</fpage>–<lpage>172</lpage>.</mixed-citation>
    </ref>
    <ref id="R36">
      <label>[36]</label>
      <mixed-citation publication-type="journal"><name><surname>Moore</surname><given-names>B</given-names></name>, <name><surname>Alcántara</surname><given-names>JI</given-names></name>, <name><surname>Stone</surname><given-names>M</given-names></name>, <name><surname>Glasberg</surname><given-names>B</given-names></name>. <article-title>Use of a loudness model for hearing aid fitting: II. Hearing aids with multi-channel compression</article-title>. <source>Br J Audiol</source><year>1999</year>;<volume>33</volume>:<fpage>157</fpage>–<lpage>70</lpage>. <pub-id pub-id-type="doi">10.3109/03005369909090095</pub-id>.<pub-id pub-id-type="pmid">10439142</pub-id></mixed-citation>
    </ref>
    <ref id="R37">
      <label>[37]</label>
      <mixed-citation publication-type="webpage"><collab>University of California San Diego</collab>. <source>Open speech platform - a real-time, open, portable, extensible speech lab</source>, URL <comment><ext-link xlink:href="https://openspeechplatform.ucsd.edu" ext-link-type="uri">https://openspeechplatform.ucsd.edu</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R38">
      <label>[38]</label>
      <mixed-citation publication-type="webpage"><source>Node-RED</source>, URL <comment><ext-link xlink:href="https://nodered.org/" ext-link-type="uri">https://nodered.org/</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R39">
      <label>[39]</label>
      <mixed-citation publication-type="confproc"><name><surname>Grimm</surname><given-names>G</given-names></name>, <name><surname>Herzke</surname><given-names>T</given-names></name>, <name><surname>Ewert</surname><given-names>S</given-names></name>, <name><surname>Hohmann</surname><given-names>V</given-names></name>. <source>Implementation and evaluation of an experimental hearing aid dynamic range compressor gain prescription</source>. In: <conf-name>DAGA 2015</conf-name>. <year>2015</year>. p. <fpage>996</fpage>–<lpage>999</lpage>.</mixed-citation>
    </ref>
    <ref id="R40">
      <label>[40]</label>
      <mixed-citation publication-type="journal"><name><surname>Schepker</surname><given-names>H</given-names></name>, <name><surname>Doclo</surname><given-names>S</given-names></name>. <article-title>A semidefinite programming approach to min-max estimation of the common part of acoustic feedback paths in hearing aids</article-title>. <source>IEEE/ACM Trans Audio Speech Lang Process</source><year>2016</year>;<volume>24</volume>(<issue>2</issue>):<fpage>366</fpage>–<lpage>77</lpage>. <pub-id pub-id-type="doi">10.1109/TASLP.2015.2507940</pub-id>.</mixed-citation>
    </ref>
    <ref id="R41">
      <label>[41]</label>
      <mixed-citation publication-type="journal"><name><surname>Grimm</surname><given-names>G</given-names></name>, <name><surname>Hohmann</surname><given-names>V</given-names></name>, <name><surname>Kollmeier</surname><given-names>B</given-names></name>. <article-title>Increase and subjective evaluation of feedback stability in hearing aids by a binaural coherence-based noise reduction scheme</article-title>. <source>IEEE/ACM Trans Audio Speech Lang Process</source><year>2009</year>;<volume>17</volume>(<issue>7</issue>):<fpage>1408</fpage>–<lpage>19</lpage>.</mixed-citation>
    </ref>
    <ref id="R42">
      <label>[42]</label>
      <mixed-citation publication-type="confproc"><name><surname>Rohdenburg</surname><given-names>T</given-names></name>, <name><surname>Hohmann</surname><given-names>V</given-names></name>, <name><surname>Kollmeier</surname><given-names>B</given-names></name>. <source>Robustness analysis of binaural hearing aid beamformer algorithms by means of objective perceptual quality measures</source>. In: <conf-name>2007 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</conf-name>. <year>2007</year>, p. <fpage>315</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1109/ASPAA.2007.4393016</pub-id>.</mixed-citation>
    </ref>
    <ref id="R43">
      <label>[43]</label>
      <mixed-citation publication-type="journal"><name><surname>Adiloğlu</surname><given-names>K</given-names></name>, <name><surname>Kayser</surname><given-names>H</given-names></name>, <name><surname>Baumgärtel</surname><given-names>RM</given-names></name>, <name><surname>Rennebeck</surname><given-names>S</given-names></name>, <name><surname>Dietz</surname><given-names>M</given-names></name>, <name><surname>Hohmann</surname><given-names>V</given-names></name>. <article-title>A binaural steering beamformer system for enhancing a moving speech source</article-title>. <source>Trends Hear</source><year>2015</year>;<volume>19</volume>. <pub-id pub-id-type="doi">10.1177/2331216515618903</pub-id>.</mixed-citation>
    </ref>
    <ref id="R44">
      <label>[44]</label>
      <mixed-citation publication-type="journal"><name><surname>Baumgärtel</surname><given-names>RM</given-names></name>, <name><surname>Krawczyk-Becker</surname><given-names>M</given-names></name>, <name><surname>Marquardt</surname><given-names>D</given-names></name>, <name><surname>Völker</surname><given-names>C</given-names></name>, <name><surname>Hu</surname><given-names>H</given-names></name>, <name><surname>Herzke</surname><given-names>T</given-names></name>, <etal/><article-title>Comparing binaural pre-processing strategies I: Instrumental evaluation</article-title>. <source>Trends Hear</source><year>2015</year>;<volume>19</volume>. <pub-id pub-id-type="doi">10.1177/2331216515617916</pub-id>.</mixed-citation>
    </ref>
    <ref id="R45">
      <label>[45]</label>
      <mixed-citation publication-type="confproc"><name><surname>Kayser</surname><given-names>H</given-names></name>, <name><surname>Anemüller</surname><given-names>J</given-names></name>. <source>A discriminative learning approach to probabilistic acoustic source localization</source>. In: <conf-name>2014 14th International Workshop on Acoustic Signal Enhancement</conf-name>. <year>2014</year>, p. <fpage>99</fpage>–<lpage>103</lpage>. <pub-id pub-id-type="doi">10.1109/IWAENC.2014.6953346</pub-id>.</mixed-citation>
    </ref>
    <ref id="R46">
      <label>[46]</label>
      <mixed-citation publication-type="confproc"><name><surname>Schmeder</surname><given-names>A</given-names></name>, <name><surname>Freed</surname><given-names>A</given-names></name>, <name><surname>Wessel</surname><given-names>D</given-names></name>. <source>Best practices for open sound control</source>. In: <conf-name>Linux Audio Conference</conf-name>. <year>2010</year>.</mixed-citation>
    </ref>
    <ref id="R47">
      <label>[47]</label>
      <mixed-citation publication-type="book">for <source>Computational Neuroscience SC</source>, <name><surname>Kothe</surname><given-names>C</given-names></name>. <publisher-name>Lab streaming layer</publisher-name>, URL <comment><ext-link xlink:href="https://github.com/sccn/labstreaminglayer" ext-link-type="uri">https://github.com/sccn/labstreaminglayer</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R48">
      <label>[48]</label>
      <mixed-citation publication-type="journal"><name><surname>Davis</surname><given-names>P</given-names></name>. <source>Jack audio connection kit</source>. <year>2003</year>, URL <comment><ext-link xlink:href="http://jackaudio.org/" ext-link-type="uri">http://jackaudio.org/</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R49">
      <label>[49]</label>
      <mixed-citation publication-type="webpage"><source>Github</source>, URL <comment><ext-link xlink:href="https://github.com/HoerTech-gGmbH/openMHA" ext-link-type="uri">https://github.com/HoerTech-gGmbH/openMHA</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R50">
      <label>[50]</label>
      <mixed-citation publication-type="webpage"><source>openMHA user forum</source>, URL <comment><ext-link xlink:href="https://forum.openmha.org/" ext-link-type="uri">https://forum.openmha.org/</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R51">
      <label>[51]</label>
      <mixed-citation publication-type="webpage"><source>Phabricator</source>, URL <comment><ext-link xlink:href="https://www.phacility.com/" ext-link-type="uri">https://www.phacility.com/</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R52">
      <label>[52]</label>
      <mixed-citation publication-type="webpage"><source>Git</source>, URL <comment><ext-link xlink:href="https://git-scm.com" ext-link-type="uri">https://git-scm.com</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R53">
      <label>[53]</label>
      <mixed-citation publication-type="webpage"><source>Googletest</source>, URL <comment><ext-link xlink:href="https://github.com/google/googletest" ext-link-type="uri">https://github.com/google/googletest</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R54">
      <label>[54]</label>
      <mixed-citation publication-type="webpage"><source>Make</source>, URL <comment><ext-link xlink:href="https://www.gnu.org/software/make" ext-link-type="uri">https://www.gnu.org/software/make</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R55">
      <label>[55]</label>
      <mixed-citation publication-type="webpage"><source>Jenkins</source>, URL <comment><ext-link xlink:href="https://www.jenkins.io" ext-link-type="uri">https://www.jenkins.io</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R56">
      <label>[56]</label>
      <mixed-citation publication-type="webpage"><source>GNU GCC compiler</source>, URL <comment><ext-link xlink:href="http://gcc.gnu.org" ext-link-type="uri">http://gcc.gnu.org</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R57">
      <label>[57]</label>
      <mixed-citation publication-type="webpage"><source>mingw-w64</source>, URL <comment><ext-link xlink:href="http://mingw-w64.org" ext-link-type="uri">http://mingw-w64.org</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R58">
      <label>[58]</label>
      <mixed-citation publication-type="webpage"><source>Clang</source>, URL <comment><ext-link xlink:href="https://clang.llvm.org" ext-link-type="uri">https://clang.llvm.org</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R59">
      <label>[59]</label>
      <mixed-citation publication-type="confproc"><name><surname>Herzke</surname><given-names>T</given-names></name>, <name><surname>Kayser</surname><given-names>H</given-names></name>, <name><surname>Seifert</surname><given-names>C</given-names></name>, <name><surname>Maanen</surname><given-names>P</given-names></name>, <name><surname>Obbard</surname><given-names>C</given-names></name>, <name><surname>Payá-Vayá</surname><given-names>G</given-names></name>, <etal/><source>Open hardware multichannel sound interface for hearing aid research on BeagleBone black with openMHA: Cape4all</source>. In: <conf-name>Linux Audio Conference</conf-name>. <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="R60">
      <label>[60]</label>
      <mixed-citation publication-type="confproc"><name><surname>Obbard</surname><given-names>C</given-names></name>, <name><surname>James</surname><given-names>D</given-names></name>. <source>PREEMPT_RT isn’t just for lasers: The perfect match for hearing aid research!</source> In: <conf-name>Embedded Linux Conference</conf-name>. <year>2018</year>,</mixed-citation>
    </ref>
    <ref id="R61">
      <label>[61]</label>
      <mixed-citation publication-type="journal"><name><surname>Keidser</surname><given-names>G</given-names></name>, <name><surname>Naylor</surname><given-names>G</given-names></name>, <name><surname>Brungart</surname><given-names>DS</given-names></name>, <name><surname>Caduff</surname><given-names>A</given-names></name>, <name><surname>Campos</surname><given-names>J</given-names></name>, <name><surname>Carlile</surname><given-names>S</given-names></name>, <etal/><article-title>The quest for ecological validity in hearing science: What it is, why it matters, and how to advance it</article-title>. <source>Ear Hear</source><year>2020</year>;<volume>41</volume>(Supplement <issue>1</issue>):<fpage>5</fpage>–<lpage>19</lpage>. <pub-id pub-id-type="doi">10.1097/AUD.0000000000000944</pub-id>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="F1">
    <label>Fig. 1.</label>
    <caption>
      <p id="P75">openMHA software architecture visualization. Black arrows: object ownership. Dashed black arrow: Some plugins contain themselves another Processing_Plugin_Loader instance by which they can load child plugins. Green arrows: audio signal path. Audio signal can have arbitrary number of audio channels. Arrow direction depicts incoming signal, the processed output signal travels in the reverse direction along the same path. Purple arrows: Configuration command execution path. Responses to configuration commands travel along the same path in reverse direction. Rectangular objects employ the configuration language parser to interpret or propagate configuration commands. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</p>
    </caption>
    <graphic xlink:href="nihms-1784836-f0001" position="float"/>
  </fig>
  <fig position="float" id="F2">
    <label>Fig. 2.</label>
    <caption>
      <p id="P76">Graphical user interface of the openMHA Octave/Matlab fitting tool.</p>
    </caption>
    <graphic xlink:href="nihms-1784836-f0002" position="float"/>
  </fig>
  <fig position="float" id="F3">
    <label>Fig. 3.</label>
    <caption>
      <p id="P77">The “Portable Hearing Laboratory” (PHL, [<xref rid="R27" ref-type="bibr">27</xref>]).</p>
    </caption>
    <graphic xlink:href="nihms-1784836-f0003" position="float"/>
  </fig>
  <fig position="float" id="F4">
    <label>Fig. 4.</label>
    <caption>
      <p id="P78">GUI running on the Portable Hearing Laboratory (PHL), containing three different tabs. The interface is accessible through a web browser from a smartphone, tablet or computer connected to the PHL via wifi. The left panel <italic toggle="yes">System control</italic> provides access to headset, microphones and gain settings. The center panel <italic toggle="yes">Hearing Aid Processing</italic> is used to control the hearing aid processing — activation/deactivation of single processing stages and modification of the compression ratio. The resulting broadband output level is monitored at the bottom. The right panel <italic toggle="yes">Signal generator</italic> allows to generate noise and pure tone signals in the hearing aid with controllable level and frequency.</p>
    </caption>
    <graphic xlink:href="nihms-1784836-f0004" position="float"/>
  </fig>
  <fig position="float" id="F5">
    <label>Fig. 5.</label>
    <caption>
      <p id="P79">Hearing aid signal processing chain provided with the Portable Hearing Laboratory (PHL) system software MAHALIA.</p>
    </caption>
    <graphic xlink:href="nihms-1784836-f0005" position="float"/>
  </fig>
</floats-group>
