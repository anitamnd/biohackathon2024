<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6787990</article-id>
    <article-id pub-id-type="publisher-id">3108</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-019-3108-7</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DECA: scalable XHMM exome copy-number variant calling with ADAM and Apache Spark</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9643-7148</contrib-id>
        <name>
          <surname>Linderman</surname>
          <given-names>Michael D.</given-names>
        </name>
        <address>
          <email>mlinderman@middlebury.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chia</surname>
          <given-names>Davin</given-names>
        </name>
        <address>
          <email>davinchia@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wallace</surname>
          <given-names>Forrest</given-names>
        </name>
        <address>
          <email>forrest.wallace.vt@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nothaft</surname>
          <given-names>Frank A.</given-names>
        </name>
        <address>
          <email>frank.nothaft@databricks.com</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9743 9925</institution-id><institution-id institution-id-type="GRID">grid.260002.6</institution-id><institution>Department of Computer Science, </institution><institution>Middlebury College, </institution></institution-wrap>75 Shannon St, Middlebury, VT 05753 USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2181 7878</institution-id><institution-id institution-id-type="GRID">grid.47840.3f</institution-id><institution>AMPLab, University of California, Berkeley, </institution></institution-wrap>Berkeley, CA USA </aff>
      <aff id="Aff3"><label>3</label>Databricks, Inc., San Francisco, CA USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>11</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>11</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <elocation-id>493</elocation-id>
    <history>
      <date date-type="received">
        <day>24</day>
        <month>6</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>9</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s). 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">XHMM is a widely used tool for copy-number variant (CNV) discovery from whole exome sequencing data but can require hours to days to run for large cohorts. A more scalable implementation would reduce the need for specialized computational resources and enable increased exploration of the configuration parameter space to obtain the best possible results.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">DECA is a horizontally scalable implementation of the XHMM algorithm using the ADAM framework and Apache Spark that incorporates novel algorithmic optimizations to eliminate unneeded computation. DECA parallelizes XHMM on both multi-core shared memory computers and large shared-nothing Spark clusters. We performed CNV discovery from the read-depth matrix in 2535 exomes in 9.3 min on a 16-core workstation (35.3× speedup vs. XHMM), 12.7 min using 10 executor cores on a Spark cluster (18.8× speedup vs. XHMM), and 9.8 min using 32 executor cores on Amazon AWS’ Elastic MapReduce. We performed CNV discovery from the original BAM files in 292 min using 640 executor cores on a Spark cluster.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">We describe DECA’s performance, our algorithmic and implementation enhancements to XHMM to obtain that performance, and our lessons learned porting a complex genome analysis application to ADAM and Spark. ADAM and Apache Spark are a performant and productive platform for implementing large-scale genome analyses, but efficiently utilizing large clusters can require algorithmic optimizations and careful attention to Spark’s configuration parameters.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Exome sequencing</kwd>
      <kwd>Copy-number variation</kwd>
      <kwd>High-performance computing</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>National Science Foundation (US)</institution>
        </funding-source>
        <award-id>CCF-1139158</award-id>
        <principal-award-recipient>
          <name>
            <surname>Nothaft</surname>
            <given-names>Frank A.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Lawrence Berkeley National Laboratory (US)</institution>
        </funding-source>
        <award-id>7076018</award-id>
        <principal-award-recipient>
          <name>
            <surname>Nothaft</surname>
            <given-names>Frank A.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000185</institution-id>
            <institution>Defense Advanced Research Projects Agency</institution>
          </institution-wrap>
        </funding-source>
        <award-id>FA8750-12-2-0331</award-id>
        <principal-award-recipient>
          <name>
            <surname>Nothaft</surname>
            <given-names>Frank A.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000051</institution-id>
            <institution>National Human Genome Research Institute</institution>
          </institution-wrap>
        </funding-source>
        <award-id>U54HG007990-01</award-id>
        <principal-award-recipient>
          <name>
            <surname>Nothaft</surname>
            <given-names>Frank A.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id>
            <institution>National Institutes of Health</institution>
          </institution-wrap>
        </funding-source>
        <award-id>HHSN261201400006C</award-id>
        <principal-award-recipient>
          <name>
            <surname>Nothaft</surname>
            <given-names>Frank A.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par12">XHMM [<xref ref-type="bibr" rid="CR1">1</xref>] is a widely used tool for copy-number variant (CNV) discovery from whole exome sequencing (WES) data, but can require hours to days of computation to complete for larger cohorts. For example, XHMM analysis of 59,898 samples in the ExAC cohort required “800 GB of RAM and ~1 month of computation time” for the principal component analysis (PCA) component of the algorithm [<xref ref-type="bibr" rid="CR2">2</xref>]. Substantial execution time and memory footprints require users to obtain correspondingly substantial computational resources and limit opportunities to explore the configuration parameter space to obtain the best possible results.</p>
    <p id="Par13">Numerous algorithms have been developed for WES CNV discovery (see [<xref ref-type="bibr" rid="CR3">3</xref>] for a review), including the recent CLAMMS [<xref ref-type="bibr" rid="CR4">4</xref>] algorithm, which was specifically designed for large cohorts. Although XHMM was not specifically designed for large cohorts, the example above shows it is being actively used on some of the largest cohorts in existence. Our focus was to: 1) improve the performance of this widely used tool for its many users; and 2) report on the process of implementing a complex genome analysis for on-premises and cloud-based distributed computing environments using the ADAM framework and Apache Spark.</p>
    <p id="Par14">ADAM is an in-memory distributed computing framework for genome analysis built with Apache Spark [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR6">6</xref>]. In addition to ADAM, multiple tools, including GATK 4, have (re)implemented genomic variant analyses with Spark [<xref ref-type="bibr" rid="CR7">7</xref>–<xref ref-type="bibr" rid="CR14">14</xref>] (see [<xref ref-type="bibr" rid="CR15">15</xref>] for a review of genomics tools implemented with Spark). The common motivation for using Spark is automatic and generalizable scalability; operations over Spark’s partitioned collections of elements, termed resilient distributed datasets (RDD), can be automatically distributed by the Spark runtime across the available computing resources on a variety of computer systems from multicore workstations to (cloud-based) share-nothing clusters [<xref ref-type="bibr" rid="CR16">16</xref>]. In contrast, many current genome analysis tools are parallelized by partitioning input files (either physically or via coordinate-sorted indices) stored on a shared file system. Relying on a shared file system for parallel execution introduces I/O overhead, excludes the use of scalable shared-nothing cluster architectures, and makes it difficult to port applications to cloud computing platforms.</p>
    <p id="Par15">Here we present DECA, a horizontally scalable implementation of XHMM using ADAM and Apache Spark. XHMM is not parallelized, although the user could partition the input files for specific steps themselves and invoke multiple instances of the XHMM executable. In contrast, as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>a, DECA parallelizes each step of the XHMM algorithm by sample and/or file region to improve execution time compared to the original XHMM implementation and a manually parallelized version of XHMM on a wide variety of computer systems, including in the cloud, while keeping the memory footprint within the resources of a typical compute node (16-256GB). Our secondary goal was to explore the utility of implementing complex genome analyses with ADAM and Apache Spark and report our “lessons learned” parallelizing XHMM with these technologies.
<fig id="Fig1"><label>Fig. 1</label><caption><p>DECA parallelization and performance. <bold>a</bold> DECA parallelization (shown by dashed outline) and data flow. The normalization and discovery steps are parallelized by sample (rows of the samples (s) × targets(t) read-depth matrix). The inputs and outputs of the different components are shown with thinner arrows. <bold>b</bold> DECA and XHMM execution time starting from the read-depth matrix for <italic>s</italic> = 2535 on both the workstation and on-premises Hadoop cluster for different numbers of executor cores. Mod. XHMM is a customized XHMM implementation that partitions the discovery input files and invokes XHMM in parallel. <bold>c</bold> DECA execution time for coverage and CNV discovery for different numbers of samples using the entire workstation (16 cores) and cluster (approximately 640 executor cores dynamically allocated by Spark)</p></caption><graphic xlink:href="12859_2019_3108_Fig1_HTML" id="MO1"/></fig></p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <p id="Par16">DECA implements the three steps of the XHMM algorithm shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>a: 1) target coverage calculation (to produce the read-depth matrix), 2) PCA normalization and filtering, and 3) CNV discovery by hidden Markov model (HMM) Viterbi decoding. XHMM is designed to use the GATK per-target coverage already calculated as part of a typical genome analysis workflow. DECA can also use a GATK per-target coverage file or can calculate the coverage directly from the original coordinate-sorted BAM files (read via Hadoop-BAM [<xref ref-type="bibr" rid="CR17">17</xref>]).</p>
    <p id="Par17">DECA implements the XHMM algorithm as a sequence of map, reduce and broadcast operations over RDDs, e.g. the rows of the read depth matrix (each row is a sample) or chunks of a BAM file, which define the operations that are independent and potentially parallelizable. Spark splits this program over RDDs into jobs (all of the actions performed between reading and writing data), splits jobs into stages (all of the actions bound by IO or communication) that must be sequentially executed, and stages into tasks (atomic units of computation which are distributed across the cluster for execution). Spark automatically and transparently partitions those RDDs and the associated computational tasks (expressed as a task graph) across the available computing resources on the different platforms. There is a single DECA implementation used with all platforms, although, as described below, the user may need to adjust the partition sizes (via command line parameters) to achieve the best possible performance on different platforms.</p>
    <p id="Par18">For example, the rows of read-depth matrix (<italic>s</italic> sample<italic>s</italic> × <italic>t</italic> targets) are typically partitioned across the worker nodes and remain resident on a single worker node throughout the entire computation (i.e. computation is sent to the data). Computations over the read depths are performed in parallel on the worker nodes with only summary statistics, e.g. per-target means, communicated between nodes (by reducing from workers to the driver and then broadcasting from the driver to the workers). The first stage of the read depth calculation job reads chunks of the BAM file (via Hadoop-BAM), assigns reads to targets, and local to each task, computes the number of reads assigned to that target from that task. Between the first and second stage, the Spark workers “shuffle” the intermediate counts over the network to co-locate all coverage counts for a given target on the same node before computing the final counts in the second stage (which are either written to storage or consumed by subsequent jobs).</p>
    <p id="Par19">Identifying and removing systematic biases is a key step in WES CNV calling. To do so, XHMM performs singular value decomposition (SVD) on the filtered and centered read-depth matrix (<italic>s</italic> sample<italic>s</italic> × <italic>t</italic> targets) and removes (by default) <italic>K</italic> components with relative variance greater than 0.7 / <italic>n</italic> (for <italic>n</italic> components) that are correlated with systematic biases. Specifically, XHMM removes the <italic>K</italic> components with variance, <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {v}_i={\sigma}_i^2 $$\end{document}</tex-math><mml:math id="M2" display="inline"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_3108_Article_IEq1.gif"/></alternatives></inline-formula> that satisfy this condition:
<disp-formula id="Equa"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {v}_i\ge \frac{0.7\sum v}{n} $$\end{document}</tex-math><mml:math id="M4" display="block"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:mfrac><mml:mrow><mml:mn>0.7</mml:mn><mml:mo>∑</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:math><graphic xlink:href="12859_2019_3108_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p>
    <p id="Par20">In practice <italic>K</italic> &lt; &lt; <italic>n</italic>. XHMM computes all <italic>n</italic> components; however, we can identically determine <italic>K</italic> by estimating the total variance from <italic>k &lt; n</italic> components, reducing the time and memory required for SVD. DECA employs a novel iterative algorithm that initially performs SVD with a small <italic>k</italic> (<italic>n /</italic> 10 by default) and increases <italic>k</italic> until the estimate of the total variance is sufficiently precise to determine <italic>K</italic>. For <italic>n</italic> = 2498, for example, DECA computes <italic>k</italic> = 250 components (instead of 2498) to remove <italic>K</italic> = 27. This iterative approach does not change the number of components removed during PCA normalization, or the effect of the normalization step compared to XHMM; instead this algorithmic optimization reduces the computational requirements for determining the number of components to remove.</p>
    <p id="Par21">Specifically, we can estimate the total variance as:
<disp-formula id="Equb"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \left(\sum \limits_{i=1}^k{v}_i\right)+\left(n-k-1\right){v}_k $$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mfenced close=")" open="("><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math><graphic xlink:href="12859_2019_3108_Article_Equb.gif" position="anchor"/></alternatives></disp-formula></p>
    <p id="Par22">Since <italic>v</italic><sub><italic>i</italic></sub> is monotonically decreasing, our estimate is necessarily greater than but approaching the total variance and thus our estimate for the cutoff to remove components is necessarily greater than but approaching the actual cutoff. Any component with <italic>v</italic><sub><italic>i</italic></sub> greater than this estimated cutoff will be removed. However, some components with <italic>v</italic><sub><italic>i</italic></sub> less than the “over” estimate could still also be removed. We can similarly compute a cutoff is that necessarily less than the actual cutoff, i.e. an “under” estimate, by assuming <italic>v</italic><sub><italic>i</italic> &gt; <italic>k</italic></sub> are 0. If the first component to be retained, i.e. the <italic>K</italic> + 1 component, has variance less than this “under” estimate, then we are guaranteed to have accurately determined K. The algorithm for determining <italic>K</italic> is shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Algorithm for determining <italic>K</italic> components to removing during PCA normalization</p></caption><graphic xlink:href="12859_2019_3108_Fig2_HTML" id="MO2"/></fig></p>
    <p id="Par23">Figure <xref rid="Fig3" ref-type="fig">3</xref> shows <italic>K</italic>, the number of components to be removed, the minimum <italic>k</italic> to accurately determine <italic>K</italic>, and the actual <italic>k</italic> DECA uses for different numbers of initial samples in the cohort. Although <italic>k</italic> is generally small relative to <italic>n</italic> (less than 10%), for some datasets the minimum <italic>k</italic> to determine <italic>K</italic> can be much larger. Since re-computing the SVD is time consuming, users may consider increasing the initial <italic>k</italic> from the default of 10% of <italic>n</italic> to reduce the chance of needing to compute more components. Tuning the initial <italic>k</italic> is area of ongoing work.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Components to be removed in PCA normalization. <italic>K</italic> components to be removed during PCA normalization, minimum <italic>k</italic> components when computing the SVD to accurately determine <italic>K</italic>, and final <italic>k</italic> used by DECA for different numbers of initial samples for the XHMM default relative variance cutoff of 0.7 / <italic>n</italic></p></caption><graphic xlink:href="12859_2019_3108_Fig3_HTML" id="MO3"/></fig></p>
    <p id="Par24">To minimize the required memory for the Spark driver and executors, on a cluster DECA does not collect the entire read-depth matrix, <italic>O(st),</italic> to a single node and SVD is implemented using the distributed algorithm in Spark’s MLlib [<xref ref-type="bibr" rid="CR18">18</xref>] that requires <italic>O(t)</italic> storage on the executors and <italic>O(kt),</italic> where <italic>k is</italic> typically 0.1 <italic>s,</italic> storage on the driver (at the cost of <italic>O(k)</italic> passes).</p>
    <p id="Par25">To mitigate underflow when multiplying small probabilities in the HMM model, XHMM implements the HMM computation in log-space using the “log-sum-exp trick” and the long double floating point type. DECA similarly implements the Viterbi algorithm in log space, but implements the scaled versions of the forward and backward algorithms [<xref ref-type="bibr" rid="CR19">19</xref>]. The long double type is not available in the Java Virtual Machine and so all computations in DECA use double precision floating point.</p>
  </sec>
  <sec id="Sec3">
    <title>Results</title>
    <sec id="Sec4">
      <title>Performance evaluation</title>
      <p id="Par26">DECA was evaluated on the on-premises single node and cluster environments described in Table <xref rid="Tab1" ref-type="table">1</xref> and using Databricks and Elastic Map Reduce on Amazon AWS. Total wall-clock execution time is measured with the time utility. The execution times for individual phases are measured with timing functionality available in the ADAM library. However, due to the lazy construction and evaluation of the Spark task graph, the timing of individual phases is approximate. Specific parameters used for benchmarking are recorded in the source repository. Unless otherwise noted, all benchmarking was performed with DECA commit 0e4a424 and an unmodified copy of XHMM downloaded from the XHMM webpage [<xref ref-type="bibr" rid="CR20">20</xref>].
<table-wrap id="Tab1"><label>Table 1</label><caption><p>On-premises evaluation systems</p></caption><table frame="hsides" rules="groups"><tbody><tr><td>Workstation</td><td>16-core workstation with two 8-core 2.1 GHz Intel Xeon E5–2620 CPUs, 256 GB RAM, and 16 TB of HDD in 2 × −striped JBOD (four 4 TB 7200 RPM HDDs connected via 6Gbps SATA).</td></tr><tr><td>Cluster</td><td>56-node Hadoop cluster with 16-core nodes managed by YARN. Each node has two 8-core 2.6 GHz Intel Xeon E5–2670 CPUs, 256 GB RAM and 4 TB of HDD (four 1 TB 7200RPM HDDs connected via 6Gpbs SATA). Nodes are connected with two 1GbE connections and one switchable 10GbE/40Gbps IB connection to a 40GbE TOR switch. HDFS was configured with 128 MB blocks and a 2× replication factor.</td></tr></tbody></table></table-wrap></p>
      <p id="Par27">We called CNVs in the 1000 Genomes Project phase 3 WES data with <italic>s</italic> = 2535 samples and <italic>t</italic> = 191,396 exome targets [<xref ref-type="bibr" rid="CR21">21</xref>]. The <italic>s</italic> = 2535 read-depth matrix was generated from the 1000 Genomes Projects phase 3 WES data using GATK DepthOfCoverage [<xref ref-type="bibr" rid="CR22">22</xref>] according to the XHMM protocol [<xref ref-type="bibr" rid="CR23">23</xref>] using the target file provided by the 1000 Genomes project. Smaller numbers of samples were obtained by taking subsets of the <italic>s</italic> = 2535 read depth matrix. We excluded targets with extreme GC fraction or low complexity as described in the XHMM protocol. Following the typical usage for XHMM, the read-depth matrix included coverage for all targets and excluded targets were removed during normalization. When performing CNV discovery directly from BAM files with DECA, excluded targets were removed prior to generating the read-depth matrix. All values for user-settable parameters of XHMM were taken from the XHMM protocol.</p>
      <p id="Par28">Figure <xref rid="Fig1" ref-type="fig">1</xref>b shows execution time for DECA and XHMM starting from the tab-delimited read-depth matrix. We performed CNV calling on the entire 1000 Genomes phase 3 cohort (<italic>s</italic> = 2535) in 9.3 min on the 16-core workstation (35.3× speedup vs. XHMM) and 12.7 min using 10 executor cores (and 5 driver cores) on the cluster (18.8× speedup vs. XHMM). Note that CNV discovery alone only utilizes a small fraction of the 56-node cluster. DECA could readily scale to much larger cohorts on such a system.</p>
      <p id="Par29">As shown in the execution time breakdown, the speedup is driven by the more efficient HMM model and parallelization of SVD and the HMM model. Using a single workstation core, DECA is approximately 4.4× faster than XHMM. The DECA HMM implementation in isolation is approximately 25× faster than the XHMM HMM on a single workstation core and 325× when using 16 workstation cores.</p>
      <p id="Par30">As noted above, although XHMM itself is not parallelized, the inputs to the CNV discovery phase can be partitioned by the user and the XHMM executable invoked independently on each sub-file. To explore the scaling of this file-based approach, we implemented a parallel wrapper script for XHMM on the workstation. The execution time breakdown is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>b as “Mod. XHMM”. The modified XHMM is 5.6× faster than single-core XHMM when using 16 workstation cores, while DECA is 7.9× faster than single-core DECA. Overall DECA is 6.3× faster than the modified XHMM when using 16 workstation cores.</p>
      <p id="Par31">Figure <xref rid="Fig1" ref-type="fig">1</xref>c shows the total execution time to discover CNVs directly from the coordinate-sorted BAM files for different numbers of samples. DECA can call CNVs from the BAM files for the entire cohort in 4:52 (4 h and 52 min) utilizing up to 640 cores on the cluster. Execution time is dominated by the coverage calculations.</p>
      <p id="Par32">Figure <xref rid="Fig1" ref-type="fig">1</xref>c also shows the effect of DECA’s iterative algorithm for PCA normalization (discovery for <italic>s</italic> = 1500 requires more time than <italic>s</italic> = 2000 or <italic>s</italic> = 2535 due to iteratively computing more SVD components) and the performance variability of the shared cluster environment.</p>
      <p id="Par33">DECA can be run unmodified on cloud-based clusters such as Databricks [<xref ref-type="bibr" rid="CR24">24</xref>] and Amazon AWS’ Elastic MapReduce (EMR), reading from and writing data to Amazon S3. We called CNVs in the full <italic>s</italic> = 2535 cohort starting from the read-depth matrix in 12.3 min using 32 executor cores on Databricks on Amazon AWS with an estimated compute cost of less than $0.35. The Databricks cluster was comprised of four 8-core i3.2xlarge executor nodes and one 4-core i3.2xlarge driver node. We similarly called CNVs on Amazon EMR in 9.8 min using a cluster of four 8-core i3.2xlarge nodes (along with a m4.large master node) with an estimated compute cost of less than $0.35 (not including cluster startup time). We called CNVs directly from the coordinate-sorted BAM files, obtained via the 1000 Genomes public S3 bucket, using 512 executor cores on Amazon EMR in 12.6 h with a compute cost of approximately $225. The EMR cluster was comprised of 64 8-core i3.2xlarge executor nodes and one 4-core i3.2xlarge driver node. We sought to minimize costs for this much larger compute tasks and so used a conservative auto-scaling policy that slowly ramped up the cluster size from 3 to 64 instances over the span of two hours. For all AWS-based clusters we exclusively used spot instances to minimize costs.</p>
    </sec>
    <sec id="Sec5">
      <title>Comparison of DECA and XHMM results</title>
      <p id="Par34">Figure <xref rid="Fig4" ref-type="fig">4</xref>a shows the comparison of XHMM and DECA CNV calls for the full 1000 Genomes Project phase 3 WES dataset (<italic>s</italic> = 2535) when starting from the same read-depth matrix (<italic>t</italic> = 191,396). Of the 70,858 XHMM calls, 99.87% are called by DECA with identical copy number and breakpoints and a further 37 have an overlapping DECA call with the same copy number. Only 55 XHMM calls do not have an overlapping DECA call. We do not expect identical results between XHMM and DECA due to differences in numerical behavior when multiplying small probabilities in the HMM algorithms.
<fig id="Fig4"><label>Fig. 4</label><caption><p>Comparison between DECA and XHMM results. <bold>a</bold> Concordance of XHMM and DECA CNV calls for the full 1000 Genomes Project phase 3 WES dataset (<italic>s</italic> = 2535) when starting from the same read-depth matrix (<italic>t</italic> = 191,396). Exact matches have identical breakpoints and copy number, while overlap matches do not have identical breakpoints. <bold>b</bold> Range of Some Quality (SQ) scores computed by DECA compared to XHMM probability for exact matching variants</p></caption><graphic xlink:href="12859_2019_3108_Fig4_HTML" id="MO4"/></fig></p>
      <p id="Par35">The 55 XHMM-only events fall into two categories: 1) 50 events spanning just targets 1–3, 2) 5 events with Q_SOME quality scores (the phred-scaled probability that at least one target is deleted or duplicated) at XHMM’s default minimum calling threshold of 30. Most overlapping CNV calls only differ by 1 target (67.6%).</p>
      <p id="Par36">Figure <xref rid="Fig4" ref-type="fig">4</xref>b shows a comparison of the XHMM and DECA-calculated quality scores for the 70,766 exactly matching calls. The root mean square (RMS) error in Q_SOME for calls with a XHMM Q_SOME of less than 40 (i.e. those calls close to the calling threshold of 30) is 0.12; the RMS error is 2.04 for all of the calls.</p>
      <p id="Par37">DECA’s coverage calculation is designed to match the GATK DepthOfCoverage command specified in the XHMM protocol. As part of the protocol, the XHMM authors distribute a subset of the 1000 Genomes exome sequencing datasets, specifically reads covering 300 targets in 30 samples. For those 9000 targets, the DECA read-depth differed from the target coverage calculated with GATK 3.7–0-gcfedb67 for only three targets and by less than 0.02.</p>
    </sec>
  </sec>
  <sec id="Sec6">
    <title>Discussion</title>
    <p id="Par38">The primary goal was to make improvements to the performance and scalability of XHMM. Our secondary goal was to explore the utility of building complex genome analyses with ADAM and Apache Spark. Here we report our “lessons learned” re-implementing XHMM with these technologies:</p>
    <sec id="Sec7">
      <title>Library choice matters</title>
      <p id="Par39">XHMM uses LAPACK to perform SVD. The OpenBLAS implementation used here can be several-fold faster than the Netlib reference implementation linked from the XHMM webpage. Table <xref rid="Tab2" ref-type="table">2</xref> shows the speedup when linking XHMM against OpenBLAS. Switching LAPACK libraries could immediately benefit XHMM users.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Execution time for XHMM PCA step (--PCA) for different LAPACK libraries. Execution time and speedup for XHMM linked to NetLib and OpenBLAS libraries on the single node workstation using a single core</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Samples</th><th>NetLib Time (s)</th><th>OpenBLAS Time (s)</th><th>Speedup</th></tr></thead><tbody><tr><td>50</td><td>9.8</td><td>9.5</td><td>1.03</td></tr><tr><td>500</td><td>208.7</td><td>112.4</td><td>1.86</td></tr><tr><td>1000</td><td>568.5</td><td>241.5</td><td>2.35</td></tr><tr><td>1500</td><td>1150.6</td><td>398.5</td><td>2.89</td></tr><tr><td>2000</td><td>2000</td><td>585.6</td><td>3.42</td></tr><tr><td>2535</td><td>3178.2</td><td>819</td><td>3.88</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec8">
      <title>Spark makes exploiting “embarrassingly parallel” easy and generalizable, but algorithmic optimizations remain important</title>
      <p id="Par40">The initial DECA implementation obtained many-fold speedups, particularly for the “embarrassingly parallel” HMM model where each sample can be analyzed independently. Using Spark MLlib and other libraries we could quickly develop implementations for the PCA normalization and filtering steps that could scale to even larger cohorts. However, without optimizations to reduce <italic>k</italic>, the slower reduced-memory implementation of SVD would reduce possible speedups. Transitioning to a normalized implementation for the HMM forward and backward algorithms and double precision floating resulted in many-fold speedup of the discovery step with minimal differences in the quality scores calculated with those algorithms. The algorithmic optimizations are independent of Spark and could be applied to any XHMM implementation.</p>
    </sec>
    <sec id="Sec9">
      <title>Performance optimization depends on Spark-specific expertise</title>
      <p id="Par41">Improving application performance requires careful attention to distributed programming best practices, e.g. locality, but also Spark-specific expertise such as: RDD caching to avoid re-computation, RDDs vs. Spark SQL (the latter is reported to improve reduce performance, but did not for DECA), and defining performant values for the many Java Virtual Machine (JVM) and Spark configuration parameters to ensure sufficient numbers of tasks, efficient construction of the task graph, and efficient cluster resource utilization.</p>
      <p id="Par42">The two key parameters the user modifies to control concurrency are the number of partitions of the input data and the Spark minimum chunk size for the input. The former determines the minimum number of partitions created when reading the XHMM read-depth matrix from a file and is generally used to increase the number of tasks beyond the number of HDFS blocks (the default partitioning for HDFS files) for computationally intensive tasks. In contrast, when computing the coverage directly from BAM files, the total number of tasks can be in the thousands and needs to be reduced to efficiently construct the task graph. Setting the minimum chunks size larger than the HDFS block size reduces the number of tasks.</p>
    </sec>
  </sec>
  <sec id="Sec10">
    <title>Conclusion</title>
    <p id="Par43">Here we describe DECA, a horizontally scalable implementation of the widely used XHMM algorithm for CNV discovery, which parallelizes XHMM on multicore workstations and large on-premise and cloud-based share-nothing Hadoop clusters using ADAM and Apache Spark. Through a combination of parallelism, novel algorithmic enhancements and other optimizations, DECA achieves a 35-fold speedup compared to the existing XHMM implementation for calling CNVs in the 2535 sample 1000 Genomes exome cohort and can scale to even larger cohorts. By parallelizing all phases of the algorithm, DECA achieves better scaling than approaches based on file partitioning. DECA can be directly deployed on public clouds reducing the need for specialized computational resources to call CNVs in large WES cohorts. We found ADAM and Apache Spark to be a performant and productive platform for implementing large-scale genome analyses, but efficiently exploiting large clusters can require algorithmic optimizations and careful attention to Spark’s many configuration parameters.</p>
  </sec>
  <sec id="Sec11">
    <title>Availability and requirements</title>
    <p id="Par44">Project name: DECA</p>
    <p id="Par45">Project home page: <ext-link ext-link-type="uri" xlink:href="https://github.com/bigdatagenomics/deca">https://github.com/bigdatagenomics/deca</ext-link></p>
    <p id="Par46">Operating system(s): Platform independent</p>
    <p id="Par47">Programming language: Scala and Java</p>
    <p id="Par48">Other requirements: Maven, Spark 2.1.0+, Hadoop 2.7, Scala 2.11</p>
    <p id="Par49">License: Apache 2</p>
    <p id="Par50">Any restrictions for use by non-academics: None</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>CNV</term>
        <def>
          <p id="Par4">Copy number variation</p>
        </def>
      </def-item>
      <def-item>
        <term>HMM</term>
        <def>
          <p id="Par5">Hidden Markov Model</p>
        </def>
      </def-item>
      <def-item>
        <term>JVM</term>
        <def>
          <p id="Par6">Java Virtual Machine</p>
        </def>
      </def-item>
      <def-item>
        <term>PCA</term>
        <def>
          <p id="Par7">Principal Components Analysis</p>
        </def>
      </def-item>
      <def-item>
        <term>RDD</term>
        <def>
          <p id="Par8">Resilient Distributed Dataset</p>
        </def>
      </def-item>
      <def-item>
        <term>RMS</term>
        <def>
          <p id="Par9">Root mean square</p>
        </def>
      </def-item>
      <def-item>
        <term>SVD</term>
        <def>
          <p id="Par10">Singular-value Decomposition</p>
        </def>
      </def-item>
      <def-item>
        <term>WES</term>
        <def>
          <p id="Par11">Whole exome sequencing</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors gratefully acknowledge Menachem Fromer’s assistance with questions about XHMM.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>MDL and FAN conceptualized the project and wrote the manuscript. MDL, FAN, DC, and FW contributed to the development of DECA. All authors have read and approved the manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was supported by Middlebury College (to MDL, DC, and FW), and the sponsors of the AMPLab (<ext-link ext-link-type="uri" xlink:href="https://amplab.cs.berkeley.edu/amp-sponsors/">https://amplab.cs.berkeley.edu/amp-sponsors/</ext-link>) including NSF [CCF-1139158], LBNL [7076018], DARPA [FA8750-12-2-0331], NIH [U54HG007990–01, HHSN261201400006C] and a NSF Graduate Research Fellowship (to FAN). The funding sources had no role in the design of the study, the collection, analysis, and interpretation of data or in writing the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The datasets analyzed during the current study are available from the International Genome Sample Resource, <ext-link ext-link-type="uri" xlink:href="http://www.internationalgenome.org">http://www.internationalgenome.org</ext-link>.</p>
  </notes>
  <notes>
    <title>Ethics approval</title>
    <p id="Par51">Not applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p id="Par52">Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par53">FAN was a consultant for and is now employed by Databricks, Inc.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fromer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Moran</surname>
            <given-names>JL</given-names>
          </name>
          <name>
            <surname>Chambert</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Banks</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Bergen</surname>
            <given-names>SE</given-names>
          </name>
          <name>
            <surname>Ruderfer</surname>
            <given-names>DM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Discovery and statistical genotyping of copy-number variation from whole-exome sequencing depth</article-title>
        <source>Am J Hum Genet</source>
        <year>2012</year>
        <volume>91</volume>
        <fpage>597</fpage>
        <lpage>607</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ajhg.2012.08.005</pub-id>
        <?supplied-pmid 23040492?>
        <pub-id pub-id-type="pmid">23040492</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ruderfer</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Hamamsy</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Lek</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Karczewski</surname>
            <given-names>KJ</given-names>
          </name>
          <name>
            <surname>Kavanagh</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Samocha</surname>
            <given-names>KE</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Patterns of genic intolerance of rare copy number variation in 59,898 human exomes</article-title>
        <source>Nat Genet</source>
        <year>2016</year>
        <volume>48</volume>
        <fpage>1107</fpage>
        <lpage>1111</lpage>
        <pub-id pub-id-type="doi">10.1038/ng.3638</pub-id>
        <?supplied-pmid 27533299?>
        <pub-id pub-id-type="pmid">27533299</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Computational tools for copy number variation (CNV) detection using next-generation sequencing data: features and perspectives</article-title>
        <source>BMC Bioinformatics</source>
        <year>2013</year>
        <volume>14</volume>
        <issue>Suppl 11</issue>
        <fpage>S1</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-14-S11-S1</pub-id>
        <?supplied-pmid 24564169?>
        <pub-id pub-id-type="pmid">24564169</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Packer</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Maxwell</surname>
            <given-names>EK</given-names>
          </name>
          <name>
            <surname>O’Dushlaine</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Lopez</surname>
            <given-names>AE</given-names>
          </name>
          <name>
            <surname>Dewey</surname>
            <given-names>FE</given-names>
          </name>
          <name>
            <surname>Chernomorsky</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>CLAMMS: a scalable algorithm for calling common and rare copy number variants from exome sequencing data</article-title>
        <source>Bioinformatics</source>
        <year>2015</year>
        <volume>32</volume>
        <fpage>btv547</fpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btv547</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Nothaft</surname>
            <given-names>FA</given-names>
          </name>
          <name>
            <surname>Massie</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Danford</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Laserson</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Yeksigian</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Rethinking data-intensive science using scalable analytics systems</article-title>
        <source>Proceedings of the 2015 ACM SIGMOD international conference on Management of Data</source>
        <year>2015</year>
        <publisher-loc>Melbourne</publisher-loc>
        <publisher-name>ACM</publisher-name>
        <fpage>631</fpage>
        <lpage>646</lpage>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Massie</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nothaft</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Hartl</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Kozanitis</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Schumacher</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Joseph</surname>
            <given-names>AD</given-names>
          </name>
          <etal/>
        </person-group>
        <source>ADAM: Genomics Formats and Processing Patterns for Cloud Scale Computing</source>
        <year>2013</year>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wiewiórka</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Messina</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pacholewska</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Maffioletti</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gawrysiak</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Okoniewski</surname>
            <given-names>MJ</given-names>
          </name>
        </person-group>
        <article-title>SparkSeq: fast, scalable and cloud-ready tool for the interactive genomic data analysis with nucleotide precision</article-title>
        <source>Bioinformatics.</source>
        <year>2014</year>
        <volume>30</volume>
        <fpage>2652</fpage>
        <lpage>2653</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu343</pub-id>
        <?supplied-pmid 24845651?>
        <pub-id pub-id-type="pmid">24845651</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>O’Brien</surname>
            <given-names>AR</given-names>
          </name>
          <name>
            <surname>Saunders</surname>
            <given-names>NFW</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Buske</surname>
            <given-names>FA</given-names>
          </name>
          <name>
            <surname>Scott</surname>
            <given-names>RJ</given-names>
          </name>
          <name>
            <surname>Bauer</surname>
            <given-names>DC</given-names>
          </name>
        </person-group>
        <article-title>VariantSpark: population scale clustering of genotype information</article-title>
        <source>BMC Genomics</source>
        <year>2015</year>
        <volume>16</volume>
        <fpage>1052</fpage>
        <pub-id pub-id-type="doi">10.1186/s12864-015-2269-7</pub-id>
        <?supplied-pmid 26651996?>
        <pub-id pub-id-type="pmid">26651996</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Bahmani A, Sibley AB, Parsian M, Owzar K, Mueller F. SparkScore: Leveraging Apache Spark for Distributed Genomic Inference. In: 2016 IEEE international parallel and distributed processing symposium workshops (IPDPSW), vol. 2016: IEEE. p. 435–42. 10.1109/IPDPSW.2016.6.</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Li X, Tan G, Zhang C, Xu L, Zhang Z, Sun N. Accelerating large-scale genomic analysis with Spark. In: 2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM): IEEE; 2016. p. 747–51. 10.1109/BIBM.2016.7822614.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Hail. <ext-link ext-link-type="uri" xlink:href="https://github.com/hail-is/hail">https://github.com/hail-is/hail</ext-link>. Accessed 8 Jun 2018.</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>GT</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>DJ</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SEQSpark: a complete analysis tool for large-scale rare variant association studies using whole-genome and exome sequence data</article-title>
        <source>Am J Hum Genet</source>
        <year>2017</year>
        <volume>101</volume>
        <fpage>115</fpage>
        <lpage>122</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ajhg.2017.05.017</pub-id>
        <?supplied-pmid 28669402?>
        <pub-id pub-id-type="pmid">28669402</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Klein</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Bohrer</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Avelis</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Roberts</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Biospark: scalable analysis of large numerical datasets from biological simulations and experiments using Hadoop and spark</article-title>
        <source>Bioinformatics.</source>
        <year>2017</year>
        <volume>33</volume>
        <fpage>303</fpage>
        <lpage>305</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btw614</pub-id>
        <?supplied-pmid 27663493?>
        <pub-id pub-id-type="pmid">27663493</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Babadi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Benjamin</surname>
            <given-names>DI</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>SK</given-names>
          </name>
          <name>
            <surname>Smirnov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Chevalier</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lichtenstein</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Abstract 3580: GATK CNV: copy-number variation discovery from coverage data</article-title>
        <source>Cancer Res</source>
        <year>2017</year>
        <volume>77</volume>
        <issue>13 Supplement</issue>
        <fpage>3580</fpage>
        <pub-id pub-id-type="doi">10.1158/1538-7445.AM2017-3580</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Guo R, Zhao Y, Zou Q, Fang X, Peng S. Bioinformatics applications on apache spark. Gigascience. 2018;7. 10.1093/gigascience/giy098.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Zaharia</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Chowdhury</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Das</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Dave</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>McCauley</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing</article-title>
        <source>Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation</source>
        <year>2012</year>
        <fpage>2</fpage>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Niemenmaa</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kallio</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Schumacher</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Klemelä</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Korpelainen</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Heljanko</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Hadoop-BAM: directly manipulating next generation sequencing data in the cloud</article-title>
        <source>Bioinformatics.</source>
        <year>2012</year>
        <volume>28</volume>
        <fpage>876</fpage>
        <lpage>877</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts054</pub-id>
        <?supplied-pmid 22302568?>
        <pub-id pub-id-type="pmid">22302568</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Meng</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Bradley</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yavuz</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sparks</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Venkataraman</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>MLlib: machine learning in apache spark</article-title>
        <source>J Mach Learn Res</source>
        <year>2016</year>
        <volume>17</volume>
        <fpage>1</fpage>
        <lpage>7</lpage>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rabiner</surname>
            <given-names>LR</given-names>
          </name>
        </person-group>
        <article-title>A tutorial on hidden Markov models and selected applications in speech recognition</article-title>
        <source>Proc IEEE</source>
        <year>1989</year>
        <volume>77</volume>
        <fpage>257</fpage>
        <lpage>286</lpage>
        <pub-id pub-id-type="doi">10.1109/5.18626</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Fromer M, Purcell SM. XHMM. <ext-link ext-link-type="uri" xlink:href="https://atgu.mgh.harvard.edu/xhmm/index.shtml">https://atgu.mgh.harvard.edu/xhmm/index.shtml</ext-link>. Accessed 8 May 2019.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Auton</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Abecasis</surname>
            <given-names>GR</given-names>
          </name>
          <name>
            <surname>Altshuler</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Durbin</surname>
            <given-names>RM</given-names>
          </name>
          <name>
            <surname>Abecasis</surname>
            <given-names>GR</given-names>
          </name>
          <name>
            <surname>Bentley</surname>
            <given-names>DR</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A global reference for human genetic variation</article-title>
        <source>Nature.</source>
        <year>2015</year>
        <volume>526</volume>
        <fpage>68</fpage>
        <lpage>74</lpage>
        <pub-id pub-id-type="doi">10.1038/nature15393</pub-id>
        <?supplied-pmid 26432245?>
        <pub-id pub-id-type="pmid">26432245</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McKenna</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hanna</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Banks</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Sivachenko</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cibulskis</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kernytsky</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The genome analysis toolkit: a MapReduce framework for analyzing next-generation DNA sequencing data</article-title>
        <source>Genome Res</source>
        <year>2010</year>
        <volume>20</volume>
        <fpage>1297</fpage>
        <lpage>1303</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.107524.110</pub-id>
        <?supplied-pmid 20644199?>
        <pub-id pub-id-type="pmid">20644199</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fromer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Purcell</surname>
            <given-names>SM</given-names>
          </name>
        </person-group>
        <article-title>Using XHMM software to detect copy number variation in whole-exome sequencing data</article-title>
        <source>Curr Protoc Hum Genet</source>
        <year>2014</year>
        <volume>81</volume>
        <fpage>7.23.1</fpage>
        <lpage>7.23.21</lpage>
        <pub-id pub-id-type="doi">10.1002/0471142905.hg0723s81</pub-id>
        <pub-id pub-id-type="pmid">24763994</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Databricks Inc. Databricks. <ext-link ext-link-type="uri" xlink:href="https://databricks.com">https://databricks.com</ext-link>. Accessed 8 Jun 2018.</mixed-citation>
    </ref>
  </ref-list>
</back>
