<?properties open_access?>
<?subarticle pone.0226428.r001?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-id journal-id-type="pmc">plosone</journal-id>
    <journal-title-group>
      <journal-title>PLoS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6914332</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-19-24023</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0226428</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Cognitive Science</subject>
            <subj-group>
              <subject>Cognitive Psychology</subject>
              <subj-group>
                <subject>Decision Making</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Cognitive Psychology</subject>
            <subj-group>
              <subject>Decision Making</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Social Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Cognitive Psychology</subject>
            <subj-group>
              <subject>Decision Making</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Cognitive Science</subject>
            <subj-group>
              <subject>Cognition</subject>
              <subj-group>
                <subject>Decision Making</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Mathematical and Statistical Techniques</subject>
          <subj-group>
            <subject>Statistical Methods</subject>
            <subj-group>
              <subject>Forecasting</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Statistics</subject>
            <subj-group>
              <subject>Statistical Methods</subject>
              <subj-group>
                <subject>Forecasting</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Simulation and Modeling</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Data Visualization</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Probability Theory</subject>
            <subj-group>
              <subject>Probability Distribution</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Probability Theory</subject>
            <subj-group>
              <subject>Probability Distribution</subject>
              <subj-group>
                <subject>Normal Distribution</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Anatomy</subject>
          <subj-group>
            <subject>Brain</subject>
            <subj-group>
              <subject>Prefrontal Cortex</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Anatomy</subject>
          <subj-group>
            <subject>Brain</subject>
            <subj-group>
              <subject>Prefrontal Cortex</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Sensory Perception</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Sensory Perception</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Social Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Sensory Perception</subject>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>GLAMbox: A Python toolbox for investigating the association between gaze allocation and decision behaviour</article-title>
      <alt-title alt-title-type="running-head">GLAMbox</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3283-1090</contrib-id>
        <name>
          <surname>Molter</surname>
          <given-names>Felix</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Formal analysis</role>
        <role content-type="http://credit.casrai.org/">Software</role>
        <role content-type="http://credit.casrai.org/">Visualization</role>
        <role content-type="http://credit.casrai.org/">Writing – original draft</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff003">
          <sup>3</sup>
        </xref>
        <xref ref-type="aff" rid="aff004">
          <sup>4</sup>
        </xref>
        <xref ref-type="corresp" rid="cor001">*</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Thomas</surname>
          <given-names>Armin W.</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Formal analysis</role>
        <role content-type="http://credit.casrai.org/">Software</role>
        <role content-type="http://credit.casrai.org/">Visualization</role>
        <role content-type="http://credit.casrai.org/">Writing – original draft</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff003">
          <sup>3</sup>
        </xref>
        <xref ref-type="aff" rid="aff005">
          <sup>5</sup>
        </xref>
        <xref ref-type="aff" rid="aff006">
          <sup>6</sup>
        </xref>
        <xref ref-type="corresp" rid="cor001">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Heekeren</surname>
          <given-names>Hauke R.</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Supervision</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff003">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5179-060X</contrib-id>
        <name>
          <surname>Mohr</surname>
          <given-names>Peter N. C.</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Supervision</role>
        <role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff004">
          <sup>4</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>WZB Berlin Social Science Center, Berlin, Germany</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Center for Cognitive Neuroscience Berlin, Freie Universität Berlin, Berlin, Germany</addr-line>
    </aff>
    <aff id="aff003">
      <label>3</label>
      <addr-line>Department of Education and Psychology, Freie Universität Berlin, Germany</addr-line>
    </aff>
    <aff id="aff004">
      <label>4</label>
      <addr-line>School of Business and Economics, Freie Universität Berlin, Germany</addr-line>
    </aff>
    <aff id="aff005">
      <label>5</label>
      <addr-line>Department of Electrical Engineering and Computer Science, Technische Universität Berlin, Berlin, Germany</addr-line>
    </aff>
    <aff id="aff006">
      <label>6</label>
      <addr-line>Max Planck School of Cognition, Leipzig, Germany</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Smith</surname>
          <given-names>David V.</given-names>
        </name>
        <role>Editor</role>
        <xref ref-type="aff" rid="edit1"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Temple University, UNITED STATES</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>felixmolter@gmail.com</email> (FM); <email>athms.research@gmail.com</email> (AT)</corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>16</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <volume>14</volume>
    <issue>12</issue>
    <elocation-id>e0226428</elocation-id>
    <history>
      <date date-type="received">
        <day>26</day>
        <month>8</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>26</day>
        <month>11</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2019 Molter et al</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Molter et al</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0226428.pdf"/>
    <abstract>
      <p>Recent empirical findings have indicated that gaze allocation plays a crucial role in simple decision behaviour. Many of these findings point towards an influence of gaze allocation onto the speed of evidence accumulation in an accumulation-to-bound decision process (resulting in generally higher choice probabilities for items that have been looked at longer). Further, researchers have shown that the strength of the association between gaze and choice behaviour is highly variable between individuals, encouraging future work to study this association on the individual level. However, few decision models exist that enable a straightforward characterization of the gaze-choice association at the individual level, due to the high cost of developing and implementing them. The model space is particularly scarce for choice sets with more than two choice alternatives. Here, we present GLAMbox, a Python-based toolbox that is built upon PyMC3 and allows the easy application of the gaze-weighted linear accumulator model (GLAM) to experimental choice data. The GLAM assumes gaze-dependent evidence accumulation in a linear stochastic race that extends to decision scenarios with many choice alternatives. GLAMbox enables Bayesian parameter estimation of the GLAM for individual, pooled or hierarchical models, provides an easy-to-use interface to predict choice behaviour and visualize choice data, and benefits from all of PyMC3’s Bayesian statistical modeling functionality. Further documentation, resources and the toolbox itself are available at <ext-link ext-link-type="uri" xlink:href="https://glambox.readthedocs.io">https://glambox.readthedocs.io</ext-link>.</p>
    </abstract>
    <funding-group>
      <funding-statement>The Junior Professorship of P.N.C.M. as well as the associated Dahlem International Network Junior Research Group Neuroeconomics is supported by Freie Universität Berlin within the Excellence Initiative of the German Research Foundation (DFG). Further support for P.N.C.M. is provided by the WZB Berlin Social Science Center. F.M. is supported by the International Max Planck Research School on the Life Course (LIFE). A.T. is supported by the BMBF and Max Planck Society. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="8"/>
      <table-count count="2"/>
      <page-count count="23"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>All files are available at Github under <ext-link ext-link-type="uri" xlink:href="https://github.com/glamlab/glambox">https://github.com/glamlab/glambox</ext-link>.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>All files are available at Github under <ext-link ext-link-type="uri" xlink:href="https://github.com/glamlab/glambox">https://github.com/glamlab/glambox</ext-link>.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>A plethora of empirical findings has established an association between gaze allocation and decision behaviour on the group-level. For example, in value-based decision making, it has been repeatedly shown that longer gaze towards one option is associated with a higher choice probability for that option [<xref rid="pone.0226428.ref001" ref-type="bibr">1</xref>–<xref rid="pone.0226428.ref013" ref-type="bibr">13</xref>] and that external manipulation of gaze allocation changes choice probabilities accordingly [<xref rid="pone.0226428.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0226428.ref009" ref-type="bibr">9</xref>, <xref rid="pone.0226428.ref010" ref-type="bibr">10</xref>, <xref rid="pone.0226428.ref014" ref-type="bibr">14</xref>]. Such gaze bias effects are not limited to value-based decisions, but have recently also been observed in perceptual choices, where participants judge the perceptual attributes of stimuli based on available sensory information [<xref rid="pone.0226428.ref014" ref-type="bibr">14</xref>].</p>
    <p>These findings have led to the development of a set of computational models, aimed at capturing the empirically observed association between gaze allocation and choice behaviour by utilizing gaze data to inform the momentary accumulation rates of diffusion decision processes [<xref rid="pone.0226428.ref002" ref-type="bibr">2</xref>, <xref rid="pone.0226428.ref007" ref-type="bibr">7</xref>, <xref rid="pone.0226428.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0226428.ref014" ref-type="bibr">14</xref>–<xref rid="pone.0226428.ref017" ref-type="bibr">17</xref>]. Specifically, these models assume that evidence accumulation in favour of an item continues while it is not looked at, but at a discounted rate. The application of these models is limited so far, as fitting them to empirical data depends on computationally expensive simulations, involving the simulation of fixation trajectories. These simulations, as well as the creation of models of the underlying fixation process, become increasingly difficult with increasing complexity of the decision setting (e.g., growing choice set sizes or number of option attributes, etc). Existing approaches that circumvent the need for simulations, model the evidence accumulation process as a single diffusion process between two decision bounds and are therefore limited to binary decisions [<xref rid="pone.0226428.ref002" ref-type="bibr">2</xref>, <xref rid="pone.0226428.ref018" ref-type="bibr">18</xref>].</p>
    <p>However, researchers are increasingly interested in choice settings involving more than two alternatives. Choices outside the laboratory usually involve larger choice sets or describe items on multiple attributes. Besides, many established behavioural effects only occur in multi-alternative and multi-attribute choice situations [<xref rid="pone.0226428.ref019" ref-type="bibr">19</xref>].</p>
    <p>Furthermore, recent findings indicate strong individual differences in the association between gaze allocation and choice behaviour [<xref rid="pone.0226428.ref020" ref-type="bibr">20</xref>, <xref rid="pone.0226428.ref021" ref-type="bibr">21</xref>] as well as individual differences in the decision mechanisms used [<xref rid="pone.0226428.ref015" ref-type="bibr">15</xref>]. While the nature of individual differences in gaze biases is still not fully understood, different mechanisms have been suggested: Smith and Krajbich [<xref rid="pone.0226428.ref020" ref-type="bibr">20</xref>] showed that gaze bias differences can be related to individual differences in attentional scope (“tunnel vision”). Vaidya and Fellows [<xref rid="pone.0226428.ref013" ref-type="bibr">13</xref>] found stronger gaze biases in patients with damage in dorsomedial prefrontal cortex (PFC). Further, recent empirical work has investigated the roles of learning and attitude accessibility in gaze dependent decision making [<xref rid="pone.0226428.ref022" ref-type="bibr">22</xref>, <xref rid="pone.0226428.ref023" ref-type="bibr">23</xref>]. However, more systematic investigations of these differences are needed, as the majority of model-based investigations of the relationship between gaze allocation and choice behaviour were focused on the group level, disregarding differences between individuals.</p>
    <p>With the Gaze-weighted linear accumulator model (GLAM; [<xref rid="pone.0226428.ref021" ref-type="bibr">21</xref>]), we have proposed an analytical tool that allows the model-based investigation of the relationship between gaze allocation and choice behaviour at the level of the individual, in choice situations involving more than two alternatives, solely requiring participants’ choice, response time (RT) and gaze data, in addition to estimates of the items’ values.</p>
    <p>Like the attentional Drift Diffusion Model (aDDM) [<xref rid="pone.0226428.ref007" ref-type="bibr">7</xref>, <xref rid="pone.0226428.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0226428.ref017" ref-type="bibr">17</xref>], the GLAM assumes that the decision process is biased by momentary gaze behaviour: While an item is not fixated, its value representation is discounted. The GLAM, however, differs from the aDDM in other important aspects: In contrast to the aDDM, the fixation-dependent value signals are averaged across the trial, using the relative amount of time individuals spend fixating the items. This step abstracts away the specific sequence of fixations in a trial, that can be investigated with the aDDM. On the other hand, this simplification allows for the construction of trial-wise constant drift rates that can enter a basic stochastic race framework. While race models like the GLAM are not statistically optimal [<xref rid="pone.0226428.ref024" ref-type="bibr">24</xref>] the GLAM has been shown to provide a good fit to empirical data [<xref rid="pone.0226428.ref021" ref-type="bibr">21</xref>]. In general, race models have at least two practical advantages: First, they often have analytical solutions to their first-passage density distributions, and secondly, they naturally generalize to choice scenarios involving more than two alternatives. The analytical tractability of the race framework further allows for efficient parameter estimation in a hierarchical Bayesian manner. The GLAM thereby integrates gaze-dependent accumulation into a practical race model shell.</p>
    <p>To make GLAM more accessible, we now introduce GLAMbox, a Python-based toolbox for the application of the GLAM to empirical choice, RT and gaze data. GLAMbox allows for individual and hierarchical estimation of the GLAM parameters, simulation of response data and model-based comparisons between experimental conditions and groups. It further contains a set of visualization functions to inspect choice and gaze data and evaluate model fit. We illustrate three application examples of the toolbox: In Example 1, we illustrate how GLAMbox can be used to analyze individual participant data with the GLAM. In particular, we perform an exemplary model comparison between multiple model variants on the individual level, as well as an out-of-sample prediction of participants’ choice and RT data. In Example 2, we demonstrate the application of the GLAM to perform a comparison of group-level parameters in a setting with limited amounts of data, using hierarchical parameter estimation. Lastly, in Example 3, we walk the reader through a step-by-step parameter recovery study with the GLAM, which is encouraged to increase confidence in the estimated parameter values.</p>
  </sec>
  <sec sec-type="materials|methods" id="sec002">
    <title>Materials and methods</title>
    <sec id="sec003">
      <title>Gaze-weighted linear accumulator model details</title>
      <p>Like the aDDM, the GLAM assumes that preference formation, during a simple choice process, is guided by the allocation of visual gaze (for an overview, see <xref ref-type="fig" rid="pone.0226428.g001">Fig 1</xref>). Particularly, the decision process is guided by a set of decision signals: An absolute and relative decision signal. Throughout the trial, the absolute signal of an item <italic>i</italic> can be in two states: An unbiased state, equal to the item’s value <italic>r</italic><sub><italic>i</italic></sub> while the item is looked at, and a biased state while any other item is looked at, where the item value <italic>r</italic><sub><italic>i</italic></sub> is discounted by a parameter <italic>γ</italic>. The average absolute decision signal <inline-formula id="pone.0226428.e001"><alternatives><graphic xlink:href="pone.0226428.e001.jpg" id="pone.0226428.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula> is given by
<disp-formula id="pone.0226428.e002"><alternatives><graphic xlink:href="pone.0226428.e002.jpg" id="pone.0226428.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>γ</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(1)</label></disp-formula>
where <italic>g</italic><sub><italic>i</italic></sub> is defined as the fraction of total trial time that item <italic>i</italic> was looked at. If <italic>γ</italic> = 1, there is no difference between the biased and unbiased state, resulting in no influence of gaze allocation on choice behaviour. For <italic>γ</italic> values less than 1, the absolute decision signal <italic>A</italic><sub><italic>i</italic></sub> is discounted, resulting in generally higher choice probabilities for items that have been looked at longer. For <italic>γ</italic> values less than 0, the sign of the absolute decision signal <italic>A</italic><sub><italic>i</italic></sub> changes, when the item is not looked at, leading to an overall even stronger gaze bias, as evidence for these items is actively lost, when they are not looked at. This type of gaze-dependent leakage mechanism is supported by a variety of recent empirical findings [<xref rid="pone.0226428.ref015" ref-type="bibr">15</xref>, <xref rid="pone.0226428.ref021" ref-type="bibr">21</xref>].</p>
      <fig id="pone.0226428.g001" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0226428.g001</object-id>
        <label>Fig 1</label>
        <caption>
          <title>Gaze-weighted linear accumulator model.</title>
          <p>In the GLAM, preference formation during the decision process is dependent on the allocation of visual gaze (A). For each item in the choice set, an average absolute decision signal <inline-formula id="pone.0226428.e003"><alternatives><graphic id="pone.0226428.e003g" xlink:href="pone.0226428.e003"/><mml:math id="M3"><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula> is computed (dashed lines in A). The magnitude of this signal is determined by the momentary allocation of visual gaze: While an item is currently not looked at, its signal is discounted by parameter <italic>γ</italic> (<italic>γ</italic> ≤ 1; discounting is illustrated by gray arrows) (A). To determine a relative decision signal <italic>R</italic><sub><italic>i</italic></sub> for each item in the choice set, absolute evidence signals are transformed in two steps (B): First, the difference between each average absolute decision signal <inline-formula id="pone.0226428.e004"><alternatives><graphic id="pone.0226428.e004g" xlink:href="pone.0226428.e004"/><mml:math id="M4"><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula> and the maximum of all others is determined. Second, the resulting differences are scaled through a logistic transform, as the GLAM assumes an adaptive representation of the relative decision signals that is especially sensitive to differences close to 0 (where the absolute signal for an item is very close to the maximum of all others). The resulting relative decision signals <italic>R</italic><sub><italic>i</italic></sub> can be used to predict choice and RT, by determining the speed of the accumulation process in a linear stochastic race (C). The stochastic race then provides first-passage time distributions <italic>p</italic><sub><italic>i</italic></sub>, describing the likelihood of each item being chosen at each time point.</p>
        </caption>
        <graphic xlink:href="pone.0226428.g001"/>
      </fig>
      <p>To determine the relative decision signals, the average absolute decision signals <inline-formula id="pone.0226428.e005"><alternatives><graphic xlink:href="pone.0226428.e005.jpg" id="pone.0226428.e005g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M5"><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula> are transformed in two steps: First, for each item <italic>i</italic>, the relative evidence <inline-formula id="pone.0226428.e006"><alternatives><graphic xlink:href="pone.0226428.e006.jpg" id="pone.0226428.e006g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M6"><mml:msubsup><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> is computed as the difference between the average absolute decision signal of the item <inline-formula id="pone.0226428.e007"><alternatives><graphic xlink:href="pone.0226428.e007.jpg" id="pone.0226428.e007g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M7"><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="pone.0226428.e002">Eq 1</xref>) and the maximum of all other average absolute decision signals <inline-formula id="pone.0226428.e008"><alternatives><graphic xlink:href="pone.0226428.e008.jpg" id="pone.0226428.e008g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M8"><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> (also obtained from <xref ref-type="disp-formula" rid="pone.0226428.e009">Eq 2</xref>) is computed:
<disp-formula id="pone.0226428.e009"><alternatives><graphic xlink:href="pone.0226428.e009.jpg" id="pone.0226428.e009g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mover accent="true"><mml:msub><mml:mi>A</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>¯</mml:mo></mml:mover><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(2)</label></disp-formula></p>
      <p>Second, the resulting difference signals <inline-formula id="pone.0226428.e010"><alternatives><graphic xlink:href="pone.0226428.e010.jpg" id="pone.0226428.e010g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M10"><mml:msubsup><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> are scaled through a logistic transform <italic>s</italic>(<italic>x</italic>). The GLAM assumes an adaptive representation of the relative decision signals, which is maximally sensitive to small differences in the absolute decision signals close to 0 (where the difference between the absolute decision signal of an item and the maximum of all others is small):
<disp-formula id="pone.0226428.e011"><alternatives><graphic xlink:href="pone.0226428.e011.jpg" id="pone.0226428.e011g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(3)</label></disp-formula>
<disp-formula id="pone.0226428.e012"><alternatives><graphic xlink:href="pone.0226428.e012.jpg" id="pone.0226428.e012g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mi>τ</mml:mi><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(4)</label></disp-formula></p>
      <p>The sensitivity of this transform is determined by the temperature parameter <italic>τ</italic> of the logistic function. Larger values of <italic>τ</italic> indicate stronger sensitivity to small differences in the average absolute decision signals <inline-formula id="pone.0226428.e013"><alternatives><graphic xlink:href="pone.0226428.e013.jpg" id="pone.0226428.e013g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M13"><mml:mover accent="true"><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula>.</p>
      <p>Unlike more traditional diffusion models (including the aDDM), the GLAM employs a linear stochastic race to capture response behaviour as well as RTs. The relative signals <italic>R</italic><sub><italic>i</italic></sub> enter a race process, where one item accumulator <italic>E</italic><sub><italic>i</italic></sub> is defined for each item in the choice set:
<disp-formula id="pone.0226428.e014"><alternatives><graphic xlink:href="pone.0226428.e014.jpg" id="pone.0226428.e014g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M14"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext>with</mml:mtext><mml:mspace width="4.pt"/><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(5)</label></disp-formula></p>
      <p>At each time step <italic>t</italic>, the amount of accumulated evidence is determined by the accumulation rate <italic>vR</italic><sub><italic>i</italic></sub>, and zero-centered normally distributed noise with standard deviation <italic>σ</italic>. The velocity parameter <italic>v</italic> linearly scales the item drift rates in the race process and thereby affects the response times produced by the model: Lower values of <italic>v</italic> produce longer response times, larger <italic>v</italic>s result in shorter response times. A choice for an item is made as soon as one accumulator reaches the decision boundary <italic>b</italic>. To avoid underdetermination of the model, either the velocity parameter <italic>v</italic>, the noise parameter <italic>σ</italic> or the boundary has to be fixed. Similar to the aDDM, the GLAM fixes the boundary to a value of 1. The first passage time density <italic>f</italic><sub><italic>i</italic></sub>(<italic>t</italic>) of a single linear stochastic accumulator <italic>E</italic><sub><italic>i</italic></sub>, with decision boundary <italic>b</italic>, is given by the inverse Gaussian distribution:
<disp-formula id="pone.0226428.e015"><alternatives><graphic xlink:href="pone.0226428.e015.jpg" id="pone.0226428.e015g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M15"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mo>λ</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:msup><mml:mo form="prefix">exp</mml:mo><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo>-</mml:mo><mml:mo>λ</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>μ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(6)</label></disp-formula>
<disp-formula id="pone.0226428.e016"><alternatives><graphic xlink:href="pone.0226428.e016.jpg" id="pone.0226428.e016g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M16"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mspace width="4.pt"/><mml:mtext>with</mml:mtext><mml:mspace width="4.pt"/><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>b</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mspace width="4.pt"/><mml:mtext>and</mml:mtext><mml:mspace width="4.pt"/><mml:mo>λ</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
      <p>However, this density does not take into account that there are multiple accumulators in each trial racing towards the same boundary. For this reason, <italic>f</italic><sub><italic>i</italic></sub>(<italic>t</italic>) must be corrected for the probability that any other accumulator crosses the boundary first. The probability that an accumulator crosses the boundary prior to <italic>t</italic>, is given by its cumulative distribution function <italic>F</italic><sub><italic>i</italic></sub>(<italic>t</italic>):
<disp-formula id="pone.0226428.e017"><alternatives><graphic xlink:href="pone.0226428.e017.jpg" id="pone.0226428.e017g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M17"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>Φ</mml:mo><mml:mo>(</mml:mo><mml:msqrt><mml:mfrac><mml:mo>λ</mml:mo><mml:mi>t</mml:mi></mml:mfrac></mml:msqrt><mml:mo>(</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:mi>μ</mml:mi></mml:mfrac><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>λ</mml:mo></mml:mrow><mml:mi>μ</mml:mi></mml:mfrac><mml:mo>)</mml:mo><mml:mo>·</mml:mo><mml:mo>Φ</mml:mo><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:msqrt><mml:mfrac><mml:mo>λ</mml:mo><mml:mi>t</mml:mi></mml:mfrac></mml:msqrt><mml:mo>(</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:mi>μ</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(7)</label></disp-formula></p>
      <p>Here, Φ(<italic>x</italic>) defines the standard normal cumulative distribution function. Hence, the joint probability <italic>p</italic><sub><italic>i</italic></sub>(<italic>t</italic>) that accumulator <italic>E</italic><sub><italic>i</italic></sub> crosses <italic>b</italic> at time <italic>t</italic>, and that no other accumulator <italic>E</italic><sub><italic>j</italic>≠<italic>i</italic></sub> has reached <italic>b</italic> first, is given by:
<disp-formula id="pone.0226428.e018"><alternatives><graphic xlink:href="pone.0226428.e018.jpg" id="pone.0226428.e018g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M18"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(8)</label></disp-formula></p>
      <sec id="sec004">
        <title>Contaminant response model</title>
        <p>To reduce the influence of erroneous responses (e.g., when the participant presses a button by accident or has a lapse of attention during the task) on parameter estimation, we include a model of contaminant response processes in all estimation procedures: In line with existing drift diffusion modelling toolboxes [<xref rid="pone.0226428.ref025" ref-type="bibr">25</xref>], we assume a fixed 5% rate of erroneous responses <italic>ϵ</italic> that is modeled as a participant-specific uniform likelihood distribution <italic>u</italic><sub><italic>s</italic></sub>(<italic>t</italic>). This likelihood describes the probability of a random choice for any of the <italic>N</italic> available choice items at a random time point in the interval of empirically observed RTs [<xref rid="pone.0226428.ref025" ref-type="bibr">25</xref>, <xref rid="pone.0226428.ref026" ref-type="bibr">26</xref>]:
<disp-formula id="pone.0226428.e019"><alternatives><graphic xlink:href="pone.0226428.e019.jpg" id="pone.0226428.e019g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M19"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:msub><mml:mtext>rt</mml:mtext><mml:mi>s</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mo form="prefix" movablelimits="true">min</mml:mo><mml:msub><mml:mtext>rt</mml:mtext><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(9)</label></disp-formula></p>
        <p>The resulting likelihood for participant <italic>s</italic> choosing item <italic>i</italic>, accounting for erroneous responses, is then given by:
<disp-formula id="pone.0226428.e020"><alternatives><graphic xlink:href="pone.0226428.e020.jpg" id="pone.0226428.e020g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M20"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>ϵ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>·</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi><mml:mo>·</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(10)</label></disp-formula></p>
        <p>The rate of error responses <italic>ϵ</italic> can be specified by the user to a different value than the default of 5% using the <monospace>error_weight</monospace> keyword in the <monospace>make_model</monospace> method (see below).</p>
      </sec>
      <sec id="sec005">
        <title>Individual parameter estimation details</title>
        <p>The GLAM is implemented in a Bayesian framework using the Python library PyMC3 [<xref rid="pone.0226428.ref027" ref-type="bibr">27</xref>]. The model has four parameters (<italic>v</italic>, <italic>γ</italic>, <italic>σ</italic>, <italic>τ</italic>). By default, uninformative, uniform priors between sensible limits (derived from earlier applications to four different datasets: [<xref rid="pone.0226428.ref021" ref-type="bibr">21</xref>]) are placed on all parameters:
<disp-formula id="pone.0226428.e021"><alternatives><graphic xlink:href="pone.0226428.e021.jpg" id="pone.0226428.e021g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M21"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>v</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mi>γ</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mi>σ</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mi>τ</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
        <p>These limits were derived by extending the range of observed parameter estimates in earlier applications of the GLAM to four different empirical choice datasets. These datasets encompass data of 117 participants in value-based and perceptual choice tasks with up to three choice alternatives (including a wide range of possible response times, gaze bias strengths and choice accuracies; for further details [<xref rid="pone.0226428.ref021" ref-type="bibr">21</xref>]). Parameter estimates for these datasets are illustrated and summarised in <xref ref-type="supplementary-material" rid="pone.0226428.s005">S1 Table</xref>, <xref ref-type="supplementary-material" rid="pone.0226428.s001">S1 Fig</xref> and <xref ref-type="supplementary-material" rid="pone.0226428.s001">S1 Fig</xref>.</p>
        <p>The velocity parameter <italic>v</italic> and the noise parameter <italic>σ</italic> must be strictly positive. Smaller <italic>v</italic> produce slower and less accurate responses (for constant <italic>σ</italic>), while smaller <italic>σ</italic> produce more accurate and slower responses (for constant <italic>v</italic>). The gaze bias parameter <italic>γ</italic> has a natural upper bound at 1 (indicating no gaze bias), while decreasing <italic>γ</italic> values indicate an increasing gaze bias strength. The sensitivity parameter <italic>τ</italic> has a natural lower bound at 0 (resulting in no sensitivity to differences in average absolute decision signals <inline-formula id="pone.0226428.e022"><alternatives><graphic xlink:href="pone.0226428.e022.jpg" id="pone.0226428.e022g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M22"><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula>), with larger values indicating increased sensitivity.</p>
      </sec>
      <sec id="sec006">
        <title>Hierarchical parameter estimation details</title>
        <p>For hierarchical models, individual parameters are assumed to be drawn from Truncated Normal distributions, parameterized by mean and standard deviation, over which weakly informative, Truncated Normal priors are assumed (based on the distribution of group level parameter estimates obtained from four different datasets in [<xref rid="pone.0226428.ref021" ref-type="bibr">21</xref>]; see <xref ref-type="fig" rid="pone.0226428.g002">Fig 2</xref>, <xref ref-type="supplementary-material" rid="pone.0226428.s001">S1</xref> and <xref ref-type="supplementary-material" rid="pone.0226428.s002">S2</xref> Figs and <xref ref-type="supplementary-material" rid="pone.0226428.s005">S1 Table</xref>):
<disp-formula id="pone.0226428.e023"><alternatives><graphic xlink:href="pone.0226428.e023.jpg" id="pone.0226428.e023g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M23"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>v</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>63</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>·</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>26</mml:mn><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mtext>truncated</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>to</mml:mtext><mml:mspace width="4.pt"/><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>v</mml:mi><mml:mi>σ</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>26</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>·</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>11</mml:mn><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mtext>truncated</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>to</mml:mtext><mml:mspace width="4.pt"/><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>γ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>12</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>·</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>11</mml:mn><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mtext>truncated</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>to</mml:mtext><mml:mspace width="4.pt"/><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>γ</mml:mi><mml:mi>σ</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>35</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>·</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mtext>truncated</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>to</mml:mtext><mml:mspace width="4.pt"/><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>27</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>·</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>08</mml:mn><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mtext>truncated</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>to</mml:mtext><mml:mspace width="4.pt"/><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>σ</mml:mi><mml:mi>σ</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>05</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>·</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>01</mml:mn><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mtext>truncated</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>to</mml:mtext><mml:mspace width="4.pt"/><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>2</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>τ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mn>03</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>·</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>58</mml:mn><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mtext>truncated</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>to</mml:mtext><mml:mspace width="4.pt"/><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>τ</mml:mi><mml:mi>σ</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>62</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>·</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>26</mml:mn><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mtext>truncated</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>to</mml:mtext><mml:mspace width="4.pt"/><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
        <fig id="pone.0226428.g002" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0226428.g002</object-id>
          <label>Fig 2</label>
          <caption>
            <title>Hierarchical model structure.</title>
            <p>In the hierarchical model, individual subject parameters <italic>γ</italic><sub><italic>i</italic></sub>, <italic>v</italic><sub><italic>i</italic></sub>. <italic>σ</italic><sub><italic>i</italic></sub>, and <italic>τ</italic><sub><italic>i</italic></sub> (subject plate) are drawn from Truncated Normal group level distributions with means <italic>μ</italic> and standard deviations <italic>σ</italic> (outside of the subject plate). Weakly informative Truncated Normal priors are placed on the group level parameters. RT and choice data <italic>x</italic><sub><italic>i</italic>,<italic>t</italic></sub> for each trial <italic>t</italic> is distributed according to the subject parameters and the GLAM likelihood (<xref ref-type="disp-formula" rid="pone.0226428.e018">Eq (8)</xref>; inner trial plate).</p>
          </caption>
          <graphic xlink:href="pone.0226428.g002"/>
        </fig>
      </sec>
    </sec>
    <sec id="sec007">
      <title>Basic usage</title>
      <sec id="sec008">
        <title>Data format, the <monospace>GLAM</monospace> class</title>
        <p>The core functionality of the GLAMbox is implemented in the <monospace>GLAM</monospace> model class. To apply the GLAM to data, an instance of the model class needs to be instantiated and supplied with the experimental data, first:</p>
        <p>
          <monospace>import glambox as gb</monospace>
        </p>
        <p>
          <monospace>glam = gb.GLAM(data=data)</monospace>
        </p>
        <p>The data must be a pandas [<xref rid="pone.0226428.ref028" ref-type="bibr">28</xref>] DataFrame with one row per trial, containing the following variable entries:</p>
        <list list-type="bullet">
          <list-item>
            <p><monospace>subject</monospace>: Subject index (integer, first subject should be 0)</p>
          </list-item>
          <list-item>
            <p><monospace>trial</monospace>: Trial index (integer, first trial should be 0)</p>
          </list-item>
          <list-item>
            <p><monospace>choice</monospace>: Chosen item in this trial (integer, items should be 0, 1, …, <italic>N</italic>)</p>
          </list-item>
          <list-item>
            <p><monospace>rt</monospace>: Response time (float, in seconds)</p>
          </list-item>
          <list-item>
            <p>for each item <italic>i</italic> in the choice set:
<list list-type="bullet"><list-item><p><monospace>item_value_i</monospace>: The item value (float, we recommend to re-scale all item values to a range between 1 and 10 to allow comparison of parameter estimates between studies)</p></list-item><list-item><p><monospace>gaze_i</monospace>: The fraction of total time in this trial that the participant spent looking at this item (float, between 0 and 1)</p></list-item></list></p>
          </list-item>
          <list-item>
            <p>additional variables coding groups or conditions (string or integer)</p>
          </list-item>
        </list>
        <p>For reference, the first two rows of a pandas DataFrame ready to be used with GLAMbox are shown in <xref rid="pone.0226428.t001" ref-type="table">Table 1</xref>.</p>
        <table-wrap id="pone.0226428.t001" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0226428.t001</object-id>
          <label>Table 1</label>
          <caption>
            <title>The first two rows of a pandas DataFrame ready to be used with GLAM.</title>
          </caption>
          <alternatives>
            <graphic id="pone.0226428.t001g" xlink:href="pone.0226428.t001"/>
            <table frame="box" rules="all" border="0">
              <colgroup span="1">
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <th align="left" style="border-bottom:thick" rowspan="1" colspan="1">subject</th>
                  <th align="left" style="border-bottom:thick" rowspan="1" colspan="1">trial</th>
                  <th align="left" style="border-bottom:thick" rowspan="1" colspan="1">choice</th>
                  <th align="left" style="border-bottom:thick" rowspan="1" colspan="1">rt</th>
                  <th align="left" style="border-bottom:thick" rowspan="1" colspan="1">item_value_0</th>
                  <th align="left" style="border-bottom:thick" rowspan="1" colspan="1">item_value_1</th>
                  <th align="left" style="border-bottom:thick" rowspan="1" colspan="1">item_value_2</th>
                  <th align="left" style="border-bottom:thick" rowspan="1" colspan="1">gaze_0</th>
                  <th align="left" style="border-bottom:thick" rowspan="1" colspan="1">gaze_1</th>
                  <th align="left" style="border-bottom:thick" rowspan="1" colspan="1">gaze_2</th>
                  <th align="left" style="border-bottom:thick" rowspan="1" colspan="1">speed</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" rowspan="1" colspan="1">0</td>
                  <td align="left" rowspan="1" colspan="1">0</td>
                  <td align="left" rowspan="1" colspan="1">0</td>
                  <td align="char" char="." rowspan="1" colspan="1">2.056</td>
                  <td align="left" rowspan="1" colspan="1">5</td>
                  <td align="left" rowspan="1" colspan="1">1</td>
                  <td align="left" rowspan="1" colspan="1">3</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.16</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.62</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.22</td>
                  <td align="left" rowspan="1" colspan="1">‘fast’</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">0</td>
                  <td align="left" rowspan="1" colspan="1">1</td>
                  <td align="left" rowspan="1" colspan="1">2</td>
                  <td align="char" char="." rowspan="1" colspan="1">3.685</td>
                  <td align="left" rowspan="1" colspan="1">3</td>
                  <td align="left" rowspan="1" colspan="1">6</td>
                  <td align="left" rowspan="1" colspan="1">9</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.44</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.22</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.34</td>
                  <td align="left" rowspan="1" colspan="1">‘slow’</td>
                </tr>
              </tbody>
            </table>
          </alternatives>
        </table-wrap>
        <p>Next, the respective PyMC3 model, which will later be used to estimate the model’s parameters, can be built using the <monospace>make_model</monospace> method. Here, the researcher specifies the kind of the model: <monospace>‘<italic>individual</italic>’</monospace> if the parameters should be estimated for each subject individually, <monospace>‘<italic>hierarchical</italic>’</monospace> for hierarchical parameter estimation, or <monospace>‘<italic>pooled</italic>’</monospace> to estimate a single parameter set for all subjects. At this stage, the researcher can also specify experimental parameter dependencies: For example, a parameter could be expected to vary between groups or conditions. In line with existing modeling toolboxes (e.g., [<xref rid="pone.0226428.ref025" ref-type="bibr">25</xref>, <xref rid="pone.0226428.ref029" ref-type="bibr">29</xref>]) dependencies are defined using the <monospace>depends_on</monospace> argument. <monospace>depends_on</monospace> expects a dictionary with parameters as keys and experimental factors as values (e.g., <monospace>depends_on=dict(v=’<italic>speed</italic>’)</monospace> for factor <monospace>‘<italic>speed</italic>’</monospace> with conditions <monospace>‘<italic>fast</italic>’</monospace> and <monospace>‘<italic>slow</italic>’</monospace> in the data). The toolbox internally handles within- and between subject designs and assigns parameters accordingly. If multiple conditions are given for a factor, one parameter will be designated for each condition. Finally, the <monospace>make_model</monospace> method allows parameters to be fixed to a specific value using the <monospace>*_val</monospace> arguments (e.g., <monospace>gamma_val=1</monospace> for a model without gaze bias). If parameters should be fixed for individual subjects, a list of individual values needs to be passed.</p>
        <p>
          <monospace>model.make_model(kind=‘<italic>individual</italic>’,</monospace>
        </p>
        <p>          <monospace>depends_on=dict(v=‘<italic>speed</italic>’),</monospace></p>
        <p>          <monospace>gamma_val=1)</monospace></p>
      </sec>
      <sec id="sec009">
        <title>Inference</title>
        <p>Once the PyMC3 model is built, parameters can be estimated using the <monospace>fit</monospace> method:</p>
        <p>
          <monospace>model.fit(method=‘<italic>MCMC</italic>’)</monospace>
        </p>
        <p>The <monospace>fit</monospace> method defaults to Markov-Chain-Monte-Carlo (MCMC; [<xref rid="pone.0226428.ref030" ref-type="bibr">30</xref>]) sampling, but also allows for Variational Inference (see below).</p>
      </sec>
      <sec id="sec010">
        <title>Markov-Chain-Monte-Carlo</title>
        <p>MCMC methods approximate the Bayesian posterior parameter distributions, describing the probability of a parameter taking certain values given the data and prior probabilities, through repeated sampling. GLAMbox can utilize all available MCMC step methods provided by PyMC3. The resulting MCMC traces can be accessed using the <monospace>trace</monospace> attribute of the model instance (note that a list of traces is stored for models of kind <monospace>‘<italic>individual</italic>’</monospace>). They should always be checked for convergence, to ascertain that the posterior distribution is approximated well. Both qualitative visual and more quantitative numerical checks of convergence, such as the Gelman-Rubin statistic <inline-formula id="pone.0226428.e024"><alternatives><graphic xlink:href="pone.0226428.e024.jpg" id="pone.0226428.e024g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M24"><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> and the number of effective samples are recommended (for detailed recommendations, see [<xref rid="pone.0226428.ref031" ref-type="bibr">31</xref>, <xref rid="pone.0226428.ref032" ref-type="bibr">32</xref>]). PyMC3 contains a range of diagnostic tools to perform such checks (such as the <monospace>summary</monospace> function).</p>
      </sec>
      <sec id="sec011">
        <title>Variational inference</title>
        <p>Estimation can also be done using all other estimation procedures provided in the PyMC3 library. This includes variational methods like Automatic Differentiation Variational Inference (ADVI; [<xref rid="pone.0226428.ref033" ref-type="bibr">33</xref>]). To use variational inference, the <monospace>method</monospace> argument can be set to <monospace>‘<italic>VI</italic>’</monospace>, defaulting to the default variational method in PyMC3. We found variational methods to quickly yield usable, but sometimes inaccurate parameter estimates, and therefore recommend using MCMC for final analyses.</p>
      </sec>
      <sec id="sec012">
        <title>Accessing parameter estimates</title>
        <p>After parameter estimation is completed, the resulting estimates can be accessed with the <monospace>estimates</monospace> attribute of the GLAM model instance. This returns a table with one row for each set of parameter estimates for each individual and condition in the data. For each parameter, a <italic>maximum a posteriori</italic> (MAP) estimate is given, in addition to the 95% Highest-Posterior Density Interval (HPD). If the parameters were estimated hierarchically, the table also contains estimates of the group-level parameters.</p>
      </sec>
      <sec id="sec013">
        <title>Comparing parameters between groups or conditions</title>
        <p>Parameter estimates can be compared between different experimental groups or conditions (specified with the <monospace>depends_on</monospace> keyword when calling <monospace>make_model</monospace>) using the <monospace>compare_parameters</monospace> function from the <monospace>analysis</monospace> module. It takes as input the fitted GLAM instance, a list of parameters (<monospace>‘<italic>v</italic>’</monospace>, <monospace>‘<italic>s</italic>’</monospace>, <monospace>‘<italic>gamma</italic>’</monospace>, <monospace>‘<italic>tau</italic>’</monospace>), and a list of pairwise comparisons between groups or conditions. The comparison argument expects a list of tuples (e.g., <monospace>[(‘<italic>group1</italic>’, ‘<italic>group2</italic>’), (’<italic>group1</italic>’, ‘<italic>group3</italic>’)</monospace>). For example, given a fitted model instance (here <monospace>glam</monospace>) a comparison of the <italic>γ</italic> parameter between two groups (<monospace>group1</monospace> and <monospace>group2</monospace>) can be computed as:</p>
        <p>
          <monospace>from gb.analysis import compare_parameters</monospace>
        </p>
        <p>
          <monospace>comparison = compare_parameters(model=glam,</monospace>
        </p>
        <p>                   <monospace>parameters=[‘<italic>gamma</italic>’],</monospace></p>
        <p>                   <monospace>comparisons=[(‘<italic>group1</italic>’, ‘<italic>group2</italic>’)])</monospace></p>
        <p>The function then returns a table with one row per specified comparison, and columns containing the mean posterior difference, percentage of the posterior above zero, and corresponding 95% HPD interval. If supplied with a hierarchical model, the function computes differences between group-level parameters. If an individual type model is given, it returns comparison statistics for each individual.</p>
        <p>Comparisons can be visualized using the <monospace>compare_parameters</monospace> function from the <monospace>plots</monospace> module. It takes the same input as its analogue in the <monospace>alysis</monospace> module. It plots posterior distributions of parameters and the posterior distributions of any differences specified using the <monospace>comparisons</monospace> argument. For a usage example and plot see Example 2.</p>
      </sec>
      <sec id="sec014">
        <title>Comparing model variants</title>
        <p>Model comparisons between multiple GLAM variants (e.g., full and restricted variants) can be performed using the <monospace>compare_models</monospace> function, which wraps the function of the same name from the PyMC3 library. The <monospace>compare_models</monospace> function takes as input a list of fitted model instances that are to be compared. Additional keyword arguments can be given and are passed on to the underlying PyMC3 <monospace>compare</monospace> function. This allows the user, for example, to specify the information criterion used for the comparison via the <monospace>ic</monospace> argument (<monospace>‘<italic>WAIC</italic>’</monospace> or <monospace>‘<italic>LOO</italic>’</monospace> for Leave-One-Out cross validation). It returns a table containing an estimate of the specified information criterion, standard errors, difference to the best-fitting model, standard error of the difference, and other output variables from PyMC3 for each inputted model (and subject, if individually estimated models were given). We refer the reader to Example 1 for a usage example and exemplary output from the <monospace>compare_models</monospace> function.</p>
      </sec>
      <sec id="sec015">
        <title>Predicting choices and response times</title>
        <p>Choices and RTs can be predicted with the GLAM by the use of the <monospace>predict</monospace> method:</p>
        <p>
          <monospace>model.predict(n_repeats=50)</monospace>
        </p>
        <p>For each trial of the dataset that is attached to the model instance, this method predicts a choice and RT according to <xref ref-type="disp-formula" rid="pone.0226428.e020">Eq (10)</xref>, using the previously determined MAP parameter estimates. To obtain a stable estimate of the GLAM’s predictions, as well as the noise contained within them, it is recommended to repeat every trial multiple times during the prediction. The number of trial repeats can be specified with the <monospace>n_repeats</monospace> argument. After the prediction is completed, the predicted data can be accessed with the <monospace>prediction</monospace> attribute of the model.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="results" id="sec016">
    <title>Results</title>
    <sec id="sec017">
      <title>Example 1: Individual level data &amp; model comparison</title>
      <p>Our first example is based on the study by [<xref rid="pone.0226428.ref021" ref-type="bibr">21</xref>]. Here, the authors study the association between gaze allocation and choice behaviour on the level of the individual. In particular, they explore whether (1) gaze biases are present on the individual level and (2) the strength of this association varies between individuals. In this example, we replicate this type of individual model-based analysis, including parameter estimation, comparison between multiple model variants, and out-of-sample prediction of choice and RT data.</p>
      <sec id="sec018">
        <title>Simulating data</title>
        <p>First, we simulate a dataset containing 30 subjects, each performing 300 simple value-based choice trials. We assume that in each trial participants are asked to choose the item that they like most out of a set of three presented alternatives (e.g., snack food items; similar to the task described in [<xref rid="pone.0226428.ref008" ref-type="bibr">8</xref>]). While participants perform the task, their eye movements, choices and RTs are measured. Before completing the choice trials, participants were asked to indicate their liking rating for each of the items used in the choice task on a liking rating scale between 1 and 10 (with 10 indicating strong liking and 1 indicating little liking). The resulting dataset contains a liking value for each item in a trial, the participants’ choice and RT, as well as the participant’s gaze towards each item in a trial (describing the fraction of trial time that the participant spent looking at each item in the choice set).</p>
        <p>To simulate individuals’ response behaviour, we utilize the parameter estimates that were obtained by [<xref rid="pone.0226428.ref021" ref-type="bibr">21</xref>] for the individuals in the three item choice dataset by [<xref rid="pone.0226428.ref008" ref-type="bibr">8</xref>] (see <xref ref-type="supplementary-material" rid="pone.0226428.s001">S1 Fig</xref>). Importantly, we assume that ten individuals do not exhibit a gaze bias, meaning that their choices are independent of the time that they spend looking at each item. To this end, we set the <italic>γ</italic> value of ten randomly selected individuals to 1. We further assume that individuals’ gaze is distributed randomly with respect to the values of the items in a choice set. An overview of the generating parameter estimates is given in <xref ref-type="supplementary-material" rid="pone.0226428.s003">S3 Fig</xref>.</p>
        <p>We first instantiate a GLAM model instance using <monospace>gb.GLAM()</monospace> and then use its <monospace>simulate_group</monospace> method. This method requires us to specify whether the individuals of the group are either simulated individually (and thereby independent of one another) or as part of a group with hierarchical parameter structure (where the individual model parameters are drawn from a group distribution, see below). For the former, the generating model parameters (indicated in the following as <monospace>gen_parameters</monospace>) are provided as a dictionary, containing a list of the individual participant values for each model parameter:</p>
        <p>
          <monospace>import glambox as gb</monospace>
        </p>
        <p>
          <monospace>import numpy as np</monospace>
        </p>
        <p>
          <monospace>glam = gb.GLAM()</monospace>
        </p>
        <p>
          <monospace>no_bias_subjects = np.random.choice(a=gen_parameters[’<italic>gamma</italic>’].size,</monospace>
        </p>
        <p>                     <monospace>size=10,</monospace></p>
        <p>                     <monospace>replace=False)</monospace></p>
        <p>
          <monospace>gen_parameters[’<italic>gamma</italic>’][no_bias_subjects] = 1</monospace>
        </p>
        <p>
          <monospace>glam.simulate_group(kind=’<italic>individual</italic>’,</monospace>
        </p>
        <p>            <monospace>n_individuals=30,</monospace></p>
        <p>            <monospace>n_trials=300,</monospace></p>
        <p>            <monospace>n_items=3,</monospace></p>
        <p>            <monospace>parameters=gen_parameters)</monospace></p>
        <p>As this example is focused on the individual level, we can further create a summary table, describing individuals’ response behaviour on three behavioural metrics, using the <monospace>aggregate_subject_level_data</monospace> function from the <monospace>analysis</monospace> module. The resulting table contains individuals’ mean RT, their probability of choosing the item with the highest item value from a choice set and a behavioural measure of the strength of the association between individuals’ gaze allocation and choice behaviour (indicating the mean increase in choice probability for an item that was fixated on longer than the others, after correcting for the influence of the item value on choice behaviour; for further details, see [<xref rid="pone.0226428.ref021" ref-type="bibr">21</xref>]).</p>
        <p>
          <monospace>from glambox.analysis import aggregate_subject_level_data</monospace>
        </p>
        <p>
          <monospace>subject_data_summary = aggregate_subject_level_data(data=glam.data,</monospace>
        </p>
        <p>                              <monospace>n_items=3)</monospace></p>
      </sec>
      <sec id="sec019">
        <title>Exploring the behavioural data</title>
        <p>In a first step of our analysis, we explore differences in individuals’ response behaviour. To this end, we plot the distributions of individuals’ scores on the three behavioural metrics, and their associations, using the <monospace>plot_behaviour_associations</monospace> function implemented in the <monospace>plots</monospace> module:</p>
        <p>
          <monospace>gb.plots.plot_behaviour_associations(data=data)</monospace>
        </p>
        <p>The resulting plot is displayed in <xref ref-type="fig" rid="pone.0226428.g003">Fig 3</xref> and shows that individuals’ probability of choosing the best item, as well as the strength of their behavioural association of gaze and choice, are not associated with their mean RT (<xref ref-type="fig" rid="pone.0226428.g003">Fig 3D and 3E</xref>). However, individuals’ probability of choosing the best item increases with decreasing strength of the behavioural association of gaze and choice (<xref ref-type="fig" rid="pone.0226428.g003">Fig 3F</xref>).</p>
        <fig id="pone.0226428.g003" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0226428.g003</object-id>
          <label>Fig 3</label>
          <caption>
            <title>Individual differences in the data.</title>
            <p>A-C: distributions of individuals’ mean RT (A), probability of choosing the highest-valued item in a trial (B), and behavioural influence of gaze allocation on choice behaviour (C). D-F: associations between individuals’ probability of choosing the highest-valued item and mean RT (D), individuals’ behavioural influence of gaze allocation on choice behaviour and their mean RT (E), individuals’ behavioural influence of gaze allocation on choice behaviour and their probability of choosing the highest-valued item (F). Red lines indicate linear regression fits with confidence bands surrounding them. Pearson’s <italic>r</italic> coefficients with corresponding <italic>P</italic>-values are reported for each association in D-F.</p>
          </caption>
          <graphic xlink:href="pone.0226428.g003"/>
        </fig>
      </sec>
      <sec id="sec020">
        <title>Likelihood-based model comparison</title>
        <p>In a second step of our analysis, we want to test whether the response behaviour of each individual is better described by a decision model with or without gaze bias. To this end, we set up the two GLAM variants:</p>
        <p>
          <monospace>glam_bias = gb.GLAM(data=data)</monospace>
        </p>
        <p>
          <monospace>glam_bias.make_model(kind=’<italic>individual</italic>’, name=’<italic>glam_bias</italic>’)</monospace>
        </p>
        <p>
          <monospace>glam_nobias = gb.GLAM(data=data)</monospace>
        </p>
        <p>
          <monospace>glam_nobias.make_model(kind=’<italic>individual</italic>’, gamma_val=1, name=’glam_nobias’)</monospace>
        </p>
        <p>For the GLAM variant without gaze bias mechanism, we use the <monospace>gamma_val</monospace> argument and set it to a value of 1 (fixing <italic>γ</italic> to 1 for all subjects). We also assign different names to each model with the <monospace>name</monospace> attribute to better identify them in our subsequent analyses.</p>
        <p>Subsequently, we fit both models to the data of each individual and compare their fit by means of the Widely Applicable Information Criterion (WAIC; [<xref rid="pone.0226428.ref034" ref-type="bibr">34</xref>]):</p>
        <p>
          <monospace>glam_bias.fit(method=’<italic>MCMC</italic>’,</monospace>
        </p>
        <p>        <monospace>tune=5000,</monospace></p>
        <p>        <monospace>draws=5000,</monospace></p>
        <p>        <monospace>chains=4)</monospace></p>
        <p>
          <monospace>glam_nobias.fit(method=’<italic>MCMC</italic>’,</monospace>
        </p>
        <p>         <monospace>tune=5000,</monospace></p>
        <p>         <monospace>draws=5000,</monospace></p>
        <p>         <monospace>chains=4)</monospace></p>
        <p>The fit method defaults to Metropolis-Hastings MCMC sampling (for methodological details, see <xref ref-type="sec" rid="sec002">Methods</xref> Section). The <monospace>draws</monospace> argument sets the number of samples to be drawn. This excludes the tuning (or burn-in) samples, which can be set with the <monospace>tune</monospace> argument. In addition, the <monospace>fit</monospace> method accepts the same keyword arguments as the PyMC3 sample function, which it wraps (see the PyMC3 documentation for additional details). The <monospace>chains</monospace> argument sets the number of MCMC traces (it defaults to four and should be set to at least two, in order to allow convergence diagnostics).</p>
        <p>After convergence has been established for all parameter traces (for details on the suggested convergence criteria, see <xref ref-type="sec" rid="sec002">Methods</xref>), we perform a model comparison on the individual level, using the <monospace>compare_models</monospace> function from the <monospace>analysis</monospace> (see Basic Usage: Comparing model variants):</p>
        <p>
          <monospace>comparison_df = gb.analysis.compare_models(models=[glam_bias, glam_nobias],</monospace>
        </p>
        <p>                          <monospace>ic=’<italic>WAIC</italic>’)</monospace></p>
        <p>The resulting table (shown in <xref rid="pone.0226428.t002" ref-type="table">Table 2</xref>) can be used to identify the best fitting model (indicated by the lowest WAIC score) per individual.</p>
        <table-wrap id="pone.0226428.t002" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0226428.t002</object-id>
          <label>Table 2</label>
          <caption>
            <title>Output from <monospace>compare_models</monospace> function for the first two subjects.</title>
          </caption>
          <alternatives>
            <graphic id="pone.0226428.t002g" xlink:href="pone.0226428.t002"/>
            <table frame="box" rules="all" border="0">
              <colgroup span="1">
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <th align="left" rowspan="1" colspan="1">subject</th>
                  <th align="left" rowspan="1" colspan="1">model</th>
                  <th align="left" rowspan="1" colspan="1">WAIC</th>
                  <th align="left" rowspan="1" colspan="1">pWAIC</th>
                  <th align="left" rowspan="1" colspan="1">dWAIC</th>
                  <th align="left" rowspan="1" colspan="1">weight</th>
                  <th align="left" rowspan="1" colspan="1">SE</th>
                  <th align="left" rowspan="1" colspan="1">dSE</th>
                  <th align="left" rowspan="1" colspan="1">var_warn</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" rowspan="1" colspan="1">0</td>
                  <td align="left" rowspan="1" colspan="1">glam_bias</td>
                  <td align="char" char="." rowspan="1" colspan="1">523.6</td>
                  <td align="char" char="." rowspan="1" colspan="1">5.75</td>
                  <td align="left" rowspan="1" colspan="1">0</td>
                  <td align="left" rowspan="1" colspan="1">0.94</td>
                  <td align="char" char="." rowspan="1" colspan="1">50.25</td>
                  <td align="left" rowspan="1" colspan="1">0</td>
                  <td align="left" rowspan="1" colspan="1">0</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">0</td>
                  <td align="left" rowspan="1" colspan="1">glam_nobias</td>
                  <td align="char" char="." rowspan="1" colspan="1">645.09</td>
                  <td align="char" char="." rowspan="1" colspan="1">3.64</td>
                  <td align="left" rowspan="1" colspan="1">121.49</td>
                  <td align="left" rowspan="1" colspan="1">0.06</td>
                  <td align="char" char="." rowspan="1" colspan="1">44.15</td>
                  <td align="left" rowspan="1" colspan="1">23.56</td>
                  <td align="left" rowspan="1" colspan="1">0</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">1</td>
                  <td align="left" rowspan="1" colspan="1">glam_bias</td>
                  <td align="char" char="." rowspan="1" colspan="1">1097.86</td>
                  <td align="char" char="." rowspan="1" colspan="1">3.69</td>
                  <td align="left" rowspan="1" colspan="1">0</td>
                  <td align="left" rowspan="1" colspan="1">1</td>
                  <td align="char" char="." rowspan="1" colspan="1">40.32</td>
                  <td align="left" rowspan="1" colspan="1">0</td>
                  <td align="left" rowspan="1" colspan="1">0</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">1</td>
                  <td align="left" rowspan="1" colspan="1">glam_nobias</td>
                  <td align="char" char="." rowspan="1" colspan="1">1185.02</td>
                  <td align="char" char="." rowspan="1" colspan="1">2.85</td>
                  <td align="left" rowspan="1" colspan="1">87.16</td>
                  <td align="left" rowspan="1" colspan="1">0</td>
                  <td align="char" char="." rowspan="1" colspan="1">38.22</td>
                  <td align="left" rowspan="1" colspan="1">18</td>
                  <td align="left" rowspan="1" colspan="1">0</td>
                </tr>
              </tbody>
            </table>
          </alternatives>
        </table-wrap>
        <p>With this comparison, we are able to identify those participants whose response behaviour matches the assumption of gaze-biased evidence accumulation. In particular, we find that we accurately recover whether an individual has a gaze bias or not for 29 out of 30 individuals.</p>
        <p>Looking at the individual parameter estimates (defined as MAP of the posterior distributions), we find that the individually fitted <italic>γ</italic> values (<xref ref-type="fig" rid="pone.0226428.g004">Fig 4A</xref>) cover a wide range between -0.8 and 1, indicating strong variability in the strength of individuals’ gaze bias. We also find that <italic>γ</italic> estimates have a strong negative correlation with individuals’ scores on the behavioural gaze bias measure (<xref ref-type="fig" rid="pone.0226428.g004">Fig 4B</xref>).</p>
        <fig id="pone.0226428.g004" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0226428.g004</object-id>
          <label>Fig 4</label>
          <caption>
            <title>Individual differences in the strength of the association of gaze allocation and choice behaviour.</title>
            <p>A: Distribution of <italic>γ</italic> estimates resulting from the in-sample individual model fits. B: Association of <italic>γ</italic> estimates and individuals’ values on the behavioural gaze bias measure. The red line indicates a linear regression fit, with surrounding 95% confidence bands. Pearson’s <italic>r</italic> correlation with <italic>P</italic>-value is given.</p>
          </caption>
          <graphic xlink:href="pone.0226428.g004"/>
        </fig>
      </sec>
      <sec id="sec021">
        <title>Out-of-sample prediction</title>
        <p>We have identified those participants whose response behaviour is better described by a GLAM variant with gaze-bias than one without. Yet, this analysis does not indicate whether the GLAM is a good model of individuals’ response behaviour on an absolute level. To test this, we perform an out-of-sample prediction exercise.</p>
        <p>We divide the data of each subject into even- and odd-numbered experiment trials and use the data of the even-numbered trials to fit both GLAM variants:</p>
        <p>
          <monospace>glam_bias.exchange_data(data_even)</monospace>
        </p>
        <p>
          <monospace>glam_bias.fit(method=’<italic>MCMC</italic>’,</monospace>
        </p>
        <p>        <monospace>tune=5000,</monospace></p>
        <p>        <monospace>draws=5000,</monospace></p>
        <p>        <monospace>chains=4)</monospace></p>
        <p>
          <monospace>glam_nobias.exchange_data(data_even)</monospace>
        </p>
        <p>
          <monospace>glam_nobias.fit(method=’<italic>MCMC</italic>’,</monospace>
        </p>
        <p>        <monospace>tune=5000,</monospace></p>
        <p>        <monospace>draws=5000,</monospace></p>
        <p>        <monospace>chains=4)</monospace></p>
        <p>Subsequently, we evaluate the performance of both models in predicting individuals’ response behaviour using the MAP estimates and item value and gaze data from the odd-numbered trials. To predict response behaviour for the odd-numbered trials, we use the <monospace>predict</monospace> method. We repeat every trial 50 times in the prediction (as specified through the <monospace>n_repeats</monospace> argument) to obtain a stable pattern of predictions:</p>
        <p>
          <monospace>glam_bias.exchange_data(data_odd)</monospace>
        </p>
        <p>
          <monospace>glam_bias.predict(n_repeats=50)</monospace>
        </p>
        <p>
          <monospace>glam_nobias.exchange_data(data_odd)</monospace>
        </p>
        <p>
          <monospace>glam_nobias.predict(n_repeats=50)</monospace>
        </p>
        <p>Lastly, to determine the absolute fit of both model variants to the data, we plot the individually predicted against the individually observed data on all three behavioural metrics. To do this, we use the <monospace>plot_individual_fit</monospace> function of the <monospace>plots</monospace> module. This function takes as input the observed data, as well as a list of the predictions of all model variants that ought to be compared. The argument <monospace>prediction_labels</monospace> specifies the naming used for each model in the resulting figure. For each model variant, the function creates a row of panels, plotting the observed against the predicted data:</p>
        <p>
          <monospace>from glambox.plots import plot_individual_fit</monospace>
        </p>
        <p>
          <monospace>plot_individual_fit(observed = data_odd,</monospace>
        </p>
        <p>            <monospace>predictions=[glam_bias.prediction,</monospace></p>
        <p>                   <monospace>glam_nobias.prediction],</monospace></p>
        <p>            <monospace>prediction_labels=[’<italic>gaze-bias</italic>’,</monospace></p>
        <p>                       <monospace>‘<italic>no gaze-bias</italic>’])</monospace></p>
        <p>The resulting plot is displayed in <xref ref-type="fig" rid="pone.0226428.g005">Fig 5</xref>. We find that both model variants perform well in capturing individuals’ RTs and probability of choosing the best item (<xref ref-type="fig" rid="pone.0226428.g005">Fig 5A, 5D, 5B and 5E</xref>). Importantly, only the GLAM variant with gaze bias is able to also recover the strength of the association between individuals’ choice behaviour and gaze allocation (<xref ref-type="fig" rid="pone.0226428.g005">Fig 5C</xref>).</p>
        <fig id="pone.0226428.g005" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0226428.g005</object-id>
          <label>Fig 5</label>
          <caption>
            <title>Out-of-sample model fits.</title>
            <p>Comparison of individuals’ simulated observed response behaviour with the out-of-sample predictions of a GLAM variant with (A-C) and without gaze bias (D-F): Individuals’ mean RT (A, D), probability of choosing the best item (B, E), and influence of gaze allocation on choice probability (C, F). Points indicate individual participant means.</p>
          </caption>
          <graphic xlink:href="pone.0226428.g005"/>
        </fig>
      </sec>
      <sec id="sec022">
        <title>Conclusion</title>
        <p>GLAMbox provides an easy-to-use tool to test the presence (and variability) of gaze biases on the individual level. With GLAMbox, we can easily fit the GLAM to individual participant data, compare different model variants and predict individuals’ response behaviour. It also provides a set of analysis functions to explore behavioural differences between individuals and to compare the fit of different model variants to observed response behaviour.</p>
      </sec>
    </sec>
    <sec id="sec023">
      <title>Example 2: Hierarchical parameter estimation in cases with limited data</title>
      <p>In some research settings, the total amount of data one can collect per individual is limited, conflicting with the large amounts of data required to obtain reliable and precise individual parameter estimates from diffusion models [<xref rid="pone.0226428.ref035" ref-type="bibr">35</xref>, <xref rid="pone.0226428.ref036" ref-type="bibr">36</xref>]. Hierarchical modeling can offer a solution to this problem. Here, each individual’s parameter estimates are assumed to be drawn from a group level distribution. Thereby, during parameter estimation, individual parameter estimates are informed by the data of the entire group. This can greatly improve parameter estimation, especially in the face of limited amounts of data [<xref rid="pone.0226428.ref025" ref-type="bibr">25</xref>, <xref rid="pone.0226428.ref037" ref-type="bibr">37</xref>]. In this example, we will simulate a clinical application setting, in which different patient groups are to be compared on the strengths of their gaze biases, during a simple value-based choice task that includes eye tracking. It is reasonable to assume that the amount of data that can be collected in such a setting is limited on at least two accounts:</p>
      <list list-type="order">
        <list-item>
          <p>The number of patients available for the experiment might be low</p>
        </list-item>
        <list-item>
          <p>The number of trials that can be performed by each participant might be low, for clinical reasons (e.g., patients feel exhausted more quickly, time to perform tests is limited, etc.)</p>
        </list-item>
      </list>
      <p>Therefore, we simulate a dataset with a low number of individuals within each group (between 5 and 15 per group), and a low number of trials per participant (50 trials). We then estimate model parameters in a hierarchical fashion, and compare the group level gaze bias parameter estimates between groups.</p>
      <sec id="sec024">
        <title>Simulating data</title>
        <p>We simulate data of three patient groups (<italic>N</italic><sub>1</sub> = 5, <italic>N</italic><sub>2</sub> = 10, <italic>N</italic><sub>3</sub> = 15), with 50 trials per individual, in a simple three item value-based choice task, where participants are instructed to simply choose the item they like the best. These numbers are roughly based on a recent clinical study on the role of the prefrontal cortex in fixation-dependent value representations [<xref rid="pone.0226428.ref013" ref-type="bibr">13</xref>]. Here, the authors found no systematic differences between frontal lobe patients and controls on integration speed or the decision threshold, controlling speed-accuracy trade-offs. Therefore, in our example we only let the gaze bias parameter <italic>γ</italic> differ systematically between the groups, with means of <italic>γ</italic><sub>1</sub> = 0.7 (weak gaze bias), <italic>γ</italic><sub>2</sub> = 0.1 (moderate gaze bias) and <italic>γ</italic><sub>3</sub> = −0.5 (strong gaze bias), respectively. We do not assume any other systematic differences between the groups and sample all other model parameters from the estimates obtained from fitting the model to the data of [<xref rid="pone.0226428.ref008" ref-type="bibr">8</xref>] (for an overview of the generating parameters, see <xref ref-type="supplementary-material" rid="pone.0226428.s004">S4 Fig</xref>).</p>
        <p>Behavioural differences between the three groups are plotted in <xref ref-type="fig" rid="pone.0226428.g006">Fig 6</xref>, using the <monospace>plot_behaviour_aggregate</monospace> function from the <monospace>plots</monospace> module. Group-level summary tables can be created using the <monospace>aggregate_group_level_data</monospace> from the <monospace>analysis</monospace> module. Even though the groups only differ in the gaze bias parameter, they also exhibit differences in RT (Group 1 mean ± s.d. = 1.96 ± 0.33 s, Group 2 mean ± s.d. = 2.38 ± 1.4 s; Group 3 mean ± s.d. = 2.59 ± 1.26 ms; <xref ref-type="fig" rid="pone.0226428.g006">Fig 6A</xref>) and choice accuracy (Group 1 mean ± s.d. = 0.88 ± 0.06, Group 2 mean ± s.d. = 0.71 ± 0.07, Group 3 mean ± s.d. = 0.50 ± 0.16; <xref ref-type="fig" rid="pone.0226428.g006">Fig 6B</xref>). As is to be expected, we can also observe behavioural differences in gaze influence measure (Group 1 mean ± s.d. = 0.08 ± 0.07, Group 2 mean ± s.d. = 0.26 ± 0.11, Group 3 mean ± s.d. = 0.38 ± 0.11; <xref ref-type="fig" rid="pone.0226428.g006">Fig 6C and 6D</xref>, where the choices of Group 3 are driven by gaze more than those of the other groups.</p>
        <fig id="pone.0226428.g006" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0226428.g006</object-id>
          <label>Fig 6</label>
          <caption>
            <title>Aggregate view of the simulated data for Example 2.</title>
            <p>(A) Mean RT binned by trial difficulty (the difference between the highest item value in a choice set and the maximum value of all other items). (B) The probability that an item is chosen based on its relative value (the difference of the item’s value and the maximum value of all other items in the choice set). (C) The probability of choosing an item based on its relative gaze (the difference between the gaze towards this item and the maximum gaze towards a different item). (D) The probability of choosing an item based on its relative gaze, when correcting for the influence of its value. Bars correspond to the pooled data, while coloured lines indicate individual groups.</p>
          </caption>
          <graphic xlink:href="pone.0226428.g006"/>
        </fig>
      </sec>
      <sec id="sec025">
        <title>Building the hierarchical model</title>
        <p>When specifying the hierarchical model, we allow all model parameters to differ between the three groups. This way, we will subsequently be able to address the question whether individuals from different groups differ on one or more model parameters (including the gaze bias parameter <italic>γ</italic>, which we are mainly interested in here). As for the individual models, we first initialize the model object using the GLAM class and supply it with the behavioural data using the <monospace>data</monospace> argument. Here, we set the model kind to <monospace>‘<italic>hierarchical</italic>’</monospace> (in contrast to <monospace>‘<italic>individual</italic>’</monospace>). Further, we specify that each model parameter can vary between groups (referring to a <monospace>‘<italic>group</italic>’</monospace> variable in the data):</p>
        <p>
          <monospace>hglam = gb.GLAM(data=data)</monospace>
        </p>
        <p>
          <monospace>hglam.make_model(kind=’<italic>hierarchical</italic>’,</monospace>
        </p>
        <p>          <monospace>depends_on = dict(v=’<italic>group</italic>’,</monospace></p>
        <p>                    <monospace>gamma=’<italic>group</italic>’,</monospace></p>
        <p>                    <monospace>s=’<italic>group</italic>’,</monospace></p>
        <p>                    <monospace>tau=’<italic>group</italic>’))</monospace></p>
        <p>In this model, each parameter is set up hierarchically within each group, so that individual estimates are informed by other individuals in that group. If the researcher does not expect group differences on a parameter, this parameter can simply be omitted from the <monospace>depends_on</monospace> dictionary. The resulting model would then have a hierarchical setup of this parameter across groups, so that individual parameter estimates were informed by all other individuals (not only those in the same group).</p>
      </sec>
      <sec id="sec026">
        <title>Parameter estimation with MCMC</title>
        <p>After the model is built, the next step is to perform statistical inference over its parameters. As we have done with the individual models, we can use MCMC to approximate the parameters’ posterior distributions (see <xref ref-type="sec" rid="sec002">Methods</xref> for details). Due to the more complex structure and drastically increased number of parameters, the chains from the hierarchical model usually have higher levels autocorrelation. To still obtain a reasonable number of effective samples [<xref rid="pone.0226428.ref032" ref-type="bibr">32</xref>], we increase the number of tuning- and draw steps:</p>
        <p>
          <monospace>hglam.fit(method=’<italic>MCMC</italic>’,</monospace>
        </p>
        <p>      <monospace>draws=20000,</monospace></p>
        <p>      <monospace>tune=20000,</monospace></p>
        <p>      <monospace>chains=4)</monospace></p>
      </sec>
      <sec id="sec027">
        <title>Evaluating parameter estimates, interpreting results</title>
        <p>After sampling is finished, and the chains were checked for convergence, we can turn back to the research question: Do the groups differ with respect to their gaze biases? Questions about differences between group-level parameters can be addressed by inspecting their posterior distributions. For example, the probability that the mean <italic>γ</italic><sub>1,<italic>μ</italic></sub> for Group 1 is larger than the mean <italic>γ</italic><sub>2,<italic>μ</italic></sub> of Group 2 is given by the proportion of posterior samples in which this was the case.</p>
        <p>GLAMbox includes a <monospace>compare_parameters</monospace> function that plots posterior distributions of group level parameters. Additionally, the user can specify a list of comparisons between groups or conditions. If comparisons are specified, the posterior distributions of their difference and corresponding relevant statistics are added to the figure:</p>
        <p>
          <monospace>from glambox.plots import compare_parameters</monospace>
        </p>
        <p>
          <monospace>parameters = [’<italic>v</italic>’, ‘<italic>gamma</italic>’, ‘<italic>s</italic>’, ‘<italic>tau</italic>’]</monospace>
        </p>
        <p>
          <monospace>comparisons = [(’<italic>group1</italic>’, ‘<italic>group2</italic>’),</monospace>
        </p>
        <p>        <monospace>(’<italic>group1</italic>’, ‘<italic>group3</italic>’),</monospace></p>
        <p>        <monospace>(’<italic>group2</italic>’, ‘<italic>group3</italic>’)]</monospace></p>
        <p>
          <monospace>compare_parameters(model=hglam,</monospace>
        </p>
        <p>           <monospace>parameters=parameters,</monospace></p>
        <p>           <monospace>comparisons=comparisons)</monospace></p>
        <p>With the resulting plot (<xref ref-type="fig" rid="pone.0226428.g007">Fig 7</xref>), the researcher can infer that the groups did not differ with respect to their mean velocity parameters <italic>v</italic><sub><italic>i</italic>,<italic>μ</italic></sub> (top row, pairwise comparisons), mean accumulation noise <italic>σ</italic><sub><italic>i</italic>,<italic>μ</italic></sub> (third row), or scaling parameters <italic>τ</italic><sub><italic>i</italic>,<italic>μ</italic></sub>. The groups differ, however, in the strength of their mean gaze bias <italic>γ</italic><sub><italic>i</italic>,<italic>μ</italic></sub> (second row): All differences between the groups were statistically meaningful (as inferred by the fact that the corresponding 95% HPD did not contain zero; second row, columns 2-4).</p>
        <fig id="pone.0226428.g007" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0226428.g007</object-id>
          <label>Fig 7</label>
          <caption>
            <title>Pairwise comparison of posterior group-level parameter estimates between groups.</title>
            <p>Each row corresponds to one model parameter. The leftmost column shows the estimated posterior distributions for each parameter and group. Pairwise differences between the group posterior distributions are shown in all other columns. For each posterior distribution of the difference, the mean and 95% HPD are indicated, as well as the proportion of samples below and above zero (in red). All three groups differ on the <italic>γ</italic> parameter (row B). No evidence for differences on any of the other model parameters is found (the 95% HPD of the pairwise differences between groups all include zero).</p>
          </caption>
          <graphic xlink:href="pone.0226428.g007"/>
        </fig>
      </sec>
      <sec id="sec028">
        <title>Conclusion</title>
        <p>When faced with limited data, GLAMbox allows users to easily build and estimate hierarchical GLAM variants, including conditional dependencies of model parameters. The Bayesian inference framework allows the researcher to answer relevant questions in a straightforward fashion. To this end, GLAMbox provides basic functions for computation and visualization.</p>
      </sec>
    </sec>
    <sec id="sec029">
      <title>Example 3: Parameter recovery</title>
      <p>When performing model-based analyses of behaviour that include the interpretation of parameter estimates, or comparisons of parameter estimates between groups or conditions, the researcher should be confident that the model’s parameters are actually identifiable. In particular, the researcher needs to be confident that the set of estimated parameters unambiguously describes the observed data better than any other set of parameters. A straightforward way of testing this is to perform a parameter recovery: The general intuition of a parameter recovery analysis is to first generate a synthetic dataset from a model using a set of known parameters, and then fitting the model to the synthetic data. Finally, the estimated parameters can be compared to the known generating parameters. If they match to a satisfying degree, the parameters were recovered successfully. Previous analyses have already indicated that the GLAM’s parameters can be recovered to a satisfying degree [<xref rid="pone.0226428.ref021" ref-type="bibr">21</xref>]. Yet, the ability to identify a given set of parameters always depends on the specific features of a given dataset. The most obvious feature of a dataset that influences recoverability of model parameters is the number of data points included. Usually this quantity refers to the number of trials that participants performed. For hierarchical models, the precision of group-level estimates also depends on the number of individuals per group. Additional features that vary between datasets and that could influence parameter estimation are the observed distribution of gaze, the distribution of item values or the number of items in each trial. For this reason, it is recommended to test whether the estimated parameters of a model can be recovered in the context of a specific dataset. slac To demonstrate the procedure of a basic parameter recovery analysis using GLAMbox, suppose we have collected and loaded a dataset called <monospace>data</monospace>. In the first step, we perform parameter estimation as in the previous examples:</p>
      <p>
        <monospace>glam = gb.GLAM(data=data)</monospace>
      </p>
      <p>
        <monospace>glam.make_model(kind=’<italic>individual</italic>’)</monospace>
      </p>
      <p>
        <monospace>glam.fit(method=’<italic>MCMC</italic>’,</monospace>
      </p>
      <p>     <monospace>draws=5000,</monospace></p>
      <p>     <monospace>tune=5000,</monospace></p>
      <p>     <monospace>chains=4)</monospace></p>
      <p>The next step is to create a synthetic, model-generated dataset using the model parameters estimated from the empirical data, together with the empirically observed stimulus and gaze data using the <monospace>predict</monospace> method. Setting <monospace>n_repeats</monospace> to 1 results in a dataset of the same size as the observed one:</p>
      <p>
        <monospace>glam.predict(n_repeats=1)</monospace>
      </p>
      <p>
        <monospace>synthetic = glam.prediction</monospace>
      </p>
      <p>The synthetic dataset should resemble the empirically observed data closely. If there are major discrepancies between the synthetic and observed data, this indicates that GLAM might not be a good candidate model for the data at hand. Next, we create a new model instance, attach the synthetic data, build a model and re-estimate its parameters:</p>
      <p>
        <monospace>glam_rec = gb.GLAM(data=synthetic)</monospace>
      </p>
      <p>
        <monospace>glam_rec.make_model(kind=’<italic>individual</italic>’)</monospace>
      </p>
      <p>
        <monospace>glam_rec.fit(method=’<italic>MCMC</italic>’,</monospace>
      </p>
      <p>        <monospace>draws=5000,</monospace></p>
      <p>        <monospace>tune=5000,</monospace></p>
      <p>        <monospace>chains=4)</monospace></p>
      <p>Finally, the recovered and generating parameters can be compared. If the recovered parameters do not match the generating parameters, the parameters cannot be identified given this specific dataset. In this case, parameter estimates should not be interpreted.</p>
      <p>If, on the other hand, generating and recovered parameters do align, the parameters have been recovered successfully. This indicates that the model’s parameters can be identified unambiguously given the general characteristics of the dataset and thereby increases confidence that the parameters obtained from the empirical data are valid and can be interpreted.</p>
      <p>Here, all parameters could be recovered as illustrated in <xref ref-type="fig" rid="pone.0226428.g008">Fig 8</xref>. For most individuals, the MAP estimates and their 95% HPDs are close to the known generating parameters. Across individuals, no systematic biases in the estimation can be identified.</p>
      <fig id="pone.0226428.g008" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0226428.g008</object-id>
        <label>Fig 8</label>
        <caption>
          <title>Results from a basic parameter recovery.</title>
          <p>The lower row (E-H) shows deviations between known generating parameter values and recovered MAP estimates (circles) and their 95% HPDs (horizontal error bars) for each participant. Green (red) colour indicates that the true value is within (outside) the 95% HPD. Most parameters were recovered with small deviations. Panels A-D show distributions of deviations across individuals. Distributions are mostly centered around zero, indicating no systematic under- or overestimation (bias) across individuals.</p>
        </caption>
        <graphic xlink:href="pone.0226428.g008"/>
      </fig>
      <sec id="sec030">
        <title>Conclusion</title>
        <p>In this example, we demonstrated how to perform a basic parameter recovery for a given dataset. When successful, this increases confidence that the parameters can be identified with the given dataset.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec031">
    <title>Discussion</title>
    <p>Researchers have recently started to systematically investigate the role of visual gaze in the decision making process. By now, it is established that eye movements do not merely serve to sample information that is then processed independently to produce a choice, but that they are actively involved in the construction of preferences [<xref rid="pone.0226428.ref002" ref-type="bibr">2</xref>, <xref rid="pone.0226428.ref004" ref-type="bibr">4</xref>, <xref rid="pone.0226428.ref006" ref-type="bibr">6</xref>–<xref rid="pone.0226428.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0226428.ref010" ref-type="bibr">10</xref>, <xref rid="pone.0226428.ref014" ref-type="bibr">14</xref>, <xref rid="pone.0226428.ref015" ref-type="bibr">15</xref>, <xref rid="pone.0226428.ref021" ref-type="bibr">21</xref>, <xref rid="pone.0226428.ref038" ref-type="bibr">38</xref>]. The dominant theoretical perspective is that evidence accumulation in favor of each option is modulated by gaze allocation, so that accumulation for non-fixated options is attenuated. This mechanism is formally specified in various models of gaze-dependent decision making, such as the attentional Drift Diffusion Model (aDDM; [<xref rid="pone.0226428.ref007" ref-type="bibr">7</xref>, <xref rid="pone.0226428.ref008" ref-type="bibr">8</xref>]) and the conceptually related Gaze-weighted Linear Accumulator Model (GLAM; [<xref rid="pone.0226428.ref021" ref-type="bibr">21</xref>]). In contrast to analyses based on behavioural and eye tracking data alone, these models can act as analytical tools that enable researchers to address questions regarding specific mechanisms in the decision process, like the gaze bias.</p>
    <p>The goal of GLAM is to provide a model-based estimate of the gaze bias on the level of an individual (as indicated by GLAM’s <italic>γ</italic> parameter), in choice situations involving more than two choice alternatives. To estimate the gaze bias, GLAM describes the decision process in the form of a linear stochastic race and aggregates over the specific sequence of fixations during the decision process (by only utilizing the fraction of the decision time that each item was looked at). These two characteristics distinguish the GLAM from other existing approaches of obtaining an estimate of individuals’ gaze bias:</p>
    <p>First, the GLAM is focused on quantifying the gaze bias on the individual level. It does not capture dynamics of the decision process on the level of single fixations. If these fine-grained dynamics are of interest to the researcher, the aDDM can be used. Here, the fixation-dependent changes in evidence accumulation rates throughout the trial are not averaged out. Keeping this level of detail, however, comes at a cost: Fitting the aDDM relies on extensive model simulations (including a simulation of the fixation process; for a more detailed discussion see [<xref rid="pone.0226428.ref021" ref-type="bibr">21</xref>]). The GLAM, on the other hand, aggregates over the fixation-dependent changes in the accumulator’s drift rate in order to simplify the estimation process of the gaze bias.</p>
    <p>Second, the GLAM directly applies to choice situations involving more than two choice alternatives. While the GLAM has been shown to also capture individuals’ gaze bias and choice behaviour well in two-alternative choice situations [<xref rid="pone.0226428.ref021" ref-type="bibr">21</xref>], there exist other computational approaches that can estimate the gaze bias of an individual in binary decisions: If response times are of interest to the researcher, the gaze bias can be estimated in the form of a gaze-weighted DDM (see for example [<xref rid="pone.0226428.ref002" ref-type="bibr">2</xref>, <xref rid="pone.0226428.ref018" ref-type="bibr">18</xref>]). Similar to the GLAM, this approach also aggregates over the dynamics of the fixation process within a trial, by only utilizing the fraction of trial time that each item was looked at. In contrast to the GLAM, however, gaze-weighted DDM approaches describe the decision process in the form of a single accumulator that evolves between two decision bounds (each representing one of the two choice alternatives). For two-alternative choice scenarios, where response times are not of interest to the researcher, Smith and colleagues [<xref rid="pone.0226428.ref039" ref-type="bibr">39</xref>] proposed a method of estimating the aDDM gaze-bias parameter through a random utility model. Here, the gaze bias can be estimated in a simple logit model.</p>
    <p>Even though the advantages of applying these types of models are apparent, their use is often limited by their complexity and the high cost of implementing, validating and optimizing them. Further, there are only few off-the-shelf solutions researchers can turn to, if they want to perform model-based analyses of gaze-dependent choice data, particularly for choice settings involving more than two alternatives.</p>
    <p>With GLAMbox, we present a Python-based toolbox, built on top of PyMC3, that allows researchers to perform model-based analyses of gaze-bias effects in decision making easily. We have provided step-by-step instructions and code to perform essential modeling analyses using the GLAM. These entail application of the GLAM to individual and group-level data, specification of parameter dependencies for both within- and between-subject designs, (hierarchical) Bayesian parameter estimation, comparisons between multiple model variants, out-of-sample prediction of choice and RT data, data visualization, Bayesian comparison of posterior parameter estimates between conditions, and parameter recovery. We hope that GLAMbox will make studying the association between gaze allocation and choice behaviour more accessible. We also hope that the resulting findings will ultimately help us better understand this association, its inter-individual variability and link to brain activity.</p>
  </sec>
  <sec sec-type="supplementary-material" id="sec032">
    <title>Supporting information</title>
    <supplementary-material content-type="local-data" id="pone.0226428.s001">
      <label>S1 Fig</label>
      <caption>
        <title>Distribution of individual parameter estimates from four datasets analysed in Thomas et al. (2019).</title>
        <p>The top row contains distributions of parameter estimates across datasets. Subsequent rows show distributions per dataset: Krajbich et al. (2010; blue), Krajbich &amp; Rangel (2011; orange), Experiment 2 from Folke et al. (2017; green) and Experiment 1 from Tavares et al. (2017; red).</p>
        <p>(TIFF)</p>
      </caption>
      <media xlink:href="pone.0226428.s001.tiff">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pone.0226428.s002">
      <label>S2 Fig</label>
      <caption>
        <title>Illustration of hyperpriors.</title>
        <p>Different hyperpriors based on group-averaged parameter values were obtained from fitting the model to four different datasets (Folke et al., 2017; Krajbich et al., 2010; Krajbich &amp; Rangel, 2011; Tavares et al., 2017; see <xref ref-type="supplementary-material" rid="pone.0226428.s005">S1 Table</xref> and <xref ref-type="supplementary-material" rid="pone.0226428.s001">S1 Fig</xref>). Panels show prior distributions on group level mean (upper row) and standard deviation (lower row) for each model parameter (columns; from left to right: <italic>v</italic>, <italic>γ</italic>, <italic>σ</italic>, <italic>τ</italic>). Observed group level estimates from the four datasets are indicated as red ticks in each panel. Blue, orange and green lines represent prior distributions with increasing levels of vagueness <italic>f</italic>. They are constructed as normal distributions with mean equal to the mean of the observed group level parameters across datasets (M), and standard deviation equal to <italic>f</italic> times the observed standard deviation across datasets (SD). Higher values of <italic>f</italic> correspond to wider, less informative priors. Prior distributions are further bounded between sensible limits. The user can specify the factor <italic>f</italic> during specification of hierarchical models. By default, hyperpriors with <italic>f</italic> = 10 (orange lines) are used.</p>
        <p>(TIFF)</p>
      </caption>
      <media xlink:href="pone.0226428.s002.tiff">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pone.0226428.s003">
      <label>S3 Fig</label>
      <caption>
        <title>Distribution of individual generating GLAM parameters of Example 1.</title>
        <p>Colours indicate whether a subject was simulated with or without gaze bias.</p>
        <p>(TIFF)</p>
      </caption>
      <media xlink:href="pone.0226428.s003.tiff">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pone.0226428.s004">
      <label>S4 Fig</label>
      <caption>
        <title>Distributions of data-generating parameters for the three groups in Example 2.</title>
        <p>The top row shows distributions pooled across groups. The bottom three rows show distributions per group. Note that the groups do not differ systematically with respect to the velocity parameter <italic>v</italic>, the noise parameter <italic>σ</italic>, or the scaling parameter <italic>τ</italic> (first, second and last column; even though there is some variability between individuals). The groups differ, however, on the gaze bias parameter <italic>γ</italic> (third column): Group 1 only has a weak gaze bias (large <italic>γ</italic>), group 2 has a medium strong gaze bias (smaller <italic>γ</italic>), and group 3 has a very strong gaze bias (even smaller, negative <italic>γ</italic>).</p>
        <p>(TIFF)</p>
      </caption>
      <media xlink:href="pone.0226428.s004.tiff">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pone.0226428.s005">
      <label>S1 Table</label>
      <caption>
        <title>Description of individual parameter estimates from four datasets analysed in Thomas et al. (2019).</title>
        <p>The datasets are originally from Folke et al., 2017 (Experiment 2); Krajbich et al., 2010; Krajbich &amp; Rangel, 2011 and Tavares et al., 2017 (Experiment 1).</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0226428.s005.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="pone.0226428.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Armel</surname><given-names>KC</given-names></name>, <name><surname>Beaumel</surname><given-names>A</given-names></name>, <name><surname>Rangel</surname><given-names>A</given-names></name>. <article-title>Biasing simple choices by manipulating relative visual attention</article-title>. <source>Judgment and Decision Making</source>. <year>2008</year>;<volume>3</volume>:<fpage>396</fpage>–<lpage>403</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0226428.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Cavanagh</surname><given-names>JF</given-names></name>, <name><surname>Wiecki</surname><given-names>TV</given-names></name>, <name><surname>Kochar</surname><given-names>A</given-names></name>, <name><surname>Frank</surname><given-names>MJ</given-names></name>. <article-title>Eye tracking and pupillometry are indicators of dissociable latent decision processes</article-title>. <source>Journal of Experimental Psychology: General</source>. <year>2014</year>;<volume>143</volume>(<issue>4</issue>):<fpage>1476</fpage>–<lpage>1488</lpage>. <pub-id pub-id-type="doi">10.1037/a0035813</pub-id><pub-id pub-id-type="pmid">24548281</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Fiedler</surname><given-names>S</given-names></name>, <name><surname>Glöckner</surname><given-names>A</given-names></name>. <article-title>The Dynamics of Decision Making in Risky Choice: An Eye-Tracking Analysis</article-title>. <source>Frontiers in Psychology</source>. <year>2012</year>;<volume>3</volume><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00335</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Folke</surname><given-names>T</given-names></name>, <name><surname>Jacobsen</surname><given-names>C</given-names></name>, <name><surname>Fleming</surname><given-names>SM</given-names></name>, <name><surname>De Martino</surname><given-names>B</given-names></name>. <article-title>Explicit representation of confidence informs future value-based decisions</article-title>. <source>Nature Human Behaviour</source>. <year>2017</year>;<volume>1</volume>(<issue>1</issue>):<fpage>0002</fpage><pub-id pub-id-type="doi">10.1038/s41562-016-0002</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Glöckner</surname><given-names>A</given-names></name>, <name><surname>Herbold</surname><given-names>AK</given-names></name>. <article-title>An eye-tracking study on information processing in risky decisions: Evidence for compensatory strategies based on automatic processes</article-title>. <source>Journal of Behavioral Decision Making</source>. <year>2011</year>;<volume>24</volume>(<issue>1</issue>):<fpage>71</fpage>–<lpage>98</lpage>. <pub-id pub-id-type="doi">10.1002/bdm.684</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Konovalov</surname><given-names>A</given-names></name>, <name><surname>Krajbich</surname><given-names>I</given-names></name>. <article-title>Gaze data reveal distinct choice processes underlying model-based and model-free reinforcement learning</article-title>. <source>Nature Communications</source>. <year>2016</year>;<volume>7</volume>:<fpage>12438</fpage><pub-id pub-id-type="doi">10.1038/ncomms12438</pub-id><?supplied-pmid 27511383?><pub-id pub-id-type="pmid">27511383</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Krajbich</surname><given-names>I</given-names></name>, <name><surname>Armel</surname><given-names>C</given-names></name>, <name><surname>Rangel</surname><given-names>A</given-names></name>. <article-title>Visual fixations and the computation and comparison of value in simple choice</article-title>. <source>Nature Neuroscience</source>. <year>2010</year>;<volume>13</volume>(<issue>10</issue>):<fpage>1292</fpage>–<lpage>1298</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2635</pub-id><?supplied-pmid 20835253?><pub-id pub-id-type="pmid">20835253</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Krajbich</surname><given-names>I</given-names></name>, <name><surname>Rangel</surname><given-names>A</given-names></name>. <article-title>Multialternative drift-diffusion model predicts the relationship between visual fixations and choice in value-based decisions</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2011</year>;<volume>108</volume>(<issue>33</issue>):<fpage>13852</fpage>–<lpage>13857</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1101328108</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Pärnamets</surname><given-names>P</given-names></name>, <name><surname>Johansson</surname><given-names>P</given-names></name>, <name><surname>Hall</surname><given-names>L</given-names></name>, <name><surname>Balkenius</surname><given-names>C</given-names></name>, <name><surname>Spivey</surname><given-names>MJ</given-names></name>, <name><surname>Richardson</surname><given-names>DC</given-names></name>. <article-title>Biasing moral decisions by exploiting the dynamics of eye gaze</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2015</year>;<volume>112</volume>(<issue>13</issue>):<fpage>4170</fpage>–<lpage>4175</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1415250112</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Shimojo</surname><given-names>S</given-names></name>, <name><surname>Simion</surname><given-names>C</given-names></name>, <name><surname>Shimojo</surname><given-names>E</given-names></name>, <name><surname>Scheier</surname><given-names>C</given-names></name>. <article-title>Gaze bias both reflects and influences preference</article-title>. <source>Nature Neuroscience</source>. <year>2003</year>;<volume>6</volume>(<issue>12</issue>):<fpage>1317</fpage>–<lpage>1322</lpage>. <pub-id pub-id-type="doi">10.1038/nn1150</pub-id><?supplied-pmid 14608360?><pub-id pub-id-type="pmid">14608360</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Stewart</surname><given-names>N</given-names></name>, <name><surname>Gächter</surname><given-names>S</given-names></name>, <name><surname>Noguchi</surname><given-names>T</given-names></name>, <name><surname>Mullett</surname><given-names>TL</given-names></name>. <article-title>Eye Movements in Strategic Choice</article-title>. <source>Journal of Behavioral Decision Making</source>. <year>2016</year>;<volume>29</volume>(<issue>2-3</issue>):<fpage>137</fpage>–<lpage>156</lpage>. <pub-id pub-id-type="doi">10.1002/bdm.1901</pub-id><?supplied-pmid 27513881?><pub-id pub-id-type="pmid">27513881</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Stewart</surname><given-names>N</given-names></name>, <name><surname>Hermens</surname><given-names>F</given-names></name>, <name><surname>Matthews</surname><given-names>WJ</given-names></name>. <article-title>Eye Movements in Risky Choice</article-title>. <source>Journal of Behavioral Decision Making</source>. <year>2016</year>;<volume>29</volume>(<issue>2-3</issue>):<fpage>116</fpage>–<lpage>136</lpage>. <pub-id pub-id-type="doi">10.1002/bdm.1854</pub-id><?supplied-pmid 27522985?><pub-id pub-id-type="pmid">27522985</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Vaidya</surname><given-names>AR</given-names></name>, <name><surname>Fellows</surname><given-names>LK</given-names></name>. <article-title>Testing necessary regional frontal contributions to value assessment and fixation-based updating</article-title>. <source>Nature Communications</source>. <year>2015</year>;<volume>6:</volume><fpage>10120</fpage><pub-id pub-id-type="doi">10.1038/ncomms10120</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Tavares</surname><given-names>G</given-names></name>, <name><surname>Perona</surname><given-names>P</given-names></name>, <name><surname>Rangel</surname><given-names>A</given-names></name>. <article-title>The Attentional Drift Diffusion Model of Simple Perceptual Decision-Making</article-title>. <source>Frontiers in Neuroscience</source>. <year>2017</year>;<volume>11</volume><pub-id pub-id-type="doi">10.3389/fnins.2017.00468</pub-id><?supplied-pmid 28894413?><pub-id pub-id-type="pmid">28894413</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Ashby</surname><given-names>NJS</given-names></name>, <name><surname>Jekel</surname><given-names>M</given-names></name>, <name><surname>Dickert</surname><given-names>S</given-names></name>, <name><surname>Glöckner</surname><given-names>A</given-names></name>. <article-title>Finding the right fit: A comparison of process assumptions underlying popular drift-diffusion models</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>. <year>2016</year>;<volume>42</volume>(<issue>12</issue>):<fpage>1982</fpage>–<lpage>1993</lpage>. <pub-id pub-id-type="doi">10.1037/xlm0000279</pub-id><?supplied-pmid 27336785?><pub-id pub-id-type="pmid">27336785</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Fisher</surname><given-names>G</given-names></name>. <article-title>An attentional drift diffusion model over binary-attribute choice</article-title>. <source>Cognition</source>. <year>2017</year>;<volume>168</volume>:<fpage>34</fpage>–<lpage>45</lpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2017.06.007</pub-id><?supplied-pmid 28646751?><pub-id pub-id-type="pmid">28646751</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Krajbich</surname><given-names>I</given-names></name>, <name><surname>Lu</surname><given-names>D</given-names></name>, <name><surname>Camerer</surname><given-names>C</given-names></name>, <name><surname>Rangel</surname><given-names>A</given-names></name>. <article-title>The Attentional Drift-Diffusion Model Extends to Simple Purchasing Decisions</article-title>. <source>Frontiers in Psychology</source>. <year>2012</year>;<volume>3</volume><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00193</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Lopez-Persem</surname><given-names>A</given-names></name>, <name><surname>Domenech</surname><given-names>P</given-names></name>, <name><surname>Pessiglione</surname><given-names>M</given-names></name>. <article-title>How prior preferences determine decision-making frames and biases in the human brain</article-title>. <source>eLife</source>. <year>2016</year>;<volume>5</volume>:<fpage>e20317</fpage><pub-id pub-id-type="doi">10.7554/eLife.20317</pub-id><?supplied-pmid 27864918?><pub-id pub-id-type="pmid">27864918</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Trueblood</surname><given-names>JS</given-names></name>, <name><surname>Brown</surname><given-names>SD</given-names></name>, <name><surname>Heathcote</surname><given-names>A</given-names></name>, <name><surname>Busemeyer</surname><given-names>JR</given-names></name>. <article-title>Not Just for Consumers: Context Effects Are Fundamental to Decision Making</article-title>. <source>Psychological Science</source>. <year>2013</year>;<volume>24</volume>(<issue>6</issue>):<fpage>901</fpage>–<lpage>908</lpage>. <pub-id pub-id-type="doi">10.1177/0956797612464241</pub-id><?supplied-pmid 23610134?><pub-id pub-id-type="pmid">23610134</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Smith</surname><given-names>SM</given-names></name>, <name><surname>Krajbich</surname><given-names>I</given-names></name>. <article-title>Attention and choice across domains</article-title>. <source>Journal of Experimental Psychology: General</source>. <year>2018</year>;<volume>147</volume>(<issue>12</issue>):<fpage>1810</fpage>–<lpage>1826</lpage>. <pub-id pub-id-type="doi">10.1037/xge0000482</pub-id><pub-id pub-id-type="pmid">30247061</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Thomas</surname><given-names>AW</given-names></name>, <name><surname>Molter</surname><given-names>F</given-names></name>, <name><surname>Krajbich</surname><given-names>I</given-names></name>, <name><surname>Heekeren</surname><given-names>HR</given-names></name>, <name><surname>Mohr</surname><given-names>PNC</given-names></name>. <article-title>Gaze bias differences capture individual choice behaviour</article-title>. <source>Nature Human Behaviour</source>. <year>2019</year>;<volume>3</volume>(<issue>6</issue>):<fpage>625</fpage>–<lpage>635</lpage>. <pub-id pub-id-type="doi">10.1038/s41562-019-0584-8</pub-id><?supplied-pmid 30988476?><pub-id pub-id-type="pmid">30988476</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref022">
      <label>22</label>
      <mixed-citation publication-type="journal"><name><surname>Cavanagh</surname><given-names>SE</given-names></name>, <name><surname>Malalasekera</surname><given-names>WMN</given-names></name>, <name><surname>Miranda</surname><given-names>B</given-names></name>, <name><surname>Hunt</surname><given-names>LT</given-names></name>, <name><surname>Kennerley</surname><given-names>SW</given-names></name>. <article-title>Visual Fixation Patterns during Economic Choice Reflect Covert Valuation Processes That Emerge with Learning</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2019</year>; p. <fpage>201906662</fpage>.</mixed-citation>
    </ref>
    <ref id="pone.0226428.ref023">
      <label>23</label>
      <mixed-citation publication-type="journal"><name><surname>Gwinn</surname><given-names>R</given-names></name>, <name><surname>Krajbich</surname><given-names>I</given-names></name>. <article-title>Attitudes and Attention</article-title>. <source>Journal of Experimental Social Psychology</source>. <year>2020</year>;<volume>86</volume>:<fpage>103892</fpage><pub-id pub-id-type="doi">10.1016/j.jesp.2019.103892</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Bogacz</surname><given-names>R</given-names></name>, <name><surname>Brown</surname><given-names>E</given-names></name>, <name><surname>Moehlis</surname><given-names>J</given-names></name>, <name><surname>Holmes</surname><given-names>P</given-names></name>, <name><surname>Cohen</surname><given-names>JD</given-names></name>. <article-title>The Physics of Optimal Decision Making: A Formal Analysis of Models of Performance in Two-Alternative Forced-Choice Tasks</article-title>. <source>Psychological review</source>. <year>2006</year>;<volume>113</volume>(<issue>4</issue>):<fpage>700</fpage><pub-id pub-id-type="doi">10.1037/0033-295X.113.4.700</pub-id><?supplied-pmid 17014301?><pub-id pub-id-type="pmid">17014301</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref025">
      <label>25</label>
      <mixed-citation publication-type="journal"><name><surname>Wiecki</surname><given-names>TV</given-names></name>, <name><surname>Sofer</surname><given-names>I</given-names></name>, <name><surname>Frank</surname><given-names>MJ</given-names></name>. <article-title>HDDM: Hierarchical Bayesian estimation of the Drift-Diffusion Model in Python</article-title>. <source>Frontiers in Neuroinformatics</source>. <year>2013</year>;<volume>7</volume><pub-id pub-id-type="doi">10.3389/fninf.2013.00014</pub-id><?supplied-pmid 23935581?><pub-id pub-id-type="pmid">23935581</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref026">
      <label>26</label>
      <mixed-citation publication-type="journal"><name><surname>Ratcliff</surname><given-names>R</given-names></name>, <name><surname>Tuerlinckx</surname><given-names>F</given-names></name>. <article-title>Estimating parameters of the diffusion model: Approaches to dealing with contaminant reaction times and parameter variability</article-title>. <source>Psychonomic Bulletin &amp; Review</source>. <year>2002</year>;<volume>9</volume>(<issue>3</issue>):<fpage>438</fpage>–<lpage>481</lpage>. <pub-id pub-id-type="doi">10.3758/BF03196302</pub-id><pub-id pub-id-type="pmid">12412886</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Salvatier</surname><given-names>J</given-names></name>, <name><surname>Wiecki</surname><given-names>TV</given-names></name>, <name><surname>Fonnesbeck</surname><given-names>C</given-names></name>. <article-title>Probabilistic programming in Python using PyMC3</article-title>. <source>PeerJ Computer Science</source>. <year>2016</year>;<volume>2</volume>:<fpage>e55</fpage><pub-id pub-id-type="doi">10.7717/peerj-cs.55</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref028">
      <label>28</label>
      <mixed-citation publication-type="other">McKinney W. Data structures for statistical computing in python. In: Proceedings of the 9th Python in Science Conference. vol. 445. Austin, TX; 2010. p. 51–56.</mixed-citation>
    </ref>
    <ref id="pone.0226428.ref029">
      <label>29</label>
      <mixed-citation publication-type="journal"><name><surname>Voss</surname><given-names>A</given-names></name>, <name><surname>Voss</surname><given-names>J</given-names></name>. <article-title>Fast-dm: A free program for efficient diffusion model analysis</article-title>. <source>Behavior Research Methods</source>. <year>2007</year>;<volume>39</volume>(<issue>4</issue>):<fpage>767</fpage>–<lpage>775</lpage>. <pub-id pub-id-type="doi">10.3758/bf03192967</pub-id><?supplied-pmid 18183889?><pub-id pub-id-type="pmid">18183889</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref030">
      <label>30</label>
      <mixed-citation publication-type="book"><name><surname>Gamerman</surname><given-names>D</given-names></name>, <name><surname>Lopes</surname><given-names>HF</given-names></name>. <source>Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference</source>, <edition>Second Edition</edition><publisher-name>Chapman and Hall/CRC</publisher-name>; <year>2006</year>.</mixed-citation>
    </ref>
    <ref id="pone.0226428.ref031">
      <label>31</label>
      <mixed-citation publication-type="book"><name><surname>Gelman</surname><given-names>A</given-names></name>, <name><surname>Shirley</surname><given-names>K</given-names></name>. <chapter-title>Inference from Simulations and Monitoring Convergence</chapter-title> In: <source>Handbook of Markov Chain Monte Carlo</source>. <publisher-name>Chapman and Hall/CRC</publisher-name>; <year>2011</year> p. <fpage>189</fpage>–<lpage>200</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0226428.ref032">
      <label>32</label>
      <mixed-citation publication-type="book"><name><surname>Kruschke</surname><given-names>J</given-names></name>. <source>Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan</source>. <publisher-name>Academic Press</publisher-name>; <year>2014</year>.</mixed-citation>
    </ref>
    <ref id="pone.0226428.ref033">
      <label>33</label>
      <mixed-citation publication-type="journal"><name><surname>Kucukelbir</surname><given-names>A</given-names></name>, <name><surname>Tran</surname><given-names>D</given-names></name>, <name><surname>Ranganath</surname><given-names>R</given-names></name>, <name><surname>Gelman</surname><given-names>A</given-names></name>, <name><surname>Blei</surname><given-names>DM</given-names></name>. <article-title>Automatic differentiation variational inference</article-title>. <source>The Journal of Machine Learning Research</source>. <year>2017</year>;<volume>18</volume>(<issue>1</issue>):<fpage>430</fpage>–<lpage>474</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0226428.ref034">
      <label>34</label>
      <mixed-citation publication-type="journal"><name><surname>Vehtari</surname><given-names>A</given-names></name>, <name><surname>Gelman</surname><given-names>A</given-names></name>, <name><surname>Gabry</surname><given-names>J</given-names></name>. <article-title>Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC</article-title>. <source>Statistics and Computing</source>. <year>2017</year>;<volume>27</volume>(<issue>5</issue>):<fpage>1413</fpage>–<lpage>1432</lpage>. <pub-id pub-id-type="doi">10.1007/s11222-016-9696-4</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref035">
      <label>35</label>
      <mixed-citation publication-type="journal"><name><surname>Lerche</surname><given-names>V</given-names></name>, <name><surname>Voss</surname><given-names>A</given-names></name>, <name><surname>Nagler</surname><given-names>M</given-names></name>. <article-title>How many trials are required for parameter estimation in diffusion modeling? A comparison of different optimization criteria</article-title>. <source>Behavior Research Methods</source>. <year>2017</year>;<volume>49</volume>(<issue>2</issue>):<fpage>513</fpage>–<lpage>537</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-016-0740-2</pub-id><?supplied-pmid 27287445?><pub-id pub-id-type="pmid">27287445</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref036">
      <label>36</label>
      <mixed-citation publication-type="journal"><name><surname>Voss</surname><given-names>A</given-names></name>, <name><surname>Nagler</surname><given-names>M</given-names></name>, <name><surname>Lerche</surname><given-names>V</given-names></name>. <article-title>Diffusion Models in Experimental Psychology</article-title>. <source>Experimental Psychology</source>. <year>2013</year>;<volume>60</volume>(<issue>6</issue>):<fpage>385</fpage>–<lpage>402</lpage>. <pub-id pub-id-type="doi">10.1027/1618-3169/a000218</pub-id><?supplied-pmid 23895923?><pub-id pub-id-type="pmid">23895923</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref037">
      <label>37</label>
      <mixed-citation publication-type="journal"><name><surname>Ratcliff</surname><given-names>R</given-names></name>, <name><surname>Childers</surname><given-names>R</given-names></name>. <article-title>Individual Differences and Fitting Methods for the Two-Choice Diffusion Model of Decision Making</article-title>. <source>Decision (Washington, DC)</source>. <year>2015</year>;<volume>2015</volume>.</mixed-citation>
    </ref>
    <ref id="pone.0226428.ref038">
      <label>38</label>
      <mixed-citation publication-type="journal"><name><surname>Orquin</surname><given-names>JL</given-names></name>, <name><surname>Mueller Loose</surname><given-names>S</given-names></name>. <article-title>Attention and choice: A review on eye movements in decision making</article-title>. <source>Acta Psychologica</source>. <year>2013</year>;<volume>144</volume>(<issue>1</issue>):<fpage>190</fpage>–<lpage>206</lpage>. <pub-id pub-id-type="doi">10.1016/j.actpsy.2013.06.003</pub-id><?supplied-pmid 23845447?><pub-id pub-id-type="pmid">23845447</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0226428.ref039">
      <label>39</label>
      <mixed-citation publication-type="journal"><name><surname>Smith</surname><given-names>SM</given-names></name>, <name><surname>Krajbich</surname><given-names>I</given-names></name>, <name><surname>Webb</surname><given-names>R</given-names></name>. <article-title>Estimating the dynamic role of attention via random utility</article-title>. <source>Journal of the Economic Science Association</source>. <year>2019</year>; p. <fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<sub-article id="pone.0226428.r001" article-type="aggregated-review-documents">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0226428.r001</article-id>
    <title-group>
      <article-title>Decision Letter 0</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Smith</surname>
          <given-names>David V.</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2019 David V. Smith</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>David V. Smith</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article id="rel-obj001" ext-link-type="doi" xlink:href="10.1371/journal.pone.0226428" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>0</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">26 Sep 2019</named-content>
    </p>
    <p>PONE-D-19-24023</p>
    <p>GLAMbox: A Python toolbox for investigating the association between gaze allocation and decision behaviour</p>
    <p>PLOS ONE</p>
    <p>Dear Prof. Dr. Mohr,</p>
    <p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
    <p>All three reviewers were enthusiastic about the paper and toolbox. The primary recommendations involve clarifying some of the parameter choices and their impact on the results and interpretation. The reviewers also highlight the importance of contextualizing the approach and comparing/contrasting with previous work. In addition, note that Reviewers 2 and 3 both had trouble using the toolbox, so it would be important to update documentation and examples in your revision. </p>
    <p>We would appreciate receiving your revised manuscript by Nov 10 2019 11:59PM. When you are ready to submit your revision, log on to <ext-link ext-link-type="uri" xlink:href="https://www.editorialmanager.com/pone/">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
    <p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter.</p>
    <p>To enhance the reproducibility of your results, we recommend that if applicable you deposit your laboratory protocols in protocols.io, where a protocol can be assigned its own identifier (DOI) such that it can be cited independently in the future. For instructions see: <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols">http://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link></p>
    <p>Please include the following items when submitting your revised manuscript:</p>
    <p>
      <list list-type="bullet">
        <list-item>
          <p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). This letter should be uploaded as separate file and labeled 'Response to Reviewers'.</p>
        </list-item>
        <list-item>
          <p>A marked-up copy of your manuscript that highlights changes made to the original version. This file should be uploaded as separate file and labeled 'Revised Manuscript with Track Changes'.</p>
        </list-item>
        <list-item>
          <p>An unmarked version of your revised paper without tracked changes. This file should be uploaded as separate file and labeled 'Manuscript'.</p>
        </list-item>
      </list>
    </p>
    <p>Please note while forming your response, if your article is accepted, you may have the opportunity to make the peer review history publicly available. The record will include editor decision letters (with reviews) and your responses to reviewer comments. If eligible, we will contact you to opt in or out.</p>
    <p>We look forward to receiving your revised manuscript.</p>
    <p>Kind regards,</p>
    <p>David V. Smith, Ph.D.</p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
    <p>Journal Requirements:</p>
    <p>1. When submitting your revision, we need you to address these additional requirements.</p>
    <p>Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at</p>
    <p><ext-link ext-link-type="uri" xlink:href="http://www.journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf">http://www.journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</ext-link> and <ext-link ext-link-type="uri" xlink:href="http://www.journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf">http://www.journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</ext-link></p>
    <p>2. Thank you for stating the following in the Acknowledgments Section of your manuscript:</p>
    <p>The Junior Professorship of P.N.C.M. as well as the associated Dahlem International Network Junior Research Group Neuroeconomics is supported by Freie Universit ¨at Berlin within the Excellence Initiative of the German Research Foundation (DFG). Further support for P.N.C.M. is provided by the WZB Berlin Social Science Center. F.M. is supported by the International Max Planck Research School on the Life Course (LIFE). The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript</p>
    <p>We note that you have provided funding information that is not currently declared in your Funding Statement. However, funding information should not appear in the Acknowledgments section or other areas of your manuscript. We will only publish funding information present in the Funding Statement section of the online submission form.</p>
    <p>Please remove any funding-related text from the manuscript and let us know how you would like to update your Funding Statement. Currently, your Funding Statement reads as follows:</p>
    <p>The author(s) received no specific funding for this work.</p>
    <p>[Note: HTML markup is below. Please do not edit.]</p>
    <p>Reviewers' comments:</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <bold>Comments to the Author</bold>
    </p>
    <p>1. Is the manuscript technically sound, and do the data support the conclusions?</p>
    <p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>Reviewer #3: Yes</p>
    <p>**********</p>
    <p>2. Has the statistical analysis been performed appropriately and rigorously? </p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>Reviewer #3: Yes</p>
    <p>**********</p>
    <p>3. Have the authors made all data underlying the findings in their manuscript fully available?</p>
    <p>The <ext-link ext-link-type="uri" xlink:href="http://www.plosone.org/static/policies.action#sharing">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.</p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>Reviewer #3: Yes</p>
    <p>**********</p>
    <p>4. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
    <p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>Reviewer #3: Yes</p>
    <p>**********</p>
    <p>5. Review Comments to the Author</p>
    <p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
    <p>Reviewer #1: This is a very interesting study that introduces the gaze-weighted linear accumulator model (GLAM) and validates a new toolbox for fitting the model in Python to understand the association between gaze bias and decision making. In this manuscript, the authors tried to validate the GLAM and the toolbox in three different cases: individual-level parameter estimation, group-level parameter estimation using hierarchical Bayesian method, and parameter recovery. The authors show convincing and converging evidence that the GLAM performs better than a model without reflecting gaze bias in value-based decisions and explain how to use the toolbox to fit GLAM in Python. I believe the manuscript is good to be considered publication in PLOS ONE with a revision. Here, I summarize several points that I was not clear or had questions while reading it through:</p>
    <p>1. It was not quite clear what the general speed parameter (parameter v in Eq 5, page 4) in the model captures. What does the general speed exactly mean? Does it capture speed-accuracy tradeoff as boundary parameter in DDM? It seems like Eq 6 captures accuracy-speed tradeoff using the speed parameter, but it is not quite clear for me. If the authors could provide more about this parameter, it would be better for readers to understand the parameter better.</p>
    <p>2. Is Eq 4 correct? Parentheses are missing after “exp”?</p>
    <p>3. In Example 1, the authors collected liking scores after choice and used the liking scores to identify the higher value items or the best items. However, choice-induced preference literature has shown that choices not only reveal preferences, but also shape preferences. Thus, chosen items might have higher liking scores than non-chosen items not only because participants liked them but also because they chose them. Is there any way to rule out this issue? Or, do the authors expect different or the results if liking scores are measured in advance and test the model?</p>
    <p>4. The “glam_bias.fit” and “glam_nobias.fit” lines in page 10 and page 11 do not have “chains” attribute (which is 4 in default), but the authors’ suggestion for model convergence is 2 in the main text. I found that the script in Github includes chains parameter in the script. Including the ‘chains attribute in the script examples in the manuscript would help readers more intuitively.</p>
    <p>5. The authors mention about the results of model comparison test result in page 11. Without output figure or table, it was not quite easy to understand what the results look like. The Github script did not include model comparison results. Adding the model comparison result table in the revised manuscript would help readers.</p>
    <p>6. Overall, I felt that the captions of figures are too long and redundant with the main text. I think it would be better to explain more in the main text and shorten the caption of figures.</p>
    <p>7. I could not install the toolbox using pip or conda. If the authors could make it available (or at least inform how to install on a local computer), it would help researchers access the toolbox.</p>
    <p>Minor points</p>
    <p>8. I am not sure this is a technical issue or not, but the figures were not clear. Some letters were broken too. Please check the clarity of the figures.</p>
    <p>9. The number label in page 5 for “individual parameter estimation details” seems quite abrupt. Please drop the numbers.</p>
    <p>Reviewer #2: Summary: this paper provides an overview of how to use the authors’ toolbox to measure individual and group differences in the extent to which gaze information influences decision-making. The GLAMbox approach was first introduced in an empirical paper published this year (Thomas et al., 2019), and the current paper expands upon the method to enable other researchers to use it in an informed way. This new method is useful for researchers who use eye tracking as a tool to understand decision-making, and the GLAMbox adds a contribution to the field as a whole. Past research has focused on one overall discount value for unattended information, whereas this allows fitting of individual differences. Moreover, it seems to be a more efficient implementation than past work, which makes it more accessible. The authors are thorough in both describing model-fitting as well as parameter recovery to promote best practices. I think that a few clarifications and additions could make this paper stronger:</p>
    <p>1. It would be helpful in the introduction and/or discussion to explain more how different individual gaze biases might arise (familiarity with items, more goal-driven approach, etc.). There is not one obvious reason, but I think some discussion of why this is important is useful. For example, Smith &amp; Krajbich (2018) discuss “tunnel vision” as one possible mechanism.</p>
    <p>2. Section 0.0.1 “Individual parameter estimation details” says that the ranges chosen were derived from “sensible limits based on previous applications” (line 118). It would be helpful to have more discussion of how these sensible limits are arrived at, whether they will apply broadly to all data sets, or how to determine appropriate ranges for one’s own data including theoretical constraints. Furthermore, the aDDM that this method seems to draw its inspiration from, uses a discount range for attention of [0-1] (Krajbich et al., 2010; Krajbich et al., 2015). However, the authors here use a range including large negative values (-10) up to 1. I think it’s important to explain why negative values are used, how to interpret them (active forgetting or leaky accumulation?) and to provide a theoretical justification for their inclusion here given the context of previous literature.</p>
    <p>3. The GLAM is explained in an option-wise manner. Given that recent research has shown that some individuals compare options with multiple attributes in an attribute-wise manner, would there be a way to incorporate attribute-wise comparisons into the GLAM? This may be outside the scope of the paper, but if there is a relatively easy way to implement it, that could be worth including.</p>
    <p>4. Figure 4 shows a strong correlation between gamma and the behavioral gaze bias. This is a good confirmation, but the behavioral measure of gaze-choice association (lines 242-246) is only very briefly mentioned. If they are so highly correlated, what does gamma add beyond the behavioral gaze bias measure? Is its main advantage including it as part of the full model estimation process?</p>
    <p>5. Example 2: does it make sense to expect similar parameter ranges for patients compared to a young, healthy sample? Parameters such as noise might be higher and drift rate might be slower. I don’t think that this should affect the gaze bias estimation, but it might affect which parameters are used to constrain hierarchical estimation.</p>
    <p>6. In Fig. 6, choice difficulty is defined as the highest value is compared with the average of the other values. However, I think a choice would be more difficult if the second highest value were quite similar to the highest value, regardless of the lower value options. For example, a choice with two similarly high value options and one very low value option would be harder than one with one high value option and 2 medium value options, but both would be similar difficulty by the metric currently used. Is there a reason this is favored over comparing the best and next best options?</p>
    <p>7. A different number of draws and burn in samples are used in model-fitting from Example 1 to Example 2; is there a reason for this? Perhaps briefly explain why if it is relevant to users.</p>
    <p>Small clarifications/phrasing corrections:</p>
    <p>• In the abstract, I would rephrase the middle sentence beginning with “However, only few decision models exist…” to something like, “However, few decision models exist that enable a straightforward characterization of the gaze-choice association at the individual level…”</p>
    <p>• In the introduction line 4, “It was repeatedly shown” should be changed to “It has been repeatedly shown”</p>
    <p>• Line 66, “i” is not explained. It can be inferred that it indexes each item, but it should be explicitly mentioned.</p>
    <p>• Figure 1 and equation 2 (lines 76-77). What does the “maximum of all other decision signals mean”? The highest average absolute decision signal among the item options? My interpretation is that you are subtracting the highest value option from all others as a sort of normalization, but this isn’t quite coming through clearly.</p>
    <p>o Line 76, equation 2. What does J represent? From reading the empirical paper using the same method, Thomas et al., 2019, it sounds like J represents the set of all items, but it should also be defined in this paper.</p>
    <p>• Figure 1e is above panel d in a way that violates expectations of reading/processing material, and I think it would be clearer if the panel positions for d and e were switched (even though I understand it was likely put there for design reasons).</p>
    <p>• Figure 3 flips the orientation of the axes between D, E, and F so that the same variables are on the x versus y axes, which makes it harder to process them all at once. It would better fit your description for “gaze influence on choice” to be on the x-axis in F. I realize that these are non-directional correlations and that the axes may be flipped to better align with the above histograms, but I find it harder to parse this way (instead of just including the histogram distributions with their own separate x-axis labels).</p>
    <p>• Figure 5, I might put “simulated observed” instead of just “observed” on the x-axis to make sure that readers don’t get confused and think that the data is actual raw data rather than data simulated from inputted parameters. Alternatively you could mention it in the figure caption.</p>
    <p>Reviewer #3: This study presents a python toolbox to fit parameters for the authors’ gaze-weighted linear accumulator model, capitalizing on python’s Bayesian package PyMC3. Fitting a DDM is quite computationally complex, with many researchers who are interested in the theory perhaps not having the skills required to write their own estimation code. Moreover, it is typically a very time-consuming process to fit these kinds of models, so a faster methods are always welcome additions. They use a race model which can handle non-binary choices will help better approximate real-world settings.</p>
    <p>I really like that they have included parameter recovery into their toolbox. In addition, doing model comparison with and without the gaze bias parameter is nice – particularly as it can help other researchers understand under which situations gaze is and is not important. Some guidance on what to do to compare multiple conditions/tasks would be a nice feature. In addition, I think the github documentation needs more details and guidance (e.g., simply to tell the reader to use Jupyter to open the readme). In addition, I ran into some errors using the code, which could have been the result of poor documentation.</p>
    <p>I have the following suggestions/issues:</p>
    <p>I would like to point the authors to Smith, Krajbich, and Webb (Estimating the dynamic role of attention via random utility – 2019) which estimates aDDM’s theta parameter using a very fast and simple regression method, which seems relevant to their work.</p>
    <p>In addition, it would be nice to see a discussion/comparison of this to other race models (an unacquainted reader may incorrectly believe that theirs is the first race model to vit ddm-eqsue parameters upon reading their introduction), as well as a discussion of the drawbacks of race models relative to more traditional aDDM methods.</p>
    <p>Although this reviewer is familiar with the authors’ previous work on the GLAM model, it may be useful to have a section with more comprehensive introduction to the model/theory and comparison to similar models like the aDDM (subject to editorial guidance – I am not sure what is appropriate).</p>
    <p>A much more extensive readme file and instructions should be included. For example, this reviewer know that the examples/readme are to be opened in jupyter, but some (nay, many – esp. those searching for a toolbox rather than writing their own code) may not. A basic guide for others would be helpful.</p>
    <p>When I attempted to run the parameter recovery exercise, I received an error originating in glam.fit (AttributeError: Can't pickle local object 'make_subject_model.&lt;locals&gt;.lda_logp'), but don’t know whether that was my poor execution or a problem in the code.</p>
    <p> &lt;/locals&gt;</p>
    <p>**********</p>
    <p>6. PLOS authors have the option to publish the peer review history of their article (<ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link ext-link-type="uri" xlink:href="https://www.plos.org/privacy-policy">Privacy Policy</ext-link>.</p>
    <p>Reviewer #1: No</p>
    <p>Reviewer #2: No</p>
    <p>Reviewer #3: No</p>
    <p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files to be viewed.]</p>
    <p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link ext-link-type="uri" xlink:href="https://pacev2.apexcovantage.com/">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email us at <email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p>
  </body>
</sub-article>
<sub-article id="pone.0226428.r002" article-type="author-comment">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0226428.r002</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 0</article-title>
    </title-group>
    <related-article id="rel-obj002" ext-link-type="doi" xlink:href="10.1371/journal.pone.0226428" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">12 Nov 2019</named-content>
    </p>
    <p>Please find attached the document "Response to Reviewers.pdf".</p>
    <supplementary-material content-type="local-data" id="pone.0226428.s006">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">Response to Reviewers.pdf</named-content></p>
      </caption>
      <media xlink:href="pone.0226428.s006.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article id="pone.0226428.r003" article-type="aggregated-review-documents">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0226428.r003</article-id>
    <title-group>
      <article-title>Decision Letter 1</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Smith</surname>
          <given-names>David V.</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2019 David V. Smith</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>David V. Smith</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article id="rel-obj003" ext-link-type="doi" xlink:href="10.1371/journal.pone.0226428" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">27 Nov 2019</named-content>
    </p>
    <p>GLAMbox: A Python toolbox for investigating the association between gaze allocation and decision behaviour</p>
    <p>PONE-D-19-24023R1</p>
    <p>Dear Dr. Mohr,</p>
    <p>We are pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it complies with all outstanding technical requirements.</p>
    <p>Within one week, you will receive an e-mail containing information on the amendments required prior to publication. When all required modifications have been addressed, you will receive a formal acceptance letter and your manuscript will proceed to our production department and be scheduled for publication.</p>
    <p>Shortly after the formal acceptance letter is sent, an invoice for payment will follow. To ensure an efficient production and billing process, please log into Editorial Manager at <ext-link ext-link-type="uri" xlink:href="https://www.editorialmanager.com/pone/">https://www.editorialmanager.com/pone/</ext-link>, click the "Update My Information" link at the top of the page, and update your user information. If you have any billing related questions, please contact our Author Billing department directly at <email>authorbilling@plos.org</email>.</p>
    <p>If your institution or institutions have a press office, please notify them about your upcoming paper to enable them to help maximize its impact. If they will be preparing press materials for this manuscript, you must inform our press team as soon as possible and no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <email>onepress@plos.org</email>.</p>
    <p>With kind regards,</p>
    <p>David V. Smith, Ph.D.</p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
    <p>Additional Editor Comments (optional):</p>
    <p>Two out of three of the original reviewers were able to re-review the manuscript. Both reviewers agreed that their original comments had been addressed. Both reviewers also made some additional (minor) suggestions that can be considered in the post-acceptance stage of production. These suggestions were primarily tied to clarifications regarding the model parameters.</p>
    <p>Reviewers' comments:</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <bold>Comments to the Author</bold>
    </p>
    <p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the “Comments to the Author” section, enter your conflict of interest statement in the “Confidential to Editor” section, and submit your "Accept" recommendation.</p>
    <p>Reviewer #1: All comments have been addressed</p>
    <p>Reviewer #2: (No Response)</p>
    <p>**********</p>
    <p>2. Is the manuscript technically sound, and do the data support the conclusions?</p>
    <p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>**********</p>
    <p>3. Has the statistical analysis been performed appropriately and rigorously? </p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>**********</p>
    <p>4. Have the authors made all data underlying the findings in their manuscript fully available?</p>
    <p>The <ext-link ext-link-type="uri" xlink:href="http://www.plosone.org/static/policies.action#sharing">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.</p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>**********</p>
    <p>5. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
    <p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>**********</p>
    <p>6. Review Comments to the Author</p>
    <p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
    <p>Reviewer #1: (No Response)</p>
    <p>Reviewer #2: All of my main concerns have been addressed. I have a few further minor suggestions/clarifications that the authors may consider but don’t require another round of review.</p>
    <p>1) The range for most parameters was expanded in the revision, but the range on gamma was narrowed from (-10,1) to (-2,1). I wasn’t sure if the previous range was overly large, or if this was a typo. Looking at the prior data set ranges it seems like this correction is appropriate, but I wanted to flag it since there wasn’t a clear reason.</p>
    <p>2) In example 1, (lines 295-297; 355-356) the authors simulate gaze data assuming it is randomly distributed with respect to value, which I agree is the most neutral way to simulate the data. However, there is literature suggesting a bi-directional interaction between attention and value (attention drives value accumulation, but reward associations can also capture gaze). Simply adding a sentence clarifying that the negative relationship between gaze bias and probability of choosing the higher valued options will hold even if the assumption of completely random gaze is relaxed (as confirmed in the authors’ 2019 empirical paper using GLAMbox), but that the relationship might be weaker (ie: if gaze is drawn to higher value options which research suggests is the case) would help acknowledge the bi-directional nature of attention and value.</p>
    <p>Small wording corrections:</p>
    <p>Line 152-153, “with smaller values producing slower and more accurate responses” I would add “, respectively” to make it clear that smaller values of v produce slower responses and smaller values of sigma produce more accurate responses rather than both serving both roles.</p>
    <p>Line 157 “which” should be “with”</p>
    <p>Line 305, The paper simulation example doesn't include the code “value_range=(1, 10))”, although it is in the online documentation. I would add it to the paper, as I was confused when reading as to how that information was simulated without being entered into the function.</p>
    <p>**********</p>
    <p>7. PLOS authors have the option to publish the peer review history of their article (<ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link ext-link-type="uri" xlink:href="https://www.plos.org/privacy-policy">Privacy Policy</ext-link>.</p>
    <p>Reviewer #1: No</p>
    <p>Reviewer #2: No</p>
  </body>
</sub-article>
<sub-article id="pone.0226428.r004" article-type="editor-report">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0226428.r004</article-id>
    <title-group>
      <article-title>Acceptance letter</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Smith</surname>
          <given-names>David V.</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2019 David V. Smith</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>David V. Smith</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article id="rel-obj004" ext-link-type="doi" xlink:href="10.1371/journal.pone.0226428" related-article-type="reviewed-article"/>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">6 Dec 2019</named-content>
    </p>
    <p>PONE-D-19-24023R1 </p>
    <p>GLAMbox: A Python toolbox for investigating the association between gaze allocation and decision behaviour </p>
    <p>Dear Dr. Mohr:</p>
    <p>I am pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now with our production department. </p>
    <p>If your institution or institutions have a press office, please notify them about your upcoming paper at this point, to enable them to help maximize its impact. If they will be preparing press materials for this manuscript, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information please contact <email>onepress@plos.org</email>.</p>
    <p>For any other questions or concerns, please email <email>plosone@plos.org</email>. </p>
    <p>Thank you for submitting your work to PLOS ONE.</p>
    <p>With kind regards,</p>
    <p>PLOS ONE Editorial Office Staff</p>
    <p>on behalf of</p>
    <p>Dr. David V. Smith </p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
  </body>
</sub-article>
