<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8756190</article-id>
    <article-id pub-id-type="pmid">34677586</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btab727</article-id>
    <article-id pub-id-type="publisher-id">btab727</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Applications Notes</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Databases and Ontologies</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>PMLB v1.0: an open-source dataset collection for benchmarking machine learning methods</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-7999-4399</contrib-id>
        <name>
          <surname>Romano</surname>
          <given-names>Joseph D</given-names>
        </name>
        <aff><institution>Institute for Biomedical Informatics, University of Pennsylvania</institution>, Philadelphia, PA 19104, <country country="US">USA</country></aff>
        <aff><institution>Center of Excellence in Environmental Toxicology, University of Pennsylvania</institution>, Philadelphia, PA 19104, <country country="US">USA</country></aff>
        <xref rid="btab727-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Le</surname>
          <given-names>Trang T</given-names>
        </name>
        <aff><institution>Institute for Biomedical Informatics, University of Pennsylvania</institution>, Philadelphia, PA 19104, <country country="US">USA</country></aff>
        <xref rid="btab727-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1332-2960</contrib-id>
        <name>
          <surname>La Cava</surname>
          <given-names>William</given-names>
        </name>
        <aff><institution>Institute for Biomedical Informatics, University of Pennsylvania</institution>, Philadelphia, PA 19104, <country country="US">USA</country></aff>
        <xref rid="btab727-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gregg</surname>
          <given-names>John T</given-names>
        </name>
        <aff><institution>Institute for Biomedical Informatics, University of Pennsylvania</institution>, Philadelphia, PA 19104, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Goldberg</surname>
          <given-names>Daniel J</given-names>
        </name>
        <aff><institution>Department of Computer Science &amp; Engineering, Washington University in St. Louis</institution>, St. Louis, MO 63130, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chakraborty</surname>
          <given-names>Praneel</given-names>
        </name>
        <aff><institution>School of Arts and Sciences, University of Pennsylvania</institution>, Philadelphia, PA 19104, <country country="US">USA</country></aff>
        <aff><institution>Wharton School, University of Pennsylvania</institution>, Philadelphia, PA 19104, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ray</surname>
          <given-names>Natasha L</given-names>
        </name>
        <aff><institution>Princeton Day School</institution>, Princeton, NJ 08540, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-3012-7446</contrib-id>
        <name>
          <surname>Himmelstein</surname>
          <given-names>Daniel</given-names>
        </name>
        <aff><institution>Related Sciences</institution>, Denver, CO 80220, <country country="US">USA</country></aff>
        <aff><institution>Department of Systems Pharmacology &amp; Translational Therapeutics, University of Pennsylvania</institution>, Philadelphia, PA 19104, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fu</surname>
          <given-names>Weixuan</given-names>
        </name>
        <aff><institution>Institute for Biomedical Informatics, University of Pennsylvania</institution>, Philadelphia, PA 19104, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-5015-1099</contrib-id>
        <name>
          <surname>Moore</surname>
          <given-names>Jason H</given-names>
        </name>
        <xref rid="btab727-cor1" ref-type="corresp"/>
        <aff><institution>Institute for Biomedical Informatics, University of Pennsylvania</institution>, Philadelphia, PA 19104, <country country="US">USA</country></aff>
        <!--jhmoore@upenn.edu-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Kelso</surname>
          <given-names>Janet</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <fn id="btab727-FM1">
        <label>†</label>
        <p>The authors wish it to be known that, in their opinion, Joseph D. Romano, Trang T. Le and William La Cava should be regarded as Joint First Authors.</p>
      </fn>
      <corresp id="btab727-cor1">To whom correspondence should be addressed. <email>jhmoore@upenn.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>01</day>
      <month>2</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2021-10-22">
      <day>22</day>
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>22</day>
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <volume>38</volume>
    <issue>3</issue>
    <fpage>878</fpage>
    <lpage>880</lpage>
    <history>
      <date date-type="received">
        <day>02</day>
        <month>4</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>17</day>
        <month>8</month>
        <year>2021</year>
      </date>
      <date date-type="editorial-decision">
        <day>15</day>
        <month>10</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>18</day>
        <month>10</month>
        <year>2021</year>
      </date>
      <date date-type="corrected-typeset">
        <day>10</day>
        <month>1</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btab727.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Novel machine learning and statistical modeling studies rely on standardized comparisons to existing methods using well-studied benchmark datasets. Few tools exist that provide rapid access to many of these datasets through a standardized, user-friendly interface that integrates well with popular data science workflows.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>This release of PMLB (Penn Machine Learning Benchmarks) provides the largest collection of diverse, public benchmark datasets for evaluating new machine learning and data science methods aggregated in one location. v1.0 introduces a number of critical improvements developed following discussions with the open-source community.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>PMLB is available at <ext-link xlink:href="https://github.com/EpistasisLab/pmlb" ext-link-type="uri">https://github.com/EpistasisLab/pmlb</ext-link>. Python and R interfaces for PMLB can be installed through the Python Package Index and Comprehensive R Archive Network, respectively.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>R01-AI116794</award-id>
        <award-id>R01-LM010098</award-id>
        <award-id>R01-LM012601</award-id>
        <award-id>T32-ES019851</award-id>
        <award-id>K99-LM012926</award-id>
        <award-id>K99-LM012646</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="3"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Benchmarking is a standard technique used for evaluating the strengths and weaknesses of machine learning (ML) algorithms with regard to different problem characteristics—namely, how well they perform on a group of well-studied benchmark datasets (<xref rid="btab727-B1" ref-type="bibr">Caruana and Niculescu-Mizil, 2006</xref>; <xref rid="btab727-B11" ref-type="bibr">Stallkamp <italic toggle="yes">et al.</italic>, 2012</xref>). Ideally, these datasets should have known measures of data quality (e.g. missing values, precision), previous results from other ML studies using the same dataset, and in the case of supervised learning, correct and unambiguous target values (i.e. dependent variables) used to calculate performance metrics for candidate models (<xref rid="btab727-B4" ref-type="bibr">Friedman <italic toggle="yes">et al.</italic>, 2001</xref>). In general, benchmarking involves assessing the performance of specific tools or protocols on a set of predefined tasks or datasets, and is used in many areas beyond evaluating ML models, such as software tools (<xref rid="btab727-B5" ref-type="bibr">Mangul <italic toggle="yes">et al.</italic>, 2019</xref>), research methods (<xref rid="btab727-B6" ref-type="bibr">Mitchell <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab727-B12" ref-type="bibr">Weber <italic toggle="yes">et al.</italic>, 2019</xref>) and clinical practice guidelines (<xref rid="btab727-B7" ref-type="bibr">Nicolucci <italic toggle="yes">et al.</italic>, 2014</xref>), among others. Although benchmark datasets for ML are plentiful, they are often difficult to access, challenging to integrate with analyses of other datasets and prone to myriad data quality issues (<xref rid="btab727-B2" ref-type="bibr">Cortes <italic toggle="yes">et al.</italic>, 1995</xref>). PMLB (Penn Machine Learning Benchmarks) is a large, curated repository of open-source benchmark datasets that aims to solve these issues.</p>
    <p>PMLB is typically used as a standalone package for the Python and R programming languages, and is available from standard package repositories. Users can select a classification or regression dataset from the collection, and then (in a single line of code) download the dataset, optionally save a local copy for future use, and load it into a data structure that is ready for use in popular machine learning libraries. Specific documentation with code examples is described Section 3.</p>
    <p>The original prototype release of PMLB (v0.2) (<xref rid="btab727-B8" ref-type="bibr">Olson <italic toggle="yes">et al.</italic>, 2017</xref>) received positive feedback from the ML community, reflecting the pressing need for a collection of standardized datasets to evaluate models without intensive preprocessing and dataset curation. As the repository becomes more widely used, community members have requested new features such as additional information about the datasets, a standardized metadata schema, and new functions to find and select datasets given specific criteria, among others. In this Applications Note, we review PMLB’s core functionality and present new enhancements that facilitate fluid interactions with the repository, both from the perspective of database contributors and end-users (<xref rid="btab727-T1" ref-type="table">Table 1</xref>).</p>
    <table-wrap position="float" id="btab727-T1">
      <label>Table 1.</label>
      <caption>
        <p>Summary of PMLB datasets (with comparison to v0.2)</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col valign="top" align="left" span="1"/>
          <col valign="top" align="left" span="1"/>
          <col valign="top" align="left" span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th rowspan="1" colspan="1"/>
            <th rowspan="1" colspan="1">PMLB v0.2</th>
            <th align="left" rowspan="1" colspan="1">PMLB v1.0</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="1" colspan="1">Num. classification datasets</td>
            <td rowspan="1" colspan="1">150</td>
            <td rowspan="1" colspan="1">162</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Num. regression datasets</td>
            <td rowspan="1" colspan="1">0</td>
            <td rowspan="1" colspan="1">255</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Mean num. instances</td>
            <td rowspan="1" colspan="1">20 865</td>
            <td rowspan="1" colspan="1">42 860</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Median num. instances</td>
            <td rowspan="1" colspan="1">500</td>
            <td rowspan="1" colspan="1">1066</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Language interfaces</td>
            <td rowspan="1" colspan="1">Python</td>
            <td rowspan="1" colspan="1">Python; R</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Miscellaneous tools</td>
            <td rowspan="1" colspan="1">—</td>
            <td rowspan="1" colspan="1">Interactive website</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1">Pandas Profiling reports</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1">Git LFS support</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1">API documentation</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1">Contributing guide</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1">Automatic dataset validation</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>To our knowledge, PMLB represents the largest publicly available collection of curated, ready-to-use ML benchmark datasets for classification and regression in existence. Competing ML dataset collections—such as the UCI Machine Learning Repository (<xref rid="btab727-B3" ref-type="bibr">Dua and Graff, 2017</xref>) or Kaggle Datasets—tend to contain a mixture of classification, regression and other datasets, with varying degrees of documentation/preprocessing and often inadequately characterized measures of data quality. Other, smaller collections of datasets—like Scikit-Learn’s <monospace>datasets</monospace> module (<xref rid="btab727-B9" ref-type="bibr">Pedregosa <italic toggle="yes">et al.</italic>, 2011</xref>)—can be well-documented and curated, but lack the breadth and scope of PMLB. PMLB aims to balance this tradeoff, a task which we approach through a combination of crowdsourcing datasets, automating the assessment of data quality, and utilizing appropriate third-party tools, such as GitHub’s continuous integration features, Pandas Profiling and Git Large File Store, as described in the following text.</p>
  </sec>
  <sec>
    <title>2 Implementation</title>
    <p>PMLB consists of three main components: (i) the collection of benchmark datasets, including metadata and associated documentation, (ii) a Python interface for easily accessing the datasets in the PMLB collection and (iii) an R interface providing similar functionality to the Python interface. PMLB synthesizes and standardizes hundreds of publicly available datasets from diverse sources such as the UCI ML repository and OpenML, enabling systematic assessment of ML methods using a single data interface. Copies of the individual datasets are stored in the GitHub repository using Git Large File Storage, and each dataset is accompanied by a user-provided set of metadata describing the dataset (including keywords that can be used to categorize datasets), as well as an automatically generated Pandas Profiling report that quantitatively describes various characteristics of each dataset.</p>
    <sec>
      <title>2.1 New datasets with rich metadata</title>
      <p>Since PMLB’s original release (v0.2) (<xref rid="btab727-B8" ref-type="bibr">Olson <italic toggle="yes">et al.</italic>, 2017</xref>), we have made substantial improvements in collecting new datasets. PMLB now includes benchmark datasets for regression problems (in addition to classification problems, which have been supported since earlier versions). Each dataset now includes a <monospace>metadata.yaml</monospace> file containing general descriptive information, including the original web address of the dataset, a text description of its purpose, any associated publications, keywords and descriptions of individual features and their coding schema, among others. Metadata files are supported by a standardized format that is formalized using JSON-Schema (version <monospace><monospace>draft-07</monospace></monospace>) (<xref rid="btab727-B10" ref-type="bibr">Pezoa <italic toggle="yes">et al.</italic>, 2016</xref>). Upcoming releases of PMLB improve upon the automated validation of datasets and metadata files to simplify contributions and maintain data accuracy.</p>
    </sec>
    <sec>
      <title>2.2 User-friendly interfaces</title>
      <p>The new version of PMLB includes a contribution guide with step-by-step instructions on how to add new datasets, edit existing datasets or improve the Python or R interfaces. When a user adds a new dataset, summary statistics are automatically computed, a profiling report is generated (see below), a corresponding metadata template is created. Once changes are approved, PMLB’s list of available datasets is automatically updated.</p>
      <p>On PMLB’s homepage, users can now browse, sort, filter and search for datasets using a responsive table that includes summary statistics (<xref rid="btab727-F1" ref-type="fig">Fig.  1</xref>). In addition to the existing Python interface for PMLB, we have included an R library for interacting with PMLB. The R library includes a number of detailed ‘vignette’ documents to help new users learn how to use the software. The website includes API reference guides detailing all user-facing functions and variables in PMLB’s Python and R libraries.</p>
      <fig position="float" id="btab727-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>Database search features on PMLB’s website. (<bold>a</bold>) Interactive scatterplot of databases in PMLB, showing number of features and number of observations in each dataset, as well as whether it is a regression or classification dataset. (<bold>b</bold>) Responsive table of PMLB databases. Users can sort on any columns’ values or filter based on ranges of values. Clicking on any dataset name will bring the user to the Pandas Profiling report for that dataset</p>
        </caption>
        <graphic xlink:href="btab727f1" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.3 Pandas profiling reports</title>
      <p>We generate summary statistic and metadata reports for each dataset using <monospace>pandas-profiling</monospace>. These reports provide detailed quantitative descriptions of each dataset, including correlation structures between features and flagging of duplicate and missing values. Browsing the reports allows users and contributors to rapidly assess dataset quality and make any necessary changes. For example, if a feature is flagged as containing a single value repeated across all samples, it is likely that the feature is uninformative and should be removed from ML analyses. Profiling reports can be accessed either by navigating to the dataset’s directory in the PMLB code repository, or by clicking the dataset name in the interactive dataset browser on the PMLB website.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Availability</title>
    <p>PMLB is publicly available, open-source and released under the MIT license. User-friendly interfaces are available for the Python and R programming languages, and can be installed via the Python Package Index (PyPI) and the Comprehensive R Archive Network (CRAN), respectively. The source code repository for PMLB is maintained at <ext-link xlink:href="https://github.com/EpistasisLab/pmlb" ext-link-type="uri">https://github.com/EpistasisLab/pmlb</ext-link>, and documentation for PMLB is provided at <ext-link xlink:href="https://epistasislab.github.io/pmlb" ext-link-type="uri">https://epistasislab.github.io/pmlb</ext-link>.</p>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>The authors thank the open-source community for their valuable contributions and improvements made to PMLB during its development. They also especially thank GitHub user makeyourownmaker for original contributions to PMLB that were adapted into the interface for the R programming language.</p>
    <sec>
      <title>Funding</title>
      <p>PMLB was developed with support from National Institutes of Health [R01-AI116794, R01-LM010098, R01-LM012601 to (PI: J.M.), T32-ES019851 to (PI: T.P.), K99-LM012926 to (PI: W.L.C.), K99-LM012646 to (PI: J.D.R.)].</p>
      <p><italic toggle="yes">Conflict of Interest</italic>: none declared. </p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btab727-B1">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Caruana</surname><given-names>R.</given-names></string-name>, <string-name><surname>Niculescu-Mizil</surname><given-names>A.</given-names></string-name></person-group> (<year>2006</year>) An empirical comparison of supervised learning algorithms. In: <italic toggle="yes">Proceedings of the 23rd International Conference on Machine Learning, Pittsburgh</italic>, pp. <fpage>161</fpage>–<lpage>168</lpage>.</mixed-citation>
    </ref>
    <ref id="btab727-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cortes</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>1995</year>) <article-title>Limits on learning machine accuracy imposed by data quality</article-title>. In: <italic toggle="yes">KDD</italic>, Vol. <volume>95, Montreal</volume>, pp. <fpage>57</fpage>–<lpage>62</lpage>.</mixed-citation>
    </ref>
    <ref id="btab727-B3">
      <mixed-citation publication-type="other">Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [<ext-link xlink:href="http://archive.ics.uci.edu/ml" ext-link-type="uri">http://archive.ics.uci.edu/ml</ext-link>]. University of California, School of Information and Computer Science, Irvine, CA.</mixed-citation>
    </ref>
    <ref id="btab727-B4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Friedman</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2001</year>) <source>The Elements of Statistical Learning</source>. Vol. <volume>1</volume>. <publisher-name>Springer Series in Statistics</publisher-name>, <publisher-loc>New York</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btab727-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mangul</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Systematic benchmarking of omics computational tools</article-title>. <source>Nat. Commun</source>., <volume>10</volume>, <fpage>1</fpage>–<lpage>11</lpage>.<pub-id pub-id-type="pmid">30602773</pub-id></mixed-citation>
    </ref>
    <ref id="btab727-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mitchell</surname><given-names>K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Benchmarking of computational error-correction methods for next-generation sequencing data</article-title>. <source>Genome Biol</source>., <volume>21</volume>, <fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation>
    </ref>
    <ref id="btab727-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nicolucci</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal>, BENCH-D Study Group. (<year>2014</year>) <article-title>Benchmarking network for clinical and humanistic outcomes in diabetes (bench-d) study: protocol, tools, and population</article-title>. <source>Springerplus</source>, <volume>3</volume>, <fpage>83</fpage>–<lpage>89</lpage>.<pub-id pub-id-type="pmid">24600541</pub-id></mixed-citation>
    </ref>
    <ref id="btab727-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Olson</surname><given-names>R.S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Pmlb: a large benchmark suite for machine learning evaluation and comparison</article-title>. <source>BioData Min</source>., <volume>10</volume>, <fpage>1</fpage>–<lpage>13</lpage>.<pub-id pub-id-type="pmid">28127402</pub-id></mixed-citation>
    </ref>
    <ref id="btab727-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pedregosa</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2011</year>) <article-title>Scikit-learn: machine learning in Python</article-title>. <source>J. Mach. Learn. Res</source>., <volume>12</volume>, <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="btab727-B10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Pezoa</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <article-title>Foundations of json schema</article-title>. In: <italic toggle="yes">Proceedings of the 25th International Conference on World Wide Web, Montreal</italic>, pp. <fpage>263</fpage>–<lpage>273</lpage>.</mixed-citation>
    </ref>
    <ref id="btab727-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stallkamp</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2012</year>) <article-title>Man vs. computer: benchmarking machine learning algorithms for traffic sign recognition</article-title>. <source>Neural Netw</source>., <volume>32</volume>, <fpage>323</fpage>–<lpage>332</lpage>.<pub-id pub-id-type="pmid">22394690</pub-id></mixed-citation>
    </ref>
    <ref id="btab727-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weber</surname><given-names>L.M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Essential guidelines for computational method benchmarking</article-title>. <source>Genome Biol</source>., <volume>20</volume>, <fpage>1</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">30606230</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
