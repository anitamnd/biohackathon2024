<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?subarticle pone.0267976.r001?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS One</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLoS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9060378</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-21-31343</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0267976</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Imaging Techniques</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Computational Techniques</subject>
          <subj-group>
            <subject>Computational Pipelines</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Applied Mathematics</subject>
            <subj-group>
              <subject>Algorithms</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Simulation and Modeling</subject>
          <subj-group>
            <subject>Algorithms</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Diagnostic Medicine</subject>
          <subj-group>
            <subject>Diagnostic Radiology</subject>
            <subj-group>
              <subject>Magnetic Resonance Imaging</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Imaging Techniques</subject>
          <subj-group>
            <subject>Diagnostic Radiology</subject>
            <subj-group>
              <subject>Magnetic Resonance Imaging</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Radiology and Imaging</subject>
          <subj-group>
            <subject>Diagnostic Radiology</subject>
            <subj-group>
              <subject>Magnetic Resonance Imaging</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Oncology</subject>
          <subj-group>
            <subject>Cancers and Neoplasms</subject>
            <subj-group>
              <subject>Blastoma</subject>
              <subj-group>
                <subject>Glioblastoma Multiforme</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Oncology</subject>
          <subj-group>
            <subject>Cancers and Neoplasms</subject>
            <subj-group>
              <subject>Neurological Tumors</subject>
              <subj-group>
                <subject>Glioblastoma Multiforme</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Neurology</subject>
          <subj-group>
            <subject>Neurological Tumors</subject>
            <subj-group>
              <subject>Glioblastoma Multiforme</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Imaging Techniques</subject>
          <subj-group>
            <subject>Image Analysis</subject>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SinGAN-Seg: Synthetic training data generation for medical image segmentation</article-title>
      <alt-title alt-title-type="running-head">SinGAN-Seg for generating synthetic segmentation data</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes" equal-contrib="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6026-0929</contrib-id>
        <name>
          <surname>Thambawita</surname>
          <given-names>Vajira</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Salehi</surname>
          <given-names>Pegah</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Sheshkal</surname>
          <given-names>Sajad Amouei</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hicks</surname>
          <given-names>Steven A.</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hammer</surname>
          <given-names>Hugo L.</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Parasa</surname>
          <given-names>Sravanthi</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff006" ref-type="aff">
          <sup>6</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3989-7487</contrib-id>
        <name>
          <surname>de Lange</surname>
          <given-names>Thomas</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff003" ref-type="aff">
          <sup>3</sup>
        </xref>
        <xref rid="aff004" ref-type="aff">
          <sup>4</sup>
        </xref>
        <xref rid="aff005" ref-type="aff">
          <sup>5</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Halvorsen</surname>
          <given-names>Pål</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3153-2064</contrib-id>
        <name>
          <surname>Riegler</surname>
          <given-names>Michael A.</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>SimulaMet, Oslo, Norway</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Oslo Metropolitan University, Oslo, Norway</addr-line>
    </aff>
    <aff id="aff003">
      <label>3</label>
      <addr-line>Medical Department, Sahlgrenska University Hospital-Möndal, Gothenburg, Sweden</addr-line>
    </aff>
    <aff id="aff004">
      <label>4</label>
      <addr-line>Department of Molecular and Clinical Medicine, Sahlgrenska Academy, University of Gothenburg, Gothenburg, Sweden</addr-line>
    </aff>
    <aff id="aff005">
      <label>5</label>
      <addr-line>Augere Medical, Oslo, Norway</addr-line>
    </aff>
    <aff id="aff006">
      <label>6</label>
      <addr-line>Department of Gastroenterology, Swedish Medical Group, Seattle, WA, United States of America</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Stoean</surname>
          <given-names>Ruxandra</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>University of Craiova, ROMANIA</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>I have read the journal’s policy and the authors of this manuscript have the following competing interests: Sravanthi Parasa: Consultant Covidien LP; Medical advisory board- Fujifilms. Thomas de Lange: Share holder and employed (20%) Augere medical. Pål Halvorson: Board member of Augere Medical.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>vajira@simula.no</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>2</day>
      <month>5</month>
      <year>2022</year>
    </pub-date>
    <volume>17</volume>
    <issue>5</issue>
    <elocation-id>e0267976</elocation-id>
    <history>
      <date date-type="received">
        <day>29</day>
        <month>9</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>19</day>
        <month>4</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 Thambawita et al</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Thambawita et al</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0267976.pdf"/>
    <abstract>
      <p>Analyzing medical data to find abnormalities is a time-consuming and costly task, particularly for rare abnormalities, requiring tremendous efforts from medical experts. Therefore, artificial intelligence has become a popular tool for the automatic processing of medical data, acting as a supportive tool for doctors. However, the machine learning models used to build these tools are highly dependent on the data used to train them. Large amounts of data can be difficult to obtain in medicine due to privacy reasons, expensive and time-consuming annotations, and a general lack of data samples for infrequent lesions. In this study, we present a novel synthetic data generation pipeline, called <italic toggle="yes">SinGAN-Seg</italic>, to produce synthetic medical images with corresponding masks using a single training image. Our method is different from the traditional generative adversarial networks (GANs) because our model needs only a single image and the corresponding ground truth to train. We also show that the synthetic data generation pipeline can be used to produce alternative artificial segmentation datasets with corresponding ground truth masks when real datasets are not allowed to share. The pipeline is evaluated using qualitative and quantitative comparisons between real data and synthetic data to show that the style transfer technique used in our pipeline significantly improves the quality of the generated data and our method is better than other state-of-the-art GANs to prepare synthetic images when the size of training datasets are limited. By training UNet++ using both real data and the synthetic data generated from the SinGAN-Seg pipeline, we show that the models trained on synthetic data have very close performances to those trained on real data when both datasets have a considerable amount of training data. In contrast, we show that synthetic data generated from the SinGAN-Seg pipeline improves the performance of segmentation models when training datasets do not have a considerable amount of data. All experiments were performed using an open dataset and the code is publicly available on GitHub.</p>
    </abstract>
    <funding-group>
      <funding-statement>The author(s) received no specific funding for this work.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="13"/>
      <table-count count="5"/>
      <page-count count="24"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>The synthetic dataset is freely available at <ext-link xlink:href="https://osf.io/xrgz8/as" ext-link-type="uri">https://osf.io/xrgz8/as</ext-link> an example synthetic dataset generated using the SinGAN-Seg pipeline. The pre-trained deep learning models are available at <ext-link xlink:href="https://github.com/vlbthambawita/singan-seg-polyp" ext-link-type="uri">https://github.com/vlbthambawita/singan-seg-polyp</ext-link>.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>The synthetic dataset is freely available at <ext-link xlink:href="https://osf.io/xrgz8/as" ext-link-type="uri">https://osf.io/xrgz8/as</ext-link> an example synthetic dataset generated using the SinGAN-Seg pipeline. The pre-trained deep learning models are available at <ext-link xlink:href="https://github.com/vlbthambawita/singan-seg-polyp" ext-link-type="uri">https://github.com/vlbthambawita/singan-seg-polyp</ext-link>.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>Artificial intelligence (AI) has become a popular tool in medicine and has been vastly discussed in recent decades to improve the performance of clinicians [<xref rid="pone.0267976.ref001" ref-type="bibr">1</xref>–<xref rid="pone.0267976.ref004" ref-type="bibr">4</xref>]. According to the statistics discussed by Jiang et al. [<xref rid="pone.0267976.ref001" ref-type="bibr">1</xref>], artificial neural networks (ANNs) [<xref rid="pone.0267976.ref005" ref-type="bibr">5</xref>], and support vector machines (SVMs) [<xref rid="pone.0267976.ref006" ref-type="bibr">6</xref>] are the most popular machine learning (ML) algorithms used with medical data. Among the various applications of AI in medicine, medical image analysis [<xref rid="pone.0267976.ref007" ref-type="bibr">7</xref>–<xref rid="pone.0267976.ref009" ref-type="bibr">9</xref>] has become a popular research area applying such ML methods. In this respect, ML models learn from data; thus, the amount and quality of medical data has a direct influence on the success of ML-based applications. While the SVM algorithms are popular for regression [<xref rid="pone.0267976.ref010" ref-type="bibr">10</xref>, <xref rid="pone.0267976.ref011" ref-type="bibr">11</xref>] and classification [<xref rid="pone.0267976.ref012" ref-type="bibr">12</xref>] tasks, ANNs or deep neural networks (DNNs) are are used widely for all types of ML tasks; regression, classification, detection, and segmentation.</p>
    <p>Segmentation models make more advanced predictions than regression, classification, and detection as it performs pixel-wise classification of the input images. Therefore, medical image segmentation is a popular application of AI in medicine for image analysis [<xref rid="pone.0267976.ref013" ref-type="bibr">13</xref>–<xref rid="pone.0267976.ref015" ref-type="bibr">15</xref>]. Image segmentation can help find the exact regions that delineate a specific lesion, which could be polyps in gastrointestinal (GI) images, skin cancer in images of the skin, brain tumors in magnetic resonance imaging (MRI), and many more. However, the success of segmentation models is highly dependent on the size and quality of the data used to train them, which is normally annotated by experts like medical doctors.</p>
    <p>In this regard, we identified three main reasons why there are mostly small public datasets in the medical domain compared to other domains. The first reason is privacy concerns attached with medical data containing potentially sensitive patient information. The second is the costly and time-consuming medical data annotation processes that the medical domain experts must perform. Finally, the third is the rarity of some abnormalities.</p>
    <p>Privacy concerns can vary across countries and regions according to the data protection regulations present in the specific areas. For example, Norway should follow the rules given by the Norwegian data protection authority (NDPA) [<xref rid="pone.0267976.ref016" ref-type="bibr">16</xref>] and enforce the personal data act [<xref rid="pone.0267976.ref017" ref-type="bibr">17</xref>], in addition to following the general data protection regulation 31 (GDPR) [<xref rid="pone.0267976.ref018" ref-type="bibr">18</xref>] guidelines being the same for all European countries. While there is no central level privacy protection guideline in the US like GDPR in Europe, US rules and regulations are enforced through other US privacy laws, such as Health Insurance Portability and Accountability Act (HIPAA) [<xref rid="pone.0267976.ref019" ref-type="bibr">19</xref>] and the California Consumer Privacy Act (CCPA) [<xref rid="pone.0267976.ref020" ref-type="bibr">20</xref>]. In Asian counties, they follow their own sets of rules, such as Japan’s Act on Protection of Personal Information [<xref rid="pone.0267976.ref021" ref-type="bibr">21</xref>], the South Korean Personal Information Protection Commission [<xref rid="pone.0267976.ref022" ref-type="bibr">22</xref>] and the Personal Data Protection Bill in India [<xref rid="pone.0267976.ref023" ref-type="bibr">23</xref>]. Additionally, if research is performed with such privacy restrictions, the papers published are often theoretical methods only. According to the analyzed medical image segmentation studies in [<xref rid="pone.0267976.ref024" ref-type="bibr">24</xref>], 30% have used private datasets. As a result, the studies are not reproducible. Furthermore, universities and research institutes that use medical data for teaching purposes use the same medical datasets for years, which affects the quality of education.</p>
    <p>In addition to the privacy concerns, the costly and time-consuming medical data labeling and annotation process [<xref rid="pone.0267976.ref025" ref-type="bibr">25</xref>] is an obstacle to producing large datasets for AI algorithms. Compared to other already time-consuming medical data labeling processes, pixel-wise data annotation is far more demanding in terms of time. If the data annotations by experts are not possible, experts should do at least a review process to make the annotations correct before using them in AI algorithms. The importance of having accurate annotations from experts for medical data is, for example, discussed by [<xref rid="pone.0267976.ref026" ref-type="bibr">26</xref>] using a mandible segmentation dataset of computed tomography (CT) images. In this regard, researching a way to produce synthetic segmentation datasets (synthetic images and the corresponding accurate ground truth masks) to extend the training datasets is important to overcome the timely and costly medical data annotation process.</p>
    <p>Synthetic data is a possible solution to overcome the privacy issues related to medical image data and reduce the cost and time needed for annotations especially in combination with differential privacy [<xref rid="pone.0267976.ref027" ref-type="bibr">27</xref>–<xref rid="pone.0267976.ref029" ref-type="bibr">29</xref>] which is important for medical datasets that can include patient identifying data such as images from faces of stroke patients. Synthetic data generated by generative adversarial networks (GANs) is used in almost all domains that use ML, including the medical domain [<xref rid="pone.0267976.ref030" ref-type="bibr">30</xref>–<xref rid="pone.0267976.ref034" ref-type="bibr">34</xref>], agriculture [<xref rid="pone.0267976.ref035" ref-type="bibr">35</xref>, <xref rid="pone.0267976.ref036" ref-type="bibr">36</xref>], and robotics [<xref rid="pone.0267976.ref037" ref-type="bibr">37</xref>–<xref rid="pone.0267976.ref039" ref-type="bibr">39</xref>]. Among these, some studies [<xref rid="pone.0267976.ref033" ref-type="bibr">33</xref>–<xref rid="pone.0267976.ref036" ref-type="bibr">36</xref>, <xref rid="pone.0267976.ref040" ref-type="bibr">40</xref>–<xref rid="pone.0267976.ref042" ref-type="bibr">42</xref>] generate only synthetic data, while other studies [<xref rid="pone.0267976.ref030" ref-type="bibr">30</xref>, <xref rid="pone.0267976.ref031" ref-type="bibr">31</xref>, <xref rid="pone.0267976.ref033" ref-type="bibr">33</xref>] generate synthetic data and the corresponding ground truth. [<xref rid="pone.0267976.ref030" ref-type="bibr">30</xref>] generated synthetic glioblastoma multiforme (GBM) in 2<italic toggle="yes">D</italic> magnetic resonance images with a segmentation mask. For this method, they manually placed artificially generated GBM in real images using MeVisLab (<ext-link xlink:href="https://www.mevislab.de" ext-link-type="uri">https://www.mevislab.de</ext-link>), calling it semi-supervised data generation. However, placing the generated synthetic GBM is a time-consuming task for generating a big synthetic dataset using this approach. Moreover, the background image is still real while the segmented GBM is synthetic. The real sections of synthetic images may raise privacy concerns.</p>
    <p>Another study [<xref rid="pone.0267976.ref031" ref-type="bibr">31</xref>], used the conditional GAN to generate synthetic polyp data conditioned on real edge filtering images and a randomly generated mask. This approach can generate diverse synthetic data, but a large dataset is required to train this generative model because it uses the deep convolution GAN (DCGAN) [<xref rid="pone.0267976.ref043" ref-type="bibr">43</xref>] which is data-hungry to train. Moreover, extracted edge-filtering images and random polyp masks should be merged to generate the corresponding synthetic data. Therefore, input data preparation is a time-consuming process.</p>
    <p>Instead of generating synthetic polyps using edge filtering, we previously used real non-polyp images and random polyp masks to convert non-polyp images into polyp images using a GAN-based image inpainting model [<xref rid="pone.0267976.ref033" ref-type="bibr">33</xref>]. In the latter two cases, a random mask should be provided to generate the corresponding synthetic polyp image. Moreover, in the image inpainting GAN for polyps, generated synthetic polyps are unrealistic as a result of locating them randomly. Therefore, a thorough post-screening process is required.</p>
    <p>For the above methods, a considerable amount of manually annotated data is needed, for which the time-consuming process of manual data annotation is required. Therefore, we present an alternative synthetic data generation process, which can be used to extend small datasets with an unlimited number of synthetic data samples and corresponding ground truth masks without any manual process.</p>
    <p>As a case study in this paper, we use polyp segmentation, which is a popular segmentation task that uses ML techniques to detect and segment polyps in images/videos collected from GI examinations, such as colonoscopy or capsule endoscopy. Early identification of polyps in GI tract is critical to prevent colorectal cancers [<xref rid="pone.0267976.ref044" ref-type="bibr">44</xref>]. Therefore, many ML models have been investigated to automatically segment polyps in GI tract videos recorded from both colonoscopies [<xref rid="pone.0267976.ref045" ref-type="bibr">45</xref>–<xref rid="pone.0267976.ref047" ref-type="bibr">47</xref>] or capsule endoscopy examinations [<xref rid="pone.0267976.ref048" ref-type="bibr">48</xref>–<xref rid="pone.0267976.ref050" ref-type="bibr">50</xref>], with the aim of decreasing the miss rates and reducing the inter- and intra-observer variations.</p>
    <p>Most polyp segmentation models are based on convolutional neural networks (CNNs) and are trained using publicly available polyp segmentation datasets [<xref rid="pone.0267976.ref051" ref-type="bibr">51</xref>–<xref rid="pone.0267976.ref055" ref-type="bibr">55</xref>]. However, these datasets have a limited number of images with corresponding expert annotated segmentation masks. For examples, the CVC-VideoClinicDB [<xref rid="pone.0267976.ref052" ref-type="bibr">52</xref>] dataset has 11, 954 images from 10 polyp videos and 10 non-polyp videos with segmentation masks, the PICCOLO dataset [<xref rid="pone.0267976.ref055" ref-type="bibr">55</xref>] has 3, 433 manually segmented images (2, 131 white-light images and 1, 302 narrow-band images), and the Hyper-Kvasir [<xref rid="pone.0267976.ref051" ref-type="bibr">51</xref>] dataset has only 1, 000 images with the corresponding segmentation masks, but also contains 100, 000 unlabeled images. In this regard, researching an alternative way, which is applicable with small datasets, to generate synthetic data to tackle the various challenges that we previously discussed, is the main objective of this study. The contributions of this paper are as follows:</p>
    <list list-type="bullet">
      <list-item>
        <p>This study introduces the SinGAN-Seg pipeline to generate synthetic medical images and their corresponding segmentation masks using a single image as training data. This method is different from traditional GAN methods, which often need large training datasets. A modified version of the state-of-the-art SinGAN architecture with a fine-tuning step using a style-transfer method is used. We use polyp segmentation as a case study, but the SinGAN-Seg can be applied for all types of segmentation tasks.</p>
      </list-item>
      <list-item>
        <p>We compare our method with different other generative methods and benchmark them specifically for GAN performance when only little amount of data is available for the training process.</p>
      </list-item>
      <list-item>
        <p>We present the largest synthetic polyp dataset with the corresponding masks and make it publicly available online at <ext-link xlink:href="https://osf.io/xrgz8/" ext-link-type="uri">https://osf.io/xrgz8/</ext-link>. Moreover, we have published our generators as a python package at Python package index (PyPI) (<ext-link xlink:href="https://pypi.org/project/singan-seg-polyp/" ext-link-type="uri">https://pypi.org/project/singan-seg-polyp/</ext-link>) to generate an unlimited number of polyps and corresponding mask images. To the best of our knowledge, this is the first publicly available synthetic polyp dataset and the corresponding generative functions as a PyPI package.</p>
      </list-item>
    </list>
  </sec>
  <sec sec-type="materials|methods" id="sec002">
    <title>Materials and methods</title>
    <p>In the SinGAN-Seg pipeline, there are as depicted in <xref rid="pone.0267976.g001" ref-type="fig">Fig 1</xref> two main steps: (1) training novel SinGAN-Seg generative models per image and (2) style transfer per image. The first step generates synthetic polyp images and the corresponding binary segmentation masks representing the polyp area. Our method, which is based on the vanilla SinGAN architecture [<xref rid="pone.0267976.ref056" ref-type="bibr">56</xref>], can generate multiple synthetic images and masks from a single real image and the corresponding mask. Therefore, this generation process can be identified as an 1: <italic toggle="yes">N</italic> generation process. <xref rid="pone.0267976.g001" ref-type="fig">Fig 1</xref> represents this 1: <italic toggle="yes">N</italic> generation using [<italic toggle="yes">img</italic>]<sub><italic toggle="yes">N</italic></sub>, where <italic toggle="yes">N</italic> represents the number of samples generated using our model and from a real image [<italic toggle="yes">img</italic>]. Then, we apply this step for every image in a target dataset, for which we want to generate synthetic data. The second step focuses on transferring styles such as features of polyps’ texture from real images into the corresponding generated synthetic images. This second step is depicted in the Step 2 in <xref rid="pone.0267976.g001" ref-type="fig">Fig 1</xref>. This second step is also applied per image.</p>
    <fig position="float" id="pone.0267976.g001">
      <object-id pub-id-type="doi">10.1371/journal.pone.0267976.g001</object-id>
      <label>Fig 1</label>
      <caption>
        <title>The complete pipeline of SinGAN-Seg to generate synthetic segmentation datasets.</title>
        <p><italic toggle="yes">Step 1</italic> represents the training of four channels SinGAN-Seg models. <italic toggle="yes">Step 2</italic> represents a fine-tuning step using the neural style transfer [<xref rid="pone.0267976.ref057" ref-type="bibr">57</xref>]. The <italic toggle="yes">four channels SinGAN</italic> is a single training step of our model. Note the stacked input and output compared to the original SinGAN implementation [<xref rid="pone.0267976.ref056" ref-type="bibr">56</xref>] which input only a single image with a noise vector and output only an image. In our SinGAN implementation, all the generators (from <italic toggle="yes">G</italic><sub>0</sub> to <italic toggle="yes">G</italic><sub><italic toggle="yes">N</italic>−1</sub>), except <italic toggle="yes">G</italic><sub><italic toggle="yes">N</italic></sub>, get four channels image (a polyp image and a ground truth) as the input in addition to the input noise vector. The first generator, <italic toggle="yes">G</italic><sub><italic toggle="yes">N</italic></sub> get only the noise vector as the input. The discriminators also get four channels images which consist of an RGB polyp image and a binary mask as input. The inputs to the discriminators can be either real or fake.</p>
      </caption>
      <graphic xlink:href="pone.0267976.g001" position="float"/>
    </fig>
    <p>SinGAN-Seg is a modified version of SinGAN [<xref rid="pone.0267976.ref056" ref-type="bibr">56</xref>], which was designed to generate synthetic data from a GAN trained using only a single image. The original SinGAN is trained using different scales (from 0 to 9) of the same input image, the so-called image pyramid. This image pyramid is a set of images of different resolutions of a single image from low resolution to high resolution. SinGAN consists of a GAN pyramid to train using the corresponding image pyramid. In our study, we build on the implementation and the training process used in SinGAN, except for the number of input and output channels. The original SinGAN implementation [<xref rid="pone.0267976.ref056" ref-type="bibr">56</xref>] uses a three-channel RGB image as input and produces a three-channel RGB image as output. However, our SinGAN-Seg uses four-channel images as the input and the output. The four-channel image consists of the three-channel RGB image and the single-channel ground truth segmentation mask by stacking them together as depicted in the SinGAN-Seg model in <xref rid="pone.0267976.g001" ref-type="fig">Fig 1</xref>. The main purpose of this modification is to generate four-channel synthetic output, which consists of a synthetic image and the corresponding segmentation mask. We have done internal modifications to the vanilla SinGAN to handle four-channel input and output.</p>
    <p>In the second step of the SinGAN-Seg pipeline, we fine-tune the output of the four-channel SinGAN-Seg model using the style-transfer method introduced by [<xref rid="pone.0267976.ref057" ref-type="bibr">57</xref>]. This method is also called Neural-style or Neural-transfer, which can take an image and reproduce a new image with a new artistic style. This algorithm calculates two distances, the content distance (<italic toggle="yes">D</italic><sub><italic toggle="yes">C</italic></sub>) and the style distance (<italic toggle="yes">D</italic><sub><italic toggle="yes">S</italic></sub>) to the third image. In the training process of this algorithm, contents and styles are transferred to the third image using the <italic toggle="yes">content</italic>: <italic toggle="yes">style</italic> ratio. More information about this algorithm can be found in the original paper [<xref rid="pone.0267976.ref057" ref-type="bibr">57</xref>]. Using this style transfer algorithm, we aim to improve the quality of the generated synthetic data by transferring realistic styles from real images to synthetic images. As depicted in Step 2 in <xref rid="pone.0267976.g001" ref-type="fig">Fig 1</xref>, every generated image <italic toggle="yes">G</italic><sub><italic toggle="yes">M</italic></sub> is enhanced by transferring style form the corresponding real image <italic toggle="yes">im</italic><sub><italic toggle="yes">M</italic></sub>. Then, the style transferred output image is presented using <italic toggle="yes">ST</italic><sub><italic toggle="yes">M</italic></sub> where <italic toggle="yes">M</italic> = [0, 1, 2…999] in this study, representing the 1, 000 images in the training dataset. In this process, a suitable <italic toggle="yes">content</italic>: <italic toggle="yes">style</italic> ratio should be found, and it is a hyper-parameter in this second stage. However, this step is a separate training step from the training step of the SinGAN-Seg generative models. Therefore, this step is optional to follow, but we strongly recommend this style-transferring step to enhance the quality of the output data from the first step.</p>
  </sec>
  <sec id="sec003">
    <title>Experiments and results</title>
    <p>This section presents the experiments and results from our experiments using a polyp dataset [<xref rid="pone.0267976.ref051" ref-type="bibr">51</xref>] as a case study. For all the experiments discussed in the following sections, we have used the Pytorch deep learning framework [<xref rid="pone.0267976.ref058" ref-type="bibr">58</xref>].</p>
    <sec id="sec004">
      <title>Data</title>
      <p>We have used a polyp dataset published with HyperKvasir dataset [<xref rid="pone.0267976.ref051" ref-type="bibr">51</xref>], which consists of polyp findings extracted from endoscopy examinations. HyperKvasir contains 1, 000 polyp images with corresponding segmentation masks annotated by medical experts. We use only this polyp dataset as a case study because of the time and resource-consuming training process of the SinGAN-Seg pipeline. However, the SinGAN-Seg model and pipeline can be used for any segmentation dataset.</p>
      <p>A few sample images and the corresponding masks of the polyp dataset in HyperKvasir are shown in <xref rid="pone.0267976.g002" ref-type="fig">Fig 2</xref>. The polyp images are RGB images. The masks of the polyp images are single-channel images with white (255) for true pixels, which represent polyp regions, and black (0) for false pixels, which represent clean colon or background regions. In this dataset, there are different sizes of polyps. The distribution of polyp sizes as a percentage of the full image size is presented in the histogram plot in <xref rid="pone.0267976.g003" ref-type="fig">Fig 3</xref>, and we can observe that there are more relatively small polyps compared to larger polyps. Additionally, a subset of this dataset was used to prove that the performance of segmentation models trained with small datasets can be improved using our SinGAN-Seg pipeline, and the whole dataset was used to show the effect of using SinGAN-Seg generated synthetic images instead of a large dataset which has enough data to train segmentation models. In this regard, this dataset was used for two purposes:</p>
      <list list-type="order">
        <list-item>
          <p>To train SinGAN-Seg models to generate synthetic data.</p>
        </list-item>
        <list-item>
          <p>To compare the performance of segmentation ML models trained using both real and synthetic data.</p>
        </list-item>
      </list>
      <fig position="float" id="pone.0267976.g002">
        <object-id pub-id-type="doi">10.1371/journal.pone.0267976.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>Sample images and corresponding segmentation masks from HyperKvasir [<xref rid="pone.0267976.ref051" ref-type="bibr">51</xref>].</title>
        </caption>
        <graphic xlink:href="pone.0267976.g002" position="float"/>
      </fig>
      <fig position="float" id="pone.0267976.g003">
        <object-id pub-id-type="doi">10.1371/journal.pone.0267976.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>Distribution of true pixel percentages of the full image size of polyp masks in HyperKvasir [<xref rid="pone.0267976.ref051" ref-type="bibr">51</xref>].</title>
        </caption>
        <graphic xlink:href="pone.0267976.g003" position="float"/>
      </fig>
    </sec>
    <sec id="sec005">
      <title>Training generators</title>
      <p>To use SinGAN-Seg to generate synthetic segmentation datasets to represent real segmentation datasets, we first trained SinGAN-Seg models one by one for each image in the training dataset. In our case study, there were 1, 000 polyp images and corresponding ground truth masks. Therefore, 1, 000 SinGAN-Seg models were trained for each image because our models are trained using a single image. This is a time-consuming process, but we can use these pre-trained models repeatedly to generate unlimited number of synthetic data samples. To train these SinGAN-Seg models, we have followed the same SinGAN settings used in the initial paper [<xref rid="pone.0267976.ref056" ref-type="bibr">56</xref>]. Despite using the original training process, the input and output of SinGAN-Seg are four channels. After training each SinGAN-Seg by iterating 2, 000 epochs per scale of pyramidal GAN structure (see the four channels SinGAN-Seg architecture in <xref rid="pone.0267976.g001" ref-type="fig">Fig 1</xref> to understand this pyramidal GAN structure), we stored final checkpoints to generate synthetic data in the later stages from each scale. The resolution of the training image of the SinGAN-Seg model is arbitrary because it depends on the size of the real polyp image. This input image is resized according to the pyramidal rescaling structure introduced in the original implementation of SinGAN [<xref rid="pone.0267976.ref056" ref-type="bibr">56</xref>]. The rescaling pattern is depicted in the four channels SinGAN architecture in <xref rid="pone.0267976.g001" ref-type="fig">Fig 1</xref>. The rescaling pattern used to train SinGAN-Seg models is used to change the randomness of synthetic data when pre-trained models are used to generate synthetic data. The models were trained on multiple computing nodes such as Google Colab with Tesla P100 16GB GPUs and a DGX-2 GPU server with 16 V100 GPUs because training 1, 000 GAN architectures one by one is a tremendous task. The average training time per SinGAN-Seg model was around 65 minutes.</p>
      <p>After training SinGAN-Seg models, we generated 10 random samples per real image using the input scale 0, which is the lowest scale that uses a random noise input instead of a re-scaled input image. For more details about these scaling numbers and corresponding output behaviors, please refer to the vanilla SinGAN paper [<xref rid="pone.0267976.ref056" ref-type="bibr">56</xref>]. Three randomly selected training images and the corresponding first 5 synthetic images generated using scale 0 are depicted in <xref rid="pone.0267976.g004" ref-type="fig">Fig 4</xref>. The first column of the figure represents the real images and the ground truth mask annotated from experts. The rest of the columns represent randomly generated synthetic images and the corresponding generated mask.</p>
      <fig position="float" id="pone.0267976.g004">
        <object-id pub-id-type="doi">10.1371/journal.pone.0267976.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>Real vs. synthetic data.</title>
          <p>Samples of real images and corresponding SinGAN-Seg generated synthetic GI-tract images with corresponding masks. The first column contains real images and masks. All other columns represent randomly generated synthetic data from our SinGAN-Seg models, which were trained using the image in the first column.</p>
        </caption>
        <graphic xlink:href="pone.0267976.g004" position="float"/>
      </fig>
      <p>In total, we have generated 10, 000 synthetic polyp images and the corresponding masks. SinGAN-Seg generates random samples with high variations when the input scale is 0. This variation can be easily recognized using the standard deviation (SD) and the mean mask images presented in <xref rid="pone.0267976.g005" ref-type="fig">Fig 5</xref>. The mean and SD images were calculated by stacking the 10 generated mask images corresponding to the 10 synthetic images related to a real image and calculating pixel-wise std and mean. Bright color in std images and dark color in mean images mean low variance of pixels. In contrast, dark color in std and bright color in mean images reflect high variance in pixel values. By investigating <xref rid="pone.0267976.g005" ref-type="fig">Fig 5</xref>, we see that small polyp masks have high variance compared to the large polyp mask as presented in the figure.</p>
      <fig position="float" id="pone.0267976.g005">
        <object-id pub-id-type="doi">10.1371/journal.pone.0267976.g005</object-id>
        <label>Fig 5</label>
        <caption>
          <title>Analyzing diversity of generated data.</title>
          <p>Real masks are presented on left column. Mean(middle column) and standard deviation (right column) calculated from 10 random masks generated from SinGAN-Seg.</p>
        </caption>
        <graphic xlink:href="pone.0267976.g005" position="float"/>
      </fig>
      <p>To understand the difference between the mask distribution of real images and synthetic images, we plotted pixel distribution of masks of synthetic 10, 000 images in <xref rid="pone.0267976.g006" ref-type="fig">Fig 6</xref>. This plot is comparable to the pixel distribution presented in <xref rid="pone.0267976.g003" ref-type="fig">Fig 3</xref>. The randomness of the generations made differences in the distribution of true pixel percentages compared to the true pixel distribution of real masks of real images. However, the overall shape of synthetic data mask distribution shows a more or less similar distribution pattern to the real true pixel percentage distribution.</p>
      <fig position="float" id="pone.0267976.g006">
        <object-id pub-id-type="doi">10.1371/journal.pone.0267976.g006</object-id>
        <label>Fig 6</label>
        <caption>
          <title>Distribution of the true pixel percentages calculated from 10, 000 synthetic masks generated with synthetic images using SinGAN-Seg.</title>
          <p>The 10, 000 generated images represent the 1, 000 real polyp images. From each real image, 10 synthetic samples were generated. The synthetic 10, 000 dataset can be downloaded from <ext-link xlink:href="https://osf.io/xrgz8/" ext-link-type="uri">https://osf.io/xrgz8/</ext-link>.</p>
        </caption>
        <graphic xlink:href="pone.0267976.g006" position="float"/>
      </fig>
    </sec>
    <sec id="sec006">
      <title>Style transferring</title>
      <p>After finishing the training of 1, 000 SinGAN-Seg models, the style transfer algorithm [<xref rid="pone.0267976.ref057" ref-type="bibr">57</xref>] was applied to every synthetic sample generated from SinGAN-Seg. In the style-transferring algorithm, we can change several parameters such as the number of epochs to transfer style from one image to another and the <italic toggle="yes">content</italic>: <italic toggle="yes">style</italic> weight ratio. In this paper, we used 1, 000 epochs to transfer style from a style image (real polyp image) to a content image (generated synthetic polyp). For performance comparisons, two <italic toggle="yes">content</italic>: <italic toggle="yes">style</italic> ratios, i.e., 1: 1 and 1: 1, 000, were used. An NVIDIA GeForce RTX 3080 GPU took around 20 seconds to transfer the style for a single image.</p>
      <p>In <xref rid="pone.0267976.g007" ref-type="fig">Fig 7</xref>, we provide a visual comparison between pure generated synthetic images and style transferred images (<italic toggle="yes">content</italic>: <italic toggle="yes">style</italic> = 1: 1, 000). Samples with the style transfer ratio 1: 1 are not depicted here because it is difficult to see the differences visually. The first column of <xref rid="pone.0267976.g007" ref-type="fig">Fig 7</xref> shows the real images used as content images to transfer styles. The rest of the images in the first row of each image shows synthetic images generated from SinGAN-Seg before applying the style transferring algorithm. Then, the images in the second row in the figure show the style transferred synthetic images. Differences of the synthetic images before and after applying the style transfer method can be easily recognized from images of the second reference image (using 3<sup>rd</sup> and 4<sup>th</sup> rows in <xref rid="pone.0267976.g007" ref-type="fig">Fig 7</xref>).</p>
      <fig position="float" id="pone.0267976.g007">
        <object-id pub-id-type="doi">10.1371/journal.pone.0267976.g007</object-id>
        <label>Fig 7</label>
        <caption>
          <title>Direct generations of SinGAN-Seg versus style transferred samples.</title>
          <p>The style transferring was performed using 1: 1, 000 content to style ratio. The first row of generated images present quality of images before applying the style transferring and the second row of the same image shows images after applying style transferring. It can be observed that the second row with the style transferring gives better quality.</p>
        </caption>
        <graphic xlink:href="pone.0267976.g007" position="float"/>
      </fig>
      <p>In addition to the visual comparison, we have calculated single image fréechet inception distance (SIFID), which was introduced in [<xref rid="pone.0267976.ref056" ref-type="bibr">56</xref>], between the real polyp dataset and generated synthetic datasets from our model. These SIFID values are shown in <xref rid="pone.0267976.t001" ref-type="table">Table 1</xref> with the mean values and SD calculated with five different synthetic datasets from each category, such as without style transferred and with style transferred of 1: 1 and 1: 1, 000 <italic toggle="yes">content</italic>: <italic toggle="yes">style</italic> ratios. The low mean-SIFID value of 0.2216 after applying the style transfer method as the post-processing technique shows the importance of this style-transfer method.</p>
      <table-wrap position="float" id="pone.0267976.t001">
        <object-id pub-id-type="doi">10.1371/journal.pone.0267976.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>SIFID value comparison for real versus fake images generated from the SinGAN-Seg models.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0267976.t001" id="pone.0267976.t001g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" style="border-bottom-width:thick;border-right-width:thick" rowspan="1" colspan="1">Target dataset (1k images)</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Set 1</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Set 2</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Set 3</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Set 4</th>
                <th align="center" style="border-bottom-width:thick;border-right-width:thick" rowspan="1" colspan="1">Set 5</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Mean</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">SD</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Real</td>
                <td align="center" colspan="6" rowspan="1">-1.31E-14</td>
                <td align="center" rowspan="1" colspan="1">-</td>
              </tr>
              <tr>
                <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">SinGAN-Seg (No ST)</td>
                <td align="char" char="." rowspan="1" colspan="1">0.3515</td>
                <td align="char" char="." rowspan="1" colspan="1">0.3499</td>
                <td align="char" char="." rowspan="1" colspan="1">0.3527</td>
                <td align="char" char="." rowspan="1" colspan="1">0.3466</td>
                <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">0.3480</td>
                <td align="char" char="." rowspan="1" colspan="1">0.3497</td>
                <td align="char" char="." rowspan="1" colspan="1">0.0025</td>
              </tr>
              <tr>
                <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">SinGAN-Seg (ST-1:1)</td>
                <td align="char" char="." rowspan="1" colspan="1">0.2218</td>
                <td align="char" char="." rowspan="1" colspan="1">0.2214</td>
                <td align="char" char="." rowspan="1" colspan="1">0.2217</td>
                <td align="char" char="." rowspan="1" colspan="1">0.2216</td>
                <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">0.2215</td>
                <td align="char" char="." rowspan="1" colspan="1">0.2216</td>
                <td align="char" char="." rowspan="1" colspan="1">0.0001</td>
              </tr>
              <tr>
                <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">SinGAN-Seg (ST-1:1000)</td>
                <td align="char" char="." rowspan="1" colspan="1">0.2206</td>
                <td align="char" char="." rowspan="1" colspan="1">0.2202</td>
                <td align="char" char="." rowspan="1" colspan="1">0.2204</td>
                <td align="char" char="." rowspan="1" colspan="1">0.2204</td>
                <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">0.2203</td>
                <td align="char" char="." rowspan="1" colspan="1">0.2204</td>
                <td align="char" char="." rowspan="1" colspan="1">0.0001</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="t001fn001">
            <p>All the SIFID values are calculated w.r.t the real dataset which has 1,000 polyp images. The baseline value calculated between the real dataset and the same real dataset is presented in the first row. The best mean SIFID value is represented using bold text. ST: style transfer, SD: standard deviation.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Furthermore, the 1: 1, 000 style transfer ratio shows a slight improvement over the 1: 1 ratio. Therefore, we have selected both ratios to generate synthetic data for the segmentation experiments. Preliminary experiments for other ratios such as 1: 10 and 1: 1, 000, 000 show that we cannot neglect these ratios when applying the style transfer mechanisms while we can see little improvement with high ratios.</p>
      <p>In conclusion, we see a higher qualitative output after applying the style transfer algorithm from real images to generated images. Based on the SIFID values in <xref rid="pone.0267976.t001" ref-type="table">Table 1</xref>, we found that 1: 1, 000 <italic toggle="yes">content</italic>: <italic toggle="yes">style</italic> ratio is better than 1: 1. Well-performing style transfer ratios can be obtained by calculating the SIFID values between the real datasets and the synthetic dataset after applying style transfer.</p>
    </sec>
    <sec id="sec007">
      <title>Baseline experiments</title>
      <p>Two different sets of baseline experiments were performed for two different objectives. The first objective was to compare the quality of generated synthetic data over the real data. Using these baseline experiments, we can identify the capability of sharing SinGAN-Seg synthetic data instead of the real datasets to make is easier to share them between health professional. The second objective was to test how to use SinGAN-Seg pipeline to improve the segmentation performance when the size of the training dataset of real images and masks is small. For all the baseline experiments, we selected UNet++ [<xref rid="pone.0267976.ref059" ref-type="bibr">59</xref>] as the main segmentation model according to the performance comparison done by the winning team at EndoCV 2021 [<xref rid="pone.0267976.ref047" ref-type="bibr">47</xref>]. The single-channel Dice loss function used in the same study was used to train the UNet++ polyp segmentation models. Moreover, we used the <monospace>se_resnext50_32x4d</monospace> network as the encoder of the UNet++ model and <monospace>softmax2d</monospace> as the activation function of the last layer, according to the result of the winning team at EndoCV 2021 [<xref rid="pone.0267976.ref047" ref-type="bibr">47</xref>].</p>
      <p>The Pytorch deep learning library was used as the main development framework for the baseline experiments. The training data stream was handled using the PYRA [<xref rid="pone.0267976.ref045" ref-type="bibr">45</xref>] data loader with the Albumentations augmentation library [<xref rid="pone.0267976.ref060" ref-type="bibr">60</xref>]. The real and synthetic images were resized into 128 × 128 using this data handler for all the baseline experiments to save training time. We have used an initial learning rate of 0.0001 for 50 epochs, and then we changed it to 0.00001 for the rest of the training epochs for all the training processes of UNet++. The UNet++ models used to compare real versus synthetic data were trained 300 epochs in total. On the other hand, the UNet++ models used to measure the effect of using SinGAN-Seg synthetic data for small segmentation datasets were trained using only 100 epochs because the size of the data splits used to train the models are getting bigger when increasing the training data. In all the experiments, we have selected the best checkpoint using the best validation intersection over union (IOU) score. Finally, Dice loss, IOU score, F-score, accuracy, recall, and precision were calculated for comparisons using validation folds. More details about these evaluation metrics can be found in [<xref rid="pone.0267976.ref061" ref-type="bibr">61</xref>].</p>
      <sec id="sec008">
        <title>Synthetic data versus real data for segmentation</title>
        <p>We have performed three-fold cross-validation to compare the polyp segmentation performance using UNet++ when using either real or synthetic data for training. First, we divided the real dataset (1, 000 polyp images and the corresponding segmentation masks) into three folds. Then, the trained SinGAN-Seg generative models and the corresponding generated synthetic data were also divided into the same three folds. These three folds are presented using three colors in Step I of <xref rid="pone.0267976.g001" ref-type="fig">Fig 1</xref>. In the other experiments, training data and synthetic data folds were not mixed with the validation data folds. If mixed, it leads to a data leakage problem [<xref rid="pone.0267976.ref062" ref-type="bibr">62</xref>].</p>
        <p>Then, the baseline performance of the UNet++ model was evaluated using the three folds of the real data. In this experiment, the UNet++ model was trained using two folds and validated using the remaining fold of the real data. In total, three UNet++ models were trained and calculated the average performance using Dice loss, IOU score, F-score, accuracy, recall, and precision only for the polyp class because the most important class of this dataset is the polyp class. This three-fold baseline experiment setup is depicted on the left side of <xref rid="pone.0267976.g008" ref-type="fig">Fig 8</xref>.</p>
        <fig position="float" id="pone.0267976.g008">
          <object-id pub-id-type="doi">10.1371/journal.pone.0267976.g008</object-id>
          <label>Fig 8</label>
          <caption>
            <title>The experiment setup to analyze the quality of SinGAN output.</title>
            <p>Experiment 01—the baseline experiments performed only using the real data. Experiment 02—in this experiment, generated synthetic data is used to train segmentation models, and the real data is used to measure the performance metrics.</p>
          </caption>
          <graphic xlink:href="pone.0267976.g008" position="float"/>
        </fig>
        <p>The usability of synthetic images and corresponding masks generated from SinGAN-Seg was investigated using three-fold experiments as organized in the right side of <xref rid="pone.0267976.g008" ref-type="fig">Fig 8</xref>. In this case, UNet++ models were trained only using synthetic data generated from pre-trained generative models and tested using the real data folds, which were not used to train the generative models used to generate the synthetic data. Five different <italic toggle="yes">N</italic>(<italic toggle="yes">N</italic> = [1, 2, 3, 4, 5]) amount of synthetic data were generated per image to train UNet++ models. This data organization process can be identified easily using the color scheme of the figure. To test the quality of pure generations, we first used the direct output from SinGAN-Seg to train the UNet++ models. Then, the style transfer method was applied with 1: 1 <italic toggle="yes">content</italic>: <italic toggle="yes">style</italic> ratio for all the synthetic data. These style transferred images were used as training data and tested using the real dataset. In addition to the 1: 1 ratio, 1: 1, 000 was tested as a style transfer ratio for the same set of experiments.</p>
        <p><xref rid="pone.0267976.t002" ref-type="table">Table 2</xref> shows the results collected from the UNet++ segmentation experiments for the baseline experiment and the experiments conducted with synthetic data, which contains pure generated synthetic data and style transferred data using 1: 1 and 1: 1, 000. Differences in IOU scores of these three experiments are plotted in <xref rid="pone.0267976.g009" ref-type="fig">Fig 9</xref> for easy comparison.</p>
        <fig position="float" id="pone.0267976.g009">
          <object-id pub-id-type="doi">10.1371/journal.pone.0267976.g009</object-id>
          <label>Fig 9</label>
          <caption>
            <title>Real versus synthetic data performance comparison with UNet++ and the effect of applying the style-transferring post processing.</title>
            <p>Note that Y-axis starts from 0.70 for better visualization of differences.</p>
          </caption>
          <graphic xlink:href="pone.0267976.g009" position="float"/>
        </fig>
        <table-wrap position="float" id="pone.0267976.t002">
          <object-id pub-id-type="doi">10.1371/journal.pone.0267976.t002</object-id>
          <label>Table 2</label>
          <caption>
            <title>Three-fold average of basic metrics to compare real versus synthetic performance with UNet++ and the effect of style-transfers performance.</title>
          </caption>
          <alternatives>
            <graphic xlink:href="pone.0267976.t002" id="pone.0267976.t002g" position="float"/>
            <table frame="box" rules="all" border="0">
              <colgroup span="1">
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <th align="left" style="border-right-width:thick;border-bottom-width:thick" rowspan="1" colspan="1">Train data</th>
                  <th align="left" style="border-right-width:thick;border-bottom-width:thick" rowspan="1" colspan="1">ST (cw:sw)</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Dice loss</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">IoU</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">f-score</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Accuracy</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Recall</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Precision</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">
                    <bold>REAL</bold>
                  </td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">NA</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.1123</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8266</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8882</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.9671</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8982</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.9161</bold>
                  </td>
                </tr>
                <tr>
                  <td align="left" rowspan="3" style="border-right-width:thick" colspan="1">FAKE-1</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">No ST</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.1645</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7617</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8357</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9531</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8630</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8793</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">1:1</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.1504</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7782</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8500</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9572</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8672</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8917</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">1:1000</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.1473</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7820</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8530</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9591</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8624</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9005</td>
                </tr>
                <tr>
                  <td align="left" rowspan="3" style="border-right-width:thick" colspan="1">FAKE-2</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">No ST</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.1549</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7729</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8453</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9561</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8692</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8895</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">1:1</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.1550</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7765</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8453</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9575</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8729</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8852</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">1:1000</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.1477</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7854</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8525</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9609</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8647</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9038</td>
                </tr>
                <tr>
                  <td align="left" rowspan="3" style="border-right-width:thick" colspan="1">FAKE-3</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">No ST</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.1610</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7683</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8391</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9556</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8568</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8945</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">1:1</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.1475</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7845</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8525</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9585</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8723</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8936</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">1:1000</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.1408</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7923</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8593</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9629</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8693</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9078</td>
                </tr>
                <tr>
                  <td align="left" rowspan="3" style="border-right-width:thick" colspan="1">FAKE-4</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">No ST</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.1649</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7638</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8352</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9525</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8669</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8780</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">1:1</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.1464</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7848</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8537</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9594</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8713</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8921</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">1:1000</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.1370</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7983</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8630</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.9636</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8653</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9185</td>
                </tr>
                <tr>
                  <td align="left" rowspan="3" style="border-right-width:thick" colspan="1">FAKE-5</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">No ST</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.1654</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7668</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8345</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9563</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8565</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8919</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">1:1</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.1453</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7887</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8547</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9610</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8703</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9000</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">1:1000</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.1458</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7889</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8543</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9620</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8527</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.9211</bold>
                  </td>
                </tr>
              </tbody>
            </table>
          </alternatives>
          <table-wrap-foot>
            <fn id="t002fn001">
              <p><italic toggle="yes">cw</italic>: <italic toggle="yes">sw</italic> is the short form of <italic toggle="yes">content</italic>: <italic toggle="yes">style</italic> weights ratio. Baseline performance values of using real data are represented using bold text. Moreover, the best performance values of using fake data are marked using bold text. FAKE-N represents a number of images generated from a single image using our model, i.e., FAKE-1 to represent one fake image, FAKE-2 to represent two fake images per real image, etc.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec id="sec009">
        <title>Synthetic segmentation data generation using few real data samples</title>
        <p>The main purpose of these experiments is to find the effect of using synthetic data generated from the SinGAN-Seg pipeline instead of small real datasets because the SinGAN-Seg pipeline can generate an unlimited number of synthetic samples per real image. A synthetic sample consists of a synthetic image and the corresponding ground truth mask. Therefore, experts’ knowledge is not required to annotate the ground truth mask. For these experiments, we have selected the best parameters of the SinGAN-Seg pipeline from the experiments performed under Section. First, we created 10 small sub-datasets from the real polyp images from fold one such that each dataset contains <italic toggle="yes">R</italic> number of images, where <italic toggle="yes">R</italic> can be one of the values of [5, 10, 15, …, 50]. The corresponding synthetic dataset was created by generating 10 synthetic images and corresponding masks per real image. Then, our synthetic datasets consist of <italic toggle="yes">S</italic> number of images such that <italic toggle="yes">S</italic> = [50, 100, 150, …, 500]. Then, we have compared true pixel percentages of real masks and synthetic masks generated from the SinGAN-Seg pipeline using histograms of bin size of 5. The histograms are depicted in <xref rid="pone.0267976.g010" ref-type="fig">Fig 10</xref>. The first row represents the histograms of real small detests, and the second row represents the histograms of corresponding synthetic datasets. Compare pairs (one from the top row and the corresponding one from the bottom) to get a clear idea of how the generated synthetic data improved the distribution of masks.</p>
        <fig position="float" id="pone.0267976.g010">
          <object-id pub-id-type="doi">10.1371/journal.pone.0267976.g010</object-id>
          <label>Fig 10</label>
          <caption>
            <title>Distribution comparison between real (top row) and synthetic (bottom row) masks.</title>
            <p>Synthetic masks were generated using the SinGAN-Seg.</p>
          </caption>
          <graphic xlink:href="pone.0267976.g010" position="float"/>
        </fig>
        <p>The UNet++ segmentation models were trained using these real and synthetic datasets separately. The synthetic dataset is generated using style transfer ratio 1: 1, 000 because it shows the best performance in the experiment, which uses only fake data to train segmentation models as presented in <xref rid="pone.0267976.t002" ref-type="table">Table 2</xref> in addition to the best SIFID values presented in <xref rid="pone.0267976.t001" ref-type="table">Table 1</xref>. Then, we have compared the performance differences using validation folds. In these experiments, the training datasets were prepared using fold one. The remaining two folds were used as validation datasets. The collected results from the UNet++ models trained with the real datasets and the synthetic datasets are tabulated in <xref rid="pone.0267976.t003" ref-type="table">Table 3</xref>. A comparison of the corresponding IOU score scores are plotted in <xref rid="pone.0267976.g011" ref-type="fig">Fig 11</xref>.</p>
        <fig position="float" id="pone.0267976.g011">
          <object-id pub-id-type="doi">10.1371/journal.pone.0267976.g011</object-id>
          <label>Fig 11</label>
          <caption>
            <title>Real versus fake performance comparison with small training datasets.</title>
            <p>Fake datasets are generated with the style transfer method using <italic toggle="yes">content</italic>: <italic toggle="yes">style</italic> ratio of 1: 1, 000.</p>
          </caption>
          <graphic xlink:href="pone.0267976.g011" position="float"/>
        </fig>
        <table-wrap position="float" id="pone.0267976.t003">
          <object-id pub-id-type="doi">10.1371/journal.pone.0267976.t003</object-id>
          <label>Table 3</label>
          <caption>
            <title>Real versus fake comparisons for small datasets after applying the style transfer method with a 1: 1000 ratio for fake data.</title>
          </caption>
          <alternatives>
            <graphic xlink:href="pone.0267976.t003" id="pone.0267976.t003g" position="float"/>
            <table frame="box" rules="all" border="0">
              <colgroup span="1">
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <th align="left" style="border-bottom-width:thick;border-right-width:thick" rowspan="1" colspan="1"/>
                  <th align="left" style="border-bottom-width:thick;border-right-width:thick" rowspan="1" colspan="1">#</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Dice loss</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">IoU</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">f-score</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Accuracy</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Recall</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Precision</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Real</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">5</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.4662</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.4618</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.5944</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8751</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7239</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.6305</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Fake</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">50</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.3063</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.5993</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7048</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.9211</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7090</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8133</bold>
                  </td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Real</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">10</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.3932</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.5969</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7079</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9164</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7785</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7516</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Fake</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">100</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.2565</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.6478</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7457</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.9259</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7911</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7970</bold>
                  </td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Real</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">15</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.2992</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.6431</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7402</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9322</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7388</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8602</bold>
                  </td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Fake</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">150</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.2852</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.6559</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7624</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.9329</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8172</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7833</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Real</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">20</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.3070</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.6680</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7668</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9328</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7771</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8566</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Fake</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">200</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.2532</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">0.6569</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7544</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.9342</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7317</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8827</bold>
                  </td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Real</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">25</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.2166</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.6995</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7929</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9405</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7955</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8804</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Fake</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">250</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.2182</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.6961</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7860</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.9418</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7690</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8957</bold>
                  </td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Real</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">30</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.2100</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7037</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7971</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.9417</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8005</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8758</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Fake</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">300</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.2228</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.6843</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7797</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9388</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7683</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8810</bold>
                  </td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Real</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">35</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.2164</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.6955</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7889</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.9398</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8157</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8456</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Fake</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">350</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.2465</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.6677</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7543</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9346</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7385</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8933</bold>
                  </td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Real</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">40</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.2065</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7085</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7974</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.9417</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7881</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8947</bold>
                  </td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Fake</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">400</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.2194</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.6894</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7816</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9305</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8276</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8219</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Real</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">45</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.1982</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7188</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8062</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.9441</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8120</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8839</bold>
                  </td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Fake</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">450</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.2319</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.6794</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7697</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9341</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7859</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8633</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Real</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">50</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.2091</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7115</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7948</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.9418</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7898</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.8932</bold>
                  </td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Fake</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">500</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.2255</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.6896</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.7756</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.9380</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.7961</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">0.8644</td>
                </tr>
              </tbody>
            </table>
          </alternatives>
        </table-wrap>
        <p>In this experiment, we have evaluated how synthetic data can be used instead of small real datasets, such as 5 − 50 images per dataset. Results collected from these experiments show that synthetic segmentation datasets can produce better segmentation performance when the corresponding real datasets are small. For the smallest dataset, the performance gain is around 30% in term of IOU score.</p>
      </sec>
      <sec id="sec010">
        <title>SinGAN-Seg versus state-of-the-art deep generative models</title>
        <p>In this section, we analyze the effect of using other state-of-the-art GANs to generate synthetic data and corresponding masks. To perform this analysis, we selected three GAN architectures, namely DCGAN [<xref rid="pone.0267976.ref063" ref-type="bibr">63</xref>], Progressive GAN [<xref rid="pone.0267976.ref064" ref-type="bibr">64</xref>] and FastGAN [<xref rid="pone.0267976.ref065" ref-type="bibr">65</xref>], which represent simple to advanced deep generative models and can be trained with small datasets in contrast to other state-of-the-art GAN architectures. All the GAN models were modified to generate four-channel output to get the RGB synthetic image and corresponding mask on the fourth channel. All the GAN models were trained using the recommended hyper-parameters and the number of epochs discussed in the original papers.</p>
        <p>We used Fréchet inception distance (FID) [<xref rid="pone.0267976.ref066" ref-type="bibr">66</xref>] to compare the output of the selected GAN architectures to our SinGAN-Seg model because SIFID is not applicable for comparing different GANs that generate random images compared to one-to-one generation process of SinGAN-Seg. However, a recent study [<xref rid="pone.0267976.ref067" ref-type="bibr">67</xref>] shows that FID depends on the images compression type, image size, and many other factors used to calculate FID values. Therefore, we have used the standard FID calculation method introduced in the recent study [<xref rid="pone.0267976.ref067" ref-type="bibr">67</xref>].</p>
        <p>We generated five sample datasets from each GAN architecture to have 1, 000 images per dataset. Then, FID values were calculated between the synthetic datasets and the real datasets. Mean and SD values were calculated using the FID values each sample datasets, where a comparison is shown in <xref rid="pone.0267976.t004" ref-type="table">Table 4</xref>. Furthermore, sample images from this each GAN architecture are presented in <xref rid="pone.0267976.g012" ref-type="fig">Fig 12</xref>. Only SinGAN-Seg has two different output. One is before applying style transfer and one is after applying style transfer. Our style transfer mechanism works when we can find one-to-one mapping from real to synthetic. Therefore, this post-processing technique is applicable only for SinGAN-Seg.</p>
        <fig position="float" id="pone.0267976.g012">
          <object-id pub-id-type="doi">10.1371/journal.pone.0267976.g012</object-id>
          <label>Fig 12</label>
          <caption>
            <title>Sample images generated from different GAN architectures.</title>
            <p>SinGAN-Seg has two versions: one is without style transfer (SinGAN-Seg) and one is with style transfer (SinGAN-Seg-ST). The best ratio of <italic toggle="yes">content</italic>: <italic toggle="yes">style</italic> = 1: 1, 000 was used for transferring style.</p>
          </caption>
          <graphic xlink:href="pone.0267976.g012" position="float"/>
        </fig>
        <table-wrap position="float" id="pone.0267976.t004">
          <object-id pub-id-type="doi">10.1371/journal.pone.0267976.t004</object-id>
          <label>Table 4</label>
          <caption>
            <title>FID value comparison between the real dataset of 1000 real images and the synthetic datasets of 1000 synthetic images generated from different GAN architectures which are modified to generate four channels outputs.</title>
          </caption>
          <alternatives>
            <graphic xlink:href="pone.0267976.t004" id="pone.0267976.t004g" position="float"/>
            <table frame="box" rules="all" border="0">
              <colgroup span="1">
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <th align="left" style="border-bottom-width:thick;border-right-width:thick" rowspan="1" colspan="1">GAN architecture</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Set 1</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Set 2</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Set 3</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Set 4</th>
                  <th align="center" style="border-bottom-width:thick;border-right-width:thick" rowspan="1" colspan="1">Set 5</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Mean</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">SD</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">DCGAN [<xref rid="pone.0267976.ref063" ref-type="bibr">63</xref>]</td>
                  <td align="char" char="." rowspan="1" colspan="1">270.82</td>
                  <td align="char" char="." rowspan="1" colspan="1">269.79</td>
                  <td align="char" char="." rowspan="1" colspan="1">268.38</td>
                  <td align="char" char="." rowspan="1" colspan="1">268.32</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">269.13</td>
                  <td align="char" char="." rowspan="1" colspan="1">269.29</td>
                  <td align="char" char="." rowspan="1" colspan="1">1.05</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Progressive GAN [<xref rid="pone.0267976.ref064" ref-type="bibr">64</xref>]</td>
                  <td align="char" char="." rowspan="1" colspan="1">285.81</td>
                  <td align="char" char="." rowspan="1" colspan="1">284.30</td>
                  <td align="char" char="." rowspan="1" colspan="1">282.81</td>
                  <td align="char" char="." rowspan="1" colspan="1">283.54</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">285.00</td>
                  <td align="char" char="." rowspan="1" colspan="1">284.29</td>
                  <td align="char" char="." rowspan="1" colspan="1">1.18</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">FastGAN [<xref rid="pone.0267976.ref065" ref-type="bibr">65</xref>]</td>
                  <td align="char" char="." rowspan="1" colspan="1">74.60</td>
                  <td align="char" char="." rowspan="1" colspan="1">74.43</td>
                  <td align="char" char="." rowspan="1" colspan="1">75.53</td>
                  <td align="char" char="." rowspan="1" colspan="1">75.08</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">76.20</td>
                  <td align="char" char="." rowspan="1" colspan="1">75.17</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.72</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">SinGAN-Seg (ours)</td>
                  <td align="char" char="." rowspan="1" colspan="1">99.61</td>
                  <td align="char" char="." rowspan="1" colspan="1">98.12</td>
                  <td align="char" char="." rowspan="1" colspan="1">98.27</td>
                  <td align="char" char="." rowspan="1" colspan="1">97.59</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">97.86</td>
                  <td align="char" char="." rowspan="1" colspan="1">98.29</td>
                  <td align="char" char="." rowspan="1" colspan="1">0.78</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">SinGAN-Seg with Style transfer (ours)</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>43.74</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>43.35</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>43.71</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>43.41</bold>
                  </td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">
                    <bold>43.11</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>43.46</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.26</bold>
                  </td>
                </tr>
              </tbody>
            </table>
          </alternatives>
          <table-wrap-foot>
            <fn id="t004fn001">
              <p>The all GAN architectures were trained with the whole polyp dataset which has 1000 polyp images and the corresponding ground-truth masks. Then, 1000 synthetic images were generated using the best checkpoints of each GAN models. Style transfer ratio used in SinGAN-Seg is 1: 1000. The best values are presented using <bold>bold</bold> text.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>According to the FID values and images in <xref rid="pone.0267976.g012" ref-type="fig">Fig 12</xref>, it is clear that FastGAN and SinGAN-Seg versions generate better quality synthetic images compared to DCGAN and Progressive GAN. FastGAN shows better FID scores than SinGAN-Seg without style transfer. However, SinGAN-Seg-ST, which is SinGAN-Seg with style transfer shows the best result among all the GANs is when 1, 000 images were available for training.</p>
        <p>As FastGAN showed the best performance among all the other GANs is, we decided to compare it with SinGAN-Seg and SinGAN-Seg-ST to evaluate generating synthetic data when training on datasets that contain few samples. We trained FastGAN architecture with a small number of images, such as 5, 10, 15, …, 50, and generated the corresponding synthetic datasets, which have synthetic images from 5 to 50. Then, we used the synthetic datasets to compare the performance with FastGAN trained on small datasets. FID values were calculated between the synthetic images and corresponding real images used to train the models. Mean and standard deviation (SD) of FID values calculated from five synthetic datasets (Set 1, Set 2, Set 3, Set 4 and Set 5) generated from each GAN model are tabulated in <xref rid="pone.0267976.t005" ref-type="table">Table 5</xref>. Moreover, mean FID values of FastGAN, SinGAN-Seg and SinGAN-Seg-ST are compared in <xref rid="pone.0267976.g013" ref-type="fig">Fig 13</xref>.</p>
        <fig position="float" id="pone.0267976.g013">
          <object-id pub-id-type="doi">10.1371/journal.pone.0267976.g013</object-id>
          <label>Fig 13</label>
          <caption>
            <title>FastGAN versus SinGAN-Seg and SinGAN-Seg-ST.</title>
            <p>SinGAN-Seg-ST represents SinGAN-Seg with style transfer of 1: 1000.</p>
          </caption>
          <graphic xlink:href="pone.0267976.g013" position="float"/>
        </fig>
        <table-wrap position="float" id="pone.0267976.t005">
          <object-id pub-id-type="doi">10.1371/journal.pone.0267976.t005</object-id>
          <label>Table 5</label>
          <caption>
            <title>FID value calculations between real and synthetic datasets generated from FastGAN, SinGAN-Seg and SinGAN-Seg-ST trained with small datasets.</title>
          </caption>
          <alternatives>
            <graphic xlink:href="pone.0267976.t005" id="pone.0267976.t005g" position="float"/>
            <table frame="box" rules="all" border="0">
              <colgroup span="1">
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <th align="left" style="border-right-width:thick;border-bottom-width:thick" rowspan="1" colspan="1">GAN type</th>
                  <th align="left" style="border-right-width:thick;border-bottom-width:thick" rowspan="1" colspan="1"># of synthetics</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Set 1</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Set 2</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Set 3</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Set 4</th>
                  <th align="center" style="border-right-width:thick;border-bottom-width:thick" rowspan="1" colspan="1">Set 5</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Mean</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">SD</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" rowspan="10" style="border-right-width:thick" colspan="1">FastGAN</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">5</td>
                  <td align="char" char="." rowspan="1" colspan="1">290.92</td>
                  <td align="char" char="." rowspan="1" colspan="1">224.22</td>
                  <td align="char" char="." rowspan="1" colspan="1">224.10</td>
                  <td align="char" char="." rowspan="1" colspan="1">240.78</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">252.58</td>
                  <td align="char" char="." rowspan="1" colspan="1">246.52</td>
                  <td align="char" char="." rowspan="1" colspan="1">27.57</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">10</td>
                  <td align="char" char="." rowspan="1" colspan="1">264.98</td>
                  <td align="char" char="." rowspan="1" colspan="1">252.69</td>
                  <td align="char" char="." rowspan="1" colspan="1">243.38</td>
                  <td align="char" char="." rowspan="1" colspan="1">230.54</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">223.88</td>
                  <td align="char" char="." rowspan="1" colspan="1">243.09</td>
                  <td align="char" char="." rowspan="1" colspan="1">16.57</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">15</td>
                  <td align="char" char="." rowspan="1" colspan="1">235.64</td>
                  <td align="char" char="." rowspan="1" colspan="1">231.45</td>
                  <td align="char" char="." rowspan="1" colspan="1">213.92</td>
                  <td align="char" char="." rowspan="1" colspan="1">220.76</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">244.86</td>
                  <td align="char" char="." rowspan="1" colspan="1">229.33</td>
                  <td align="char" char="." rowspan="1" colspan="1">12.21</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">20</td>
                  <td align="char" char="." rowspan="1" colspan="1">249.57</td>
                  <td align="char" char="." rowspan="1" colspan="1">268.03</td>
                  <td align="char" char="." rowspan="1" colspan="1">259.91</td>
                  <td align="char" char="." rowspan="1" colspan="1">261.15</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">264.05</td>
                  <td align="char" char="." rowspan="1" colspan="1">260.54</td>
                  <td align="char" char="." rowspan="1" colspan="1">6.88</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">25</td>
                  <td align="char" char="." rowspan="1" colspan="1">356.25</td>
                  <td align="char" char="." rowspan="1" colspan="1">354.63</td>
                  <td align="char" char="." rowspan="1" colspan="1">354.06</td>
                  <td align="char" char="." rowspan="1" colspan="1">355.91</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">354.32</td>
                  <td align="char" char="." rowspan="1" colspan="1">355.04</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.98</bold>
                  </td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">30</td>
                  <td align="char" char="." rowspan="1" colspan="1">312.53</td>
                  <td align="char" char="." rowspan="1" colspan="1">299.09</td>
                  <td align="char" char="." rowspan="1" colspan="1">298.20</td>
                  <td align="char" char="." rowspan="1" colspan="1">302.74</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">298.36</td>
                  <td align="char" char="." rowspan="1" colspan="1">302.18</td>
                  <td align="char" char="." rowspan="1" colspan="1">6.07</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">35</td>
                  <td align="char" char="." rowspan="1" colspan="1">266.21</td>
                  <td align="char" char="." rowspan="1" colspan="1">273.29</td>
                  <td align="char" char="." rowspan="1" colspan="1">265.75</td>
                  <td align="char" char="." rowspan="1" colspan="1">266.97</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">262.26</td>
                  <td align="char" char="." rowspan="1" colspan="1">266.90</td>
                  <td align="char" char="." rowspan="1" colspan="1">4.01</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">40</td>
                  <td align="char" char="." rowspan="1" colspan="1">257.10</td>
                  <td align="char" char="." rowspan="1" colspan="1">256.23</td>
                  <td align="char" char="." rowspan="1" colspan="1">256.23</td>
                  <td align="char" char="." rowspan="1" colspan="1">256.26</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">260.88</td>
                  <td align="char" char="." rowspan="1" colspan="1">257.34</td>
                  <td align="char" char="." rowspan="1" colspan="1">2.01</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">45</td>
                  <td align="char" char="." rowspan="1" colspan="1">228.93</td>
                  <td align="char" char="." rowspan="1" colspan="1">226.10</td>
                  <td align="char" char="." rowspan="1" colspan="1">226.42</td>
                  <td align="char" char="." rowspan="1" colspan="1">222.41</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">219.24</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>224.62</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">3.80</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">50</td>
                  <td align="char" char="." rowspan="1" colspan="1">236.34</td>
                  <td align="char" char="." rowspan="1" colspan="1">224.25</td>
                  <td align="char" char="." rowspan="1" colspan="1">235.53</td>
                  <td align="char" char="." rowspan="1" colspan="1">224.69</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">225.59</td>
                  <td align="char" char="." rowspan="1" colspan="1">229.28</td>
                  <td align="char" char="." rowspan="1" colspan="1">6.10</td>
                </tr>
                <tr>
                  <td align="left" rowspan="10" style="border-right-width:thick" colspan="1">SinGAN-Seg (ours)</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">5</td>
                  <td align="char" char="." rowspan="1" colspan="1">245.80</td>
                  <td align="char" char="." rowspan="1" colspan="1">264.39</td>
                  <td align="char" char="." rowspan="1" colspan="1">271.63</td>
                  <td align="char" char="." rowspan="1" colspan="1">280.08</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">288.36</td>
                  <td align="char" char="." rowspan="1" colspan="1">270.05</td>
                  <td align="char" char="." rowspan="1" colspan="1">16.27</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">10</td>
                  <td align="char" char="." rowspan="1" colspan="1">224.76</td>
                  <td align="char" char="." rowspan="1" colspan="1">223.50</td>
                  <td align="char" char="." rowspan="1" colspan="1">234.41</td>
                  <td align="char" char="." rowspan="1" colspan="1">240.93</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">230.56</td>
                  <td align="char" char="." rowspan="1" colspan="1">230.83</td>
                  <td align="char" char="." rowspan="1" colspan="1">7.17</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">15</td>
                  <td align="char" char="." rowspan="1" colspan="1">208.48</td>
                  <td align="char" char="." rowspan="1" colspan="1">208.25</td>
                  <td align="char" char="." rowspan="1" colspan="1">217.65</td>
                  <td align="char" char="." rowspan="1" colspan="1">222.08</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">212.05</td>
                  <td align="char" char="." rowspan="1" colspan="1">213.70</td>
                  <td align="char" char="." rowspan="1" colspan="1">6.03</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">20</td>
                  <td align="char" char="." rowspan="1" colspan="1">210.21</td>
                  <td align="char" char="." rowspan="1" colspan="1">220.01</td>
                  <td align="char" char="." rowspan="1" colspan="1">224.06</td>
                  <td align="char" char="." rowspan="1" colspan="1">222.79</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">217.42</td>
                  <td align="char" char="." rowspan="1" colspan="1">218.90</td>
                  <td align="char" char="." rowspan="1" colspan="1">5.49</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">25</td>
                  <td align="char" char="." rowspan="1" colspan="1">207.81</td>
                  <td align="char" char="." rowspan="1" colspan="1">216.58</td>
                  <td align="char" char="." rowspan="1" colspan="1">221.12</td>
                  <td align="char" char="." rowspan="1" colspan="1">217.13</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">216.05</td>
                  <td align="char" char="." rowspan="1" colspan="1">215.74</td>
                  <td align="char" char="." rowspan="1" colspan="1">4.86</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">30</td>
                  <td align="char" char="." rowspan="1" colspan="1">200.99</td>
                  <td align="char" char="." rowspan="1" colspan="1">208.70</td>
                  <td align="char" char="." rowspan="1" colspan="1">211.04</td>
                  <td align="char" char="." rowspan="1" colspan="1">210.92</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">206.85</td>
                  <td align="char" char="." rowspan="1" colspan="1">207.70</td>
                  <td align="char" char="." rowspan="1" colspan="1">4.13</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">35</td>
                  <td align="char" char="." rowspan="1" colspan="1">201.40</td>
                  <td align="char" char="." rowspan="1" colspan="1">206.30</td>
                  <td align="char" char="." rowspan="1" colspan="1">209.27</td>
                  <td align="char" char="." rowspan="1" colspan="1">206.52</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">207.09</td>
                  <td align="char" char="." rowspan="1" colspan="1">206.12</td>
                  <td align="char" char="." rowspan="1" colspan="1">2.89</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">40</td>
                  <td align="char" char="." rowspan="1" colspan="1">198.33</td>
                  <td align="char" char="." rowspan="1" colspan="1">202.14</td>
                  <td align="char" char="." rowspan="1" colspan="1">204.32</td>
                  <td align="char" char="." rowspan="1" colspan="1">202.65</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">202.51</td>
                  <td align="char" char="." rowspan="1" colspan="1">201.99</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>2.21</bold>
                  </td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">45</td>
                  <td align="char" char="." rowspan="1" colspan="1">188.88</td>
                  <td align="char" char="." rowspan="1" colspan="1">190.03</td>
                  <td align="char" char="." rowspan="1" colspan="1">194.49</td>
                  <td align="char" char="." rowspan="1" colspan="1">192.98</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">195.08</td>
                  <td align="char" char="." rowspan="1" colspan="1">192.29</td>
                  <td align="char" char="." rowspan="1" colspan="1">2.73</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">50</td>
                  <td align="char" char="." rowspan="1" colspan="1">186.83</td>
                  <td align="char" char="." rowspan="1" colspan="1">187.84</td>
                  <td align="char" char="." rowspan="1" colspan="1">192.75</td>
                  <td align="char" char="." rowspan="1" colspan="1">191.32</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">192.60</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>190.27</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">2.76</td>
                </tr>
                <tr>
                  <td align="left" rowspan="10" style="border-right-width:thick" colspan="1">SinGAN-Seg-ST (ours)</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">5</td>
                  <td align="char" char="." rowspan="1" colspan="1">139.66</td>
                  <td align="char" char="." rowspan="1" colspan="1">152.66</td>
                  <td align="char" char="." rowspan="1" colspan="1">153.20</td>
                  <td align="char" char="." rowspan="1" colspan="1">170.99</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">168.65</td>
                  <td align="char" char="." rowspan="1" colspan="1">157.03</td>
                  <td align="char" char="." rowspan="1" colspan="1">12.90</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">10</td>
                  <td align="char" char="." rowspan="1" colspan="1">131.15</td>
                  <td align="char" char="." rowspan="1" colspan="1">131.93</td>
                  <td align="char" char="." rowspan="1" colspan="1">127.93</td>
                  <td align="char" char="." rowspan="1" colspan="1">140.65</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">140.34</td>
                  <td align="char" char="." rowspan="1" colspan="1">134.40</td>
                  <td align="char" char="." rowspan="1" colspan="1">5.76</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">15</td>
                  <td align="char" char="." rowspan="1" colspan="1">121.27</td>
                  <td align="char" char="." rowspan="1" colspan="1">124.40</td>
                  <td align="char" char="." rowspan="1" colspan="1">120.90</td>
                  <td align="char" char="." rowspan="1" colspan="1">129.80</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">129.41</td>
                  <td align="char" char="." rowspan="1" colspan="1">125.16</td>
                  <td align="char" char="." rowspan="1" colspan="1">4.29</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">20</td>
                  <td align="char" char="." rowspan="1" colspan="1">119.75</td>
                  <td align="char" char="." rowspan="1" colspan="1">133.64</td>
                  <td align="char" char="." rowspan="1" colspan="1">123.64</td>
                  <td align="char" char="." rowspan="1" colspan="1">129.75</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">126.55</td>
                  <td align="char" char="." rowspan="1" colspan="1">126.67</td>
                  <td align="char" char="." rowspan="1" colspan="1">5.37</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">25</td>
                  <td align="char" char="." rowspan="1" colspan="1">118.98</td>
                  <td align="char" char="." rowspan="1" colspan="1">134.12</td>
                  <td align="char" char="." rowspan="1" colspan="1">123.60</td>
                  <td align="char" char="." rowspan="1" colspan="1">125.04</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">126.60</td>
                  <td align="char" char="." rowspan="1" colspan="1">125.67</td>
                  <td align="char" char="." rowspan="1" colspan="1">5.51</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">30</td>
                  <td align="char" char="." rowspan="1" colspan="1">118.95</td>
                  <td align="char" char="." rowspan="1" colspan="1">125.96</td>
                  <td align="char" char="." rowspan="1" colspan="1">117.96</td>
                  <td align="char" char="." rowspan="1" colspan="1">120.81</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">121.73</td>
                  <td align="char" char="." rowspan="1" colspan="1">121.08</td>
                  <td align="char" char="." rowspan="1" colspan="1">3.11</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">35</td>
                  <td align="char" char="." rowspan="1" colspan="1">121.06</td>
                  <td align="char" char="." rowspan="1" colspan="1">124.96</td>
                  <td align="char" char="." rowspan="1" colspan="1">121.85</td>
                  <td align="char" char="." rowspan="1" colspan="1">120.90</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">121.00</td>
                  <td align="char" char="." rowspan="1" colspan="1">121.95</td>
                  <td align="char" char="." rowspan="1" colspan="1">1.72</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">40</td>
                  <td align="char" char="." rowspan="1" colspan="1">117.79</td>
                  <td align="char" char="." rowspan="1" colspan="1">119.27</td>
                  <td align="char" char="." rowspan="1" colspan="1">119.24</td>
                  <td align="char" char="." rowspan="1" colspan="1">115.24</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">118.06</td>
                  <td align="char" char="." rowspan="1" colspan="1">117.92</td>
                  <td align="char" char="." rowspan="1" colspan="1">1.64</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">45</td>
                  <td align="char" char="." rowspan="1" colspan="1">114.19</td>
                  <td align="char" char="." rowspan="1" colspan="1">112.88</td>
                  <td align="char" char="." rowspan="1" colspan="1">113.11</td>
                  <td align="char" char="." rowspan="1" colspan="1">111.95</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">114.76</td>
                  <td align="char" char="." rowspan="1" colspan="1">113.38</td>
                  <td align="char" char="." rowspan="1" colspan="1">1.11</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">50</td>
                  <td align="char" char="." rowspan="1" colspan="1">111.16</td>
                  <td align="char" char="." rowspan="1" colspan="1">110.66</td>
                  <td align="char" char="." rowspan="1" colspan="1">111.48</td>
                  <td align="char" char="." rowspan="1" colspan="1">112.39</td>
                  <td align="char" char="." style="border-right-width:thick" rowspan="1" colspan="1">111.97</td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>111.53</bold>
                  </td>
                  <td align="char" char="." rowspan="1" colspan="1">
                    <bold>0.68</bold>
                  </td>
                </tr>
              </tbody>
            </table>
          </alternatives>
        </table-wrap>
        <p>According the the table and the figure, it is clear that FastGAN is unstable with small training datasets while SinGAN-Seg and SinGAN-Seg-ST show good progress toward the number of images used to calculate FID values. SinGAN-Seg-ST is better than SinGAN-Seg as we experienced with previous experiments discussed in this study. It it worth to remind that SinGAN-Seg and SinGAN-Seg-ST does not depend on number of training images because it needs only a single image to train. Therefore, SinGAN-Seg model and it´s training pipeline used in this study prove the importance using it to generate synthetic datasets when GAN models do not have access to large training datasets.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec011">
    <title>Discussion</title>
    <p>The SinGAN-Seg pipeline has two steps. The first one is generating synthetic polyp images and the corresponding ground truth masks. The second is transferring style from real polyp images to synthetic polyp images to make them more realistic compared to the pure generation of images using only SinGAN-Seg from the first step. We have developed this pipeline to achieve two goals. The first one is to make it easier to share medical data and generate more annotated data. The second one is to improve the polyp segmentation performance when the size of the training dataset is small by augmenting the training dataset with synthetic images.</p>
    <sec id="sec012">
      <title>SinGAN-Seg as data sharing technique</title>
      <p>The SinGAN-Seg can generate unlimited synthetic data with the corresponding ground truth mask, representing real datasets. This SinGAN-Seg pipeline is applicable for any dataset with segmentation masks, particularly when the dataset is not allowed to share. However, in this study, we applied this pipeline to a public polyp dataset with segmentation masks as a case study. Assuming that the polyp dataset is private, we used this polyp dataset as a proof of concept medical dataset. In this case, we published a PyPI package, <monospace>singan-seg-polyp</monospace> which can generate an unlimited number of polyp images and corresponding ground truth masks. If the real polyp dataset is restricted to share, then this type of pre-trained models can be used to generate an alternative dataset to represent the real dataset and share. Alternatively, we can publish a pre-generated synthetic dataset using the SinGAN-Seg pipeline, such as the synthetic polyp dataset published as a case study at <ext-link xlink:href="https://osf.io/xrgz8" ext-link-type="uri">https://osf.io/xrgz8</ext-link>.</p>
      <p>According to the results presented in <xref rid="pone.0267976.t002" ref-type="table">Table 2</xref>, the UNet++ segmentation network performs slightly better when the real data is used as training data compared to using synthetic data as training data. However, the small performance gap between real and synthetic data as training data implies that the synthetic data generated from the SinGAN-Seg can use as an alternative to sharing segmentation data instead of real datasets, which are restricted to share. The style-transferring step of the SinGAN-Seg pipeline could reduce the performance gap between real and synthetic data as training data for UNet++. The performance gap between real data and the synthetic data as training data for segmentation models is negotiable because the primary purpose of producing the synthetic data is not to improve the performance of segmentation models, but to introduce an alternative data sharing which are practically applicable when datasets should be shared between different health institutions and research labs.</p>
    </sec>
    <sec id="sec013">
      <title>SinGAN-Seg with small datasets</title>
      <p>In addition to using the SinGAN-Seg pipeline as a data-sharing technique when the real datasets are restricted to publish, the pipeline can improve the performance of segmentation tasks when a dataset is small. In this case, the SinGAN-Seg pipeline can generate synthetic data to overcome the problem associated with the small dataset. In other words, the SinGAN-Seg pipeline acts as a stochastic data augmentation technique due to the randomness of the synthetic data generated from this model. For example, consider a manual segmentation process such as cell segmentation in any medical laboratory experiment. This type of tasks is really hard to perform for experts as well. As a result, the amount of data collected with manually annotated segmentation masks is limited, and applying only traditional transformations such horizontal flip, shift scale rotation, resizing, motion blur and other augmentation techniques introduced for segmentation task (see Albumentations [<xref rid="pone.0267976.ref060" ref-type="bibr">60</xref>] library for more details) are not enough for achieving good performance. To show that, we have combined SinGAN-Seg with Albumentation data augmentation techniques. Then, SinGAN-Seg shows a performance improvement over the performance achieved only using the traditional augmentation techniques when the initial real dataset size is small (such as 5 to 20).</p>
      <p>Our SinGAN-Seg pipeline can increase the size of small datasets, and thus the quality, by generating an unlimited number of random samples from a single manually annotated image. Furthermore, in contrast to our method, conventional GAN models cannot be used to generate high-quality synthetic data when the training datasets have a limited number of images. This is one of the largest advantages of SinGAN-Seg, specially in the medical domain which usually has a lack of good training data. This is proven by training three state-of-the-art conventional GAN models, namely DCGAN, ProgressiveGAN and FastGAN and comparing FID values with SinGAN-Seg models when training datasets consist of a small number of images.</p>
      <p>This study showed that the synthetic data generated from a small real dataset can improve the performance of ML models for image segmentation. For example, when the real polyp dataset size is 5 to train our UNnet++ model, the synthetic dataset with 50 samples showed 30% improvement over the IOU score. Similarly, when the real dataset is 10 and the corresponding generated dataset is 100 (we always take 10 times as the real dataset in our case studies, but there is no any limit.), the synthetic dataset shows an 8.5% improvement over the real dataset. These experiments emphasize that SinGAN-Seg generated synthetic data can be used instead of small real datasets.</p>
    </sec>
  </sec>
  <sec id="sec014">
    <title>Conclusions and future work</title>
    <p>This paper presented our four-channel SinGAN-Seg model and the corresponding SinGAN-Seg pipeline with a style transfer method to generate realistic synthetic polyp images and the corresponding ground truth segmentation masks. Our pipeline can be used as an alternative method to provide and share data when real datasets are restricted. Moreover, this pipeline can be used to improve segmentation performance when we have small segmentation datasets, i.e., as an effective data augmentation technique. The results from the conducted three-fold cross-validation segmentation experiments show that models trained on synthetic data can achieve performance very close to the performance of segmentation models using only real data to train the ML models. On the other hand, we also show that SinGAN-Seg can achieve better segmentation performance when the training datasets are very small because of the advantage of being able to learn from one single image. Furthermore, we performed a qualitative and quantitative comparison with other state-of-the-art GANs and show that SinGAN-Seg with style-transfer technique (SinGAN-Seg-ST) performs better than other GAN architectures.</p>
    <p>In future studies, researchers can combine super-resolution GAN models [<xref rid="pone.0267976.ref068" ref-type="bibr">68</xref>] with this pipeline to improve the quality of the output after the style transfer step. When we have high-resolution images, ML algorithms show better performance than algorithms trained using low-resolution images [<xref rid="pone.0267976.ref069" ref-type="bibr">69</xref>].</p>
    <sec id="sec015">
      <title>Code and dataset availability</title>
      <p>Using all the pre-trained SinGAN-Seg checkpoints, we have published a PyPI package and the corresponding GitHub repository to make all the experiments reproducible. Additionally, we have published the first synthetic polyp dataset to demonstrate how to share synthetic data instead of a real dataset. The synthetic dataset is freely available at <ext-link xlink:href="https://osf.io/xrgz8/" ext-link-type="uri">https://osf.io/xrgz8/</ext-link> as an example synthetic dataset generated using the SinGAN-Seg pipeline. Furthermore, this dataset is an example showing how to increase a segmentation dataset size without using the time-consuming and costly medical data annotation process that needs medical experts’ knowledge.</p>
      <p>We named this PyPI package as <monospace>singan-seg-polyp (pip install singan-seg-polyp)</monospace>, and it can be found here: <ext-link xlink:href="https://pypi.org/project/singan-seg-polyp/" ext-link-type="uri">https://pypi.org/project/singan-seg-polyp/</ext-link>. To the best of our knowledge, this is the only PyPI package to generate an unlimited number of synthetic polyps and corresponding masks. The corresponding GitHub repository is available at <ext-link xlink:href="https://github.com/vlbthambawita/singan-seg-polyp" ext-link-type="uri">https://github.com/vlbthambawita/singan-seg-polyp</ext-link>. A set of functionalities were introduced in this package for end-users. Generative functions can generate random synthetic polyp data with their corresponding mask for a given image id from 1 to 1, 000 or for the given checkpoint directory, which is downloaded automatically when the generative functions are called. The package contains the style transfer function that can be used to transfer the style from real polyp images to the corresponding synthetic polyp images. In both functionalities, the relevant hyper-parameters can be changed as needed to end-users of this PyPI package.</p>
    </sec>
  </sec>
</body>
<back>
  <ack>
    <p>For this research the Experimental Infrastructure for Exploration of Exascale Computing (eX3), Research Council of Norway Project 270053 was used.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="pone.0267976.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Jiang</surname><given-names>F</given-names></name>, <name><surname>Jiang</surname><given-names>Y</given-names></name>, <name><surname>Zhi</surname><given-names>H</given-names></name>, <name><surname>Dong</surname><given-names>Y</given-names></name>, <name><surname>Li</surname><given-names>H</given-names></name>, <name><surname>Ma</surname><given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Artificial intelligence in healthcare: past, present and future</article-title>. <source>Stroke and vascular neurology</source>. <year>2017</year>;<volume>2</volume>(<issue>4</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1136/svn-2017-000101</pub-id><?supplied-pmid 29507784?><pub-id pub-id-type="pmid">29507784</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Dilsizian</surname><given-names>SE</given-names></name>, <name><surname>Siegel</surname><given-names>EL</given-names></name>. <article-title>Artificial Intelligence in Medicine and Cardiac Imaging: Harnessing Big Data and Advanced Computing to Provide Personalized Medical Diagnosis and Treatment</article-title>. <source>Current Cardiology Reports</source>. <year>2013</year>;<volume>16</volume>(<issue>1</issue>):<fpage>441</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11886-013-0441-8</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Patel</surname><given-names>VL</given-names></name>, <name><surname>Shortliffe</surname><given-names>EH</given-names></name>, <name><surname>Stefanelli</surname><given-names>M</given-names></name>, <name><surname>Szolovits</surname><given-names>P</given-names></name>, <name><surname>Berthold</surname><given-names>MR</given-names></name>, <name><surname>Bellazzi</surname><given-names>R</given-names></name>, <etal>et al</etal>. <article-title>The coming of age of artificial intelligence in medicine</article-title>. <source>Artificial Intelligence in Medicine</source>. <year>2009</year>;<volume>46</volume>(<issue>1</issue>):<fpage>5</fpage>–<lpage>17</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.artmed.2008.07.017</pub-id><?supplied-pmid 18790621?><pub-id pub-id-type="pmid">18790621</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Jha</surname><given-names>S</given-names></name>, <name><surname>Topol</surname><given-names>EJ</given-names></name>. <article-title>Adapting to artificial intelligence: radiologists and pathologists as information specialists</article-title>. <source>Jama</source>. <year>2016</year>;<volume>316</volume>(<issue>22</issue>):<fpage>2353</fpage>–<lpage>2354</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1001/jama.2016.17438</pub-id><?supplied-pmid 27898975?><pub-id pub-id-type="pmid">27898975</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>McCulloch</surname><given-names>WS</given-names></name>, <name><surname>Pitts</surname><given-names>W</given-names></name>. <article-title>A logical calculus of the ideas immanent in nervous activity</article-title>. <source>The bulletin of mathematical biophysics</source>. <year>1943</year>;<volume>5</volume>(<issue>4</issue>):<fpage>115</fpage>–<lpage>133</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/BF02478259</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Hearst</surname><given-names>MA</given-names></name>. <article-title>Support Vector Machines</article-title>. <source>IEEE Intelligent Systems</source>. <year>1998</year>;<volume>13</volume>(<issue>4</issue>):<fpage>18</fpage>–<lpage>28</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/5254.708428</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref007">
      <label>7</label>
      <mixed-citation publication-type="book"><name><surname>Dhawan</surname><given-names>AP</given-names></name>. <source>Medical image analysis</source>. <volume>vol. 31</volume>. <publisher-name>John Wiley x0026; Sons</publisher-name>; <year>2011</year>.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Shen</surname><given-names>D</given-names></name>, <name><surname>Wu</surname><given-names>G</given-names></name>, <name><surname>Suk</surname><given-names>HI</given-names></name>. <article-title>Deep learning in medical image analysis</article-title>. <source>Annual review of biomedical engineering</source>. <year>2017</year>;<volume>19</volume>:<fpage>221</fpage>–<lpage>248</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1146/annurev-bioeng-071516-044442</pub-id><?supplied-pmid 28301734?><pub-id pub-id-type="pmid">28301734</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Ritter</surname><given-names>F</given-names></name>, <name><surname>Boskamp</surname><given-names>T</given-names></name>, <name><surname>Homeyer</surname><given-names>A</given-names></name>, <name><surname>Laue</surname><given-names>H</given-names></name>, <name><surname>Schwier</surname><given-names>M</given-names></name>, <name><surname>Link</surname><given-names>F</given-names></name>, <etal>et al</etal>. <article-title>Medical image analysis</article-title>. <source>IEEE pulse</source>. <year>2011</year>;<volume>2</volume>(<issue>6</issue>):<fpage>60</fpage>–<lpage>70</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/MPUL.2011.942929</pub-id><?supplied-pmid 22147070?><pub-id pub-id-type="pmid">22147070</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref010">
      <label>10</label>
      <mixed-citation publication-type="other">Haifeng Wang, Dejin Hu. Comparison of SVM and LS-SVM for Regression. In: International Conference on Neural Networks and Brain. vol. 1; 2005. p. 279–283.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Suárez Sánchez</surname><given-names>A</given-names></name>, <name><surname>García Nieto</surname><given-names>PJ</given-names></name>, <name><surname>Riesgo Fernández</surname><given-names>P</given-names></name>, <name><surname>del Coz Díaz</surname><given-names>JJ</given-names></name>, <name><surname>Iglesias-Rodríguez</surname><given-names>FJ</given-names></name>. <article-title>Application of an SVM-based regression model to the air quality study at local scale in the Avilés urban area (Spain)</article-title>. <source>Mathematical and Computer Modelling</source>. <year>2011</year>;<volume>54</volume>(<issue>5</issue>):<fpage>1453</fpage>–<lpage>1466</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.mcm.2011.04.017</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Yue</surname><given-names>S</given-names></name>, <name><surname>Li</surname><given-names>P</given-names></name>, <name><surname>Hao</surname><given-names>P</given-names></name>. <article-title>SVM classification:Its contents and challenges</article-title>. <source>Applied Mathematics-A Journal of Chinese Universities</source>. <year>2003</year>;<volume>18</volume>(<issue>3</issue>):<fpage>332</fpage>–<lpage>342</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11766-003-0059-5</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Pham</surname><given-names>DL</given-names></name>, <name><surname>Xu</surname><given-names>C</given-names></name>, <name><surname>Prince</surname><given-names>JL</given-names></name>. <article-title>Current methods in medical image segmentation</article-title>. <source>Annual review of biomedical engineering</source>. <year>2000</year>;<volume>2</volume>(<issue>1</issue>):<fpage>315</fpage>–<lpage>337</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1146/annurev.bioeng.2.1.315</pub-id><?supplied-pmid 11701515?><pub-id pub-id-type="pmid">11701515</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Taha</surname><given-names>AA</given-names></name>, <name><surname>Hanbury</surname><given-names>A</given-names></name>. <article-title>Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool</article-title>. <source>BMC Medical Imaging</source>. <year>2015</year>;<volume>15</volume>(<issue>1</issue>):<fpage>29</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1186/s12880-015-0068-x</pub-id><?supplied-pmid 26263899?><pub-id pub-id-type="pmid">26263899</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>LK</given-names></name>, <name><surname>Liew</surname><given-names>SC</given-names></name>, <name><surname>Thong</surname><given-names>WJ</given-names></name>. <article-title>A review of image segmentation methodologies in medical image</article-title>. <source>Advanced computer and communication engineering technology</source>. <year>2015</year>; p. <fpage>1069</fpage>–<lpage>1080</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/978-3-319-07674-4_99</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref016">
      <label>16</label>
      <mixed-citation publication-type="other">The Norwegian Data Protection Authority;. Available from: <ext-link xlink:href="https://www.datatilsynet.no/en/" ext-link-type="uri">https://www.datatilsynet.no/en/</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref017">
      <label>17</label>
      <mixed-citation publication-type="other">The Personal Data Act;. Available from: <ext-link xlink:href="https://www.forskningsetikk.no/en/resources/the-research-ethics-library/legal-statutes-and-guidelines/the-personal-data-act/" ext-link-type="uri">https://www.forskningsetikk.no/en/resources/the-research-ethics-library/legal-statutes-and-guidelines/the-personal-data-act/</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref018">
      <label>18</label>
      <mixed-citation publication-type="other">Voigt P, Von dem Bussche A. The eu general data protection regulation (gdpr);.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref019">
      <label>19</label>
      <mixed-citation publication-type="other">Edemekong P, Annamaraju P, Haydel M. Health Insurance Portability and Accountability Act. StatPearls. 2020;.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref020">
      <label>20</label>
      <mixed-citation publication-type="other">California Consumer Privacy Act; 2018. Available from: <ext-link xlink:href="https://oag.ca.gov/privacy/ccpa" ext-link-type="uri">https://oag.ca.gov/privacy/ccpa</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref021">
      <label>21</label>
      <mixed-citation publication-type="other">Act on the Protection of Personal Information; 2003. Available from: <ext-link xlink:href="https://www.cas.go.jp/jp/seisaku/hourei/data/APPI.pdf" ext-link-type="uri">https://www.cas.go.jp/jp/seisaku/hourei/data/APPI.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref022">
      <label>22</label>
      <mixed-citation publication-type="other">Personal Information Protection Commission; 2011. Available from: <ext-link xlink:href="http://www.pipc.go.kr/cmt/main/english.do" ext-link-type="uri">http://www.pipc.go.kr/cmt/main/english.do</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref023">
      <label>23</label>
      <mixed-citation publication-type="other">The personal data protection bill; 2018. Available from: <ext-link xlink:href="https://www.meity.gov.in/writereaddata/files/Personal_Data_Protection_Bill,2018.pdf" ext-link-type="uri">https://www.meity.gov.in/writereaddata/files/Personal_Data_Protection_Bill,2018.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Renard</surname><given-names>F</given-names></name>, <name><surname>Guedria</surname><given-names>S</given-names></name>, <name><surname>Palma</surname><given-names>ND</given-names></name>, <name><surname>Vuillerme</surname><given-names>N</given-names></name>. <article-title>Variability and reproducibility in deep learning for medical image segmentation</article-title>. <source>Scientific Reports</source>. <year>2020</year>;<volume>10</volume>(<issue>1</issue>):<fpage>13724</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41598-020-69920-0</pub-id><?supplied-pmid 32792540?><pub-id pub-id-type="pmid">32792540</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref025">
      <label>25</label>
      <mixed-citation publication-type="journal"><name><surname>Willemink</surname><given-names>MJ</given-names></name>, <name><surname>Koszek</surname><given-names>WA</given-names></name>, <name><surname>Hardell</surname><given-names>C</given-names></name>, <name><surname>Wu</surname><given-names>J</given-names></name>, <name><surname>Fleischmann</surname><given-names>D</given-names></name>, <name><surname>Harvey</surname><given-names>H</given-names></name>, <etal>et al</etal>. <article-title>Preparing medical imaging data for machine learning</article-title>. <source>Radiology</source>. <year>2020</year>;<volume>295</volume>(<issue>1</issue>):<fpage>4</fpage>–<lpage>15</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1148/radiol.2020192224</pub-id><?supplied-pmid 32068507?><pub-id pub-id-type="pmid">32068507</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref026">
      <label>26</label>
      <mixed-citation publication-type="journal"><name><surname>Yu</surname><given-names>S</given-names></name>, <name><surname>Chen</surname><given-names>M</given-names></name>, <name><surname>Zhang</surname><given-names>E</given-names></name>, <name><surname>Wu</surname><given-names>J</given-names></name>, <name><surname>Yu</surname><given-names>H</given-names></name>, <name><surname>Yang</surname><given-names>Z</given-names></name>, <etal>et al</etal>. <article-title>Robustness study of noisy annotation in deep learning based medical image segmentation</article-title>. <source>Physics in Medicine &amp; Biology</source>. <year>2020</year>;<volume>65</volume>(<issue>17</issue>):<fpage>175007</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1088/1361-6560/ab99e5</pub-id><?supplied-pmid 32503027?><pub-id pub-id-type="pmid">32503027</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Dwork</surname><given-names>C</given-names></name>, <name><surname>Roth</surname><given-names>A</given-names></name>, <etal>et al</etal>. <article-title>The algorithmic foundations of differential privacy</article-title>. <source>Found Trends Theor Comput Sci</source>. <year>2014</year>;<volume>9</volume>(<issue>3-4</issue>):<fpage>211</fpage>–<lpage>407</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1561/0400000042</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref028">
      <label>28</label>
      <mixed-citation publication-type="other">Abadi M, Chu A, Goodfellow I, McMahan HB, Mironov I, Talwar K, et al. Deep Learning with Differential Privacy. In: ACM SIGSAC Conference on Computer and Communications Security. New York, NY, USA: Association for Computing Machinery; 2016. p. 308–318. Available from: <pub-id pub-id-type="doi">10.1145/2976749.2978318</pub-id>.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref029">
      <label>29</label>
      <mixed-citation publication-type="journal"><name><surname>Ficek</surname><given-names>J</given-names></name>, <name><surname>Wang</surname><given-names>W</given-names></name>, <name><surname>Chen</surname><given-names>H</given-names></name>, <name><surname>Dagne</surname><given-names>G</given-names></name>, <name><surname>Daley</surname><given-names>E</given-names></name>. <article-title>Differential privacy in health research: A scoping review</article-title>. <source>Journal of the American Medical Informatics Association</source>. <year>2021</year>;<volume>28</volume>(<issue>10</issue>):<fpage>2269</fpage>–<lpage>2276</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/jamia/ocab135</pub-id><?supplied-pmid 34333623?><pub-id pub-id-type="pmid">34333623</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref030">
      <label>30</label>
      <mixed-citation publication-type="other">Lindner L, Narnhofer D, Weber M, Gsaxner C, Kolodziej M, Egger J. Using Synthetic Training Data for Deep Learning-Based GBM Segmentation. In: 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society; 2019. p. 6724–6729.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref031">
      <label>31</label>
      <mixed-citation publication-type="journal"><name><surname>Shin</surname><given-names>Y</given-names></name>, <name><surname>Qadir</surname><given-names>HA</given-names></name>, <name><surname>Balasingham</surname><given-names>I</given-names></name>. <article-title>Abnormal Colon Polyp Image Synthesis Using Conditional Adversarial Networks for Improved Detection Performance</article-title>. <source>IEEE Access</source>. <year>2018</year>;<volume>6</volume>:<fpage>56007</fpage>–<lpage>56017</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/ACCESS.2018.2872717</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref032">
      <label>32</label>
      <mixed-citation publication-type="other">Thambawita V, Hicks SA, Isaksen J, Stensen MH, Haugen TB, Kanters J, et al. DeepSynthBody: the beginning of the end for data deficiency in medicine. In: International Conference on Applied Artificial Intelligence; 2021. p. 1–8.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref033">
      <label>33</label>
      <mixed-citation publication-type="journal"><name><surname>Thambawita</surname><given-names>VL</given-names></name>, <name><surname>Strümke</surname><given-names>I</given-names></name>, <name><surname>Hicks</surname><given-names>S</given-names></name>, <name><surname>Riegler</surname><given-names>MA</given-names></name>, <name><surname>Halvorsen</surname><given-names>P</given-names></name>, <name><surname>Parasa</surname><given-names>S</given-names></name>. <article-title>ID: 3523524 Data augmentation using generative adversarial networks for creating realistic artificial colon polyp images: validation study by endoscopists</article-title>. <source>Gastrointestinal Endoscopy</source>. <year>2021</year>;<volume>93</volume>(<issue>6</issue>):<fpage>AB190</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.gie.2021.03.431</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref034">
      <label>34</label>
      <mixed-citation publication-type="journal"><name><surname>Thambawita</surname><given-names>V</given-names></name>, <name><surname>Isaksen</surname><given-names>JL</given-names></name>, <name><surname>Hicks</surname><given-names>SA</given-names></name>, <name><surname>Ghouse</surname><given-names>J</given-names></name>, <name><surname>Ahlberg</surname><given-names>G</given-names></name>, <name><surname>Linneberg</surname><given-names>A</given-names></name>, <etal>et al</etal>. <article-title>DeepFake electrocardiograms: the key for open science for artificial intelligence in medicine</article-title>. <source>medRxiv</source>. <year>2021</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1101/2021.04.27.21256189</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref035">
      <label>35</label>
      <mixed-citation publication-type="other">Valerio Giuffrida M, Scharr H, Tsaftaris SA. Arigan: Synthetic arabidopsis plants using generative adversarial network. In: IEEE International Conference on Computer Vision Workshops; 2017. p. 2064–2071.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref036">
      <label>36</label>
      <mixed-citation publication-type="journal"><name><surname>Arsenovic</surname><given-names>M</given-names></name>, <name><surname>Karanovic</surname><given-names>M</given-names></name>, <name><surname>Sladojevic</surname><given-names>S</given-names></name>, <name><surname>Anderla</surname><given-names>A</given-names></name>, <name><surname>Stefanovic</surname><given-names>D</given-names></name>. <article-title>Solving current limitations of deep learning based approaches for plant disease detection</article-title>. <source>Symmetry</source>. <year>2019</year>;<volume>11</volume>(<issue>7</issue>):<fpage>939</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/sym11070939</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref037">
      <label>37</label>
      <mixed-citation publication-type="journal"><name><surname>Chao</surname><given-names>F</given-names></name>, <name><surname>Lin</surname><given-names>G</given-names></name>, <name><surname>Zheng</surname><given-names>L</given-names></name>, <name><surname>Chang</surname><given-names>X</given-names></name>, <name><surname>Lin</surname><given-names>CM</given-names></name>, <name><surname>Yang</surname><given-names>L</given-names></name>, <etal>et al</etal>. <article-title>An LSTM Based Generative Adversarial Architecture for Robotic Calligraphy Learning System</article-title>. <source>Sustainability</source>. <year>2020</year>;<volume>12</volume>(<issue>21</issue>):<fpage>9092</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/su12219092</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref038">
      <label>38</label>
      <mixed-citation publication-type="journal"><name><surname>Zhan</surname><given-names>H</given-names></name>, <name><surname>Tao</surname><given-names>F</given-names></name>, <name><surname>Cao</surname><given-names>Y</given-names></name>. <article-title>Human-guided Robot Behavior Learning: A GAN-assisted Preference-based Reinforcement Learning Approach</article-title>. <source>IEEE Robotics and Automation Letters</source>. <year>2021</year>;<volume>6</volume>(<issue>2</issue>):<fpage>3545</fpage>–<lpage>3552</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/LRA.2021.3063927</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref039">
      <label>39</label>
      <mixed-citation publication-type="journal"><name><surname>Lu</surname><given-names>H</given-names></name>, <name><surname>Du</surname><given-names>M</given-names></name>, <name><surname>Qian</surname><given-names>K</given-names></name>, <name><surname>He</surname><given-names>X</given-names></name>, <name><surname>Wang</surname><given-names>K</given-names></name>. <article-title>GAN-based Data Augmentation Strategy for Sensor Anomaly Detection in Industrial Robots</article-title>. <source>IEEE Sensors Journal</source>. <year>2021</year>;.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref040">
      <label>40</label>
      <mixed-citation publication-type="journal"><name><surname>Theagarajan</surname><given-names>R</given-names></name>, <name><surname>Bhanu</surname><given-names>B</given-names></name>. <article-title>DeephESC 2.0: Deep generative multi adversarial networks for improving the classification of hESC</article-title>. <source>PloS one</source>. <year>2019</year>;<volume>14</volume>(<issue>3</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pone.0212849</pub-id><?supplied-pmid 30840685?><pub-id pub-id-type="pmid">30840685</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref041">
      <label>41</label>
      <mixed-citation publication-type="other">Witmer A, Bhanu B. HESCNET: A Synthetically Pre-Trained Convolutional Neural Network for Human Embryonic Stem Cell Colony Classification. In: IEEE International Conference on Image Processing. IEEE; 2018. p. 2441–2445.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref042">
      <label>42</label>
      <mixed-citation publication-type="other">Jonnalagedda P, Weinberg B, Allen J, Min TL, Bhanu S, Bhanu B. SAGE: Sequential Attribute Generator for Analyzing Glioblastomas using Limited Dataset. In: International Conference on Pattern Recognition. IEEE; 2021. p. 4941–4948.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref043">
      <label>43</label>
      <mixed-citation publication-type="other">Radford A, Metz L, Chintala S. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:151106434. 2015;.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref044">
      <label>44</label>
      <mixed-citation publication-type="journal"><name><surname>Torre</surname><given-names>LA</given-names></name>, <name><surname>Bray</surname><given-names>F</given-names></name>, <name><surname>Siegel</surname><given-names>RL</given-names></name>, <name><surname>Ferlay</surname><given-names>J</given-names></name>, <name><surname>Lortet-Tieulent</surname><given-names>J</given-names></name>, <name><surname>Jemal</surname><given-names>A</given-names></name>. <article-title>Global cancer statistics, 2012</article-title>. <source>CA: a cancer journal for clinicians</source>. <year>2015</year>;<volume>65</volume>(<issue>2</issue>):<fpage>87</fpage>–<lpage>108</lpage>. <?supplied-pmid 25651787?><pub-id pub-id-type="pmid">25651787</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref045">
      <label>45</label>
      <mixed-citation publication-type="other">Thambawita V, Hicks S, Halvorsen P, Riegler MA. Pyramid-Focus-Augmentation: Medical Image Segmentation with Step-Wise Focus. arXiv preprint arXiv:201207430. 2020;.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref046">
      <label>46</label>
      <mixed-citation publication-type="journal"><name><surname>Jha</surname><given-names>D</given-names></name>, <name><surname>Ali</surname><given-names>S</given-names></name>, <name><surname>Tomar</surname><given-names>NK</given-names></name>, <name><surname>Johansen</surname><given-names>HD</given-names></name>, <name><surname>Johansen</surname><given-names>D</given-names></name>, <name><surname>Rittscher</surname><given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Real-time polyp detection, localization and segmentation in colonoscopy using deep learning</article-title>. <source>Ieee Access</source>. <year>2021</year>;<volume>9</volume>:<fpage>40496</fpage>–<lpage>40510</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/ACCESS.2021.3063716</pub-id><?supplied-pmid 33747684?><pub-id pub-id-type="pmid">33747684</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref047">
      <label>47</label>
      <mixed-citation publication-type="other">Thambawita V, Hicks SA, Halvorsen P, Riegler MA. DivergentNets: Medical Image Segmentation by Network Ensemble. In: EndoCV at International Symposium on Biomedical Imaging; 2021.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref048">
      <label>48</label>
      <mixed-citation publication-type="journal"><name><surname>Prasath</surname><given-names>V</given-names></name>. <article-title>Polyp detection and segmentation from video capsule endoscopy: A review</article-title>. <source>Journal of Imaging</source>. <year>2017</year>;<volume>3</volume>(<issue>1</issue>):<fpage>1</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/jimaging3010001</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref049">
      <label>49</label>
      <mixed-citation publication-type="other">Jha D, Tomar NK, Ali S, Riegler MA, Johansen HD, Johansen D, et al. NanoNet: Real-Time Polyp Segmentation in Video Capsule Endoscopy and Colonoscopy. arXiv preprint arXiv:210411138. 2021;.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref050">
      <label>50</label>
      <mixed-citation publication-type="other">Figueiredo IN, Prasath S, Tsai YHR, Figueiredo PN. Automatic detection and segmentation of colonic polyps in wireless capsule images. ICES REPORT. 2010; p. 10–36.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref051">
      <label>51</label>
      <mixed-citation publication-type="journal"><name><surname>Borgli</surname><given-names>H</given-names></name>, <name><surname>Thambawita</surname><given-names>V</given-names></name>, <name><surname>Smedsrud</surname><given-names>PH</given-names></name>, <name><surname>Hicks</surname><given-names>S</given-names></name>, <name><surname>Jha</surname><given-names>D</given-names></name>, <name><surname>Eskeland</surname><given-names>SL</given-names></name>, <etal>et al</etal>. <article-title>HyperKvasir, a comprehensive multi-class image and video dataset for gastrointestinal endoscopy</article-title>. <source>Scientific Data</source>. <year>2020</year>;<volume>7</volume>(<issue>1</issue>):<fpage>283</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41597-020-00622-y</pub-id><?supplied-pmid 32859981?><pub-id pub-id-type="pmid">32859981</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref052">
      <label>52</label>
      <mixed-citation publication-type="journal"><name><surname>Bernal</surname><given-names>J</given-names></name>, <name><surname>Sánchez</surname><given-names>FJ</given-names></name>, <name><surname>Fernández-Esparrach</surname><given-names>G</given-names></name>, <name><surname>Gil</surname><given-names>D</given-names></name>, <name><surname>Rodríguez</surname><given-names>C</given-names></name>, <name><surname>Vilariño</surname><given-names>F</given-names></name>. <article-title>WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</article-title>. <source>Computerized Medical Imaging and Graphics</source>. <year>2015</year>;<volume>43</volume>:<fpage>99</fpage>–<lpage>111</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.compmedimag.2015.02.007</pub-id><?supplied-pmid 25863519?><pub-id pub-id-type="pmid">25863519</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref053">
      <label>53</label>
      <mixed-citation publication-type="journal"><name><surname>Silva</surname><given-names>J</given-names></name>, <name><surname>Histace</surname><given-names>A</given-names></name>, <name><surname>Romain</surname><given-names>O</given-names></name>, <name><surname>Dray</surname><given-names>X</given-names></name>, <name><surname>Granado</surname><given-names>B</given-names></name>. <article-title>Toward embedded detection of polyps in wce images for early diagnosis of colorectal cancer</article-title>. <source>International journal of computer assisted radiology and surgery</source>. <year>2014</year>;<volume>9</volume>(<issue>2</issue>):<fpage>283</fpage>–<lpage>293</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11548-013-0926-3</pub-id><?supplied-pmid 24037504?><pub-id pub-id-type="pmid">24037504</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref054">
      <label>54</label>
      <mixed-citation publication-type="journal"><name><surname>Tajbakhsh</surname><given-names>N</given-names></name>, <name><surname>Gurudu</surname><given-names>SR</given-names></name>, <name><surname>Liang</surname><given-names>J</given-names></name>. <article-title>Automated polyp detection in colonoscopy videos using shape and context information</article-title>. <source>IEEE transactions on medical imaging</source>. <year>2015</year>;<volume>35</volume>(<issue>2</issue>):<fpage>630</fpage>–<lpage>644</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TMI.2015.2487997</pub-id><?supplied-pmid 26462083?><pub-id pub-id-type="pmid">26462083</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref055">
      <label>55</label>
      <mixed-citation publication-type="journal"><name><surname>Sánchez-Peralta</surname><given-names>LF</given-names></name>, <name><surname>Pagador</surname><given-names>JB</given-names></name>, <name><surname>Picón</surname><given-names>A</given-names></name>, <name><surname>Calderón</surname><given-names>ÁJ</given-names></name>, <name><surname>Polo</surname><given-names>F</given-names></name>, <name><surname>Andraka</surname><given-names>N</given-names></name>, <etal>et al</etal>. <article-title>PICCOLO White-Light and Narrow-Band Imaging Colonoscopic Dataset: A Performance Comparative of Models and Datasets</article-title>. <source>Applied Sciences</source>. <year>2020</year>;<volume>10</volume>(<issue>23</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/app10238501</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref056">
      <label>56</label>
      <mixed-citation publication-type="other">Shaham TR, Dekel T, Michaeli T. Singan: Learning a generative model from a single natural image. In: IEEE/CVF International Conference on Computer Vision; 2019. p. 4570–4580.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref057">
      <label>57</label>
      <mixed-citation publication-type="other">Gatys LA, Ecker AS, Bethge M. A neural algorithm of artistic style. arXiv preprint arXiv:150806576. 2015;.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref058">
      <label>58</label>
      <mixed-citation publication-type="journal"><name><surname>Paszke</surname><given-names>A</given-names></name>, <name><surname>Gross</surname><given-names>S</given-names></name>, <name><surname>Massa</surname><given-names>F</given-names></name>, <name><surname>Lerer</surname><given-names>A</given-names></name>, <name><surname>Bradbury</surname><given-names>J</given-names></name>, <name><surname>Chanan</surname><given-names>G</given-names></name>, <etal>et al</etal>. <article-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</article-title>. In: <source>Advances in Neural Information Processing Systems</source>; <year>2019</year>. p. <fpage>8024</fpage>–<lpage>8035</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref059">
      <label>59</label>
      <mixed-citation publication-type="book"><name><surname>Zhou</surname><given-names>Z</given-names></name>, <name><surname>Siddiquee</surname><given-names>MMR</given-names></name>, <name><surname>Tajbakhsh</surname><given-names>N</given-names></name>, <name><surname>Liang</surname><given-names>J</given-names></name>. <part-title>Unet++: A nested u-net architecture for medical image segmentation</part-title>. In: <source>Deep learning in medical image analysis and multimodal learning for clinical decision support</source>. <publisher-name>Springer</publisher-name>; <year>2018</year>. p. <fpage>3</fpage>–<lpage>11</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref060">
      <label>60</label>
      <mixed-citation publication-type="journal"><name><surname>Buslaev</surname><given-names>A</given-names></name>, <name><surname>Iglovikov</surname><given-names>VI</given-names></name>, <name><surname>Khvedchenya</surname><given-names>E</given-names></name>, <name><surname>Parinov</surname><given-names>A</given-names></name>, <name><surname>Druzhinin</surname><given-names>M</given-names></name>, <name><surname>Kalinin</surname><given-names>AA</given-names></name>. <article-title>Albumentations: Fast and Flexible Image Augmentations</article-title>. <source>Information</source>. <year>2020</year>;<volume>11</volume>(<issue>2</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/info11020125</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref061">
      <label>61</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Z</given-names></name>, <name><surname>Wang</surname><given-names>E</given-names></name>, <name><surname>Zhu</surname><given-names>Y</given-names></name>. <article-title>Image segmentation evaluation: a survey of methods</article-title>. <source>Artificial Intelligence Review</source>. <year>2020</year>;<volume>53</volume>(<issue>8</issue>):<fpage>5637</fpage>–<lpage>5674</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s10462-020-09830-9</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0267976.ref062">
      <label>62</label>
      <mixed-citation publication-type="other">Wen J, Thibeau E, Samper-González J, Routier A, Bottani S, Dormont D, et al. How serious is data leakage in deep learning studies on Alzheimer’s disease classification? In: OHBM Annual meeting-Organization for Human Brain Mapping; 2019.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref063">
      <label>63</label>
      <mixed-citation publication-type="journal"><name><surname>Radford</surname><given-names>A</given-names></name>, <name><surname>Metz</surname><given-names>L</given-names></name>, <name><surname>Chintala</surname><given-names>S</given-names></name>. <article-title>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</article-title>. <source>CoRR</source>. <year>2016</year>;abs/1511.06434.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref064">
      <label>64</label>
      <mixed-citation publication-type="other">Karras T, Aila T, Laine S, Lehtinen J. Progressive Growing of GANs for Improved Quality, Stability, and Variation. In: International Conference on Learning Representations; 2018.Available from: <ext-link xlink:href="https://openreview.net/forum?id=Hk99zCeAb" ext-link-type="uri">https://openreview.net/forum?id=Hk99zCeAb</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref065">
      <label>65</label>
      <mixed-citation publication-type="other">Liu B, Zhu Y, Song K, Elgammal A. Towards faster and stabilized gan training for high-fidelity few-shot image synthesis. In: International Conference on Learning Representations; 2020.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref066">
      <label>66</label>
      <mixed-citation publication-type="journal"><name><surname>Heusel</surname><given-names>M</given-names></name>, <name><surname>Ramsauer</surname><given-names>H</given-names></name>, <name><surname>Unterthiner</surname><given-names>T</given-names></name>, <name><surname>Nessler</surname><given-names>B</given-names></name>, <name><surname>Hochreiter</surname><given-names>S</given-names></name>. <article-title>Gans trained by a two time-scale update rule converge to a local nash equilibrium</article-title>. <source>Advances in neural information processing systems</source>. <year>2017</year>;<volume>30</volume>.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref067">
      <label>67</label>
      <mixed-citation publication-type="other">Parmar G, Zhang R, Zhu JY. On Aliased Resizing and Surprising Subtleties in GAN Evaluation. In: Computer Vision and Pattern Recognition; 2022.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref068">
      <label>68</label>
      <mixed-citation publication-type="other">Ledig C, Theis L, Huszár F, Caballero J, Cunningham A, Acosta A, et al. Photo-realistic single image super-resolution using a generative adversarial network. In: IEEE conference on computer vision and pattern recognition; 2017. p. 4681–4690.</mixed-citation>
    </ref>
    <ref id="pone.0267976.ref069">
      <label>69</label>
      <mixed-citation publication-type="journal"><name><surname>Thambawita</surname><given-names>VL</given-names></name>, <name><surname>Hicks</surname><given-names>S</given-names></name>, <name><surname>Strümke</surname><given-names>I</given-names></name>, <name><surname>Riegler</surname><given-names>MA</given-names></name>, <name><surname>Halvorsen</surname><given-names>P</given-names></name>, <name><surname>Parasa</surname><given-names>S</given-names></name>. <article-title>Impact of image resolution on convolutional neural networks performance in gastrointestinal endoscopy</article-title>. <source>Gastroenterology</source>. <year>2021</year>;<volume>160</volume>(<issue>6</issue>):<fpage>S</fpage>–<lpage>377</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/S0016-5085(21)01616-4</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<sub-article article-type="aggregated-review-documents" id="pone.0267976.r001" specific-use="decision-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0267976.r001</article-id>
    <title-group>
      <article-title>Decision Letter 0</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Stoean</surname>
          <given-names>Ruxandra</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2022 Ruxandra Stoean</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Ruxandra Stoean</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0267976" id="rel-obj001" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>0</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">10 Dec 2021</named-content>
    </p>
    <p><!-- <div> -->PONE-D-21-31343<!-- </div> --><!-- <div> -->SinGAN-Seg: Synthetic training data generation for medical image segmentation<!-- </div> --><!-- <div> -->PLOS ONE</p>
    <p>Dear Dr. Thambawita,</p>
    <p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
    <p>The paper needs significant improvement. It should position the proposed method with respect to the state of the art in GAN-based synthetic data generation - detail the contribution, show the advantages, put the results in comparison. It should also explain the improvement of results due to style transfer. More metrics should be employed to evaluate the generated images.</p>
    <p>Please submit your revised manuscript by Jan 24 2022 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
    <p>Please include the following items when submitting your revised manuscript:<!-- </div> --><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list></p>
    <p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p>
    <p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</ext-link>.</p>
    <p>We look forward to receiving your revised manuscript.</p>
    <p>Kind regards,</p>
    <p>Ruxandra Stoean</p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
    <p>Journal Requirements:</p>
    <p>When submitting your revision, we need you to address these additional requirements.</p>
    <p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at </p>
    <p><ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</ext-link> and </p>
    <p>
      <ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</ext-link>
    </p>
    <p>2.  Thank you for stating the following in the Acknowledgments Section of your manuscript: </p>
    <p>"The research has benefited from the Experimental Infrastructure for Exploration of</p>
    <p>Exascale Computing (eX3), which is financially supported by the Research Council of Norway under contract."</p>
    <p>We note that you have provided funding information that is not currently declared in your Funding Statement. However, funding information should not appear in the Acknowledgments section or other areas of your manuscript. We will only publish funding information present in the Funding Statement section of the online submission form. </p>
    <p>Please remove any funding-related text from the manuscript and let us know how you would like to update your Funding Statement. Currently, your Funding Statement reads as follows: </p>
    <p>"The author(s) received no specific funding for this work."</p>
    <p>Please include your amended statements within your cover letter; we will change the online submission form on your behalf</p>
    <p>[Note: HTML markup is below. Please do not edit.]</p>
    <p>Reviewers' comments:</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <!-- <font color="black"> -->
      <bold>Comments to the Author</bold>
    </p>
    <p>1. Is the manuscript technically sound, and do the data support the conclusions?</p>
    <p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!-- </font> --></p>
    <p>Reviewer #1: Partly</p>
    <p>Reviewer #2: Partly</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->2. Has the statistical analysis been performed appropriately and rigorously? <!-- </font> --></p>
    <p>Reviewer #1: No</p>
    <p>Reviewer #2: No</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->3. Have the authors made all data underlying the findings in their manuscript fully available?</p>
    <p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.<!-- </font> --></p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->4. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
    <p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!-- </font> --></p>
    <p>Reviewer #1: No</p>
    <p>Reviewer #2: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->5. Review Comments to the Author</p>
    <p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!-- </font> --></p>
    <p>Reviewer #1: 1) First of all, the figures need to be significantly improved. Right now they are of very poor quality and very blurred. It is hard for the reviewers to make sense of the figures as they are. Use previously published PLOS ONE papers as a reference.</p>
    <p>2) There are many spelling mistakes. Please run the manuscript through a spell check.</p>
    <p>3) The proposed approach uses state-of-the-art generation approach SinGAN to generate synthetic images along with the corresponding ground-truth followed by a style transfer. This is not a significant novelty other than the part of generating the corresponding ground-truth.</p>
    <p>4) The last 2 contributions of the paper are not new:</p>
    <p>a) "We show that synthetic images and corresponding mask images can improve the segmentation performance when the size of a training dataset is limited."</p>
    <p>b) "We show that synthetic data can achieve a very close performance to the real data when the real segmentation datasets are large enough."</p>
    <p>Recent works such as [1 - 3] have already shown this with medical images. The references are missing.</p>
    <p>[1] Theagarajan, R. and Bhanu, B., 2019. DeephESC 2.0: Deep generative multi adversarial networks for improving the classification of hESC. PloS one, 14(3), p.e0212849.</p>
    <p>[2] Witmer, A. and Bhanu, B., 2018, October. HESCNET: A Synthetically Pre-Trained Convolutional Neural Network for Human Embryonic Stem Cell Colony Classification. In 2018 25th IEEE International Conference on Image Processing (ICIP) (pp. 2441-2445). IEEE.</p>
    <p>[3] Jonnalagedda, P., Weinberg, B., Allen, J., Min, T.L., Bhanu, S. and Bhanu, B., 2021, January. SAGE: Sequential Attribute Generator for Analyzing Glioblastomas using Limited Dataset. In 2020 25th International Conference on Pattern Recognition (ICPR) (pp. 4941-4948). IEEE.</p>
    <p>4) quantitative evaluation of the generated synthetic images are required. The authors need to provide scores such as the FID and SIFID scores as used in the SinGAN paper.</p>
    <p>4) The results provided in Tables 1 and 2 are not sufficient. Since training data is very important, how are the qualities of the generated images verified in Tables 1 and 2? One suggestion would be to compute the quality metric (FID, SIFID, etc.) of the generated images and take only the top X% of the synthetic images for training</p>
    <p>Reviewer #2: The authors have developed a method for generating synthetic data for medical image segmentation based on sinGAN and style transfer. It is an interesting study to deal with privacy and small dataset of medical images. However, there is no comparison with existing methods, and the superiority of the proposed method is not clear. The reviewer suggests the authors to revise the manuscript according to the following comments.</p>
    <p>SinGAN-Seg as data sharing technique</p>
    <p>1. Please add a comparison of the proposed method with other data augmentation methods (e.g., conventional GAN). What is the advantage of the proposed method over the conventional GAN based synthetic data generation?</p>
    <p>SinGAN-Seg with small datasets</p>
    <p>2. As mentioned above, please highlight the superiority of the proposed method compared to other common data augmentation methods for small datasets analysis, too.</p>
    <p>3. Did you apply style transfer in the results of Table 2 and Fig.11? Please clarify how style transfer improves the results.</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->6. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!-- </font> --></p>
    <p>Reviewer #1: No</p>
    <p>Reviewer #2: No</p>
    <p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
    <p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p>
  </body>
</sub-article>
<sub-article article-type="author-comment" id="pone.0267976.r002">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0267976.r002</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 0</article-title>
    </title-group>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0267976" id="rel-obj002" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">24 Jan 2022</named-content>
    </p>
    <p>Ruxandra Stoean</p>
    <p>Academic editor</p>
    <p>PLOS ONE</p>
    <p>Dear editor,</p>
    <p>Herewith, we submit our revised manuscript titled “SinGAN-Seg: Synthetic training data generation for medical image segmentation”.</p>
    <p>Thank you very much for giving us the opportunity to revise our manuscript and resubmit. We really appreciate that the reviewers and you have commented and given us feedback to improve our manuscript. We have thoroughly gone through the reviewers comments and we addressed all comments point by point. The response letters are attached in the submission system for each reviewer.</p>
    <p>We believe the review process has improved the quality of our study significantly. Moreover, we have followed the latex format given by PLOS ONE to complete the manuscript and the final tex file is attached in the submission system. </p>
    <p>Regarding the acknowledgement statement, "The research has benefited from the Experimental Infrastructure for Exploration of Exascale Computing (eX3), which is financially supported by the Research Council of Norway under contract." : This is not a funding source for our project, but for a general project which provides the hardware resources to run experiments in our lab. Therefore, the current funding statement "The author(s) received no specific funding for this work." is still valid. </p>
    <p>However, if you think that we should remove the acknowledgement statement then it would be no problem for us to do so. </p>
    <p>Thank you very much.</p>
    <p>Yours sincerely,</p>
    <p>Vajira Thambawita</p>
    <p>
      <email>vajira@simula.no</email>
    </p>
    <supplementary-material id="pone.0267976.s001" position="float" content-type="local-data">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">Response to Reviewers.pdf</named-content></p>
      </caption>
      <media xlink:href="pone.0267976.s001.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article article-type="aggregated-review-documents" id="pone.0267976.r003" specific-use="decision-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0267976.r003</article-id>
    <title-group>
      <article-title>Decision Letter 1</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Stoean</surname>
          <given-names>Ruxandra</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2022 Ruxandra Stoean</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Ruxandra Stoean</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0267976" id="rel-obj003" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">9 Feb 2022</named-content>
    </p>
    <p><!-- <div> -->PONE-D-21-31343R1<!-- </div> --><!-- <div> -->SinGAN-Seg: Synthetic training data generation for medical image segmentation<!-- </div> --><!-- <div> -->PLOS ONE</p>
    <p>Dear Dr. Thambawita,</p>
    <p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
    <p>Improvement is still needed regarding experimental comparison and formatting.</p>
    <p>Please submit your revised manuscript by Mar 26 2022 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
    <p>Please include the following items when submitting your revised manuscript:<!-- </div> --><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list></p>
    <p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p>
    <p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</ext-link>.</p>
    <p>We look forward to receiving your revised manuscript.</p>
    <p>Kind regards,</p>
    <p>Ruxandra Stoean</p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
    <p>[Note: HTML markup is below. Please do not edit.]</p>
    <p>Reviewers' comments:</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <!-- <font color="black"> -->
      <bold>Comments to the Author</bold>
    </p>
    <p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the “Comments to the Author” section, enter your conflict of interest statement in the “Confidential to Editor” section, and submit your "Accept" recommendation.<!-- </font> --></p>
    <p>Reviewer #1: (No Response)</p>
    <p>Reviewer #2: All comments have been addressed</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->2. Is the manuscript technically sound, and do the data support the conclusions?</p>
    <p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!-- </font> --></p>
    <p>Reviewer #1: Partly</p>
    <p>Reviewer #2: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->3. Has the statistical analysis been performed appropriately and rigorously? <!-- </font> --></p>
    <p>Reviewer #1: No</p>
    <p>Reviewer #2: N/A</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->4. Have the authors made all data underlying the findings in their manuscript fully available?</p>
    <p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.<!-- </font> --></p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->5. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
    <p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!-- </font> --></p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->6. Review Comments to the Author</p>
    <p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!-- </font> --></p>
    <p>Reviewer #1: 1) The authors mentioned that they have uploaded better quality images, but I still do not see any improvement in the quality of the images.</p>
    <p>2) There are still some minor spelling mistakes, please check one more time. Line 93 date -&gt; data.</p>
    <p>3) Table 1 in the revised manuscript is very poorly formatted. I cannot see anything, except the first column. Please check the manuscript before uploading for review!</p>
    <p>4) The authors mention and discuss the quality of the generated images using SinGan-seg throughout the paper but, there is no comparison with other GAN related works. Hence it is not possible to validate if the proposed approach is significantly better in terms of quality, other than the fact that it requires only 1 training image.</p>
    <p>4.1) Although SinGan-seg takes only 1 input to train compared to conventional GANs that require more images, the authors need to generate results using at least 2 - 3 other approaches that use conventional GANs (even if it requires more training images compared to SinGan-seg). We need to see if there is significant difference in quality of generated images between the different approaches. This comparison will greatly benefit the readers to understand if the difference in no. of training images Vs. quality of generated images is significantly different.</p>
    <p>4.2) Similarly compare the SIFID scores of SinGan-seg with these other approaches.</p>
    <p>Reviewer #2: (No Response)</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->7. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!-- </font> --></p>
    <p>Reviewer #1: No</p>
    <p>Reviewer #2: No</p>
    <p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
    <p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p>
  </body>
</sub-article>
<sub-article article-type="author-comment" id="pone.0267976.r004">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0267976.r004</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 1</article-title>
    </title-group>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0267976" id="rel-obj004" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>2</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">29 Mar 2022</named-content>
    </p>
    <p>Response to reviwers letter is attached to this submission.</p>
    <supplementary-material id="pone.0267976.s002" position="float" content-type="local-data">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">Response to Reviewers.pdf</named-content></p>
      </caption>
      <media xlink:href="pone.0267976.s002.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article article-type="aggregated-review-documents" id="pone.0267976.r005" specific-use="decision-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0267976.r005</article-id>
    <title-group>
      <article-title>Decision Letter 2</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Stoean</surname>
          <given-names>Ruxandra</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2022 Ruxandra Stoean</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Ruxandra Stoean</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0267976" id="rel-obj005" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>2</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">20 Apr 2022</named-content>
    </p>
    <p>SinGAN-Seg: Synthetic training data generation for medical image segmentation</p>
    <p>PONE-D-21-31343R2</p>
    <p>Dear Dr. Thambawita,</p>
    <p>We’re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p>
    <p>Within one week, you’ll receive an e-mail detailing the required amendments. When these have been addressed, you’ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p>
    <p>An invoice for payment will follow shortly after the formal acceptance. To ensure an efficient process, please log into Editorial Manager at <ext-link xlink:href="http://www.editorialmanager.com/pone/" ext-link-type="uri">http://www.editorialmanager.com/pone/</ext-link>, click the 'Update My Information' link at the top of the page, and double check that your user information is up-to-date. If you have any billing related questions, please contact our Author Billing department directly at <email>authorbilling@plos.org</email>.</p>
    <p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they’ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <email>onepress@plos.org</email>.</p>
    <p>Kind regards,</p>
    <p>Ruxandra Stoean</p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
    <p>Additional Editor Comments (optional):</p>
    <p>Reviewers' comments:</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <!-- <font color="black"> -->
      <bold>Comments to the Author</bold>
    </p>
    <p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the “Comments to the Author” section, enter your conflict of interest statement in the “Confidential to Editor” section, and submit your "Accept" recommendation.<!-- </font> --></p>
    <p>Reviewer #1: All comments have been addressed</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->2. Is the manuscript technically sound, and do the data support the conclusions?</p>
    <p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!-- </font> --></p>
    <p>Reviewer #1: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->3. Has the statistical analysis been performed appropriately and rigorously? <!-- </font> --></p>
    <p>Reviewer #1: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->4. Have the authors made all data underlying the findings in their manuscript fully available?</p>
    <p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.<!-- </font> --></p>
    <p>Reviewer #1: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->5. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
    <p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!-- </font> --></p>
    <p>Reviewer #1: Yes</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->6. Review Comments to the Author</p>
    <p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!-- </font> --></p>
    <p>Reviewer #1: The paper has been formatted well. The authors have addressed all my comments and necessary statistical experiments have been performed.</p>
    <p>**********</p>
    <p><!-- <font color="black"> -->7. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!-- </font> --></p>
    <p>Reviewer #1: No</p>
  </body>
</sub-article>
<sub-article article-type="editor-report" id="pone.0267976.r006" specific-use="acceptance-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0267976.r006</article-id>
    <title-group>
      <article-title>Acceptance letter</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Stoean</surname>
          <given-names>Ruxandra</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2022 Ruxandra Stoean</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Ruxandra Stoean</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0267976" id="rel-obj006" related-article-type="reviewed-article"/>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">22 Apr 2022</named-content>
    </p>
    <p>PONE-D-21-31343R2 </p>
    <p>SinGAN-Seg: Synthetic training data generation for medical image segmentation </p>
    <p>Dear Dr. Thambawita:</p>
    <p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now with our production department. </p>
    <p>If your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information please contact <email>onepress@plos.org</email>.</p>
    <p>If we can help with anything else, please email us at <email>plosone@plos.org</email>. </p>
    <p>Thank you for submitting your work to PLOS ONE and supporting open access. </p>
    <p>Kind regards, </p>
    <p>PLOS ONE Editorial Office Staff</p>
    <p>on behalf of</p>
    <p>Dr. Ruxandra Stoean </p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
  </body>
</sub-article>
