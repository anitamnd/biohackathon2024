<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Data</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Data</journal-id>
    <journal-title-group>
      <journal-title>Scientific Data</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2052-4463</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7658204</article-id>
    <article-id pub-id-type="publisher-id">715</article-id>
    <article-id pub-id-type="doi">10.1038/s41597-020-00715-8</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Data Descriptor</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CT-ORG, a new dataset for multiple organ segmentation in computed tomography</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4490-0444</contrib-id>
        <name>
          <surname>Rister</surname>
          <given-names>Blaine</given-names>
        </name>
        <address>
          <email>blaine@stanford.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yi</surname>
          <given-names>Darvin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Shivakumar</surname>
          <given-names>Kaushik</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nobashi</surname>
          <given-names>Tomomi</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5057-4369</contrib-id>
        <name>
          <surname>Rubin</surname>
          <given-names>Daniel L.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.168010.e</institution-id><institution-id institution-id-type="ISNI">0000000419368956</institution-id><institution>Department of Electrical Engineering, </institution><institution>Stanford University, </institution></institution-wrap>350 Jane Stanford Way, Stanford, CA 94305 USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.168010.e</institution-id><institution-id institution-id-type="ISNI">0000000419368956</institution-id><institution>Department of Biomedical Data Science, </institution><institution>Stanford University, </institution></institution-wrap>1265 Welch Road, Stanford, CA 94305 USA </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.168010.e</institution-id><institution-id institution-id-type="ISNI">0000000419368956</institution-id><institution>Department of Radiology, </institution><institution>Stanford University, </institution></institution-wrap>300 Pasteur Drive, Stanford, CA 94305 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>11</day>
      <month>11</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>11</day>
      <month>11</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>7</volume>
    <elocation-id>381</elocation-id>
    <history>
      <date date-type="received">
        <day>17</day>
        <month>2</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>1</day>
        <month>10</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
        <license-p>The Creative Commons Public Domain Dedication waiver <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link> applies to the metadata files associated with this article.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Despite the relative ease of locating organs in the human body, automated organ segmentation has been hindered by the scarcity of labeled training data. Due to the tedium of labeling organ boundaries, most datasets are limited to either a small number of cases or a single organ. Furthermore, many are restricted to specific imaging conditions unrepresentative of clinical practice. To address this need, we developed a diverse dataset of 140 CT scans containing six organ classes: liver, lungs, bladder, kidney, bones and brain. For the lungs and bones, we expedited annotation using unsupervised morphological segmentation algorithms, which were accelerated by 3D Fourier transforms. Demonstrating the utility of the data, we trained a deep neural network which requires only 4.3 s to simultaneously segment all the organs in a case. We also show how to efficiently augment the data to improve model generalization, providing a GPU library for doing so. We hope this dataset and code, available through TCIA, will be useful for training and evaluating organ segmentation models.</p>
    </abstract>
    <abstract id="Abs2" abstract-type="summary">
      <p id="Par2">
        <table-wrap id="Taba">
          <table frame="hsides" rules="groups">
            <tbody>
              <tr>
                <td>Measurement(s)</td>
                <td>organ subunit • image segmentation • brain segmentation • anatomical phenotype annotation</td>
              </tr>
              <tr>
                <td>Technology Type(s)</td>
                <td>unsupervised machine learning • Manual • computed tomography • supervised machine learning</td>
              </tr>
              <tr>
                <td>Factor Type(s)</td>
                <td>human organ</td>
              </tr>
              <tr>
                <td>Sample Characteristic - Organism</td>
                <td>Homo sapiens</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
      <p id="Par3">Machine-accessible metadata file describing the reported data: 10.6084/m9.figshare.13055663</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Bladder</kwd>
      <kwd>Liver</kwd>
      <kwd>Skeleton</kwd>
      <kwd>Kidney</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/100000054</institution-id>
            <institution>U.S. Department of Health &amp; Human Services | NIH | National Cancer Institute (NCI)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>1U01CA190214</award-id>
        <award-id>U01CA242879</award-id>
        <principal-award-recipient>
          <name>
            <surname>Rister</surname>
            <given-names>Blaine</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>U.S. Department of Health &amp; Human Services | NIH | National Cancer Institute (NCI)</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background &amp; Summary</title>
    <p id="Par4">Machine learning has the potential to transform healthcare by automating those tasks which defy mathematical description. In particular, fully-convolutional neural networks (FCNs) have far surpassed what was previously thought possible in semantic segmentation across all imaging domains<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. These models rival human visual recognition on any task for which sufficient data are available. However, with limited training data these models will overfit, failing to generalize to unseen examples. This is particularly important for medical images, which are often expensive and laborious to annotate, even when the task is intuitive. One such task is detecting and segmenting organs in computed tomography (CT) images, which has wide applicability in treatment planning, morphology and computer-aided diagnosis. Despite the relative ease of locating various organs in the human body, precise annotation of organ boundaries is tedious and vague, presenting a serious obstacle to automated solutions.</p>
    <p id="Par5">Several CT organ segmentation datasets are already publicly available, including the SLIVER, Pancreas-CT, and Medical Decathlon collections<sup><xref ref-type="bibr" rid="CR3">3</xref>–<xref ref-type="bibr" rid="CR5">5</xref></sup>. Most of these datasets are limited to a single organ, and it is not possible to simply combine all the data together for multi-organ segmentation, since this requires consistent labeling of all objects of interest across all the images. Some existing datasets label multiple organs in every scan. The most popular of these are the one from Gibson <italic>et al</italic>. and the VISCERAL Anatomy3 dataset from Jimenez-del-Toro <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR7">7</xref></sup>. While the Anatomy3 dataset offers an impressive range of organs and imaging modalities, it is limited to only 20 cases, far too few to train a model that will be robust in clinical practice where rare and anomalous conditions are frequently found. On the other hand, Gibson et. al provide 90 labeled cases, comprised of data from the Beyond the Cranial Vault and Pancreas-CT datasets<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>. However, these annotations are manually cropped around a specific region of the abdomen. This requires user interaction, and precludes the possibility of training a model which works on the whole body. A further limitation is that none of these datasets label the skeleton, which is an important site of metastatic cancer and other disease. All of these factors suggest the need for a new dataset encompassing multiple organs across the whole body with a large number of cases.</p>
    <p id="Par6">To address this need, we developed a new dataset consisting of 140 CT scans with six organ classes, which we call CT-ORG. We started from an existing dataset, the LiTS Challenge, which focuses on the liver, and significantly expanded it to encompass a wider range of organs and imaging protocols<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Our dataset contains volumetric labels for the liver, lungs, bladder, kidney, bones and brain. The data are divided into 119 training volumes and 21 testing volumes, which were annotated to a higher degree of accuracy for certain organs. The data exhibit a wide variety of imaging conditions collected from various medical centers, to ensure generalizability of the trained models. To our knowledge, this is the largest publicly available multi-organ dataset.</p>
    <p id="Par7">We developed several methods to annotate the data more efficiently. First, we segmented the lungs and bones using unsupervised morphological algorithms, which were accelerated using 3D Fourier transforms. This reduced annotation time to reasonable levels which would not be possible manually. We then manually refined and corrected the automatic labels to create a withheld test set of 21 volumes, which allows us to evaluate the accuracy of the morphological algorithms. The test set could serve as a useful benchmark for evaluating various organ segmentation methods, as it comprises a wide variety of organs, from the large and easily identifiable liver, to the small and discreet bladder.</p>
    <p id="Par8">To demonstrate the utility of our data, we trained a deep neural network to simultaneously segment all the labeled organs. We applied various data augmentations to improve model generalization. The model achieves high accuracy on the manually-annotated test set, processing a CT scan at 3 mm<sup>3</sup> resolution in only 4.3 s. Dice scores on the test set range from 78–96% per organ, with an average of 90% across all organs. Our experiments show that the dataset suffices to train a deep neural network, which outperforms the morphological algorithms from which it was trained.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <p id="Par9">This section describes the process of annotating the CT images with organ masks. Because computation was used to accelerate the process, we first describe the mathematical background, then proceed to the specific morphological algorithms used to segment the lungs and bones, and finally the manual annotation process used for all organs.</p>
    <sec id="Sec3">
      <title>Morphological segmentation</title>
      <p id="Par10">We used morphological algorithms to generate training data for the bones and lungs. This concept is called <italic>weak supervision</italic>, an active area of machine learning research. In the medical domain, weak supervision was previously exploited for brain ventricle segmentation<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>.</p>
      <p id="Par11">In what follows, we describe the basics of <italic>n</italic>-dimensional image morphology, and how we accelerated these operations using Fourier transforms. Then we describe the specific algorithms used to segment the lungs and bones.</p>
    </sec>
    <sec id="Sec4">
      <title>Morphology basics, acceleration by Fourier transforms</title>
      <p id="Par12">Let <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f:{{\mathbb{Z}}}^{n}\to {{\mathbb{F}}}^{2}$$\end{document}</tex-math><mml:math id="M2"><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq1.gif"/></alternatives></inline-formula> denote a binary image, and <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k:{{\mathbb{Z}}}^{n}\to {{\mathbb{F}}}^{2}$$\end{document}</tex-math><mml:math id="M4"><mml:mi>k</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq2.gif"/></alternatives></inline-formula> the structuring element. Then the familiar operation of morphological dilation can be written as<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D(f,k)(x)=\left\{\begin{array}{cc}0, &amp; (f\ast k)(x)=0\\ 1, &amp; {\rm{otherwise}}.\end{array}\right.$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>∗</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi mathvariant="normal"> otherwise</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41597_2020_715_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par13">That is, we first convolve <italic>f</italic> with <italic>k</italic>, treating the two as real-valued functions on <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathbb{Z}}}^{n}$$\end{document}</tex-math><mml:math id="M8"><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq3.gif"/></alternatives></inline-formula>. Then, we convert back to a binary image by setting zero-valued pixels to black, and all others to white. Erosion is computed similarly: let <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bar{f}$$\end{document}</tex-math><mml:math id="M10"><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq4.gif"/></alternatives></inline-formula> denote the binary complement of <italic>f</italic>; then erosion is just <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$E(f,k)(x)=\bar{D(\bar{f}\,,k)}$$\end{document}</tex-math><mml:math id="M12"><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mo>®</mml:mo></mml:mover><mml:mspace width="-.25em"/><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq5.gif"/></alternatives></inline-formula>. Similarly, the opening and closing operations are compositions of erosion and dilation. Note that in actual implementation, it is more numerically stable to take <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f\ast k &lt; \epsilon $$\end{document}</tex-math><mml:math id="M14"><mml:mi>f</mml:mi><mml:mo>∗</mml:mo><mml:mi>k</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>ϵ</mml:mi></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq6.gif"/></alternatives></inline-formula>, for some small <inline-formula id="IEq21"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\epsilon $$\end{document}</tex-math><mml:math id="M16"><mml:mi>ϵ</mml:mi></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq21.gif"/></alternatives></inline-formula>, as a proxy for <inline-formula id="IEq7"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f\ast k=0$$\end{document}</tex-math><mml:math id="M18"><mml:mi>f</mml:mi><mml:mo>∗</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq7.gif"/></alternatives></inline-formula>. Also, for finite image volumes, the convolution must be suitably cropped so that <inline-formula id="IEq8"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f\ast k$$\end{document}</tex-math><mml:math id="M20"><mml:mi>f</mml:mi><mml:mo>∗</mml:mo><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq8.gif"/></alternatives></inline-formula> has the same dimensions as <italic>f</italic>. This can be implemented by shifting the phase of the Fourier transforms and extrapolating the out-of-bounds values with 0 for <inline-formula id="IEq9"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widehat{f}$$\end{document}</tex-math><mml:math id="M22"><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq9.gif"/></alternatives></inline-formula> and 1 for <inline-formula id="IEq10"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widehat{\bar{f}}$$\end{document}</tex-math><mml:math id="M24"><mml:mover accent="true"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>¯</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq10.gif"/></alternatives></inline-formula>.</p>
      <p id="Par14">The advantage of writing dilation this way is that all of the basic operations in <italic>n</italic>-dimensional binary morphology reduce to a mixture of complements and convolutions. Complements are .inexpensive to compute, while convolutions can be accelerated by fast Fourier transforms, due to the identity <inline-formula id="IEq23"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widehat{f\ast k}=\widehat{f}\,\cdot \,\widehat{k}$$\end{document}</tex-math><mml:math id="M26"><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi><mml:mo>∗</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">^</mml:mo></mml:mrow></mml:mover><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover><mml:mspace width="-.25em"/><mml:mo>·</mml:mo><mml:mspace width="-.15em"/><mml:mover accent="true"><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq23.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq22"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widehat{f}$$\end{document}</tex-math><mml:math id="M28"><mml:mover accent="true"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq22.gif"/></alternatives></inline-formula> denotes the Fourier transform of <italic>f</italic>. This allows us to quickly generate training labels by morphological operations, which is especially beneficial when the structuring elements are large. In what follows, we describe how morphology is used to generate labels for two organs of interest, the skeleton and lungs. These were chosen because their size and intensity contrast enable detection by simple thresholding.</p>
    </sec>
    <sec id="Sec5">
      <title>Morphological detection and segmentation of CT lungs</title>
      <p id="Par15">The lungs were detected and segmented based on the simple observation that they are the two largest air pockets in the body. The morphological algorithm is as follows:<list list-type="order"><list-item><p id="Par16">Extract the air pockets from the CT scan by removing all voxels greater than <italic>τ</italic> = −150 Hounsfield units (HU). The resulting mask is called the thresheld image, denoted <italic>f</italic><sub><italic>τ</italic></sub>.</p></list-item><list-item><p id="Par17">Puncture the thin wall of the exam table by closing <italic>f</italic><sub><italic>τ</italic></sub> with a rectangular prism-shaped structuring element of width 1 × 10/<italic>d</italic> × 1 voxels, where <italic>d</italic> is the pixel spacing in mm. This connects the air inside the hollow exam table to the air outside the patient, assuming the usual patient orientation.</p></list-item><list-item><p id="Par18">Remove any mask component that is connected to the boundary of any axial slice. This removes air outside of the body, as well as the hollow interior of the exam table, while preserving the lungs.</p></list-item><list-item><p id="Par19">Remove the chest wall and other small air pockets by opening the image using a spherical structuring element with a diameter of 1 cm.</p></list-item><list-item><p id="Par20">From the remaining mask, take the two largest connected components, which are almost certainly the lungs.</p></list-item><list-item><p id="Par21">Finally, undo the effect of erosion by taking the components of <italic>f</italic><sub><italic>τ</italic></sub> which are connected to the two detected lungs.</p></list-item></list></p>
      <p id="Par22">Note that the final step is a form of morphological reconstruction, as elaborated in Vincent<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>.</p>
      <p id="Par23">This algorithm is fairly robust, on both abdominal as well as full-body CT exams showing the full width of the exam table. The only major drawback is that the lungs are not separated from the trachea, which may not necessarily be an issue. See the Experimental results section for a quantitative evaluation. An example output is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. A quick web search reveals that similar <ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/help/images/segment-lungs-from-3-d-chest-mri-data.html">algorithms</ext-link> have previously been used to segment the lungs, although not necessarily for the purpose of training a deep neural network.<fig id="Fig1"><label>Fig. 1</label><caption><p>Example of CT lung detection and segmentation by image morphology. Lung mask overlaid in blue. Rendered by 3D Slicer<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>.</p></caption><graphic xlink:href="41597_2020_715_Fig1_HTML" id="d30e899"/></fig></p>
    </sec>
    <sec id="Sec6">
      <title>Morphological detection and segmentation of CT bones</title>
      <p id="Par24">Bone segmentation proceeds similarly to lung segmentation, by a combination of thresholding, morphology and selection of the largest connected components. This time we define two intensity thresholds, <italic>τ</italic><sub>1</sub> = 0 and <italic>τ</italic><sub>2</sub> = 200 HU. These were selected so that almost all bone tissue is greater than <italic>τ</italic><sub>1</sub>, while the hard exterior of each bone is usually greater than <italic>τ</italic><sub>2</sub>. The algorithm is as follows:<list list-type="order"><list-item><p id="Par25">Threshold the image by <italic>τ</italic><sub>2</sub>. This extracts the hard exteriors of the bones, but also inevitably includes some unwanted tissues, such as the aorta, kidneys and intestines, especially in contrast-enhanced CTs.</p></list-item><list-item><p id="Par26">Select only the largest connected component from the thresheld image, which is the skeleton. This removes unwanted hyper-intense tissues which are typically not connected to the skeleton. It does have the drawback of excluding the ribs on abdominal images, in which they are disconnected from the rest of the skeleton. However, this drawback is acceptable for the purposes of generating training data.</p></list-item><list-item><p id="Par27">Close the mask using a spherical structuring element with a diameter of 2.5 cm. This fills gaps in the cortical bone, which may be too thin to be seen in digital CT images.</p></list-item><list-item><p id="Par28">Apply the threshold <italic>τ</italic><sub>1</sub> to remove most of this unwanted tissue between the bones, which might have been closed in the previous step.</p></list-item><list-item><p id="Par29">For each <italic>xy</italic>-plane (axial slice) in the image, fill any holes not connected to the boundary. This fills in the centers of large bones, such as the pelvis and femurs, assuming the usual patient orientation.</p></list-item></list></p>
      <p id="Par30">This simple algorithm is sufficiently accurate to train a deep neural network, and serves as a useful basis for future manual refinement. The accuracy is evaluated quantitatively in the [subsec:experiment_CT-organ-segmentation]Experimental results section. See Fig. <xref rid="Fig2" ref-type="fig">2</xref> for an example output, which omits some sections of the sacrum and pelvis that will need to be manually corrected in the testing set.<fig id="Fig2"><label>Fig. 2</label><caption><p>Example of CT skeleton detection and segmentation by image morphology. Skeleton mask overlaid in red. Rendered by 3D Slicer<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>.</p></caption><graphic xlink:href="41597_2020_715_Fig2_HTML" id="d30e987"/></fig></p>
    </sec>
    <sec id="Sec7">
      <title>Manual annotation</title>
      <p id="Par31">Morphological segmentation is sufficiently accurate to create training data, but manual correction is required for the more accurate testing set. Furthermore, segmentation based on thresholding is not suitable for soft tissues, which exhibit poor intensity contrast in CT. For these reasons, we annotated the remaining organs using the ITK-SNAP software<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. The annotation process consisted of manual initialization, followed by active contour segmentation with user-adjusted parameters, and finally a manual correction using the 3D paintbrush tool. The liver, kidneys, bladder and brain were manually annotated in both the testing and training set. The lungs were manually annotated in the test set only. The bones followed the same basic procedure as the lungs, but we saved time on the initialization stage by starting from the output of the morphological algorithm, followed by manual refinement. Refining the bone masks consisted mostly of including small, disconnected bones such as the ribs and scapulae, and removing the spinal cord.</p>
      <p id="Par32">All manual annotations were made or overseen by a graduate student with several years’ experience annotating CT images. Following the initial annotation, the 21 cases in the test set were reviewed by an experienced board-certified radiologist, after which 9 out of 21 cases were refined to a higher degree of accuracy. The radiologist noted that the annotations were of high quality, as subsequent corrections were minor.</p>
      <p id="Par33">In the majority of cases, probably around 130 out of the 140 total, liver annotations were taken from the existing LiTS Challenge dataset<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. We added our own liver annotations where the original dataset was missing them, as well as for new images not in LiTS. We included LiTS liver annotations in both the test and training set, as their methodology for manual annotation was very similar to ours.</p>
    </sec>
    <sec id="Sec8">
      <title>Human subjects</title>
      <p id="Par34">All imaging data was either publicly available, or collected from Stanford Healthcare. This study was approved by the relevant IRB.</p>
    </sec>
  </sec>
  <sec id="Sec9">
    <title>Data Records</title>
    <p id="Par35">All data are available on The Cancer Imaging Archive (TCIA) under the title <italic>CT-ORG: CT volumes with multiple organ segmentations</italic><sup><xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR13">13</xref></sup>.</p>
    <p id="Par36">The dataset consists of 140 CT scans, each with five organs labeled in 3D: lung, bones, liver, kidneys and bladder. The brain is also labeled on the minority of scans which show it. Of the 140 total image volumes, 131 are dedicated CTs, the remaining 9 are the CT component taken from PET-CT exams. The dedicated CT images are derive from several clinical sites including Ludwig Maxmilian University of Munich, Radboud University Medical Center of Nijmegen, Poly-technique &amp; CHUM Research Center Montreal, Tel Aviv University, Sheba Medical Center, IRCAD Institute Strasbourg and Hebrew University of Jerusalem<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. The PET-CT images all derive from Stanford Healthcare.</p>
    <p id="Par37">Each image was taken from a different patient. The gender distribution is 62.5% male and 37.5% female. While the ages of most patients could not be determined, they should be considered representative of typical liver cancer patients. Patients were included based on the presence of lesions in one or more of the labeled organs. Most of the images exhibit liver lesions, both benign and malignant. The number of distinct liver lesions per patient varies from 0 to 75, and the volume of lesions varies from 38 mm<sup>3</sup> to 349 mm<sup>3</sup>. Some also exhibit metastatic disease derived from cancer in the breast, colon, bones and lungs. The axial resolution of dedicated CTs varies from 0.56 mm to 1.0 mm. Images derive from pre- and post-treatment stages, with and without contrast enhancement. See Bilic <italic>et al</italic>. for more information<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>.</p>
    <p id="Par38">All files are stored in NIfTI-1 format with 32-bit floating point data.</p>
    <p id="Par39">Images are stored as “volume-<italic>x</italic>.nii.gz” where <italic>x</italic> is the one- to three-digit case number, ranging from 0–139. All images are CT scans, under a wide variety of imaging conditions including high-dose and low-dose, with and without contrast, abdominal, neck-to-pelvis and whole-body. Many patients exhibit cancer lesions, especially in the liver, but they were not selected according to any specific disease criteria. Numeric values are in Hounsfield units.</p>
    <p id="Par40">The first 21 volumes (case numbers 0–20) constitute the testing split. The remaining volumes constitute the training split. Training masks suffice for training a deep neural network, but should not be considered reliable for evaluation.</p>
    <p id="Par41">Segmentations are stored as “labels-<italic>x</italic>.nii.gz”, where <italic>x</italic> is the same number as the corresponding volume file. Organs are encoded as 32-bit floating point numbers according to Table <xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Organ labeling scheme.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Organ class</th><th>Label</th></tr></thead><tbody><tr><td>Background</td><td>0</td></tr><tr><td>Liver</td><td>1</td></tr><tr><td>Bladder</td><td>2</td></tr><tr><td>Lungs</td><td>3</td></tr><tr><td>Kidneys</td><td>4</td></tr><tr><td>Bone</td><td>5</td></tr><tr><td>Brain</td><td>6</td></tr></tbody></table></table-wrap></p>
  </sec>
  <sec id="Sec10">
    <title>Technical Validation</title>
    <p id="Par42">To confirm the utility of the dataset, we trained an FCN to segment the labeled organs, and evaluated the model performance on our test set. This model was chosen to represent a broad class of deep learning methods which are the current state of the art in organ segmentation<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR14">14</xref>–<xref ref-type="bibr" rid="CR17">17</xref></sup>. Our experiments confirm that the training data are sufficiently numerous for the model to generalize to unseen examples, and that the morphological segmentations of bones and lungs are sufficiently realistic to yield acceptable performance when compared to manual organ annotations. We also directly evaluate the similarity between morphological segmentation and the ground-truth manual annotations.</p>
    <sec id="Sec11">
      <title>Neural network architecture</title>
      <p id="Par43">This section describes the design and training of our predictive model; both the inputs and outputs, pre- and post-processing, the neural network itself, and the training loss function.</p>
    </sec>
    <sec id="Sec12">
      <title>Pre- and post-processing</title>
      <p id="Par44">Our neural network takes as input a 120 × 120 × 160 image volume, and outputs a 120 × 120 × 160 × 6 probability map, where each voxel is assigned a class probability distribution. As usual, we take the the argmax probability for each voxel to convert the six probabilities to a single class label. Some works have addressed the issue of limited viewing resolution by training multiple FCNs, each working at different scales<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. However, we found that a single resolution achieves reasonable performance. To reduce memory requirements, we resample all image volumes to 3 mm<sup>3</sup>. Resampling uses Gaussian smoothing as a lowpass filter to avoid aliasing artifacts, followed by interpolation at the new resolution. Since each CT scan has its own millimeter resolution for each dimension <italic>u</italic> = (<italic>u</italic><sub>1</sub>, <italic>u</italic><sub>2</sub>, <italic>u</italic><sub>3</sub>), we adjust the Gaussian smoothing kernel according to the formula <inline-formula id="IEq13"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g(x)\propto {\rm{\exp }}(-{\sum }_{k=1}^{3}{x}_{k}^{2}/{\sigma }_{k}^{2})$$\end{document}</tex-math><mml:math id="M30"><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mi mathvariant="normal">exp</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq13.gif"/></alternatives></inline-formula> where the smoothing factors are computed from the desired resolution <italic>r</italic> = 3 according to <inline-formula id="IEq14"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\sigma }_{k}=\frac{1}{3}{\rm{\max }}(r/{u}_{k}-1,0).$$\end{document}</tex-math><mml:math id="M32"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac><mml:mi mathvariant="normal">max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq14.gif"/></alternatives></inline-formula> This heuristic formula is based on the fact from digital signal processing that, in order to avoid aliasing, the cutoff frequency should be placed at <italic>r</italic>/<italic>u</italic><sub><italic>k</italic></sub>, the ratio of sampling rates, on a [0,1] frequency scale.</p>
      <p id="Par45">After processing the resampled input with the neural network, we resample the 120 × 120 × 160 prediction map back to the original image resolution by nearest neighbor interpolation. One difficulty with this scheme is that CT scans vary in resolution and number of slices, and at 3 mm<sup>3</sup> we are unlikely to fit the whole scan in our network. For training, we address this by selecting a 120 × 120 × 160 sub-region from the scan uniformly at random. For inference, we cover the scan by partially-overlapping sub-regions, averaging predictions where overlap occurs. We distribute the tiles across all available GPUs to process them in parallel.</p>
    </sec>
    <sec id="Sec13">
      <title>Model</title>
      <p id="Par46">We chose a relatively simple model which balances speed and memory consumption with accuracy. Our model is essentially a three dimensional U-Net, which is to say an FCN divided into <italic>chunks</italic> which are separated by either pooling or upsampling, where chunk <italic>k</italic> has a skip connection to chunk <inline-formula id="IEq15"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N-k+1$$\end{document}</tex-math><mml:math id="M34"><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq15.gif"/></alternatives></inline-formula>, i.e. the first chunk is connected to the last, the second to the second-from-last, etc. This is probably the most popular model category for semantic segmentation, which we defer to the literature for explanation<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR17">17</xref></sup>. For a more advanced variant, see the DeepLab family of models which utilize separable convolutions at various scales<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>.</p>
      <p id="Par47">Each of our U-Net chunks contains two convolution layers and either a max-pooling or “transposed convolution” (also called deconvolution) upsampling layer. Decimation chunks are <italic>followed</italic> by pooling, while interpolation chunks are <italic>preceded</italic> by upsampling. All of our convolution kernels have 3 × 3 × 3 dimensions, while our max-pooling layers use 2 × 2 × 2 windows. All convolutions are followed by constant “bias” addition, batch normalization and rectification<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. All layers have a stride of one voxel. The only significant departure for volumetric imaging is the considerable memory requirement. To save memory, we use fewer feature maps per layer. The parameters of each chunk are enumerated in Table <xref rid="Tab2" ref-type="table">2</xref>. The final chunk is followed by a convolution layer outputting six feature maps, one per object class, which are then passed into a softmax layer yielding the output probabilities.<table-wrap id="Tab2"><label>Table 2</label><caption><p>List of chunks in our fully-convolutional neural network.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Chunk Type</th><th>Output channels</th></tr></thead><tbody><tr><td rowspan="4">Decimation</td><td>32</td></tr><tr><td>64</td></tr><tr><td>64</td></tr><tr><td>128</td></tr><tr><td rowspan="3">Interpolation</td><td>64</td></tr><tr><td>64</td></tr><tr><td>32</td></tr></tbody></table><table-wrap-foot><p>Each chunk consists of three layers.</p></table-wrap-foot></table-wrap></p>
      <p id="Par48">Counting either convolution or pooling as one layer, our model contains 22 layers in total, three per chunk and one for the final convolution. The number and width of the layers was chosen heuristically to fit in the 12 GB of memory available to each of our graphics cards. No special techniques were used to reduce memory consumption, such as checkpointing or reducing the arithmetic precision<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. We explored checkpointing, but abandoned it as the memory savings were equal to the increase in computation time.</p>
    </sec>
    <sec id="Sec14">
      <title>Training loss</title>
      <p id="Par49">Most tasks in medical image segmentation exhibit extreme class imbalance, where the vast majority of voxels are members of the “background” class. Furthermore, in multi-organ segmentation we must differentiate between objects of vastly different sizes, ranging from the bladder on one extreme to the lungs on the other. To address this class imbalance, we employed the IOU loss, a variant of the popular Dice loss proposed by Milletari <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. The IOU loss attempts to minimize the intersection over union (IOU) of the output segmentation, also called the Jaccard index. Since the IOU depends on binary values, it cannot be optimized by gradient descent, and thus it is not directly suitable as a loss function for deep learning. Instead, our strategy is to define a smooth function which is equal to the IOU when the output probabilities are all either 0 or 1. Since <inline-formula id="IEq16"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\{0,1\}}^{n}\subset {[0,1]}^{n}$$\end{document}</tex-math><mml:math id="M36"><mml:msup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>⊂</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq16.gif"/></alternatives></inline-formula>, we consider the training labels <italic>y</italic> as a vector in [0,1]<sup><italic>n</italic></sup> consisting only of probabilities 0 and 1. Then, we define the smooth IOU loss as<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{L}}=1-\frac{p\cdot y}{{\left\Vert p\right\Vert }_{1}+{\left\Vert y\right\Vert }_{1}-p\cdot y}$$\end{document}</tex-math><mml:math id="M38" display="block"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>⋅</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mfenced close="∥" open="∥"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced close="∥" open="∥"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo>⋅</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41597_2020_715_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq17"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\left\Vert p\right\Vert }_{1}={\sum }_{k}pk$$\end{document}</tex-math><mml:math id="M40"><mml:msub><mml:mrow><mml:mfenced close="∥" open="∥"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq17.gif"/></alternatives></inline-formula>, since <inline-formula id="IEq18"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p}_{k}\ge 0$$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq18.gif"/></alternatives></inline-formula> for all <italic>k</italic>.</p>
      <p id="Par50">The binary version of the IOU loss function was previously proposed by Rahman and Wang<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. To extend this to multi-class segmentation, we simply average <inline-formula id="IEq19"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{L}}$$\end{document}</tex-math><mml:math id="M44"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq19.gif"/></alternatives></inline-formula> for each class, including the background. This approach seems simpler and more naturally motivated than the multi-class Dice scheme of Sudre <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>.</p>
    </sec>
  </sec>
  <sec id="Sec15">
    <title>Experimental Results</title>
    <p id="Par51">In this section we describe our experiments validating the suitability of our data for deep learning tasks. We trained the previously-described neural network both with and without data augmentation, evaluating its performance against unseen manual annotations. We also evaluated the accuracy of the morphological segmentation algorithms.</p>
    <p id="Par52">As is typically the case in deep learning, we used data augmentation to expand our dataset beyond what was manually annotated. We randomly generated additional training data from the 119 original volumes, by applying known transformations to both the image and the training labels. This typically reduces the generalization error of the model, and promotes robustness to expected geometric and photometric transformations, especially when applied to smaller datasets. We applied a variety of transformations including affine warping, intensity windowing, additive noise and occlusion. These operations were accelerated by the GPU with our publicly-available CUDA library, as described in the Code availability section.</p>
    <p id="Par53">We implemented our model in TensorFlow according to the design decisions in the <xref rid="Sec13" ref-type="sec">Model</xref> section<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. We used fairly standard choices for hyperparameters and training procedure. Our loss function was optimized by the RMSProp algorithm, along with an added <inline-formula id="IEq20"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\ell }_{2}$$\end{document}</tex-math><mml:math id="M46"><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41597_2020_715_Article_IEq20.gif"/></alternatives></inline-formula> regularization term of weight <italic>λ</italic> = 10<sup>−7</sup>. The learning rate began at <italic>α</italic> = 10<sup>−3</sup>, with no data augmentation. During each training iteration we selected 3 separate cases uniformly with replacement, extracting a 120 × 120 × 160 volume from each to form the training batch. We used three GPUs for training and one for data augmentation, with a single volume assigned to each GPU. After 15666 training iterations we decreased <italic>α</italic> to 10<sup>−4</sup> and enabled data augmentation. Then we trained on randomly-generated volumes for an additional 32582 iterations. These parameters were selected heuristically to maximize performance, as training took more than one day on our server, prohibiting exhaustive search.</p>
    <p id="Par54">To robustly estimate the performance on unseen data, we averaged the Dice scores across all classes for each case, then selected the median score across all checkpoints from the pre- and post-data augmentation phases, possibly removing the first score to ensure the median is achieved. Finally, we took the per-class scores corresponding to the earliest median example for each phase. We then repeated this procedure for the mean symmetric surface distance and Hausdorff distance, possibly choosing a different representative iteration for each metric. We excluded the brain class from evaluation, as it is present in only 8 out of 119 training volumes, so the model had trouble learning to distinguish it when trained with uniform sampling. Inference took an average of 4.3 s per CT scan using all four GPUs. The results are shown in Tables <xref rid="Tab3" ref-type="table">3</xref>–<xref rid="Tab5" ref-type="table">5</xref>. See Fig. <xref rid="Fig3" ref-type="fig">3</xref> for an example prediciton on an unseen test case.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Dice scores, mean (± standard deviation) per case over the test set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">MethodData augmentation</th><th colspan="2">Neural network</th><th rowspan="2">Morphologyn/a</th></tr><tr><th>No</th><th>Yes</th></tr></thead><tbody><tr><td>Lung</td><td>93.8 ± 5.9</td><td><bold>95.5</bold> ± 4.5</td><td>93.6 ± 12.5</td></tr><tr><td>Liver</td><td>92.0 ± 3.6</td><td><bold>95.2</bold> ± 2.5</td><td>n/a</td></tr><tr><td>Bone</td><td>82.7 ± 7.6</td><td>85.8 ± 6.2</td><td><bold>86.0</bold> ± 6.1</td></tr><tr><td>Kidney</td><td>88.2 ± 7.9</td><td><bold>91.8</bold> ± 4.0</td><td>n/a</td></tr><tr><td>Bladder</td><td>58.1 ± 22.3</td><td><bold>77.7</bold> ± 17.8</td><td>n/a</td></tr></tbody></table><table-wrap-foot><p>Median performance from training checkpoints taken every 50 iterations.</p></table-wrap-foot></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>Mean symmetric surface distance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">MethodData augmentation</th><th colspan="2">Neural network</th><th rowspan="2">Morphologyn/a</th></tr><tr><th>No</th><th>Yes</th></tr></thead><tbody><tr><td>Lung</td><td>1.93 ± 3.16</td><td><bold>1.63</bold> ± 2.85</td><td>4.66 ± 2.58</td></tr><tr><td>Liver</td><td>1.21 ± 1.55</td><td><bold>1.09</bold> ± 1.19</td><td>n/a</td></tr><tr><td>Bone</td><td>0.95 ± 0.42</td><td><bold>0.92</bold> ± 0.44</td><td>4.55 ± 8.53</td></tr><tr><td>Kidney</td><td>1.36 ± 0.98</td><td><bold>0.51</bold> ± 0.21</td><td>n/a</td></tr><tr><td>Bladder</td><td><bold>5.20</bold> ± 12.4</td><td>6.25 ± 14.0</td><td>n/a</td></tr></tbody></table><table-wrap-foot><p>Compare to Table <xref rid="Tab3" ref-type="table">3</xref>.</p></table-wrap-foot></table-wrap><table-wrap id="Tab5"><label>Table 5</label><caption><p>Hausdorff distance. Compare to Table <xref rid="Tab3" ref-type="table">3</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">MethodData augmentation</th><th colspan="2">Neural network</th><th rowspan="2">Morphologyn/a</th></tr><tr><th>No</th><th>Yes</th></tr></thead><tbody><tr><td>Lung</td><td>59.8 ± 75.2</td><td><bold>35.2</bold> ± 31.5</td><td>57.9 ± 42.9</td></tr><tr><td>Liver</td><td>40.5 ± 23.1</td><td><bold>32.8</bold> ± 43.0</td><td>n/a</td></tr><tr><td>Bone</td><td>29.9 ± 9.55</td><td><bold>29.8</bold> ± 12.1</td><td>153.9 ± 67.8</td></tr><tr><td>Kidney</td><td>28.0 ± 27.2</td><td><bold>17.3</bold> ± 20.5</td><td>n/a</td></tr><tr><td>Bladder</td><td><bold>16.5</bold> ± 28.3</td><td>29.9 ± 47.5</td><td>n/a</td></tr></tbody></table></table-wrap><fig id="Fig3"><label>Fig. 3</label><caption><p>Example neural network prediction on the unseen test set.</p></caption><graphic xlink:href="41597_2020_715_Fig3_HTML" id="d30e2039"/></fig></p>
    <sec id="Sec16">
      <title>Discussion of experimental results</title>
      <p id="Par55">These results show that our data are sufficient to train a multi-organ segmentation model to a level of accuracy which could be clinically useful. They also show that our methodology of generating training data with unsupervised algorithms is sufficient to train a deep neural network for certain organs. Finally, they show that the unsupervised algorithms reasonably agree with manual annotations from the test set. Indeed, the last of column of Table <xref rid="Tab3" ref-type="table">3</xref> suggests that the morphological bone and lung segmentations agreed closely with the manual corrections.</p>
      <p id="Par56">While 140 cases is fewer than typical for deep learning, our model still avoids overfitting because organs are segmented on a per-voxel basis. Compared to classification models which output a single label per image, segmentation models typically succeed with fewer data. This is especially true when using data augmentation, which changes the labels as well as the image content.</p>
      <p id="Par57">In our experiments, data augmentation improved the Dice score for all organs. It also improved the surface distances, with the exception of the bladder. This suggests that the training dataset of 119 cases is not yet large enough for peak performance, and that some overfitting occurs when the data are not augmented. Future improvements could be had by expanding our dataset, or combining it with existing ones. The main limitation in combining datasets is that we can only reliably predict those organs labeled in all data sources.</p>
      <p id="Par58">Interestingly, the neural network outperformed the morphological lung segmenter by all metrics, and outperformed the bone segmenter in all except for the Dice score, in which it was nearly equal. This closely matches the results of Ghafoorian <italic>et al</italic>., in which a neural network surpasses the unsupervised algorithm from which it is trained<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. For organ segmentation, the main disadvantage of morphology is that it is prone to catastrophic failure in surprising ways. For example, the morphological lung segmenter considers both lungs as belonging to the same object if they are sufficiently close together, instead detecting some abdominal air pocket as the remaining lung. Similarly, the bone segmenter fails to detect the ribs if the scan is cut off before they meet the spine, and sometimes fails to connect bones across large joints such as the shoulder. These issues are likely correctable with more complex hand-engineered rules, or on a case-by-case basis with manual adjustment of parameters. In contrast, a neural network captures the visual appearance of each organ, which is often more reliable than the shape or number of objects. When segmenting a large variety of organs, a single network has obvious conceptual and computational advantages over a litany of brittle, organ-specific solutions.</p>
    </sec>
  </sec>
  <sec id="Sec17">
    <title>Usage Notes</title>
    <p id="Par59">The data are suitable for visualization in a variety of software, including 3D Slicer and ITK-SNAP<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup>. For best results, users should take into account the resolution of each volume, which is stored in the NIfTI file headers, resampling the data accordingly. At higher resolutions, the images will likely need to be cropped and processed in tiles, as described in the <xref rid="Sec12" ref-type="sec">Pre- and post-processing</xref> section. We found that model performance was consistently improved by training data augmentation, as elaborated in the Data transformations section. See the Code availability section for our data augmentation code and pre-trained model.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>This work was supported in part by grants from the National Cancer Institute, National Institutes of Health, 1U01CA190214 and 1U01CA187947. The authors would like to thank Hersh Sagrieya for his help in reading difficult CT exams.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>Blaine Rister annotated the majority of the data, developed the unsupervised morphological algorithms, wrote the software for preprocessing and data augmentation, and refined the deep learning model. Darvin Yi wrote the original software for the deep learning model, and helped to repurpose it for organ segmentation. Kaushik Shivakumar assisted with the data annotation. Tomomi Nobashi provided medical guidance to the annotators, and also curated the PET-CT image cohort from Stanford Healthcare. Daniel L. Rubin served as the guarantor of the work, providing scientific input and medical advice to the investigators. He also reviewed the annotations to ensure their accuracy.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>All code is available on Github for our <ext-link ext-link-type="uri" xlink:href="https://github.com/bbrister/ctOrganSegmentation">morphological segmentation</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://github.com/bbrister/cudaImageWarp">GPU data augmentation</ext-link>, and pre-trained <ext-link ext-link-type="uri" xlink:href="https://github.com/bbrister/ct_organ_seg_docker">model</ext-link>. The pre-trained model is packaged in a <ext-link ext-link-type="uri" xlink:href="https://github.com/bbrister/ct_organ_seg_docker">Docker container</ext-link> for ease of use. (Please see <ext-link ext-link-type="uri" xlink:href="https://github.com/bbrister/ctOrganSegmentation">https://github.com/bbrister/ctOrganSegmentation</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://github.com/bbrister/cudaImageWarp">https://github.com/bbrister/cudaImageWarp</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://github.com/bbrister/ctorgansegdocker">https://github.com/bbrister/ctorgansegdocker</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://hub.docker.com/repository/docker/bbrister/organseg">https://hub.docker.com/repository/docker/bbrister/organseg</ext-link>.)</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par60">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shelhamer</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Long</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Darrell</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Fully convolutional networks for se-mantic segmentation</article-title>
        <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
        <year>2017</year>
        <volume>39</volume>
        <fpage>640</fpage>
        <lpage>651</lpage>
        <pub-id pub-id-type="pmid">27244717</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <mixed-citation publication-type="other">Bilic, P. <italic>et al</italic>. The liver tumor segmentation benchmark (LiTS). Preprint at <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1901.04056">http://arxiv.org/abs/1901.04056</ext-link> (2019).</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Heimann</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Comparison and evaluation of methods for liver seg-mentation from ct datasets</article-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <year>2009</year>
        <volume>28</volume>
        <fpage>1251</fpage>
        <lpage>1265</lpage>
        <?supplied-pmid 19211338?>
        <pub-id pub-id-type="pmid">19211338</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="data">
        <name>
          <surname>Roth</surname>
          <given-names>HR</given-names>
        </name>
        <etal/>
        <year>2016</year>
        <data-title>Data from Pancreas-CT</data-title>
        <source>The Cancer Imaging Archive</source>
        <pub-id pub-id-type="doi">10.7937/K9/TCIA.2016.tNB1kqBU</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Simpson, A. L. et al. A large annotated medical image dataset for the development and evaluation of segmentation algorithms. Preprint at <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1902.09063">http://arxiv.org/abs/1902.09063</ext-link> (2019).</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="data">
        <name>
          <surname>Gibson</surname>
          <given-names>E</given-names>
        </name>
        <etal/>
        <year>2018</year>
        <data-title>Multi-organ Abdominal CT Reference Standard Segmen-tations</data-title>
        <source>Zenodo</source>
        <pub-id pub-id-type="doi">10.5281/zenodo.1169361</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jimenez-del-Toro</surname>
            <given-names>O</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Cloud-based evaluation of anatomical structure segmentation and landmark detection algorithms: Visceral anatomy bench-marks</article-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <year>2016</year>
        <volume>35</volume>
        <fpage>2459</fpage>
        <lpage>2475</lpage>
        <?supplied-pmid 27305669?>
        <pub-id pub-id-type="pmid">27305669</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Evaluation of six registration methods for the human abdomen on clinically acquired CT</article-title>
        <source>IEEE Transactions on Biomedical Engineering</source>
        <year>2016</year>
        <volume>63</volume>
        <fpage>1563</fpage>
        <lpage>1572</lpage>
        <?supplied-pmid 27254856?>
        <pub-id pub-id-type="pmid">27254856</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ghafoorian</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Student beats the teacher: deep neural networks for lateral ventricles segmentation in brain MR</article-title>
        <source>In Proc. SPIE</source>
        <year>2018</year>
        <volume>10574</volume>
        <fpage>99</fpage>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vincent</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Morphological grayscale reconstruction in image analysis: Ap-plications and efficient algorithms</article-title>
        <source>IEEE Transactions on Image Processing</source>
        <year>1993</year>
        <volume>2</volume>
        <fpage>176</fpage>
        <lpage>201</lpage>
        <pub-id pub-id-type="pmid">18296207</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yushkevich</surname>
            <given-names>PA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>User-guided 3D active contour segmentation of anatomical structures: Significantly improved efficiency and reliability</article-title>
        <source>Neuroimage</source>
        <year>2006</year>
        <volume>31</volume>
        <fpage>1116</fpage>
        <lpage>1128</lpage>
        <?supplied-pmid 16545965?>
        <pub-id pub-id-type="pmid">16545965</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="data">
        <name>
          <surname>Rister</surname>
          <given-names>B</given-names>
        </name>
        <name>
          <surname>Shivakumar</surname>
          <given-names>K</given-names>
        </name>
        <name>
          <surname>Nobashi</surname>
          <given-names>T</given-names>
        </name>
        <name>
          <surname>Rubin</surname>
          <given-names>DL</given-names>
        </name>
        <year>2019</year>
        <data-title>CT-ORG: CT volumes with multiple organ segmentations</data-title>
        <source>The Cancer Imaging Archive</source>
        <pub-id pub-id-type="doi">10.7937/tcia.2019.tt7f4v7o</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Clark</surname>
            <given-names>K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The cancer imaging archive (TCIA): Maintaining and operating a public information repository</article-title>
        <source>Journal of Digital Imaging</source>
        <year>2013</year>
        <volume>26</volume>
        <fpage>1045</fpage>
        <lpage>1057</lpage>
        <?supplied-pmid 23884657?>
        <pub-id pub-id-type="pmid">23884657</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Milletari, F., Navab, N. &amp; Ahmadi, S. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In <italic>Proc. 3D Vision (3DV)</italic>, 565–571 (2016).</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Roth</surname>
            <given-names>HR</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>An application of cascaded 3D fully convolutional net-works for medical image segmentation</article-title>
        <source>Computerized Medical Imaging and Graphics</source>
        <year>2018</year>
        <volume>66</volume>
        <fpage>90</fpage>
        <lpage>99</lpage>
        <?supplied-pmid 29573583?>
        <pub-id pub-id-type="pmid">29573583</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>H-DenseUNet: Hybrid densely connected UNet for liver and tumor segmentation from CT volumes</article-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <year>2018</year>
        <volume>37</volume>
        <fpage>2663</fpage>
        <lpage>2674</lpage>
        <?supplied-pmid 29994201?>
        <pub-id pub-id-type="pmid">29994201</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gibson</surname>
            <given-names>E</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automatic multi-organ segmentation on abdominal CT with dense V-networks</article-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <year>2018</year>
        <volume>37</volume>
        <fpage>1822</fpage>
        <lpage>1834</lpage>
        <?supplied-pmid 29994628?>
        <pub-id pub-id-type="pmid">29994628</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F. &amp; Adam, H. Encoder-decoder with atrous separable convolution for semantic image segmenta-tion. In <italic>Proc. European Conference on Computer Vision (ECCV)</italic> (2018).</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Ioffe, S. &amp; Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Bach, F. &amp; Blei, D. (eds.) <italic>Proc. International Conference on Machine Learning (ICML)</italic>, vol. 37, 448–456 (PMLR, 2015).</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other"> Sohoni, N. S., Aberger, C. R., Leszczynski, M., Zhang, J. &amp; Ré, C. Low-memory neural network training: A technical report. Preprint at <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1904.10631">http://arxiv.org/abs/1904.10631</ext-link> (2019).</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Rahman, M. A. &amp; Wang, Y. Optimizing intersection-over-union in deep neural networks for image segmentation. In Bebis, G. <italic>et al</italic>. (eds.) <italic>Advances in Visual Computing</italic>, 234–244 (Springer International Publishing, 2016).</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Sudre, C. H., Li, W., Vercauteren, T., Ourselin, S. &amp; Jorge Cardoso, M. Generalised Dice overlap as a deep learning loss function for highly unbal-anced segmentations. In Cardoso, J. <italic>et al</italic>. (eds.) <italic>Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</italic>, 240–248 (Springer International Publishing, 2017).</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Abadi, M. <italic>et al</italic>. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1603.04467">https://arxiv.org/abs/1603.04467</ext-link> (2016).</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Kikinis, R., Pieper, S. D. &amp; Vosburgh, K. G. 3D Slicer: A platform for subject-specific image analysis, visualization, and clinical support. In Jolesz, F. (ed.) <italic>Intraoperative Imaging and Image-Guided Therapy</italic>, 277–289 (Springer New York, 2014).</mixed-citation>
    </ref>
  </ref-list>
</back>
