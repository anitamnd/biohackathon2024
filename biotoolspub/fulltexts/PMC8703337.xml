<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-3.dtd?>
<?SourceDTD.Version 1.3?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id>
    <journal-id journal-id-type="publisher-id">sensors</journal-id>
    <journal-title-group>
      <journal-title>Sensors (Basel, Switzerland)</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1424-8220</issn>
    <publisher>
      <publisher-name>MDPI</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8703337</article-id>
    <article-id pub-id-type="doi">10.3390/s21248184</article-id>
    <article-id pub-id-type="publisher-id">sensors-21-08184</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>HyperSeed: An End-to-End Method to Process Hyperspectral Images of Seeds</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4075-4125</contrib-id>
        <name>
          <surname>Gao</surname>
          <given-names>Tian</given-names>
        </name>
        <xref rid="af1-sensors-21-08184" ref-type="aff">1</xref>
        <xref rid="fn1-sensors-21-08184" ref-type="author-notes">†</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chandran</surname>
          <given-names>Anil Kumar Nalini</given-names>
        </name>
        <xref rid="af2-sensors-21-08184" ref-type="aff">2</xref>
        <xref rid="fn1-sensors-21-08184" ref-type="author-notes">†</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8220-8021</contrib-id>
        <name>
          <surname>Paul</surname>
          <given-names>Puneet</given-names>
        </name>
        <xref rid="af2-sensors-21-08184" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9712-5824</contrib-id>
        <name>
          <surname>Walia</surname>
          <given-names>Harkamal</given-names>
        </name>
        <xref rid="af2-sensors-21-08184" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0596-8227</contrib-id>
        <name>
          <surname>Yu</surname>
          <given-names>Hongfeng</given-names>
        </name>
        <xref rid="af1-sensors-21-08184" ref-type="aff">1</xref>
        <xref rid="c1-sensors-21-08184" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Graña</surname>
          <given-names>Manuel</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <aff id="af1-sensors-21-08184"><label>1</label>School of Computing, University of Nebraska-Lincoln, Lincoln, NE 68588, USA; <email>tgao@huskers.unl.edu</email></aff>
    <aff id="af2-sensors-21-08184"><label>2</label>Department of Agronomy and Horticulture, University of Nebraska-Lincoln, Lincoln, NE 68583, USA; <email>analinichandran2@unl.edu</email> (A.K.N.C.); <email>puneetpaul@unl.edu</email> (P.P.); <email>hwalia2@unl.edu</email> (H.W.)</aff>
    <author-notes>
      <corresp id="c1-sensors-21-08184"><label>*</label>Correspondence: <email>hfyu@unl.edu</email>; Tel.: +1-402-472-5013</corresp>
      <fn id="fn1-sensors-21-08184">
        <label>†</label>
        <p>These authors contributed equally to this work.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>08</day>
      <month>12</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>12</month>
      <year>2021</year>
    </pub-date>
    <volume>21</volume>
    <issue>24</issue>
    <elocation-id>8184</elocation-id>
    <history>
      <date date-type="received">
        <day>29</day>
        <month>10</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>04</day>
        <month>12</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 by the authors.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p>High-throughput, nondestructive, and precise measurement of seeds is critical for the evaluation of seed quality and the improvement of agricultural productions. To this end, we have developed a novel end-to-end platform named <italic toggle="yes">HyperSeed</italic> to provide hyperspectral information for seeds. As a test case, the hyperspectral images of rice seeds are obtained from a high-performance line-scan image spectrograph covering the spectral range from 600 to 1700 nm. The acquired images are processed via a graphical user interface (GUI)-based open-source software for background removal and seed segmentation. The output is generated in the form of a hyperspectral cube and curve for each seed. In our experiment, we presented the visual results of seed segmentation on different seed species. Moreover, we conducted a classification of seeds raised in heat stress and control environments using both traditional machine learning models and neural network models. The results show that the proposed 3D convolutional neural network (3D CNN) model has the highest accuracy, which is 97.5% in seed-based classification and 94.21% in pixel-based classification, compared to 80.0% in seed-based classification and 85.67% in seed-based classification from the support vector machine (SVM) model. Moreover, our pipeline enables systematic analysis of spectral curves and identification of wavelengths of biological interest.</p>
    </abstract>
    <kwd-group>
      <kwd>hyperspectral imaging system</kwd>
      <kwd>high-throughput seed phenotyping</kwd>
      <kwd>phenotyping software</kwd>
      <kwd>seed heat stress</kwd>
      <kwd>3D convolutional neural network (CNN)</kwd>
      <kwd>support vector machine (SVM)</kwd>
      <kwd>light gradient boosting machine (LightGBM)</kwd>
      <kwd>hyperspectral analysis</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1-sensors-21-08184">
    <title>1. Introduction</title>
    <p>Seeds are essential for the modern agricultural industry since they are not only an important source for food supply but are also closely related to crop yield [<xref rid="B1-sensors-21-08184" ref-type="bibr">1</xref>]. To obtain a precise quantitative evaluation of seed quality, plant scientists have studied various phenotyping methods. Traditionally, the seeds traits are measured manually. However, the manual measurement often involves error-prone, laborious, and time-consuming procedures for tackling massive number of seeds. Thus, there is a growing need for high-throughput phenotyping methods to generate a more precise quantitative measurement of seeds.</p>
    <p>With the rapid development of sensors and computer vision technology, various methods have been presented for phenotyping. Based on 2D images captured by regular red–green–blue (RGB) cameras, researchers proposed various imaging systems [<xref rid="B2-sensors-21-08184" ref-type="bibr">2</xref>,<xref rid="B3-sensors-21-08184" ref-type="bibr">3</xref>,<xref rid="B4-sensors-21-08184" ref-type="bibr">4</xref>,<xref rid="B5-sensors-21-08184" ref-type="bibr">5</xref>]. Computer vision algorithms were also explored to obtain the traits of seeds. For example, Tanabata et al. proposed a software <italic toggle="yes">SmartGrain</italic> to obtain seed size and shape [<xref rid="B6-sensors-21-08184" ref-type="bibr">6</xref>]. Zhu et al. developed an open-source software <italic toggle="yes">SeedExtractor</italic> for seed phenotyping using seed shape and color [<xref rid="B7-sensors-21-08184" ref-type="bibr">7</xref>]. Besides RGB camera imaging, X-ray-based imaging has also proved a great potential for seed phenotyping. X-ray-based methods have been presented to obtain the structure of the seeds and to predict the germination capacity [<xref rid="B8-sensors-21-08184" ref-type="bibr">8</xref>,<xref rid="B9-sensors-21-08184" ref-type="bibr">9</xref>]. However, these methods have some limitations. For example, the X-ray assessment is usually time-consuming, potentially harmful, and not scalable for large-scale operations [<xref rid="B10-sensors-21-08184" ref-type="bibr">10</xref>]. The methods based on RGB cameras can only capture the surface characteristics. Moreover, since RGB cameras only acquire three wavelengths (red, green, and blue), the spectral information on other wavelengths that potentially indicate the chemical composition traits are not measured [<xref rid="B11-sensors-21-08184" ref-type="bibr">11</xref>].</p>
    <p>To overcome some of these issues, plant scientists are exploring the hyperspectral imaging (HSI) technique that captures both spatial and spectral information. With a hyperspectral sensor, a wide range of wavelengths, including the ultraviolet (UV), visible (VIS), and near-infrared (NIR) spectra, can be obtained [<xref rid="B12-sensors-21-08184" ref-type="bibr">12</xref>]. Due to its easy-to-operate advantages and scalability, HSI is becoming a popular tool to explore seed traits. For example, Wu et al. introduced an imaging system to determine seed viability by capturing the hyperspectral images from two sides of wheat seeds [<xref rid="B13-sensors-21-08184" ref-type="bibr">13</xref>]. Hyperspectral imaging has also been applied to identify specific rice cultivars using deep learning techniques [<xref rid="B14-sensors-21-08184" ref-type="bibr">14</xref>]. NIR hyperspectral images have been used to quantify the seed starch content [<xref rid="B15-sensors-21-08184" ref-type="bibr">15</xref>].</p>
    <p>Although various solutions have been developed for hyperspectral analysis of seeds, there are still unsolved problems. First, the existing software tools to process hyperspectral images are usually designed for general purposes rather than seed phenotyping. They are not directly suitable for tackling high-throughput phenotyping of seeds since they do not consider the unique features of seeds, such as shape. Second, the existing software tools developed by the vendors selling the hyperspectral cameras (e.g., Headwall and Middleton Spectral Vision) are proprietary, which makes it challenging to customize. Third, the cost of solutions, including imaging platforms and analytic software, is a limiting factor for many research laboratories in the public domain.</p>
    <p>Moreover, climate change is driving the rising trend in temperature and posing a challenge for sustaining agricultural productivity. The mean annual temperature has increased by 1 °C for major regions in the past century [<xref rid="B16-sensors-21-08184" ref-type="bibr">16</xref>]. The impact of high temperature has been well-documented for rice, which is estimated to suffer a 10% reduction in yield for every 1 °C increment in the growing season minimum temperature [<xref rid="B17-sensors-21-08184" ref-type="bibr">17</xref>]. Besides yield, supranormal temperatures also decrease seed quality, as heat stress (HS) during the seed development phase drastically reduce the seed agronomic properties [<xref rid="B18-sensors-21-08184" ref-type="bibr">18</xref>,<xref rid="B19-sensors-21-08184" ref-type="bibr">19</xref>]. Poor seed quality under high temperature is a consequence of alteration in metabolic and transcriptomic signatures of seed development. For instance, phytohormones auxin, cytokinin, and ABA mediate the grain filling by regulating several genes that catalyze starch biosynthesis [<xref rid="B20-sensors-21-08184" ref-type="bibr">20</xref>]. However, genes that fine-tune these biochemical pathways are impaired under HS [<xref rid="B21-sensors-21-08184" ref-type="bibr">21</xref>]. A net effect of changes in the metabolic flux at the seed developmental phase leads to the production of low-quality seeds. Along with the reduction in seed size, abnormalities in grain filling pathways trigger distortion in the starch granule packing, thus rendering an opaque white appearance at the center of the endosperm, termed as grain chalkiness. While translucent grain fetches a maximum market price, chalky grains with low milling and cooking quality have less consumer acceptance [<xref rid="B22-sensors-21-08184" ref-type="bibr">22</xref>]. The development of rice cultivars that can maintain seed quality under HS has become a key target for rice improvement. A similar scenario is just as pertinent for other major crops, which are relatively less characterized for heat sensitivity but are equally likely to suffer loss of yield and quality.</p>
    <p>To address the challenges of HSI, we propose an end-to-end solution named <italic toggle="yes">HyperSeed</italic> that is designed for high-throughput seed phenotyping using HSI techniques. Moreover, to explore the rice seeds under HS and illustrate the application of <italic toggle="yes">HyperSeed</italic>, we also conducted an experiment as a case study. The proposed solution includes a lab-based imaging system coupled to an application with a graphical user interface (GUI) for hyperspectral analysis on seeds. The imaging system, which is cost-effective and easy to build, can capture the hyperspectral images of seeds at a large scale. The application is designed to process the images and extract the averaged hyperspectral reflectance for each seed in the form of comma-separated values (CSV) files that can be opened and viewed using conventional spreadsheet software tools. Hypercubes for each seed are generated for pixel-based analysis. This software removes the background, segments every single seed in an image, and calibrates the output. The general shape of seeds is considered in the process of seed segmentation so that the application can process seeds with overlapping regions, thus saving effort and time to spatially separate individual seeds. <italic toggle="yes">HyperSeed</italic> allows users to explore and modify parameters for better performance of their own datasets. Our experiments demonstrate that the system can be adapted to evaluate seeds from different plant species without any major modifications. Moreover, the application is implemented in MATLAB, which is open-source and available for researchers with an institutional license. For the users without a MATLAB license, we also provide a standalone version of <italic toggle="yes">HyperSeed</italic>, which only requires the free accessible MATLAB Compiler Runtime (MCR) for its operation.</p>
    <p>In our case study on rice seeds, we performed an experiment for seed classification and hyperspectral analysis. We used heat-stressed rice seeds as the test case since the seeds developed under higher temperatures undergo morphological and biochemical changes that otherwise are challenging to quantify using manual methods. For this, two groups of rice seeds were utilized as samples; the seeds in the first group (control) were harvested from plants grown under a ambient temperature, while the seeds in the second group were harvested at maturity from plants exposed to a transient HS during seed development. Then, we applied a 3D convolutional neural network (3D CNN) [<xref rid="B23-sensors-21-08184" ref-type="bibr">23</xref>] to classify the two groups and compared it with support vector machine (SVM) [<xref rid="B24-sensors-21-08184" ref-type="bibr">24</xref>]. These popular supervised models differ in training sample generation and spatial information extraction. The experiment showed that the proposed 3D CNN achieved high accuracy in the classification of the two groups, probably by extracting the spectrum of neighboring pixels in the spatial direction. Moreover, we implemented a spectral analysis using a light gradient boosting machine (LightGBM) model [<xref rid="B25-sensors-21-08184" ref-type="bibr">25</xref>]. The wavelengths of biological interest were identified.</p>
  </sec>
  <sec id="sec2-sensors-21-08184">
    <title>2. Materials and Methods</title>
    <p>The workflow of the proposed hyperspectral imaging system—<italic toggle="yes">HyperSeed</italic>—is illustrated in <xref rid="sensors-21-08184-f001" ref-type="fig">Figure 1</xref>. The seed samples (<xref rid="sensors-21-08184-f001" ref-type="fig">Figure 1</xref>a) are placed in the imaging system, and hyperspectral images are generated in the form of a hypercube (<xref rid="sensors-21-08184-f001" ref-type="fig">Figure 1</xref>b). The software processes the hypercube and segments each seed (<xref rid="sensors-21-08184-f001" ref-type="fig">Figure 1</xref>c). Finally, seed-based and pixel-based reflectance are extracted for further analysis. The seed-based reflectance is shown in <xref rid="sensors-21-08184-f001" ref-type="fig">Figure 1</xref>d, and each curve represents a single seed.</p>
    <sec id="sec2dot1-sensors-21-08184">
      <title>2.1. Sample Preparation</title>
      <p>Seeds were dehusked and surface sterilized with bleach (40% <italic toggle="yes">v</italic>/<italic toggle="yes">v</italic>) for 40 min and soaked in sterile water overnight. The sterilized seeds were germinated for 2 days in dark and followed by 4 days in light on half-strength Murashige and Skoog media. Seedlings were then moved to the greenhouse in 4-inch (101.6 mm) pots filled with pasteurized soil. These plants were grown in the greenhouse under a diurnal condition with temperature 28/25 ± 2 °C, light/dark 16/8 h, and relative humidity of 55–60% until flowering. At flowering, open florets were marked for tracking the flowering time. Half of the plants at 1 day after the fertilization stage were moved and maintained in the high day and night temperature (HDNT) chamber (36/32 ± 2 °C) for 5 days to impose HS. HS-treated plants were then transferred to control conditions and grown until maturity. Mature and dehusked seeds from the control group and seeds of these HS-treated plants were used for HSI and further analysis.</p>
    </sec>
    <sec id="sec2dot2-sensors-21-08184">
      <title>2.2. Imaging System</title>
      <p><xref rid="sensors-21-08184-f002" ref-type="fig">Figure 2</xref>a demonstrates our imaging system. A metal frame is assembled using aluminum profile extrusion to hold the hyperspectral camera and the light source. A high-performance line-scan image spectrograph (Micro-Hyperspec<inline-formula><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mrow/><mml:mo>®</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> Imaging Sensors, Extended VNIR version, Headwall Photonics, Fitchburg, MA, USA) is fixed on the top of the frame, which covers the spectral range from 600 to 1700 nm, with a 5.5 nm spectral resolution. The focal length and the minimum working distance of the camera lens are 25 mm and 300 mm, respectively. A two-line lighting unit with four 20 W tungsten–halogen bulbs for each line (MRC-920-029, MSV Series Illumination, Middleton Spectral Vision, Middleton, WI, USA) is fixed in the middle of the frame to illuminate the seed samples. The emission spectrum of the light source spans from 350 to 3000 nm. A camera slider with a track is placed on the bottom of the frame, and a square platform driven by a motor is installed on the track. Seed samples are placed on the platform and scanned line by line. The platform is painted black to reduce noise and facilitate the extraction of the seeds for downstream analysis. The whole system is placed in a dark chamber to eliminate external and varying light source. The chamber is installed in a room where the temperature and the humidity are controlled in the range of 66 to 74 °F, and 16% to 20%, respectively. A computer (Intel(R) Core (TM) i7-9700K CPU @ 3.60 GHz (Santa Clara, CA, USA), RAM 8 G) is located next to the dark chamber, and camera-controlling software (HyperSpec<inline-formula><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mrow/><mml:mo>®</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>III, Headwall Photonics, Fitchburg, MA, USA) is installed to set parameters for image acquisition.</p>
    </sec>
    <sec id="sec2dot3-sensors-21-08184">
      <title>2.3. Image Acquisition</title>
      <p>Before image acquisition, the camera is turned on and warmed up to avoid baseline drift [<xref rid="B26-sensors-21-08184" ref-type="bibr">26</xref>]. Subsequently, the controlling software developed by Headwall Photonics is used to adjust the parameters of the camera, such as exposure time, and take hyperspectral images. Other settings, such as the distance between the camera and the platform, are also calibrated to acquire the best quality images without distortion. For this study, the distance, exposure time, and frame period are set to 15 cm, 12 ms, and 18 ms, respectively. Rice samples are placed on the platform, and the image acquisition process is initiated. On average, it takes 15 s to capture one image, and the images are obtained in the form of three-dimensional (<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, and <inline-formula><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:math></inline-formula>) hypercubes. In this study, the hypercube includes 640 pixels in the <italic toggle="yes">x</italic> direction and 268 wavelength bands in the <inline-formula><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:math></inline-formula> direction, respectively. The number of pixels in the <italic toggle="yes">y</italic> direction depends on the duration of imaging, and it varies from 1100 to 1600 in our experiment. As an example illustrated in <xref rid="sensors-21-08184-f002" ref-type="fig">Figure 2</xref>b, one captured hyperspectral image dataset in the form of a hypercube is shown in pseudocolor. A pixel on the <italic toggle="yes">x</italic>–<italic toggle="yes">y</italic> plane corresponds to a spectrum curve. A sliced image at a specific wavelength in the <inline-formula><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:math></inline-formula> direction of the hypercube is shown in grayscale.</p>
    </sec>
    <sec id="sec2dot4-sensors-21-08184">
      <title>2.4. Software Implementation</title>
      <p>The <italic toggle="yes">HyperSeed</italic> software is utilized for the analysis of hyperspectral images. This open-source software is developed in MATLAB, and its standalone version can be operated without a MATLAB license. It has a GUI with multiple adjustable parameters for flexibility to process seed images shown in <xref rid="sensors-21-08184-f003" ref-type="fig">Figure 3</xref>. Users can process their seed samples following these steps: (1) path specification, (2) data visualization, (3) parameters setting, and (4) hypercube processing.</p>
      <sec id="sec2dot4dot1-sensors-21-08184">
        <title>2.4.1. Path Specification</title>
        <p><italic toggle="yes">HyperSeed</italic> application is compatible with hyperspectral images in the ENVI format consisting of pairs of raw images and header files. As shown in region 1 (<xref rid="sensors-21-08184-f003" ref-type="fig">Figure 3</xref>), the path of header files (*.hdr) and data filesof the hyperspectral image needs to be specified in <italic toggle="yes">input data path</italic>. <italic toggle="yes">HyperSeed</italic> is designed for batch processing, and a path with a regular expression is supported. As an example, <italic toggle="yes">[EXAMPLE PATH]/*.hdr</italic> loads all the hyperspectral images in the given path. Users also need to specify a path to output results in the <italic toggle="yes">output path</italic> box.</p>
      </sec>
      <sec id="sec2dot4dot2-sensors-21-08184">
        <title>2.4.2. Data Visualization</title>
        <p>After specifying the data path, users can visualize the hypercube. By clicking the <italic toggle="yes">Load</italic> button in region 2 (<xref rid="sensors-21-08184-f003" ref-type="fig">Figure 3</xref>), a sliced 2D image is extracted and demonstrated in region 5 (<xref rid="sensors-21-08184-f003" ref-type="fig">Figure 3</xref>) for visualization. A slider under the image in region 6 (<xref rid="sensors-21-08184-f003" ref-type="fig">Figure 3</xref>) can be used to specify the wavelength for the visualized image. A corresponding histogram for image intensity is also generated and displayed on the right of the sliced image.</p>
      </sec>
      <sec id="sec2dot4dot3-sensors-21-08184">
        <title>2.4.3. Parameters Setting</title>
        <p>A set of default parameters are automatically loaded when the application is launched. In region 3 (<xref rid="sensors-21-08184-f003" ref-type="fig">Figure 3</xref>), the checkbox <italic toggle="yes">remove bands in beginning/end</italic> is used to decide whether the software removes the bands at the beginning and end to increase accuracy. The checkbox <italic toggle="yes">enable overwriting</italic> decides whether the software overwrites the existing result files. The checkbox <italic toggle="yes">enable ellipse fitting</italic> decides whether an ellipse fitting algorithm, <italic toggle="yes">RFOVE</italic>, is included (<xref rid="sec2dot4dot6-sensors-21-08184" ref-type="sec">Section 2.4.6</xref>). The dropdown box <italic toggle="yes">input data type</italic> controls whether the software generates intensity or reflectance. If intensity mode in the dropdown box is selected, only the path of hyperspectral images in region 1 (<xref rid="sensors-21-08184-f003" ref-type="fig">Figure 3</xref>) needs to be specified. Otherwise, if reflectance mode is selected, the paths of white and dark reference also need to be specified. The parameters of <italic toggle="yes">band id</italic>, <italic toggle="yes">min intensity</italic> and <italic toggle="yes">max intensity</italic> are utilized to create a mask for background removal. The parameter of <italic toggle="yes">min pixel for clustering</italic> decides the threshold for the minimal pixels of one seed in segmentation (<xref rid="sec2dot4dot5-sensors-21-08184" ref-type="sec">Section 2.4.5</xref>).</p>
      </sec>
      <sec id="sec2dot4dot4-sensors-21-08184">
        <title>2.4.4. Hypercube Processing</title>
        <p>Once all the parameters are set, the batch processing is ready to start. The application has no assumption about the position of the seeds, so that seeds do not need to be placed in the center. Moreover, results can be correctly generated even if there are overlapping regions between seeds. The users can start the batch processing by clicking the <italic toggle="yes">Run</italic> button in region 8 (<xref rid="sensors-21-08184-f003" ref-type="fig">Figure 3</xref>). The seed-based averaged reflectance and pixel-based reflectance are extracted if reflectance mode (<xref rid="sec2dot4dot3-sensors-21-08184" ref-type="sec">Section 2.4.3</xref>) are selected in GUI. Otherwise, image intensity instead of reflectance is generated. All the results are sent to the <italic toggle="yes">output path</italic> for further analysis. The critical information, such as the total number of images and the path of current image being processed, is displayed in the log region, region 4 (<xref rid="sensors-21-08184-f003" ref-type="fig">Figure 3</xref>) for users to visualize the progress. In general, there are four main steps to process a hypercube: (1) initial seed segmentation, (2) refined seed segmentation, (3) spectral data extraction, and (4) image calibration. In the first two steps, we generate masks using image processing algorithms (<xref rid="sec2dot4dot5-sensors-21-08184" ref-type="sec">Section 2.4.5</xref> and <xref rid="sec2dot4dot6-sensors-21-08184" ref-type="sec">Section 2.4.6</xref>) on a sliced image with <italic toggle="yes">band id</italic> specified in region 3 (<xref rid="sensors-21-08184-f003" ref-type="fig">Figure 3</xref>). In this work, various bands are tested, and band 20 is utilized due to the clear contrast between our seeds and the black background. The corresponding wavelength for band 20 is 675 nm. Then, the segmentation is achieved by utilizing the generated masks to the <italic toggle="yes">x</italic>–<italic toggle="yes">y</italic> plane of the hypercube. Segmentation results are visualized in region 7 (<xref rid="sensors-21-08184-f003" ref-type="fig">Figure 3</xref>).</p>
      </sec>
      <sec id="sec2dot4dot5-sensors-21-08184">
        <title>2.4.5. Initial Seed Segmentation</title>
        <p>The first step to process the hypercube is background removal in the sliced image. To achieve this, we firstly filter out pixels using intensity thresholding techniques. The application loads the parameters <italic toggle="yes">minimal intensity</italic> (<inline-formula><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) and <italic toggle="yes">maximal intensity</italic> (<inline-formula><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) in region 3 (<xref rid="sensors-21-08184-f003" ref-type="fig">Figure 3</xref>) for thresholding. More specifically, a pixel with an intensity <italic toggle="yes">I</italic> will be kept if <inline-formula><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>I</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The default minimal and maximal intensities are set to 400 and 2000, respectively. After background removal, a components searching algorithm [<xref rid="B27-sensors-21-08184" ref-type="bibr">27</xref>] is used to find all the connected regions to further remove the remaining background. The searching algorithm, which is integrated with MATLAB function <italic toggle="yes">bwlabel</italic>, extracts the connected sets of pixels and labels each set with a unique number. After all the connected components are located, the number of pixels for each component is computed. The components are removed if their pixel count is less than the threshold. In this work, the threshold is set to 500. Examples of the raw image and the results of initial segmentation are illustrated in <xref rid="sensors-21-08184-f004" ref-type="fig">Figure 4</xref>a,b, respectively.</p>
      </sec>
      <sec id="sec2dot4dot6-sensors-21-08184">
        <title>2.4.6. Refined Seed Segmentation</title>
        <p>Though the seed candidates are obtained after the initial segmentation, the results still need to be improved since some pixels in seeds may be mistakenly treated as background and removed. Moreover, multiple seeds may be considered as a single entity due to possible overlapping. To improve the segmentation results, we utilize a morphological-reconstruction-based algorithm [<xref rid="B28-sensors-21-08184" ref-type="bibr">28</xref>] to repopulate the erroneously removed seed pixels on the initial segmentation results. The algorithm is integrated with MATLAB function <italic toggle="yes">imfill</italic> and considers the pixels as seed pixels if they are fully enclosed by seed pixels. Furthermore, if the seeds are overlapped and mainly ellipse-shaped, we further conduct a fitting algorithm named <italic toggle="yes">RFOVE</italic> [<xref rid="B29-sensors-21-08184" ref-type="bibr">29</xref>]. <italic toggle="yes">RFOVE</italic> fits ellipses to each initial component, and the components with multiple overlapped seeds are further segmented. Finally, we obtain a series of masks, and each mask represents only one seed. An example of refined segmentation result using <italic toggle="yes">RFOVE</italic> rendered in pseudo color is shown in <xref rid="sensors-21-08184-f004" ref-type="fig">Figure 4</xref>c, and all the seeds are labeled with the corresponding index.</p>
      </sec>
      <sec id="sec2dot4dot7-sensors-21-08184">
        <title>2.4.7. Spectral Data Extraction</title>
        <p>The results of the refined seed segmentation are in the form of a series of masks. By utilizing these masks to the <italic toggle="yes">x</italic>–<italic toggle="yes">y</italic> plane of the hypercube, the spectra of pixels in each seed sample are extracted. The pixel-based intensityof each seed is outputted in the form of a hypercube, and the number of hypercubes matches the number of seeds. To further obtain the seed-based intensity, the spectra of these pixels are averaged as the mean spectrum for the corresponding seed. Eventually, each seed will generate one corresponding record of the averaged intensity. In addition, due to the low sensitivity of the camera sensors at the beginning and end of the spectrum, outliers caused by random noises usually appear. Therefore, these bands could be omitted for better accuracy [<xref rid="B30-sensors-21-08184" ref-type="bibr">30</xref>]. If the checkbox <italic toggle="yes">Remove bands in beginning/end</italic> is selected, the application will remove 5% of bands in the beginning and end. In this work, the spectral bands with a wavelength not in the range of 655–1642 nm are filtered out.</p>
      </sec>
      <sec sec-type="results" id="sec2dot4dot8-sensors-21-08184">
        <title>2.4.8. Results Calibration</title>
        <p>After the spectral data extraction, spectra are obtained in the form of intensity (<inline-formula><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), which is easily affected by the inconstant factors, such as the varying light source and temperature-dependent hot pixels [<xref rid="B31-sensors-21-08184" ref-type="bibr">31</xref>,<xref rid="B32-sensors-21-08184" ref-type="bibr">32</xref>]. To solve this, a white reference image (<inline-formula><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) and a dark reference image (<inline-formula><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) is acquired for calibration. The white reference image with nearly 100% reflectance is captured using a standard white Teflon tile. The dark reference image with reflectance close to 0% is collected with the light source turned off and the camera lens covered by an opaque cap. Subsequently, the calibrated images (<inline-formula><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), which are also known as reflectance, are calculated using Equation (<xref rid="FD1-sensors-21-08184" ref-type="disp-formula">1</xref>) [<xref rid="B31-sensors-21-08184" ref-type="bibr">31</xref>]. Moreover, the calibration step can be skipped if intensity mode is selected (<xref rid="sec2dot4dot3-sensors-21-08184" ref-type="sec">Section 2.4.3</xref>). If so, the intensity instead of reflectance will be directly generated as the final results.
<disp-formula id="FD1-sensors-21-08184"><label>(1)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      </sec>
    </sec>
    <sec id="sec2dot5-sensors-21-08184">
      <title>2.5. Seed Classification and Wavelength Analysis</title>
      <p>As a case study, wavelength analysis and classification are conducted using the extracted spectrum. The support vector machine (SVM) models [<xref rid="B24-sensors-21-08184" ref-type="bibr">24</xref>] and a neural network model are utilized to classify the seeds between control and HS groups. LightGBM [<xref rid="B25-sensors-21-08184" ref-type="bibr">25</xref>] is used for wavelength importance analysis.</p>
      <sec id="sec2dot5dot1-sensors-21-08184">
        <title>2.5.1. Support Vector Machine (SVM)</title>
        <p>SVM is a widely used supervised learning model to analyze spectral data due to its capability to process both linear and nonlinear data [<xref rid="B13-sensors-21-08184" ref-type="bibr">13</xref>,<xref rid="B14-sensors-21-08184" ref-type="bibr">14</xref>]. With a kernel function, SVM maps the input data into a high-dimensional space, in which the mapped data is linearly separable. Then, a linear classifier in the form of a hyperplane (or a set of hyperplanes for multiple-class classification) is created to separate the mapped data in high-dimensional space. In this work, the radial bias function (RBF) is selected as the kernel function.</p>
      </sec>
      <sec id="sec2dot5dot2-sensors-21-08184">
        <title>2.5.2. Neural Network Models—3D Convolutional Neural Network (3D CNN)</title>
        <p>Neural network models have been used for processing hyperspectral images [<xref rid="B33-sensors-21-08184" ref-type="bibr">33</xref>]. One of the essential neural networks in processing RGB images and hyperspectral images is convolutional neural network (CNN), which is widely used for classification. In this work, our network is adapted from the 3D CNN [<xref rid="B23-sensors-21-08184" ref-type="bibr">23</xref>]. Compared to 1D or 2D networks, in which either spatial or spectral neighborhood is not considered, 3D CNN directly processes the sub-hypercubes and thus works on the spatial and spectral dimensions simultaneously. The 3D CNN has three main steps. The first step is sample extraction, as shown in <xref rid="sensors-21-08184-f005" ref-type="fig">Figure 5</xref>a. Given an original hypercube with a size of <inline-formula><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in the <italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, and <inline-formula><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:math></inline-formula> dimensions, <inline-formula><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>×</mml:mo><mml:mi>S</mml:mi><mml:mo>×</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> sub-cubes are extracted as samples. The extraction is implemented for each pixel of the seeds, and the group labels of these sub-cubes are the same as the labels of their central pixels. In this work, <italic toggle="yes">S</italic> and <italic toggle="yes">L</italic> are set to 5 and 239, respectively. The second step is spectral-spatial feature extraction. <xref rid="sensors-21-08184-f005" ref-type="fig">Figure 5</xref>b shows the network architecture. The <inline-formula><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>×</mml:mo><mml:mi>S</mml:mi><mml:mo>×</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> sub-cubes are fed to the first 3D convolution layer C1 with two 3D kernels. Then, the output is sent to the second 3D convolution layer C2 convoluted with four 3D kernels. After that, all the output features are flattened and sent to a fully connected layer. The third step is feature-based classification. The features in the last layer are used for classification. The parameters of the network are optimized using the stochastic gradient descent (SGD) algorithm by minimizing the Softmax loss [<xref rid="B34-sensors-21-08184" ref-type="bibr">34</xref>].</p>
      </sec>
      <sec id="sec2dot5dot3-sensors-21-08184">
        <title>2.5.3. Dataset for Classification</title>
        <p>Two datasets extracted by <italic toggle="yes">HyperSeed</italic> software are used to train models for comparison. The first one is the seed-based reflectance for the seed-based SVM model. One sample represents one seed, and the dataset is divided into two parts: 80% of seeds for training and 20% of seeds for testing. The second one is the pixel-based reflectance for the pixel-based SVM model and the 3D CNN model. The test set of the second dataset consists of the corresponding pixels of the seeds in test set of the first dataset. The rest of the pixels are further divided into two sets: 95% of pixels for the training set and 5% of pixels for the validation set. The number of the samples of the two datasets for classification are listed in <xref rid="sensors-21-08184-t001" ref-type="table">Table 1</xref>.</p>
      </sec>
      <sec id="sec2dot5dot4-sensors-21-08184">
        <title>2.5.4. Metrics for Classification</title>
        <p>In this work, the models are evaluated using four metrics on the test samples: <inline-formula><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <italic toggle="yes">F</italic>-<inline-formula><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, as shown in <xref rid="sensors-21-08184-t002" ref-type="table">Table 2</xref>. These metrics are popular for machine learning model evaluation and a higher value usually represents better performance. The formula of the four metrics is shown in Equation (<xref rid="FD2-sensors-21-08184" ref-type="disp-formula">2</xref>):<disp-formula id="FD2-sensors-21-08184"><label>(2)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where true positive (<inline-formula><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) is the number of samples in the HS group predicted as HS group; true negative (<inline-formula><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) is the number of samples in the control group predicted as the control group; false positive (<inline-formula><mml:math id="mm25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) is the number of samples in the control group but predicted as HS group; false negative (<inline-formula><mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) is the number of samples in the HS group but predicted as the control group. Moreover, the model is also evaluated using seed group prediction accuracy for a fair comparison, which presents the percentage of correctly predicted seeds in the test set.</p>
      </sec>
      <sec id="sec2dot5dot5-sensors-21-08184">
        <title>2.5.5. LightGBM for Feature Importance Analysis</title>
        <p>LightGBM is a gradient boosting framework widely used to solve machine learning tasks such as feature selection, ranking, regression, and classification. As a decision-tree-based model, LightGBM highly improves the strategy for tree construction. Unlike other similar tree-based models in which level-wise strategy is adopted, LightGBM implements a leaf-wise method with depth constraints. The leaf-wise strategy chooses a leaf that leads to the most significant decrease in loss and thus improves the accuracy. The depth constraints limit the depth of the tree, which avoids overfitting. In addition, instead of searching for the best node for splitting, LightGBM proposes an algorithm for nodes selection based on the histogram. Since searching is usually time- and memory-consuming, LightGBM with histogram improves the efficiency and reduces memory consumption. Moreover, parallel GPU learning is supported in LightGBM. In summary, LightGBM is designed for high accuracy, low memory cost, and parallel learning and, therefore, has the better potentiality to analyze large-scale data. With the LightGBM, the importance of features in the form of tree nodes in the model can be evaluated using various metrics. In this work, the number of times for which a node is used for splitting is calculated as importance.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="results" id="sec3-sensors-21-08184">
    <title>3. Results</title>
    <sec id="sec3dot1-sensors-21-08184">
      <title>3.1. Performance Testing</title>
      <p>To test the performance of <italic toggle="yes">HyperSeed</italic>, we evaluated the time required to capture the segmented seeds from hyperspectral images. The computing platform we used is an Intel(R) Core (TM) i7-8700 K CPU @3.70 GHz (Santa Clara, CA, USA) and 16 GB DDR4 random-access memory. Generally, more seeds in one image indicates a more time-costly procedure. On average, one additional seed in the image led to an extra 3.8 s in time cost, which showed the potential in high-throughput processing.</p>
    </sec>
    <sec id="sec3dot2-sensors-21-08184">
      <title>3.2. Segmentation Results Using Seeds from Various Plant Species</title>
      <p>To evaluate the effectiveness of the software on seeds with different shapes, we captured the hyperspectral images of seeds from various plant species and conducted the segmentation using <italic toggle="yes">HyperSeed</italic>. The first row in <xref rid="sensors-21-08184-f006" ref-type="fig">Figure 6</xref>a–d shows the 2D images of seeds of maize, rice, sorghum, and wheat, respectively. The second row in <xref rid="sensors-21-08184-f006" ref-type="fig">Figure 6</xref>e–h demonstrates the corresponding segmentation results in which each seed are labeled with the corresponding index. In general, <italic toggle="yes">HyperSeed</italic> is capable of accurately segmenting seeds with various shapes.</p>
    </sec>
    <sec id="sec3dot3-sensors-21-08184">
      <title>3.3. Spectral Analysis</title>
      <p>The averaged hyperspectral reflectance of control and heat stress (HS) groups were obtained by averaging the reflectance of seeds in the two groups, respectively. As shown in <xref rid="sensors-21-08184-f007" ref-type="fig">Figure 7</xref>, each curve presents the averaged reflectance of 100 seeds in the responding group. The two curves illuminate similar patterns, and the HS group exhibited higher reflectance than the control group on average. However, at the wavelength of 671–771 nm, curves with similar reflectance were observed between the two groups. The differences indicate that the transient HS might have modified the content of the seeds and thus influenced the corresponding reflectance.</p>
    </sec>
    <sec id="sec3dot4-sensors-21-08184">
      <title>3.4. Classification</title>
      <p>In this section, we utilized multiple models for the classification of control and HS groups. The classification results are described in <xref rid="sensors-21-08184-f008" ref-type="fig">Figure 8</xref>. For better visualization, all the 40 seeds in the test set are demonstrated in one figure. In each subfigure, the first two rows and last two rows are the seeds from the control and HS groups, respectively. The seeds or pixels are marked red if they are in the control group as ground truth (<xref rid="sensors-21-08184-f008" ref-type="fig">Figure 8</xref>a) or predicted as control group (<xref rid="sensors-21-08184-f008" ref-type="fig">Figure 8</xref>b–d). In contrast, the blue color represents the HS group in each subfigure.</p>
      <sec id="sec3dot4dot1-sensors-21-08184">
        <title>3.4.1. Seed-Based Support Vector Machine (Seed-Based SVM)</title>
        <p>To extend the machine learning applications in determining seed viability and seed varieties detection [<xref rid="B13-sensors-21-08184" ref-type="bibr">13</xref>,<xref rid="B35-sensors-21-08184" ref-type="bibr">35</xref>], we implemented seed-based classification using the SVM model. The averaged reflectance of each seed was fed to the model for training, and the number of training samples matched the number of seeds. Since the number of training samples (160) in this work was limited compared to the number of features (239), the SVM model was expected to suffer from underfit if all the features are to be used in the model. Moreover, the extracted spectral reflectance is usually redundant. Some bands as features for training are highly correlated with each other. Therefore, the features can be preprocessed to reduce the dimension of feature space without affecting the accuracy of classification [<xref rid="B35-sensors-21-08184" ref-type="bibr">35</xref>,<xref rid="B36-sensors-21-08184" ref-type="bibr">36</xref>]. To achieve this, we implemented principal component analysis (PCA) [<xref rid="B37-sensors-21-08184" ref-type="bibr">37</xref>]. PCA treated the bands as initial features and mapped them to orthogonal components by implementing a linear transformation. These components, which were the linear combination of initial features, were utilized as the new features and ranked according to their corresponding eigenvalues. By selecting the top-ranked new features, the correlation problem was solved. In this work, 50 features were finally selected to train the SVM model. The accuracy of the model on test samples was 80.0%, and since one seed represented one sample, the seed group prediction accuracy was also 80.0%. The classification results and other metrics are shown in <xref rid="sensors-21-08184-f008" ref-type="fig">Figure 8</xref>b and <xref rid="sensors-21-08184-t002" ref-type="table">Table 2</xref>, respectively.</p>
      </sec>
      <sec id="sec3dot4dot2-sensors-21-08184">
        <title>3.4.2. Pixel-Based Support Vector Machine (Pixel-Based SVM)</title>
        <p>The performance of the seed-based SVM was limited as the number of training samples were not sufficient. To address this issue, we fed the model with pixel-based reflectance, in which each pixel, rather than each seed, was considered as one sample. As shown in <xref rid="sensors-21-08184-t001" ref-type="table">Table 1</xref>, the number of training samples increased from 160 to 209,236. Then, we classified a seed by comparing the number of predicted pixels in the two groups. For example, a seed was considered to be in the control group if more than half of pixels in this seed were in the control group. Afterwards, the seed group prediction accuracy was calculated by counting the number of correctly predicted seeds. Therefore, the issues of the number of samples were solved, and the performance was improved. In the classification results shown in <xref rid="sensors-21-08184-f008" ref-type="fig">Figure 8</xref>c and the metrics shown in <xref rid="sensors-21-08184-t002" ref-type="table">Table 2</xref>, we observed that the accuracy of seed-based classification increased from 80.0% to 92.5%.</p>
      </sec>
      <sec id="sec3dot4dot3-sensors-21-08184">
        <title>3.4.3. 3D Convolutional Neural Network (3D CNN)</title>
        <p>Compared to seed-based methods, the pixel-based SVM increased the accuracy in seed-based classification. However, it could be observed that the pixel-based SVM still had space for improvement since it included many mispredicted pixels. One of the issues with the pixel-based SVM was that it only considered each pixel as separate samples and ignored the connection between them. Therefore, the spatial information of the seeds was lost in the SVM model. In contrast, the 3D CNN proposed in this work processed the pixels in the spatial and spectral dimensions simultaneously. As shown in <xref rid="sensors-21-08184-f008" ref-type="fig">Figure 8</xref>d and <xref rid="sensors-21-08184-t002" ref-type="table">Table 2</xref>, 3D CNN has better performance than the pixel-based SVM. The accuracy of 3D CNN increases from 85.67% to 94.21% in the pixel-based classification. The accuracy of seed-based classification calculated using the same methods as pixel-based SVM increases from 92.5% to 97.5%.</p>
      </sec>
    </sec>
    <sec id="sec3dot5-sensors-21-08184">
      <title>3.5. Wavelengths Analysis</title>
      <p>The extracted spectrum includes 239 wavelengths after calibration. In this section, we considered each wavelength as a feature and utilized LightGBM to analyze the importance with respect to group labels. By building a LightGBM model using the spectral data of pixels, the wavelengths are used as nodes in the model. All the wavelengths are fed to the model, and the importance of these wavelengths is evaluated by calculating the number of times for which a spectral band is used to split the data across all trees. Then, the importance is normalized by dividing the total number of splitting to rescale the range to 0–1. The wavelengths are sorted with respect to normalized importance, and the wavelengths with top 12 importance are shown in <xref rid="sensors-21-08184-f009" ref-type="fig">Figure 9</xref>. These wavelengths also match our observation in <xref rid="sec3dot3-sensors-21-08184" ref-type="sec">Section 3.3</xref> where the differences of reflectance for these wavelengths is clear. Moreover, <xref rid="sensors-21-08184-f009" ref-type="fig">Figure 9</xref> showed that most of the top important wavelengths for classification ranged from 1000 to 1600 nm. Based on this analysis, the NIR component of the hyperspectral camera capturing 900–1700 nm could be selected for cost-effective imaging in future studies.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="sec4-sensors-21-08184">
    <title>4. Discussion</title>
    <p>The proposed system <italic toggle="yes">HyperSeed</italic> has provided an end-to-end solution to hyperspectral imaging of seeds. The system is specially designed for seed imaging and has achieved high accuracy and efficiency. The cost-effective imaging system and the open-source MATLAB software facilitate easy access and customized modifications. Our experiments on rice seeds have shown the data analytic capabilities of <italic toggle="yes">HyperSeed</italic>.</p>
    <p>Though <italic toggle="yes">HyperSeed</italic> has demonstrated its capability to process hyperspectral images in a high-throughput manner, there is still space for improvement. First, the current version of <italic toggle="yes">HyperSeed</italic> software is single-threaded. Since the initial segmentation has already detected the potential individual seeds, it is possible to apply the refined segmentation to these seed candidates simultaneously using multithread techniques. As a result, the time cost to process each hyperspectral image can be further reduced if the software is implemented in a multithreaded manner. Second, we only explored the labeled seed samples and use them for wavelength analysis and classification. In HSI, compared to the labeled samples, unlabeled samples are usually much easier to access. More applications should be achievable if unsupervised machine learning methods with unlabeled samples are applied. Third, though the 3D CNN method already has high accuracy on seed-based classification, it still has the potential to be improved on pixel-based classification. In the sample extraction step of 3D CNN, only the local spatial information is extracted to generate training samples for fast training and easy convergence. The model performance could be further improved if the global spatial traits such as shape are captured by the model. Moreover, since the focus of this work is the end-to-end solution, we did not explore the relationship between the seed composition and wavelengths. Due to the same reason, the analysis of the activation maps in 3D CNN is not included.</p>
  </sec>
  <sec sec-type="conclusions" id="sec5-sensors-21-08184">
    <title>5. Conclusions</title>
    <p>We propose a novel end-to-end system called <italic toggle="yes">HyperSeed</italic> to process the hyperspectral images of seeds in a high-throughput manner and provide details to establish both hardware and software components. The system can be used on seeds from various plant species. The cost-effective hardware is capable of capturing hyperspectral images of multiple seeds. The open-sourced software with GUI extracts the calibrated hyperspectral reflectance of the segmented seeds effectively. The software’s output includes seed-based averaged reflectance and pixel-based reflectance for each seed. To demonstrate the potential of the proposed tool for biological interest, we performed experiments on classification and hyperspectral analysis using the extracted reflectance data of control and HS seeds. By comparing various machine learning models, the proposed 3D CNN showed a high classification accuracy (94.21% at the pixel level and 97.5% at the seed level). The spectral curves of the seeds were analyzed, and the wavelengths with top importance were identified. Our future work will aim to implement the software in a multithreaded manner to further improve efficiency. We will also explore the hidden layers in 3D CNN and the relationship between the seed composition and wavelengths.</p>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgments</title>
    <p>The authors would like to thank the staff members of the University of Nebraska-Lincoln’s Plant Pathology Greenhouse for their help in the data collection.</p>
  </ack>
  <fn-group>
    <fn>
      <p><bold>Publisher’s Note:</bold> MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <notes>
    <title>Author Contributions</title>
    <p>T.G., P.P., H.Y. and H.W. designed the instrument and software. T.G. built the instrument and implemented the software. P.P. raised the plants. A.K.N.C. and P.P. collected data. T.G. and A.K.N.C. analyzed data and drafted the manuscript. P.P., H.Y. and H.W. edited the manuscript. All authors have read and agreed to the published version of the manuscript.</p>
  </notes>
  <notes>
    <title>Funding</title>
    <p>This research was funded by the National Science Foundation Award grant number 1736192.</p>
  </notes>
  <notes>
    <title>Institutional Review Board Statement</title>
    <p>Not applicable.</p>
  </notes>
  <notes>
    <title>Informed Consent Statement</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data Availability Statement</title>
    <p>The software and data for testing is accessible in Github: <uri xlink:href="https://github.com/tgaochn/HyperSeed">https://github.com/tgaochn/HyperSeed</uri> (accessed on 3 December 2021).</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Conflicts of Interest</title>
    <p>The authors declare no conflict of interest.</p>
  </notes>
  <ref-list>
    <title>References</title>
    <ref id="B1-sensors-21-08184">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>TeKrony</surname>
            <given-names>D.M.</given-names>
          </name>
          <name>
            <surname>Egli</surname>
            <given-names>D.B.</given-names>
          </name>
        </person-group>
        <article-title>Relationship of seed vigor to crop yield: A review</article-title>
        <source>Crop Sci.</source>
        <year>1991</year>
        <volume>31</volume>
        <fpage>816</fpage>
        <lpage>822</lpage>
        <pub-id pub-id-type="doi">10.2135/cropsci1991.0011183X003100030054x</pub-id>
      </element-citation>
    </ref>
    <ref id="B2-sensors-21-08184">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dell’Aquila</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Digital imaging information technology applied to seed germination testing: A review</article-title>
        <source>Agron. Sustain. Dev.</source>
        <year>2009</year>
        <volume>29</volume>
        <fpage>213</fpage>
        <lpage>221</lpage>
        <pub-id pub-id-type="doi">10.1051/agro:2008039</pub-id>
      </element-citation>
    </ref>
    <ref id="B3-sensors-21-08184">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sandhu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Paul</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Dhatt</surname>
            <given-names>B.K.</given-names>
          </name>
          <name>
            <surname>Ge</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Staswick</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Walia</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>PI-Plat: A high-resolution image-based 3D reconstruction method to estimate growth dynamics of rice inflorescence traits</article-title>
        <source>Plant Methods</source>
        <year>2019</year>
        <volume>15</volume>
        <fpage>162</fpage>
        <pub-id pub-id-type="doi">10.1186/s13007-019-0545-2</pub-id>
        <pub-id pub-id-type="pmid">31889986</pub-id>
      </element-citation>
    </ref>
    <ref id="B4-sensors-21-08184">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gao</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Paul</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Sandhu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Doku</surname>
            <given-names>H.A.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Staswick</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Walia</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Novel 3D Imaging Systems for High-Throughput Phenotyping of Plants</article-title>
        <source>Remote Sens.</source>
        <year>2021</year>
        <volume>13</volume>
        <elocation-id>2113</elocation-id>
        <pub-id pub-id-type="doi">10.3390/rs13112113</pub-id>
      </element-citation>
    </ref>
    <ref id="B5-sensors-21-08184">
      <label>5.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Gao</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Doku</surname>
            <given-names>H.A.</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Walia</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Plant Event Detection from Time-Varying Point Clouds</article-title>
        <source>Proceedings of the 2019 IEEE International Conference on Big Data (Big Data)</source>
        <conf-loc>Los Angeles, CA, USA</conf-loc>
        <conf-date>9–12 December 2019</conf-date>
        <fpage>3321</fpage>
        <lpage>3329</lpage>
      </element-citation>
    </ref>
    <ref id="B6-sensors-21-08184">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tanabata</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Shibaya</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Hori</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Ebana</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Yano</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>SmartGrain: High-throughput phenotyping software for measuring seed shape through image analysis</article-title>
        <source>Plant Physiol.</source>
        <year>2012</year>
        <volume>160</volume>
        <fpage>1871</fpage>
        <lpage>1880</lpage>
        <pub-id pub-id-type="doi">10.1104/pp.112.205120</pub-id>
        <?supplied-pmid 23054566?>
        <pub-id pub-id-type="pmid">23054566</pub-id>
      </element-citation>
    </ref>
    <ref id="B7-sensors-21-08184">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Paul</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Hussain</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Wallman</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Dhatt</surname>
            <given-names>B.K.</given-names>
          </name>
          <name>
            <surname>Sandhu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Irvin</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Morota</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Walia</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>SeedExtractor: An open-source GUI for seed image analysis</article-title>
        <source>Front. Plant Sci.</source>
        <year>2020</year>
        <volume>11</volume>
        <fpage>581546</fpage>
        <pub-id pub-id-type="doi">10.3389/fpls.2020.581546</pub-id>
        <?supplied-pmid 33597957?>
        <pub-id pub-id-type="pmid">33597957</pub-id>
      </element-citation>
    </ref>
    <ref id="B8-sensors-21-08184">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gagliardi</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Marcos-Filho</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Relationship between germination and bell pepper seed structure assessed by the X-ray test</article-title>
        <source>Sci. Agric.</source>
        <year>2011</year>
        <volume>68</volume>
        <fpage>411</fpage>
        <lpage>416</lpage>
        <pub-id pub-id-type="doi">10.1590/S0103-90162011000400004</pub-id>
      </element-citation>
    </ref>
    <ref id="B9-sensors-21-08184">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gomes-Junior</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Yagushi</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Belini</surname>
            <given-names>U.</given-names>
          </name>
          <name>
            <surname>Cicero</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Tomazello-Filho</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>X-ray densitometry to assess internal seed morphology and quality</article-title>
        <source>Seed Sci. Technol.</source>
        <year>2012</year>
        <volume>40</volume>
        <fpage>102</fpage>
        <lpage>107</lpage>
        <pub-id pub-id-type="doi">10.15258/sst.2012.40.1.11</pub-id>
      </element-citation>
    </ref>
    <ref id="B10-sensors-21-08184">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dumont</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hirvonen</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Heikkinen</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Mistretta</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Granlund</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Himanen</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Fauch</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Porali</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Hiltunen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Keski-Saari</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Thermal and hyperspectral imaging for Norway spruce (Picea abies) seeds screening</article-title>
        <source>Comput. Electron. Agric.</source>
        <year>2015</year>
        <volume>116</volume>
        <fpage>118</fpage>
        <lpage>124</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2015.06.010</pub-id>
      </element-citation>
    </ref>
    <ref id="B11-sensors-21-08184">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Hong</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>You</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Spectral and image integrated analysis of hyperspectral data for waxy corn seed variety classification</article-title>
        <source>Sensors</source>
        <year>2015</year>
        <volume>15</volume>
        <fpage>15578</fpage>
        <lpage>15594</lpage>
        <pub-id pub-id-type="doi">10.3390/s150715578</pub-id>
        <pub-id pub-id-type="pmid">26140347</pub-id>
      </element-citation>
    </ref>
    <ref id="B12-sensors-21-08184">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bock</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Poole</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Parker</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Gottwald</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Plant disease severity estimated visually, by digital photography and image analysis, and by hyperspectral imaging</article-title>
        <source>Crit. Rev. Plant Sci.</source>
        <year>2010</year>
        <volume>29</volume>
        <fpage>59</fpage>
        <lpage>107</lpage>
        <pub-id pub-id-type="doi">10.1080/07352681003617285</pub-id>
      </element-citation>
    </ref>
    <ref id="B13-sensors-21-08184">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <article-title>A reliable methodology for determining seed viability by using hyperspectral data from two sides of wheat seeds</article-title>
        <source>Sensors</source>
        <year>2018</year>
        <volume>18</volume>
        <elocation-id>813</elocation-id>
        <pub-id pub-id-type="doi">10.3390/s18030813</pub-id>
        <?supplied-pmid 29517991?>
        <pub-id pub-id-type="pmid">29517991</pub-id>
      </element-citation>
    </ref>
    <ref id="B14-sensors-21-08184">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qiu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Variety identification of single rice seed using hyperspectral imaging combined with convolutional neural network</article-title>
        <source>Appl. Sci.</source>
        <year>2018</year>
        <volume>8</volume>
        <elocation-id>212</elocation-id>
        <pub-id pub-id-type="doi">10.3390/app8020212</pub-id>
      </element-citation>
    </ref>
    <ref id="B15-sensors-21-08184">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Determination of starch content in single kernel using near-infrared hyperspectral images from two sides of corn seeds</article-title>
        <source>Infrared Phys. Technol.</source>
        <year>2020</year>
        <volume>110</volume>
        <fpage>103462</fpage>
        <pub-id pub-id-type="doi">10.1016/j.infrared.2020.103462</pub-id>
      </element-citation>
    </ref>
    <ref id="B16-sensors-21-08184">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Piao</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Lobell</surname>
            <given-names>D.B.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Bassu</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Ciais</surname>
            <given-names>P.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Temperature increase reduces global yields of major crops in four independent estimates</article-title>
        <source>Proc. Natl. Acad. Sci. USA</source>
        <year>2017</year>
        <volume>114</volume>
        <fpage>9326</fpage>
        <lpage>9331</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1701762114</pub-id>
        <pub-id pub-id-type="pmid">28811375</pub-id>
      </element-citation>
    </ref>
    <ref id="B17-sensors-21-08184">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Sheehy</surname>
            <given-names>J.E.</given-names>
          </name>
          <name>
            <surname>Laza</surname>
            <given-names>R.C.</given-names>
          </name>
          <name>
            <surname>Visperas</surname>
            <given-names>R.M.</given-names>
          </name>
          <name>
            <surname>Zhong</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Centeno</surname>
            <given-names>G.S.</given-names>
          </name>
          <name>
            <surname>Khush</surname>
            <given-names>G.S.</given-names>
          </name>
          <name>
            <surname>Cassman</surname>
            <given-names>K.G.</given-names>
          </name>
        </person-group>
        <article-title>Rice yields decline with higher night temperature from global warming</article-title>
        <source>Proc. Natl. Acad. Sci. USA</source>
        <year>2004</year>
        <volume>101</volume>
        <fpage>9971</fpage>
        <lpage>9975</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.0403720101</pub-id>
        <pub-id pub-id-type="pmid">15226500</pub-id>
      </element-citation>
    </ref>
    <ref id="B18-sensors-21-08184">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dhatt</surname>
            <given-names>B.K.</given-names>
          </name>
          <name>
            <surname>Paul</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Sandhu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hussain</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Irvin</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Adviento-Borbe</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Lorence</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Staswick</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>H.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Allelic variation in rice Fertilization Independent Endosperm 1 contributes to grain width under high night temperature stress</article-title>
        <source>New Phytol.</source>
        <year>2021</year>
        <volume>229</volume>
        <fpage>335</fpage>
        <pub-id pub-id-type="doi">10.1111/nph.16897</pub-id>
        <?supplied-pmid 32858766?>
        <pub-id pub-id-type="pmid">32858766</pub-id>
      </element-citation>
    </ref>
    <ref id="B19-sensors-21-08184">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Paul</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Dhatt</surname>
            <given-names>B.K.</given-names>
          </name>
          <name>
            <surname>Sandhu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hussain</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Irvin</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Morota</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Staswick</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Walia</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Divergent phenotypic response of rice accessions to transient heat stress during early seed development</article-title>
        <source>Plant Direct</source>
        <year>2020</year>
        <volume>4</volume>
        <fpage>e00196</fpage>
        <pub-id pub-id-type="doi">10.1002/pld3.196</pub-id>
        <pub-id pub-id-type="pmid">31956854</pub-id>
      </element-citation>
    </ref>
    <ref id="B20-sensors-21-08184">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Lü</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>The Rice G protein <italic toggle="yes">γ</italic> subunit DEP1/qPE9–1 positively regulates grain-filling process by increasing Auxin and Cytokinin content in Rice grains</article-title>
        <source>Rice</source>
        <year>2019</year>
        <volume>12</volume>
        <fpage>91</fpage>
        <pub-id pub-id-type="doi">10.1186/s12284-019-0344-4</pub-id>
        <pub-id pub-id-type="pmid">31844998</pub-id>
      </element-citation>
    </ref>
    <ref id="B21-sensors-21-08184">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ren</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Xiong</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>A heat stress responsive NAC transcription factor heterodimer plays key roles in rice grain filling</article-title>
        <source>J. Exp. Bot.</source>
        <year>2021</year>
        <volume>72</volume>
        <fpage>2947</fpage>
        <lpage>2964</lpage>
        <pub-id pub-id-type="doi">10.1093/jxb/erab027</pub-id>
        <pub-id pub-id-type="pmid">33476364</pub-id>
      </element-citation>
    </ref>
    <ref id="B22-sensors-21-08184">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Misra</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Badoni</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Parween</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>R.K.</given-names>
          </name>
          <name>
            <surname>Leung</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Ladejobi</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Mott</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Sreenivasulu</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <article-title>Genome-wide association coupled gene to gene interaction studies unveil novel epistatic targets among major effect loci impacting rice grain chalkiness</article-title>
        <source>Plant Biotechnol. J.</source>
        <year>2021</year>
        <volume>19</volume>
        <fpage>910</fpage>
        <lpage>925</lpage>
        <pub-id pub-id-type="doi">10.1111/pbi.13516</pub-id>
        <?supplied-pmid 33220119?>
        <pub-id pub-id-type="pmid">33220119</pub-id>
      </element-citation>
    </ref>
    <ref id="B23-sensors-21-08184">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <article-title>Spectral–spatial classification of hyperspectral imagery with 3D convolutional neural network</article-title>
        <source>Remote Sens.</source>
        <year>2017</year>
        <volume>9</volume>
        <elocation-id>67</elocation-id>
        <pub-id pub-id-type="doi">10.3390/rs9010067</pub-id>
      </element-citation>
    </ref>
    <ref id="B24-sensors-21-08184">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Burges</surname>
            <given-names>C.J.</given-names>
          </name>
        </person-group>
        <article-title>A tutorial on support vector machines for pattern recognition</article-title>
        <source>Data Min. Knowl. Discov.</source>
        <year>1998</year>
        <volume>2</volume>
        <fpage>121</fpage>
        <lpage>167</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1009715923555</pub-id>
      </element-citation>
    </ref>
    <ref id="B25-sensors-21-08184">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ke</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Meng</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Finley</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>T.Y.</given-names>
          </name>
        </person-group>
        <article-title>LightGBM: A highly efficient gradient boosting decision tree</article-title>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <year>2017</year>
        <volume>30</volume>
        <fpage>3146</fpage>
        <lpage>3154</lpage>
      </element-citation>
    </ref>
    <ref id="B26-sensors-21-08184">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Rao</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Hyperspectral imaging technology combined with multivariate data analysis to identify heat-damaged rice seeds</article-title>
        <source>Spectrosc. Lett.</source>
        <year>2020</year>
        <volume>53</volume>
        <fpage>207</fpage>
        <lpage>221</lpage>
        <pub-id pub-id-type="doi">10.1080/00387010.2020.1726402</pub-id>
      </element-citation>
    </ref>
    <ref id="B27-sensors-21-08184">
      <label>27.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Haralock</surname>
            <given-names>R.M.</given-names>
          </name>
          <name>
            <surname>Shapiro</surname>
            <given-names>L.G.</given-names>
          </name>
        </person-group>
        <source>Computer and Robot Vision</source>
        <publisher-name>Addison-Wesley Longman Publishing Co., Inc.</publisher-name>
        <publisher-loc>Boston, MA, USA</publisher-loc>
        <year>1991</year>
        <volume>Volume 1</volume>
        <fpage>28</fpage>
        <lpage>48</lpage>
      </element-citation>
    </ref>
    <ref id="B28-sensors-21-08184">
      <label>28.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Soille</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <source>Morphological Image Analysis: Principles and Applications</source>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
        <year>2013</year>
        <fpage>173</fpage>
        <lpage>174</lpage>
      </element-citation>
    </ref>
    <ref id="B29-sensors-21-08184">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Panagiotakis</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Argyros</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Region-based Fitting of Overlapping Ellipses and its application to cells segmentation</article-title>
        <source>Image Vis. Comput.</source>
        <year>2020</year>
        <volume>93</volume>
        <fpage>103810</fpage>
        <pub-id pub-id-type="doi">10.1016/j.imavis.2019.09.001</pub-id>
      </element-citation>
    </ref>
    <ref id="B30-sensors-21-08184">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wakholi</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Kandpal</surname>
            <given-names>L.M.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Bae</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>M.S.</given-names>
          </name>
          <name>
            <surname>Mo</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>W.H.</given-names>
          </name>
          <name>
            <surname>Cho</surname>
            <given-names>B.K.</given-names>
          </name>
        </person-group>
        <article-title>Rapid assessment of corn seed viability using short wave infrared line-scan hyperspectral imaging and chemometrics</article-title>
        <source>Sens. Actuators B Chem.</source>
        <year>2018</year>
        <volume>255</volume>
        <fpage>498</fpage>
        <lpage>507</lpage>
        <pub-id pub-id-type="doi">10.1016/j.snb.2017.08.036</pub-id>
      </element-citation>
    </ref>
    <ref id="B31-sensors-21-08184">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Polder</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>van der Heijden</surname>
            <given-names>G.W.</given-names>
          </name>
          <name>
            <surname>Keizer</surname>
            <given-names>L.P.</given-names>
          </name>
          <name>
            <surname>Young</surname>
            <given-names>I.T.</given-names>
          </name>
        </person-group>
        <article-title>Calibration and characterisation of imaging spectrographs</article-title>
        <source>J. Near Infrared Spectrosc.</source>
        <year>2003</year>
        <volume>11</volume>
        <fpage>193</fpage>
        <lpage>210</lpage>
        <pub-id pub-id-type="doi">10.1255/jnirs.366</pub-id>
      </element-citation>
    </ref>
    <ref id="B32-sensors-21-08184">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Polder</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>van der Heijden</surname>
            <given-names>G.W.</given-names>
          </name>
          <name>
            <surname>Young</surname>
            <given-names>I.T.</given-names>
          </name>
        </person-group>
        <article-title>Spectral image analysis for measuring ripeness of tomatoes</article-title>
        <source>Trans. ASAE</source>
        <year>2002</year>
        <volume>45</volume>
        <fpage>1155</fpage>
        <pub-id pub-id-type="doi">10.13031/2013.9924</pub-id>
      </element-citation>
    </ref>
    <ref id="B33-sensors-21-08184">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Walia</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Interactive Visualization of Hyperspectral Images based on Neural Networks</article-title>
        <source>IEEE Comput. Graph. Appl.</source>
        <year>2021</year>
        <volume>41</volume>
        <fpage>57</fpage>
        <lpage>66</lpage>
        <pub-id pub-id-type="doi">10.1109/MCG.2021.3097730</pub-id>
        <?supplied-pmid 34280091?>
        <pub-id pub-id-type="pmid">34280091</pub-id>
      </element-citation>
    </ref>
    <ref id="B34-sensors-21-08184">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Simonyan</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zisserman</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Very deep convolutional networks for large-scale image recognition</article-title>
        <source>arXiv</source>
        <year>2014</year>
        <pub-id pub-id-type="arxiv">1409.1556</pub-id>
      </element-citation>
    </ref>
    <ref id="B35-sensors-21-08184">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Bao</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Near-infrared hyperspectral imaging combined with deep learning to identify cotton seed varieties</article-title>
        <source>Molecules</source>
        <year>2019</year>
        <volume>24</volume>
        <elocation-id>3268</elocation-id>
        <pub-id pub-id-type="doi">10.3390/molecules24183268</pub-id>
        <?supplied-pmid 31500333?>
        <pub-id pub-id-type="pmid">31500333</pub-id>
      </element-citation>
    </ref>
    <ref id="B36-sensors-21-08184">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Araújo</surname>
            <given-names>M.C.U.</given-names>
          </name>
          <name>
            <surname>Saldanha</surname>
            <given-names>T.C.B.</given-names>
          </name>
          <name>
            <surname>Galvao</surname>
            <given-names>R.K.H.</given-names>
          </name>
          <name>
            <surname>Yoneyama</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Chame</surname>
            <given-names>H.C.</given-names>
          </name>
          <name>
            <surname>Visani</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>The successive projections algorithm for variable selection in spectroscopic multicomponent analysis</article-title>
        <source>Chemom. Intell. Lab. Syst.</source>
        <year>2001</year>
        <volume>57</volume>
        <fpage>65</fpage>
        <lpage>73</lpage>
        <pub-id pub-id-type="doi">10.1016/S0169-7439(01)00119-8</pub-id>
      </element-citation>
    </ref>
    <ref id="B37-sensors-21-08184">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wold</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Esbensen</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Geladi</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Principal component analysis</article-title>
        <source>Chemom. Intell. Lab. Syst.</source>
        <year>1987</year>
        <volume>2</volume>
        <fpage>37</fpage>
        <lpage>52</lpage>
        <pub-id pub-id-type="doi">10.1016/0169-7439(87)80084-9</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="sensors-21-08184-f001">
    <label>Figure 1</label>
    <caption>
      <p>The overall workflow of proposed platform: (<bold>a</bold>) seed sample placement; (<bold>b</bold>) hypercube generation; (<bold>c</bold>) seed segmentation; (<bold>d</bold>) reflectance curves extraction.</p>
    </caption>
    <graphic xlink:href="sensors-21-08184-g001" position="float"/>
  </fig>
  <fig position="float" id="sensors-21-08184-f002">
    <label>Figure 2</label>
    <caption>
      <p>Hyperspectral imaging (HIS) system: (<bold>a</bold>) <italic toggle="yes">HyperSeed</italic> platform; (<bold>b</bold>) a generated hypercube of test-case seed samples.</p>
    </caption>
    <graphic xlink:href="sensors-21-08184-g002" position="float"/>
  </fig>
  <fig position="float" id="sensors-21-08184-f003">
    <label>Figure 3</label>
    <caption>
      <p>The GUI of <italic toggle="yes">HyperSeed</italic> software: (1) path region to define the path for input and output files; (2) loading region to load the dataset of interest for visualization; (3) setting region for parameter setting; (4) log region to display progress information; (5) visualization region for the sliced image and the corresponding histogram; (6) slider to specify wavelength for visualization; (7) visualization region for segmentation results; (8) buttons for initiating the image processing.</p>
    </caption>
    <graphic xlink:href="sensors-21-08184-g003" position="float"/>
  </fig>
  <fig position="float" id="sensors-21-08184-f004">
    <label>Figure 4</label>
    <caption>
      <p>Seed segmentation: (<bold>a</bold>) the raw hyperspectral image of rice seeds; (<bold>b</bold>) the initial segmentation results showing overlapping region of seeds; (<bold>c</bold>) the refined segmentation results clearly defining seed boundaries.</p>
    </caption>
    <graphic xlink:href="sensors-21-08184-g004" position="float"/>
  </fig>
  <fig position="float" id="sensors-21-08184-f005">
    <label>Figure 5</label>
    <caption>
      <p>(<bold>a</bold>) Sample extraction and (<bold>b</bold>) network architecture of 3D CNN.</p>
    </caption>
    <graphic xlink:href="sensors-21-08184-g005" position="float"/>
  </fig>
  <fig position="float" id="sensors-21-08184-f006">
    <label>Figure 6</label>
    <caption>
      <p>The 2D images of seeds of maize, rice, sorghum, and wheat (<bold>a</bold>–<bold>d</bold>); and the corresponding segmentation results with each seed labeled with the corresponding index (<bold>e</bold>–<bold>h</bold>).</p>
    </caption>
    <graphic xlink:href="sensors-21-08184-g006" position="float"/>
  </fig>
  <fig position="float" id="sensors-21-08184-f007">
    <label>Figure 7</label>
    <caption>
      <p>The averaged spectral curves of seeds in control and HS groups (<italic toggle="yes">n</italic> = 100 seeds for each group).</p>
    </caption>
    <graphic xlink:href="sensors-21-08184-g007" position="float"/>
  </fig>
  <fig position="float" id="sensors-21-08184-f008">
    <label>Figure 8</label>
    <caption>
      <p>The ground truth and the predicted results using different models: (<bold>a</bold>) the ground truth; (<bold>b</bold>) the seed-based prediction results using SVM; (<bold>c</bold>) the pixel-based prediction results using SVM; and (<bold>d</bold>) the pixel-based prediction results using 3D CNN.</p>
    </caption>
    <graphic xlink:href="sensors-21-08184-g008" position="float"/>
  </fig>
  <fig position="float" id="sensors-21-08184-f009">
    <label>Figure 9</label>
    <caption>
      <p>The wavelengths sorted by normalized importance, which is computed and scaled based on the number for splitting the trees in the LightGBM model.</p>
    </caption>
    <graphic xlink:href="sensors-21-08184-g009" position="float"/>
  </fig>
  <table-wrap position="float" id="sensors-21-08184-t001">
    <object-id pub-id-type="pii">sensors-21-08184-t001_Table 1</object-id>
    <label>Table 1</label>
    <caption>
      <p>Number of samples in each dataset.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Reflectance<break/>Type</th>
          <th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Total <break/>Number</th>
          <th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Training Set</th>
          <th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Validation Set</th>
          <th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Test Set</th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Control</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HS</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Control</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HS</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Control</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HS</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Seed-based</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">200</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pixel-based</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">274,641</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">104,517</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">104,719</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5501</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5512</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">27,527</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26,865</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="sensors-21-08184-t002">
    <object-id pub-id-type="pii">sensors-21-08184-t002_Table 2</object-id>
    <label>Table 2</label>
    <caption>
      <p>The metrics of each model.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Model</th>
          <th colspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Metrics on Test Samples</th>
          <th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Seed Group<break/>Prediction Accuracy</th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <italic toggle="yes">Accuracy</italic>
          </th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <italic toggle="yes">Precision</italic>
          </th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <italic toggle="yes">Recall</italic>
          </th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <italic toggle="yes">F-score</italic>
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Seed-based SVM</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.00%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.00%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.33%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.94%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.00%</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pixel-based SVM</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.67%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.36%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.30%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.32%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.50%</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3D CNN</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.21%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.83%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.18%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.37%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.50%</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
