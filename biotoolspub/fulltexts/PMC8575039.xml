<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Brief Bioinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Brief Bioinform</journal-id>
    <journal-id journal-id-type="publisher-id">bib</journal-id>
    <journal-title-group>
      <journal-title>Briefings in Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1467-5463</issn>
    <issn pub-type="epub">1477-4054</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8575039</article-id>
    <article-id pub-id-type="pmid">34368836</article-id>
    <article-id pub-id-type="doi">10.1093/bib/bbab297</article-id>
    <article-id pub-id-type="publisher-id">bbab297</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Problem Solving Protocol</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepFeature: feature selection in nonimage data using convolutional neural network</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-7668-3501</contrib-id>
        <name>
          <surname>Sharma</surname>
          <given-names>Alok</given-names>
        </name>
        <!--alok.fj@gmail.com-->
        <aff><institution>Laboratory for Medical Science Mathematics, RIKEN Center for Integrative Medical Sciences</institution>, Yokohama 230-0045, <country country="JP">Japan</country></aff>
        <xref rid="afn1" ref-type="author-notes"/>
        <xref rid="cor1" ref-type="corresp"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1441-0604</contrib-id>
        <name>
          <surname>Lysenko</surname>
          <given-names>Artem</given-names>
        </name>
        <!--artem.lysenko@riken.jp-->
        <aff><institution>Laboratory for Medical Science Mathematics, RIKEN Center for Integrative Medical Sciences</institution>, Yokohama 230-0045, <country country="JP">Japan</country></aff>
        <xref rid="afn1" ref-type="author-notes"/>
        <xref rid="cor1" ref-type="corresp"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-7095-7332</contrib-id>
        <name>
          <surname>Boroevich</surname>
          <given-names>Keith A</given-names>
        </name>
        <!--keith.boroevich@riken.jp-->
        <aff><institution>Laboratory for Medical Science Mathematics, RIKEN Center for Integrative Medical Sciences</institution>, Yokohama 230-0045, <country country="JP">Japan</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-4225-0879</contrib-id>
        <name>
          <surname>Vans</surname>
          <given-names>Edwin</given-names>
        </name>
        <!--vans.edw@gmail.com-->
        <aff><institution>STEMP, University of the South Pacific</institution>, Suva, Fiji</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tsunoda</surname>
          <given-names>Tatsuhiko</given-names>
        </name>
        <!--tsunoda@bs.s.u-tokyo.ac.jp-->
        <aff><institution>Laboratory for Medical Science Mathematics, Department of Biological Sciences, Graduate School of Science, The University of Tokyo</institution>, Tokyo 113-0033, <country country="JP">Japan</country></aff>
        <xref rid="cor1" ref-type="corresp"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="cor1">Corresponding authors: Alok Sharma, Laboratory for Medical Science Mathematics, RIKEN Center for Integrative Medical Sciences, Yokohama 230-0045, Japan; Institute for Integrated and Intelligent Systems, Griffith University, QLD-4111, Australia; STEMP, University of the South Pacific, Suva, Fiji. E-mail: <email>alok.fj@gmail.com</email>; Artem Lysenko, Laboratory for Medical Science Mathematics, RIKEN Center for Integrative Medical Sciences, Yokohama 230-0045, Japan. E-mail: <email>artem.lysenko@riken.jp</email>; Tatsuhiko Tsunoda, Laboratory for Medical Science Mathematics, Department of Biological Sciences, Graduate School of Science, The University of Tokyo, Tokyo 113-0033, Japan; Laboratory for Medical Science Mathematics, RIKEN Center for Integrative Medical Sciences, Yokohama 230-0045, Japan; Department of Medical Science Mathematics, Medical Research Institute, Tokyo Medical and Dental University, Tokyo 113-8510, Japan; CREST, JST, Tokyo 113-0033, Japan. E-mail: <email>tsunoda@bs.s.u-tokyo.ac.jp</email></corresp>
      <fn id="afn1">
        <p>Alok Sharma and Artem Lysenko authors contributed equally to this work.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="collection">
      <month>11</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2021-08-06">
      <day>06</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>06</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <volume>22</volume>
    <issue>6</issue>
    <elocation-id>bbab297</elocation-id>
    <history>
      <date date-type="received">
        <day>11</day>
        <month>5</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>30</day>
        <month>6</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>14</day>
        <month>7</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Â© The Author(s) 2021. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="bbab297.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>Artificial intelligence methods offer exciting new capabilities for the discovery of biological mechanisms from raw data because they are able to detect vastly more complex patterns of association that cannot be captured by classical statistical tests. Among these methods, deep neural networks are currently among the most advanced approaches and, in particular, convolutional neural networks (CNNs) have been shown to perform excellently for a variety of difficult tasks. Despite that applications of this type of networks to high-dimensional omics data and, most importantly, meaningful interpretation of the results returned from such models in a biomedical context remains an open problem. Here we present, an approach applying a CNN to nonimage data for feature selection. Our pipeline, DeepFeature, can both successfully transform omics data into a form that is optimal for fitting a CNN model and can also return sets of the most important genes used internally for computing predictions. Within the framework, the Snowfall compression algorithm is introduced to enable more elements in the fixed pixel framework, and region accumulation and element decoder is developed to find elements or genes from the class activation maps. In comparative tests for cancer type prediction task, DeepFeature simultaneously achieved superior predictive performance and better ability to discover key pathways and biological processes meaningful for this context. Capabilities offered by the proposed framework can enable the effective use of powerful deep learning methods to facilitate the discovery of causal mechanisms in high-dimensional biomedical data.</p>
    </abstract>
    <kwd-group>
      <kwd>Feature selection</kwd>
      <kwd>Non-image data</kwd>
      <kwd>Convolutional neural network</kwd>
      <kwd>Omics data</kwd>
      <kwd>DeepInsight</kwd>
    </kwd-group>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>JST CREST</institution>
          </institution-wrap>
        </funding-source>
        <award-id>JPMJCR 1412</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>JSPS KAKENHI</institution>
          </institution-wrap>
        </funding-source>
        <award-id>JP17H06307</award-id>
        <award-id>JP17H06299</award-id>
        <award-id>JP20H03240</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Ministry of Education, Culture, Sports, Science and Technology of Japan</institution>
          </institution-wrap>
        </funding-source>
        <award-id>JP16H06299</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="12"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <title>Introduction</title>
    <p>Gene set selection facilitates interpretation of high-dimensional omics data by reducing it to components of greatest relevance and then allowing them to be linked to specific functional themes (e.g. using methods like process and pathway enrichment analysis) and to extract meaningful clues for clarifying biological mechanisms. Other important applications include disease diagnostics, where the goal is to produce gene sets small enough to be used for diagnostic tests in clinical practice.</p>
    <p>Traditional machine learning (ML) algorithms (such as support vector machines [<xref rid="ref1" ref-type="bibr">1</xref>], random forest, RF [<xref rid="ref2" ref-type="bibr">2</xref>] and logistic regression [<xref rid="ref3" ref-type="bibr">3</xref>]) are the ones most commonly applied in classification and feature selection (or gene selection) of nonimage data. Primarily, a <inline-formula><tex-math id="M1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$d\times 1$\end{document}</tex-math></inline-formula> column vector is supplied to an ML algorithm to find a smaller set of features and/or to classify samples into one of defined classes. In the medical field, ever increasing data complexity is pushing the limits of ML algorithms to extract relevant information for phenotype identification related to disease diagnosis and analysis. In this respect, the selection of a small subset of critical elements or genes from a larger set has become a critical step. The element or gene selection (also known as feature selection) problem is not limited to genomic data analysis, but is an important process in many areas of research. The reliability of ML algorithms to find a subset of genes is mostly determined by the feature selection, feature extraction and classification steps.</p>
    <p>On the other hand, convolutional neural networks (CNNs) are a class of deep learning architectures that have shown promising results and gained widespread attention in all types image analysis [<xref rid="ref4" ref-type="bibr">4â14</xref>]. CNN takes an input image (a pâÃâq feature matrix) and through its hidden layers conducts feature extraction and classification (note for 1-dimensional (D) CNN the input is not of <inline-formula><tex-math id="M2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathrm{p}\times \mathrm{q}$\end{document}</tex-math></inline-formula> size as the applied input is not an image sample). The 2-dimensional CNN is generally referred to as a CNN and this paper will also follow suit. One of the key advantages of CNNs are their high efficiency, i.e. fewer samples and less training time are needed to achieve good levels of performance. This led to their high popularity in a myriad of cutting-edge commercial applications (e.g. driverless cars). A CNN has several advantages: it automatically derives important features from spatially coherent pixels, it finds higher-order image statistics and nonlinear correlations, it requires less neurons as its convolution architecture can processes data for its receptive fields (or small subareas), allowing a deeper network with fewer parameters and its receptive fields share the coefficients and biases, reducing the memory footprint [<xref rid="ref15" ref-type="bibr">15</xref>]. In a local region, an image is comprised of spatially coherent pixels; i.e. similar information is shared by the pixels near each other. CNNs take into account the neighborhood information by extracting features from the adjacent pixels. On the other hand, ML techniques discard the neighborhood information and assume every element of a sample to be independent. Therefore, to get the maximum performance from a CNN, the adjacent pixels of a 2D input feature matrix should have reasonable coherence. Fortunately, for CNNs, the images utilized are usually a representation of physical entities and, therefore, do not require pixel rearrangement, as the lenses of camera correctly pass the appropriate light shades of animate or inanimate objects to the pixels. Previous attempts to apply CNN to nonimage data were restricted to 1D CNN architectures [<xref rid="ref16" ref-type="bibr">16â18</xref>]. The input to 1D CNNs are in the form of feature vectors and therefore they cannot deal with images.</p>
    <p>One possible way CNNs can process nonimage samples is by first converting it to an image sample considering spatially coherent pixels in its local regions, i.e. the pixels close to each other share similar information sometimes with patterns. The arbitrary arrangement of pixel locations can induce an unfavorable impact on the feature extraction and classification performance of CNN architecture. Therefore, the order of neighboring pixels in an image utilized by CNN is no longer independent as they were in ML techniques [<xref rid="ref19" ref-type="bibr">19</xref>]. The DeepInsight approach [<xref rid="ref19" ref-type="bibr">19</xref>] pioneered a variant of this strategy that used t-SNE [<xref rid="ref20" ref-type="bibr">20</xref>] for element arrangement, followed by mappings, feature extraction and classification steps. The element arrangement is done by positioning elements or genes within a 2D pixel frame based on their relative similarities, followed by the mapping of element values onto these locations. This approach ubiquitously transforms nonimage samples into images suitable for CNNs. To our knowledge, it was the first approach to convert various kinds of nonimage data to image forms for the application of CNN architecture. ButuriviÄ and MiljkoviÄ [<xref rid="ref21" ref-type="bibr">21</xref>] introduced a variant of DeepInsight approach where tabular data (generally a nonimage data in the arrangement having rows and columns) for CNN (with ResNet architecture) is used by transforming rows of tabular data as an image filter, and then by applying it to a fixed-base image. They applied their approach to gene expression data derived from blood samples of patients with bacterial or viral infections and showed that this pipeline can outperform many ML algorithms. Kanber [<xref rid="ref22" ref-type="bibr">22</xref>] applied the DeepInsight approach to the sparse data of the MINST database of 70k samples and showed it had superior performance than a state-of-the-art ML (RF) method. DeepInsight approach has been applied in various other projects [<xref rid="ref23" ref-type="bibr">23â42</xref>].</p>
    <p>The usability of DeepInsight based model has also been noticed in data science online platform (such as <ext-link xlink:href="http://Kaggle.com" ext-link-type="uri">Kaggle.com</ext-link>). Recently, a competition was organized by the Connectivity Map, a project within the Broad Institute of MIT and Harvard, the Laboratory for Innovation Science at Harvard (LISH) and the NIH Common Funds Library of Integrated Network-Based Cellular Signatures (LINCS), on the <ext-link xlink:href="http://Kaggle.com" ext-link-type="uri">Kaggle.com</ext-link> platform (<ext-link xlink:href="https://www.kaggle.com/c/lish-moa/overview" ext-link-type="uri">https://www.kaggle.com/c/lish-moa/overview</ext-link>). The title of the competition was Mechanisms of Action (MoA) predictions. The organizers posed a problem where it was required to develop an algorithm that can classify drugs based on their biological activities. A total of 4373 teams participated and submitted their models. The winning team of Peng <italic toggle="yes">etÂ al.</italic> [<xref rid="ref43" ref-type="bibr">43</xref>] applied DeepInsight feature mapping with EfficientNet-B3 NS model and ResNeSt model with five other models to score rank 1 out of the total of 4373 teams (<ext-link xlink:href="https://www.kaggle.com/c/lish-moa/discussion/201510" ext-link-type="uri">https://www.kaggle.com/c/lish-moa/discussion/201510</ext-link>). The implementation and description of DeepInsight part by Peng <italic toggle="yes">etÂ al.</italic> [<xref rid="ref43" ref-type="bibr">43</xref>] can be accessed at <ext-link xlink:href="https://www.kaggle.com/c/lish-moa/discussion/195378" ext-link-type="uri">https://www.kaggle.com/c/lish-moa/discussion/195378</ext-link>.</p>
    <p>To date there are very few studies about how to perform feature selection by CNN for nonimage samples, such as finding a subset of genes. In this work, we focus on developing a methodology to show that gene selection can be done using CNNs. An obvious comparable method for this type of analysis is differential gene expression analysis (DGE) and therefore it is important to highlight some key differences between it and the proposed DeepFeature method. In most typical types of DEG analysis genes are processed individually and the comparisons are made between two or more conditions based on some variation of a linear model. The analysis would return all the genes found to be substantially different, though ability to meet the selection threshold is heavily influenced by the variance and magnitude of the expression, which may not necessarily align with the overall importance of these genes to the condition of interest. In contrast, deep learning classifier at the core of the DeepFeature model performs selection by considering all of the available genes simultaneously (which allows collinearity to be exploited to compensate for noise) and the priority is solely given to the overall predictive importance of the gene within the context of the selected set. Due to these differences in selection criteria DeepFeature can offer a rich, complementary perspective to that of the traditional methods. As illustrated by the analysis reported in this paper, DeepFeature-selected gene sets are both very different from more traditional approaches like lasso and analysis of variance (ANOVA), but also appeared to be better aligned with meaningful biological mechanisms and therefore consistently achieved higher enrichment for key pathways and functional groups.</p>
    <p>The hidden layers of CNNs can reveal complex mechanisms (such as pathways) for nonimage samples. The development of CNNs is inspired by biological processes to perform feature extraction from image patterns [<xref rid="ref14" ref-type="bibr">14</xref>, <xref rid="ref44" ref-type="bibr">44</xref>, <xref rid="ref45" ref-type="bibr">45</xref>]. Both in industries (e.g. as driverless cars) and academia, the usage of CNN is becoming increasingly important. It has been primarily used for image processing but now CNNs are expanded to many fields. Numerous research can be cited showing CNNs reveal a complex pattern in the data to achieve superior performance [<xref rid="ref14" ref-type="bibr">14</xref>, <xref rid="ref46" ref-type="bibr">46</xref>, <xref rid="ref47" ref-type="bibr">47</xref>]. It is possible that the proper utilization of CNN with DeepInsight feature mapping can also reveal complex mechanisms for nonimage samples. The same methodology can be extended to other kinds of nonimage cases and is not restricted to genomic or transcriptomic data. To this end, the proposed Feature Selection algorithm via CNNs for nonimage samples, abbreviated as DeepFeature, was developed (<xref rid="f1" ref-type="fig">Figure 1</xref>). The DeepFeature approach encompasses four main steps: element arrangement, feature selection, feature extraction and classification (see <xref rid="sup1" ref-type="supplementary-material">Supplementary File 1</xref> for the definition about these terms). The steps of DeepFeature are discussed in the Materials and methods section.</p>
    <fig position="float" id="f1">
      <label>
Figure 1
</label>
      <caption>
        <p>An overall DeepFeature procedure for feature selection using CNN.</p>
      </caption>
      <graphic xlink:href="bbab297f1" position="float"/>
    </fig>
    <p>The innovation and/or contributions of this paper are as follows. DeepFeature pipeline is introduced where feature selection can be performed for nonimage samples (or tabular data) via CNNs. Snowfall compression algorithm is developed to allow more elements of data in a fixed pixel frame. Region accumulation and element decoder (READ) are introduced to find genes or elements from the activation maps. In addition to delivering high-classification performance, DeepFeature also offers a powerful means for the identification of biologically relevant gene sets. When applied to the task of cancer classification, our DeepFeature approach was able to identify coherent sets with significant enrichment of genes in cancer-associated pathways from MSigDB and a gold-standard reference set. Further analysis of these results suggested biologically meaningful connections of potential interest to our understanding of the differences between major cancer types.</p>
  </sec>
  <sec id="sec2">
    <title>Results</title>
    <p>The results are produced by following an overall procedure of DeepFeature (<xref rid="f1" ref-type="fig">Figure 1</xref>). The model takes nonimage data, e.g. transcriptomic or RNA-seq data, and finds a subset of genes or elements via CNN. The model carries the following steps: image transformation by DeepInsight, optional Snowfall compression to accommodate more elements in a pixel-frame, classification via SqueezeNet model of CNN architecture [<xref rid="ref48" ref-type="bibr">48</xref>], identification of activation maps using CAM [<xref rid="ref49" ref-type="bibr">49</xref>], discovery of the overall activated regions for a category or class via the region accumulation step and the decoding of gene subsets by the element decoder procedure.</p>
    <p>First, we compared the number of selected genes and classification performance for the TCGA cancer study identification task using different ML algorithms (<xref rid="TB1" ref-type="table">Table 1</xref>). Lasso gave 1018 nonzero coefficients and as a result it discarded all other elements or genes. To perform enrichment analysis, we limited the size of gene subset to be around 1000â2000. In this respect, for ANOVA and variable genes method, 1000 genes were extracted. These techniques did not provide class specific gene subsets, however, gave one subset of genes for the all 10 cancer studies. The classification accuracy was computed on gene subsets using the RF classifier.</p>
    <p>On the other hand, the logistic regression+RF method generates models for each cancer study <inline-formula><tex-math id="M3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$i$\end{document}</tex-math></inline-formula> (for <italic toggle="yes">i</italic>â=â1,2, â¦ 10) with coefficients <inline-formula><tex-math id="M4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${w}_j^i$\end{document}</tex-math></inline-formula>, where <inline-formula><tex-math id="M5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$j=1,2,\dots, d$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M6">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$d$\end{document}</tex-math></inline-formula> depicts the number of features or genes. If we take the average coefficients over 10 classes and find top <inline-formula><tex-math id="M7">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$r$\end{document}</tex-math></inline-formula> features, we can then get one subset of genes for all the 10 cancer studies. However, it will not be as useful because in that case it will not be possible to obtain class specific features. Considering each model separately, we take an absolute value or modulus of the coefficients and arrange them in descending order to find specific features related to each cancer study (details are discussed in the Materials and methods section).</p>
    <p>Therefore, these algorithms, with the exception of logistic regression+RF, provide one subset of genes for all the categories. However, in some cases pairwise analysis is possible (e.g. by using post-hoc Tuckeyâs test [<xref rid="ref50" ref-type="bibr">50</xref>]). In such a case, a total of <inline-formula><tex-math id="M8">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${m}\choose{2}$\end{document}</tex-math></inline-formula> subsets are to be generated, where <inline-formula><tex-math id="M9">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$m$\end{document}</tex-math></inline-formula> is the number of categories. On the other hand, logistic regression+RF provides a separate model for each cancer study, and with some modifications, it is possible to obtain class dependent features. This way, it is possible to find a subset of genes corresponding to a particular cancer class. Other methods, such as Seurat [<xref rid="ref51" ref-type="bibr">51</xref>], pcaReduce [<xref rid="ref52" ref-type="bibr">52</xref>], TSCAN [<xref rid="ref53" ref-type="bibr">53</xref>] and SINCERA [<xref rid="ref54" ref-type="bibr">54</xref>], also produce gene subsets. To make sure the evaluation is meaningful, DeepFeature method was only compared with approaches that are also capable of returning individual sets of features for each class.</p>
    <p>For DeepFeature, three different visualization tools were used/developed to plot feature locations on a 2D-plane. These methods were (1) t-SNE, (2) t-SNE with Snowfall and (3) PHATE [<xref rid="ref55" ref-type="bibr">55</xref>] (see <xref rid="sup1" ref-type="supplementary-material">Supplementary File 1</xref> for details).</p>
    <p>DeepFeature achieved 97â98% classification accuracy on the independent test set when using t-SNE (with or without Snowfall algorithm), and 96.8% accuracy when using PHATE.</p>
    <p>For DeepFeature with t-SNE and Snowfall, four distances (Chebychev, correlation, cosine and Hamming) were used which gave a subset of 5228 genes (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S1.1</xref>â<xref rid="sup1" ref-type="supplementary-material">S1.3</xref> for hyperparameter details, <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S1.2</xref> for an illustration of corresponding activations and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S1.3</xref> for gene subsets per cancer study). This subset of genes was further processed with DeepFeature using Hamming distance which gave 1806 genes. The details about DeepFeature execution and results at various stages that eventually led to the selection of 1806 genes can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary</xref><xref rid="sup1" ref-type="supplementary-material">File 1</xref>.</p>
    <p>The same four distances and additional iteration were also used in the case of DeepFeature with t-SNE (no Snowfall). The two resulting subsets were of 1914 genes and 962 genes (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1.4</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Figures S1.4</xref> and <xref rid="sup1" ref-type="supplementary-material">S1.5</xref> in <xref rid="sup1" ref-type="supplementary-material">Supplementary File 1</xref> for details). Two different sizes of gene subsets were generated from a particular model to examine whether gene subset size effects pathway enrichment.</p>
    <p>Lastly, the PHATE algorithm was used for visualization with DeepFeature, which gave 1569 genes (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1.5</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Figures S1.6</xref> and <xref rid="sup1" ref-type="supplementary-material">S1.7</xref> in <xref rid="sup1" ref-type="supplementary-material">Supplementary File 1</xref> for details).</p>
    <p>Different visualization algorithms (t-SNE with or without Snowfall and PHATE) were applied in DeepFeature and the resulting gene subsets were examined for enrichment of genes in pathways of interest.</p>
    <p>DeepFeature is also capable of finding different gene subsets belonging to different cancer studies. As discussed above, applying different visualization tools resulted in different gene subsets of sizes 962, 1569, 1806 and 1914 genes for each of the 10 cancer studies. We applied enrichment analysis on these 962, 1569, 1806 and 1914 gene subsets. DeepFeature using t-SNEâ+âSnowfall gave a 1806 gene subset and the number of genes for each cancer study ranged between 930 and 1450 (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S2.1</xref> in <xref rid="sup1" ref-type="supplementary-material">Supplementary File 2</xref>). DeepFeature using t-SNE gave the 962 and 1914 gene subsets. In the case of 962 gene set, between 307 and 679 genes per cancer study were detected (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S2.1</xref> in <xref rid="sup1" ref-type="supplementary-material">Supplementary File 2</xref>). For the 1914 gene set, between 405 and 1571 genes per cancer subtype were found. DeepFeature using PHATE gave 1569 genes, and in this case between 24 and 621 genes per cancer study were found (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S2.1</xref>). DeepFeature using t-SNEâ+âSnowfall more consistently identified a common set of genes selected for all cancer studies (<xref rid="f2" ref-type="fig">Figure 2A</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S2.2</xref> for performance evaluation of all the algorithms; and, <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S2.3</xref> for overlap of gene subsets among all the methods including gene annotation). DeepFeature using t-SNE (962 and 1914 genes) performed satisfactorily, however, it did not outperform DeepFeature with t-SNEâ+âSnowfall (<xref rid="f2" ref-type="fig">Figure 2B</xref>). This shows that using Snowfall algorithm, the enrichment is improved while using DeepFeature. The PHATE visualization method for DeepFeature did not perform well and consistently showed inferior performance (<xref rid="f2" ref-type="fig">Figure 2B</xref>). The number of selected genes for PHATE (1569) was also higher than t-SNE (962), but the number of significantly enriched pathways was less than half for most of the cancer studies. Interestingly, in addition to this overall trend, logistic regression and DeepFeature appear to preferentially select the nonoverlapping sets of genes relative to each other. Still, DeepFeature was far better in recovering significantly enriched cancer-relevant pathways (<xref rid="f2" ref-type="fig">Figure 2B</xref>). This suggests both that the proposed algorithm is better at discovering biologically coherent groupings of genes and that most of these grouping also appear to be highly relevant to cancer-associated processes. Notably, sets of genes from DeepFeature are also depleted for housekeeping genes (<xref rid="f2" ref-type="fig">Figure 2A</xref>). As housekeeping genes usually tend to have stable expression across all cell types and tissues, this result is likely an indication that fewer false positive genes, which are unlikely to be of interest, were chosen.</p>
    <table-wrap position="float" id="TB1">
      <label>Table 1</label>
      <caption>
        <p>Classification accuracy and gene selection using ML and DeepFeature algorithms</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th align="left" rowspan="1" colspan="1">Machine learning methods</th>
            <th align="left" rowspan="1" colspan="1">#Genes selected</th>
            <th align="left" rowspan="1" colspan="1">Classification accuracy</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" rowspan="1" colspan="1">ANOVA+RF</td>
            <td align="left" rowspan="1" colspan="1">1000</td>
            <td align="left" rowspan="1" colspan="1">94.2%</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Lasso+RF</td>
            <td align="left" rowspan="1" colspan="1">1018</td>
            <td align="left" rowspan="1" colspan="1">96.0%</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Variable genes+RF</td>
            <td align="left" rowspan="1" colspan="1">1000</td>
            <td align="left" rowspan="1" colspan="1">92.8%</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Logistic regression+RF</td>
            <td align="left" rowspan="1" colspan="1">1051</td>
            <td align="left" rowspan="1" colspan="1">96.6%</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">DeepFeature (t-SNE)</td>
            <td align="left" rowspan="1" colspan="1">962</td>
            <td align="left" rowspan="1" colspan="1">97.6%</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">DeepFeature (t-SNE)</td>
            <td align="left" rowspan="1" colspan="1">1914</td>
            <td align="left" rowspan="1" colspan="1">96.6%</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">DeepFeature (t-SNE with SnowFall)</td>
            <td align="left" rowspan="1" colspan="1">1806</td>
            <td align="left" rowspan="1" colspan="1">97.9%</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">DeepFeature (PHATE)</td>
            <td align="left" rowspan="1" colspan="1">1569</td>
            <td align="left" rowspan="1" colspan="1">96.8%</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <fig position="float" id="f2">
      <label>
Figure 2
</label>
      <caption>
        <p>(<bold>A</bold>) Commonality of gene selected by DeepFeature (t-SNEâ+âSnowfall) and logistic regression techniques. Each row represents a gene and yellow denotes that gene was selected for that cancer study (column). Housekeeping genes, as identified by two annotation sources, are shown in blue. (<bold>B</bold>) Enrichment analysis in MSigDB C6 gene sets. The number of significantly enriched cancer sets for genes selected by DeepFeature (orange, pink, purple and light green) and logistic regression (dark green) for each of the 10 cancer studies. DeepFeature (orange) produced higher counts than logistic regression for MSigDB for all the cancer studies.</p>
      </caption>
      <graphic xlink:href="bbab297f2" position="float"/>
    </fig>
    <p>To compare these results with previously proposed gene sets suitable for identification of cancer types, an additional comparison was made with a âhallmark gene setsâ and âchemical and genetic perturbationsâ collections from the MSigDB database (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S2.4</xref>) that aim to characterize coherently expressed biological processes and signatures for key biological and clinical sets, respectively. Although the overlap with this dataset was somewhat marginal, the topmost enriched pathways recovered from this set were still highly relevant. From the hallmarks collection, most significant gene set was âEpithelial mesenchyma transitionâ, which is a core process in both tissue differentiation and initiation of cancer metastasis. Other cell proliferation and differentiation processes near the top of the list were angiogenesis (3rd highest) and myogeneis (6th); notably the former of these encompasses genes responsible for regulation of new blood vessel formation, which is crucial for tumor growth and cancer progression. Core cancer signaling pathways mediated by KRAS, TNF-alpha and JAK/STAT proteins were also found near the very top of the list. Other recovered mechanisms commonly associated with cancer were coagulation (2nd highest, typically abnormally enhanced in cancer patients) inflammatory response (7th, associated with anti-tumor immune response) and hypoxia (5th, a common condition that occurs in regions of tumor that grow too rapidly to ensure adequate oxygen supply). In summary, this analysis has confirmed that DeepFeature method was able to consistently identify important processes well-known to be highly relevant for cancer and therefore may be also be potentially promising for the discovery of new candidate pathways and genes.</p>
    <p>To explore possible underlying biological meaning behind these selected gene groupings, each set of cancer study-specific genes was evaluated for enrichment of specific pathways in Reactome database (<xref rid="sup2" ref-type="supplementary-material">Supplementary File 3</xref>). This analysis revealed that DeepFeature consistently identified genes belonging to the âExtracellular matrix organizationâ pathway (Reactome: R-HSA-1474244), which was the top group for all cancer studiesâboth by number of genes and enrichment significance (see <xref rid="sup2" ref-type="supplementary-material">Supplementary File 3</xref> for top pathways). The importance of this pathway is in line with current understanding of its wider biological role, where it is known to be both highly diverse in its expression across different tissues and subject to dysregulation in cancer [<xref rid="ref56" ref-type="bibr">56</xref>]. Another two pathways that were highly enriched across all cancer studies were âSignaling by Receptor Tyrosine Kinasesâ (R-HSA-9006934) and âGPCR ligand bindingâ (R-HSA-500792). Tyrosine kinases mediate both cell proliferation and apoptosis, and their importance for multiple types of cancers is relatively well-studied [<xref rid="ref57" ref-type="bibr">57</xref>]. Interestingly, although it has been observed that different GPCRs are expressed in different cancers and some of them may be suitable for use as biomarkers [<xref rid="ref58" ref-type="bibr">58</xref>], overall roles of specific genes in cancer is not well understood and the ligands of many GPCRs still have not been identified [<xref rid="ref59" ref-type="bibr">59</xref>]. Further analysis of these DeepFeature results, like holistic analysis of which parts of specific pathways were found to be important for different types, may help to explain key cancer study specific differences, and allow to connect the GPCR signaling to other better understood mechanisms of oncogenesis.</p>
  </sec>
  <sec id="sec3">
    <title>Discussion</title>
    <p>The performance evaluation of DeepFeature is gauged by both finding significantly enriched pathways within the selected subset of genes and classification accuracy on the independent test set. As expected, promising results were obtained when compared with the state-of-the-art ML techniques. The gene enrichment results for DeepFeature were reasonably sound compared with the benchmarked ML methods. The classification accuracy on the independent test set was ~98%, which is better than the ML technique. This shows that deep learning architectures have a possibility to provide solutions for biomarker discovery, genomic analysis for a variety of input samples ranging from RNA-seq to various omics data. In general, this method is suitable for applications where given data is in a nonimage form.</p>
    <p>The three visualization methods used in the DeepFeature pipeline were t-SNE, PHATE and t-SNEâ+âSnowfall. Though the Snowfall algorithm adds distortion in the visualization of t-SNE, t-SNEâ+âSnowfall performed better than t-SNE as measured by gene enrichment analysis. The distortion is introduced when the image is transformed from a Cartesian coordinate system to the fixed size pixel framework, as many features overlap, i.e. various features having the same pixel location. This overlapping reduces the chance of a feature to be selected. In such a case, the application of Snowfall is to find nearby pixel locations for features and expose more features to selection. If we could have very large pixel frame size where no overlaps occur while transforming from Cartesian coordinate system to pixel frame, then perhaps the application of Snowfall will not be useful as distortion is added by it while adjusting the features in the nearby pixel locations. Nonetheless, in a given fixed size pixel framework, the application of Snowfall for DeepFeature was found to be useful in finding more meaningful features or genes. On the other hand, although PHATE [<xref rid="ref55" ref-type="bibr">55</xref>] has been promoted to be an advanced visualization technique, it did not perform well when compared with t-SNE in the DeepFeature pipeline. This inferior performance may be attributed to the error induced while converting from Cartesian coordinates system to pixel coordinates system as many elements are susceptible to overlap due to the fixed size of pixel frame.</p>
    <p>The feature selection results indicated that DeepFeature was much better than alternative algorithms at recovering biologically meaningful groups of genes that were relevant to classes (phenotypes) of interest. A notable advantage of the method is its ability to narrow down the set of candidate genes even in cases where the differences are very substantial, like different cancer types. On the contrary, under such circumstances approaches like DGE return very large number of significant hits and therefore are not useful in sufficiently reducing the candidate list(s) to allow meaningful interpretation. Therefore, the algorithm is likely to have great utility for tasks like prioritization of diagnostical signatures and interpretation of complex multi-omics data.</p>
    <p>It is now generally accepted that in order to fully understand the dynamic quintessential complexity of cancer, it is essential to profile and collect data using a wide array of possible methods. As a result, a typical study commonly needs to consider wide array of possible data types, possibly including images, clinical records, as well as protein-coding and noncoding RNA expression profiles and full genome sequencing. Understanding mechanisms and discovery of clinically relevant subtypes frequently require appropriate aligning of these different types of data and, most often, subsequent use of ML algorithms to identify meaningful patterns. Among current generation of approaches, deep learning-based methods offer the greatest flexibility and can potentially allow fuller automation and more comprehensive integration of these different types. In principle, by combining different specialized encoder layers, any types of data can be converted into a set of inter-compatible embeddings and used within the same ML system. This innate flexibility is difficult to match using other methods and is particularly valuable for mining complex multimodal datasets.</p>
    <p>Good correspondence of identified gene sets with known gold standard cancer-associated pathways is particularly promising as a means of interpreting the deep learning results from a biological perspective. Despite many recent advances, deep learning still commonly has a reputation for generating high quality but ultimately âblack boxâ models, where discovering how an algorithm arrived at a particular conclusion is very challenging. However, given that structures of key cancer pathways are very well understood, their high degree of overlap with DeepFeature results could be crucial for correctly contextualizing the importance of individual genes and offer possible explanations for why they were selected by the algorithm.</p>
  </sec>
  <sec id="sec4">
    <title>Materials and methods</title>
    <p>We obtained RNA-seq data from the TCGA project. To maintain large enough categories, only 10 cancer studies were considered, namely, TCGA-BRCA, TCGA-COAD, TCGA-HNSC, TCGA-KIRC, TCGA-LGG, TCGA-LUAD, TCGA-LUSC, TCGA-PRAD, TCGA-THCA and TCGA-UCEC. A total of 6280 HTSeq-FPKM-UQ expression files were downloaded using the GDC data transfer tool.</p>
    <p>From these, 64 files were removed for sharing submitter IDs, resulting in a final total of 6126 samples. The FPKM-UQ files contain expression for 60Â 483 genes. In this study, we only used the 19Â 086 genes classified as protein-coding genes by the HUGO Gene Nomenclature Committee (download date: 22 November 2019). Next, we will describe the DeepFeature model.</p>
    <p>An overall procedure for feature selection using CNN (<xref rid="f1" ref-type="fig">Figure 1</xref>). The model takes nonimage data, e.g. transcriptomic or RNA-seq data, and finds a subset of genes or elements via CNN. The transformation of nonimage samples to image samples is done following the element arrangement step of the DeepInsight model. A Snowfall compression algorithm is developed to fit more elements in a given pixel-frame to enable every possible gene to be part of the selection. Here three visualization methods are used: t-SNE, t-SNE with Snowfall and PHATE. Image samples obtained from the DeepInsight and Snowfall algorithms (if selected) are submitted to the CNN model (using SqueezeNet architecture). The feature extraction and classification are performed by CNN. Feature selection is performed collectively by the class activation maps (CAMs), region accumulation and element decoder (READ). CAM are used to find activations of each sample. The activations for individual samples are integrated to find active regions for a class or category at the region accumulation step. The accumulated regions (for one or all classes) define pixel locations of interest for categorization of samples. These selected pixels are decoded to provide a subset of elements at the element decoder step. If the number of selected genes is higher than the desired number of genes, then the whole procedure can be executed again with the selected genes as input to find further subsets of genes. Repeating these steps will reduce the number of selected genes. This way the feature selection is performed with DeepFeature method.</p>
    <sec id="sec5">
      <title>DeepFeature: feature selection for nonimage data using CNN</title>
      <p>This section defines the proposed DeepFeature methodology. The constituents of the model are (1) image transformation by DeepInsight, (2) Snowfall compression to enable more elements in a pixel-frame, (3) SqueezeNet model of CNN architecture, (4) CAM model to find activation maps, (5) region accumulation to obtain overall activated regions for a category or dataset and (6) element decoder to decode genes from active regions. These steps are discussed below.</p>
    </sec>
    <sec id="sec6">
      <title>DeepInsight: nonimage to image conversion for CNN</title>
      <p>DeepInsight transforms a nonimage sample to a well-organized image form by effectively arranging elements while considering neighborhood information. The feature extraction and classification tasks are done by CNN. DeepInsight integrates three steps: (1) element arrangement, (2) feature extraction and (3) classification. This approach of element arrangement can be useful in uncovering hidden mechanisms (e.g. pathways). In this way, the relative importance of features for assignment of samples to particular classes can be better understood. An input feature vector is transformed to a feature matrix using t-SNE [<xref rid="ref20" ref-type="bibr">20</xref>], kernel PCA [<xref rid="ref60" ref-type="bibr">60</xref>], PHATE [<xref rid="ref55" ref-type="bibr">55</xref>] or UMAP [<xref rid="ref61" ref-type="bibr">61</xref>], and then the smallest rectangle containing all the elements is found using the convex hull algorithm. A necessary rotation is performed to align the image, and then Cartesian coordinates are converted to pixel coordinates. After that, mapping of element values onto pixel locations is performed to construct an image of a feature vector. The details of image transformation procedure have been previously described in our earlier work [<xref rid="ref19" ref-type="bibr">19</xref>].</p>
    </sec>
    <sec id="sec7">
      <title>Snowfall compression algorithm</title>
      <p>If the dimensionality of a sample with <italic toggle="yes">d</italic> elements, <inline-formula><tex-math id="M10">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$x\in{\mathbb{R}}^d$\end{document}</tex-math></inline-formula>, is very large, then it becomes very difficult to place all the elements in a given pixel frame of size <inline-formula><tex-math id="M11">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$m\times n$\end{document}</tex-math></inline-formula>. Therefore, the question is, how to compress, such that all the elements can be arranged in the same pixel size, while maintaining the data topology. There are two ways of performing compression, quantized compression and non-quantized compression. In quantized compression, two or more elements can overlap, i.e. these elements will have an identical pixel location. In this case, the values of elements are averaged at that particular pixel location. On the other hand, in non-quantized compression, no overlap occurs and each element maintains a unique pixel location and thus there is no averaging of their values at a given location. The Snowfall algorithm is a non-quantized compression algorithm. However, depending upon the memory requirements of a given hardware by CNN, the size of the pixel frame can be adjusted such that all the elements are represented in the frame, but with quantized compression. See <xref rid="sup1" ref-type="supplementary-material">Supplementary File 4</xref> for details about the Snowfall compression algorithm.</p>
      <p>It is important to note that when an image in transformed from Cartesian coordinates to pixel coordinates, it brings distortion due to the limited or fixed size of the pixel frame. Due to limited size, many features may overlap on the same location and it would become difficult to perform feature selection. The Snowfall algorithm tries to find nearby empty pixel locations of such features so that overlapping can be minimized. Therefore, it tries to make features visible to CNN by relocating them to neighboring points thereby helping to perform feature selection.</p>
    </sec>
    <sec id="sec8">
      <title>CNN architecture for feature selection and classification</title>
      <p>Since class activation maps (CAMs) [<xref rid="ref49" ref-type="bibr">49</xref>] cannot be used for networks that have multiple fully connected layers at the output layer, we used SqueezeNet in this work [<xref rid="ref48" ref-type="bibr">48</xref>]. Other series nets (e.g. AlexNet, VGG-16 and VGG19) can also be used to find CAMs.</p>
      <p>The SqueezeNet architecture of DeepFeature has fixed input size (see <xref rid="f3" ref-type="fig">Figure 3A</xref>). The input image of size <inline-formula><tex-math id="M12">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N\times M$\end{document}</tex-math></inline-formula> is adjusted to <inline-formula><tex-math id="M13">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$227\times 227$\end{document}</tex-math></inline-formula> due to the input image size requirement of 227âÃâ227 the first convolutional layer of SqueezeNet (<italic toggle="yes">conv1</italic>). Therefore, the input image size would be <inline-formula><tex-math id="M14">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$227\times 227$\end{document}</tex-math></inline-formula>. The activation maps are retrieved from the last connected ReLu layer (<italic toggle="yes">ReLu_conv10</italic>). After training the CNN model on the optimal hyperparameters, each new sample could be categorized into one of the classes or phenotypes (<italic toggle="yes">new_classoutput</italic>) at the output layer. Here, an RNA-seq gene expression sample was first converted to an image by DeepInsight and Snowfall compression algorithm, and then used as an input. At the input, the RNAseq sample can be visualized and a particular region leading to its identification can be analyzed at ReLu layer. The activation map at ReLu layer defines which localities of an image are of interest for decision making process. This activation map has three colors in order of preference as red, yellow and blue. The red zone is the most active and blue is the least (<xref rid="f3" ref-type="fig">Figure 3A</xref>).</p>
      <fig position="float" id="f3">
        <label>
Figure 3
</label>
        <caption>
          <p>(<bold>A</bold>) SqueezeNet structure with 227âÃâ227 input image sample from DeepInsight and Snowfall algorithm, and corresponding class activation map at the output ReLu layer. (<bold>B</bold>) Region accumulation (RA) step for the DeepFeature method. (<bold>C</bold>) Element decoder for the DeepFeature method, the region R is supplied to the element decoder. A pixel under R could have three possibilities: a pixel representing only one gene g_m at location (a_1,b_1), a pixel representing multiple genes (g_1,g_2,â¦,g_k) at location (a_2,b_2), and a pixel that has no element, e.g. at location (a_3,b_3) (here a pixel value or Base is 1, see <xref rid="sup1" ref-type="supplementary-material">Supplementary File 1</xref> for further discussion).</p>
        </caption>
        <graphic xlink:href="bbab297f3" position="float"/>
      </fig>
      <p>CNN has various hyperparameters such as momentum, L2 regularization and learning rate. These hyperparameters are tuned on the training set, and the modelâs fitness is evaluated on the validation set by employing a Bayesian optimization technique for all the trials. The hyperparameters are selected to minimize the validation error. The test set is never been used in the training or model fitting steps. The chosen CNN hyperparameters produce the optimum performance on the validation set. Description of the parameters is further discussed in <xref rid="sup1" ref-type="supplementary-material">Supplementary File 1</xref>.</p>
    </sec>
    <sec id="sec9">
      <title>Class activation maps (CAMs)</title>
      <p>Zhou <italic toggle="yes">etÂ al.</italic> [<xref rid="ref49" ref-type="bibr">49</xref>] proposed CAMs which is an interesting addition of CNNs with global average pooling. CAMs reveals discriminatory image regions of a particular class or category of CNN used for classification. Here, the predicted scores of a category are mapped back to the previous convolutional layer to generate CAMs [<xref rid="ref49" ref-type="bibr">49</xref>]. The region of image used for classification by CNNs can be visually observed by CAMs. The CAMs are fitted in the last convolutional layer at spatial location to perform global average pooling. The CAM for class <inline-formula><tex-math id="M15">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$c$\end{document}</tex-math></inline-formula> is defined as<disp-formula id="deqn01"><label>(1)</label><tex-math id="M16">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} {M}_{c\left(x,y\right)}={\sum}_k{\omega}_k^c{f}_{k\left(x,y\right)} \end{equation*}\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M17">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${f}_k\big(x,y\big)$\end{document}</tex-math></inline-formula> depicts the activation of unit <inline-formula><tex-math id="M18">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$k$\end{document}</tex-math></inline-formula> in the last convolutional layer at spatial location <inline-formula><tex-math id="M19">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\big(x,y\big)$\end{document}</tex-math></inline-formula>, <inline-formula><tex-math id="M20">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\omega}_k^c$\end{document}</tex-math></inline-formula> is the weight corresponding to class <inline-formula><tex-math id="M21">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$c$\end{document}</tex-math></inline-formula> for unit <inline-formula><tex-math id="M22">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$k$\end{document}</tex-math></inline-formula>. The score for class <inline-formula><tex-math id="M23">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$c$\end{document}</tex-math></inline-formula> can be obtained by <inline-formula><tex-math id="M24">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${S}_c={\sum}_{x,y}{M}_c\big(x,y\big)$\end{document}</tex-math></inline-formula>.</p>
    </sec>
    <sec id="sec10">
      <title>Region accumulation</title>
      <p>The region accumulation (RA) step integrates regions of importance. Let a training set with <inline-formula><tex-math id="M25">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N$\end{document}</tex-math></inline-formula> samples be defined as <inline-formula><tex-math id="M26">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$S=\big\{{s}_1,{s}_2,\dots, {s}_N\big\}$\end{document}</tex-math></inline-formula>. Let the activation region corresponding to <inline-formula><tex-math id="M27">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N$\end{document}</tex-math></inline-formula> samples be given as <inline-formula><tex-math id="M28">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal{H}=\big\{{r}_1,{r}_2,\dots, {r}_N\big\}$\end{document}</tex-math></inline-formula>; i.e. cardinality of <inline-formula><tex-math id="M29">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal{H}$\end{document}</tex-math></inline-formula> is same as <inline-formula><tex-math id="M30">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$S$\end{document}</tex-math></inline-formula>; i.e. <inline-formula><tex-math id="M31">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\Big|\mathcal{H}\Big|=\mid S\mid$\end{document}</tex-math></inline-formula>. Let <inline-formula><tex-math id="M32">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$c$\end{document}</tex-math></inline-formula> be the number of categories (or phenotypes) defined as <inline-formula><tex-math id="M33">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\Omega =\big\{{\omega}_1,{\omega}_2,\dots, {\omega}_c\big\}$\end{document}</tex-math></inline-formula>. Each of the sample will have one of these categories; i.e. <inline-formula><tex-math id="M34">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${s}_i\in \Omega$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M35">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${r}_i\in \Omega$\end{document}</tex-math></inline-formula>, for <inline-formula><tex-math id="M36">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$i=1,2,\dots, N$\end{document}</tex-math></inline-formula>. The overall region of the training data <inline-formula><tex-math id="M37">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$S$\end{document}</tex-math></inline-formula> can be evaluated by performing <italic toggle="yes">union operation</italic> of individual regions; i.e. <inline-formula><tex-math id="M38">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$R={r}_1\cup{r}_2\dots \cup{r}_N$\end{document}</tex-math></inline-formula> is the integrated region for all samples or for all categories. The region per class is also important to find genes belonging to a particular phenotype. In this case, <inline-formula><tex-math id="M39">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${R}_j={\cup}_{k=1}^{n_j}{\hat{r}}_k$\end{document}</tex-math></inline-formula>, where <inline-formula><tex-math id="M40">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${n}_j$\end{document}</tex-math></inline-formula> is the number of samples in the subset represented by <inline-formula><tex-math id="M41">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\omega}_j$\end{document}</tex-math></inline-formula> category, the region <inline-formula><tex-math id="M42">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\hat{r}}_k\in \mathcal{H}$\end{document}</tex-math></inline-formula> for <inline-formula><tex-math id="M43">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$k=1,2,\dots, {n}_j$\end{document}</tex-math></inline-formula> and belongs to a particular class, <inline-formula><tex-math id="M44">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\hat{r}}_k\in{\omega}_j$\end{document}</tex-math></inline-formula>. Basically, <inline-formula><tex-math id="M45">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${R}_j$\end{document}</tex-math></inline-formula> is the union of all regions depicting a particular class or phenotype. The active region size can vary depending on the threshold value. An illustration is given in <xref rid="f3" ref-type="fig">Figure 3B</xref> where two samples <inline-formula><tex-math id="M46">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\big\{{s}_1,{s}_2\big\}$\end{document}</tex-math></inline-formula> are from category <inline-formula><tex-math id="M47">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\omega}_1$\end{document}</tex-math></inline-formula>, and another two samples <inline-formula><tex-math id="M48">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\big\{{s}_3,{s}_4\big\}$\end{document}</tex-math></inline-formula> are from <inline-formula><tex-math id="M49">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\omega}_2$\end{document}</tex-math></inline-formula>. The active region in each of the samples is depicted as <inline-formula><tex-math id="M50">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\big\{{r}_1,{r}_2,{r}_3,{r}_4\big\}$\end{document}</tex-math></inline-formula>. All the four samples are processed via RA step, and it gives two outputs. In the first output, integrated regions <inline-formula><tex-math id="M51">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${R}_j$\end{document}</tex-math></inline-formula> (where <inline-formula><tex-math id="M52">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$j=1,2$\end{document}</tex-math></inline-formula>), belongs to individual categories are shown, i.e. active regions related to a particular phenotype. In the second output, all the active regions are integrated as <inline-formula><tex-math id="M53">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$R$\end{document}</tex-math></inline-formula>, depicting the necessary active zones for classification of two phenotypes.</p>
    </sec>
    <sec id="sec11">
      <title>Element decoder</title>
      <p>The output of RA is processed to the element decoder model. The pixels underneath the selected region are considered for this task. <xref rid="f3" ref-type="fig">Figure 3C</xref> shows the element decoder system. In general, a pixel <inline-formula><tex-math id="M54">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${p}_i$\end{document}</tex-math></inline-formula> will have a normalized value <inline-formula><tex-math id="M55">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$0\le{v}_i,\le 1$\end{document}</tex-math></inline-formula>. The decoder will find the argument or index of this pixel <inline-formula><tex-math id="M56">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${p}_i$\end{document}</tex-math></inline-formula> located at <inline-formula><tex-math id="M57">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\big({a}_i,{b}_i\big)$\end{document}</tex-math></inline-formula>, i.e. the unique elements or genes, <inline-formula><tex-math id="M58">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${G}_i$\end{document}</tex-math></inline-formula>, that it contains. If the compression (see the section describing the Snowfall compression algorithm) is quantized then chances are high to get <inline-formula><tex-math id="M59">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\big|{G}_i\big|&gt;1$\end{document}</tex-math></inline-formula>, and if non-quantized then <inline-formula><tex-math id="M60">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\big|{G}_i\big|=1$\end{document}</tex-math></inline-formula>, where <inline-formula><tex-math id="M61">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mid \bullet \mid$\end{document}</tex-math></inline-formula> is its cardinality. In general, for some pixels <inline-formula><tex-math id="M62">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\big|{G}_j\big|=1$\end{document}</tex-math></inline-formula> and for some <inline-formula><tex-math id="M63">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\big|{G}_k\big|&gt;1$\end{document}</tex-math></inline-formula> in a given pixel frame (where <inline-formula><tex-math id="M64">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$j$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M65">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$k$\end{document}</tex-math></inline-formula> are any two pixels).</p>
      <p>For a region <inline-formula><tex-math id="M66">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$R$\end{document}</tex-math></inline-formula>, a subset of elements or genes will be obtained. It is possible to find a subset of genes for a particular class and also for all the classes. Furthermore, the element decoder can also reveal a subset of genes for a sample. It should be noted that the model can give different subsets of elements for different categories enabling class dependent findings.</p>
    </sec>
    <sec id="sec12">
      <title>Reduction of elements through iteration</title>
      <p>For genomic or transcriptomic data, the number of genes is normally very high and it becomes very difficult to fit all genes into a finite image size due to fixed hardware limitations. In this case, it is inevitable to get quantized images, i.e. some image pixels will carry multiple genes in a location. One might wonder, how to perform the selection on those batch genes (where batch gene refers to a set of two or more genes having the same pixel location in the frame). One may also want to reduce the number of selected elements. These issues can be addressed by running DeepFeature iteratively. The first iteration will find a subset of elements, which can be used as the input for the subsequent iteration. Continuing this procedure will reduce the number of elements.</p>
    </sec>
    <sec id="sec13">
      <title>Logistic regression</title>
      <p>Here we developed L2-regularized logistic regression model, i.e. each of the 10 cancer studies has its model as shown below [<xref rid="ref62" ref-type="bibr">62</xref>].<disp-formula id="deqn02"><label>(2)</label><tex-math id="M67">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \underset{\mathbf{w}}{\min}\frac{1}{2}{\mathbf{w}}^T\mathbf{w}+C{\sum}_{j=1}^d\log \left(1+{e}^{-{y}_j{\mathbf{w}}^T{x}_j}\right), \end{equation*}\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M68">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\frac{1}{2}{\mathbf{w}}^T\mathbf{w}$\end{document}</tex-math></inline-formula> is the L2 regularization term (Ridge), <inline-formula><tex-math id="M69">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$i=1,2,\dots, 10$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M70">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$d$\end{document}</tex-math></inline-formula> is the number of genes. We create models for all the 10 cancer studies and therefore get coefficients <inline-formula><tex-math id="M71">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${w}_j^i$\end{document}</tex-math></inline-formula> (where <inline-formula><tex-math id="M72">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$j=1,2,\dots d$\end{document}</tex-math></inline-formula>). Arranging the absolute of coefficients in the descending order, we get<disp-formula id="deqn03"><label>(3)</label><tex-math id="M73">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} \left|{w}_1^i\right|&gt;\left|{w}_2^i\right|&gt;\dots &gt;\mid{w}_m^i\mid \end{equation*}\end{document}</tex-math></disp-formula></p>
      <p>This gives the top <inline-formula><tex-math id="M74">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$m$\end{document}</tex-math></inline-formula> genes <inline-formula><tex-math id="M75">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${x}_j$\end{document}</tex-math></inline-formula>. Since <inline-formula><tex-math id="M76">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$i=1,\dots, 10$\end{document}</tex-math></inline-formula>, this gives different gene subsets <inline-formula><tex-math id="M77">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${S}_i$\end{document}</tex-math></inline-formula> for 10 cancer studies. Training dataset was employed to find gene subsets specific to each cancer studies. However, it will be difficult to apply a ML technique for computing classification accuracy using different genes belonging to different cancer studies. Since for ML techniques, the same features (or genes) should be employed, it is not possible to find classification accuracy by using class specific features obtained from Equation (<xref rid="deqn03" ref-type="disp-formula">3</xref>). Therefore, we took a union of all gene subsets <inline-formula><tex-math id="M78">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$S={\cup}_{i=1}^{10}{S}_i$\end{document}</tex-math></inline-formula> and used subset <inline-formula><tex-math id="M79">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$S$\end{document}</tex-math></inline-formula> to find classification accuracy using a RF classifier. The validation set is used to tune the hyperparameters of the RF, and a separate test set is employed to perform evaluation. We used liblinear package in MATLAB to implement L2 regularized logistic regression (<ext-link xlink:href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/" ext-link-type="uri">https://www.csie.ntu.edu.tw/â¼cjlin/liblinear/</ext-link>).</p>
    </sec>
    <sec id="sec14">
      <title>Experimental setup</title>
      <p>The dataset was partitioned into 80:10:10 segments corresponding to training set, validation set and an independent test set, respectively. The training set is employed to achieve model fitting, whereas the validation set is used to evaluate its fitness. This picks the hyperparameters for which the validation error is small. The independent test set is held aside and applied to provide an unprejudiced evaluation of the final model. The DeepFeature model is implement on Matlab R2020a version software. The appropriate links are given under Code Availability section.</p>
    </sec>
    <sec id="sec15">
      <title>Performance evaluation</title>
      <p>The main aim of this experiment is to show that a subset of imperative elements or genes of nonimage data can be selected by CNN with the utilization of DeepFeature method. We have used high-quality reference resources for enrichment analysis like Reactome and Molecular Signatures Database (MSigDB) [<xref rid="ref63" ref-type="bibr">63</xref>, <xref rid="ref64" ref-type="bibr">64</xref>] to discover the importance of the selected genes to phenotypes by the proposed deep-learning based technique. DeepFeature can find different subset of genes for each of the phenotypes.</p>
      <p>We compared our methodology with alternative ML techniques. A number of ML algorithms exist to find a subset of genes [<xref rid="ref65" ref-type="bibr">65</xref>, <xref rid="ref66" ref-type="bibr">66</xref>]. We applied ANOVA, Lasso [<xref rid="ref67" ref-type="bibr">67</xref>], highly variable genes method [<xref rid="ref68" ref-type="bibr">68</xref>] and logistic regression [<xref rid="ref3" ref-type="bibr">3</xref>] for feature selection and RF as a classifier. The feature selection step was applied on the training set. This gives a subset of genes. Since enrichment analysis usually requires a smaller set of genes, the aim is to find roughly ~1500 genes. Thereafter, hyperparameters of RF is tuned using training set on a subset of selected genes, and model fitness is evaluated using the validation set. The classification accuracy was from the independent test set.</p>
    </sec>
    <sec id="sec16">
      <title>Evaluation of feature selection capabilities</title>
      <p>As outlined above, both DeepFeature and other exemplar algorithms offer some functionality for reducing the number of genes to a more focused subset enriched for the genes used for correctly identifying the relevant cancer study. This feature selection is of particular importance in biological data analysis where identification of key genes and underlying mechanisms is usually part of the overall goalâespecially for tasks like identification of clinically useful sparse diagnostic signatures. To evaluate the utility of DeepFeature from this perspective, we have quantified the enrichment of âgold standardâ cancer specific gene sets and pathways from MSigDB (C6 subset). In all instances the enrichment was calculated using Fisherâs exact test and reported <italic toggle="yes">P</italic>-values were corrected for multiple testing using BenjaminiâHochberg FDR method. In addition, we report the housekeeping gene counts based on the annotation from The Human Protein Atlas (THPA, <ext-link xlink:href="http://www.proteinatlas.org" ext-link-type="uri">http://www.proteinatlas.org</ext-link>) [<xref rid="ref69" ref-type="bibr">69</xref>] and a study by Eisenberg and Levanon [<xref rid="ref70" ref-type="bibr">70</xref>]. Here, the assumption is that a more relevant selection would tend to have fewer housekeeping genes.</p>
    </sec>
    <sec id="sec17">
      <title>Running the DeepFeature algorithm</title>
      <p>Analyzing a large number of elements will cause overlaps in the small pixel frame and it becomes challenging to perform feature selection. This can cause important elements to be overlooked in the selection process. Therefore, it is useful to perform element reduction to reach a manageable size due to the pixel frame size and hardware limitations.</p>
      <p>The element arrangement step of DeepFeature utilizes t-SNE (with or without Snowfall) and PHATE. In the case of t-SNE technique, it supports various distance measures, <inline-formula><tex-math id="M80">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$dis{t}_j$\end{document}</tex-math></inline-formula>. In this study, <inline-formula><tex-math id="M81">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$dis{t}_j$\end{document}</tex-math></inline-formula> are Chebyshev, cosine, correlation and Hamming. A gene set, <inline-formula><tex-math id="M82">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$G$\end{document}</tex-math></inline-formula>, processed to DeepFeature with a distance <inline-formula><tex-math id="M83">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$dis{t}_j$\end{document}</tex-math></inline-formula> of t-SNE, gives a gene subset <inline-formula><tex-math id="M84">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${g}_j$\end{document}</tex-math></inline-formula>. Since four distances are adopted, a union of gene subsets are retrieved, i.e. <inline-formula><tex-math id="M85">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\hat{g}={\cup}_{j=1}^4{g}_j$\end{document}</tex-math></inline-formula>. Furthermore, the gene subset <inline-formula><tex-math id="M86">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\hat{g}$\end{document}</tex-math></inline-formula> is sent to DeepFeature with Hamming distance until a subset of around 1500 genes are obtained (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S1.1</xref> and corresponding discussion in <xref rid="sup1" ref-type="supplementary-material">Supplementary File 1</xref>). Thereafter, gene set and pathway enrichment analyses were performed to evaluate overall relevance of the recovered gene set with respect to current knowledge.</p>
      <boxed-text id="box01" position="float">
        <sec id="sec17z">
          <title>Key Points</title>
          <list list-type="bullet">
            <list-item>
              <p>In this paper, we present, DeepFeature, a more advanced version of our previous work, DeepInsight, that can now recover underlying feature combinations specific to each class of interest. DeepInsight (which is part of a winning model in Kaggle.com organized by MIT and Harvard, and applied in many fields of research), established a novel approach that allows high-throughput biological data to be represented in a form compatible with current state-of-art convolutional neural network (CNN) architectures.</p>
            </list-item>
            <list-item>
              <p>In addition to delivering high classification performance, DeepFeature also offers a powerful means for the identification of biologically-relevant gene sets.</p>
            </list-item>
            <list-item>
              <p>DeepFeature converts non-image samples of RNA-seq data into image-form, and, furthermore, performs gene selection via CNN. To our knowledge, this is the first approach to employ CNN for element or gene selection on non-image data.</p>
            </list-item>
            <list-item>
              <p>When applied to the task of cancer classification, our DeepFeature approach was able to identify coherent sets with significant enrichment of genes in cancer-associated pathways from MSigDB and a gold-standard reference set. Further analysis of these results suggested biologically meaningful connections of potential interest to our understanding of the differences between major cancer types.</p>
            </list-item>
            <list-item>
              <p>DeepFeature is available for download and the web-links provided in the paper.</p>
            </list-item>
          </list>
        </sec>
      </boxed-text>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>Supplement_File_1_2_4_bbab297</label>
      <media xlink:href="supplement_file_1_2_4_bbab297.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup2" position="float" content-type="local-data">
      <label>SupplementFile_3_bbab297</label>
      <media xlink:href="supplementfile_3_bbab297.xlsx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="bbab297-ack">
    <title>Acknowledgment</title>
    <p>The results shown here are in whole or part based upon data generated by the TCGA Research Network: <ext-link xlink:href="https://www.cancer.gov/tcga" ext-link-type="uri">https://www.cancer.gov/tcga</ext-link>.</p>
  </ack>
  <sec id="sec18">
    <title>Authorsâ contributions</title>
    <p>AS perceived, designed the classification and feature selection models, and wrote the first draft and contributed in the subsequent versions of the manuscript. AL designed the enrichment analysis, improving selection models and contributed in the first draft and thereafter of the manuscript. KAB further built the enrichment analysis, prepared the data and helped in the manuscript write-up. EV provided art support, build the machine learning tools and contributed in the write-up. TT perceived and contributed in the manuscript write-ups. All authors read and approved the manuscript.</p>
  </sec>
  <sec id="sec20">
    <title>Code availability</title>
    <p>DeepFeature software package (in Matlab), dataset, installation instructions and user-manual are available from the GitHub link <ext-link xlink:href="https://alok-ai-lab.github.io/deepfeature/" ext-link-type="uri">https://alok-ai-lab.github.io/deepfeature/</ext-link>. The dataset is also separately available from the link <ext-link xlink:href="http://emu.src.riken.jp/DeepInsightFS/dataset1.mat" ext-link-type="uri">http://emu.src.riken.jp/DeepInsightFS/dataset1.mat</ext-link>, note the size is 1.5GB. The following links are also given for software package <ext-link xlink:href="http://www.alok-ai-lab.com/tools.php" ext-link-type="uri">http://www.alok-ai-lab.com/tools.php</ext-link> and/or <ext-link xlink:href="http://emu.src.riken.jp/" ext-link-type="uri">http://emu.src.riken.jp/</ext-link>. The supporting tool, DeepInsight, also has released reference implementation in Matlab (<ext-link xlink:href="http://www.alok-ai-lab.com/tools.php" ext-link-type="uri">http://www.alok-ai-lab.com/tools.php</ext-link>) and Python as pyDeepInsight <ext-link xlink:href="https://alok-ai-lab.github.io/DeepInsight/" ext-link-type="uri">https://alok-ai-lab.github.io/DeepInsight/</ext-link>. The description of it can also be accessed by visiting the official page of Matlab at <ext-link xlink:href="https://www.mathworks.com/company/user_stories/case-studies/riken-develops-a-method-to-apply-cnn-to-non-image-data.html" ext-link-type="uri">https://www.mathworks.com/company/user_stories/case-studies/riken-develops-a-method-to-apply-cnn-to-non-image-data.html</ext-link></p>
  </sec>
  <sec id="sec19a">
    <title>Funding</title>
    <p>This work was funded by JST CREST (grant number JPMJCR 1412, Japan); JSPS KAKENHI (grant Numbers JP17H06307, JP17H06299 and JP20H03240; Grant-in-Aid for Scientific Research (JP16H06299) from the Ministry of Education, Culture, Sports, Science and Technology of Japan.</p>
  </sec>
  <notes id="bio3">
    <p><bold>Alok Sharma</bold> is a Senior Research Scientist at RIKEN, Japan; Adjunct Professor at Griffith University, Australia. His research interests are bioinformatics, AI and proteomics. He has published over 140 scientific papers with h-index 37.</p>
    <p><bold>Artem Lysenko</bold> is a Researcher at the RIKEN Laboratory for Medical Science Mathematics. His research is in the areas of biomedical deep learning, applied data science and biological network analysis.</p>
    <p><bold>Keith A. Boroevich</bold> is a research technician at the RIKEN Laboratory for Medical Science Mathematics.</p>
    <p><bold>Edwin Vans</bold> is a PhD student at the University of the South Pacific and a Lecturer at Fiji National University. His research interests are in bioinformatics and machine learning.</p>
    <p><bold>Tatsuhiko Tsunoda</bold> is a Professor at the School of Science, the University of Tokyo (UT). He is leading laboratories for Medical Science Mathematics at UT, Tokyo Medical and Dental University, and RIKEN.</p>
  </notes>
  <ref-list id="bib1">
    <title>References</title>
    <ref id="ref1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cortes</surname><given-names>C</given-names></string-name>, <string-name><surname>Vapnik</surname><given-names>V</given-names></string-name></person-group>. <article-title>Support-vector networks</article-title>. <source>Mach Learn</source><year>1995</year>;<volume>20</volume>:<fpage>273</fpage>â<lpage>97</lpage>. <pub-id pub-id-type="doi">10.1007/bf00994018</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tin Kam</surname><given-names>H</given-names></string-name></person-group>. <article-title>The random subspace method for constructing decision forests</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source><year>1998</year>;<volume>20</volume>:<fpage>832</fpage>â<lpage>44</lpage>. <pub-id pub-id-type="doi">10.1109/34.709601</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tolles</surname><given-names>J</given-names></string-name>, <string-name><surname>Meurer</surname><given-names>WJ</given-names></string-name></person-group>. <article-title>Logistic regression relating patient characteristics to outcomes</article-title>. <source>JAMA</source><year>2016</year>;<volume>316</volume>:<fpage>533</fpage>â<lpage>4</lpage>. <pub-id pub-id-type="doi">10.1001/jama.2016.7653</pub-id>.<pub-id pub-id-type="pmid">27483067</pub-id></mixed-citation>
    </ref>
    <ref id="ref4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lecun</surname><given-names>Y</given-names></string-name>, <string-name><surname>Bottou</surname><given-names>L</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Gradient-based learning applied to document recognition</article-title>. <source>Proc IEEE</source><year>1998</year>;<volume>86</volume>:<fpage>2278</fpage>â<lpage>324</lpage>. <pub-id pub-id-type="doi">10.1109/5.726791</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref5">
      <label>5.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Krizhevsky</surname><given-names>A</given-names></string-name>, <string-name><surname>Sutskever</surname><given-names>I</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>GE</given-names></string-name></person-group>. <source>Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1</source>. <publisher-loc>Lake Tahoe</publisher-loc>:
<publisher-name>Nevada</publisher-name>, <year>2012</year>, <fpage>1097</fpage>â<lpage>105</lpage>.</mixed-citation>
    </ref>
    <ref id="ref6">
      <label>6.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Simonyan</surname><given-names>K</given-names></string-name>, <string-name><surname>Zisserman</surname><given-names>A</given-names></string-name></person-group>. <source>Very Deep Convolutional Networks for Large-Scale Image Recognition</source>. <comment>arXiv:1409.1556v6</comment>, <year>2015</year>.</mixed-citation>
    </ref>
    <ref id="ref7">
      <label>7.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>He</surname><given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>X</given-names></string-name>, <string-name><surname>Ren</surname><given-names>S</given-names></string-name>, <etal>etÂ al.</etal></person-group><source>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Las Vegas, NV</publisher-loc>, <year>2016</year>, <fpage>770</fpage>â<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="ref8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yan</surname><given-names>ML</given-names></string-name>, <string-name><surname>Chen</surname><given-names>Q</given-names></string-name>, <collab>Shuicheng</collab></person-group>. <article-title>Network In Network</article-title>. <source>CoRR</source><comment>arXiv:1312.4400v3</comment>, <year>2014</year>.</mixed-citation>
    </ref>
    <ref id="ref9">
      <label>9.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author">Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, <etal>etÂ al.</etal></person-group><source>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Boston</publisher-loc>:
<publisher-name>MA: IEEE</publisher-name>, <year>2015</year>, <fpage>1</fpage>â<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="ref10">
      <label>10.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sermanet</surname><given-names>P</given-names></string-name>, Eigen D, Zhang X, <etal>etÂ al.</etal></person-group><source>International Conference on Learning Representations (ICLR2014), CBLS</source>.
<publisher-name>USA: Cornell University</publisher-name>, <year>2014</year>.</mixed-citation>
    </ref>
    <ref id="ref11">
      <label>11.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Redmon</surname><given-names>J</given-names></string-name>, <string-name><surname>Divvala</surname><given-names>SK</given-names></string-name>, <string-name><surname>Girshick</surname><given-names>RB</given-names></string-name>, <etal>etÂ al.</etal></person-group><source>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>. USA: IEEE, <year>2016</year>, <fpage>779</fpage>â<lpage>88</lpage>.</mixed-citation>
    </ref>
    <ref id="ref12">
      <label>12.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Girshick</surname><given-names>R</given-names></string-name></person-group>. <source>IEEE International Conference on Computer Vision (ICCV) 1440â1448</source>. <publisher-loc>Santiago</publisher-loc>:
<publisher-name>Chile</publisher-name>, <year>2015</year>.</mixed-citation>
    </ref>
    <ref id="ref13">
      <label>13.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ren</surname><given-names>S</given-names></string-name>, <string-name><surname>He</surname><given-names>K</given-names></string-name>, <string-name><surname>Girshick</surname><given-names>R</given-names></string-name>, <etal>etÂ al.</etal></person-group><source>Advances in Neural Information Processing Systems</source>. USA: MIT Press, <year>2015</year>, <fpage>91</fpage>â<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="ref14">
      <label>14.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Goodfellow</surname><given-names>I</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y</given-names></string-name>, <string-name><surname>Courville</surname><given-names>A</given-names></string-name>, <etal>etÂ al.</etal></person-group><source>Deep Learning</source>.
<publisher-name>Cambridge, MA, USA: The MIT Press</publisher-name>, <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="ref15">
      <label>15.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Habibi Aghdam</surname><given-names>H</given-names></string-name>, <string-name><surname>Jahani Heravi</surname><given-names>E</given-names></string-name></person-group>. <source>Guide to Convolutional Neural Networks</source>. Switzerland: Springer international publishing, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="ref16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>J</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X</given-names></string-name>, <string-name><surname>Cheng</surname><given-names>Y</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Tumor gene expression data classification via sample expansion-based deep learning</article-title>. <source>Oncotarget</source><year>2017</year>;<volume>8</volume>:<fpage>109646</fpage>â<lpage>60</lpage>.<pub-id pub-id-type="pmid">29312636</pub-id></mixed-citation>
    </ref>
    <ref id="ref17">
      <label>17.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Xu</surname><given-names>J</given-names></string-name>, Wang P, Tian G, <etal>etÂ al.</etal></person-group><source>IJCAI'15 Proceedings of the 24th International Conference on Artificial Intelligence</source>. <publisher-loc>Argentina</publisher-loc>, <year>2015</year>, <fpage>1369</fpage>â<lpage>75</lpage>.</mixed-citation>
    </ref>
    <ref id="ref18">
      <label>18.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>X</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>J</given-names></string-name>, <string-name><surname>LeCun</surname><given-names>Y</given-names></string-name></person-group>. <source>NIPS'15 Proceedings of the 28th International Conference on Neural Information Processing Systems</source>, Vol. <volume>1</volume>. <publisher-loc>Montreal</publisher-loc>:
<publisher-name>Canada</publisher-name>, <year>2015</year>.</mixed-citation>
    </ref>
    <ref id="ref19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sharma</surname><given-names>A</given-names></string-name>, <string-name><surname>Vans</surname><given-names>E</given-names></string-name>, <string-name><surname>Shigemizu</surname><given-names>D</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>DeepInsight: a methodology to transform a non-image data to an image for convolution neural network architecture</article-title>. <source>Sci Rep</source><year>2019</year>;<volume>9</volume>:<fpage>11399</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-019-47765-6</pub-id>.<pub-id pub-id-type="pmid">31388036</pub-id></mixed-citation>
    </ref>
    <ref id="ref20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maaten</surname><given-names>L v d</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>G</given-names></string-name></person-group>. <article-title>Visualizing data using t-SNE</article-title>. <source>J Mach Learn Res</source><year>2008</year>;<volume>9</volume>:<fpage>2579</fpage>â<lpage>605</lpage>.</mixed-citation>
    </ref>
    <ref id="ref21">
      <label>21.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>ButuroviÄ</surname><given-names>L</given-names></string-name>, <string-name><surname>MiljkoviÄ</surname><given-names>D</given-names></string-name></person-group>. <article-title>A novel method for classification of tabular data using convolutional neural networks</article-title><comment>bioRxiv</comment>. <year>2020</year>;<fpage>1</fpage>â<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1101/2020.05.02.074203</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref22">
      <label>22.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kanber</surname><given-names>B</given-names></string-name></person-group>. <article-title>Sparse data to structured imageset transformation</article-title><comment>arXiv:2005.10045</comment>. <year>2020</year>.</mixed-citation>
    </ref>
    <ref id="ref23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kobayashi</surname><given-names>K</given-names></string-name>, <string-name><surname>Bolatkan</surname><given-names>A</given-names></string-name>, <string-name><surname>Shiina</surname><given-names>S</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Fully-connected neural networks with reduced parameterization for predicting histological types of lung cancer from somatic mutations</article-title>. <source>Biomolecules</source><year>2020</year>;<volume>10</volume>. <pub-id pub-id-type="doi">10.3390/biom10091249</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hamrani</surname><given-names>A</given-names></string-name>, <string-name><surname>Akbarzadeh</surname><given-names>A</given-names></string-name>, <string-name><surname>Madramootoo</surname><given-names>CA</given-names></string-name></person-group>. <article-title>Machine learning for predicting greenhouse gas emissions from agricultural soils</article-title>. <source>Sci Total Environ</source><year>2020</year>;<volume>741</volume>:<fpage>140338</fpage>. <pub-id pub-id-type="doi">10.1016/j.scitotenv.2020.140338</pub-id>.<pub-id pub-id-type="pmid">32610233</pub-id></mixed-citation>
    </ref>
    <ref id="ref25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>LÃ³pez-GarcÃ­a</surname><given-names>G</given-names></string-name>, <string-name><surname>Jerez</surname><given-names>JM</given-names></string-name>, <string-name><surname>Franco</surname><given-names>L</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Transfer learning with convolutional neural networks for cancer survival prediction using gene-expression data</article-title>. <source>Plos One</source><year>2020</year>;<volume>15</volume>:<elocation-id>e0230536</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pone.0230536</pub-id>.<pub-id pub-id-type="pmid">32214348</pub-id></mixed-citation>
    </ref>
    <ref id="ref26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname><given-names>J</given-names></string-name>, <string-name><surname>Han</surname><given-names>D</given-names></string-name>, <string-name><surname>Shin</surname><given-names>M</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Different spectral domain transformation for land cover classification using convolutional neural networks with multi-temporal satellite imagery</article-title>. <source>Remote Sens (Basel)</source><year>2020</year>;<volume>12</volume>:<elocation-id>1097</elocation-id>. <pub-id pub-id-type="doi">10.3390/rs12071097</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lopez-Martin</surname><given-names>M</given-names></string-name>, <string-name><surname>Nevado</surname><given-names>A</given-names></string-name>, <string-name><surname>Carro</surname><given-names>B</given-names></string-name></person-group>. <article-title>Detection of early stages of Alzheimerâs disease based on MEG activity with a randomized convolutional neural network</article-title>. <source>Artif Intell Med</source><year>2020</year>;<volume>107</volume>:<elocation-id>101924</elocation-id>. <pub-id pub-id-type="doi">10.1016/j.artmed.2020.101924</pub-id>.<pub-id pub-id-type="pmid">32828459</pub-id></mixed-citation>
    </ref>
    <ref id="ref28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vans</surname><given-names>E</given-names></string-name>, <string-name><surname>Patil</surname><given-names>A</given-names></string-name>, <string-name><surname>Sharma</surname><given-names>A</given-names></string-name></person-group>. <article-title>FEATS: feature selection-based clustering of single-cell RNA-seq data</article-title>. <source>Brief Bioinform</source><year>2021</year>;<volume>22</volume>(<issue>4</issue>):<fpage>1</fpage>â<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1093/bib/bbaa306</pub-id>.<pub-id pub-id-type="pmid">33401308</pub-id></mixed-citation>
    </ref>
    <ref id="ref29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mulenga</surname><given-names>M</given-names></string-name>, <string-name><surname>Abdul Kareem</surname><given-names>S</given-names></string-name>, <string-name><surname>Qalid Md Sabri</surname><given-names>A</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Feature extension of gut microbiome data for deep neural network-based colorectal cancer classification</article-title>. <source>IEEE Access</source><year>2021</year>;<volume>9</volume>:<fpage>23565</fpage>â<lpage>78</lpage>. <pub-id pub-id-type="doi">10.1109/access.2021.3050838</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref30">
      <label>30.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Laguna</surname><given-names>R</given-names></string-name>, Geremias N, Mauiri D, <etal>etÂ al.</etal></person-group><source>2020 IEEE PES Transmission &amp; Distribution Conference and Exhibition - Latin America (T&amp;D LA)</source>, <year>2020</year>, <fpage>1</fpage>â<lpage>6</lpage>.</mixed-citation>
    </ref>
    <ref id="ref31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kumar</surname><given-names>S</given-names></string-name>, <string-name><surname>Sharma</surname><given-names>R</given-names></string-name>, <string-name><surname>Sharma</surname><given-names>A</given-names></string-name></person-group>. <article-title>OPTICAL+: a frequency-based deep learning scheme for recognizing brain wave signals</article-title>. <source>PeerJ Comput Sci</source><year>2021</year>;<volume>7</volume>:<fpage>e375</fpage>. <pub-id pub-id-type="doi">10.7717/peerj-cs.375</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sharma</surname><given-names>R</given-names></string-name>, <string-name><surname>Kumar</surname><given-names>S</given-names></string-name>, <string-name><surname>Tsunoda</surname><given-names>T</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Single-stranded and double-stranded DNA-binding protein prediction using HMM profiles</article-title>. <source>Anal Biochem</source><year>2021</year>;<volume>612</volume>:<fpage>113954</fpage>. <pub-id pub-id-type="doi">10.1016/j.ab.2020.113954</pub-id>.<pub-id pub-id-type="pmid">32946833</pub-id></mixed-citation>
    </ref>
    <ref id="ref33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pasquadibisceglie</surname><given-names>V</given-names></string-name>, <string-name><surname>Appice</surname><given-names>A</given-names></string-name>, <string-name><surname>Castellano</surname><given-names>G</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>ORANGE: outcome-oriented predictive process monitoring based on image encoding and CNNs</article-title>. <source>IEEE Access</source><year>2020</year>;<volume>8</volume>:<fpage>184073</fpage>â<lpage>86</lpage>. <pub-id pub-id-type="doi">10.1109/access.2020.3029323</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tanabe</surname><given-names>K</given-names></string-name>, <string-name><surname>Ikeda</surname><given-names>M</given-names></string-name>, <string-name><surname>Hayashi</surname><given-names>M</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Comprehensive serum glycopeptide spectra analysis combined with artificial intelligence (CSGSA-AI) to diagnose early-stage ovarian cancer</article-title>. <source>Cancer</source><year>2020</year>;<volume>12</volume>:<elocation-id>2373</elocation-id>. <pub-id pub-id-type="doi">10.3390/cancers12092373</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pomyen</surname><given-names>Y</given-names></string-name>, <string-name><surname>Wanichthanarak</surname><given-names>K</given-names></string-name>, <string-name><surname>Poungsombat</surname><given-names>P</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Deep metabolome: applications of deep learning in metabolomics</article-title>. <source>Comput Struct Biotechnol J</source><year>2020</year>;<volume>18</volume>:<fpage>2818</fpage>â<lpage>25</lpage>. <pub-id pub-id-type="doi">10.1016/j.csbj.2020.09.033</pub-id>.<pub-id pub-id-type="pmid">33133423</pub-id></mixed-citation>
    </ref>
    <ref id="ref36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>H</given-names></string-name>, <string-name><surname>Pujos-Guillot</surname><given-names>E</given-names></string-name>, <string-name><surname>Comte</surname><given-names>B</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Deep learning in systems medicine</article-title>. <source>Brief Bioinform</source><year>2020</year>;<fpage>1543</fpage>â<lpage>59</lpage>. <pub-id pub-id-type="doi">10.1093/bib/bbaa237</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arafat</surname><given-names>ME</given-names></string-name>, <string-name><surname>Ahmad</surname><given-names>MW</given-names></string-name>, <string-name><surname>Shovan</surname><given-names>SM</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Accurately predicting glutarylation sites using sequential bi-peptide-based evolutionary features</article-title>. <source>Genes</source><year>2020</year>;<volume>11</volume>:<elocation-id>1023</elocation-id>. <pub-id pub-id-type="doi">10.3390/genes11091023</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref38">
      <label>38.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Rodrigues</surname><given-names>NM</given-names></string-name>, Batista JE, Trujillo L, <etal>etÂ al.</etal></person-group><article-title>Plotting time: on the usage of CNNs for time series classification</article-title>. <comment>arXiv:2102.04179v1</comment>, <year>2021</year>.</mixed-citation>
    </ref>
    <ref id="ref39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rahim</surname><given-names>MA</given-names></string-name>, <string-name><surname>Hassan</surname><given-names>HM</given-names></string-name></person-group>. <article-title>A deep learning based traffic crash severity prediction framework</article-title>. <source>Accid Anal Prevent</source><year>2021</year>;<volume>154</volume>:<fpage>106090</fpage>. <pub-id pub-id-type="doi">10.1016/j.aap.2021.106090</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref40">
      <label>40.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>Z</given-names></string-name>, <string-name><surname>Balch</surname><given-names>T</given-names></string-name>, <string-name><surname>Veloso</surname><given-names>M</given-names></string-name></person-group>. <article-title>Deep video prediction for time series forecasting</article-title>. <comment>arXiv:2102.12061v1</comment>. <year>2021</year>.</mixed-citation>
    </ref>
    <ref id="ref41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>SerrÃ£o</surname><given-names>MK</given-names></string-name>, <string-name><prefix>de A. E</prefix><surname>Aquino</surname><given-names>G</given-names></string-name>, <string-name><surname>Costa</surname><given-names>MGF</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Human activity recognition from accelerometer with convolutional and recurrent neural networks</article-title>. <source>Polytechnica</source><year>2021</year>. <pub-id pub-id-type="doi">10.1007/s41050-021-00028-8</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>W</given-names></string-name>, <string-name><surname>Feng</surname><given-names>P</given-names></string-name>, <string-name><surname>Ding</surname><given-names>H</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>iRNA-methyl: identifying N6-methyladenosine sites using pseudo nucleotide composition</article-title>. <source>Anal Biochem</source><year>2015</year>;<volume>490</volume>:<fpage>26</fpage>â<lpage>33</lpage>.<pub-id pub-id-type="pmid">26314792</pub-id></mixed-citation>
    </ref>
    <ref id="ref43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peng</surname><given-names>T-C</given-names></string-name>, <string-name><surname>Dhankhar</surname><given-names>N</given-names></string-name>, <string-name><surname>Aizawa</surname><given-names>T</given-names></string-name></person-group>. <article-title>1st Place Winning Solution â Hungry for Gold</article-title>. <source>Laboratory for Innovation Science at Harvard, Mechanisms of Action (MoA) Prediction Competition</source><year>2020</year>.</mixed-citation>
    </ref>
    <ref id="ref44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fukushima</surname><given-names>K</given-names></string-name></person-group>. <article-title>Neocognitron</article-title>. <source>Scholarpedia</source><year>2007</year>;<volume>2</volume>:<elocation-id>1717</elocation-id>. <pub-id pub-id-type="doi">10.4249/scholarpedia.1717</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref45">
      <label>45.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Matsugu</surname><given-names>M</given-names></string-name>, <string-name><surname>Mori</surname><given-names>K</given-names></string-name>, <string-name><surname>Mitari</surname><given-names>Y</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Subject independent facial expression recognition with robust face detection using a convolutional neural network</article-title>. <source>Neural Netw</source><year>2003</year>;<volume>16</volume>:<fpage>555</fpage>â<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1016/s0893-6080(03)00115-1</pub-id>.<pub-id pub-id-type="pmid">12850007</pub-id></mixed-citation>
    </ref>
    <ref id="ref46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kindel</surname><given-names>WF</given-names></string-name>, <string-name><surname>Christensen</surname><given-names>ED</given-names></string-name>, <string-name><surname>Zylberberg</surname><given-names>J</given-names></string-name></person-group>. <article-title>Using deep learning to probe the neural code for images in primary visual cortex</article-title>. <source>J Vis</source><year>2019</year>;<volume>19</volume>:<fpage>29</fpage>. <pub-id pub-id-type="doi">10.1167/19.4.29</pub-id>.<pub-id pub-id-type="pmid">31026016</pub-id></mixed-citation>
    </ref>
    <ref id="ref47">
      <label>47.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jimenez-Carretero</surname><given-names>D</given-names></string-name>, <string-name><surname>Abrishami</surname><given-names>V</given-names></string-name>, <string-name><surname>FernÃ¡ndez-de-Manuel</surname><given-names>L</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Tox_(R)CNN: deep learning-based nuclei profiling tool for drug toxicity screening</article-title>. <source>PLoS Comput Biol</source><year>2018</year>;<volume>14</volume>:<fpage>e1006238</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006238</pub-id>.<pub-id pub-id-type="pmid">30500821</pub-id></mixed-citation>
    </ref>
    <ref id="ref48">
      <label>48.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Iandola</surname><given-names>F</given-names></string-name></person-group>. <source>1 Online Resource (126 pages) (University of California)</source>. <publisher-loc>Berkeley, CA</publisher-loc>:
<publisher-name>Berkeley</publisher-name>, <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="ref49">
      <label>49.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zhou</surname><given-names>B</given-names></string-name>, <string-name><surname>Khosla</surname><given-names>A</given-names></string-name>, <string-name><surname>Lapedriza</surname><given-names>Ã</given-names></string-name>, <etal>etÂ al.</etal></person-group><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>, <fpage>2921</fpage>â<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="ref50">
      <label>50.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Mallows</surname><given-names>C</given-names></string-name></person-group>. <source>The Collected Works of John W. Tukey: More Mathematical 1938â1984 (Wadsworth &amp; Brooks/Cole Statistics/probability Series)</source>.
<publisher-name>MA, USA: Kluwer Academic Publishers Group</publisher-name>, <year>1991</year>.</mixed-citation>
    </ref>
    <ref id="ref51">
      <label>51.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Satija</surname><given-names>R</given-names></string-name>, <string-name><surname>Farrell</surname><given-names>JA</given-names></string-name>, <string-name><surname>Gennert</surname><given-names>D</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Spatial reconstruction of single-cell gene expression data</article-title>. <source>Nat Biotechnol</source><year>2015</year>;<volume>33</volume>:<fpage>495</fpage>â<lpage>502</lpage>. <pub-id pub-id-type="doi">10.1038/nbt.3192</pub-id>.<pub-id pub-id-type="pmid">25867923</pub-id></mixed-citation>
    </ref>
    <ref id="ref52">
      <label>52.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>ZurauskienÄ</surname><given-names>J</given-names></string-name>, <string-name><surname>Yau</surname><given-names>C</given-names></string-name></person-group>. <article-title>pcaReduce: hierarchical clustering of single cell transcriptional profiles</article-title>. <source>BMC Bioinformatics</source><year>2016</year>;<volume>17</volume>:1â11. <pub-id pub-id-type="doi">10.1186/s12859-016-0984-y</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref53">
      <label>53.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ji</surname><given-names>Z</given-names></string-name>, <string-name><surname>Ji</surname><given-names>H</given-names></string-name></person-group>. <article-title>TSCAN: Pseudo-time reconstruction and evaluation in single-cell RNA-seq analysis</article-title>. <source>Nucleic Acids Res</source><year>2016</year>;<volume>44</volume>:<fpage>e117</fpage>â<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkw430</pub-id>.<pub-id pub-id-type="pmid">27179027</pub-id></mixed-citation>
    </ref>
    <ref id="ref54">
      <label>54.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guo</surname><given-names>M</given-names></string-name>, <string-name><surname>Wang</surname><given-names>H</given-names></string-name>, <string-name><surname>Potter</surname><given-names>SS</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>SINCERA: a pipeline for single-cell RNA-seq profiling analysis</article-title>. <source>PLoS Comput Biol</source><year>2015</year>;<volume>11</volume>:<fpage>e1004575</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004575</pub-id>.<pub-id pub-id-type="pmid">26600239</pub-id></mixed-citation>
    </ref>
    <ref id="ref55">
      <label>55.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moon</surname><given-names>KR</given-names></string-name>, <string-name><surname>van Dijk</surname><given-names>D</given-names></string-name>, <string-name><surname>Wang</surname><given-names>Z</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Visualizing structure and transitions in high-dimensional biological data</article-title>. <source>Nat Biotechnol</source><year>2019</year>;<volume>37</volume>:<fpage>1482</fpage>â<lpage>92</lpage>. <pub-id pub-id-type="doi">10.1038/s41587-019-0336-3</pub-id>.<pub-id pub-id-type="pmid">31796933</pub-id></mixed-citation>
    </ref>
    <ref id="ref56">
      <label>56.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Walker</surname><given-names>C</given-names></string-name>, <string-name><surname>Mojares</surname><given-names>E</given-names></string-name>, <string-name><prefix>del</prefix><surname>RÃ­o HernÃ¡ndez</surname><given-names>A</given-names></string-name></person-group>. <article-title>Role of extracellular matrix in development and cancer progression</article-title>. <source>Int J Mol Sci</source><year>2018</year>;<volume>19</volume>:<elocation-id>3028</elocation-id>. <pub-id pub-id-type="doi">10.3390/ijms19103028</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref57">
      <label>57.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Qin</surname><given-names>S</given-names></string-name>, <string-name><surname>Li</surname><given-names>A</given-names></string-name>, <string-name><surname>Yi</surname><given-names>M</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Recent advances on anti-angiogenesis receptor tyrosine kinase inhibitors in cancer therapy</article-title>. <source>J Hematol Oncol</source><year>2019</year>;<volume>12</volume>:<fpage>27</fpage>. <pub-id pub-id-type="doi">10.1186/s13045-019-0718-5</pub-id>.<pub-id pub-id-type="pmid">30866992</pub-id></mixed-citation>
    </ref>
    <ref id="ref58">
      <label>58.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Insel</surname><given-names>PA</given-names></string-name>, <string-name><surname>Sriram</surname><given-names>K</given-names></string-name>, <string-name><surname>Wiley</surname><given-names>SZ</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>GPCRomics: GPCR expression in cancer cells and tumors identifies new, potential biomarkers and therapeutic targets</article-title>. <source>Front Pharmacol</source><year>2018</year>;<volume>9</volume>. <pub-id pub-id-type="doi">10.3389/fphar.2018.00431</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref59">
      <label>59.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gad</surname><given-names>AA</given-names></string-name>, <string-name><surname>Balenga</surname><given-names>N</given-names></string-name></person-group>. <article-title>The emerging role of adhesion GPCRs in cancer</article-title>. <source>ACS Pharmacol Transl Sci</source><year>2020</year>;<volume>3</volume>:<fpage>29</fpage>â<lpage>42</lpage>. <pub-id pub-id-type="doi">10.1021/acsptsci.9b00093</pub-id>.<pub-id pub-id-type="pmid">32259086</pub-id></mixed-citation>
    </ref>
    <ref id="ref60">
      <label>60.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>SchÃ¶lkopf</surname><given-names>B</given-names></string-name>, <string-name><surname>Smola</surname><given-names>A</given-names></string-name>, <string-name><surname>MÃ¼ller</surname><given-names>K-R</given-names></string-name></person-group>. <article-title>Nonlinear component analysis as a kernel eigenvalue problem</article-title>. <source>Neural Comput</source><year>1998</year>;<volume>10</volume>:<fpage>1299</fpage>â<lpage>319</lpage>. <pub-id pub-id-type="doi">10.1162/089976698300017467</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref61">
      <label>61.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>McInnes</surname><given-names>L</given-names></string-name>, <string-name><surname>Healy</surname><given-names>J</given-names></string-name>, <string-name><surname>Melville</surname><given-names>J</given-names></string-name></person-group>. <article-title>UMAP: uniform manifold approximation and projection for dimension reduction</article-title>. arXiv:1802.03426, <year>2018</year>, pp 1â63.</mixed-citation>
    </ref>
    <ref id="ref62">
      <label>62.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fan</surname><given-names>R-E</given-names></string-name>, <string-name><surname>Chang</surname><given-names>K-W</given-names></string-name>, <string-name><surname>Hsieh</surname><given-names>C-J</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>LIBLINEAR: a library for large linear classification</article-title>. <source>J Mach Learn Res</source><year>2008</year>;<volume>9</volume>:<fpage>1871</fpage>â<lpage>4</lpage>.</mixed-citation>
    </ref>
    <ref id="ref63">
      <label>63.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liberzon</surname><given-names>A</given-names></string-name>, <string-name><surname>Subramanian</surname><given-names>A</given-names></string-name>, <string-name><surname>Pinchback</surname><given-names>R</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Molecular signatures database (MSigDB) 3.0</article-title>. <source>Bioinformatics</source><year>2011</year>;<volume>27</volume>:<fpage>1739</fpage>â<lpage>40</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btr260</pub-id>.<pub-id pub-id-type="pmid">21546393</pub-id></mixed-citation>
    </ref>
    <ref id="ref64">
      <label>64.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Subramanian</surname><given-names>A</given-names></string-name>, <string-name><surname>Tamayo</surname><given-names>P</given-names></string-name>, <string-name><surname>Mootha</surname><given-names>VK</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles</article-title>. <source>Proc Natl Acad Sci</source><year>2005</year>;<volume>102</volume>:<fpage>15545</fpage>â<lpage>50</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0506580102</pub-id>.<pub-id pub-id-type="pmid">16199517</pub-id></mixed-citation>
    </ref>
    <ref id="ref65">
      <label>65.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>T</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>C</given-names></string-name>, <string-name><surname>Ogihara</surname><given-names>M</given-names></string-name></person-group>. <article-title>A comparative study of feature selection and multiclass classification methods for tissue classification based on gene expression</article-title>. <source>Bioinformatics</source><year>2004</year>;<volume>20</volume>:<fpage>2429</fpage>â<lpage>37</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bth267</pub-id>.<pub-id pub-id-type="pmid">15087314</pub-id></mixed-citation>
    </ref>
    <ref id="ref66">
      <label>66.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sharma</surname><given-names>A</given-names></string-name>, <string-name><surname>Imoto</surname><given-names>S</given-names></string-name>, <string-name><surname>Miyano</surname><given-names>S</given-names></string-name></person-group>. <article-title>A top-r feature selection algorithm for microarray gene expression data</article-title>. <source>IEEE/ACM Trans Comput Biol Bioinform</source><year>2012</year>;<volume>9</volume>:<fpage>754</fpage>â<lpage>64</lpage>. <pub-id pub-id-type="doi">10.1109/TCBB.2011.151</pub-id>.<pub-id pub-id-type="pmid">22084149</pub-id></mixed-citation>
    </ref>
    <ref id="ref67">
      <label>67.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tibshirani</surname><given-names>R</given-names></string-name></person-group>. <article-title>Regression shrinkage and selection via the Lasso</article-title>. <source>J R Stat Soc B Methodol</source><year>1996</year>;<volume>58</volume>:<fpage>267</fpage>â<lpage>88</lpage>. <pub-id pub-id-type="doi">10.1111/j.2517-6161.1996.tb02080.x</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref68">
      <label>68.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Butler</surname><given-names>A</given-names></string-name>, <string-name><surname>Hoffman</surname><given-names>P</given-names></string-name>, <string-name><surname>Smibert</surname><given-names>P</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Integrating single-cell transcriptomic data across different conditions, technologies, and species</article-title>. <source>Nat Biotechnol</source><year>2018</year>;<volume>36</volume>:<fpage>411</fpage>â<lpage>20</lpage>. <pub-id pub-id-type="doi">10.1038/nbt.4096</pub-id>.<pub-id pub-id-type="pmid">29608179</pub-id></mixed-citation>
    </ref>
    <ref id="ref69">
      <label>69.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Uhlen</surname><given-names>M</given-names></string-name>, <string-name><surname>Fagerberg</surname><given-names>L</given-names></string-name>, <string-name><surname>Hallstrom</surname><given-names>BM</given-names></string-name>, <etal>etÂ al.</etal></person-group><article-title>Tissue-based map of the human proteome</article-title>. <source>Science</source><year>2015</year>;<volume>347</volume>:<fpage>1260419</fpage>â<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1126/science.1260419</pub-id>.<pub-id pub-id-type="pmid">25613900</pub-id></mixed-citation>
    </ref>
    <ref id="ref70">
      <label>70.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eisenberg</surname><given-names>E</given-names></string-name>, <string-name><surname>Levanon</surname><given-names>EY</given-names></string-name></person-group>. <article-title>Human housekeeping genes, revisited</article-title>. <source>Trends Genet</source><year>2013</year>;<volume>29</volume>:<fpage>569</fpage>â<lpage>74</lpage>. <pub-id pub-id-type="doi">10.1016/j.tig.2013.05.010</pub-id>.<pub-id pub-id-type="pmid">23810203</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
