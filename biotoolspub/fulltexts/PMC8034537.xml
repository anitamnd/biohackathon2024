<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8034537</article-id>
    <article-id pub-id-type="pmid">33416852</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btaa1094</article-id>
    <article-id pub-id-type="publisher-id">btaa1094</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Bioimage Informatics</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>EM-stellar: benchmarking deep learning for electron microscopy image segmentation</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0496-5219</contrib-id>
        <name>
          <surname>Khadangi</surname>
          <given-names>Afshin</given-names>
        </name>
        <aff><institution>Department of Biomedical Engineering, University of Melbourne</institution>, Victoria, 3000, <country country="AU">Australia</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0148-7733</contrib-id>
        <name>
          <surname>Boudier</surname>
          <given-names>Thomas</given-names>
        </name>
        <aff><institution>Institute of Molecular Biology, Academia Sinica</institution>, Taipei, <country country="TW">Taiwan</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-5509-402X</contrib-id>
        <name>
          <surname>Rajagopal</surname>
          <given-names>Vijay</given-names>
        </name>
        <xref rid="btaa1094-cor1" ref-type="corresp"/>
        <aff><institution>Department of Biomedical Engineering, University of Melbourne</institution>, Victoria, 3000, <country country="AU">Australia</country></aff>
        <!--vijay.rajagopal@unimelb.edu.au-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Valencia</surname>
          <given-names>Alfonso</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btaa1094-cor1">To whom correspondence should be addressed. <email>vijay.rajagopal@unimelb.edu.au</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>01</day>
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2021-01-08">
      <day>08</day>
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>08</day>
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <volume>37</volume>
    <issue>1</issue>
    <fpage>97</fpage>
    <lpage>106</lpage>
    <history>
      <date date-type="received">
        <day>29</day>
        <month>7</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>06</day>
        <month>10</month>
        <year>2020</year>
      </date>
      <date date-type="editorial-decision">
        <day>06</day>
        <month>12</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>22</day>
        <month>12</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Â© The Author(s) 2021. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btaa1094.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>The inherent low contrast of electron microscopy (EM) datasets presents a significant challenge for rapid segmentation of cellular ultrastructures from EM data. This challenge is particularly prominent when working with high-resolution big-datasets that are now acquired using electron tomography and serial block-face imaging techniques. Deep learning (DL) methods offer an exciting opportunity to automate the segmentation process by learning from manual annotations of a small sample of EM data. While many DL methods are being rapidly adopted to segment EM data no benchmark analysis has been conducted on these methods to date.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We present EM-stellar, a platform that is hosted on Google Colab that can be used to benchmark the performance of a range of state-of-the-art DL methods on user-provided datasets. Using EM-stellar we show that the performance of any DL method is dependent on the properties of the images being segmented. It also follows that no single DL method performs consistently across all performance evaluation metrics.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>EM-stellar (code and data) is written in Python and is freely available under MIT license on GitHub (<ext-link xlink:href="https://github.com/cellsmb/em-stellar" ext-link-type="uri">https://github.com/cellsmb/em-stellar</ext-link>).</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>LIEF</institution>
          </institution-wrap>
        </funding-source>
        <award-id>LE170100200</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>NIH</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>NHLBI</institution>
            <institution-id institution-id-type="DOI">10.13039/100000050</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="10"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Electron microscopy (EM) is a fundamentally important modality for basic biomedical science research. In recent years, we have seen significant advances in EM technologies with the advent of, first, electron tomography and, more recently, focused ion-beam and serial block-face scanning EM (<xref rid="btaa1094-B12" ref-type="bibr">Hussain <italic toggle="yes">et al.</italic>, 2018</xref>). These technologies generate giga- and tera-bytes of high-resolution images of subcellular architecture which must be segmented manually or using image segmentation algorithms. The inherent low contrast in EM has motivated the use of crowd-sourcing platforms, and image segmentation challenges such as to reduce the image postprocessing time. Deep learning (DL) is a powerful approach to image segmentation that is being widely explored as a way to segment high-throughput biological datasets, including EM images (<xref rid="btaa1094-B31" ref-type="bibr">Xing <italic toggle="yes">et al.</italic>, 2018</xref>). In recent years, there have been several efforts to streamline the usage of such technologies for the community (<xref rid="btaa1094-B9" ref-type="bibr">Haberl <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btaa1094-B18" ref-type="bibr">Khadangi <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btaa1094-B25" ref-type="bibr">Schmidt <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btaa1094-B30" ref-type="bibr">Von Chamier <italic toggle="yes">et al.</italic>, 2020</xref>). One crucial question that arises is whether we can use such platforms to segment all types of EM data and whether they have inherent limitations in segmenting particular types of ultrastructures. Typically, these platforms utilize one unique or a limited number of available deep neural network architectures. No study has investigated the relationship between the choice of the DL method and the segmentation performance. Moreover, segmentation performance evaluation remains under investigated as current studies use a limited number of the evaluation metrics, and the impact of the choice of evaluation metric has not been investigated.</p>
    <p>One of the other challenges that the community faces when using such methods is the lack of sophisticated DL utilities and functionalities as the current platforms often resort to demo-based DL applications. For example, such platforms opt for one single optimization method or use a limited set of evaluation criteria as the inbuilt evaluation metrics of the popular application programming interfaces (API) are often limited. Or the segmentation objective function or loss function is constrained to a limited number of inbuilt functions that such API provide. However, we have witnessed in recent years that successful DL applications in computer vision problems rely on a strategic blend of data processing, network architecture, optimization method, loss function, validation method, validation criteria and hyperparameter tuning. We have also seen how the choice of network architecture, loss function or optimization method can affect the DL performance (<xref rid="btaa1094-B15" ref-type="bibr">Janocha and Czarnecki, 2017</xref>; <xref rid="btaa1094-B16" ref-type="bibr">Khadangi <italic toggle="yes">et al.</italic>, 2018</xref>, <xref rid="btaa1094-B17" ref-type="bibr">2019</xref>; <xref rid="btaa1094-B19" ref-type="bibr">Khadangi and Zarandi, 2016</xref>; <xref rid="btaa1094-B21" ref-type="bibr">Lin <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btaa1094-B22" ref-type="bibr">Liu <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btaa1094-B29" ref-type="bibr">Tan and Le, 2019</xref>). In addition to the above, the cost-effectiveness or the computational efficiency of DL applications have not been investigated before.</p>
    <p>We present EM-stellar (the official implementation is provided as a Colab Notebook on GitHub (https://github.com/cellsmb/em-stellar). We also provide a python implementation for the users who might face issues with storage limitations on Google Drive or have data security and privacy concerns), a comprehensive interface between the user and the DL application that is dedicated to EM image segmentation. <xref rid="btaa1094-F1" ref-type="fig">FigureÂ 1</xref> represents the workflow of EM-stellar and the analysis that we have investigated in this study. EM-stellar provides a wide range of DL network architectures, evaluation metrics and easy to use utilities. Such utilities involve a wide range of custom loss functions, validation criteria, state-of-the-art optimization methods that minimize the hyperarameter tuning and K-fold cross-validation. Moreover, it enables the user to benchmark the performances of a diverse set of DL algorithms and use the desired methods as the ensemble of models for the final inference stage. EM-stellar is provided as the self-explanatory Jupyter Notebook for Google Colab which simplifies the use of such sophisticated technologies and utilities to a set of simple user clicks. This approach will save lots of time as the user will not face problems with software dependencies installment, and they do not need to learn the workflow of applications as EM-stellar is ready to use interface with guidelines of the usage for the users.</p>
    <fig position="float" id="btaa1094-F1">
      <label>Fig. 1.</label>
      <caption>
        <p>Overview of the EM-stellar. Top of the figure shows the workflow of the EM-stellar. The user uploads the raw data to Google Drive and uses networks to segment the raw data. A wide range of metrics can be used to monitor the validation or to assess the inference performance. Moreover, in this study, we have performed a wide range of analysis including complexity analysis, convergence times. The bottom (computational demand comparison, BS and RT represent batch size and running time, respectively) shows effect of the batch size on the segmentation performance and the computational demand. As shown, increasing the batch size will require more GPU resources and, hence incurring more charges. We also show that increasing the batch size during the training expedites the convergence and often leads to better inference results. Moreover, we have also compared the DL performance with the previously developed machine-learning software packages including ilastik and Weka (we provide detailed analysis on our experiments with Weka and ilastik in <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>) </p>
      </caption>
      <graphic xlink:href="btaa1094f1" position="float"/>
    </fig>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Deep convolutional neural networks</title>
      <p>Here, we briefly outline the deep neural networks that we have included in this study. The methods have been ordered chronologically by year and month. Software packages used in this study had open-source licenses. We use modified versions of the mentioned networks to test their performance on experimental datasets, and the corresponding architectures were correctly implemented to the best of our knowledge as validated with the literature.
</p>
      <list list-type="order">
        <list-item>
          <p>VGG (<xref rid="btaa1094-B26" ref-type="bibr">Simonyan and Zisserman, 2014</xref>): VGG was proposed in 2014 as one of the first deep convolutional neural networks used for ImageNet challenge (<xref rid="btaa1094-B6" ref-type="bibr">Deng <italic toggle="yes">et al.</italic>, 2009</xref>) where the authors had proposed to push the depth to 16 or even 19 layers. The network uses tiny <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mn>3</mml:mn><mml:mo>Ã</mml:mo><mml:mn>3</mml:mn></mml:math></inline-formula> convolution filters and rectified linear units ((ReLU) as the main activation function. We have used VGG-16 in this study as the encoder, then we have employed two-dimensional (2D) upsampling with skip connections to retrieve the feature channels back to the original resolution. Batch normalization (<xref rid="btaa1094-B14" ref-type="bibr">Ioffe and Szegedy, 2015</xref>) has been used in the network to stabilize the training. The kernels were initialized using He-normal initialization method (<xref rid="btaa1094-B10" ref-type="bibr">He <italic toggle="yes">et al.</italic>, 2015</xref>).</p>
        </list-item>
        <list-item>
          <p>PReLU-net (<xref rid="btaa1094-B10" ref-type="bibr">He <italic toggle="yes">et al.</italic>, 2015</xref>): Parametric rectified linear unit (PReLU) was proposed in 2015 to generalize ReLU. The authors had also proposed a novel initialization method (He-normal) for kernels which improved the classification performance on ImageNet challenge. We adapted PReLU-net to use in this study where batch normalization has been used to stabilize the training.</p>
        </list-item>
        <list-item>
          <p>U-net (<xref rid="btaa1094-B24" ref-type="bibr">Ronneberger <italic toggle="yes">et al.</italic>, 2015</xref>): U-net was proposed in 2015 for medical and biological image segmentation. The network uses two symmetric paths, namely called contractive and expansive paths to enhance capturing context and localization, respectively. In this study, we have implemented U-net in two versions: in the first version, we have used batch normalization across all the layers; however, the second version lacks batch normalization layers. ResNet (<xref rid="btaa1094-B11" ref-type="bibr">He <italic toggle="yes">et al.</italic>, 2016</xref>): Deep residual learning was proposed in 2016 to ease the training deep neural networks by reducing their complexities. The authors had evaluated a 152-layer residual network which had eight times more depth than VGG, but representing lower complexity as compared to VGG. In this study, we have used ResNet-50 as the encoder, and a U-net like decoder with skip connections have been utilized to retrieve the feature channels back to the original input resolution.</p>
        </list-item>
        <list-item>
          <p>SegNet (<xref rid="btaa1094-B3" ref-type="bibr">Badrinarayanan <italic toggle="yes">et al.</italic>, 2017</xref>): SegNet was proposed in 2017 as a deep, fully convolutional neural network for semantic pixel-wise segmentation. The network consists of an encoder network followed by a decoder network and subsequently pixel-wise classification layer. We have used SegNet in this study to segment the EM images; however, we have modified the output layer to suit the binary classification task.</p>
        </list-item>
        <list-item>
          <p>CDeep3M (<xref rid="btaa1094-B9" ref-type="bibr">Haberl <italic toggle="yes">et al.</italic>, 2018</xref>): CDeep3M was proposed in 2018 to facilitate access to complex computational environments and high-performance computational resources for the community. The authors had implemented InceptionResnetV2 (<xref rid="btaa1094-B27" ref-type="bibr">Szegedy <italic toggle="yes">et al.</italic>, 2017</xref>) using Caffe on Amazon Web Services (AWS) EC2 instance. Access to these facilities requires the user to pay for the resources on an hourly rate basis for both training and inference. CDeep3M offers 2D and three-dimensional (3D) segmentation pipelines.</p>
        </list-item>
        <list-item>
          <p>EM-net (<xref rid="btaa1094-B18" ref-type="bibr">Khadangi <italic toggle="yes">et al.</italic>, 2020</xref>): EM-net was proposed in 2020 for 2D segmentation of EM images. The authors have proposed trainable linear units which generalize PReLU and ReLU and have evaluated the proposed network and the base classifiers on a FIB-SEM cardiac dataset and ISBI challenge for neuronal stacks segmentation. EM-net represents lower computational complexity in terms of the number of trainable parameters and floating point operations per second (FLOPs).</p>
        </list-item>
      </list>
    </sec>
    <sec>
      <title>2.2 Data</title>
      <p>We used two publicly available datasets in this study. The first dataset includes left ventricular myocyte FIB-SEM image datasets collected from mice, as described previously (<xref rid="btaa1094-B7" ref-type="bibr">Glancy <italic toggle="yes">et al.</italic>, 2015</xref>). We extracted 24 random patches from this dataset each having <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mn>512</mml:mn><mml:mo>Â </mml:mo><mml:mo>Ã</mml:mo><mml:mo>Â </mml:mo><mml:mn>512</mml:mn><mml:mo>Â </mml:mo></mml:math></inline-formula>pixels for training, testing and validation. After we manually annotated mitochondria on this sample, we split data randomly into training, validation and testing by 16/24, 4/24 and 4/24, respectively. The second dataset involves mice lateral habenula serial block-face scanning electron microscopy (SBEM) <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mn>1024</mml:mn><mml:mo>Â </mml:mo><mml:mo>Ã</mml:mo><mml:mo>Â </mml:mo><mml:mn>1024</mml:mn><mml:mo>Â </mml:mo><mml:mo>Ã</mml:mo><mml:mo>Â </mml:mo><mml:mn>80</mml:mn></mml:math></inline-formula> voxels, as described previously (<xref rid="btaa1094-B9" ref-type="bibr">Haberl <italic toggle="yes">et al.</italic>, 2018</xref>). We extracted 320 random patches from this dataset and the corresponding binary mitochondria masks that had already been annotated for training, validation and testing. We split data randomly into training, validation and testing by 80%, 10% and 10%, respectively. All the random data splits were performed using K-fold cross-validation, and the inference performance is reported based on the best fold model.</p>
    </sec>
    <sec>
      <title>2.3 Training and testing</title>
      <p>All the experiments in this study except CDeep3M were implemented using TensorFlow GPU 1.8.0 CUDA 9.0 (<xref rid="btaa1094-B1" ref-type="bibr">Abadi <italic toggle="yes">et al.</italic>, 2016</xref>) and KERAS 2.2.4 (<xref rid="btaa1094-B23" ref-type="bibr">others, 2015</xref>). These experiments were performed on a GPU cluster, HPC Spartan (<xref rid="btaa1094-B20" ref-type="bibr">Lafayette <italic toggle="yes">et al.</italic>, 2016</xref>) as described in(<xref rid="btaa1094-B18" ref-type="bibr">Khadangi <italic toggle="yes">et al.</italic> (2020)</xref>. A stack of 100GB GPU instance was launched on AWS p3.2xlarge in the US West (Oregon) region to train CDeep3M. CDeep3M is implemented in Caffe, and we utilized the default settings for training as described in <xref rid="btaa1094-B9" ref-type="bibr">Haberl <italic toggle="yes">et al.</italic> (2018)</xref> and <xref rid="btaa1094-B18" ref-type="bibr">Khadangi <italic toggle="yes">et al.</italic> (2020)</xref>. More details about the training and testing are provided in <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Overview of DL methods</title>
      <p>We performed an extensive survey of the literature to identify state-of-the-art deep neural networks that have been utilized for EM image segmentation. We chose CDeep3M (<xref rid="btaa1094-B9" ref-type="bibr">Haberl <italic toggle="yes">et al.</italic>, 2018</xref>), EM-net (<xref rid="btaa1094-B18" ref-type="bibr">Khadangi <italic toggle="yes">et al.</italic>, 2020</xref>), PReLU-net (<xref rid="btaa1094-B10" ref-type="bibr">He <italic toggle="yes">et al.</italic>, 2015</xref>), ResNet-50 (<xref rid="btaa1094-B11" ref-type="bibr">He <italic toggle="yes">et al.</italic>, 2016</xref>), SegNet (<xref rid="btaa1094-B3" ref-type="bibr">Badrinarayanan <italic toggle="yes">et al.</italic>, 2017</xref>), U-net (<xref rid="btaa1094-B24" ref-type="bibr">Ronneberger <italic toggle="yes">et al.</italic>, 2015</xref>) and VGG-16 (<xref rid="btaa1094-B26" ref-type="bibr">Simonyan and Zisserman, 2014</xref>) for our experiments. Among these methods, we have experimented EM-net with all of its seven base classifiers bringing the total number of networks and methods to a maximum of thirteen. We used two publicly available focused ion-beam scanning electron microscopy (FIB-SEM) datasets for our experiments and evaluation purposes. We utilized a wide range of segmentation evaluation metrics to compare the results including F1-score, Foreground-restricted Rand Scoring after border thinning [<inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Rand</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="italic">thinned</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula>] Foreground-restricted Information-Theoretic Scoring after border thinning [<inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Info</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="italic">thinned</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula>] (<xref rid="btaa1094-B2" ref-type="bibr">Arganda-Carreras <italic toggle="yes">et al.</italic>, 2015</xref>). More details about datasets and evaluation metrics are highlighted in methods section.</p>
    </sec>
    <sec>
      <title>3.2 EM-net variants demonstrate reliable learning capacity on both small and large datasets</title>
      <p>We trained and evaluated chosen networks with two FIB-SEM datasets, including one small cardiac dataset comprising <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mn>24</mml:mn><mml:mo>Â </mml:mo></mml:math></inline-formula>serial sections each of pixel size <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mn>512</mml:mn><mml:mo>Â </mml:mo><mml:mo>Ã</mml:mo><mml:mo>Â </mml:mo><mml:mn>512</mml:mn></mml:math></inline-formula>, and another large neuronal dataset consisting of <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mn>320</mml:mn></mml:math></inline-formula> serial sections with the same image size as the cardiac dataset. Mitochondria were manually annotated on both datasets. <xref rid="btaa1094-F2" ref-type="fig">FigureÂ 2</xref> illustrates the results of evaluating networks on the test datasets that were held out randomly and not used for training. The result values have been normalized using minâmax normalization per metric category for comparison.</p>
      <fig position="float" id="btaa1094-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>Heatmap of evaluation metrics for different methods based on the test datasets. The values are normalized using minâmax normalization per metric category. The black boxes correspond to EM-net base classifiers, including the ensemble methods. Top: cardiac, bottom: neuronal data</p>
        </caption>
        <graphic xlink:href="btaa1094f2" position="float"/>
      </fig>
      <p>As shown, despite the difference in size between these two datasets, EM-net variants (grouped within a box on both heatmaps) demonstrate competitive evaluation metric values when compared to other methods. The ensemble of top EM-net base classifiers outperforms other methods majority of the metrics on the cardiac dataset; however, the segmentation performance metric values were not as high performing on the neuronal dataset based on average voting.</p>
    </sec>
    <sec>
      <title>3.3 No one network can fit them all</title>
      <p><xref rid="btaa1094-F2" ref-type="fig">FigureÂ 2</xref> shows how the underlying texture and intensity distribution of different datasets, and the target ultrastructures can affect the performance of a deep neural network in segmenting a dataset. One network cannot achieve high performance for all datasetsâone network cannot fit them all. Considering U-net BN and EM-net V2 4X, both methods demonstrate only above-average performance on segmenting the cardiac dataset in terms of the <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Rand</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="italic">thinned</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> score; however, they achieve top performance based on the same metric for the neuronal data. Additionally, almost all of the methods represent relatively similar performance based on sensitivity metric for the cardiac dataset, whereas they have dropped below 50% on the neuronal dataset. CDeep3M demonstrates above-average performance for the cardiac dataset, whereas it provides inferior performance on the neuronal dataset. However, <xref rid="btaa1094-F2" ref-type="fig">FigureÂ 2</xref> implies that an ensemble of top classifiers may lead to reliable performance across different datasets.</p>
    </sec>
    <sec>
      <title>3.4 Evaluation metrics remain subjective and probably unique to the deep neural network</title>
      <p>Choosing the right evaluation metric for segmenting EM data is a critical and still challenging step as depending on the objective of the segmentation, the user might prefer a specific set of segmentation metrics (<xref rid="btaa1094-B2" ref-type="bibr">Arganda-Carreras <italic toggle="yes">et al.</italic>, 2015</xref>). In other words, there is no one universal evaluation metric for such tasks. The choice of such metrics might even depend on the segmentation task; e.g. 2D or 3D segmentation may require different evaluation metrics. One previous study (<xref rid="btaa1094-B28" ref-type="bibr">Taha and Hanbury, 2015</xref>) has investigated benchmarking segmentation metrics for biomedical images in the 3D setting. Still, most of the studies have opted for F1-score and Jaccard index as the segmentation metric of choice. In one other research (<xref rid="btaa1094-B4" ref-type="bibr">Caicedo <italic toggle="yes">et al.</italic>, 2019</xref>), the same metrics have been utilized as the main evaluation metrics for nucleus segmentation.</p>
      <p>We extended our analysis to monitor the response of the neural networks to different evaluation metrics. We followed this aim as the evaluation metrics reported for EM image segmentation remains sparse in the literature, and no study has investigated such a broad range of analysis on evaluation metrics. Our analysis shows that performances of these networks are subject to change depending on the evaluation criteria. Take the result of U-net BN on neuronal test dataset as an example shown in <xref rid="btaa1094-F2" ref-type="fig">FigureÂ 2</xref>. This network achieved top-performing <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Rand</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="italic">thinned</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> score; however, it demonstrated average performance when using other metrics, including accuracy, e.g. Moreover, our analysis shows that the Jaccard similarity index and F1-score are mostly correlated for those instances that have achieved top Jaccard index scores.</p>
      <p>In addition to the above, we found that some methods demonstrate unique behavior when applied to different datasets. As shown in <xref rid="btaa1094-F2" ref-type="fig">FigureÂ 2</xref>, CDeep3M demonstrates the same performance for specificity and PPV, meaning that this network produces minimal false-negative segmentation instances. However, the performance of VGG implies that this network delivers low sensitivity and high specificity on both neuronal and cardiac datasets. These findings suggest that the architecture of the deep neuronal networks and the underlying layers can affect the performance of the networks when evaluated with different metrics. In summary, the users might prefer one network over another depending on the desirable evaluation metrics, and they should not expect that one method will be the top performer for all the metrics.</p>
    </sec>
    <sec>
      <title>3.5 Convergence times vary depending on the size of the dataset or the underlying data structures</title>
      <p><xref rid="btaa1094-F3" ref-type="fig">FigureÂ 3</xref> shows the convergence times of the networks on both cardiac and neuronal datasets according to the validation metrics that we have chosen during the training. The convergence times imply that the large ground-truth datasets (in this case, neuronal dataset), and potentially diverse structural variations in the data will impact the convergence time of the network. However, this does not necessarily mean that the convergence times are positively correlated with the data size only.</p>
      <fig position="float" id="btaa1094-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>Convergence times (hours) of different networks for cardiac and neuronal data based on the different evaluation metrics. These times have been reported based on our runnings on four parallelly pooled Nvidia Tesla P100 GPUs</p>
        </caption>
        <graphic xlink:href="btaa1094f3" position="float"/>
      </fig>
      <p>Take EM-net V1 BN and V1 BN 2X as an example shown in <xref rid="btaa1094-F3" ref-type="fig">FigureÂ 3</xref>. Based on a comparison between the convergence times of these two networks for the cardiac dataset, one user might expect that V1 BN 2X will demonstrate lower mean and median values for the convergence times relative to the V1 BN on the neuronal dataset as well. However, <xref rid="btaa1094-F3" ref-type="fig">FigureÂ 3</xref> shows precisely the opposite. On the other hand, U-net and U-net BN demonstrate relatively similar convergence times based on their mean and median values for the neuronal dataset; however, U-net BN has converged faster than U-net on the cardiac dataset. Our analysis shows that convergence times does not only depend on the ground-truth data size but is also affected by underlying data structures and the feature bank of the network used for the training. In general, EM-net V1 2X and EM-net V2 show less sensitivity to the data size or data structures, as shown in <xref rid="btaa1094-F3" ref-type="fig">FigureÂ 3</xref>.</p>
    </sec>
    <sec>
      <title>3.6 Complex networks might not perform well and might also exhaust resources</title>
      <p><xref rid="btaa1094-F4" ref-type="fig">FigureÂ 4</xref> illustrates the ball chart reporting the complexity of the networks in terms of Giga FLOPs, the associated number of parameters and top <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Rand</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="italic">thinned</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> score (thresholded to above 0.90) on the corresponding test datasets. The operations are reported for one iteration based on an input tensor with the shape of (1, 512, 512, 1) representing a single batch of monochromic image. As shown, ResNet and CDeep3M required the lowest and highest FLOPs, respectively.</p>
      <fig position="float" id="btaa1094-F4">
        <label>Fig. 4.</label>
        <caption>
          <p>Ball chart reporting complexity of networks based on Giga FLOPs and the corresponding performances in terms of the <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Rand</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="italic">thinned</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mo>Â </mml:mo></mml:math></inline-formula>score. This figure also illustrates the number of trainable parameters for individual networks (millions)</p>
        </caption>
        <graphic xlink:href="btaa1094f4" position="float"/>
      </fig>
      <p>First, the number of parameters does not directly reflect the complexity. Considering EM-net V2 4X and U-net BN, both these networks have a relatively similar number of parameters; however, EM-net V2 4X required less computational resources to perform the same job as compared to U-net BN. Second, this figure shows that the high number of parameters or complexity of the networks do not necessarily yield top test performances. This figure shows how EM-net V1 2X and V1 BN 2X have achieved top <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Rand</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="italic">thinned</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> score on the cardiac dataset despite their very low complexity and the number of parameters. However, we can observe that their performances have been not as good when tested on the neuronal dataset but they are still competitive when compared to the VGG and CDeep3M results. Finally, we can observe that U-net BN demonstrates similar performance in terms of <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Rand</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="italic">thinned</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> score for both cardiac and neuronal datasets.</p>
    </sec>
    <sec>
      <title>3.7 Visualization of the intermediate layers reveals the redundancy of the feature channels</title>
      <p>We visualized over 140Â 000 intermediate feature channels for U-net BN, VGG and EM-net V2 4X on the test datasets. We analyzed the 2D correlation between each of the individual channels leading to over than 9 billion feature correlation maps. <xref rid="btaa1094-F5" ref-type="fig">FigureÂ 5</xref> illustrates the distributions of the correlation maps between each block of the networks. Each of these blocks shares the same characteristic as the underlying feature channels, which have the same resolutions. For example, B1 represents the distributions of the feature correlation maps for these three networks within their corresponding block one, and they all have the same feature resolutions in this case (512, 512) in <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> as we have used for training datasets. We have also visualized the interblock feature correlation maps by which we can analyze the relationships of the feature maps within each of the blocks of these networks. For example, take B4 and B3 in <italic toggle="yes">y</italic> and <italic toggle="yes">x</italic> axes, respectively. This location corresponds to 2D contours of the feature correlation distributions between these blocks. It suggests that U-net represents similar feature correlation distributions in blocks three and four; however, VGG and EM-net show much spread distributions which means they extract less redundant feature maps. We have also visualized the scatter plots of these feature correlation maps distributions in the upper-diagonal plots.</p>
      <fig position="float" id="btaa1094-F5">
        <label>Fig. 5.</label>
        <caption>
          <p>Block-wise feature correlation map distributions for EM-net, U-net and VGG. B1 corresponds to the first block (maximum feature resolution), B5 represents the bottleneck of the networks (minimum resolution), and B9 represents the block corresponding to the output node. The upper-diagonal plots show the scatter plots of the feature correlation maps, the bottom-diagonal plots represent the 2D contours of the block-wise distributions and diagonal plots correspond to univariate feature correlation distributions of individual blocks</p>
        </caption>
        <graphic xlink:href="btaa1094f5" position="float"/>
      </fig>
      <p>Our analysis shows that in general, VGG and EM-net demonstrate fewer feature correlations within each block and even between the individual blocks as compared to U-net. The distributions of these feature channel correlation maps reach their maximum between blocks three and four in VGG, implying that features are less correlated within these two blocks. However, EM-net demonstrates less correlation between the feature channels within the block five called âthe bottleneckâ (where almost 30â50% of the features are concentrated here) as compared to the two others.</p>
      <p>From the interblock feature correlation map perspective, we can observe that U-net demonstrates a high correlation between the correlation map distributions of the different blocks as these distributions are centered or peaked. This implies that feature maps extracted by the U-net could potentially lead to redundant feature maps, especially in the bottleneck as almost all the blocks represent the same level of feature correlation map distributions. One study (<xref rid="btaa1094-B13" ref-type="bibr">Iglovikov and Shvets, 2018</xref>) has investigated this phenomenon by tweaking the U-net architecture where they have substituted the encoder of the U-net to the encoder of the VGG-11, and the authors have obtained better performance in terms of Jaccard similarity index.</p>
    </sec>
    <sec>
      <title>3.8 Visualization of the segmentation masks reveals that CDeep3M is less prone to false-negative</title>
      <p>We have visualized the overlays of the segmentation results on the sample cardiac and neuronal test datasets. <xref rid="btaa1094-F6" ref-type="fig">FiguresÂ 6</xref> and <xref rid="btaa1094-F7" ref-type="fig">7</xref> illustrate the overlay of binary masks on the sample cardiac and neuronal test datasets, respectively. These illustrations have been obtained based on the results of EM-net (average and majority voting), CDeep3M and U-net BN. As shown, CDeep3M provides minimal false-negative or missing mitochondria on these images; however, the number of false-positives is higher than EM-net and U-net. U-net and EM-net offer a higher number of false-negative instances in comparison with CDeep3M as they are more prone to missing mitochondria.</p>
      <fig position="float" id="btaa1094-F6">
        <label>Fig. 6.</label>
        <caption>
          <p>Comparison of mitochondria segmentation results on sample cardiac test dataset. Yellow, green, red and blue correspond to true-positive, true-negative, false-negative (missing mitochondria) and false-positive. EM-net, U-net and VGG are less prone to false-positives; however; CDeep3M demonstrates minimum false-negative segmentation errors. The left column represents the sample test FIB-SEM images of cardiomyocytes. Other columns correspond to the overlay of result masks for CDeep3M, EM-net, U-net BN and VGG</p>
        </caption>
        <graphic xlink:href="btaa1094f6" position="float"/>
      </fig>
      <fig position="float" id="btaa1094-F7">
        <label>Fig. 7.</label>
        <caption>
          <p>Comparison of mitochondria segmentation results on sample neuronal test dataset. Yellow, green, red and blue correspond to true-positive, true-negative, false-negative (missing mitochondria) and false-positive. EM-net ensembles and U-net are less prone to false-positives; however; CDeep3M demonstrates minimum false-negative segmentation errors. The left column represents the sample test SBEM images of mice brain cells. Other columns correspond to the overlay of result masks for EM-net ensembles based on average and majority voting, CDeep3M and U-net</p>
        </caption>
        <graphic xlink:href="btaa1094f7" position="float"/>
      </fig>
      <p>We have used a threshold value of 0.5 to obtain binary masks of the segmentation probability maps resulted from the networks in these visualizations, as illustrated in <xref rid="btaa1094-F6" ref-type="fig">FiguresÂ 6</xref> and <xref rid="btaa1094-F7" ref-type="fig">7</xref>. However, metrics like <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Rand</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="italic">thinned</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Info</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="italic">thinned</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> handle such a limitation by thinning the border or using a threshold step value of 0.1. As a result, we can monitor the desired performance metric and finally determine the best performing threshold value.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <p>We have presented EM-stellar, a framework for benchmarking DL methods for EM image segmentation that is hosted on Google Colab. Although a couple of reviews of using DL methods for microscopy image analysis and segmentation have been reported (<xref rid="btaa1094-B5" ref-type="bibr">Carneiro <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btaa1094-B31" ref-type="bibr">Xing <italic toggle="yes">et al.</italic>, 2018</xref>), a comprehensive evaluation of the segmentation methods has not been conducted to date. In this paper, we have compared seven different deep convolutional neural networks for EM image segmentation. Most of the studies reported in the literature are limited to one tissue type such as neuronal microscopy datasets; however, we report our analysis not only using a neuronal dataset but also using cardiac EM data. We also have extended our study to analyze the performance of these methods using a wide range of segmentation metrics.</p>
    <p>Moreover, we report the computational complexity of these algorithms and their associated computational demand. This is the first study in the literature that reports such analysis in the context of EM image segmentation, which is implemented in the cloud for persistent reusability by biologists. Our Colab notebook enables the users to benefit from state-of-the-art software and hardware resources in the context of DL to achieve the maximum segmentation performance.</p>
    <p>We found considerable variation in the segmentation performance metrics across individual algorithms. Our study shows that different deep neural networks perform differently when using a single segmentation metric. Among many validation performance monitoring criteria, high validation F1-score and Jaccard similarity index are associated with high test Jaccard, F1-sore and <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Rand</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="italic">thinned</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> scores. In terms of the objective function, we have found that using binary cross-entropy for highly imbalanced binary segmentation tasks will not necessarily lead to best inference results and using focal loss (<xref rid="btaa1094-B21" ref-type="bibr">Lin <italic toggle="yes">et al.</italic>, 2017</xref>) is highly recommended in such cases. In terms of the optimization methods, using warm-up strategy (<xref rid="btaa1094-B8" ref-type="bibr">Goyal <italic toggle="yes">et al.</italic>, 2017</xref>) has led to best inference performance in ISBI challenge and mitochondria segmentation in both cardiac and neuronal datasets. For small and limited training datasets, complex networks tend to overfit more often; however, they show reliable performance as exposure to an abundant training dataset. Convergence times and computational resource expense depend on both variations of structures in image data and changes across serial sections. Moreover, our experiments suggest that training these networks on GPUs in parallel mode with increased batch size boost the segmentation performance and minimize the convergence times.</p>
    <p>We also report the segmentation performance using ilastik and Weka (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>). Our experiments suggest that DL methods perform better than ilastik and Weka in terms of accuracy. However, ilastik and Weka offer a significant advantage over DL methods as they can save userâs time by segmenting the data with limited and sparse ground-truth labels in the expense of accuracy and require less training time and computational resources.</p>
    <p>Finally, we highlight the importance of ensemble learning in EM image segmentation. Our experiments show that using only one type of classifier or deep neural network, or even one randomly chosen validation dataset will not lead to maximum test segmentation performance. Hence, we have equipped EM-stellar with ensemble learning which enables the user to select the inference model based on majority or average voting. Moreover, EM-stellar allows the user to benefit from K-fold cross-validation, which maximizes the chance of obtaining maximum inference performance.</p>
    <p>To summarize, EM-stellar is a cloud-based platform hosted on Google Colab which gives free access to GPU and TPU resources and enables the user to use state-of-the-art DL methods across a wide range of segmentation performance metrics. It is equipped with several machine-learning strategies including K-fold cross-validation, different loss functions and optimization methods. It enables the user to choose top-performing models for ensemble learning based on report insights that are provided at the end of the training. We recommend the users to use U-net BN and EM-net V2 4X for segmentation when a large training dataset is available; otherwise, they may use EM-net V1 2X. However, the users might opt for arbitrary networks if they aim at using an ensemble of different models. We plan to utilize TPUs in the future as part of EM-stellar release versions and integrate other state-of-the-art networks such as EfficientNet (<xref rid="btaa1094-B29" ref-type="bibr">Tan and Le, 2019</xref>) to deliver maximum performance and efficiency. <xref rid="btaa1094-T1" ref-type="table">TableÂ 1</xref> illustrates a summary of our findings on the network selection strategy for the prospective users.</p>
    <table-wrap position="float" id="btaa1094-T1">
      <label>Table 1.</label>
      <caption>
        <p>Summary of network/method and evaluation criteria strategy selection based on the data size</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col valign="top" align="left" span="1"/>
          <col valign="top" align="left" span="1"/>
          <col valign="top" align="left" span="1"/>
          <col valign="top" align="left" span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th rowspan="1" colspan="1">Data size</th>
            <th rowspan="1" colspan="1">Evaluation criteria</th>
            <th rowspan="1" colspan="1">Network/method</th>
            <th rowspan="1" colspan="1">Error prevalence</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="3" colspan="1">Small</td>
            <td rowspan="3" colspan="1"><inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Rand</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>Â </mml:mo><mml:msup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Info</mml:mi></mml:mrow></mml:msup><mml:mo>Â </mml:mo><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:math></inline-formula> Â <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mi mathvariant="normal">F</mml:mi><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi mathvariant="normal">score</mml:mi></mml:math></inline-formula></td>
            <td rowspan="1" colspan="1">EM-net V1 2X</td>
            <td rowspan="1" colspan="1">False-negative</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Average voting</td>
            <td rowspan="1" colspan="1">False-positive</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Majority voting</td>
            <td rowspan="1" colspan="1">False-negative</td>
          </tr>
          <tr>
            <td rowspan="4" colspan="1">Large</td>
            <td rowspan="3" colspan="1">
              <inline-formula id="IE20">
                <mml:math id="IM20" display="inline" overflow="scroll">
                  <mml:msup>
                    <mml:mrow>
                      <mml:mi>V</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi mathvariant="italic">Rand</mml:mi>
                    </mml:mrow>
                  </mml:msup>
                  <mml:mo>,</mml:mo>
                  <mml:mo>Â </mml:mo>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mi>V</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi mathvariant="italic">Info</mml:mi>
                    </mml:mrow>
                  </mml:msup>
                </mml:math>
              </inline-formula>
            </td>
            <td rowspan="1" colspan="1">CDeep3M</td>
            <td rowspan="1" colspan="1">False-positive</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">EM-net V1 4X, U-net</td>
            <td rowspan="3" colspan="1">False-negative</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Average voting</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Jaccard similarity index</td>
            <td rowspan="1" colspan="1">Majority voting</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>This summary is acquired based on our experiments and may not be generalizable under other data or methodology settings.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btaa1094_Supplementary_Data</label>
      <media xlink:href="btaa1094_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>This research was undertaken using the LIEF HPC-GPGPU Facility hosted at the University of Melbourne. This Facility was established with the assistance of [LIEF Grant LE170100200]. We also thank Dr. Brian Glancy at NIH/NHLBI for access to the FIB-SEM data. </p>
    <p><italic toggle="yes">Financial Support</italic>: none declared.</p>
    <p><italic toggle="yes">Conflict of Interest:</italic> none declared.</p>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btaa1094-B1">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Abadi</surname><given-names>M.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2016</year>) Tensorflow: a system for large-scale machine learning. In: <italic toggle="yes">12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)</italic>, pp. <fpage>265</fpage>â<lpage>283</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arganda-Carreras</surname><given-names>I.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2015</year>) 
<article-title>Crowdsourcing the creation of image segmentation algorithms for connectomics</article-title>. <source>Front. Neuroanat</source>., <volume>9</volume>, <fpage>142</fpage>.<pub-id pub-id-type="pmid">26594156</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1094-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Badrinarayanan</surname><given-names>V.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2017</year>) 
<article-title>Segnet: a deep convolutional encoder-decoder architecture for image segmentation</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>., <volume>39</volume>, <fpage>2481</fpage>â<lpage>2495</lpage>.<pub-id pub-id-type="pmid">28060704</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1094-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Caicedo</surname><given-names>J.C.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2019</year>) 
<article-title>Evaluation of deep learning strategies for nucleus segmentation in fluorescence images</article-title>. <source>Cytometry Part A</source>, <volume>95</volume>, <fpage>952</fpage>â<lpage>965</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B5">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Carneiro</surname><given-names>G.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2017</year>) <part-title>Review of deep learning methods in mammography, cardiovascular, and microscopy image analysis</part-title>. In: <source>Deep Learning and Convolutional Neural Networks for Medical Image Computing</source>. 
<publisher-name>Springer</publisher-name>, pp. <fpage>11</fpage>â<lpage>32</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B6">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Deng</surname><given-names>J.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2009</year>) Imagenet: a large-scale hierarchical image database. In: <italic toggle="yes">2009 IEEE Conference on Computer Vision and Pattern Recognition</italic>. IEEE, pp. <fpage>248</fpage>â<lpage>255</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glancy</surname><given-names>B.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2015</year>) 
<article-title>Mitochondrial reticulum for cellular energy distribution in muscle</article-title>. <source>Nature</source>, <volume>523</volume>, <fpage>617</fpage>â<lpage>620</lpage>.<pub-id pub-id-type="pmid">26223627</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1094-B8">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Goyal</surname><given-names>P.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2017</year>) Accurate, large minibatch SGD: training imagenet in 1 hour. arXiv:170602677.</mixed-citation>
    </ref>
    <ref id="btaa1094-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Haberl</surname><given-names>M.G.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2018</year>) 
<article-title>CDeep3Mâplug-and-play cloud-based deep learning for image segmentation</article-title>. <source>Nat. Methods</source>, <volume>15</volume>, <fpage>677</fpage>â<lpage>680</lpage>.<pub-id pub-id-type="pmid">30171236</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1094-B10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>He</surname><given-names>K.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2015</year>) Delving deep into rectifiers: surpassing human-level performance on imagenet classification. In: <italic toggle="yes">Proceedings of the IEEE International Conference on Computer Vision</italic>, pp. <fpage>1026</fpage>â<lpage>1034</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>He</surname><given-names>K.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2016</year>) Deep residual learning for image recognition. In: <italic toggle="yes">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>, pp. <fpage>770</fpage>â<lpage>778</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hussain</surname><given-names>A.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2018</year>) 
<article-title>An automated workflow for segmenting single adult cardiac cells from large-volume serial block-face scanning electron microscopy data</article-title>. <source>J. Struct. Biol</source>., <volume>202</volume>, <fpage>275</fpage>â<lpage>285</lpage>.<pub-id pub-id-type="pmid">29477758</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1094-B13">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Iglovikov</surname><given-names>V.</given-names></string-name>, <string-name><surname>Shvets</surname><given-names>A.</given-names></string-name></person-group> (<year>2018</year>) Ternausnet: U-net with vgg11 encoder pre-trained on imagenet for image segmentation. arXiv:180105746.</mixed-citation>
    </ref>
    <ref id="btaa1094-B14">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ioffe</surname><given-names>S.</given-names></string-name>, <string-name><surname>Szegedy</surname><given-names>C.</given-names></string-name></person-group> (<year>2015</year>) Batch normalization: accelerating deep network training by reducing internal covariate shift. arXiv:150203167.</mixed-citation>
    </ref>
    <ref id="btaa1094-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Janocha</surname><given-names>K.</given-names></string-name>, <string-name><surname>Czarnecki</surname><given-names>W.M.</given-names></string-name></person-group> (<year>2017</year>) 
<article-title>On loss functions for deep neural networks in classification</article-title>. <source>arXiv:170205659</source>, <volume>1</volume>/<issue>2016</issue>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B16">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Khadangi</surname><given-names>A.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2018</year>) Automated framework to reconstruct 3D model of cardiac Z-disk: an image processing approach. In: <italic toggle="yes">2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</italic>. IEEE, pp. <fpage>877</fpage>â<lpage>884</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khadangi</surname><given-names>A.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2019</year>) 
<article-title>Automated segmentation of cardiomyocyte Z-disks from high-throughput scanning electron microscopy data</article-title>. <source>BMC Med. Inform. Decis. Making</source>, <volume>19</volume>, <fpage>1</fpage>â<lpage>14</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khadangi</surname><given-names>A.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2020</year>) 
<article-title>EM-net: deep learning for electron microscopy image segmentation</article-title>. <source>bioRxiv</source>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B19">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Khadangi</surname><given-names>A.</given-names></string-name>, <string-name><surname>Zarandi</surname><given-names>M.F.</given-names></string-name></person-group> (<year>2016</year>) From type-2 fuzzy rate-based neural networks to social networks' behaviors. In: <italic toggle="yes">2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE).</italic> IEEE, pp. <fpage>1970</fpage>â<lpage>1975</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B20">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Lafayette</surname><given-names>L.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2016</year>) 
<article-title>Spartan performance and flexibility: an HPCâcloud chimera</article-title>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B21">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>T.-Y.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2017</year>) Focal loss for dense object detection. In: <italic toggle="yes">Proceedings of the IEEE International Conference on Computer Vision</italic>, pp. <fpage>2980</fpage>â<lpage>2988</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B22">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>L.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2019</year>) On the variance of the adaptive learning rate and beyond. arXiv:190803265.</mixed-citation>
    </ref>
    <ref id="btaa1094-B23">
      <mixed-citation publication-type="other">others FCa (<year>2015</year>) Keras. <ext-link xlink:href="https://keras.io" ext-link-type="uri">https://keras.io</ext-link>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B24">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ronneberger</surname><given-names>O.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2015</year>) 
<article-title>U</article-title>-net: convolutional networks for biomedical image segmentation. In: <italic toggle="yes">International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>. Springer, pp. <fpage>234</fpage>â<lpage>241</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B25">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Schmidt</surname><given-names>U.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2018</year>) Cell detection with star-convex polygons. In: <italic toggle="yes">International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>. Springer, pp. <fpage>265</fpage>â<lpage>273</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Simonyan</surname><given-names>K.</given-names></string-name>, <string-name><surname>Zisserman</surname><given-names>A.</given-names></string-name></person-group> (<year>2014</year>) 
<article-title>Very deep convolutional networks for large-scale image recognition</article-title>. <source>arXiv:14091556</source>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B27">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Szegedy</surname><given-names>C.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2017</year>) 
<article-title>Inception-v4, inception-resnet and the impact of residual connections on learning</article-title>. In: <italic toggle="yes">Thirty-First AAAI Conference on Artificial Intelligence</italic>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Taha</surname><given-names>A.A.</given-names></string-name>, <string-name><surname>Hanbury</surname><given-names>A.</given-names></string-name></person-group> (<year>2015</year>) 
<article-title>Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool</article-title>. <source>BMC Med. Imaging</source>, <volume>15</volume>, <fpage>29</fpage>.<pub-id pub-id-type="pmid">26263899</pub-id></mixed-citation>
    </ref>
    <ref id="btaa1094-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tan</surname><given-names>M.</given-names></string-name>, <string-name><surname>Le</surname><given-names>Q.V.</given-names></string-name></person-group> (<year>2019</year>) 
<article-title>EfficientNet: rethinking model scaling for convolutional neural networks</article-title>. <source>arXiv:190511946</source>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Von Chamier</surname><given-names>L.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2020</year>) 
<article-title>ZeroCostDL4Mic: an open platform to simplify access and use of deep-learning in microscopy</article-title>. <source>BioRxiv</source>.</mixed-citation>
    </ref>
    <ref id="btaa1094-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xing</surname><given-names>F.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2018</year>) 
<article-title>Deep learning in microscopy image analysis: a survey</article-title>. <source>IEEE Trans. Neural Netw. Learn. Syst</source>., <volume>29</volume>, <fpage>4550</fpage>â<lpage>4568</lpage>.<pub-id pub-id-type="pmid">29989994</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
