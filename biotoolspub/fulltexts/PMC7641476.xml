<?properties open_access?>
<?subarticle pgen.1009141.r001?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS Genet</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS Genet</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-id journal-id-type="pmc">plosgen</journal-id>
    <journal-title-group>
      <journal-title>PLoS Genetics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1553-7390</issn>
    <issn pub-type="epub">1553-7404</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7641476</article-id>
    <article-id pub-id-type="pmid">33095761</article-id>
    <article-id pub-id-type="publisher-id">PGENETICS-D-20-00068</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pgen.1009141</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Computational Biology</subject>
          <subj-group>
            <subject>Genome Analysis</subject>
            <subj-group>
              <subject>Genome-Wide Association Studies</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Genetics</subject>
          <subj-group>
            <subject>Genomics</subject>
            <subj-group>
              <subject>Genome Analysis</subject>
              <subj-group>
                <subject>Genome-Wide Association Studies</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Genetics</subject>
          <subj-group>
            <subject>Human Genetics</subject>
            <subj-group>
              <subject>Genome-Wide Association Studies</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Medical Conditions</subject>
          <subj-group>
            <subject>Respiratory Disorders</subject>
            <subj-group>
              <subject>Asthma</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Pulmonology</subject>
          <subj-group>
            <subject>Respiratory Disorders</subject>
            <subj-group>
              <subject>Asthma</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Clinical Medicine</subject>
          <subj-group>
            <subject>Signs and Symptoms</subject>
            <subj-group>
              <subject>Hyperlipidemia</subject>
              <subj-group>
                <subject>Hypercholesterolemia</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Genetics</subject>
          <subj-group>
            <subject>Single Nucleotide Polymorphisms</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Physiology</subject>
          <subj-group>
            <subject>Physiological Parameters</subject>
            <subj-group>
              <subject>Body Weight</subject>
              <subj-group>
                <subject>Body Mass Index</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Applied Mathematics</subject>
            <subj-group>
              <subject>Algorithms</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Simulation and Modeling</subject>
          <subj-group>
            <subject>Algorithms</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Genetics</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Genetics</subject>
          <subj-group>
            <subject>Heredity</subject>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>A fast and scalable framework for large-scale and ultrahigh-dimensional sparse regression with application to the UK Biobank</article-title>
      <alt-title alt-title-type="running-head">A fast framework for high-dimensional sparse regression with application to the UK Biobank</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Qian</surname>
          <given-names>Junyang</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Formal analysis</role>
        <role content-type="https://casrai.org/credit/">Investigation</role>
        <role content-type="https://casrai.org/credit/">Methodology</role>
        <role content-type="https://casrai.org/credit/">Software</role>
        <role content-type="https://casrai.org/credit/">Visualization</role>
        <role content-type="https://casrai.org/credit/">Writing – original draft</role>
        <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9759-157X</contrib-id>
        <name>
          <surname>Tanigawa</surname>
          <given-names>Yosuke</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Data curation</role>
        <role content-type="https://casrai.org/credit/">Formal analysis</role>
        <role content-type="https://casrai.org/credit/">Investigation</role>
        <role content-type="https://casrai.org/credit/">Software</role>
        <role content-type="https://casrai.org/credit/">Validation</role>
        <role content-type="https://casrai.org/credit/">Visualization</role>
        <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7592-0336</contrib-id>
        <name>
          <surname>Du</surname>
          <given-names>Wenfei</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Investigation</role>
        <role content-type="https://casrai.org/credit/">Visualization</role>
        <role content-type="https://casrai.org/credit/">Writing – original draft</role>
        <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9069-6475</contrib-id>
        <name>
          <surname>Aguirre</surname>
          <given-names>Matthew</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Data curation</role>
        <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3290-3306</contrib-id>
        <name>
          <surname>Chang</surname>
          <given-names>Chris</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Software</role>
        <xref ref-type="aff" rid="aff003">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tibshirani</surname>
          <given-names>Robert</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Conceptualization</role>
        <role content-type="https://casrai.org/credit/">Formal analysis</role>
        <role content-type="https://casrai.org/credit/">Funding acquisition</role>
        <role content-type="https://casrai.org/credit/">Investigation</role>
        <role content-type="https://casrai.org/credit/">Methodology</role>
        <role content-type="https://casrai.org/credit/">Project administration</role>
        <role content-type="https://casrai.org/credit/">Resources</role>
        <role content-type="https://casrai.org/credit/">Supervision</role>
        <role content-type="https://casrai.org/credit/">Validation</role>
        <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1457-9925</contrib-id>
        <name>
          <surname>Rivas</surname>
          <given-names>Manuel A.</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Conceptualization</role>
        <role content-type="https://casrai.org/credit/">Data curation</role>
        <role content-type="https://casrai.org/credit/">Formal analysis</role>
        <role content-type="https://casrai.org/credit/">Funding acquisition</role>
        <role content-type="https://casrai.org/credit/">Investigation</role>
        <role content-type="https://casrai.org/credit/">Methodology</role>
        <role content-type="https://casrai.org/credit/">Project administration</role>
        <role content-type="https://casrai.org/credit/">Resources</role>
        <role content-type="https://casrai.org/credit/">Supervision</role>
        <role content-type="https://casrai.org/credit/">Validation</role>
        <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hastie</surname>
          <given-names>Trevor</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Conceptualization</role>
        <role content-type="https://casrai.org/credit/">Formal analysis</role>
        <role content-type="https://casrai.org/credit/">Funding acquisition</role>
        <role content-type="https://casrai.org/credit/">Investigation</role>
        <role content-type="https://casrai.org/credit/">Methodology</role>
        <role content-type="https://casrai.org/credit/">Project administration</role>
        <role content-type="https://casrai.org/credit/">Resources</role>
        <role content-type="https://casrai.org/credit/">Supervision</role>
        <role content-type="https://casrai.org/credit/">Validation</role>
        <role content-type="https://casrai.org/credit/">Writing – original draft</role>
        <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
        <xref ref-type="corresp" rid="cor001">*</xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>Department of Statistics, Stanford University, Stanford, CA, United States of America</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Department of Biomedical Data Science, Stanford University, Stanford, CA, United States of America</addr-line>
    </aff>
    <aff id="aff003">
      <label>3</label>
      <addr-line>Grail, Inc., Menlo Park, CA, United States of America</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Zhu</surname>
          <given-names>Xiaofeng</given-names>
        </name>
        <role>Editor</role>
        <xref ref-type="aff" rid="edit1"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Case Western Reserve University, UNITED STATES</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p>I have read the journal’s policy and the authors of this manuscript have the following competing interests: Chris Chang is an employee of Grail, Inc.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>hastie@stanford.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>10</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>23</day>
      <month>10</month>
      <year>2020</year>
    </pub-date>
    <volume>16</volume>
    <issue>10</issue>
    <elocation-id>e1009141</elocation-id>
    <history>
      <date date-type="received">
        <day>21</day>
        <month>1</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>4</day>
        <month>9</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2020 Qian et al</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Qian et al</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pgen.1009141.pdf"/>
    <abstract>
      <p>The UK Biobank is a very large, prospective population-based cohort study across the United Kingdom. It provides unprecedented opportunities for researchers to investigate the relationship between genotypic information and phenotypes of interest. Multiple regression methods, compared with genome-wide association studies (GWAS), have already been showed to greatly improve the prediction performance for a variety of phenotypes. In the high-dimensional settings, the lasso, since its first proposal in statistics, has been proved to be an effective method for simultaneous variable selection and estimation. However, the large-scale and ultrahigh dimension seen in the UK Biobank pose new challenges for applying the lasso method, as many existing algorithms and their implementations are not scalable to large applications. In this paper, we propose a computational framework called batch screening iterative lasso (BASIL) that can take advantage of any existing lasso solver and easily build a scalable solution for very large data, including those that are larger than the memory size. We introduce <bold>snpnet</bold>, an <monospace>R</monospace> package that implements the proposed algorithm on top of <bold>glmnet</bold> and optimizes for single nucleotide polymorphism (SNP) datasets. It currently supports <italic>ℓ</italic><sub>1</sub>-penalized linear model, logistic regression, Cox model, and also extends to the elastic net with <italic>ℓ</italic><sub>1</sub>/<italic>ℓ</italic><sub>2</sub> penalty. We demonstrate results on the UK Biobank dataset, where we achieve competitive predictive performance for all four phenotypes considered (height, body mass index, asthma, high cholesterol) using only a small fraction of the variants compared with other established polygenic risk score methods.</p>
    </abstract>
    <abstract abstract-type="summary">
      <title>Author summary</title>
      <p>With the advent and evolution of large-scale and comprehensive biobanks, there come up unprecedented opportunities for researchers to further uncover the complex landscape of human genetics. One major direction that attracts long-standing interest is the investigation of the relationships between genotypes and phenotypes. This includes but doesn’t limit to the identification of genotypes that are significantly associated with the phenotypes, and the prediction of phenotypic values based on the genotypic information. Genome-wide association studies (GWAS) is a very powerful and widely used framework for the former task, having produced a number of very impactful discoveries. However, when it comes to the latter, its performance is fairly limited by the univariate nature. To address this, multiple regression methods have been suggested to fill in the gap. That said, challenges emerge as the dimension and the size of datasets both become large nowadays. In this paper, we present a novel computational framework that enables us to solve efficiently the entire lasso or elastic-net solution path on large-scale and ultrahigh-dimensional data, and therefore make simultaneous variable selection and prediction. Our approach can build on any existing lasso solver for small or moderate-sized problems, scale it up to a big-data solution, and incorporate other extensions easily. We provide a package <bold>snpnet</bold> that extends the <bold>glmnet</bold> package in R and optimizes for large phenotype-genotype data. On the UK Biobank, we observe competitive prediction performance of the lasso and the elastic-net for all four phenotypes considered from the UK Biobank. That said, the scope of our approach goes beyond genetic studies. It can be applied to general sparse regression problems and build scalable solution for a variety of distribution families based on existing solvers.</p>
    </abstract>
    <funding-group>
      <funding-statement>This research has been conducted using the UK Biobank Resource under application number 24983, "Generating effective therapeutic hypotheses from genomic and hospital linkage data" (<ext-link ext-link-type="uri" xlink:href="http://www.ukbiobank.ac.uk/wp-content/uploads/2017/06/24983-Dr-Manuel-Rivas.pdf">http://www.ukbiobank.ac.uk/wp-content/uploads/2017/06/24983-Dr-Manuel-Rivas.pdf</ext-link>). This work was supported by National Human Genome Research Institute (NHGRI) of the National Institutes of Health (NIH) under awards R01HG010140. This work was funded in part by the Two Sigma Graduate Fellowship (to J.Q.), Funai Overseas Scholarship from Funai Foundation for Information Technology and the Stanford University School of Medicine (to Y.T.). NIH grants 5U01HG009080 (to M.A.R.), 5R01EB001988-16 (to R.T.), 5R01EB 001988-21 (to T.H.). NSF grants 19DMS1208164 (to R.T.) and DMS-1407548 (to T.H.). The content is solely the responsibility of the authors and does not necessarily represent the official views of the funders; funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="7"/>
      <table-count count="5"/>
      <page-count count="30"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>PLOS Publication Stage</meta-name>
        <meta-value>vor-update-to-uncorrected-proof</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>Publication Update</meta-name>
        <meta-value>2020-11-04</meta-value>
      </custom-meta>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>The analyses presented in this study were based on data accessed through the UK Biobank: <ext-link ext-link-type="uri" xlink:href="http://www.ukbiobank.ac.uk">http://www.ukbiobank.ac.uk</ext-link>. Experiment scripts are available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/junyangq/scripts_snpnet_paper">https://github.com/junyangq/scripts_snpnet_paper</ext-link>).</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>The analyses presented in this study were based on data accessed through the UK Biobank: <ext-link ext-link-type="uri" xlink:href="http://www.ukbiobank.ac.uk">http://www.ukbiobank.ac.uk</ext-link>. Experiment scripts are available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/junyangq/scripts_snpnet_paper">https://github.com/junyangq/scripts_snpnet_paper</ext-link>).</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>The past two decades have witnessed rapid growth in the amount of data available to us. Many areas such as genomics, neuroscience, economics and Internet services are producing big datasets that have high dimension, large sample size, or both. A variety of statistical methods and computing tools have been developed to accommodate this change. See, for example, [<xref rid="pgen.1009141.ref001" ref-type="bibr">1</xref>–<xref rid="pgen.1009141.ref005" ref-type="bibr">5</xref>] and the references therein for more details.</p>
    <p>In high-dimensional regression problems, we have a large number of predictors, and it is likely that only a subset of them have a relationship with the response and will be useful for prediction. Identifying such a subset is desirable for both scientific interests and the ability to predict outcomes in the future. The lasso [<xref rid="pgen.1009141.ref006" ref-type="bibr">6</xref>] is a widely used and effective method for simultaneous estimation and variable selection. Given a continuous response <inline-formula id="pgen.1009141.e001"><alternatives><graphic xlink:href="pgen.1009141.e001.jpg" id="pgen.1009141.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> and a model matrix <inline-formula id="pgen.1009141.e002"><alternatives><graphic xlink:href="pgen.1009141.e002.jpg" id="pgen.1009141.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mrow><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, it solves the following regularized regression problem.
<disp-formula id="pgen.1009141.e003"><alternatives><graphic xlink:href="pgen.1009141.e003.jpg" id="pgen.1009141.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi>β</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mspace width="4pt"/><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>X</mml:mi><mml:mi>β</mml:mi><mml:mo>‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:msub><mml:mrow><mml:mo>‖</mml:mo><mml:mi>β</mml:mi><mml:mo>‖</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(1)</label></disp-formula>
where <inline-formula id="pgen.1009141.e004"><alternatives><graphic xlink:href="pgen.1009141.e004.jpg" id="pgen.1009141.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:mrow><mml:msub><mml:mrow><mml:mo>‖</mml:mo><mml:mi>x</mml:mi><mml:mo>‖</mml:mo></mml:mrow><mml:mi>q</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mo>|</mml:mo><mml:mi>q</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the vector <italic>ℓ</italic><sub><italic>q</italic></sub> norm of <inline-formula id="pgen.1009141.e005"><alternatives><graphic xlink:href="pgen.1009141.e005.jpg" id="pgen.1009141.e005g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M5"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> and λ ≥ 0 is the tuning parameter. The <italic>ℓ</italic><sub>1</sub> penalty on <italic>β</italic> allows for selection as well as estimation. Normally there is an unpenalized intercept in the model, but for ease of presentation we leave it out, or we may assume that both <italic>X</italic> and <italic>y</italic> have been centered with mean 0. One typically solves the entire lasso solution path over a grid of λ values λ<sub>1</sub> ≥ λ<sub>2</sub> ⋯ ≥λ<sub><italic>L</italic></sub> and chooses the best λ by cross-validation or by predictive performance on an independent validation set. In <monospace>R</monospace> [<xref rid="pgen.1009141.ref007" ref-type="bibr">7</xref>], several packages, such as <bold>glmnet</bold> [<xref rid="pgen.1009141.ref008" ref-type="bibr">8</xref>] and <bold>ncvreg</bold> [<xref rid="pgen.1009141.ref009" ref-type="bibr">9</xref>], provide efficient procedures to obtain the solution path for the Gaussian model <xref ref-type="disp-formula" rid="pgen.1009141.e003">(1)</xref>, and for other generalized linear models with the residual sum of squared replaced by the negative log-likelihood of the corresponding model. Among them, <bold>glmnet</bold>, equipped with highly optimized Fortran subroutines, is widely considered the fastest off-the-shelf lasso solver. It can, for example, fit a sequence of 100 logistic regression models on a sparse dataset with 54 million samples and 7 million predictors within only 2 hours [<xref rid="pgen.1009141.ref010" ref-type="bibr">10</xref>].</p>
    <p>However, as the data become increasingly large, many existing methods and tools may not be able to serve the need, especially if the size exceeds the memory size. Most packages, including the ones mentioned above, assume that the data or at least its sparse representation can be fully loaded in memory and that the remaining memory is sufficient to hold other intermediate results. This becomes a real bottleneck for big datasets. For example, in our motivating application, the UK Biobank genotypes and phenotypes dataset [<xref rid="pgen.1009141.ref011" ref-type="bibr">11</xref>] contains about 500,000 individuals and more than 800,000 genotyped single nucleotide polymorphisms (SNPs) and small indel measurements per person. This provides unprecedented opportunities to explore more comprehensive genotypic relationships with phenotypes of interest. For polygenic traits such as height and body mass index (BMI), specific variants discovered by genome-wide association studies (GWAS) used to explain only a small proportion of the estimated heritability [<xref rid="pgen.1009141.ref012" ref-type="bibr">12</xref>], an upper bound of the proportion of phenotypic variance explained by the genetic components. While GWAS with larger sample size on the UK Biobank can be used to detect more SNPs and rare variants, their prediction performance is fairly limited by univariate models. It is very interesting to see if full-scale multiple regression methods such as the lasso or elastic-net can improve the prediction performance and simultaneously select relevant variants for the phenotypes. That being said, the computational challenges are two fold. First is the memory bound. Even though each bi-allelic SNP value can be represented by only two bits and the <bold>PLINK</bold> software and its <monospace>bed</monospace>/<monospace>pgen</monospace> format [<xref rid="pgen.1009141.ref013" ref-type="bibr">13</xref>, <xref rid="pgen.1009141.ref014" ref-type="bibr">14</xref>] stores such SNP datasets in a binary compressed format, statistical packages such as <bold>glmnet</bold> and <bold>ncvreg</bold> require that the data be loaded in memory in a normal double-precision format. Given its sample size and dimension, the genotype matrix itself will take up around one terabyte of space, which may well exceed the size of the memory available and is infeasible for the packages. Second is the efficiency bound. For a larger-than-RAM dataset, it has to sit on the disk and we may only read part of it into the memory. In such scenario, the overall efficiency of the algorithm is not only determined by the number of basic arithmetic operations but also the disk I/O—data transfer between the memory and the disk—an operation several magnitudes slower than in-memory operations.</p>
    <p>In this paper, we propose an efficient and scalable meta algorithm for the lasso called Batch Screening Iterative Lasso (BASIL) that is applicable to larger-than-RAM datasets and designed to tackle the memory and efficiency bound. It computes the entire lasso path and can easily build on any existing package to make it a scalable solution. As the name suggests, it is done in an iterative fashion on an adaptively screened subset of variables. At each iteration, we exploit an efficient, parallelizable screening operation to significantly reduce the problem to one of manageable size, solve the resulting smaller lasso problem, and then reconstruct and validate a full solution through another efficient, parallelizable step. In other words, the iterations have a screen-solve-check substructure. That being said, it is the goal and also the guarantee of the BASIL algorithm that the final solution exactly solves the full lasso problem <xref ref-type="disp-formula" rid="pgen.1009141.e003">(1)</xref> rather than any approximation, even if the intermediate steps work repeatedly on subsets of variables.</p>
    <p>The screen-solve-check substructure is inspired by [<xref rid="pgen.1009141.ref015" ref-type="bibr">15</xref>] and especially the proposed strong rules. The strong rules state: assume <inline-formula id="pgen.1009141.e006"><alternatives><graphic xlink:href="pgen.1009141.e006.jpg" id="pgen.1009141.e006g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M6"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the lasso solution in <xref ref-type="disp-formula" rid="pgen.1009141.e003">(1)</xref> at λ<sub><italic>k</italic>−1</sub>, then the <italic>j</italic>th predictor is discarded at λ<sub><italic>k</italic></sub> if
<disp-formula id="pgen.1009141.e007"><alternatives><graphic xlink:href="pgen.1009141.e007.jpg" id="pgen.1009141.e007g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M7"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>⊤</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>X</mml:mi><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo>&lt;</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(2)</label></disp-formula>
The key idea is that the inner product above is almost “non-expansive” in λ and that the lasso solution is characterized equivalently by the Karush-Kuhn-Tucker (KKT) condition [<xref rid="pgen.1009141.ref016" ref-type="bibr">16</xref>]. For the lasso, the KKT condition states that <inline-formula id="pgen.1009141.e008"><alternatives><graphic xlink:href="pgen.1009141.e008.jpg" id="pgen.1009141.e008g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M8"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is a solution to <xref ref-type="disp-formula" rid="pgen.1009141.e003">(1)</xref> if for all 1 ≤ <italic>j</italic> ≤ <italic>p</italic>,
<disp-formula id="pgen.1009141.e009"><alternatives><graphic xlink:href="pgen.1009141.e009.jpg" id="pgen.1009141.e009g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mo>·</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>⊤</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>X</mml:mi><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>·</mml:mo><mml:mtext>sign</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="2pt"/><mml:mtext>if</mml:mtext><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>≤</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mtext>if</mml:mtext><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(3)</label></disp-formula>
The KKT condition suggests that the variables discarded based on the strong rules would have coefficient 0 at the next λ<sub><italic>k</italic></sub>. The checking step comes into play because this is not a guarantee. The strong rules can fail, though failures occur rarely when <italic>p</italic> &gt; <italic>n</italic>. In any case, the KKT condition will be checked to see if the coefficients of the left-out variables are indeed 0 at λ<sub><italic>k</italic></sub>. If the check fails, we add in the violated variables and repeat the process. Otherwise, we successfully reconstruct a full solution and move to the next λ. This is the iterative algorithm proposed by these authors and has been implemented efficienly into the <bold>glmnet</bold> package.</p>
    <p>The BASIL algorithm proceeds in a similar way but is designed to optimize for datasets that are too big to fit into the memory. Considering the fact that screening and KKT check need to scan through the entire data and are thus costly in the disk Input/Output (I/O) operations, we attempt to do batch screening and solve <italic>a series of</italic> models (at different λ values) in each iteration, where a single sweep over the full data would suffice. Followed by a checking step, we can obtain the lasso solution for multiple λ’s in one iteration. This can effectively reduce the total number of iterations needed to compute the full solution path and thus reduce the expensive disk read operations that often cause significant delay in the computation. The process is illustrated in <xref ref-type="fig" rid="pgen.1009141.g001">Fig 1</xref> and will be detailed in the next section.</p>
    <fig id="pgen.1009141.g001" orientation="portrait" position="float">
      <object-id pub-id-type="doi">10.1371/journal.pgen.1009141.g001</object-id>
      <label>Fig 1</label>
      <caption>
        <title>The lasso coefficient profile that shows the progression of the BASIL algorithm.</title>
        <p>The horizontal axis represents the index of lambda values, 1 ≤ <italic>ℓ</italic> ≤ <italic>L</italic>, which correspond to the sequence of the regularization parameters, λ<sub>1</sub> &gt; λ<sub>2</sub> &gt; ⋯ &gt; λ<sub><italic>L</italic></sub>. The previously finished part of the path is colored grey, the newly completed and verified is in sky blue, and the part that is newly computed but failed the verification is colored orange. The largest lambda index with the verified model is highlighted with vertical dotted gray line.</p>
      </caption>
      <graphic xlink:href="pgen.1009141.g001"/>
    </fig>
  </sec>
  <sec sec-type="results" id="sec002">
    <title>Results</title>
    <sec id="sec003">
      <title>Overview of the BASIL algorithm</title>
      <p>For convenience, we first introduce some notation. Let Ω = {1, 2, …, <italic>p</italic>} be the universe of variable indices. For 1 ≤ <italic>ℓ</italic> ≤ <italic>L</italic>, let <inline-formula id="pgen.1009141.e010"><alternatives><graphic xlink:href="pgen.1009141.e010.jpg" id="pgen.1009141.e010g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M10"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> be the lasso solution at λ = λ<sub><italic>ℓ</italic></sub>, and <inline-formula id="pgen.1009141.e011"><alternatives><graphic xlink:href="pgen.1009141.e011.jpg" id="pgen.1009141.e011g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M11"><mml:mrow><mml:mi mathvariant="script">A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>j</mml:mi><mml:mo>≤</mml:mo><mml:mi>p</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>≠</mml:mo><mml:mn>0</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> be the active set. When <italic>X</italic> is a matrix, we use <inline-formula id="pgen.1009141.e012"><alternatives><graphic xlink:href="pgen.1009141.e012.jpg" id="pgen.1009141.e012g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M12"><mml:msub><mml:mi>X</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:msub></mml:math></alternatives></inline-formula> to represent the submatrix including only columns indexed by <inline-formula id="pgen.1009141.e013"><alternatives><graphic xlink:href="pgen.1009141.e013.jpg" id="pgen.1009141.e013g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M13"><mml:mi mathvariant="script">S</mml:mi></mml:math></alternatives></inline-formula>. Similarly when <italic>β</italic> is a vector, <inline-formula id="pgen.1009141.e014"><alternatives><graphic xlink:href="pgen.1009141.e014.jpg" id="pgen.1009141.e014g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M14"><mml:msub><mml:mi>β</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:msub></mml:math></alternatives></inline-formula> represents the subvector including only elements indexed by <inline-formula id="pgen.1009141.e015"><alternatives><graphic xlink:href="pgen.1009141.e015.jpg" id="pgen.1009141.e015g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M15"><mml:mi mathvariant="script">S</mml:mi></mml:math></alternatives></inline-formula>. Given any two vectors <inline-formula id="pgen.1009141.e016"><alternatives><graphic xlink:href="pgen.1009141.e016.jpg" id="pgen.1009141.e016g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M16"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, the dot product or inner product can be written as <inline-formula id="pgen.1009141.e017"><alternatives><graphic xlink:href="pgen.1009141.e017.jpg" id="pgen.1009141.e017g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M17"><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mi>⊤</mml:mi></mml:msup><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. Throughout the paper, we use predictors, features, variables and variants interchangeably. We use the strong set to refer to the screened subset of variables on which the lasso fit is computed at each iteration, and the active set to refer to the subset of variables with nonzero lasso coefficients.</p>
      <p>Remember that our goal is to compute the exact lasso solution <xref ref-type="disp-formula" rid="pgen.1009141.e003">(1)</xref> for larger-than-RAM datasets over a grid of regularization parameters λ<sub>1</sub> &gt; λ<sub>2</sub> &gt; ⋯ &gt; λ<sub><italic>L</italic></sub> ≥ 0. We describe the procedure for the Gaussian family in this section and discuss extension to general problems in the next. A common choice is <italic>L</italic> = 100 and <inline-formula id="pgen.1009141.e018"><alternatives><graphic xlink:href="pgen.1009141.e018.jpg" id="pgen.1009141.e018g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M18"><mml:mrow><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>j</mml:mi><mml:mo>≤</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mspace width="2pt"/><mml:mrow><mml:mo>|</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>⊤</mml:mi></mml:msubsup><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>|</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, the largest λ at which the estimated coefficients start to deviate from zero. Here <italic>r</italic><sup>(0)</sup> = <italic>y</italic> if we do not include an intercept term and <inline-formula id="pgen.1009141.e019"><alternatives><graphic xlink:href="pgen.1009141.e019.jpg" id="pgen.1009141.e019g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M19"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> if we do. In general, <italic>r</italic><sup>(0)</sup> is the residual of regressing <italic>y</italic> on the unpenalized variables, if any. The other λ’s can be determined, for example, by an equally spaced array on the log scale. The solution path is found iteratively with a screening-solving-checking substructure similar to the one proposed in [<xref rid="pgen.1009141.ref015" ref-type="bibr">15</xref>]. Designed for large-scale and ultrahigh-dimensional data, the BASIL algorithm can be viewed as a batch version of the strong rules. At each iteration we attempt to find valid lasso solution for <italic>multiple</italic> λ values on the path and thus reduce the burden of disk reads of the big dataset. Specifically, as summarized in Algorithm 1, we start with an empty strong set <inline-formula id="pgen.1009141.e020"><alternatives><graphic xlink:href="pgen.1009141.e020.jpg" id="pgen.1009141.e020g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M20"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>Ø</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and active set <inline-formula id="pgen.1009141.e021"><alternatives><graphic xlink:href="pgen.1009141.e021.jpg" id="pgen.1009141.e021g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M21"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>Ø</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. Each of the following iterations consists of three steps: screening, fitting and checking.</p>
      <p><bold>Algorithm 1</bold> BASIL for the Gaussian Model</p>
      <p specific-use="line">1: <bold>Initialization</bold>: active set <inline-formula id="pgen.1009141.e022"><alternatives><graphic xlink:href="pgen.1009141.e022.jpg" id="pgen.1009141.e022g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M22"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>Ø</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, initial residual <italic>r</italic><sup>(0)</sup> (with respect to the intercept or other unpenalized variables) at λ<sub>1</sub> = λ<sub>max</sub>, a short list of initial parameters Λ<sup>(0)</sup> = {λ<sub>1</sub>, …, λ<sub><italic>L</italic><sup>(0)</sup></sub>}.</p>
      <p specific-use="line">2: <bold>for</bold>
<italic>k</italic> = 0 <bold>to</bold>
<italic>K</italic>
<bold>do</bold></p>
      <p specific-use="line">3:  <bold>Screening</bold>: for each 1 ≤ <italic>j</italic> ≤ <italic>p</italic>, compute inner product with current residual <inline-formula id="pgen.1009141.e023"><alternatives><graphic xlink:href="pgen.1009141.e023.jpg" id="pgen.1009141.e023g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M23"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>〉</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Construct the strong set
<disp-formula id="pgen.1009141.e024"><alternatives><graphic xlink:href="pgen.1009141.e024.jpg" id="pgen.1009141.e024g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M24"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>∪</mml:mo><mml:msubsup><mml:mi mathvariant="script">E</mml:mi><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
 where <inline-formula id="pgen.1009141.e025"><alternatives><graphic xlink:href="pgen.1009141.e025.jpg" id="pgen.1009141.e025g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M25"><mml:msubsup><mml:mi mathvariant="script">E</mml:mi><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> is the set of <italic>M</italic> variables in <inline-formula id="pgen.1009141.e026"><alternatives><graphic xlink:href="pgen.1009141.e026.jpg" id="pgen.1009141.e026g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M26"><mml:mrow><mml:mo>Ω</mml:mo><mml:mo>\</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> with largest |<italic>c</italic><sup>(<italic>k</italic>)</sup>|.</p>
      <p specific-use="line">4:  <bold>Fitting</bold>: for all λ ∈ Λ<sup>(<italic>k</italic>)</sup>, solve the lasso only on the strong set <inline-formula id="pgen.1009141.e027"><alternatives><graphic xlink:href="pgen.1009141.e027.jpg" id="pgen.1009141.e027g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M27"><mml:msup><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>, and find the coefficients <inline-formula id="pgen.1009141.e028"><alternatives><graphic xlink:href="pgen.1009141.e028.jpg" id="pgen.1009141.e028g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M28"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and the residuals <italic>r</italic><sup>(<italic>k</italic>)</sup>(λ).</p>
      <p specific-use="line">5:  <bold>Checking</bold>: search for the smallest λ such that the KKT conditions are satisfied, i.e.,
<disp-formula id="pgen.1009141.e029"><alternatives><graphic xlink:href="pgen.1009141.e029.jpg" id="pgen.1009141.e029g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M29"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo form="prefix" movablelimits="true">min</mml:mo><mml:mo>{</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mo>Λ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>:</mml:mo><mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mo>Ω</mml:mo><mml:mo>\</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>⊤</mml:mi></mml:msubsup><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>}</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
For empty set, we define <inline-formula id="pgen.1009141.e030"><alternatives><graphic xlink:href="pgen.1009141.e030.jpg" id="pgen.1009141.e030g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M30"><mml:msup><mml:mover accent="true"><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> to be the immediate previous λ to Λ<sup>(<italic>k</italic>)</sup> but increment <italic>M</italic> by Δ<italic>M</italic>. Let the current active set <inline-formula id="pgen.1009141.e031"><alternatives><graphic xlink:href="pgen.1009141.e031.jpg" id="pgen.1009141.e031g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M31"><mml:msup><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> and residuals <italic>r</italic><sup>(<italic>k</italic>+1)</sup> defined by the solution at <inline-formula id="pgen.1009141.e032"><alternatives><graphic xlink:href="pgen.1009141.e032.jpg" id="pgen.1009141.e032g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M32"><mml:msup><mml:mover accent="true"><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>. Define the next parameter list <inline-formula id="pgen.1009141.e033"><alternatives><graphic xlink:href="pgen.1009141.e033.jpg" id="pgen.1009141.e033g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M33"><mml:mrow><mml:msup><mml:mi>Λ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>Λ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>:</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mover accent="true"><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Extend this list if it consists of too few elements. For λ ∈ Λ<sup>(<italic>k</italic>)</sup> \ Λ<sup>(<italic>k</italic>+1)</sup>, we obtain exact lasso solutions for the full problem:
<disp-formula id="pgen.1009141.e034"><alternatives><graphic xlink:href="pgen.1009141.e034.jpg" id="pgen.1009141.e034g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M34"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:msup><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>Ω</mml:mo><mml:mo>\</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p specific-use="line">6: <bold>end for</bold></p>
      <p>In the screening step, an updated strong set is found as the candidate for the subsequent fitting. Suppose that so far (valid) lasso solutions have been found for λ<sub>1</sub>, …, λ<sub><italic>ℓ</italic></sub> but not for λ<sub><italic>ℓ</italic>+1</sub>. The new set will be based on the lasso solution at λ<sub><italic>ℓ</italic></sub>. In particular, we will select the top <italic>M</italic> variables with largest absolute inner products <inline-formula id="pgen.1009141.e035"><alternatives><graphic xlink:href="pgen.1009141.e035.jpg" id="pgen.1009141.e035g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M35"><mml:mrow><mml:mo>|</mml:mo><mml:mo>〈</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>X</mml:mi><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. They are the variables that are most likely to be active in the lasso model for the next λ values. In addition, we include the ever-active variables at λ<sub>1</sub>, …, λ<sub><italic>ℓ</italic></sub> because they have been “important” variables and might continue to be important at a later stage.</p>
      <p>In the fitting step, the lasso is fit on the updated strong set for the next λ values λ<sub><italic>ℓ</italic>+1</sub>, …, λ<sub><italic>ℓ</italic>′</sub>. Here <italic>ℓ</italic>′ is often smaller than <italic>L</italic> because we do not have to solve for all of the remaining λ values on this strong set. The full lasso solutions at much smaller λ’s are very likely to have active variables outside of the current strong set. In other words even if we were to compute solutions for those very small λ values on the current strong set, they would probably fail the KKT test. These λ’s are left to later iterations when the strong set is expanded.</p>
      <p>In the checking step, we check if the newly obtained solutions on the strong set can be valid part of the full solutions by evaluating the KKT condition. Given a solution <inline-formula id="pgen.1009141.e036"><alternatives><graphic xlink:href="pgen.1009141.e036.jpg" id="pgen.1009141.e036g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M36"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi mathvariant="script">S</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> to the sub-problem at λ, if we can verify for every left-out variable <italic>j</italic> that <inline-formula id="pgen.1009141.e037"><alternatives><graphic xlink:href="pgen.1009141.e037.jpg" id="pgen.1009141.e037g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M37"><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi mathvariant="script">S</mml:mi></mml:msub><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo>&lt;</mml:mo><mml:mi mathvariant="normal">λ</mml:mi></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, we can then safely set their coefficients to 0. The full lasso solution <inline-formula id="pgen.1009141.e038"><alternatives><graphic xlink:href="pgen.1009141.e038.jpg" id="pgen.1009141.e038g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M38"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is then assembled by letting <inline-formula id="pgen.1009141.e039"><alternatives><graphic xlink:href="pgen.1009141.e039.jpg" id="pgen.1009141.e039g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M39"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi mathvariant="script">S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi mathvariant="script">S</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pgen.1009141.e040"><alternatives><graphic xlink:href="pgen.1009141.e040.jpg" id="pgen.1009141.e040g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M40"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>Ω</mml:mo><mml:mo>\</mml:mo><mml:mi mathvariant="script">S</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. We look for the λ value prior to the one that causes the first failure down the λ sequence and use its residual as the basis for the next screening. Nevertheless, there is still chance that none of the solutions on the current strong set passes the KKT check for the λ subsequence considered in this iterations. That suggests the number of previously added variables in the current iteration was not sufficient. In this case, we are unable to move forward along the λ sequence, but will fall back to the λ value where the strong set was last updated and include Δ<italic>M</italic> more variables based on the sorted absolute inner product.</p>
      <p>The three steps above can be applied repeatedly to roll out the complete lasso solution path for the original problem. However, if our goal is choosing the best model along the path, we can stop fitting once an optimal model is found evidenced by the performance on a validation set. At a high level, we run the iterative procedure on the training data, monitor the error on the validation set, and stop when the model starts to overfit, or in other words, when the validation error shows a clear upward trend.</p>
    </sec>
    <sec id="sec004">
      <title>Extension to general problems</title>
      <p>It is straightforward to extend the algorithm from the Gaussian case to more general problems. In fact, the only changes we need to make are the screening step and the strong set update step. Wherever the strong rules can be applied, we have a corresponding version of the iterative algorithm. In [<xref rid="pgen.1009141.ref015" ref-type="bibr">15</xref>], the general problem is
<disp-formula id="pgen.1009141.e041"><alternatives><graphic xlink:href="pgen.1009141.e041.jpg" id="pgen.1009141.e041g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M41"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi>β</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mspace width="4pt"/><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>r</mml:mi></mml:munderover><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mo>‖</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>‖</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(4)</label></disp-formula>
where <italic>f</italic> is a convex differentiable function, and for all 1 ≤ <italic>j</italic> ≤ <italic>r</italic>, a separate penalty factor <italic>c</italic><sub><italic>j</italic></sub> ≥ 0, <italic>p</italic><sub><italic>j</italic></sub> ≥ 1, and <italic>β</italic><sub><italic>j</italic></sub> can be a scalar or vector whose <inline-formula id="pgen.1009141.e042"><alternatives><graphic xlink:href="pgen.1009141.e042.jpg" id="pgen.1009141.e042g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M42"><mml:msub><mml:mi>ℓ</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub></mml:math></alternatives></inline-formula>-norm is represented by <inline-formula id="pgen.1009141.e043"><alternatives><graphic xlink:href="pgen.1009141.e043.jpg" id="pgen.1009141.e043g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M43"><mml:mrow><mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. If all <italic>c</italic><sub><italic>j</italic></sub>’s are positive, we can derive that the starting value of the λ sequence (i.e. the minimum value of λ such that all coefficients are 0) is <inline-formula id="pgen.1009141.e044"><alternatives><graphic xlink:href="pgen.1009141.e044.jpg" id="pgen.1009141.e044g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mo form="prefix" movablelimits="true">max</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>j</mml:mi><mml:mo>≤</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mo>‖</mml:mo><mml:msub><mml:mo>∇</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></alternatives></inline-formula>. The general strong rule discards predictor <italic>j</italic> if
<disp-formula id="pgen.1009141.e045"><alternatives><graphic xlink:href="pgen.1009141.e045.jpg" id="pgen.1009141.e045g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M45"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(5)</label></disp-formula>
where 1/<italic>p</italic><sub><italic>j</italic></sub> + 1/<italic>q</italic><sub><italic>j</italic></sub> = 1. Hence, our algorithm can adapt and screen by choosing variables with large values of <inline-formula id="pgen.1009141.e046"><alternatives><graphic xlink:href="pgen.1009141.e046.jpg" id="pgen.1009141.e046g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M46"><mml:mrow><mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> that are not in the current active set. We expand in more detail two important applications of the general rule: logistic regression and Cox’s proportional hazards model in survival analysis.</p>
    </sec>
    <sec id="sec005">
      <title>Logistic regression</title>
      <p>In the lasso penalized logistic regression [<xref rid="pgen.1009141.ref017" ref-type="bibr">17</xref>] where the observed outcome <italic>y</italic> ∈ {0, 1}<sup><italic>n</italic></sup>, the convex differential function in <xref ref-type="disp-formula" rid="pgen.1009141.e041">(4)</xref> is
<disp-formula id="pgen.1009141.e047"><alternatives><graphic xlink:href="pgen.1009141.e047.jpg" id="pgen.1009141.e047g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M47"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix">log</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo form="prefix">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <inline-formula id="pgen.1009141.e048"><alternatives><graphic xlink:href="pgen.1009141.e048.jpg" id="pgen.1009141.e048g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M48"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>⊤</mml:mi></mml:msubsup><mml:mi>β</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> for all 1 ≤ <italic>i</italic> ≤ <italic>n</italic>. The rule in <xref ref-type="disp-formula" rid="pgen.1009141.e045">(5)</xref> is reduced to
<disp-formula id="pgen.1009141.e049"><alternatives><graphic xlink:href="pgen.1009141.e049.jpg" id="pgen.1009141.e049g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M49"><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>⊤</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo>&lt;</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <inline-formula id="pgen.1009141.e050"><alternatives><graphic xlink:href="pgen.1009141.e050.jpg" id="pgen.1009141.e050g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M50"><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the predicted probabilities at λ = λ<sub><italic>k</italic>−1</sub>. Similar to the Gaussian case, we can still fit relaxed lasso [<xref rid="pgen.1009141.ref024" ref-type="bibr">24</xref>] and allow adjustment covariates in the model to adjust for confounding effect.</p>
    </sec>
    <sec id="sec006">
      <title>Cox’s proportional hazards model</title>
      <p>In the usual survival analysis framework, for each sample, in addition to the predictors <inline-formula id="pgen.1009141.e051"><alternatives><graphic xlink:href="pgen.1009141.e051.jpg" id="pgen.1009141.e051g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M51"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> and the observed time <italic>y</italic><sub><italic>i</italic></sub>, there is an associated right-censoring indicator <italic>δ</italic><sub><italic>i</italic></sub> ∈ {0, 1} such that <italic>δ</italic><sub><italic>i</italic></sub> = 0 if failure and <italic>δ</italic><sub><italic>i</italic></sub> = 1 if right-censored. Let <italic>t</italic><sub>1</sub> &lt; <italic>t</italic><sub>2</sub> &lt; … &lt; <italic>t</italic><sub><italic>m</italic></sub> be the increasing list of unique failure times, and <italic>j</italic>(<italic>i</italic>) denote the index of the observation failing at time <italic>t</italic><sub><italic>i</italic></sub>. The Cox’s proportional hazards model [<xref rid="pgen.1009141.ref018" ref-type="bibr">18</xref>] assumes the hazard for the <italic>i</italic>th individual as <inline-formula id="pgen.1009141.e052"><alternatives><graphic xlink:href="pgen.1009141.e052.jpg" id="pgen.1009141.e052g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M52"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo form="prefix">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>⊤</mml:mi></mml:msubsup><mml:mi>β</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> where <italic>h</italic><sub>0</sub>(<italic>t</italic>) is a shared baseline hazard at time <italic>t</italic>. We can let <italic>f</italic>(<italic>β</italic>) be the negative log partial likelihood in <xref ref-type="disp-formula" rid="pgen.1009141.e041">(4)</xref> and screen based on its gradient at the most recent lasso solution as suggested in <xref ref-type="disp-formula" rid="pgen.1009141.e045">(5)</xref>. In particular,
<disp-formula id="pgen.1009141.e053"><alternatives><graphic xlink:href="pgen.1009141.e053.jpg" id="pgen.1009141.e053g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M53"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mo>(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>⊤</mml:mi></mml:msubsup><mml:mi>β</mml:mi><mml:mo>-</mml:mo><mml:mo form="prefix">log</mml:mo><mml:mo>(</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mo form="prefix">exp</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>⊤</mml:mi></mml:msubsup><mml:mi>β</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <italic>R</italic><sub><italic>i</italic></sub> is the set of indices <italic>j</italic> with <italic>y</italic><sub><italic>j</italic></sub> ≥ <italic>t</italic><sub><italic>i</italic></sub> (those at risk at time <italic>t</italic><sub><italic>i</italic></sub>). We can derive the associated rule based on <xref ref-type="disp-formula" rid="pgen.1009141.e045">(5)</xref> and thus the survival BASIL algorithm. Further discussion and comprehensive experiments are included in a follow-up paper [<xref rid="pgen.1009141.ref019" ref-type="bibr">19</xref>].</p>
    </sec>
    <sec id="sec007">
      <title>Extension to the elastic net</title>
      <p>Our discussion so far focuses solely on the lasso penalty, which aims to achieve a rather sparse set of linear coefficients. In spite of good performance in many high-dimensional settings, it has limitations. For example, when there is a group of highly correlated variables, the lasso will often pick out one of them and ignore the others. This poses some hardness in interpretation. Also, under high-correlation structure like that, it has been empirically observed that when the predictors are highly correlated, the ridge can often outperform the lasso [<xref rid="pgen.1009141.ref006" ref-type="bibr">6</xref>].</p>
      <p>The elastic net, proposed in [<xref rid="pgen.1009141.ref020" ref-type="bibr">20</xref>], extends the lasso and tries to find a sweet spot between the lasso and the ridge penalty. It can capture the grouping effect of highly correlated variables and sometimes perform better than both methods especially when the number of variables is much larger than the number of samples. In particular, instead of imposing the <italic>ℓ</italic><sub>1</sub> penalty, the elastic net solves the following regularized regression problem.
<disp-formula id="pgen.1009141.e054"><alternatives><graphic xlink:href="pgen.1009141.e054.jpg" id="pgen.1009141.e054g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M54"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi>β</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mspace width="4pt"/><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>(</mml:mo><mml:mi>α</mml:mi><mml:mo>‖</mml:mo><mml:mi>β</mml:mi><mml:mo>‖</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>α</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>‖</mml:mo><mml:mi>β</mml:mi><mml:mo>‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(6)</label></disp-formula>
where the mixing parameter <italic>α</italic> ∈ [0, 1] determines the proportion of lasso and ridge in the penalty term.</p>
      <p>It is straightforward to adapt the BASIL procedure to the elastic net. It follows from the gradient motivation of the strong rules and KKT condition of convex optimization. We take the Gaussian family as an example. The others are similar. In the screening step, it is easy to derive that we can still rank <italic>among the currently inactive variables</italic> on their absolute inner product with the residual <inline-formula id="pgen.1009141.e055"><alternatives><graphic xlink:href="pgen.1009141.e055.jpg" id="pgen.1009141.e055g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M55"><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>⊤</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>X</mml:mi><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> to determine the next candidate set. In the checking step, to verify that all the left-out variables indeed have zero coefficients, we need to make sure that <inline-formula id="pgen.1009141.e056"><alternatives><graphic xlink:href="pgen.1009141.e056.jpg" id="pgen.1009141.e056g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M56"><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>⊤</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>X</mml:mi><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo>≤</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> holds for all such variables. It turns out that in our UK Biobank applications, the elastic-net results (after selection of <italic>α</italic> and λ on the validation set) do not differ significantly from the lasso results, which will be immediately seen in the next section.</p>
    </sec>
    <sec id="sec008">
      <title>UK Biobank analysis</title>
      <p>We describe a real-data application on the UK Biobank that in fact motivates our development of the BASIL algorithm.</p>
      <p>The UK Biobank [<xref rid="pgen.1009141.ref011" ref-type="bibr">11</xref>] is a very large, prospective population-based cohort study with individuals collected from multiple sites across the United Kingdom. It contains extensive genotypic and phenotypic detail such as genomewide genotyping, questionnaires and physical measures for a wide range of health-related outcomes for over 500,000 participants, who were aged 40-69 years when recruited in 2006-2010. In this study, we are interested in the relationship between an individual’s genotype and his/her phenotypic outcome. While GWAS focus on identifying SNPs that may be marginally associated with the outcome using univariate tests, we would like to find relevant SNPs in a multivariate prediction model using the lasso. A recent study [<xref rid="pgen.1009141.ref021" ref-type="bibr">21</xref>] fits the lasso on a subset of the variables after one-shot univariate <italic>p</italic>-value screening and suggests improvement in explaining the variation in the phenotypes. However, the left-out variants with relatively weak marginal association may still provide additional predictive power in a multiple regression environment. The BASIL algorithm enables us to fit the lasso model at full scale and gives further improvement in the explained variance over the alternative models considered.</p>
      <p>We focused on 337,199 White British unrelated individuals out of the full set of over 500,000 from the UK Biobank dataset [<xref rid="pgen.1009141.ref011" ref-type="bibr">11</xref>] that satisfy the same set of population stratification criteria as in [<xref rid="pgen.1009141.ref022" ref-type="bibr">22</xref>]. The dataset is partitioned randomly into training (60%), validation (20%) and test (20%) subsets (<xref ref-type="sec" rid="sec010">Methods</xref>). Each individual has up to 805,426 measured variants, and each variant is encoded by one of the four levels where 0 corresponds to homozygous major alleles, 1 to heterozygous alleles, 2 to homozygous minor alleles and NA to a missing genotype. In addition, we have available covariates such as age, sex, and fortypre-computed principal components of the SNP matrix.</p>
      <p>To evaluate the predictive performance for quantitative response, we use a common measure R-squared (<italic>R</italic><sup>2</sup>). Given a linear estimator <inline-formula id="pgen.1009141.e057"><alternatives><graphic xlink:href="pgen.1009141.e057.jpg" id="pgen.1009141.e057g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M57"><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> and data (<italic>y</italic>, <italic>X</italic>), it is defined as
<disp-formula id="pgen.1009141.e058"><alternatives><graphic xlink:href="pgen.1009141.e058.jpg" id="pgen.1009141.e058g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M58"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mi>y</mml:mi><mml:mo>-</mml:mo></mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
We evaluate this criteria for all the training, validation and test sets. For a dichotomous response, misclassification error could be used but it would depend on the calibration. Instead the receiver operating characteristic (ROC) curve provides more information and illustrates the tradeoff between true positive and false positive rates under different thresholds. The AUC computes the area under the ROC curve—a larger value indicates a generally better classifier. Therefore, we will evaluate AUCs on the training, validation and test sets for dichotomous responses.</p>
      <p>We compare the performance of the lasso with related methods to have a sense of the contribution of different components. Starting from the baseline, we fit a linear model that includes only age and sex (Model 1 in the tables below), and then one that includes additionally the top 10 principal components (Model 2). These are the adjustment covariates used in our main lasso fitting and we use these two models to highlight the contribution of the SNP information over and above that of age, sex and the top 10 PCs. In addition, the strongest univariate model is also evaluated (Model 3). This includes the 12 adjustment covariates together with the single SNP that is most correlated with the outcome after adjustment. Toward multivariate models, we first compare with a univariate method with some multivariate flavor (Models 4 and 5). We select a subset of the <italic>K</italic> most marginally significant variants (after adjusting for the covariates), construct a new variable by linearly combining these variants using their univariate coefficients, and fit an ordinary least squares (OLS) on the new variable together with the adjustment variables. It is similar to a one-step partial least squares [<xref rid="pgen.1009141.ref023" ref-type="bibr">23</xref>] with <italic>p</italic>-value based truncation. We take <italic>K</italic> = 10, 000 and 100, 000 in the experiments. We further compare with a hierarchical sequence of multivariate models where each is fit on a subset of the most significant SNPs. In particular, the <italic>ℓ</italic>-th model selects <italic>ℓ</italic> × 1000 SNPs with the smallest univariate <italic>p</italic>-values, and a multivariate linear or logistic regression is fit on those variants jointly. The sequence of models are evaluated on the validation set, and the one with the smallest validation error is chosen. We call this method Sequential LR or SeqLR (Model 6) for convenience in the rest of the paper. As a byproduct of the lasso, the relaxed lasso [<xref rid="pgen.1009141.ref024" ref-type="bibr">24</xref>] fits a debiased model by refitting an OLS on the variables selected by the lasso. This can potentially recover some of the bias introduced by lasso shrinkage. For the elastic-net, we fit separate solution paths with varying λ’s at <italic>α</italic> = 0.1, 0.5, 0.9, and evaluate their performance (<italic>R</italic><sup>2</sup> or AUC) on the validation set. The best pair of hyperparameters (<italic>α</italic>, λ) is selected and the corresponding test performance is reported.</p>
      <p>There are thousands of measured phenotypes in the dataset. For demonstration purpose, we analyze four phenotypes that are known to be highly or moderately heritable and polygenic. For these complex traits, univariate studies may not find SNPs with smaller effects, but the lasso model may include them and predict the phenotype better. We look at two quantitative traits: standing height and body mass index (BMI), which are defined as a non-NA median of up to 3 measurements [<xref rid="pgen.1009141.ref025" ref-type="bibr">25</xref>], and two qualitative traits: asthma and high cholesterol (HC) [<xref rid="pgen.1009141.ref022" ref-type="bibr">22</xref>].</p>
      <p>We first summarize the test performance of the methods above in <xref ref-type="fig" rid="pgen.1009141.g002">Fig 2</xref>. The lasso and elastic net show significant improvement in test <italic>R</italic><sup>2</sup> and AUC over the others. Details of the model for height are given in the next section and for the other phenotypes (BMI, asthma and high cholesterol) in Supporting Information. A comparison of the univariate <italic>p</italic>-values and the lasso coefficients for all these traits is shown in the form of Manhattan plots and coefficient plots in the Supporting Information.</p>
      <fig id="pgen.1009141.g002" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pgen.1009141.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>Comparison of the predictive performance of the different polygenic prediction methods evaluated on the test set.</title>
          <p><italic>R</italic><sup>2</sup> are evaluated for continuous phenotypes height and BMI, and AUC evaluated for binary phenotypes asthma and high cholesterol. The error bar uses 2 standard errors to show the statistical significance.</p>
        </caption>
        <graphic xlink:href="pgen.1009141.g002"/>
      </fig>
      <p>There are many other well-established methods for constructing the polygenic risk scores from large-scale cohorts. Among them, we compare with ridge regression, pruning and thresholding (P + T), clumping, and summary statistics-based Bayesian regression methods such as PRS-CS [<xref rid="pgen.1009141.ref026" ref-type="bibr">26</xref>] and SBayesR [<xref rid="pgen.1009141.ref027" ref-type="bibr">27</xref>]. Ridge regression, also known as BLUP in the quantitative genetics field, fits a multiple linear regression model with <italic>ℓ</italic><sub>2</sub>-penalty. It is a special case of the elastic-net with <italic>α</italic> = 0 in <xref ref-type="disp-formula" rid="pgen.1009141.e054">(6)</xref>. While it is simple and has been widely used in a variety of prediction tasks, the fact that the resulting model always includes all the variables can pose great computational challenge (for example, memory) in large-scale problems. In our experiments, the size of the data prevents us from doing exact, full-scale ridge regression. Instead, we approximate its performance by fitting the elastic-net with very small <italic>α</italic> = 10<sup>−3</sup>, which can be easily handled by our <bold>snpnet</bold> package. For P + T, we first identified LD independent set of variants using <monospace>--indep-pairwise 50 5.5</monospace> subcommand in PLINK2.0. We subsequently applied univariate genome-wide association analysis (<monospace>--glm firth-fallback</monospace>), focused on the LD independent variants, imposed the different p-value thresholds (1 × 10<sup>−3</sup>, 1 × 10<sup>−4</sup>, and 1 × 10<sup>−5</sup>), and extracted the univariate BETAs for the remaining variants to construct PRS [<xref rid="pgen.1009141.ref013" ref-type="bibr">13</xref>]. For clumping, we applied <monospace>--clump</monospace> subcommand to the GWAS summary statistics with a varying (1 × 10<sup>−3</sup>, 1 × 10<sup>−4</sup>, and 1 × 10<sup>−5</sup>) p-value threshold (<monospace>--clump-p1</monospace>), and extracted the univariate BETAs for the identified lead SNPs [<xref rid="pgen.1009141.ref013" ref-type="bibr">13</xref>]. For each of those two methods, we computed the PRS for each individual and fit an additional model consisting of covariates and the genotype PRS to report the predictive performance of the model consisting of both the genetic features and covariates. For PRS-CS, we first characterized the GWAS summary statistics using the combined set of training and validation set (<italic>n</italic> = 269, 927) with age, sex, and the top 10 PCs as covariates using PLINK v2.00a3LM (9 Apr 2020) [<xref rid="pgen.1009141.ref013" ref-type="bibr">13</xref>, <xref rid="pgen.1009141.ref028" ref-type="bibr">28</xref>]. Using the LD reference dataset precomputed for the European Ancestry using the 1000 genome samples (<ext-link ext-link-type="uri" xlink:href="https://github.com/getian107/PRScs">https://github.com/getian107/PRScs</ext-link>), we applied PRS-CS with the default option. We took the posterior effect size estimates and computed the polygenic risk scores using PLINK2’s <monospace>--score</monospace> subcommand [<xref rid="pgen.1009141.ref013" ref-type="bibr">13</xref>]. For SBayesR, we computed the sparse LD matrix using the combined set of training and validation set individuals (<italic>n</italic> = 269, 927) using the <monospace>-- make-sparse-ldm</monospace> subcommand implemented in GCTB version 2.0.1 [<xref rid="pgen.1009141.ref029" ref-type="bibr">29</xref>]. Using the GWAS summary statistics computed on the set of individuals and following the GCTB’s recommendations, we applied SBayesR with the following options: <monospace>gctb --sbayes R --ldm [the LD matrix]</monospace>
<monospace>--pi 0.95,0.02,0.02,0.01</monospace>
<monospace>--gamma 0.0,0.01,0.1,1</monospace>
<monospace>--chain-length 10000</monospace>
<monospace>--burn-in 4000</monospace>
<monospace>--gwas-summary [the GWAS summary statistics]</monospace>. We report the model performance on the test set.</p>
      <p>We train PRS-CS and SBayesR on the combined training and validation dataset with recommended settings. To make it a fair comparison, for the other methods with tuning parameter(s), we follow a refitting mechanism:</p>
      <list list-type="order">
        <list-item>
          <p>fit models on the training set under different parameters;</p>
        </list-item>
        <list-item>
          <p>choose the optimal parameter based on the metric (<italic>R</italic><sup>2</sup>/AUC) on the validation set;</p>
        </list-item>
        <list-item>
          <p>refit the model with the chosen parameter on a combined training and validation set.</p>
        </list-item>
      </list>
      <p>This is often recommended for methods with tuning parameters to make the most of the validation set, and for the lasso/elastic-net, we demonstrate those steps with a code example in the vignette of our <bold>snpnet</bold> package. The predictive performance is compared in <xref ref-type="fig" rid="pgen.1009141.g003">Fig 3</xref>. SBayesR seems fairly competitive on binary phenotypes, achieving higher test AUC on asthma and high cholesterol. For continuous phenotypes, the lasso and the elastic-net seem to have some advantage, with the lasso doing the best for height and the elastic-net doing the best for BMI. Aside from the predictive performance, SBayesR would always include all the variables in the model, while the lasso/elastic-net class often ends up using only a small fraction of the variables. While prediction is key to the relevance of PRS methods, the sparsity of the solution achieved by the lasso/elastic-net class is also very important for scientific understanding of the genetics behind.</p>
      <fig id="pgen.1009141.g003" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pgen.1009141.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>Comparison of the test set predictive performance of the different polygenic risk score (PRS) methods with refitting on the training and the validation set.</title>
          <p><italic>R</italic><sup>2</sup> are evaluated for continuous phenotypes height and BMI, and AUC evaluated for binary phenotypes asthma and high cholesterol. The error bar uses 2 standard errors to show the statistical significance.</p>
        </caption>
        <graphic xlink:href="pgen.1009141.g003"/>
      </fig>
      <p>Height is a polygenic and heritable trait that has been studied for a long time. It has been used as a model for other quantitative traits, since it is easy to measure reliably. From twin and sibling studies, the narrow sense heritability is estimated to be 70-80% [<xref rid="pgen.1009141.ref030" ref-type="bibr">30</xref>–<xref rid="pgen.1009141.ref032" ref-type="bibr">32</xref>]. Recent estimates controlling for shared environmental factors present in twin studies calculate heritability at 0.69 [<xref rid="pgen.1009141.ref033" ref-type="bibr">33</xref>, <xref rid="pgen.1009141.ref034" ref-type="bibr">34</xref>]. A linear based model with common SNPs explains 45% of the variance [<xref rid="pgen.1009141.ref035" ref-type="bibr">35</xref>] and a model including imputed variants explains 56% of the variance, almost matching the estimated heritability [<xref rid="pgen.1009141.ref036" ref-type="bibr">36</xref>]. So far, GWAS studies have discovered 697 associated variants that explain one fifth of the heritability [<xref rid="pgen.1009141.ref037" ref-type="bibr">37</xref>, <xref rid="pgen.1009141.ref038" ref-type="bibr">38</xref>]. Recently, a large sample study was able to identify more variants with low frequencies that are associated with height [<xref rid="pgen.1009141.ref039" ref-type="bibr">39</xref>]. Using the lasso with the larger UK Biobank dataset allows both a better estimate of the proportion of variance that can be explained by genomic predictors and simultaneous selection of SNPs that may be associated. The results are summarized in <xref rid="pgen.1009141.t001" ref-type="table">Table 1</xref>, where for each model class (row), the reported numbers are based on the fitted model on the training set that achieves the best validation performance (if any hyper-parameter). The associated <italic>R</italic><sup>2</sup> curves for the lasso and the relaxed lasso are shown in <xref ref-type="fig" rid="pgen.1009141.g004">Fig 4</xref>. The residuals of the optimal lasso prediction are plotted in <xref ref-type="fig" rid="pgen.1009141.g005">Fig 5</xref>.</p>
      <fig id="pgen.1009141.g004" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pgen.1009141.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title><italic>R</italic><sup>2</sup> plot for height.</title>
          <p>The primary horizontal axis on the bottom represents the index of lambda values, 1 ≤ <italic>ℓ</italic> ≤ <italic>L</italic>, which correspond to the sequence of the regularization parameters, λ<sub>1</sub> &gt; λ<sub>2</sub> &gt; ⋯ &gt; λ<sub><italic>L</italic></sub>. The top axis shows the number of active variables in the model. ReLasso: relaxed lasso.</p>
        </caption>
        <graphic xlink:href="pgen.1009141.g004"/>
      </fig>
      <fig id="pgen.1009141.g005" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pgen.1009141.g005</object-id>
        <label>Fig 5</label>
        <caption>
          <p>Left: actual height versus predicted height on 5000 random samples from the test set. A regression line with its 95% confidence band is also added on top of the dots. The correlation between actual height and predicted height is 0.9416. Right: histogram of the lasso residuals for height. Standard deviation of the residual is 5.05 (cm).</p>
        </caption>
        <graphic xlink:href="pgen.1009141.g005"/>
      </fig>
      <table-wrap id="pgen.1009141.t001" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pgen.1009141.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title><italic>R</italic><sup>2</sup> values for height (without refitting).</title>
          <p>For sequential LR, lasso and relaxed lasso, the chosen model is based on maximum <italic>R</italic><sup>2</sup> on the validation set. Model (3) to (9) each includes Model (2) plus their own specification as stated in the Form column. The elastic-net picks <italic>α</italic> = 0.9 based on the validation performance.</p>
        </caption>
        <alternatives>
          <graphic id="pgen.1009141.t001g" xlink:href="pgen.1009141.t001"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" style="border-right:thick" rowspan="1" colspan="1">Model</th>
                <th align="center" rowspan="1" colspan="1">Form</th>
                <th align="center" rowspan="1" colspan="1">
                  <inline-formula id="pgen.1009141.e059">
                    <alternatives>
                      <graphic id="pgen.1009141.e059g" xlink:href="pgen.1009141.e059"/>
                      <mml:math id="M59">
                        <mml:msubsup>
                          <mml:mi>R</mml:mi>
                          <mml:mtext>train</mml:mtext>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                      </mml:math>
                    </alternatives>
                  </inline-formula>
                </th>
                <th align="center" rowspan="1" colspan="1">
                  <inline-formula id="pgen.1009141.e060">
                    <alternatives>
                      <graphic id="pgen.1009141.e060g" xlink:href="pgen.1009141.e060"/>
                      <mml:math id="M60">
                        <mml:msubsup>
                          <mml:mi>R</mml:mi>
                          <mml:mtext>val</mml:mtext>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                      </mml:math>
                    </alternatives>
                  </inline-formula>
                </th>
                <th align="center" rowspan="1" colspan="1">
                  <inline-formula id="pgen.1009141.e061">
                    <alternatives>
                      <graphic id="pgen.1009141.e061g" xlink:href="pgen.1009141.e061"/>
                      <mml:math id="M61">
                        <mml:msubsup>
                          <mml:mi>R</mml:mi>
                          <mml:mtext>test</mml:mtext>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                      </mml:math>
                    </alternatives>
                  </inline-formula>
                </th>
                <th align="center" rowspan="1" colspan="1">Size</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" style="border-right:thick" rowspan="1" colspan="1">(1)</td>
                <td align="center" rowspan="1" colspan="1">Age + Sex</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5300</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5260</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5288</td>
                <td align="center" rowspan="1" colspan="1">2</td>
              </tr>
              <tr>
                <td align="center" style="border-right:thick" rowspan="1" colspan="1">(2)</td>
                <td align="center" rowspan="1" colspan="1">Age + Sex + 10 PCs</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5344</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5304</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5336</td>
                <td align="center" rowspan="1" colspan="1">12</td>
              </tr>
              <tr>
                <td align="center" style="border-right:thick" rowspan="1" colspan="1">(3)</td>
                <td align="center" rowspan="1" colspan="1">Strong Single SNP</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5364</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5323</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5355</td>
                <td align="center" rowspan="1" colspan="1">13</td>
              </tr>
              <tr>
                <td align="center" style="border-right:thick" rowspan="1" colspan="1">(4)</td>
                <td align="center" rowspan="1" colspan="1">10K Combined</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5482</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5408</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5444</td>
                <td align="center" rowspan="1" colspan="1">10,012</td>
              </tr>
              <tr>
                <td align="center" style="border-right:thick" rowspan="1" colspan="1">(5)</td>
                <td align="center" rowspan="1" colspan="1">100K Combined</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5833</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5515</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5551</td>
                <td align="center" rowspan="1" colspan="1">100,012</td>
              </tr>
              <tr>
                <td align="center" style="border-right:thick" rowspan="1" colspan="1">(6)</td>
                <td align="center" rowspan="1" colspan="1">Sequential LR</td>
                <td align="char" char="." rowspan="1" colspan="1">0.7416</td>
                <td align="char" char="." rowspan="1" colspan="1">0.6596</td>
                <td align="char" char="." rowspan="1" colspan="1">0.6601</td>
                <td align="center" rowspan="1" colspan="1">17,012</td>
              </tr>
              <tr>
                <td align="center" style="border-right:thick" rowspan="1" colspan="1">(7)</td>
                <td align="center" rowspan="1" colspan="1">Lasso</td>
                <td align="char" char="." rowspan="1" colspan="1">0.8304</td>
                <td align="char" char="." rowspan="1" colspan="1">0.6992</td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>0.6999</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">47,673</td>
              </tr>
              <tr>
                <td align="center" style="border-right:thick" rowspan="1" colspan="1">(8)</td>
                <td align="center" rowspan="1" colspan="1">Relaxed Lasso</td>
                <td align="char" char="." rowspan="1" colspan="1">0.7789</td>
                <td align="char" char="." rowspan="1" colspan="1">0.6718</td>
                <td align="char" char="." rowspan="1" colspan="1">0.6727</td>
                <td align="center" rowspan="1" colspan="1">13,395</td>
              </tr>
              <tr>
                <td align="center" style="border-right:thick" rowspan="1" colspan="1">(9)</td>
                <td align="center" rowspan="1" colspan="1">Elastic Net</td>
                <td align="char" char="." rowspan="1" colspan="1">0.8282</td>
                <td align="char" char="." rowspan="1" colspan="1">0.6991</td>
                <td align="char" char="." rowspan="1" colspan="1">0.6998</td>
                <td align="center" rowspan="1" colspan="1">48,268</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>A large number (47,673) of SNPs need to be selected in order to achieve the optimal <inline-formula id="pgen.1009141.e062"><alternatives><graphic xlink:href="pgen.1009141.e062.jpg" id="pgen.1009141.e062g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M62"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mtext>test</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>6999</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> for the lasso and similarly for the elastic-net, though it is only a small fraction considering the entire variant set. Comparatively, the relaxed lasso sacrifices some predictive performance by including a much smaller subset of variables (13,395). Past the optimal point, the additional variance introduced by refitting such large models may be larger than the reduction in bias. The large models confirm the extreme polygenicity of standing height.</p>
      <p>In comparison to the other models, the lasso performs significantly better in terms of <inline-formula id="pgen.1009141.e063"><alternatives><graphic xlink:href="pgen.1009141.e063.jpg" id="pgen.1009141.e063g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M63"><mml:msubsup><mml:mi>R</mml:mi><mml:mtext>test</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> than all univariate methods, and outperforms multivariate methods based on univariate <italic>p</italic>-value ordering. That demonstrates the value of simultaneous variable selection and estimation from a multivariate perspective, and enables us to predict height to within 10 cm about 95% of the time based only on SNP information (together with age and sex). We also notice that the sequential linear regression approach does a good job, whose performance gets close to that of the relaxed lasso. It is straightforward and easy to implement using existing softwares such as <bold>PLINK</bold> [<xref rid="pgen.1009141.ref013" ref-type="bibr">13</xref>].</p>
      <p>Recently [<xref rid="pgen.1009141.ref021" ref-type="bibr">21</xref>] apply a lasso based method to predict height and other phenotypes on the UK Biobank. Instead of fitting on all QC-satisfied SNPs (as stated in the experiment details paragraph of <xref ref-type="sec" rid="sec010">Materials and methods</xref>), they pre-screen 50K or 100K most significant SNPs in terms of the univariate <italic>p</italic>-value and apply lasso on that set only. In addition, although both datasets come from the same UK Biobank, the subset of individuals they used is larger than ours. While we restrict the analysis to the unrelated individuals who have self-reported white British ancestry, they look at Europeans including British, Irish and Any Other White. For a fair comparison, we follow their procedure (pre-screening 100K SNPs) but run on our subset of the dataset. The results are shown in <xref rid="pgen.1009141.t002" ref-type="table">Table 2</xref>. We see that the improvement of the full lasso over the prescreened lasso is almost 0.5% in test <italic>R</italic><sup>2</sup>, and 1% relative to the proportion of residual variance explained after covariate adjustment.</p>
      <table-wrap id="pgen.1009141.t002" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pgen.1009141.t002</object-id>
        <label>Table 2</label>
        <caption>
          <title>Comparison of prediction results on height with the model trained following the same procedure as ours except for an additional prescreening step as done in [<xref rid="pgen.1009141.ref021" ref-type="bibr">21</xref>].</title>
          <p>In addition to <italic>R</italic><sup>2</sup>, proportion of residual variance explained (denoted by <inline-formula id="pgen.1009141.e064"><alternatives><graphic id="pgen.1009141.e064g" xlink:href="pgen.1009141.e064"/><mml:math id="M64"><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mtext>test</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>) and correlation between the fitted values and actual values are computed. We also compute an adjusted correlation between the residual after regressing age and sex out from the prediction and the residual after regressing age and sex out from the true response, both on the test set.</p>
        </caption>
        <alternatives>
          <graphic id="pgen.1009141.t002g" xlink:href="pgen.1009141.t002"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" rowspan="1" colspan="1">Method</th>
                <th align="center" rowspan="1" colspan="1">
                  <inline-formula id="pgen.1009141.e065">
                    <alternatives>
                      <graphic id="pgen.1009141.e065g" xlink:href="pgen.1009141.e065"/>
                      <mml:math id="M65">
                        <mml:msubsup>
                          <mml:mi>R</mml:mi>
                          <mml:mtext>val</mml:mtext>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                      </mml:math>
                    </alternatives>
                  </inline-formula>
                </th>
                <th align="center" rowspan="1" colspan="1">
                  <inline-formula id="pgen.1009141.e066">
                    <alternatives>
                      <graphic id="pgen.1009141.e066g" xlink:href="pgen.1009141.e066"/>
                      <mml:math id="M66">
                        <mml:msubsup>
                          <mml:mi>R</mml:mi>
                          <mml:mtext>test</mml:mtext>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                      </mml:math>
                    </alternatives>
                  </inline-formula>
                </th>
                <th align="center" rowspan="1" colspan="1">
                  <inline-formula id="pgen.1009141.e067">
                    <alternatives>
                      <graphic id="pgen.1009141.e067g" xlink:href="pgen.1009141.e067"/>
                      <mml:math id="M67">
                        <mml:msubsup>
                          <mml:mi>h</mml:mi>
                          <mml:mtext>test</mml:mtext>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                      </mml:math>
                    </alternatives>
                  </inline-formula>
                </th>
                <th align="center" rowspan="1" colspan="1">Cor<sub>test</sub></th>
                <th align="center" rowspan="1" colspan="1">Cor<sub>test</sub>−{age, sex}</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" rowspan="1" colspan="1">Lasso</td>
                <td align="char" char="." rowspan="1" colspan="1">69.92%</td>
                <td align="char" char="." rowspan="1" colspan="1">69.99%</td>
                <td align="char" char="." rowspan="1" colspan="1">35.66%</td>
                <td align="char" char="." rowspan="1" colspan="1">0.8366</td>
                <td align="char" char="." rowspan="1" colspan="1">0.4079</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">Prescreened lasso</td>
                <td align="char" char="." rowspan="1" colspan="1">69.40%</td>
                <td align="char" char="." rowspan="1" colspan="1">69.56%</td>
                <td align="char" char="." rowspan="1" colspan="1">34.73%</td>
                <td align="char" char="." rowspan="1" colspan="1">0.8340</td>
                <td align="char" char="." rowspan="1" colspan="1">0.4025</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>In addition, we compare the full lasso coefficients and the univariate <italic>p</italic>-values from GWAS in <xref ref-type="fig" rid="pgen.1009141.g006">Fig 6</xref>. The vertical grey dotted line indicates the top 100K cutoff in terms of <italic>p</italic>-value. We see although a general decreasing trend appears in the magnitude of the lasso coefficients with respect to increasing <italic>p</italic>-values (decreasing − log<sub>10</sub>(<italic>p</italic>)), there are a number of spikes even in the large <italic>p</italic>-value region which is considered marginally insignificant. This shows that variants beyond the strongest univariate ones contribute to prediction.</p>
      <fig id="pgen.1009141.g006" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pgen.1009141.g006</object-id>
        <label>Fig 6</label>
        <caption>
          <title>Comparison of the lasso coefficients and univariate <italic>p</italic>-values for height.</title>
          <p>The index on the horizontal axis represents the SNPs sorted by their univariate <italic>p</italic>-values. The red curve associated with the left vertical axis shows the −log<sub>10</sub> of the univariate <italic>p</italic>-values. The blue bars associated with the right vertical axis show the corresponding lasso coefficients for each (sorted) SNP. The horizontal dotted lines in gray identifies lasso coefficients of ±0.05. The vertical one represents the 100K cutoff used in [<xref rid="pgen.1009141.ref021" ref-type="bibr">21</xref>].</p>
        </caption>
        <graphic xlink:href="pgen.1009141.g006"/>
      </fig>
      <p>We conduct the lasso and elastic-net with the refitting mechanism and compare them with the other well-established PRS methods. From <xref rid="pgen.1009141.t003" ref-type="table">Table 3</xref>, we see that the lasso and the elastic-net do the best job and also uses only a small fraction of the variables.</p>
      <table-wrap id="pgen.1009141.t003" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pgen.1009141.t003</object-id>
        <label>Table 3</label>
        <caption>
          <title><italic>R</italic><sup>2</sup> values for height by different PRS methods with refitting.</title>
        </caption>
        <alternatives>
          <graphic id="pgen.1009141.t003g" xlink:href="pgen.1009141.t003"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" style="border-right:thick" rowspan="1" colspan="1">Model</th>
                <th align="center" rowspan="1" colspan="1">Form</th>
                <th align="center" rowspan="1" colspan="1">
                  <inline-formula id="pgen.1009141.e068">
                    <alternatives>
                      <graphic id="pgen.1009141.e068g" xlink:href="pgen.1009141.e068"/>
                      <mml:math id="M68">
                        <mml:msubsup>
                          <mml:mi>R</mml:mi>
                          <mml:mtext>test</mml:mtext>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                      </mml:math>
                    </alternatives>
                  </inline-formula>
                </th>
                <th align="center" rowspan="1" colspan="1">Size</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" style="border-right:thick" rowspan="1" colspan="1">(1)</td>
                <td align="center" rowspan="1" colspan="1">Lasso</td>
                <td align="char" char="." rowspan="1" colspan="1">0.7127</td>
                <td align="center" rowspan="1" colspan="1">45,653</td>
              </tr>
              <tr>
                <td align="center" style="border-right:thick" rowspan="1" colspan="1">(2)</td>
                <td align="center" rowspan="1" colspan="1">Elastic Net</td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>0.7128</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">45,549</td>
              </tr>
              <tr>
                <td align="center" style="border-right:thick" rowspan="1" colspan="1">(3)</td>
                <td align="center" rowspan="1" colspan="1">Ridge</td>
                <td align="char" char="." rowspan="1" colspan="1">0.6986</td>
                <td align="center" rowspan="1" colspan="1">175,012</td>
              </tr>
              <tr>
                <td align="center" style="border-right:thick" rowspan="1" colspan="1">(4)</td>
                <td align="center" rowspan="1" colspan="1">PRS-CS</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5615</td>
                <td align="center" rowspan="1" colspan="1">148,064</td>
              </tr>
              <tr>
                <td align="center" style="border-right:thick" rowspan="1" colspan="1">(5)</td>
                <td align="center" rowspan="1" colspan="1">SBayesR</td>
                <td align="char" char="." rowspan="1" colspan="1">0.7019</td>
                <td align="center" rowspan="1" colspan="1">667,057</td>
              </tr>
              <tr>
                <td align="center" style="border-right:thick" rowspan="1" colspan="1">(6)</td>
                <td align="center" rowspan="1" colspan="1">P + T</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5912</td>
                <td align="center" rowspan="1" colspan="1">15,544</td>
              </tr>
              <tr>
                <td align="center" style="border-right:thick" rowspan="1" colspan="1">(7)</td>
                <td align="center" rowspan="1" colspan="1">Clumping</td>
                <td align="char" char="." rowspan="1" colspan="1">0.6181</td>
                <td align="center" rowspan="1" colspan="1">17,433</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec009">
    <title>Discussion</title>
    <p>In this paper, we propose a novel batch screening iterative lasso (BASIL) algorithm to fit the full lasso solution path for very large and high-dimensional datasets. It can be used, among the others, for Gaussian linear model, logistic regression and Cox regression, and can be easily extended to fit the elastic-net with mixed <italic>ℓ</italic><sub>1</sub>/<italic>ℓ</italic><sub>2</sub> penalty. It enjoys the advantages of high efficiency, flexibility and easy implementation. For SNP data as in our applications, we develop an <monospace>R</monospace> package <bold>snpnet</bold> that incorporates SNP-specific optimizations and are able to process datasets of wide interest from the UK Biobank.</p>
    <p>In our algorithm, the choice of <italic>M</italic> is important for the practical performance. It trades off between the number of iterations and the computation per iteration. With a small <italic>M</italic> or small update of the strong set, it is very likely that we are unable to proceed fast along the λ sequence in each iteration. Although the design of the BASIL algorithm guarantees that for any <italic>M</italic>, Δ<italic>M</italic> &gt; 0, we are able to obtain the full solution path after sufficient iterations, many iterations will be needed if <italic>M</italic> is chosen too small, and the disk I/O cost will be dominant. In contrast, a large <italic>M</italic> will incur more memory burden and more expensive lasso computation, but with the hope to find more valid lasso solutions in one iteration, save the number of iterations and the disk I/O. It is hard to identify the optimal <italic>M</italic> a priori. It depends on the computing architecture, the size of the problem, the nature of the phenotype, etc. For this reason, we tend to leave it as a subjective parameter to the user’s choice. However in the meantime, we do plan to provide a more systematic option to determine <italic>M</italic>, which leverages the strong rules again. Recall that in the simple setting with no intercept and no covariates, the initial strong set is constructed by <inline-formula id="pgen.1009141.e069"><alternatives><graphic xlink:href="pgen.1009141.e069.jpg" id="pgen.1009141.e069g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M69"><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>⊤</mml:mi></mml:msubsup><mml:mrow><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mo>≤</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>-</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mo form="prefix" movablelimits="true">max</mml:mo></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. Since the strong rules rarely make mistakes and are fairly effective in discarding inactive variables, we can guide the choice of batch size <italic>M</italic> by the number of λ values we want to cover in the first iteration. For example, one may want the strong set to be large enough to solve for the first 10 λ’s in the first iteration. We can then let <inline-formula id="pgen.1009141.e070"><alternatives><graphic xlink:href="pgen.1009141.e070.jpg" id="pgen.1009141.e070g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M70"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>j</mml:mi><mml:mo>≤</mml:mo><mml:mi>p</mml:mi><mml:mo>:</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>⊤</mml:mi></mml:msubsup><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mn>10</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="normal">λ</mml:mi><mml:mo form="prefix" movablelimits="true">max</mml:mo></mml:msub><mml:mo>}</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Despite being adaptive to the data in some sense, this approach is by no means computationally optimal. It is more based on heuristics that the iteration should make reasonable progress along the path.</p>
    <p>Our numerical studies demonstrate that the iterative procedure effectively reduces a big-<italic>n</italic>-big-<italic>p</italic> lasso problem into one that is manageable by in-memory computation. In each iteration, we are able to use parallel computing when applying screening rules to filter out a large number of variables. After screening, we are left with only a small subset of data on which we are able to conduct intensive computation like cyclical coordinate descent all in memory. For the subproblem, we can use existing fast procedures for small or moderate-size lasso problems. Thus, our method allows easy reuse of previous software with lightweight development effort.</p>
    <p>When a large number of variables is needed in the optimal predictive model, it may still require either large memory or long computation time to solve the smaller subproblem. In that case, we may consider more scalable and parallelizable methods like proximal gradient descent [<xref rid="pgen.1009141.ref040" ref-type="bibr">40</xref>] or dual averaging [<xref rid="pgen.1009141.ref041" ref-type="bibr">41</xref>, <xref rid="pgen.1009141.ref042" ref-type="bibr">42</xref>]. One may think why don’t we directly use these methods for the original full problem? First, the ultra high dimension makes the evaluation of gradients, even on mini-batch very expensive. Second, it can take a lot more steps for such first-order methods to converge to a good objective value. Moreover, the speed of convergence depends on the choice of other parameters such as step size and additional constants in dual averaging. For those reasons, we still prefer the tuning-free and fast coordinate descent methods when the subproblem is manageable.</p>
    <p>The lasso has nice variable selection and prediction properties if the linear model assumption together with some additional assumptions such as the restricted eigenvalue condition [<xref rid="pgen.1009141.ref043" ref-type="bibr">43</xref>] or the irrepresentable condition [<xref rid="pgen.1009141.ref044" ref-type="bibr">44</xref>] holds. In practice, such assumptions do not always hold and are often hard to verify. In our UK Biobank application, we don’t attempt to verify the exact conditions, and the selected model can be subject to false positives. However, we demonstrate relevance of the selection via empirical consistency with the GWAS results. We have seen superior prediction performance by the lasso as a regularized regression method compared to other methods. More importantly, by leveraging the sparsity property of the lasso, we are able to manage the ultrahigh-dimensional problem and obtain a computationally efficient solution.</p>
    <p>When comparing with other methods in the UK Biobank experiments, due to the large number of test samples (60,000+), we are confident that the lasso and the elastic-net methods are able to do significantly better than all other methods on height and BMI, and are as competitive as SBayesR on asthma and high cholesterol. In fact, the standard error of <italic>R</italic><sup>2</sup> can be easily derived by the delta method, and the standard error of the AUC can be estimated and upper bounded by 1/(4 min(<italic>m</italic>, <italic>n</italic>)) [<xref rid="pgen.1009141.ref045" ref-type="bibr">45</xref>, <xref rid="pgen.1009141.ref046" ref-type="bibr">46</xref>], where <italic>m</italic>, <italic>n</italic> represents the number of positive and negative samples. For height and BMI, it turns out that the standard errors are roughly 0.001, or 0.1%. For asthma and high cholesterol, considering the case rate around 12%, the standard errors can be upper bounded by 0.005, or 0.5%. The estimated standard errors are reflected in the error bars in Figs <xref ref-type="fig" rid="pgen.1009141.g002">2</xref> and <xref ref-type="fig" rid="pgen.1009141.g003">3</xref>. Therefore, speaking of the predictive performance, on height and BMI, the lasso/elastic-net class performs significantly better than the other methods, while on asthma and high cholesterol, the lasso/elastic-net and the SBayesR are both fairly competitive—their difference is not statistically significant. Moreover, the lasso/elastic-net method builds parsimonious models using only a small fraction of the variants. It is more interpretable and can have meaningful implications on the genetics behind.</p>
  </sec>
  <sec sec-type="materials|methods" id="sec010">
    <title>Materials and methods</title>
    <sec id="sec011">
      <title>Variants in the BASIL framework</title>
      <p>Some other very useful components can be easily incorporated into the BASIL framework. We will discuss debiasing using the relaxed lasso and the inclusion of adjustment covariates.</p>
      <p>The lasso is known to shrink coefficients to exclude noise variables, but sometimes such shrinkage can degrade the predictive performance due to its effect on actual signal variables. [<xref rid="pgen.1009141.ref024" ref-type="bibr">24</xref>] introduces the relaxed lasso to correct for the potential over-shrinkage of the original lasso estimator. They propose a refitting step on the active set of the lasso solution with less regularization, while a common way of using it is to fit a standard OLS on the active set. The active set coefficients are then set to
<disp-formula id="pgen.1009141.e071"><alternatives><graphic xlink:href="pgen.1009141.e071.jpg" id="pgen.1009141.e071g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M71"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="script">A</mml:mi><mml:mo>,</mml:mo><mml:mtext>Relax</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi mathvariant="script">A</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="script">A</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi mathvariant="script">A</mml:mi></mml:msub><mml:msub><mml:mi>β</mml:mi><mml:mi mathvariant="script">A</mml:mi></mml:msub><mml:mo>‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
whereas the coefficients for the inactive set remain at 0. This refitting step can revert some of the shrinkage bias introduced by the vanilla lasso. It doesn’t always reduce prediction error due to the accompanied increase in variance when there are many variables in the model or when the signals are weak. That being said, we can still insert a relaxed lasso step with little effort in our iterative procedure: once a valid lasso solution is found for a new λ, we may refit with OLS. As we iterate, we can monitor validation error for the lasso and the relaxed lasso. The relaxed lasso will generally end up choosing a smaller set of variables than the lasso solution in the optimal model.</p>
      <p>In some applications such as GWAS, there may be confounding variables <inline-formula id="pgen.1009141.e072"><alternatives><graphic xlink:href="pgen.1009141.e072.jpg" id="pgen.1009141.e072g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M72"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> that we want to adjust for in the model. Population stratification, defined as the existence of a systematic ancestry difference in the sample data, is one of the common factors in GWAS that can lead to spurious discoveries. This can be controlled for by including some leading principal components of the SNP matrix as variables in the regression [<xref rid="pgen.1009141.ref047" ref-type="bibr">47</xref>]. In the presence of such variables, we instead solve
<disp-formula id="pgen.1009141.e073"><alternatives><graphic xlink:href="pgen.1009141.e073.jpg" id="pgen.1009141.e073g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M73"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi>α</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>q</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mspace width="4pt"/><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>Z</mml:mi><mml:mi>α</mml:mi><mml:mo>-</mml:mo><mml:mi>X</mml:mi><mml:mi>β</mml:mi><mml:mo>‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:msub><mml:mrow><mml:mo>‖</mml:mo><mml:mi>β</mml:mi><mml:mo>‖</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(7)</label></disp-formula>
This variation can be easily handled with small changes in the algorithm. Instead of initializing the residual with the response <italic>y</italic>, we set <italic>r</italic><sup>(0)</sup> equal to the residual from the regression of <italic>y</italic> on the covariates. In the fitting step, in addition to the variables in the strong set, we include the covariates but leave their coefficients unpenalized as in <xref ref-type="disp-formula" rid="pgen.1009141.e073">(7)</xref>. Notice that if we want to find relaxed lasso fit with the presence of adjustment covariates, we need to include those covariates in the OLS as well, i.e.,
<disp-formula id="pgen.1009141.e074"><alternatives><graphic xlink:href="pgen.1009141.e074.jpg" id="pgen.1009141.e074g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M74"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mtext>Relax</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="script">A</mml:mi><mml:mo>,</mml:mo><mml:mtext>Relax</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi>α</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>q</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi mathvariant="script">A</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="script">A</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>Z</mml:mi><mml:mi>α</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi mathvariant="script">A</mml:mi></mml:msub><mml:msub><mml:mi>β</mml:mi><mml:mi mathvariant="script">A</mml:mi></mml:msub><mml:mo>‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(8)</label></disp-formula></p>
    </sec>
    <sec id="sec012">
      <title>UK Biobank experiment details</title>
      <p>We focused on 337,199 White British unrelated individuals out of the full set of over 500,000 from the UK Biobank dataset [<xref rid="pgen.1009141.ref011" ref-type="bibr">11</xref>] that satisfy the same set of population stratification criteria as in [<xref rid="pgen.1009141.ref022" ref-type="bibr">22</xref>]: (1) self-reported White British ancestry, (2) used to compute principal components, (3) not marked as outliers for heterozygosity and missing rates, (4) do not show putative sex chromosome aneuploidy, and (5) have at most 10 putative third-degree relatives. These criteria are meant to reduce the effect of confoundedness and unreliable observations.</p>
      <p>The number of samples is large in the UK Biobank dataset, so we can afford to set aside an independent validation set without resorting to the costly cross-validation to find an optimal regularization parameter. We also leave out a subset of observations as test set to evaluate the final model. In particular, we randomly partition the original dataset so that 60% is used for training, 20% for validation and 20% for test. The lasso solution path is fit on the training set, whereas the desired regularization is selected on the validation set, and the resulting model is evaluated on the test set.</p>
      <p>We are going to further discuss some details in our application that one might also encounter in practice. They include adjustment for confounders, missing value imputation and variable standardization in the algorithm.</p>
      <p>In genetic studies, spurious associations are often found due to confounding factors. Among the others, one major source is the so-called population stratification [<xref rid="pgen.1009141.ref048" ref-type="bibr">48</xref>]. To adjust for that effect, it is common is to introduce the top principal components and include them in the regression model. Therefore in the lasso method, we are going to solve <xref ref-type="disp-formula" rid="pgen.1009141.e073">(7)</xref> where in addition to the SNP matrix <italic>X</italic>, we let <italic>Z</italic> include covariates such as age, sex and the top 10 PCs of the SNP matrix.</p>
      <p>Missing values are present in the dataset. As quality control normally done in genetics, we first discard observations whose phenotypic value of interest is not available. We further exclude variants whose missing rate is greater than 10% or the minor allele frequency (MAF) is less than 0.1%, which results in around 685,000 SNPs for height. In particulr, 685,362 for height, 685,371 for BMI, 685,357 for asthma and 685,357 for HC. The number varies because the criteria are evaluated on the subset of individuals whose phenotypic value is observed (after excluding the missing ones), which can be different across different phenotypes. For those remaining variants, mean imputation is conducted to fill the missing SNP values; that is, the missing values in every SNP are imputed with the mean observed level of that SNP in the population under study.</p>
      <p>When it comes to the lasso fitting, there are some subtleties that can affect its variable selection and prediction performance. One of them is variable standardization. It is often a step done without much thought to deal with heterogeneity in variables so that they are treated fairly in the objective. However in our studies, standardization may create some undesired effect. To see this, notice that all the SNPs can only take values in 0, 1, 2 and NA—they are already on the same scale by nature. As we know, standardization would use the current standard deviation of each predictor as the divisor to equalize the variance across all predictors in the lasso fitting that follows. In this case, standardization would unintentionally inflate the magnitude of rare variants and give them an advantage in the selection process since their coefficients effectively receive less penalty after standardization. In <xref ref-type="fig" rid="pgen.1009141.g007">Fig 7</xref>, we can see the distribution of standard deviation across all variants in our dataset. Hence, to avoid potential spurious findings, we choose not to standardize the variants in the experiments.</p>
      <fig id="pgen.1009141.g007" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pgen.1009141.g007</object-id>
        <label>Fig 7</label>
        <caption>
          <title>Histogram of the standard deviations of the SNPs.</title>
          <p>They are computed <italic>after</italic> mean imputation of the missing values because they would be the exact standardization factors to be used if the lasso were applied with variable standardization on the mean-imputed SNP matrix.</p>
        </caption>
        <graphic xlink:href="pgen.1009141.g007"/>
      </fig>
    </sec>
    <sec id="sec013">
      <title>Computational optimization in software implementation</title>
      <p>Among the iterative steps in BASIL, screening and checking are where we need to deal with the full dataset. To deal with the memory bound, we can use memory-mapped I/O. In <monospace>R</monospace>, <bold>bigmemory</bold> [<xref rid="pgen.1009141.ref049" ref-type="bibr">49</xref>] provides a convenient implementation for that purpose. That being said, we do not want to rely on that for intensive computation modules such as cyclic coordinate descent, because frequent visits to the on-disk data would still be slow. Instead, since the subset of strong variables would be small, we can afford to bring them to memory and do fast lasso fitting there. We only use the full memory-mapped dataset in KKT checking and screening. Moreover since checking in the current iteration can be done together with the screening in the next iteration, effectively only one expensive pass over the full dataset is needed every iteration.</p>
      <p>In addition, we use a set of techniques to speed up the computation. First, the KKT check can be easily parallelized by splitting on the features when multi-core machines are available. The speedup of this part is immediate and (slightly less than) proportional to the number of cores available. Second, specific to the application, we exploit the fact that there are only 4 levels for each SNP value and design a faster inner product routine to replace normal float number multiplication in the KKT check step. In fact, given any SNP vector <italic>x</italic> ∈ {0, 1, 2, <italic>μ</italic>}<sup><italic>n</italic></sup> where <italic>μ</italic> is the imputed value for the missing ones, we can write the dot product with a vector <inline-formula id="pgen.1009141.e075"><alternatives><graphic xlink:href="pgen.1009141.e075.jpg" id="pgen.1009141.e075g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M75"><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> as
<disp-formula id="pgen.1009141.e076"><alternatives><graphic xlink:href="pgen.1009141.e076.jpg" id="pgen.1009141.e076g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M76"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mi>⊤</mml:mi></mml:msup><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>·</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>·</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:munder><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>μ</mml:mi><mml:mo>·</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(9)</label></disp-formula>
We see that the terms corresponding to 0 SNP value can be ignored because they don’t contribute to the final result. This will significantly reduce the number of arithmetic operations needed to compute the inner product with rare variants. Further, we only need to set up 3 registers, each for one SNP value accumulating the corresponding terms in <italic>r</italic>. A series of multiplications is then converted to summations. In our UK Biobank studies, although the SNP matrix is not sparse enough to exploit sparse matrix representation, it still has around 70% 0’s. We conduct a small experiment to compare the time needed to compute <italic>X</italic><sup>⊤</sup><italic>R</italic>, where <inline-formula id="pgen.1009141.e077"><alternatives><graphic xlink:href="pgen.1009141.e077.jpg" id="pgen.1009141.e077g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M77"><mml:mrow><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>×</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. The proportions for the levels in <italic>X</italic> are about 70%, 10%, 10%, 10%, similar to the distribution of SNP levels in our study, and <italic>R</italic> resembles the residual matrix when checking the KKT condition. The number of residual vectors is <italic>k</italic> = 20. The mean time over 100 repetitions is shown in <xref rid="pgen.1009141.t004" ref-type="table">Table 4</xref>.</p>
      <table-wrap id="pgen.1009141.t004" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pgen.1009141.t004</object-id>
        <label>Table 4</label>
        <caption>
          <title>Timing performance (milliseconds) on multiplication of SNP matrix and residual matrix.</title>
          <p>The methods are all implemented in <monospace>C++</monospace> and run on a Macbook with 2.9 GHz Intel Core i7 and 8 GB 1600 MHz DDR3.</p>
        </caption>
        <alternatives>
          <graphic id="pgen.1009141.t004g" xlink:href="pgen.1009141.t004"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" rowspan="1" colspan="1">Multiplication Method</th>
                <th align="center" rowspan="1" colspan="1"><italic>n</italic> = 200, <italic>p</italic> = 800</th>
                <th align="center" rowspan="1" colspan="1"><italic>n</italic> = 2000, <italic>p</italic> = 8000</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" rowspan="1" colspan="1">Standard</td>
                <td align="char" char="." rowspan="1" colspan="1">3.20</td>
                <td align="char" char="." rowspan="1" colspan="1">306.01</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">SNP-Optimized</td>
                <td align="char" char="." rowspan="1" colspan="1">1.32</td>
                <td align="char" char="." rowspan="1" colspan="1">130.21</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>We implement the procedure with all the optimizations above in an <monospace>R</monospace> package called <bold>snpnet</bold>, which is currently available at <ext-link ext-link-type="uri" xlink:href="https://github.com/junyangq/snpnet">https://github.com/junyangq/snpnet</ext-link> and will be submitted to the CRAN repository of <monospace>R</monospace> packages. While most of the numerical experiments throughout the paper are based on an earlier version of the package (available in the V1.0 branch of the repository) assuming <monospace>bed</monospace> file format provided by <bold>PLINK</bold> 1.9, we highly recommend one to use the current version that works with the <monospace>pgen</monospace> file format provided by <bold>PLINK</bold> 2.0 [<xref rid="pgen.1009141.ref013" ref-type="bibr">13</xref>, <xref rid="pgen.1009141.ref028" ref-type="bibr">28</xref>]. It takes advantage of <bold>PLINK</bold> 2.0’s new <monospace>R</monospace> interface as well as its efficient <monospace>--variant-score</monospace> module for matrix multiplication. The module exploits combined techniques of multithreading, a good linear algebra library, and an alternate code path for very-low-MAF SNPs (similar to the one proposed in <xref ref-type="disp-formula" rid="pgen.1009141.e076">(9)</xref>) to make the computation even faster. In order to achieve better efficiency in each lasso fitting, we suggest using <bold>snpnet</bold> together with <bold>glmnetPlus</bold>, a warm-started version of <bold>glmnet</bold>, which is currently available at <ext-link ext-link-type="uri" xlink:href="https://github.com/junyangq/glmnetPlus">https://github.com/junyangq/glmnetPlus</ext-link>. It allows one to provide a good initialization of the coefficients to fit part of the solution path instead of always starting from the all-zero solution by <bold>glmnet</bold>.</p>
    </sec>
    <sec id="sec014">
      <title>Related methods and packages</title>
      <p>There are a number of existing screening rules for solving big lasso problems. [<xref rid="pgen.1009141.ref050" ref-type="bibr">50</xref>] use a screened set to scale down the logistic lasso problem and check the KKT condition to validate the solution. Their focus, however, is on selecting a lasso model of particular size and only the initial screened set is expanded if the KKT condition is violated. In contrast, we are interested in finding the whole solution path (before overfitting). We adopt a sequential approach and keep updating the screened set at each iteration. This allows us to potentially keep the screened set small as we move along the solution path. Other rules include the SAFE rule [<xref rid="pgen.1009141.ref051" ref-type="bibr">51</xref>], Sure Independence Screening [<xref rid="pgen.1009141.ref052" ref-type="bibr">52</xref>], and the DPP and EDPP rules [<xref rid="pgen.1009141.ref053" ref-type="bibr">53</xref>].</p>
      <p>We expand the discussion on these screening rules a bit. [<xref rid="pgen.1009141.ref052" ref-type="bibr">52</xref>] exploits marginal information of correlation to conduct screening but the focus there is not optimization algorithm. Most of the screening rules mentioned above (except for EDPP) use inner product with the current residual vector to measure the importance of each predictor at the next λ—those under a threshold can be ignored. The key difference across those rules is the threshold defined and whether the resulting discard is safe. If it is safe, one can guarantee that only one iteration is needed for each λ value, compared with others that would need more rounds if an active variable was falsely discarded. Though the strong rules rarely make this mistake, safe screening is still a nice feature to have in single-λ solutions. However, under the batch mode we consider due to the desire of reducing the number of full passes over the dataset, the advantage of safe threshold may not be as much. In fact, one way we might be able to leverage the safe rules in the batch mode is to first find out the set of candidate predictors for the several λ values up to λ<sub><italic>k</italic></sub> we wish to solve in the next iteration based on the current inner products and the rules’ safe threshold, and then solve the lasso for these parameters. Since these rules can often be conservative, we would then have strong incentive to solve for, say, one further λ value λ<sub><italic>k</italic>+1</sub> because if the current screening turns out to be a valid one as well, we will find one more lasso solution and move one step forward along the λ sequence we want to solve for. This can potentially save one iteration of the procedure and thus one expensive pass over the dataset. The only cost there is computing the lasso solution for one more λ<sub><italic>k</italic>+1</sub> and computing inner products with one more residual vector at λ<sub><italic>k</italic>+1</sub> (to check the KKT condition). The latter can be done in the same pass as we compute inner products at λ<sub><italic>k</italic></sub> for preparing the screening in the next iteration, and so no additional pass is needed. Thus under the batch mode, the property of safe screening may not be as important due to the incentive of aggressive model fitting. Nevertheless it would be interesting to see in the future EDPP-type batch screening. It uses inner products with a modification of the residual vector. Our algorithm still focuses of inner products with the vanilla residual vector.</p>
      <p>To address the large-scale lasso problems, several packages have been developed such as <bold>biglasso</bold> [<xref rid="pgen.1009141.ref054" ref-type="bibr">54</xref>], <bold>bigstatsr</bold> [<xref rid="pgen.1009141.ref055" ref-type="bibr">55</xref>], <bold>oem</bold> [<xref rid="pgen.1009141.ref056" ref-type="bibr">56</xref>] and the lasso routine from <bold>PLINK</bold> 1.9 [<xref rid="pgen.1009141.ref013" ref-type="bibr">13</xref>, <xref rid="pgen.1009141.ref014" ref-type="bibr">14</xref>].</p>
      <p>Among them, <bold>oem</bold> specializes in tall data (big <italic>n</italic>) and can be slow when <italic>p</italic> &gt; <italic>n</italic>. In many real data applications including ours, the data are both large-sample and high-dimensional. However, we might still be able to use <bold>oem</bold> for the small lasso subroutine since a large number of variables have already been excluded. The other packages, <bold>biglasso</bold>, <bold>bigstatsr</bold>, <bold>PLINK</bold> 1.9, all provide efficient implementations of the pathwise coordinate descent with warm start. <bold>PLINK</bold> 1.9 is specifically developed for genetic datasets and is widely used in GWAS and research in population genetics. In <bold>bigstatsr</bold>, the <monospace>big_spLinReg</monospace> function adapts from the <monospace>biglasso</monospace> function in <bold>biglasso</bold> and incorporates a Cross-Model Selection and Averaging (CMSA) procedure, which is a variant of cross-validation that saves computation by directly averaging the results from different folds instead of retraining the model at the chosen optimal parameter. They both use memory-mapping to process larger-than-RAM, on-disk datasets as if they were in memory, and based on that implement coordinate descent with strong rules and warm start.</p>
      <p>The main difference between BASIL and the algorithm these packages use is that BASIL tries to solve a series of models every full scan of the dataset (at checking and screening) and thus effectively reduce the number of passes over the dataset. This difference may not be significant in small or moderate-sized problems, but can be critical in big data applications especially when the dataset cannot be fully loaded into the memory. A full scan of a larger-than-RAM dataset can incur a lot of swap-in/out between the memory and the disk, and thus a lot of disk I/O operations, which is known to be orders of magnitude slower than in-memory operations. Thus reducing the number of full scans can greatly improve the overall performance of the algorithm.</p>
      <p>Aside from potential efficiency consideration, all of those packages aforementioned have to re-implement a variety of features existent in many small-data solutions but for big-data context. Nevertheless, currently they don’t provide as much functionality as needed in our real-data application. First, in the current implementations, <bold>PLINK</bold> 1.9 only supports the Gaussian family, <bold>biglasso</bold> and <bold>bigstatsr</bold> only supports the Gaussian and binomial families, whereas <bold>snpnet</bold> can easily extend to other regression families and already built in Gaussian, binomial and Cox families. Also, <bold>biglasso</bold>, <bold>bigstatsr</bold> and <bold>PLINK</bold> 1.9 all standardize the predictors beforehand, but in many applications such as our UK Biobank studies, it is more reasonable to leave the predictors unstandardized. In addition, it can take some effort to convert the data to the desired format by these packages. This would be a headache if the raw data is in some special format and one cannot afford to first convert the full dataset into an intermediate format for which a tool is provided to convert to the desired one by <bold>biglasso</bold> or <bold>bigstatsr</bold>. This can happen, for example, if the raw data is highly compressed in a special format. For the BED binary format we work with in our application, <monospace>readRAW_big.matrix</monospace> function from <bold>BGData</bold> can convert a raw file to a <monospace>big.matrix</monospace> object desired by <bold>biglasso</bold>, and <monospace>snp_readBed</monospace> function from <bold>bigsnpr</bold> [<xref rid="pgen.1009141.ref055" ref-type="bibr">55</xref>] allows one to convert it to <monospace>FBM</monospace> object desired by <bold>bigstatsr</bold>. However, <bold>bigsnpr</bold> doesn’t take input data that has any missing values, which can prevalent in an SNP matrix (as in our application). Although <bold>PLINK</bold> 1.9 works directly with the BED binary file, its lasso solver currently only supports the Gaussian family, and it doesn’t return the full solution path. Instead it returns the solution at the smallest λ value computed and needs a good heritability estimate as input from the user, which may not be immediately available.</p>
      <p>We summarize the main advantages of the BASIL algorithm:</p>
      <list list-type="bullet">
        <list-item>
          <p><bold>Input data flexibility</bold>. Our algorithm allows one to deal directly with any data type as long as the screening and checking steps are implemented, which is often very lightweight development work like matrix multiplication. This can be important in large-scale applications especially when the data is stored in a compressed format or a distributed way since then we would not need to unpack the full data and can conduct KKT check and screening on its original format. Instead only a small screened subset of the data needs to be converted to the desired format by the lasso solver in the fitting step.</p>
        </list-item>
        <list-item>
          <p><bold>Model flexibility</bold>. We can easily transfer the modeling flexibility provided by existing packages to the big data context, such as the options of standardization, sample weights, lower/upper coefficient limits and other families in generalized linear models provided by existing packages such as <bold>glmnet</bold>. This can be useful, for example, when we may not want to standardize predictors already in the same unit to avoid unintentionally different penalization of the predictors due to difference in their variance.</p>
        </list-item>
        <list-item>
          <p><bold>Effortless development</bold>. The BASIL algorithm allows one to maximally reuse the existing lasso solutions for small or moderate-sized problems. The main extra work would be an implementation of batch screening and KKT check with respect to a particular data type. For example, in the <bold>snpnet</bold> package, we are able to quickly extend the in-memory <bold>glmnet</bold> solution to large-scale, ultrahigh-dimentional SNP data. Moreover, the existing convenient data interface provided by the <bold>BEDMatrix</bold> package further facilitates our implementation.</p>
        </list-item>
        <list-item>
          <p><bold>Computational efficiency</bold>. Our design reduces the number of visits to the original data that sits on the disk, which is crucial to the overall efficiency as disk read can be orders of magnitude slower than reading from the RAM. The key to achieving this is to bring batches of promising variables into the main memory, hoping to find the lasso solutions for more than one λ value each iteration and check the KKT condition for those λ values in one pass of the entire dataset.</p>
        </list-item>
      </list>
      <p>Lastly, we are going to provide some timing comparison with existing packages. As mentioned in previous sections, those packages provide different functionalities and have different restrictions on the dataset. For example, most of them (<bold>biglasso</bold>, <bold>bigstatsr</bold>) assume that there are no missing values, or the missing ones have already been imputed. In <bold>bigsnpr</bold>, for example, we shouldn’t have SNPs with 0 MAF either. Some packages always standardize the variants before fitting the lasso. To provide a common playground, we create a synthetic dataset with no missing values, and follow a standardized lasso procedure in the fitting stage, simply to test the computation. The dataset has 50,000 samples and 100,000 variables, and each takes value in the SNP range, i.e., in 0, 1, or 2. We fit the first 50 lasso solutions along a prefix λ sequence that contains 100 initial λ values (like early stopping for most phenotypes). The total time spent is displayed in <xref rid="pgen.1009141.t005" ref-type="table">Table 5</xref>. For <bold>bigstatsr</bold>, we include two versions since it does cross-validation by default. In one version, we make it comply with our single train/val/test split, while in the other version, we use its default 10-fold cross-validation version—Cross-Model Selection and Averaging (CMSA). Notice that the final solution of iCMSA is different from the exact lasso solution on the full data because the returned coefficient vector is a linear combination of the coefficient vectors from the 10 folds rather than from a retrained model on the full data. We uses 128GB memory and 16 cores for the computation.</p>
      <table-wrap id="pgen.1009141.t005" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pgen.1009141.t005</object-id>
        <label>Table 5</label>
        <caption>
          <title>Timing comparison on a synthetic dataset of size <italic>n</italic> = 50, 000 and <italic>p</italic> = 100, 000.</title>
          <p>The time for bigstatsr and biglasso has two components: one for the conversion to the desired data type and the other for the actual computation. The experiments are all run with 16 cores and 64 GB memory.</p>
        </caption>
        <alternatives>
          <graphic id="pgen.1009141.t005g" xlink:href="pgen.1009141.t005"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" rowspan="1" colspan="1"><monospace>R</monospace> Package</th>
                <th align="center" rowspan="1" colspan="1">Elapsed Time (minutes)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" rowspan="1" colspan="1"><bold>bigstatsr</bold> [<xref rid="pgen.1009141.ref055" ref-type="bibr">55</xref>]</td>
                <td align="center" rowspan="1" colspan="1">2.93 + 56.80</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1"><bold>bigstatsr</bold> + CMSA [<xref rid="pgen.1009141.ref055" ref-type="bibr">55</xref>]</td>
                <td align="center" rowspan="1" colspan="1">2.93 + 101.75</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1"><bold>biglasso</bold> [<xref rid="pgen.1009141.ref054" ref-type="bibr">54</xref>]</td>
                <td align="center" rowspan="1" colspan="1">4.55 + 54.27</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1"><bold>PLINK</bold> [<xref rid="pgen.1009141.ref013" ref-type="bibr">13</xref>, <xref rid="pgen.1009141.ref014" ref-type="bibr">14</xref>]</td>
                <td align="center" rowspan="1" colspan="1">53.52</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <bold>snpnet</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>44.79</bold>
                </td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>From the table, we see that <bold>snpnet</bold> is at about 20% faster than other packages concerned. The numbers before the “+” sign are the time spent on converting the raw data to the required data format by those packages. The second numbers are time spent on actual computation.</p>
      <p>It is important to note though that the performance relies not only on the algorithm, but also heavily on the implementations. The other packages in comparison all have their major computation done with <monospace>C++</monospace> or <monospace>Fortran</monospace>. Ours, for the purpose of meta algorithm where users can easily integrate with any lasso solver in <monospace>R</monospace>, still has a significant portion (the iterations) in <monospace>R</monospace> and multiple rounds of cross-language communication. That can degrade the timing performance to some degree. If there is further pursuit of speed performance, there is still space for improvement by more designated implementation.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material" id="sec015">
    <title>Supporting information</title>
    <supplementary-material content-type="local-data" id="pgen.1009141.s001">
      <label>S1 Appendix</label>
      <caption>
        <title>Results for body mass index (BMI).</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s001.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s002">
      <label>S2 Appendix</label>
      <caption>
        <title>Results for asthma.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s002.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s003">
      <label>S3 Appendix</label>
      <caption>
        <title>Results for high cholesterol.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s003.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s004">
      <label>S4 Appendix</label>
      <caption>
        <title>Manhattan plots.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s004.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s005">
      <label>S1 Table</label>
      <caption>
        <title><italic>R</italic><sup>2</sup> values for BMI (without refitting).</title>
        <p><italic>R</italic><sup>2</sup> values for BMI (without refitting). For lasso and relaxed lasso, the chosen model is based on maximum <italic>R</italic><sup>2</sup> on the validation set. Model (3) to (9) each includes Model (2) plus their own specification as stated in the Form column. The elastic-net picks <italic>α</italic> = 0.1 based on the validation performance.</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s005.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s006">
      <label>S2 Table</label>
      <caption>
        <title><italic>R</italic><sup>2</sup> values for body mass index by different PRS methods with refitting.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s006.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s007">
      <label>S3 Table</label>
      <caption>
        <title>AUC values for asthma (without refitting).</title>
        <p>For lasso and relaxed lasso, the chosen model is based on maximum AUC on the validation set. Model (3) to (9) each includes Model (2) plus their own specification as stated in the Form column. The elastic-net picks <italic>α</italic> = 0.1 based on the validation performance.</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s007.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s008">
      <label>S4 Table</label>
      <caption>
        <title>AUC values for asthma by different PRS methods with refitting.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s008.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s009">
      <label>S5 Table</label>
      <caption>
        <title>AUC values for high cholesterol (without refitting).</title>
        <p>For lasso and relaxed lasso, the chosen model is based on maximum AUC on the validation set. Model (3) to (9) each includes Model (2) plus their own specification as stated in the Form column. The elastic-net picks <italic>α</italic> = 0.9 based on the validation performance.</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s009.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s010">
      <label>S6 Table</label>
      <caption>
        <title>AUC values for high cholesterol by different PRS methods with refitting.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s010.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s011">
      <label>S1 Fig</label>
      <caption>
        <title><italic>R</italic><sup>2</sup> plot for body mass index.</title>
        <p>The primary horizontal axis on the bottom represents the index of lambda values, 1 ≤ <italic>ℓ</italic> ≤ <italic>L</italic>, which correspond to the sequence of the regularization parameters, λ<sub>1</sub> &gt; λ<sub>2</sub> &gt; ⋯ &gt; λ<sub><italic>L</italic></sub>. The top axis shows the number of active variables in the model. ReLasso: relaxed lasso.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s011.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s012">
      <label>S2 Fig</label>
      <caption>
        <title>Actual body mass index (BMI) versus predicted BMI on 5000 random samples from the test set.</title>
        <p>A regression line with its 95% confidence band is also added on top of the dots. The correlation between actual BMI and predicted BMI is 0.3256.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s012.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s013">
      <label>S3 Fig</label>
      <caption>
        <title>Residuals of lasso prediction for body mass index.</title>
        <p>Standard deviation of the residual is 4.51 kg/m<sup>2</sup>.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s013.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s014">
      <label>S4 Fig</label>
      <caption>
        <title>AUC plot for asthma.</title>
        <p>The primary horizontal axis on the bottom represents the index of lambda values, 1 ≤ <italic>ℓ</italic> ≤ <italic>L</italic>, which correspond to the sequence of the regularization parameters, λ<sub>1</sub> &gt; λ<sub>2</sub> &gt; ⋯ &gt; λ<sub><italic>L</italic></sub>. The top axis shows the number of active variables in the model. ReLasso: relaxed lasso.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s014.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s015">
      <label>S5 Fig</label>
      <caption>
        <title>Box plot of the percentile of the linear prediction score among cases versus controls for asthma.</title>
        <p>This is based on the optimal lasso model.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s015.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s016">
      <label>S6 Fig</label>
      <caption>
        <title>Stratified prevalence across different percentile bins based on the predicted scores for asthma.</title>
        <p>This is based on the optimal lasso model.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s016.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s017">
      <label>S7 Fig</label>
      <caption>
        <title>AUC plot for high cholesterol.</title>
        <p>The primary horizontal axis on the bottom represents the index of lambda values, 1 ≤ <italic>ℓ</italic> ≤ <italic>L</italic>, which correspond to the sequence of the regularization parameters, λ<sub>1</sub> &gt; λ<sub>2</sub> &gt; ⋯ &gt; λ<sub><italic>L</italic></sub>. The top axis shows the number of active variables in the model. ReLasso: relaxed lasso.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s017.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s018">
      <label>S8 Fig</label>
      <caption>
        <title>Box plot of the percentile of the linear prediction score among cases versus controls for high cholesterol.</title>
        <p>This is based on the optimal lasso model.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s018.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s019">
      <label>S9 Fig</label>
      <caption>
        <title>Stratified prevalence across different percentile bins based on the predicted scores for high cholesterol.</title>
        <p>This is based on the optimal lasso model.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s019.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s020">
      <label>S10 Fig</label>
      <caption>
        <title>ROC curve for asthma.</title>
        <p>This is based on the optimal lasso model.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s020.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s021">
      <label>S11 Fig</label>
      <caption>
        <title>ROC curve for high cholesterol.</title>
        <p>This is based on the optimal lasso model.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s021.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s022">
      <label>S12 Fig</label>
      <caption>
        <title>Manhattan plot of the univariate <italic>p</italic>-values for height.</title>
        <p>This is based on the optimal lasso model. The vertical axis shows −log<sub>10</sub>(<italic>p</italic>) for each SNP. The red horizontal line represents a reference level of <italic>p</italic> = 5 × 10<sup>−8</sup>.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s022.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s023">
      <label>S13 Fig</label>
      <caption>
        <title>Manhattan plot of the univariate <italic>p</italic>-values for body mass index.</title>
        <p>This is based on the optimal lasso model. The vertical axis shows −log<sub>10</sub>(<italic>p</italic>) for each SNP. The red horizontal line represents a reference level of <italic>p</italic> = 5 × 10<sup>−8</sup>.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s023.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s024">
      <label>S14 Fig</label>
      <caption>
        <title>The lasso coefficients for height.</title>
        <p>This is based on the optimal lasso model. The vertical axis shows the magnitude of the coefficients from <bold>snpnet</bold>. The SNPs with relatively large lasso coefficients are highlighted in green.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s024.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s025">
      <label>S15 Fig</label>
      <caption>
        <title>The lasso coefficients for body mass index.</title>
        <p>This is based on the optimal lasso model. The vertical axis shows the magnitude of the coefficients from <bold>snpnet</bold>. The SNPs with relatively large lasso coefficients are highlighted in green.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s025.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s026">
      <label>S16 Fig</label>
      <caption>
        <title>Manhattan plot of the univariate <italic>p</italic>-values for asthma.</title>
        <p>This is based on the optimal lasso model. The vertical axis shows −log<sub>10</sub>(<italic>p</italic>) for each SNP. The red horizontal line represents a reference level of <italic>p</italic> = 5 × 10<sup>−8</sup>.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s026.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s027">
      <label>S17 Fig</label>
      <caption>
        <title>Manhattan plot of the univariate <italic>p</italic>-values for high cholesterol.</title>
        <p>This is based on the optimal lasso model. The vertical axis shows −log<sub>10</sub>(<italic>p</italic>) for each SNP. The red horizontal line represents a reference level of <italic>p</italic> = 5 × 10<sup>−8</sup>.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s027.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s028">
      <label>S18 Fig</label>
      <caption>
        <title>The lasso coefficients for asthma.</title>
        <p>This is based on the optimal lasso model. The vertical axis shows the magnitude of the coefficients from <bold>snpnet</bold>. The SNPs with relatively large lasso coefficients are highlighted in green.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s028.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pgen.1009141.s029">
      <label>S19 Fig</label>
      <caption>
        <title>The lasso coefficients for high cholesterol.</title>
        <p>This is based on the optimal lasso model. The vertical axis shows the magnitude of the coefficients from <bold>snpnet</bold>. The SNPs with relatively large lasso coefficients are highlighted in green.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pgen.1009141.s029.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <p>We thank Balasubramanian Narasimhan for helpful discussion on the package development, Kenneth Tay and the members of the Rivas lab for insightful feedback. We thank the SBayesR and GCTB authors for their invaluable feedback and help with our SBayesR experiments.</p>
    <p>This research has been conducted using the UK Biobank Resource under application number 24983. We thank all the participants in the study. The primary and processed data used to generate the analyses presented here are available in the UK Biobank access management system (<ext-link ext-link-type="uri" xlink:href="https://amsportal.ukbiobank.ac.uk/">https://amsportal.ukbiobank.ac.uk/</ext-link>) for application 24983, “Generating effective therapeutic hypotheses from genomic and hospital linkage data” (<ext-link ext-link-type="uri" xlink:href="http://www.ukbiobank.ac.uk/wp-content/uploads/2017/06/24983-Dr-Manuel-Rivas.pdf">http://www.ukbiobank.ac.uk/wp-content/uploads/2017/06/24983-Dr-Manuel-Rivas.pdf</ext-link>), and the results are displayed in the Global Biobank Engine (<ext-link ext-link-type="uri" xlink:href="https://biobankengine.stanford.edu">https://biobankengine.stanford.edu</ext-link>).</p>
    <p>Some of the computing for this project was performed on the Sherlock cluster. We would like to thank Stanford University and the Stanford Research Computing Center for providing computational resources and support that contributed to these research results.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="pgen.1009141.ref001">
      <label>1</label>
      <mixed-citation publication-type="book"><name><surname>Friedman</surname><given-names>J</given-names></name>, <name><surname>Hastie</surname><given-names>T</given-names></name>, <name><surname>Tibshirani</surname><given-names>R</given-names></name>. <chapter-title>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</chapter-title>, <edition designator="2">2nd Edition</edition><source>Springer series in statistics</source>. <publisher-name>Springer-Verlag</publisher-name>; <year>2009</year>.</mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref002">
      <label>2</label>
      <mixed-citation publication-type="book"><name><surname>Efron</surname><given-names>B</given-names></name>, <name><surname>Hastie</surname><given-names>T</given-names></name>. <source>Computer Age Statistical Inference: Algorithms, Evidence, and Data Science</source>. vol. <volume>5</volume><publisher-name>Cambridge University Press</publisher-name>; <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Dean</surname><given-names>J</given-names></name>, <name><surname>Ghemawat</surname><given-names>S</given-names></name>. <article-title>MapReduce: Simplified Data Processing on Large Clusters</article-title>. <source>Commun ACM</source>. <year>2008</year>;<volume>51</volume>(<issue>1</issue>):<fpage>107</fpage>–<lpage>113</lpage>. <pub-id pub-id-type="doi">10.1145/1327452.1327492</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref004">
      <label>4</label>
      <mixed-citation publication-type="other">Zaharia M, Chowdhury M, Franklin MJ, Shenker S, Stoica I. <bold>Spark</bold>: Cluster Computing with Working Sets. In: Proceedings of the 2Nd USENIX Conference on Hot Topics in Cloud Computing. HotCloud’10. Berkeley, CA, USA: USENIX Association; 2010. p. 10–10. Available from: <ext-link ext-link-type="uri" xlink:href="http://dl.acm.org/citation.cfm?id=1863103.1863113">http://dl.acm.org/citation.cfm?id=1863103.1863113</ext-link>.</mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref005">
      <label>5</label>
      <mixed-citation publication-type="other">Abadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, et al. <bold>TensorFlow</bold>: A System for Large-scale Machine Learning. In: Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation. OSDI’16. Berkeley, CA, USA: USENIX Association; 2016. p. 265–283. Available from: <ext-link ext-link-type="uri" xlink:href="http://dl.acm.org/citation.cfm?id=3026877.3026899">http://dl.acm.org/citation.cfm?id=3026877.3026899</ext-link>.</mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Tibshirani</surname><given-names>R</given-names></name>. <article-title>Regression Shrinkage and Selection via the Lasso</article-title>. <source>Journal of the Royal Statistical Society Series B (Methodological)</source>. <year>1996</year>;<volume>58</volume>(<issue>1</issue>):<fpage>267</fpage>–<lpage>288</lpage>. <pub-id pub-id-type="doi">10.1111/j.2517-6161.1996.tb02080.x</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref007">
      <label>7</label>
      <mixed-citation publication-type="other"><monospace>R</monospace> Core Team. <monospace>R</monospace>: A Language and Environment for Statistical Computing; 2017. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.R-project.org/">https://www.R-project.org/</ext-link>.</mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Friedman</surname><given-names>J</given-names></name>, <name><surname>Hastie</surname><given-names>T</given-names></name>, <name><surname>Tibshirani</surname><given-names>R</given-names></name>. <article-title>Regularization Paths for Generalized Linear Models via Coordinate Descent</article-title><source>Journal of Statistical Software</source>; <year>2010</year><pub-id pub-id-type="doi">10.18637/jss.v033.i01</pub-id><?supplied-pmid 20808728?><pub-id pub-id-type="pmid">20808728</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Breheny</surname><given-names>P</given-names></name>, <name><surname>Huang</surname><given-names>J</given-names></name>. <article-title>Coordinate Descent Algorithms for Nonconvex Penalized Regression, with Applications to Biological Feature Selection</article-title>. <source>The Annals of Applied Statistics</source>. <year>2011</year>;<volume>5</volume>(<issue>1</issue>):<fpage>232</fpage>–<lpage>253</lpage>. <pub-id pub-id-type="doi">10.1214/10-AOAS388</pub-id><pub-id pub-id-type="pmid">22081779</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref010">
      <label>10</label>
      <mixed-citation publication-type="other">Hastie T. Statistical Learning with Big Data; 2015. Presentation at Data Science at Stanford Seminar. Available from: <ext-link ext-link-type="uri" xlink:href="https://web.stanford.edu/~hastie/TALKS/SLBD_new.pdf">https://web.stanford.edu/~hastie/TALKS/SLBD_new.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Bycroft</surname><given-names>C</given-names></name>, <name><surname>Freeman</surname><given-names>C</given-names></name>, <name><surname>Petkova</surname><given-names>D</given-names></name>, <name><surname>Band</surname><given-names>G</given-names></name>, <name><surname>Elliott</surname><given-names>LT</given-names></name>, <name><surname>Sharp</surname><given-names>K</given-names></name>, <etal>et al</etal><article-title>The UK Biobank Resource with Deep Phenotyping and Genomic Data</article-title>. <source>Nature</source>. <year>2018</year>;<volume>562</volume>(<issue>7726</issue>):<fpage>203</fpage>–<lpage>209</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-018-0579-z</pub-id><?supplied-pmid 30305743?><pub-id pub-id-type="pmid">30305743</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Visscher</surname><given-names>PM</given-names></name>, <name><surname>Wray</surname><given-names>NR</given-names></name>, <name><surname>Zhang</surname><given-names>Q</given-names></name>, <name><surname>Sklar</surname><given-names>P</given-names></name>, <name><surname>McCarthy</surname><given-names>MI</given-names></name>, <name><surname>Brown</surname><given-names>MA</given-names></name>, <etal>et al</etal><article-title>10 Years of GWAS Discovery: Biology, Function, and Translation</article-title>. <source>The American Journal of Human Genetics</source>. <year>2017</year>;<volume>101</volume>(<issue>1</issue>):<fpage>5</fpage>–<lpage>22</lpage>. <pub-id pub-id-type="doi">10.1016/j.ajhg.2017.06.005</pub-id><?supplied-pmid 28686856?><pub-id pub-id-type="pmid">28686856</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Chang</surname><given-names>CC</given-names></name>, <name><surname>Chow</surname><given-names>CC</given-names></name>, <name><surname>Tellier</surname><given-names>LC</given-names></name>, <name><surname>Vattikuti</surname><given-names>S</given-names></name>, <name><surname>Purcell</surname><given-names>SM</given-names></name>, <name><surname>Lee</surname><given-names>JJ</given-names></name>. <article-title>Second-generation PLINK: rising to the challenge of larger and richer datasets</article-title>. <source>GigaScience</source>. <year>2015</year>;<volume>4</volume>(<issue>1</issue>). <pub-id pub-id-type="doi">10.1186/s13742-015-0047-8</pub-id><?supplied-pmid 25722852?><pub-id pub-id-type="pmid">25722852</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref014">
      <label>14</label>
      <mixed-citation publication-type="other">Purcell S, Chang C. PLINK 1.9; 2015. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.cog-genomics.org/plink/1.9/">www.cog-genomics.org/plink/1.9/</ext-link>.</mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Tibshirani</surname><given-names>R</given-names></name>, <name><surname>Bien</surname><given-names>J</given-names></name>, <name><surname>Friedman</surname><given-names>J</given-names></name>, <name><surname>Hastie</surname><given-names>T</given-names></name>, <name><surname>Simon</surname><given-names>N</given-names></name>, <name><surname>Taylor</surname><given-names>J</given-names></name>, <etal>et al</etal><article-title>Strong Rules for Discarding Predictors in Lasso-Type Problems</article-title>. <source>Journal of the Royal Statistical Society Series B (Statistical Methodology)</source>. <year>2012</year>;<volume>74</volume>(<issue>2</issue>):<fpage>245</fpage>–<lpage>266</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-9868.2011.01004.x</pub-id><?supplied-pmid 25506256?><pub-id pub-id-type="pmid">25506256</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref016">
      <label>16</label>
      <mixed-citation publication-type="book"><name><surname>Boyd</surname><given-names>S</given-names></name>, <name><surname>Vandenberghe</surname><given-names>L</given-names></name>. <source>Convex Optimization</source>. <publisher-name>Cambridge university press</publisher-name>; <year>2004</year>.</mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Friedman</surname><given-names>J</given-names></name>, <name><surname>Hastie</surname><given-names>T</given-names></name>, <name><surname>Tibshirani</surname><given-names>R</given-names></name>. <article-title>Regularization Paths for Generalized Linear Models via Coordinate Descent</article-title>. <source>Journal of Statistical Software, Articles</source>. <year>2010</year>;<volume>33</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>22</lpage>. <?supplied-pmid 20808728?><pub-id pub-id-type="pmid">20808728</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Cox</surname><given-names>DR</given-names></name>. <article-title>Regression Models and Life-Tables</article-title>. <source>Journal of the Royal Statistical Society Series B (Methodological)</source>. <year>1972</year>;<volume>34</volume>(<issue>2</issue>):<fpage>187</fpage>–<lpage>220</lpage>. <pub-id pub-id-type="doi">10.1111/j.2517-6161.1972.tb00899.x</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>R</given-names></name>, <name><surname>Chang</surname><given-names>C</given-names></name>, <name><surname>Justesen</surname><given-names>JM</given-names></name>, <name><surname>Tanigawa</surname><given-names>Y</given-names></name>, <name><surname>Qian</surname><given-names>J</given-names></name>, <name><surname>Hastie</surname><given-names>T</given-names></name>, <etal>et al</etal><article-title>Fast Lasso method for Large-scale and Ultrahigh-dimensional Cox Model with applications to UK Biobank</article-title>. <source>Biostatistics</source>, kxaa038 <year>2020</year><pub-id pub-id-type="doi">10.1093/biostatistics/kxaa038</pub-id><?supplied-pmid 32989444?><pub-id pub-id-type="pmid">32989444</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Zou</surname><given-names>H</given-names></name>, <name><surname>Hastie</surname><given-names>T</given-names></name>. <article-title>Regularization and variable selection via the elastic net</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>. <year>2005</year>;<volume>67</volume>(<issue>2</issue>):<fpage>301</fpage>–<lpage>320</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-9868.2005.00503.x</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Lello</surname><given-names>L</given-names></name>, <name><surname>Avery</surname><given-names>SG</given-names></name>, <name><surname>Tellier</surname><given-names>L</given-names></name>, <name><surname>Vazquez</surname><given-names>AI</given-names></name>, <name><surname>de los Campos</surname><given-names>G</given-names></name>, <name><surname>Hsu</surname><given-names>SDH</given-names></name>. <article-title>Accurate Genomic Prediction of Human Height</article-title>. <source>Genetics</source>. <year>2018</year>;<volume>210</volume>(<issue>2</issue>):<fpage>477</fpage>–<lpage>497</lpage>. <pub-id pub-id-type="doi">10.1534/genetics.118.301267</pub-id><?supplied-pmid 30150289?><pub-id pub-id-type="pmid">30150289</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref022">
      <label>22</label>
      <mixed-citation publication-type="journal"><name><surname>DeBoever</surname><given-names>C</given-names></name>, <name><surname>Tanigawa</surname><given-names>Y</given-names></name>, <name><surname>Lindholm</surname><given-names>ME</given-names></name>, <name><surname>McInnes</surname><given-names>G</given-names></name>, <name><surname>Lavertu</surname><given-names>A</given-names></name>, <name><surname>Ingelsson</surname><given-names>E</given-names></name>, <etal>et al</etal><article-title>Medical Relevance of Protein-Truncating Variants across 337,205 Individuals in the UK Biobank Study</article-title>. <source>Nature Communications</source>. <year>2018</year>;<volume>9</volume>(<issue>1</issue>):<fpage>1612</fpage><pub-id pub-id-type="doi">10.1038/s41467-018-03910-9</pub-id><?supplied-pmid 29691392?><pub-id pub-id-type="pmid">29691392</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref023">
      <label>23</label>
      <mixed-citation publication-type="journal"><name><surname>Wold</surname><given-names>H</given-names></name>. <article-title>Soft Modelling by Latent Variables: The Non-Linear Iterative Partial Least Squares (NIPALS) Approach</article-title>. <source>Journal of Applied Probability</source>. <year>1975</year>;<volume>12</volume>(<issue>S1</issue>):<fpage>117</fpage>–<lpage>142</lpage>. <pub-id pub-id-type="doi">10.1017/S0021900200047604</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Meinshausen</surname><given-names>N</given-names></name>. <article-title>Relaxed Lasso</article-title>. <source>Computational Statistics &amp; Data Analysis</source>. <year>2007</year>;<volume>52</volume>(<issue>1</issue>):<fpage>374</fpage>–<lpage>393</lpage>. <pub-id pub-id-type="doi">10.1016/j.csda.2006.12.019</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref025">
      <label>25</label>
      <mixed-citation publication-type="journal"><name><surname>Tanigawa</surname><given-names>Y</given-names></name>, <name><surname>Li</surname><given-names>J</given-names></name>, <name><surname>Justesen</surname><given-names>JM</given-names></name>, <name><surname>Horn</surname><given-names>H</given-names></name>, <name><surname>Aguirre</surname><given-names>M</given-names></name>, <name><surname>DeBoever</surname><given-names>C</given-names></name>, <etal>et al</etal><article-title>Components of genetic associations across 2,138 phenotypes in the UK Biobank highlight adipocyte biology</article-title>. <source>Nature communications</source>. <year>2019</year>;<volume>10</volume>(<issue>1</issue>):<fpage>4064</fpage><pub-id pub-id-type="doi">10.1038/s41467-019-11953-9</pub-id><?supplied-pmid 31492854?><pub-id pub-id-type="pmid">31492854</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref026">
      <label>26</label>
      <mixed-citation publication-type="journal"><name><surname>Ge</surname><given-names>T</given-names></name>, <name><surname>Chen</surname><given-names>CY</given-names></name>, <name><surname>Ni</surname><given-names>Y</given-names></name>, <name><surname>Feng</surname><given-names>YCA</given-names></name>, <name><surname>Smoller</surname><given-names>JW</given-names></name>. <article-title>Polygenic prediction via Bayesian regression and continuous shrinkage priors</article-title>. <source>Nature Communications</source>. <year>2019</year>;<volume>10</volume>(<issue>1</issue>):<fpage>1776</fpage><pub-id pub-id-type="doi">10.1038/s41467-019-09718-5</pub-id><?supplied-pmid 30992449?><pub-id pub-id-type="pmid">30992449</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Lloyd-Jones</surname><given-names>LR</given-names></name>, <name><surname>Zeng</surname><given-names>J</given-names></name>, <name><surname>Sidorenko</surname><given-names>J</given-names></name>, <name><surname>Yengo</surname><given-names>L</given-names></name>, <name><surname>Moser</surname><given-names>G</given-names></name>, <name><surname>Kemper</surname><given-names>KE</given-names></name>, <etal>et al</etal><article-title>Improved polygenic prediction by Bayesian multiple regression on summary statistics</article-title>. <source>Nature Communications</source>. <year>2019</year>;<volume>10</volume>(<issue>1</issue>):<fpage>1776</fpage><pub-id pub-id-type="doi">10.1038/s41467-019-12653-0</pub-id><?supplied-pmid 30992449?><pub-id pub-id-type="pmid">30992449</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref028">
      <label>28</label>
      <mixed-citation publication-type="other">Purcell S, Chang C. PLINK 2.0; 2020. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.cog-genomics.org/plink/2.0/">www.cog-genomics.org/plink/2.0/</ext-link>.</mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref029">
      <label>29</label>
      <mixed-citation publication-type="journal"><name><surname>Zeng</surname><given-names>J</given-names></name>, <name><surname>De Vlaming</surname><given-names>R</given-names></name>, <name><surname>Wu</surname><given-names>Y</given-names></name>, <name><surname>Robinson</surname><given-names>MR</given-names></name>, <name><surname>Lloyd-Jones</surname><given-names>LR</given-names></name>, <name><surname>Yengo</surname><given-names>L</given-names></name>, <etal>et al</etal><article-title>Signatures of negative selection in the genetic architecture of human complex traits</article-title>. <source>Nature Genetics</source>. <year>2018</year>;<volume>50</volume>(<issue>5</issue>):<fpage>746</fpage>–<lpage>753</lpage>. <pub-id pub-id-type="doi">10.1038/s41588-018-0101-4</pub-id><?supplied-pmid 29662166?><pub-id pub-id-type="pmid">29662166</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref030">
      <label>30</label>
      <mixed-citation publication-type="journal"><name><surname>Silventoinen</surname><given-names>K</given-names></name>, <name><surname>Sammalisto</surname><given-names>S</given-names></name>, <name><surname>Perola</surname><given-names>M</given-names></name>, <name><surname>Boomsma</surname><given-names>DI</given-names></name>, <name><surname>Cornes</surname><given-names>BK</given-names></name>, <name><surname>Davis</surname><given-names>C</given-names></name>, <etal>et al</etal><article-title>Heritability of Adult Body Height: A Comparative Study of Twin Cohorts in Eight Countries</article-title>. <source>Twin Research</source>. <year>2003</year>;<volume>6</volume>(<issue>5</issue>):<fpage>399</fpage>–<lpage>408</lpage>. <pub-id pub-id-type="doi">10.1375/136905203770326402</pub-id><?supplied-pmid 14624724?><pub-id pub-id-type="pmid">14624724</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref031">
      <label>31</label>
      <mixed-citation publication-type="journal"><name><surname>Visscher</surname><given-names>PM</given-names></name>, <name><surname>Medland</surname><given-names>SE</given-names></name>, <name><surname>Ferreira</surname><given-names>MAR</given-names></name>, <name><surname>Morley</surname><given-names>KI</given-names></name>, <name><surname>Zhu</surname><given-names>G</given-names></name>, <name><surname>Cornes</surname><given-names>BK</given-names></name>, <etal>et al</etal><article-title>Assumption-Free Estimation of Heritability from Genome-Wide Identity-by-Descent Sharing between Full Siblings</article-title>. <source>PLOS Genetics</source>. <year>2006</year>;<volume>2</volume>(<issue>3</issue>):<fpage>e41</fpage><pub-id pub-id-type="doi">10.1371/journal.pgen.0020041</pub-id><?supplied-pmid 16565746?><pub-id pub-id-type="pmid">16565746</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref032">
      <label>32</label>
      <mixed-citation publication-type="journal"><name><surname>Visscher</surname><given-names>PM</given-names></name>, <name><surname>McEvoy</surname><given-names>B</given-names></name>, <name><surname>Yang</surname><given-names>J</given-names></name>. <article-title>From Galton to GWAS: Quantitative Genetics of Human Height</article-title>. <source>Genetics Research</source>. <year>2010</year>;<volume>92</volume>(<issue>5-6</issue>):<fpage>371</fpage>–<lpage>379</lpage>. <pub-id pub-id-type="doi">10.1017/S0016672310000571</pub-id><?supplied-pmid 21429269?><pub-id pub-id-type="pmid">21429269</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref033">
      <label>33</label>
      <mixed-citation publication-type="journal"><name><surname>Zaitlen</surname><given-names>N</given-names></name>, <name><surname>Kraft</surname><given-names>P</given-names></name>, <name><surname>Patterson</surname><given-names>N</given-names></name>, <name><surname>Pasaniuc</surname><given-names>B</given-names></name>, <name><surname>Bhatia</surname><given-names>G</given-names></name>, <name><surname>Pollack</surname><given-names>S</given-names></name>, <etal>et al</etal><article-title>Using Extended Genealogy to Estimate Components of Heritability for 23 Quantitative and Dichotomous Traits</article-title>. <source>PLOS Genetics</source>. <year>2013</year>;<volume>9</volume>(<issue>5</issue>):<fpage>e1003520</fpage><pub-id pub-id-type="doi">10.1371/journal.pgen.1003520</pub-id><?supplied-pmid 23737753?><pub-id pub-id-type="pmid">23737753</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref034">
      <label>34</label>
      <mixed-citation publication-type="journal"><name><surname>Hemani</surname><given-names>G</given-names></name>, <name><surname>Yang</surname><given-names>J</given-names></name>, <name><surname>Vinkhuyzen</surname><given-names>A</given-names></name>, <name><surname>Powell</surname><given-names>J</given-names></name>, <name><surname>Willemsen</surname><given-names>G</given-names></name>, <name><surname>Hottenga</surname><given-names>JJ</given-names></name>, <etal>et al</etal><article-title>Inference of the Genetic Architecture Underlying BMI and Height with the Use of 20,240 Sibling Pairs</article-title>. <source>The American Journal of Human Genetics</source>. <year>2013</year>;<volume>93</volume>(<issue>5</issue>):<fpage>865</fpage>–<lpage>875</lpage>. <pub-id pub-id-type="doi">10.1016/j.ajhg.2013.10.005</pub-id><?supplied-pmid 24183453?><pub-id pub-id-type="pmid">24183453</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref035">
      <label>35</label>
      <mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>J</given-names></name>, <name><surname>Benyamin</surname><given-names>B</given-names></name>, <name><surname>McEvoy</surname><given-names>BP</given-names></name>, <name><surname>Gordon</surname><given-names>S</given-names></name>, <name><surname>Henders</surname><given-names>AK</given-names></name>, <name><surname>Nyholt</surname><given-names>DR</given-names></name>, <etal>et al</etal><article-title>Common SNPs Explain a Large Proportion of the Heritability for Human Height</article-title>. <source>Nature Genetics</source>. <year>2010</year>;<volume>42</volume>:<fpage>565</fpage><pub-id pub-id-type="doi">10.1038/ng.608</pub-id><?supplied-pmid 20562875?><pub-id pub-id-type="pmid">20562875</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref036">
      <label>36</label>
      <mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>J</given-names></name>, <name><surname>Bakshi</surname><given-names>A</given-names></name>, <name><surname>Zhu</surname><given-names>Z</given-names></name>, <name><surname>Hemani</surname><given-names>G</given-names></name>, <name><surname>Vinkhuyzen</surname><given-names>AAE</given-names></name>, <name><surname>Lee</surname><given-names>SH</given-names></name>, <etal>et al</etal><article-title>Genetic Variance Estimation with Imputed Variants Finds Negligible Missing Heritability for Human Height and Body Mass Index</article-title>. <source>Nature Genetics</source>. <year>2015</year>;<volume>47</volume>:<fpage>1114</fpage><pub-id pub-id-type="doi">10.1038/ng.3390</pub-id><?supplied-pmid 26323059?><pub-id pub-id-type="pmid">26323059</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref037">
      <label>37</label>
      <mixed-citation publication-type="journal"><name><surname>Lango Allen</surname><given-names>H</given-names></name>, <name><surname>Estrada</surname><given-names>K</given-names></name>, <name><surname>Lettre</surname><given-names>G</given-names></name>, <name><surname>Berndt</surname><given-names>SI</given-names></name>, <name><surname>Weedon</surname><given-names>MN</given-names></name>, <name><surname>Rivadeneira</surname><given-names>F</given-names></name>, <etal>et al</etal><article-title>Hundreds of Variants Clustered in Genomic Loci and Biological Pathways Affect Human Height</article-title>. <source>Nature</source>. <year>2010</year>;<volume>467</volume>:<fpage>832</fpage><pub-id pub-id-type="doi">10.1038/nature09410</pub-id><?supplied-pmid 20881960?><pub-id pub-id-type="pmid">20881960</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref038">
      <label>38</label>
      <mixed-citation publication-type="journal"><name><surname>Wood</surname><given-names>AR</given-names></name>, <name><surname>Esko</surname><given-names>T</given-names></name>, <name><surname>Yang</surname><given-names>J</given-names></name>, <name><surname>Vedantam</surname><given-names>S</given-names></name>, <name><surname>Pers</surname><given-names>TH</given-names></name>, <name><surname>Gustafsson</surname><given-names>S</given-names></name>, <etal>et al</etal><article-title>Defining the Role of Common Variation in the Genomic and Biological Architecture of Adult Human Height</article-title>. <source>Nature Genetics</source>. <year>2014</year>;<volume>46</volume>:<fpage>1173</fpage><pub-id pub-id-type="doi">10.1038/ng.3097</pub-id><?supplied-pmid 25282103?><pub-id pub-id-type="pmid">25282103</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref039">
      <label>39</label>
      <mixed-citation publication-type="journal"><name><surname>Marouli</surname><given-names>E</given-names></name>, <name><surname>Graff</surname><given-names>M</given-names></name>, <name><surname>Medina-Gomez</surname><given-names>C</given-names></name>, <name><surname>Lo</surname><given-names>KS</given-names></name>, <name><surname>Wood</surname><given-names>AR</given-names></name>, <name><surname>Kjaer</surname><given-names>TR</given-names></name>, <etal>et al</etal><article-title>Rare and Low-Frequency Coding Variants Alter Human Adult Height</article-title>. <source>Nature</source>. <year>2017</year>;<volume>542</volume>:<fpage>186</fpage><pub-id pub-id-type="doi">10.1038/nature21039</pub-id><?supplied-pmid 28146470?><pub-id pub-id-type="pmid">28146470</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref040">
      <label>40</label>
      <mixed-citation publication-type="journal"><name><surname>Parikh</surname><given-names>N</given-names></name>, <name><surname>Boyd</surname><given-names>S</given-names></name>. <article-title>Proximal Algorithms</article-title>. <source>Foundations and Trends in Optimization</source>. <year>2014</year>;<volume>1</volume>(<issue>3</issue>):<fpage>127</fpage>–<lpage>239</lpage>. <pub-id pub-id-type="doi">10.1561/2400000003</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref041">
      <label>41</label>
      <mixed-citation publication-type="journal"><name><surname>Xiao</surname><given-names>L</given-names></name>. <article-title>Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</article-title>. <source>Journal of Machine Learning Research</source>. <year>2010</year>;<volume>11</volume>(<issue>88</issue>):<fpage>2543</fpage>–<lpage>2596</lpage>.</mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref042">
      <label>42</label>
      <mixed-citation publication-type="journal"><name><surname>Duchi</surname><given-names>JC</given-names></name>, <name><surname>Agarwal</surname><given-names>A</given-names></name>, <name><surname>Wainwright</surname><given-names>MJ</given-names></name>. <article-title>Dual Averaging for Distributed Optimization: Convergence Analysis and Network Scaling</article-title>. <source>IEEE Transactions on Automatic Control</source>. <year>2012</year>;<volume>57</volume>(<issue>3</issue>):<fpage>592</fpage>–<lpage>606</lpage>. <pub-id pub-id-type="doi">10.1109/TAC.2011.2161027</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref043">
      <label>43</label>
      <mixed-citation publication-type="journal"><name><surname>Bickel</surname><given-names>PJ</given-names></name>, <name><surname>Ritov</surname><given-names>Y</given-names></name>, <name><surname>Tsybakov</surname><given-names>AB</given-names></name>. <article-title>Simultaneous analysis of Lasso and Dantzig selector</article-title>. <source>Ann Statist</source>. <year>2009</year>;<volume>37</volume>(<issue>4</issue>):<fpage>1705</fpage>–<lpage>1732</lpage>. <pub-id pub-id-type="doi">10.1214/08-AOS620</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref044">
      <label>44</label>
      <mixed-citation publication-type="journal"><name><surname>Zhao</surname><given-names>P</given-names></name>, <name><surname>Yu</surname><given-names>B</given-names></name>. <article-title>On model selection consistency of Lasso</article-title>. <source>Journal of Machine learning research</source>. <year>2006</year>;<volume>7</volume>(<issue>90</issue>):<fpage>2541</fpage>–<lpage>2563</lpage>.</mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref045">
      <label>45</label>
      <mixed-citation publication-type="journal"><name><surname>DeLong</surname><given-names>ER</given-names></name>, <name><surname>DeLong</surname><given-names>DM</given-names></name>, <name><surname>Clarke-Pearson</surname><given-names>DL</given-names></name>. <article-title>Comparing the Areas under Two or More Correlated Receiver Operating Characteristic Curves: A Nonparametric Approach</article-title>. <source>Biometrics</source>. <year>1988</year>;<volume>44</volume>(<issue>3</issue>):<fpage>837</fpage>–<lpage>845</lpage>. <pub-id pub-id-type="doi">10.2307/2531595</pub-id><pub-id pub-id-type="pmid">3203132</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref046">
      <label>46</label>
      <mixed-citation publication-type="journal"><name><surname>Cortes</surname><given-names>C</given-names></name>, <name><surname>Mohri</surname><given-names>M</given-names></name>. <article-title>Confidence intervals for the area under the ROC curve</article-title>. In: <source>Advances in Neural Information Processing Systems</source>; <year>2005</year> p. <fpage>305</fpage>–<lpage>312</lpage>.</mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref047">
      <label>47</label>
      <mixed-citation publication-type="journal"><name><surname>Price</surname><given-names>AL</given-names></name>, <name><surname>Patterson</surname><given-names>NJ</given-names></name>, <name><surname>Plenge</surname><given-names>RM</given-names></name>, <name><surname>Weinblatt</surname><given-names>ME</given-names></name>, <name><surname>Shadick</surname><given-names>NA</given-names></name>, <name><surname>Reich</surname><given-names>D</given-names></name>. <article-title>Principal Components Analysis Corrects for Stratification in Genome-Wide Association Studies</article-title>. <source>Nature Genetics</source>. <year>2006</year>;<volume>38</volume>:<fpage>904</fpage><pub-id pub-id-type="doi">10.1038/ng1847</pub-id><pub-id pub-id-type="pmid">16862161</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref048">
      <label>48</label>
      <mixed-citation publication-type="journal"><name><surname>Patterson</surname><given-names>N</given-names></name>, <name><surname>Price</surname><given-names>AL</given-names></name>, <name><surname>Reich</surname><given-names>D</given-names></name>. <article-title>Population Structure and Eigenanalysis</article-title>. <source>PLOS Genetics</source>. <year>2006</year>;<volume>2</volume>(<issue>12</issue>):<fpage>1</fpage>–<lpage>20</lpage>. <pub-id pub-id-type="doi">10.1038/ng1847</pub-id><?supplied-pmid 16862161?><pub-id pub-id-type="pmid">16862161</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref049">
      <label>49</label>
      <mixed-citation publication-type="journal"><name><surname>Kane</surname><given-names>MJ</given-names></name>, <name><surname>Emerson</surname><given-names>J</given-names></name>, <name><surname>Weston</surname><given-names>S</given-names></name>. <article-title>Scalable Strategies for Computing with Massive Data</article-title>. <source>Journal of Statistical Software</source>. <year>2013</year>;<volume>55</volume>(<issue>14</issue>):<fpage>1</fpage>–<lpage>19</lpage>. <pub-id pub-id-type="doi">10.18637/jss.v055.i14</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref050">
      <label>50</label>
      <mixed-citation publication-type="journal"><name><surname>Sobel</surname><given-names>E</given-names></name>, <name><surname>Lange</surname><given-names>K</given-names></name>, <name><surname>Wu</surname><given-names>TT</given-names></name>, <name><surname>Hastie</surname><given-names>T</given-names></name>, <name><surname>Chen</surname><given-names>YF</given-names></name>. <article-title>Genome-Wide Association Analysis by Lasso Penalized Logistic Regression</article-title>. <source>Bioinformatics</source>. <year>2009</year>;<volume>25</volume>(<issue>6</issue>):<fpage>714</fpage>–<lpage>721</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btp041</pub-id><?supplied-pmid 19176549?><pub-id pub-id-type="pmid">19176549</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref051">
      <label>51</label>
      <mixed-citation publication-type="other">El Ghaoui L, Viallon V, Rabbani T. Safe Feature Elimination for the Lasso and Sparse Supervised Learning Problems. arXiv preprint arXiv:10094219. 2010;.</mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref052">
      <label>52</label>
      <mixed-citation publication-type="journal"><name><surname>Fan</surname><given-names>J</given-names></name>, <name><surname>Lv</surname><given-names>J</given-names></name>. <article-title>Sure Independence Screening for Ultrahigh Dimensional Feature Space</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>. <year>2008</year>;<volume>70</volume>(<issue>5</issue>):<fpage>849</fpage>–<lpage>911</lpage>. <pub-id pub-id-type="doi">10.1111/j.1467-9868.2008.00674.x</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref053">
      <label>53</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Wonka</surname><given-names>P</given-names></name>, <name><surname>Ye</surname><given-names>J</given-names></name>. <article-title>Lasso Screening Rules via Dual Polytope Projection</article-title>. <source>Journal of Machine Learning Research</source>. <year>2015</year>;<volume>16</volume>:<fpage>1063</fpage>–<lpage>1101</lpage>.</mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref054">
      <label>54</label>
      <mixed-citation publication-type="other">Zeng Y, Breheny P. The <bold>biglasso</bold> Package: A Memory-and Computation-Efficient Solver for Lasso Model Fitting with Big Data in R. arXiv preprint arXiv:170105936. 2017;.</mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref055">
      <label>55</label>
      <mixed-citation publication-type="journal"><name><surname>Privé</surname><given-names>F</given-names></name>, <name><surname>Blum</surname><given-names>MGB</given-names></name>, <name><surname>Aschard</surname><given-names>H</given-names></name>, <name><surname>Ziyatdinov</surname><given-names>A</given-names></name>. <article-title>Efficient Analysis of Large-Scale Genome-Wide Data with Two R packages: <bold>bigstatsr</bold> and <bold>bigsnpr</bold></article-title>. <source>Bioinformatics</source>. <year>2018</year>;<volume>34</volume>(<issue>16</issue>):<fpage>2781</fpage>–<lpage>2787</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bty185</pub-id><?supplied-pmid 29617937?><pub-id pub-id-type="pmid">29617937</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref056">
      <label>56</label>
      <mixed-citation publication-type="other">Huling JD, Qian PZ. Fast Penalized Regression and Cross Validation for Tall Data with the <bold>oem</bold> Package. arXiv preprint arXiv:180109661. 2018;.</mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref057">
      <label>57</label>
      <mixed-citation publication-type="journal"><name><surname>Speliotes</surname><given-names>EK</given-names></name>, <name><surname>Willer</surname><given-names>CJ</given-names></name>, <name><surname>Berndt</surname><given-names>SI</given-names></name>, <name><surname>Monda</surname><given-names>KL</given-names></name>, <name><surname>Thorleifsson</surname><given-names>G</given-names></name>, <name><surname>Jackson</surname><given-names>AU</given-names></name>, <etal>et al</etal><article-title>Association Analyses of 249,796 Individuals Reveal 18 New Loci Associated with Body Mass Index</article-title>. <source>Nature Genetics</source>. <year>2010</year>;<volume>42</volume>:<fpage>937</fpage><pub-id pub-id-type="doi">10.1038/ng.686</pub-id><?supplied-pmid 20935630?><pub-id pub-id-type="pmid">20935630</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref058">
      <label>58</label>
      <mixed-citation publication-type="journal"><name><surname>Locke</surname><given-names>AE</given-names></name>, <name><surname>Kahali</surname><given-names>B</given-names></name>, <name><surname>Berndt</surname><given-names>SI</given-names></name>, <name><surname>Justice</surname><given-names>AE</given-names></name>, <name><surname>Pers</surname><given-names>TH</given-names></name>, <name><surname>Day</surname><given-names>FR</given-names></name>, <etal>et al</etal><article-title>Genetic Studies of Body Mass Index Yield New Insights for Obesity Biology</article-title>. <source>Nature</source>. <year>2015</year>;<volume>518</volume>:<fpage>197</fpage><pub-id pub-id-type="doi">10.1038/nature14177</pub-id><?supplied-pmid 25673413?><pub-id pub-id-type="pmid">25673413</pub-id></mixed-citation>
    </ref>
    <ref id="pgen.1009141.ref059">
      <label>59</label>
      <mixed-citation publication-type="journal"><name><surname>Turner</surname><given-names>SD</given-names></name>. <article-title><bold>qqman</bold>: An R Package for Visualizing GWAS Results Using Q-Q and Manhattan Plots</article-title>. <source>Journal of Open Source Software</source>. <year>2018</year>;<volume>3</volume>(<issue>25</issue>):<fpage>731</fpage><pub-id pub-id-type="doi">10.21105/joss.00731</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<sub-article id="pgen.1009141.r001" article-type="aggregated-review-documents">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pgen.1009141.r001</article-id>
    <title-group>
      <article-title>Decision Letter 0</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Copenhaver</surname>
          <given-names>Gregory P.</given-names>
        </name>
        <role>Editor-in-Chief</role>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhu</surname>
          <given-names>Xiaofeng</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2020 Copenhaver, Zhu</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Copenhaver, Zhu</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article id="rel-obj001" ext-link-type="doi" xlink:href="10.1371/journal.pgen.1009141" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>0</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">20 Mar 2020</named-content>
    </p>
    <p>Dear Dr Hastie,</p>
    <p>Thank you very much for submitting your Research Article entitled 'A Fast and Scalable Framework for Large-scale and Ultrahigh-dimensional Multivariate Genome-wide Predictive Modeling with Application to the UK Biobank' to PLOS Genetics. Your manuscript was fully evaluated at the editorial level and by independent peer reviewers. The reviewers appreciated the attention to an important problem, but raised some substantial concerns about the current manuscript. Based on the reviews, we will not be able to accept this version of the manuscript, but we would be willing to review again a much-revised version. That is, we will consider a revised manuscript that robustly demonstrates marked improvement over the existing approaches (i.e. Bayesian Approaches, BLUP and polygenic risk scores), as the reviewer 1 pointed out.    We cannot, of course, promise publication at that time.</p>
    <p>Should you decide to revise the manuscript for further consideration here, your revisions should address the specific points made by each reviewer. We will also require a detailed list of your responses to the review comments and a description of the changes you have made in the manuscript.</p>
    <p>If you decide to revise the manuscript for further consideration at PLOS Genetics, please aim to resubmit within the next 60 days, unless it will take extra time to address the concerns of the reviewers, in which case we would appreciate an expected resubmission date by email to <email>plosgenetics@plos.org</email>.</p>
    <p>If present, accompanying reviewer attachments are included with this email; please notify the journal office if any appear to be missing. They will also be available for download from the link below. You can use this link to log into the system when you are ready to submit a revised version, having first consulted our <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosgenetics/s/submit-now#loc-submission-checklist">Submission Checklist</ext-link>.</p>
    <p>To enhance the reproducibility of your results, we recommend that you deposit your laboratory protocols in protocols.io, where a protocol can be assigned its own identifier (DOI) such that it can be cited independently in the future. For instructions see our <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosgenetics/s/submission-guidelines#loc-materials-and-methods">guidelines</ext-link>.</p>
    <p>Please be aware that our <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosgenetics/s/data-availability">data availability policy</ext-link> requires that all numerical data underlying graphs or summary statistics are included with the submission, and you will need to provide this upon resubmission if not already present. In addition, we do not permit the inclusion of phrases such as "data not shown" or "unpublished results" in manuscripts. All points should be backed up by data provided with the submission.</p>
    <p>While revising your submission, please upload your figure files to the <ext-link ext-link-type="uri" xlink:href="http://pace.apexcovantage.com/">Preflight Analysis and Conversion Engine</ext-link> (PACE) digital diagnostic tool.  PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email us at <email>figures@plos.org</email>.</p>
    <p>PLOS has incorporated <ext-link ext-link-type="uri" xlink:href="http://www.crossref.org/crosscheck.html">Similarity Check</ext-link>, powered by iThenticate, into its journal-wide submission system in order to screen submitted content for originality before publication. Each PLOS journal undertakes screening on a proportion of submitted articles. You will be contacted if needed following the screening process.</p>
    <p>To resubmit, use the link below and 'Revise Submission' in the 'Submissions Needing Revision' folder.</p>
    <p>[LINK]</p>
    <p>We are sorry that we cannot be more positive about your manuscript at this stage. Please do not hesitate to contact us if you have any concerns or questions.</p>
    <p>Yours sincerely,</p>
    <p>Xiaofeng Zhu</p>
    <p>Associate Editor</p>
    <p>PLOS Genetics</p>
    <p>Gregory P. Copenhaver</p>
    <p>Editor-in-Chief</p>
    <p>PLOS Genetics</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <bold>Comments to the Authors:</bold>
    </p>
    <p>
      <bold>Please note here if the review is uploaded as an attachment.</bold>
    </p>
    <p>Reviewer #1: Review of Qian et al</p>
    <p>Summary:</p>
    <p>This paper describes an efficient algorithm for fitting</p>
    <p>the lasso regression model to large data sets, along with an implementation</p>
    <p>(R package snpnet) and application to the UK biobank data to</p>
    <p>obtain predictors (effectively performing genomic prediction, or</p>
    <p>computing polygenic risk scores, PRS) for several different phenotypes.</p>
    <p>The paper compares prediction accuracy with some other simpler methods.</p>
    <p>The lasso is, in general, a widely studied, and also quite widely used method.</p>
    <p>As such an algorithm and implementation for very large datasets are of potential interest</p>
    <p>to a general audience both inside and outside genetics. However,</p>
    <p>for readers of PloS genetics the interest is going to stand or fall on</p>
    <p>the application: is Lasso a good method to do genomic prediction?</p>
    <p>I am skeptical of this: the Lasso has never been the method of choice</p>
    <p>for genomic prediction in smaller data sets, with the field generally</p>
    <p>preferring other large-scale regression methods, including</p>
    <p>very simple methods (eg "ridge regression", usually known</p>
    <p>as BLUP in the quantitative genetics literature) or very computationally</p>
    <p>intensive methods (Bayesian regression, usually fit via MCMC). The</p>
    <p>Elastic Net is also sometimes used. But the Lasso, rarely.</p>
    <p>While I will keep an open mind on whether this could change for</p>
    <p>biobank-sized data, the current paper is unconvincing</p>
    <p>on this because none of the comparisons are with state-of-the-art</p>
    <p>approaches to this problem.</p>
    <p>Overall then I think the main contribution of this paper</p>
    <p>is the the algorithmic ideas, whose main appeal is their</p>
    <p>simplicity and generality: I like the fact that the design allows</p>
    <p>the algorithm to maximally exploit previous implementations, rather than</p>
    <p>having to reimplement the coordinate ascent steps for example.</p>
    <p>However, unless the resulting method is really competitive with state</p>
    <p>of the art for genomic prediction then this seems better suited to</p>
    <p>another journal.</p>
    <p>Detailed comments:</p>
    <p>1. Comparisons with other methods:</p>
    <p>The methods used here do not seem to</p>
    <p>represent a reasonable selection of state-of-the-art approaches to forming predictors for</p>
    <p>genetic data, on which there is a large literature. Historically</p>
    <p>genomic prediction has been done using multiple linear regression</p>
    <p>fit either using very simple methods (eg "ridge regression", usually known</p>
    <p>as BLUP in the quantitative genetics literature) or very computationally</p>
    <p>intensive methods (Bayesian regression, usually fit via MCMC).</p>
    <p>More recently, motivated by the difficulty of accessing/sharing</p>
    <p>genotype data, as well as computational considerations,</p>
    <p>a literature has sprung up around methods that attempt</p>
    <p>to build predictors based on summary statistics only (and LD from</p>
    <p>a reference panel). For example, Ge et al and Lloyd-Jones et al:</p>
    <p>
      <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-019-09718-5">https://www.nature.com/articles/s41467-019-09718-5</ext-link>
    </p>
    <p>
      <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-019-12653-0">https://www.nature.com/articles/s41467-019-12653-0</ext-link>
    </p>
    <p>are recent examples, and includes comparisons with other methods,</p>
    <p>the latter specifically on the UK biobank data with some of the same</p>
    <p>phenotypes considered here.</p>
    <p>To take a quick example, in Fig 2 of Lloyd Jones, looking at R2</p>
    <p>for BMI, the performance among the methods they consider</p>
    <p>ranges from 0.1 to 0.126. In this paper</p>
    <p>(Table 3) Lasso achieves 0.103. I realize these numbers are not directly</p>
    <p>comparable, being based on different protocols (CV splits etc)</p>
    <p>for analyzing the UK biobank data, but it illustrates my concern that</p>
    <p>Lasso may not be competitive with the best existing methods.</p>
    <p>2. Algorithmic description</p>
    <p>I found much of the algorithmic description in the overview overly long and hard to follow.</p>
    <p>The basic idea seems rather simple (which is a good thing!) but the presentation seems</p>
    <p>to obscure the simplicity rather than highlighting it.</p>
    <p>The formal presentation of Algorithm 1 in section 4 helps a lot, and I suggest it should be</p>
    <p>moved to the overview section. This should allow the text in the algorithmic</p>
    <p>overview to be shortened, since much of the words seems to be</p>
    <p>repeating, in less precise terms, what is given in Algorithm 1.</p>
    <p>Also:</p>
    <p>- the algorithm and text did not seem to address what happens if the "checking"</p>
    <p>step fails. That is, in step 5 of Algorithm 1, what if no lambda satisfies</p>
    <p>the KKT conditions? Or is this guaranteed not to happen?</p>
    <p>- How is M chosen? Does it matter?</p>
    <p>- the algorithm seems to rely on the fact that marginal screening is</p>
    <p>going to be effective at identifying the correct variables to add in.</p>
    <p>In some cases with complex correlations among variables this may not be true -</p>
    <p>one can construct problems where the best pair of variables to include</p>
    <p>are not among the marginally strongest. How does the algorithm</p>
    <p>cope with that kind of situation? Is it guaranteed to</p>
    <p>converge in practice?</p>
    <p>3. Standardization</p>
    <p>The question of whether or not to standardize variables is usually phrased in terms of modeling</p>
    <p>assumptions -- if rare SNPs have bigger effects than common SNPs then standardization</p>
    <p>could be appropriate and improve predictive performance. The paper suggests</p>
    <p>that standardization will produce</p>
    <p>worse performance but this is not obvious a priori - it should be shown empirically.</p>
    <p>4. Implementation</p>
    <p>The software implementation does not</p>
    <p>appear to be quite ready for widespread distribution</p>
    <p>(e.g. the R package on github has no man pages, and I could</p>
    <p>not find a minimal working example).</p>
    <p>Other</p>
    <p>- the use of the term "multivariate" in the context of a multiple regression with univariate</p>
    <p>outcome is rather confusing. From the title I expected the paper to deal with multivariate outcomes.</p>
    <p>Better to stick to "multiple regression", or perhaps "multi-SNP regression" if you prefer.</p>
    <p>- references to heritability were also confusing. E.g.</p>
    <p>the abstract refers to "state-of-the-art heritability estimation",</p>
    <p>when the goal here seems not to be heritability estimation but</p>
    <p>building a predictor, which are different things.</p>
    <p>Heritability provides an upper</p>
    <p>bound on prediction accuracy from genetic data, but building a predictor is not</p>
    <p>the same as "estimating heritability", and most approaches to estimating heritability</p>
    <p>do not explicitly build predictors. I think you can (and probably should)</p>
    <p>write the whole paper without mentioning heritability, and focussing entirely on PRS and prediction accuracy.</p>
    <p>- the presentation of result is much longer than it need be. The main results for different</p>
    <p>phenotypes and methods could probably be shown in a single figure (e.g. Lloyd-Jones Fig 2).</p>
    <p>Many of the other figures did not seem essential to the main story.</p>
    <p>Refs:</p>
    <p>Ge, T., Chen, C., Ni, Y. et al. Polygenic prediction via Bayesian regression and continuous shrinkage priors. Nat Commun 10, 1776 (2019). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-019-09718-5">https://doi.org/10.1038/s41467-019-09718-5</ext-link></p>
    <p>Lloyd-Jones, L.R., Zeng, J., Sidorenko, J. et al. Improved polygenic prediction by Bayesian multiple regression on summary statistics. Nat Commun 10, 5086 (2019). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41467-019-12653-0">https://doi.org/10.1038/s41467-019-12653-0</ext-link></p>
    <p>Reviewer #2: How to build the best predictive model using large-scale genetic data is important in health and disease studies. This paper provides a true regression approach for this problem, an important alternative to the polygenic risk scores. The results from analysis of the UK Biobank are convincing and interesting. The algorithm seems to be quite reasonable.</p>
    <p>I only have a few minor comments - (1) since Lasso results in biased estimates of the regression coefficients. Do the authors think that by performing further debased estimation, one can further improve the prediction performance? (2) since a very large number of SNPs are selection for each of the data examples, would the consistency results still hold? Lasso theory requires that the model has to be very sparse. (3) Why univariate screening + Lasso does not perform as well as fitting Lasso using all the SNPs? Does this mean that the univariate screening as proposed by Jianqin Fan etc does not really work in the settings considered in this paper?</p>
    <p>**********</p>
    <p>
      <bold>Have all data underlying the figures and results presented in the manuscript been provided?</bold>
    </p>
    <p>Large-scale datasets should be made available via a public repository as described in the <italic>PLOS Genetics</italic>
<ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosgenetics/s/data-availability">data availability policy</ext-link>, and numerical data that underlies graphs or summary statistics should be provided in spreadsheet form as supporting information.</p>
    <p>Reviewer #1: None</p>
    <p>Reviewer #2: Yes</p>
    <p>**********</p>
    <p>PLOS authors have the option to publish the peer review history of their article (<ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/plosgenetics/s/editorial-and-peer-review-process#loc-peer-review-history">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link ext-link-type="uri" xlink:href="https://www.plos.org/privacy-policy">Privacy Policy</ext-link>.</p>
    <p>Reviewer #1: Yes: Matthew Stephens</p>
    <p>Reviewer #2: No</p>
  </body>
</sub-article>
<sub-article id="pgen.1009141.r002" article-type="author-comment">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pgen.1009141.r002</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 0</article-title>
    </title-group>
    <related-article id="rel-obj002" ext-link-type="doi" xlink:href="10.1371/journal.pgen.1009141" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">20 May 2020</named-content>
    </p>
    <supplementary-material content-type="local-data" id="pgen.1009141.s030">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">response_to_reviewers.pdf</named-content></p>
      </caption>
      <media xlink:href="pgen.1009141.s030.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article id="pgen.1009141.r003" article-type="aggregated-review-documents">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pgen.1009141.r003</article-id>
    <title-group>
      <article-title>Decision Letter 1</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Copenhaver</surname>
          <given-names>Gregory P.</given-names>
        </name>
        <role>Editor-in-Chief</role>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhu</surname>
          <given-names>Xiaofeng</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2020 Copenhaver, Zhu</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Copenhaver, Zhu</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article id="rel-obj003" ext-link-type="doi" xlink:href="10.1371/journal.pgen.1009141" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">13 Jul 2020</named-content>
    </p>
    <p>* Please note while forming your response, if your article is accepted, you may have the opportunity to make the peer review history publicly available. The record will include editor decision letters (with reviews) and your responses to reviewer comments. If eligible, we will contact you to opt in or out. *</p>
    <p>Dear Dr Hastie,</p>
    <p>Thank you very much for submitting your Research Article entitled 'A Fast and Scalable Framework for Large-scale and Ultrahigh-dimensional Sparse Regression with Application to the UK Biobank' to PLOS Genetics. Your manuscript was fully evaluated at the editorial level and by independent peer reviewers. The reviewers appreciated the attention to an important topic but identified some aspects of the manuscript that should be improved.</p>
    <p>We therefore ask you to modify the manuscript according to the review recommendations before we can consider your manuscript for acceptance. Your revisions should address the specific points made by reviewers. Reviewer #1 raised important issues regarding the results of SBayesR.  This issue will need to be fully resolved in the revision and the editors agree that it may be very helpful for you to reach out the authors of SBayesR during your revision process, but we leave that up to you to decide.</p>
    <p>In addition we ask that you:</p>
    <p>1) Provide a detailed list of your responses to the review comments and a description of the changes you have made in the manuscript.</p>
    <p>2) Upload a Striking Image with a corresponding caption to accompany your manuscript if one is available (either a new image or an existing one from within your manuscript). If this image is judged to be suitable, it may be featured on our website. Images should ideally be high resolution, eye-catching, single panel square images. For examples, please browse our <ext-link ext-link-type="uri" xlink:href="http://www.plosgenetics.org/article/browse/volume">archive</ext-link>. If your image is from someone other than yourself, please ensure that the artist has read and agreed to the terms and conditions of the Creative Commons Attribution License. Note: we cannot publish copyrighted images.</p>
    <p>We hope to receive your revised manuscript within the next 30 days. If you anticipate any delay in its return, we would ask you to let us know the expected resubmission date by email to <email>plosgenetics@plos.org</email>.</p>
    <p>If present, accompanying reviewer attachments should be included with this email; please notify the journal office if any appear to be missing. They will also be available for download from the link below. You can use this link to log into the system when you are ready to submit a revised version, having first consulted our <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosgenetics/s/submit-now#loc-submission-checklist">Submission Checklist</ext-link>.</p>
    <p>While revising your submission, please upload your figure files to the <ext-link ext-link-type="uri" xlink:href="http://pace.apexcovantage.com/">Preflight Analysis and Conversion Engine</ext-link> (PACE) digital diagnostic tool. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email us at <email>figures@plos.org</email>.</p>
    <p>Please be aware that our <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosgenetics/s/data-availability">data availability policy</ext-link> requires that all numerical data underlying graphs or summary statistics are included with the submission, and you will need to provide this upon resubmission if not already present. In addition, we do not permit the inclusion of phrases such as "data not shown" or "unpublished results" in manuscripts. All points should be backed up by data provided with the submission.</p>
    <p>PLOS has incorporated <ext-link ext-link-type="uri" xlink:href="http://www.crossref.org/crosscheck.html">Similarity Check</ext-link>, powered by iThenticate, into its journal-wide submission system in order to screen submitted content for originality before publication. Each PLOS journal undertakes screening on a proportion of submitted articles. You will be contacted if needed following the screening process.</p>
    <p>To resubmit, you will need to go to the link below and 'Revise Submission' in the 'Submissions Needing Revision' folder.</p>
    <p>[LINK]</p>
    <p>Please let us know if you have any questions while making these revisions.</p>
    <p>Yours sincerely,</p>
    <p>Xiaofeng Zhu</p>
    <p>Associate Editor</p>
    <p>PLOS Genetics</p>
    <p>Gregory P. Copenhaver</p>
    <p>Editor-in-Chief</p>
    <p>PLOS Genetics</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <bold>Comments to the Authors:</bold>
    </p>
    <p>
      <bold>Please note here if the review is uploaded as an attachment.</bold>
    </p>
    <p>Reviewer #1: The authors have done a thorough job responding to my comments, and I believe the whole paper is much improved.</p>
    <p>Just one new substantive issue has arisen during this revision: the results reported for the SBayesR method</p>
    <p>are very poor, and seem to strongly contradict the original publication on this method. Indeed, it is a bit</p>
    <p>hard to believe that it performs quite so poorly, and the reasons for its poor performance need to be understood</p>
    <p>and either corrected or explained. For example, for height, SBayesR does no better than just Age + Sex in predicting</p>
    <p>height - so it essentially has a 0% R2 when you consider the genetic component only. In contrast, LLoyd-Jones et al</p>
    <p>report that SBayesR achieved an R2 of &gt;35% for height in the UK biobank.</p>
    <p>Something is clearly wrong, either with the SBayes software or with</p>
    <p>the way it has been applied. (Other traits show a similar pattern, but the height result is particularly striking.)</p>
    <p>Of course, I don't know what the problem is, but I suggest a first step would be to ask the SBayesR authors</p>
    <p>if they have suggestions, and/or get their original code and see if you can reproduce their published results.</p>
    <p>Other items:</p>
    <p>in SBayes i noticed you excluded the MHC. Maybe this is recommended by SBayes software, but it seems likely to hurt R2 and</p>
    <p>AUC for many traits as the MHC has a strong effect on many traits.</p>
    <p>To make results comparable across methods it seems necessary to either exclude or include MHC for all methods.</p>
    <p>(It seems unlikely that this issue explains the poor performance on height noted above.)</p>
    <p>Reviewer #2: my previous comments were minor and the authors have addressed these comments.</p>
    <p>**********</p>
    <p>
      <bold>Have all data underlying the figures and results presented in the manuscript been provided?</bold>
    </p>
    <p>Large-scale datasets should be made available via a public repository as described in the <italic>PLOS Genetics</italic>
<ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosgenetics/s/data-availability">data availability policy</ext-link>, and numerical data that underlies graphs or summary statistics should be provided in spreadsheet form as supporting information.</p>
    <p>Reviewer #1: <bold>No: </bold>UK Biobank data can't be provided</p>
    <p>Reviewer #2: Yes</p>
    <p>**********</p>
    <p>PLOS authors have the option to publish the peer review history of their article (<ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/plosgenetics/s/editorial-and-peer-review-process#loc-peer-review-history">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link ext-link-type="uri" xlink:href="https://www.plos.org/privacy-policy">Privacy Policy</ext-link>.</p>
    <p>Reviewer #1: <bold>Yes: </bold>Matthew Stephens</p>
    <p>Reviewer #2: No</p>
  </body>
</sub-article>
<sub-article id="pgen.1009141.r004" article-type="author-comment">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pgen.1009141.r004</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 1</article-title>
    </title-group>
    <related-article id="rel-obj004" ext-link-type="doi" xlink:href="10.1371/journal.pgen.1009141" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>2</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">17 Aug 2020</named-content>
    </p>
    <supplementary-material content-type="local-data" id="pgen.1009141.s031">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">response_to_reviewers_v2.pdf</named-content></p>
      </caption>
      <media xlink:href="pgen.1009141.s031.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article id="pgen.1009141.r005" article-type="editor-report">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pgen.1009141.r005</article-id>
    <title-group>
      <article-title>Decision Letter 2</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Copenhaver</surname>
          <given-names>Gregory P.</given-names>
        </name>
        <role>Editor-in-Chief</role>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhu</surname>
          <given-names>Xiaofeng</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2020 Copenhaver, Zhu</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Copenhaver, Zhu</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article id="rel-obj005" ext-link-type="doi" xlink:href="10.1371/journal.pgen.1009141" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>2</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">4 Sep 2020</named-content>
    </p>
    <p>Dear Dr Hastie,</p>
    <p>We are pleased to inform you that your manuscript entitled "A Fast and Scalable Framework for Large-scale and Ultrahigh-dimensional Sparse Regression with Application to the UK Biobank" has been editorially accepted for publication in PLOS Genetics. Congratulations!</p>
    <p>Before your submission can be formally accepted and sent to production you will need to complete our formatting changes, which you will receive in a follow up email. Please be aware that it may take several days for you to receive this email; during this time no action is required by you. Please note: the accept date on your published article will reflect the date of this provisional accept, but your manuscript will not be scheduled for publication until the required changes have been made.</p>
    <p>Once your paper is formally accepted, an uncorrected proof of your manuscript will be published online ahead of the final version, unless you’ve already opted out via the online submission form. If, for any reason, you do not want an earlier version of your manuscript published online or are unsure if you have already indicated as such, please let the journal staff know immediately at <email>plosgenetics@plos.org</email>.</p>
    <p>In the meantime, please log into Editorial Manager at <ext-link ext-link-type="uri" xlink:href="https://www.editorialmanager.com/pgenetics/">https://www.editorialmanager.com/pgenetics/</ext-link>, click the "Update My Information" link at the top of the page, and update your user information to ensure an efficient production and billing process. Note that PLOS requires an ORCID iD for all corresponding authors. Therefore, please ensure that you have an ORCID iD and that it is validated in Editorial Manager. To do this, go to ‘Update my Information’ (in the upper left-hand corner of the main menu), and click on the Fetch/Validate link next to the ORCID field.  This will take you to the ORCID site and allow you to create a new iD or authenticate a pre-existing iD in Editorial Manager.</p>
    <p>If you have a press-related query, or would like to know about one way to make your underlying data available (as you will be aware, this is required for publication), please see the end of this email. If your institution or institutions have a press office, please notify them about your upcoming article at this point, to enable them to help maximise its impact. Inform journal staff as soon as possible if you are preparing a press release for your article and need a publication date.</p>
    <p>Thank you again for supporting open-access publishing; we are looking forward to publishing your work in PLOS Genetics!</p>
    <p>Yours sincerely,</p>
    <p>Xiaofeng Zhu</p>
    <p>Associate Editor</p>
    <p>PLOS Genetics</p>
    <p>Gregory P. Copenhaver</p>
    <p>Editor-in-Chief</p>
    <p>PLOS Genetics</p>
    <p>
      <ext-link ext-link-type="uri" xlink:href="http://www.plosgenetics.org">www.plosgenetics.org</ext-link>
    </p>
    <p>Twitter: @PLOSGenetics</p>
    <p>----------------------------------------------------</p>
    <p>Comments from the reviewers (if applicable):</p>
    <p>----------------------------------------------------</p>
    <p>
      <bold>Data Deposition</bold>
    </p>
    <p>If you have submitted a Research Article or Front Matter that has associated data that are not suitable for deposition in a subject-specific public repository (such as GenBank or ArrayExpress), one way to make that data available is to deposit it in the <ext-link ext-link-type="uri" xlink:href="http://www.datadryad.org">Dryad Digital Repository</ext-link>. As you may recall, we ask all authors to agree to make data available; this is one way to achieve that. A full list of recommended repositories can be found on our <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosgenetics/s/data-availability#loc-recommended-repositories">website</ext-link>.</p>
    <p>The following link will take you to the Dryad record for your article, so you won't have to re‐enter its bibliographic information, and can upload your files directly: </p>
    <p>
      <ext-link ext-link-type="uri" xlink:href="http://datadryad.org/submit?journalID=pgenetics&amp;manu=PGENETICS-D-20-00068R2">http://datadryad.org/submit?journalID=pgenetics&amp;manu=PGENETICS-D-20-00068R2</ext-link>
    </p>
    <p>More information about depositing data in Dryad is available at <ext-link ext-link-type="uri" xlink:href="http://www.datadryad.org/depositing">http://www.datadryad.org/depositing</ext-link>. If you experience any difficulties in submitting your data, please contact <email>help@datadryad.org</email> for support.</p>
    <p>Additionally, please be aware that our <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosgenetics/s/data-availability">data availability policy</ext-link> requires that all numerical data underlying display items are included with the submission, and you will need to provide this before we can formally accept your manuscript, if not already present.</p>
    <p>----------------------------------------------------</p>
    <p>
      <bold>Press Queries</bold>
    </p>
    <p>If you or your institution will be preparing press materials for this manuscript, or if you need to know your paper's publication date for media purposes, please inform the journal staff as soon as possible so that your submission can be scheduled accordingly. Your manuscript will remain under a strict press embargo until the publication date and time. This means an early version of your manuscript will not be published ahead of your final version. PLOS Genetics may also choose to issue a press release for your article. If there's anything the journal should know or you'd like more information, please get in touch via <email>plosgenetics@plos.org</email>.</p>
  </body>
</sub-article>
<sub-article id="pgen.1009141.r006" article-type="editor-report">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pgen.1009141.r006</article-id>
    <title-group>
      <article-title>Acceptance letter</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Copenhaver</surname>
          <given-names>Gregory P.</given-names>
        </name>
        <role>Editor-in-Chief</role>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhu</surname>
          <given-names>Xiaofeng</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2020 Copenhaver, Zhu</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Copenhaver, Zhu</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article id="rel-obj006" ext-link-type="doi" xlink:href="10.1371/journal.pgen.1009141" related-article-type="reviewed-article"/>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">13 Oct 2020</named-content>
    </p>
    <p>PGENETICS-D-20-00068R2 </p>
    <p>A Fast and Scalable Framework for Large-scale and Ultrahigh-dimensional Sparse Regression with Application to the UK Biobank </p>
    <p>Dear Dr Hastie, </p>
    <p>We are pleased to inform you that your manuscript entitled "A Fast and Scalable Framework for Large-scale and Ultrahigh-dimensional Sparse Regression with Application to the UK Biobank" has been formally accepted for publication in PLOS Genetics! Your manuscript is now with our production department and you will be notified of the publication date in due course.</p>
    <p>The corresponding author will soon be receiving a typeset proof for review, to ensure errors have not been introduced during production. Please review the PDF proof of your manuscript carefully, as this is the last chance to correct any errors. Please note that major changes, or those which affect the scientific understanding of the work, will likely cause delays to the publication date of your manuscript. </p>
    <p>Soon after your final files are uploaded, unless you have opted out or your manuscript is a front-matter piece, the early version of your manuscript will be published online. The date of the early version will be your article's publication date. The final article will be published to the same URL, and all versions of the paper will be accessible to readers.</p>
    <p>Thank you again for supporting PLOS Genetics and open-access publishing. We are looking forward to publishing your work! </p>
    <p>With kind regards,</p>
    <p>Matt Lyles</p>
    <p>PLOS Genetics</p>
    <p>On behalf of:</p>
    <p>The PLOS Genetics Team</p>
    <p>Carlyle House, Carlyle Road, Cambridge CB4 3DN | United Kingdom</p>
    <p><email>plosgenetics@plos.org</email> | +44 (0) 1223-442823</p>
    <p><ext-link ext-link-type="uri" xlink:href="http://plosgenetics.org">plosgenetics.org</ext-link> | Twitter: @PLOSGenetics</p>
  </body>
</sub-article>
