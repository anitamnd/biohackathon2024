<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Commun Med (Lond)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Commun Med (Lond)</journal-id>
    <journal-title-group>
      <journal-title>Communications Medicine</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2730-664X</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10110614</article-id>
    <article-id pub-id-type="pmid">37069396</article-id>
    <article-id pub-id-type="publisher-id">287</article-id>
    <article-id pub-id-type="doi">10.1038/s43856-023-00287-9</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>An open-source deep learning network AVA-Net for arterial-venous area segmentation in optical coherence tomography angiography</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7463-9470</contrib-id>
        <name>
          <surname>Abtahi</surname>
          <given-names>Mansour</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Le</surname>
          <given-names>David</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3390-7330</contrib-id>
        <name>
          <surname>Ebrahimi</surname>
          <given-names>Behrouz</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dadzie</surname>
          <given-names>Albert K.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lim</surname>
          <given-names>Jennifer I.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0356-3242</contrib-id>
        <name>
          <surname>Yao</surname>
          <given-names>Xincheng</given-names>
        </name>
        <address>
          <email>xcy@uic.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.185648.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2175 0319</institution-id><institution>Department of Biomedical Engineering, </institution><institution>University of Illinois at Chicago, </institution></institution-wrap>Chicago, IL 60607 USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.185648.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2175 0319</institution-id><institution>Department of Ophthalmology and Visual Sciences, </institution><institution>University of Illinois at Chicago, </institution></institution-wrap>Chicago, IL 60612 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>17</day>
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>17</day>
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>3</volume>
    <elocation-id>54</elocation-id>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>11</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>6</day>
        <month>4</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Differential artery-vein (AV) analysis in optical coherence tomography angiography (OCTA) holds promise for the early detection of eye diseases. However, currently available methods for AV analysis are limited for binary processing of retinal vasculature in OCTA, without quantitative information of vascular perfusion intensity. This study is to develop and validate a method for quantitative AV analysis of vascular perfusion intensity.</p>
      </sec>
      <sec>
        <title>Method</title>
        <p id="Par2">A deep learning network AVA-Net has been developed for automated AV area (AVA) segmentation in OCTA. Seven new OCTA features, including arterial area (AA), venous area (VA), AVA ratio (AVAR), total perfusion intensity density (T-PID), arterial PID (A-PID), venous PID (V-PID), and arterial-venous PID ratio (AV-PIDR), were extracted and tested for early detection of diabetic retinopathy (DR). Each of these seven features was evaluated for quantitative evaluation of OCTA images from healthy controls, diabetic patients without DR (NoDR), and mild DR.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par3">It was observed that the area features, i.e., AA, VA and AVAR, can reveal significant differences between the control and mild DR. Vascular perfusion parameters, including T-PID and A-PID, can differentiate mild DR from control group. AV-PIDR can disclose significant differences among all three groups, i.e., control, NoDR, and mild DR. According to Bonferroni correction, the combination of A-PID and AV-PIDR can reveal significant differences in all three groups.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par4">AVA-Net, which is available on GitHub for open access, enables quantitative AV analysis of AV area and vascular perfusion intensity. Comparative analysis revealed AV-PIDR as the most sensitive feature for OCTA detection of early DR. Ensemble AV feature analysis, e.g., the combination of A-PID and AV-PIDR, can further improve the performance for early DR assessment.</p>
      </sec>
    </abstract>
    <abstract id="Abs2" abstract-type="summary">
      <title>Plain Language Summary</title>
      <p id="Par5">Some people with diabetes develop diabetic retinopathy, in which the blood flow through the eye changes, resulting in damage to the back of the eye, called the retina. Changes in blood flow can be measured by imaging the eye using a method called optical coherence tomography angiography (OCTA). The authors developed a computer program named AVA-Net that determines changes in blood flow through the eye from OCTA images. The program was tested on images from people with healthy eyes, people with diabetes but no eye disease, and people with mild diabetic retinopathy. Their program found differences between these groups and so could be used to improve diagnosis of people with diabetic retinopathy.</p>
    </abstract>
    <abstract id="Abs3" abstract-type="web-summary">
      <p id="Par6">Abtahi et al. develop a deep learning network named AVA-Net for automated arterial-venous area segmentation in optical coherence tomography angiography. AVA-Net enables quantitative artery-vein analysis of vascular perfusion intensity density, supporting the early detection of diabetic retinopathy.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Diagnostic markers</kwd>
      <kwd>Biomarkers</kwd>
      <kwd>Macular degeneration</kwd>
      <kwd>Imaging</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/100000053</institution-id>
            <institution>U.S. Department of Health &amp; Human Services | NIH | National Eye Institute (NEI)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>P30 EY001792</award-id>
        <award-id>R01 EY023522</award-id>
        <award-id>R01 EY030101</award-id>
        <award-id>R01EY029673</award-id>
        <award-id>R01EY030842</award-id>
        <principal-award-recipient>
          <name>
            <surname>Yao</surname>
            <given-names>Xincheng</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>U.S. Department of Health &amp; Human Services | NIH | National Eye Institute (NEI)</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>U.S. Department of Health &amp; Human Services | NIH | National Eye Institute (NEI)</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>U.S. Department of Health &amp; Human Services | NIH | National Eye Institute (NEI)</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>U.S. Department of Health &amp; Human Services | NIH | National Eye Institute (NEI)</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/100001818</institution-id>
            <institution>Research to Prevent Blindness (RPB)</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Richard and Loan Hill Endowment</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2023</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par7">Early disease diagnosis and effective treatment assessment require quantitative analysis of retinal neurovascular changes. Diabetes, strokes, hypertension, and vascular disorders are among the diseases that affect retinal vessels. The blood vessels show abnormalities in the early stages of diabetic retinopathy (DR), including alterations in diameter<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. During the early stages of disease development, as well as throughout the process, arteries and veins are affected differently. Therefore, differential artery-vein (AV) analysis has been shown to be useful for evaluating diabetes, hypertension, strokes, cardiovascular disease, and common retinopathies<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. The addition of AV analysis capabilities to clinical imaging devices would enhance the accuracy of disease detection and classification. For differential AV analysis, the first step is to perform AV segmentation or classification in retinal images. The AV segmentation has been conducted using a variety of imaging modalities, including fundus photography, OCT, and OCTA<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. AV segmentation and classification have been mostly performed on color fundus images<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> using feature extraction-based methods<sup><xref ref-type="bibr" rid="CR4">4</xref>–<xref ref-type="bibr" rid="CR6">6</xref></sup> and machine learning based approaches<sup><xref ref-type="bibr" rid="CR7">7</xref>–<xref ref-type="bibr" rid="CR9">9</xref></sup>. However, fundus images have limited resolution and contrast to reveal microvascular abnormalities in the retina, particularly difficult for evaluating smaller capillary level blood vessels around the fovea<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>.</p>
    <p id="Par8">By providing micrometer scale resolution to visualize retinal neurovasculature, OCT has been widely used for ophthalmic research and clinical management of eye conditions<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. As one new modality of OCT, OCTA detects microvascular distortions associated with eye diseases noninvasively at the capillary level<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. Especially in the 6 mm × 6 mm and 3 mm × 3 mm field of views, the OCTA can provide depth-resolved visualization of individual retinal layers with the capillary level resolution. Quantitative analysis of the OCTA images has been extensively studied for the objective detection and classification of retinal diseases<sup><xref ref-type="bibr" rid="CR14">14</xref>–<xref ref-type="bibr" rid="CR19">19</xref></sup>. Through the identification of capillary vortices in the deep capillary plexus, Xu et al.<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> demonstrated how to manually distinguish retinal arteries from veins in clinical OCTA images. Depth resolved OCT profiles were studied by Kim et al.<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> and Adejumo et al.<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> for objective AV classification. Alam et al.<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> and Son et al.<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> analyzed en face OCT and en face OCTA features for AV classification using quantitative feature analysis.</p>
    <p id="Par9">For clinical deployments of differential AV analysis, the development of a fully automated method is essential. Using a convolutional neural network (CNN), Alam et al.<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> demonstrated deep learning AV segmentation with early fusion of en face OCT image and OCTA images for the first time. Using montaged wide-field OCTA images, Gao et al.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> developed a unimodal strategy in deep learning for AV segmentation. Using different approaches, all above mentioned studies exploring the AV classification or segmentation have been primarily focused on the detection and segmentation of large vessels as arteries or veins. Abtahi et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> quantitatively evaluated multimodal architectures with early and late OCT-OCTA fusions, compared to the unimodal architectures with OCT-only and OCTA-only inputs. They observed that the OCTA-only architecture with OCTA images as input is sufficient for robust AV segmentation. Using 3 mm × 3 mm OCTA images, they were able to segment arteries and veins to the capillary level.</p>
    <p id="Par10">All previous studies for differential AV analysis in OCTA were limited to blood vessel density and caliber quantification. In other words, previously derived AV maps were in binary format to separate arteries and veins, without the preservation of signal intensity, i.e., vascular perfusion information. Recent studies<sup><xref ref-type="bibr" rid="CR26">26</xref>–<xref ref-type="bibr" rid="CR28">28</xref></sup> indicate that OCTA intensity (non-binarized) derived quantitative features such as flux can provide a better sensitivity to detect vascular perfusion abnormalities, compared to binarized vessel area density (VAD) analysis. In this study, we present a new approach to achieve AV segmentation with preserved OCTA intensity information for AV analysis of the perfusion intensity density (PID). A deep learning network AVA-Net is developed to achieve automatic segmentation of AV areas to generate the AVA map. By multiplying the OCTA image by the AVA map, we can have the OCTA-AV map that contains the OCTA intensity, i.e., vascular perfusion information, with red and blue channels to separate arterial and venous areas. Using this approach, we can classify all the visible vessels in the OCTA images at different orders and scales as arterial or venous. The intersection-over-union (IoU), Dice coefficient, and segmentation accuracy are used as the evaluation metrics for the validation of AVA-Net performance. Seven new quantitative OCTA features, termed arterial area (AA), venous area (VA), AVA ratio (AVAR), total PID (T-PID), arterial PID (A-PID), venous PID (V-PID), and AV PID ratio (AV-PIDR) are verified for objective detection of DR.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>Data acquisition</title>
      <p id="Par11">The study was approved by the Institutional Review Board (IRB) of the University of Illinois at Chicago (UIC) and is in adherence to the ethical standards set forth in the Declaration of Helsinki. The en face OCTA images used for this study are 6 mm × 6 mm scans collected at UIC. In this study, we have two different datasets. The training dataset which consists of 104 OCTA images and their ground truths (68 control, 12 mild DR, 11 moderate DR, and 13 severe DR scans) is planned to be used for training and validation of the CNN. The test dataset which consists of 64 OCTA images without ground truths (25 eyes from 17 control participants, 18 eyes from 13 NoDR patients, and 21 eyes from 18 mild DR patients) is planned to be used for qualitative testing of the CNN and quantitative analysis with the focus on early detection of DR. Table <xref rid="Tab1" ref-type="table">1</xref> summarizes all participant demographics and diabetes-related parameters in the test dataset. Control subjects and diabetic patients without and with DR in different stages were recruited from the UIC retina clinic. The patients present in this study are representative of a university population of diabetic patients who require clinical diagnosis and management of DR. Subjects who were 18 years of age or older met the inclusion criteria. In addition, diabetic patients having a diagnosis of Type II diabetes mellitus met the inclusion criteria for our diabetic cohort. The diabetic patients were not insulin dependent. Subjects with macular edema, proliferative DR, previous vitrectomy surgery, history of other ocular disorders other than cataracts or minor refractive error, and ungradable and low-quality OCT pictures were exclusion criterions. There is no preference between left or right eyes. A board-certified retina specialist classified the patients as NoDR or different stages of NPDR according to the Early Treatment Diabetic Retinopathy Study (ETDRS) staging system. All patients underwent a complete anterior and dilated posterior segment examination. All control OCTA images were obtained from healthy volunteers that provided informed consent for OCT/OCTA imaging. Deidentified diabetic datasets were obtained for retrospective analysis. The IRB waived the need for informed consent from the patients, as patient privacy and confidentiality were maintained according to IRB guidelines. All subjects underwent OCT and OCTA imaging of both eyes (OD and OS). One en face OCTA of each eye was used for this study.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Demographic of the healthy subjects, NoDR and mild NPDR patients.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Healthy subjects</th><th>NoDR</th><th>Mild NPDR</th></tr></thead><tbody><tr><td>Number of subjects (<italic>n</italic>)</td><td>17</td><td>13</td><td>18</td></tr><tr><td>Number of images (<italic>n</italic>)</td><td>25</td><td>18</td><td>21</td></tr><tr><td>Age (years)</td><td>52.4 ± 14.57</td><td>56.54 ± 9.21</td><td>62.28 ± 12.81</td></tr><tr><td>Age range</td><td>35–80</td><td>40–70</td><td>24–78</td></tr><tr><td>Sex (male/female)</td><td>10/7</td><td>4/9</td><td>9/9</td></tr><tr><td>Duration of diabetes (years)</td><td>−</td><td>11.44 ± 5.06</td><td>18.10 ± 5.50</td></tr><tr><td>Diabetes type</td><td>−</td><td>Type II</td><td>Type II</td></tr><tr><td>Quality index (1–10)</td><td>8.00 ± 0.89</td><td>8.00 ± 1.11</td><td>7.43 ± 0.73</td></tr><tr><td>HTN prevalence (Y/N)</td><td>0/17</td><td>11/2</td><td>15/3</td></tr></tbody></table><table-wrap-foot><p><italic>HTN</italic> hypertension.</p></table-wrap-foot></table-wrap></p>
      <p id="Par12">Spectral domain (SD) en face OCTA data were acquired using an AngioVue SD-OCT device (Optovue, Fremont, CA, USA). The OCT device had a 70,000 Hz A-scan rate, ~5 μm axial resolution, and ~15 μm lateral resolution for 6 mm × 6 mm scans. Only superficial OCTA images were used in this study. After image reconstruction, en face OCTA was exported from the ReVue software interface (Optovue) for further processing.</p>
    </sec>
    <sec id="Sec4">
      <title>Generating ground truths</title>
      <p id="Par13">As reported in previous publication<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, readers can rely on various characteristics in OCTA images to manually detect arteries and veins accurately in the 6 mm × 6 mm dataset: (1) The presence of the capillary-free zone is associated with arteries; (2) arteries do not cross other arteries and veins do not cross other veins, physiologically; (3) vessels can be traced back proximally and distally to aid in identification; (4) arteries and veins typically alternate as each vein drains capillary beds perfused by adjacent arteries. Figure <xref rid="Fig1" ref-type="fig">1a, b</xref> show a representative OCTA image and corresponding manually generated AV map. For generating AVA maps for the training dataset, the k-nearest neighbor (kNN) classifier is used to classify background pixels in Fig. <xref rid="Fig1" ref-type="fig">1b</xref> as pixels in arterial or venous areas. Since we segmented all the visible large vessels in AV maps and used kNN only to classify background pixels as pixels in arterial or venous areas, the generated AVA maps using kNN are reliable and accurate. They were reviewed and approved by an ophthalmologist. Considering Euclidean distance as distance metric and distance-weighted voting, k values between 4 and 25 generate approximately similar and smooth AVA maps. To minimize the computation cost, the k value of 5 is selected. The output of the kNN classifier is presented in Fig. <xref rid="Fig1" ref-type="fig">1c</xref> with a lighter tone of blue and red comparing to arteries and veins presented in Fig. <xref rid="Fig1" ref-type="fig">1b</xref>. The union of the arteries and veins with corresponding arterial and venous areas generate the AVA maps represented in Fig. <xref rid="Fig1" ref-type="fig">1d</xref>. Generating ground truth AV maps for 3 mm × 3 mm OCTA images is discussed in our previous study<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. The above-mentioned procedure can be used to generate ground truth AVA maps for 3 mm × 3 mm OCTA images.<fig id="Fig1"><label>Fig. 1</label><caption><title>Illustration of basic procedures for generating AVA map from OCTA image and calculating quantitative features.</title><p><bold>a</bold> OCTA image. <bold>b</bold> Manually generated AV map. <bold>c</bold> Output of kNN classifier. <bold>d</bold> AVA map. <bold>e</bold> OCTA-AV map. <bold>f</bold> Fovea and OCTA layer indicator in the OCTA-AV map. <bold>g</bold> OCTA-AV map excluding the fovea and OCTA layer indicator area. <bold>h</bold> AVA map excluding the fovea and OCTA layer indicator area. <bold>i</bold> Arterial area. <bold>j</bold> Venous area. <bold>k</bold> Total area shows the summation of arterial and venous areas with white color. <bold>l</bold> Arterial area of the OCTA image. <bold>m</bold> Venous area of the OCTA image. <bold>n</bold> Total area of the OCTA image excluding the fovea and OCTA layer indicator area. Calculating quantitative features is indicated by blue arrows and boxes. OCTA optical coherence tomography angiography, AV artery-vein, kNN k-nearest neighbor, AVA arterial-venous area, AVAR α<sub>AV</sub>, arterial-venous area ratio, AA α<sub>A</sub>, arterial area, VA α<sub>V</sub>, venous area, A-PID P<sub>A</sub>, arterial perfusion intensity density, V-PID P<sub>V</sub>, venous perfusion intensity density, T-PID P<sub>T</sub>, total perfusion intensity density, AV-PIDR P<sub>AV</sub>, arterial-venous perfusion intensity density ratio.</p></caption><graphic xlink:href="43856_2023_287_Fig1_HTML" id="d32e678"/></fig></p>
    </sec>
    <sec id="Sec5">
      <title>Quantitative features</title>
      <p id="Par14">By multiplying the OCTA image by the AVA map represented in Fig. <xref rid="Fig1" ref-type="fig">1a and d</xref>, respectively, we can have the OCTA-AV map demonstrated in Fig. <xref rid="Fig1" ref-type="fig">1e</xref>. To the best of our knowledge, for the first time, OCTA-AV maps are introduced in this paper as images that contain the intensity information of an OCTA image, with separate red and blue channels for arterial and venous areas. By using this method, all the vessels at different orders and scales which are visible in the OCTA images can be classified as arteries or veins. Figure <xref rid="Fig1" ref-type="fig">1f</xref> shows the fovea (diameter 1 mm) and OCTA layer indicator with a white circle and yellow rectangle, respectively. During image scanning of the macula, the commercial imaging device detects the fovea center automatically to keep the fovea at the center of the image. Accordingly, we can say that the center of the circle is approximately in the center of the image. Since the foveal avascular zone (FAZ) is devoid of blood vessels, the arterial and venous area segmentation in this area is artificial. As with the fovea (diameter 1 mm), the OCTA layer indicator area at the bottom left corner of the OCTA image is excluded from the OCTA-AV map as well as the AVA map presented in Fig. <xref rid="Fig1" ref-type="fig">1g and h</xref>, respectively.</p>
      <p id="Par15">Using the AVA maps and OCTA-AV maps, we can conduct the quantitative analysis for control, NoDR, and mild DR stages. The area of arterial or venous areas can be quantified using the AVA maps. As two novel quantitative features, the percentage of the arterial or venous areas in the total area can be defined as arterial area (AA) or venous area (VA). Therefore, AA and VA can be calculated as follows<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\alpha }_{A}=100\times \frac{{A}_{A}}{{A}_{T}}$$\end{document}</tex-math><mml:math id="M2"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="43856_2023_287_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\alpha }_{V}=100\times \frac{{A}_{V}}{{A}_{T}}$$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="43856_2023_287_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <italic>A</italic><sub><italic>A</italic></sub>, <italic>A</italic><sub><italic>V</italic></sub>, and <italic>A</italic><sub><italic>T</italic></sub> are arterial, venous, and total area in AVA maps, respectively, and <italic>α</italic><sub><italic>A</italic></sub> and <italic>α</italic><sub><italic>V</italic></sub> are AA and VA, respectively. To calculate AA or VA, the number of pixels in the arterial or venous area can be divided by the number of total pixels multiplied by 100. Since the summation of arterial and venous areas are the total area, mathematically we have the following relationship between AA and VA<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\alpha }_{A}=100-{\alpha }_{V}\end{array}$$\end{document}</tex-math><mml:math id="M6"><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="43856_2023_287_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par16">We also can define the arterial-venous area ratio (AVAR), <italic>α</italic><sub><italic>AV</italic></sub>, as follows<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\alpha }_{AV}=\frac{{A}_{A}}{{A}_{V}}=\frac{{\alpha }_{A}}{{\alpha }_{V}}\end{array}$$\end{document}</tex-math><mml:math id="M8"><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="43856_2023_287_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par17">Most commonly, the binarized OCTA images are used to calculate VAD, also known as vessel density (VD), perfusion density (PD), blood vessel density (BVD), and capillary density<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>. VAD in binarized OCTA images is the ratio of the area occupied by vessels divided by the total area converted to a percentage. In this paper, using the OCTA-AV maps that contain the OCTA intensity information, we define perfusion intensity density (PID) as a novel quantitative feature that does not require binarization with any thresholding method. The mean of the pixel intensities converted to a percentage in the total area, arterial area, and venous area can be defined as total PID (T-PID), arterial PID (A-PID), and venous PID (V-PID), respectively. So, T-PID, A-PID, and V-PID can be calculated as follows<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{T}=\frac{100}{255}\times \frac{{{{{{\rm{summation}}}}}}\,{{{{{\rm{of}}}}}}\,{{{{{\rm{intensities}}}}}}\,{{{{{\rm{in}}}}}}\,{{{{{\rm{the}}}}}}\,{{{{{\rm{total}}}}}}\,{{{{{\rm{area}}}}}}}{total\,number\,of\,pixels}=\frac{100}{255}\times \frac{1}{{A}_{T}}\mathop{\sum} _{{A}_{T}}I$$\end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>100</mml:mn></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">summation</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">of</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">intensities</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">in</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">the</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">total</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">area</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.25em"/><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.25em"/><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>100</mml:mn></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mi>I</mml:mi></mml:math><graphic xlink:href="43856_2023_287_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{A}=\frac{100}{255}\times \frac{{{{{{\rm{summation}}}}}}\,{{{{{\rm{of}}}}}}\,{{{{{\rm{intensities}}}}}}\,{{{{{\rm{in}}}}}}\,{{{{{\rm{arterial}}}}}}\,{{{{{\rm{areas}}}}}}}{total\,number\,of\,pixels\,in\,arterial\,areas}=\frac{100}{255}\times \frac{1}{{A}_{A}}\mathop{\sum} _{{A}_{A}}I$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>100</mml:mn></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">summation</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">of</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">intensities</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">in</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">arterial</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">areas</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.25em"/><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.25em"/><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.25em"/><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.25em"/><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>100</mml:mn></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mi>I</mml:mi></mml:math><graphic xlink:href="43856_2023_287_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{V}=\frac{100}{255}\times \frac{{{{{{\rm{summation}}}}}}\,{{{{{\rm{of}}}}}}\,{{{{{\rm{intensities}}}}}}\,{{{{{\rm{in}}}}}}\,{{{{{\rm{venous}}}}}}\,{{{{{\rm{areas}}}}}}}{total\,number\,of\,pixels\,in\,venous\,areas}=\frac{100}{255}\times \frac{1}{{A}_{V}}\mathop{\sum} _{{A}_{V}}I$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>100</mml:mn></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">summation</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">of</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">intensities</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">in</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">venous</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">areas</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.25em"/><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.25em"/><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.25em"/><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.25em"/><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>100</mml:mn></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mi>I</mml:mi></mml:math><graphic xlink:href="43856_2023_287_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>where <italic>P</italic><sub><italic>A</italic></sub>, <italic>P</italic><sub><italic>V</italic></sub>, and <italic>P</italic><sub><italic>T</italic></sub> are A-PID, V-PID, and T-PID, respectively, and <italic>I</italic> is intensity values in OCTA-AV maps. T-PID represents quantitative feature calculation without differentiation of the AV areas, while A-PID and V-PID represent quantitative features after AV area segmentation for differential AV analysis. The arterial-venous PID ratio (AV-PIDR), <italic>P</italic><sub><italic>AV</italic></sub>, can also be defined as the division of the A-PID by V-PID as formulated bellow<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{P}_{AV}=\frac{{P}_{A}}{{P}_{V}}\end{array}$$\end{document}</tex-math><mml:math id="M16"><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="43856_2023_287_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par18">To the best of our knowledge, we defined seven new quantitative OCTA features (AA, VA, AVAR, T-PID, A-PID, V-PID, and AV-PIDR) related to AVA maps and OCTA-AV maps that can be used for quantitative analysis of diseases at different stages. The processes for calculating these quantitative features are shown in Fig. <xref rid="Fig1" ref-type="fig">1i–n</xref>, with blue arrows and boxes. We are going to report these quantitative features in the whole image for control, NoDR, and mild DR subjects to show their importance. These quantitative features can also be measured and reported in other regions of the OCTA images such as parafovea and perifovea, as well as their quadrants, however, that is beyond the scope of this paper.</p>
    </sec>
    <sec id="Sec6">
      <title>Statistical analyses</title>
      <p id="Par19">For the statistical analysis of quantitative features, due to the limited dataset size, we treated each eye as a single observation for some subjects with images of both eyes. We performed the Shapiro–Wilk test to check if quantitative features are normally distributed. We used χ<sup>2</sup> tests to assess the distribution of sex and hypertension among different groups. Age and quality index distributions were compared using analysis of variance (ANOVA). One-versus-one comparison of quantitative features between different groups was performed by the unpaired two-sided Student’s <italic>t</italic> test. We also applied the Bonferroni correction to compare the difference in mean values of the quantitative features. <italic>P</italic> &lt; <italic>0.05</italic> was considered statistically significant.</p>
    </sec>
    <sec id="Sec7">
      <title>AVA-Net architecture</title>
      <p id="Par20">For fully automated AVA segmentation in en face OCTA images, we propose the AVA-Net, a U-Net-like architecture, illustrated in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. The input of the AVA-Net is the grayscale OCTA image. The OCTA image contains the information of blood flow strength and vessel geometry features. Since there are two classes for segmentation: arterial areas and venous areas, this is a binary segmentation problem. So, the output of AVA-Net is a single-channel grayscale image in which arterial pixels are denoted by 1 and venous pixels are denoted by 0. In this article, they are shown in blue and red colors for better demonstration. The overall design of the AVA-Net is composed of an encoder to extract features from the image and a decoder to construct the AVA map from the encoded features. The encoder includes 5 encoder blocks to reduce the image resolution by down-sampling. As shown in Fig. <xref rid="Fig2" ref-type="fig">2b</xref>, encoder blocks composed of two 3 × 3 convolution operations, 4 dilated convolution operations in parallel with dilation rates from 2 to 5, a concatenation operation, and a max-pooling operation with a pooling size of 2 × 2.<fig id="Fig2"><label>Fig. 2</label><caption><title>AVA-Net architecture.</title><p><bold>a</bold> overview of the blocks in AVA-Net architecture. <bold>b</bold> the individual blocks that comprises AVA-Net. In this figure, Conv, f, s, d, and n<sub>f</sub> stand for convolution operation, number of filters in the convolution, strides of the convolution, dilation rate of the convolution, and number of filters specified for the corresponding block, respectively. OCTA optical coherence tomography angiography, AVA arterial-venous area, CBR Conv - Batch Normalization – ReLU.</p></caption><graphic xlink:href="43856_2023_287_Fig2_HTML" id="d32e1484"/></fig></p>
      <p id="Par21">On the other hand, the decoder is composed of 5 decoder blocks, followed by 2 CBR (Conv - Batch Normalization - ReLU) blocks, and final convolutional operation with a sigmoid activation function. As shown in Fig. <xref rid="Fig2" ref-type="fig">2b</xref>, CBR block is composed of a 3 × 3 convolution operation, followed by a batch normalization and a ReLU activation function. The decoder blocks are composed of 2 CBR blocks, up-sampling, and concatenation operation that concatenate generated features with features coming from encoders blocks by skip connections. In all the five levels, the features prior to the max-pooling operation in the encoder blocks is transferred to the decoder blocks by skip connections. These feature maps are then concatenated with the output of the up-sampling operation in the decoder block, and the concatenated feature map is propagated to the successive layers. These skip connections allow the network to retrieve the information lost by max-pooling operations. Details of the different operations in the AVA-Net layers are presented in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.</p>
    </sec>
    <sec id="Sec8">
      <title>Loss function</title>
      <p id="Par22">In this study, the CNN was trained using IoU loss<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> or Jaccard loss to directly optimize the IoU score, the most commonly used evaluation metric in segmentation<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. For multiclass segmentation, it is defined by<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{L}_{IoU}=1-\frac{{\sum }_{c=1}^{C}{\sum }_{i=1}^{N}{g}_{i}^{c}{s}_{i}^{c}}{{\sum }_{c=1}^{C}{\sum }_{i=1}^{N}({g}_{i}^{c}+{s}_{i}^{c}-{g}_{i}^{c}{s}_{i}^{c})}\end{array}$$\end{document}</tex-math><mml:math id="M18"><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="43856_2023_287_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq1"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${g}_{i}^{c}$$\end{document}</tex-math><mml:math id="M20"><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="43856_2023_287_Article_IEq1.gif"/></alternatives></inline-formula> is the ground truth binary indicator of class label <italic>c</italic> of voxel <italic>i</italic>, and <inline-formula id="IEq2"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{i}^{c}$$\end{document}</tex-math><mml:math id="M22"><mml:msubsup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="43856_2023_287_Article_IEq2.gif"/></alternatives></inline-formula> is the corresponding predicted segmentation probability. <italic>N</italic> is the number of voxels in the image and <italic>C</italic> is the number of classes. Since we have two classes, this is a binary segmentation problem. So, we have<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{L}_{IoU}=1-\frac{{\sum }_{i=1}^{N}{g}_{i}{s}_{i}}{{\sum }_{i=1}^{N}({g}_{i}+{s}_{i}-{g}_{i}{s}_{i})}\end{array}$$\end{document}</tex-math><mml:math id="M24"><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="43856_2023_287_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula></p>
    </sec>
    <sec id="Sec9">
      <title>Training process</title>
      <p id="Par23">AVA-Net architecture was trained using the Adam optimizer with a learning rate of 0.0001 (<italic>β</italic><sub>1</sub> = 0.9, <inline-formula id="IEq3"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\beta }_{2}=0.999$$\end{document}</tex-math><mml:math id="M26"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.999</mml:mn></mml:math><inline-graphic xlink:href="43856_2023_287_Article_IEq3.gif"/></alternatives></inline-formula>, <inline-formula id="IEq4"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\epsilon }={10}^{-7}$$\end{document}</tex-math><mml:math id="M28"><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="43856_2023_287_Article_IEq4.gif"/></alternatives></inline-formula>) to have a smooth learning curve for the validation dataset. With the IoU loss function, mini-batch sizes of 28 were utilized for the training. The training process takes up to 5000 epochs when the model performance stops improving on the validation dataset. One epoch is defined as the iteration over 3 training batches. To avoid overfitting, data augmentation methods are applied on the fly during training, including random flipping along horizontal and vertical axes, random zooming, random rotation, random image shifting, random shearing, random brightness shifting. As retinal vessels in OCTA images have diverse tree-like patterns and differing vessel diameters, and because images can be taken from different locations of right and left eyes with diverse quality, the above-mentioned data augmentation methods could help improve the generalization capability of the CNN for segmenting unseen images. Since our data is limited, the 5-fold cross-validation procedure is used for CNN evaluation after shuffling the data based on the patient’s eye. Therefore, in each fold, the network was trained with 80 percent of the images, and evaluation was performed on the other 20 percent of the images.</p>
      <p id="Par24">The IoU and Dice coefficient metrics, also known as the Jaccard Index and F1 Score, respectively, are mostly used in image segmentation. Therefore, in addition to segmentation accuracy, IoU and Dice coefficient were used as the evaluation metrics for evaluating the AVA segmentation, by comparing the predicted AVA maps with manually labeled ground truths.</p>
      <p id="Par25">The training was performed on a Windows 10 computer using NVIDIA Quadro RTX 6000 Graphics Processing Units (GPU). The CNN was trained and evaluated on Python (v3.9.6) using Keras (2.6.0) with Tensorflow (v2.6.0) backend. Training every fold of AVA-Net took about 10 h with the mentioned resources.</p>
    </sec>
    <sec id="Sec10">
      <title>Reporting summary</title>
      <p id="Par26">Further information on research design is available in the <xref rid="MOESM4" ref-type="media">Nature Portfolio Reporting Summary</xref> linked to this article.</p>
    </sec>
  </sec>
  <sec id="Sec11" sec-type="results">
    <title>Results</title>
    <sec id="Sec12">
      <title>AVA-Net performance</title>
      <p id="Par27">We performed 5-fold cross-validation on the training dataset, using 80 percent of the images for training and 20 percent of them for validating AVA-Net. Table <xref rid="Tab2" ref-type="table">2</xref> presents the average and standard deviation of the IoU and Dice, as well as accuracy for AVA-Net. In Fig. <xref rid="Fig3" ref-type="fig">3</xref>, the visual results of the AVA segmentation achieved by AVA-Net for six (three control, one mild, one moderate, and one severe samples) validation samples in the training dataset are illustrated. Figure <xref rid="Fig3" ref-type="fig">3</xref> presents OCTA images, ground truth of AVA maps, predicted AVA maps, the ground truth of OCTA-AV maps (GT-OCTA-AV maps), and predicted OCTA-AV maps in the rows from top to bottom, respectively. It can be concluded that the predicted AVA maps for healthy and NPDR subjects are highly consistent with the ground truth. That means AVA-Net is able to detect and classify arteries and veins, and their corresponding areas. However, there are some incorrect segmentations that are shown by yellow rectangles in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. The visual performance of AVA-Net for segmenting representative OCTA images in the test dataset is presented in Fig. <xref rid="Fig4" ref-type="fig">4</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Performance results of AVA-Net.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="3">Mean (±std dev)</th></tr></thead><tbody><tr><td>IoU (%)</td><td>Dice (%)</td><td>Accuracy (%)</td></tr><tr><td>78.02 (±0.54)</td><td>87.65 (±0.34)</td><td>86.33 (±0.20)</td></tr></tbody></table></table-wrap><fig id="Fig3"><label>Fig. 3</label><caption><title>Comparative illustration of the AVA segmentation performance achieved by AVA-Net trained on the training dataset.</title><p>Each column shows a different sample. Row 1 OCTA images. Row 2 Ground truth AVA maps. Row 3 Predicted AVA maps by AVA-Net. Row 4 GT-OCTA-AV maps generated by multiplying the OCTA images by ground truth AVA maps. Row 5 predicted OCTA-AV maps generated by multiplying the OCTA images by predicted AVA maps. Yellow rectangles indicate some areas that are segmented incorrectly. DR diabetic retinopathy, OCTA optical coherence tomography angiography, AVA arterial-venous area, AV artery-vein, GT ground truth; Scale bar is 1 mm.</p></caption><graphic xlink:href="43856_2023_287_Fig3_HTML" id="d32e1959"/></fig><fig id="Fig4"><label>Fig. 4</label><caption><title>The performance of the AVA-Net on representative samples in the test dataset.</title><p>Each column shows a different sample. Row 1 OCTA images. Row 2 Predicted AVA maps by AVA-Net. Row 3 predicted OCTA-AV maps. DR diabetic retinopathy, NoDR diabetic patients without DR, OCTA optical coherence tomography angiography, AVA arterial-venous area; Scale bar is 1 mm.</p></caption><graphic xlink:href="43856_2023_287_Fig4_HTML" id="d32e1970"/></fig></p>
    </sec>
    <sec id="Sec13">
      <title>Quantitative analysis</title>
      <p id="Par28">Studying the test dataset with demographic details presented in Table <xref rid="Tab1" ref-type="table">1</xref>, no statistically significant differences were found between the different groups with regards to age, quality index, and sex (ANOVA, <italic>p</italic> = 0.077, ANOVA, <italic>p</italic> = 0.079, and χ<sup>2</sup> test, <italic>p</italic> = 0.305). Furthermore, there was no difference in hypertension across diabetes groups (χ<sup>2</sup> test, <italic>p</italic> = 0.92). Between the diabetic groups, there was a significant difference in the duration of diabetes (Student’s <italic>t</italic> test, <italic>p</italic> = 0.019). The diabetic patients in this study were not insulin dependent.</p>
      <p id="Par29">The AA, VA, AVAR, T-PID, A-PID, V-PID, and AV-PIDR in the whole image for the test dataset are calculated as described in section 2.3 using the AVA maps predicted by the AVA-Net and OCTA-AV maps produced after that. Shapiro–Wilk test indicated that all features were normally distributed. Thus, we performed individual pairwise comparisons using an unpaired two-sided Student’s <italic>t</italic> test. Significant differences between groups corresponding to <italic>P</italic> &lt; <italic>0.05</italic>, <italic>P</italic> &lt; <italic>0.01</italic>, and <italic>P</italic> &lt; <italic>0.001</italic> are denoted by *, **, and ***, respectively. To reduce the occurrence of false positives in multiple hypothesis testing, we applied a Bonferroni correction as a conservative method for probability thresholding. Applying the Bonferroni correction, the statistical significance of the P value between the three groups was adjusted as <italic>P</italic> &lt; 0.0167, and significant differences are denoted by the † symbol. Figure <xref rid="Fig5" ref-type="fig">5</xref> presents the boxplots of the AA, VA, and AVAR for control, NoDR, and mild DR subjects in the whole image. Based on Eq. (<xref rid="Equ3" ref-type="disp-formula">3</xref>), the summation of AA and VA is 100. Thus, an increase in AA means a decrease in VA with the same value. Thus, the <italic>P</italic> values related to AA and VA features are identical. We observed a 3.6% increase in whole image AA for the mild DR group compared to the control group. While VA decreased by 4.0% for the mild DR group compared to the control group. The AVAR analysis in Fig. <xref rid="Fig5" ref-type="fig">5b</xref> further confirms the observation with an increase of 7.6% for the mild DR group. The Bonferroni correction indicates that none of these above-mentioned changes are significant.<fig id="Fig5"><label>Fig. 5</label><caption><title>Boxplot of area related quantitative features for control, NoDR, and mild DR groups.</title><p><bold>a</bold> boxplot of AA and VA. <bold>b</bold> boxplot of AVAR. *Significant at <italic>P</italic> &lt; 0.05. Each box indicates the interquartile range (top: the third quartile; bottom: the first quartile) with the whiskers extending to the most extreme points and with a horizontal line and cross mark indicating the median and mean, respectively. The number of samples used for the analysis is <italic>N</italic><sub>Control</sub> = 25, <italic>N</italic><sub>NoDR</sub> = 18 and <italic>N</italic><sub>mild</sub> = 21. AV artery-vein, AA arterial area, VA venous area, DR diabetic retinopathy, NoDR diabetic patients without DR.</p></caption><graphic xlink:href="43856_2023_287_Fig5_HTML" id="d32e2078"/></fig></p>
      <p id="Par30">The boxplots of the T-PID, A-PID, V-PID, and AV-PIDR for control, NoDR, and mild DR groups in the whole image are presented in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. T-PID and A-PID respectively showed 6.1 and 7.2% decreases from control to mild DR groups but no significant differences between control and NoDR eyes. We observed a 7.4% decrease in V-PID for mild DR groups compared to the NoDR group. Differential AV PID analysis reveals that the A-PID decreases, but V-PID increases in NoDR subjects compared to control subjects. Because of the opposite AV changes, the relative AV-PIDR shown in Fig. <xref rid="Fig6" ref-type="fig">6b</xref> provides a sensitive metric to differentiate control, NoDR, and mild DR groups from each other. We observed 5.1 and 2.6% decreases in the whole image AV-PIDR, respectively, for NoDR and mild DR groups compared to the control group, and a 2.6% increase from NoDR to mild DR groups. Based on the Bonferroni correction, changes in A-PID from control to mild DR and changes in AV-PIDR from control to mild DR and from NoDR to mild DR are significant. According to Bonferroni correction, A-PID and AV-PIDR together can show significant changes in all three pairs.<fig id="Fig6"><label>Fig. 6</label><caption><title>Boxplot of perfusion intensity related quantitative features for control, NoDR, and mild DR groups.</title><p><bold>a</bold> Boxplot of T-PID, A-PID, and V-PID. <bold>b</bold> Boxplot of AV-PIDR. *Significant at <italic>P</italic> &lt; 0.05. **Significant at <italic>P</italic> &lt; 0.01. ***Significant at <italic>P</italic> &lt; 0.001. † Bonferroni correction significant at <italic>P</italic> &lt; 0.0167. Each box indicates the interquartile range (top: the third quartile; bottom: the first quartile) with the whiskers extending to the most extreme points and with a horizontal line and cross mark indicating the median and mean, respectively. The number of samples used for the analysis is <italic>N</italic><sub>Control</sub> = 25, <italic>N</italic><sub>NoDR</sub> = 18 and <italic>N</italic><sub>mild</sub>  =  21. PID perfusion intensity density, T-PID total PID, A-PID arterial PID, V-PID venous PID, AV-PID arterial-venous PID, DR diabetic retinopathy, NoDR diabetic patients without DR.</p></caption><graphic xlink:href="43856_2023_287_Fig6_HTML" id="d32e2130"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec14" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par31">Differential AV analysis has been demonstrated to be important for evaluating diabetes, hypertension, strokes, cardiovascular disease, and common retinopathies<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. The addition of AV analysis capabilities to clinical imaging devices would enhance the accuracy of disease detection and classification. The segmentation of AV has been conducted using a variety of imaging modalities, including fundus photography, OCT, and OCTA. Traditional OCT and fundus images are limited in their ability to detect microvascular abnormalities at capillary level. As a new OCT modality, OCTA provides a noninvasive method of detecting microvascular distortions associated with eye diseases with capillary level resolution. Using feature extraction-based methods and machine learning based approaches, all previous studies exploring the AV classification or segmentation have been primarily focused on the detection and segmentation of large vessels as arteries or veins.</p>
    <p id="Par32">In this study, we employed a deep learning network AVA-Net for AVA segmentation in OCTA. By multiplying the OCTA image by the AVA map generated by AVA-Net, we have the OCTA-AV map that contains highly detailed vasculature maps of the OCTA image, with separate red and blue channels for arterial and venous areas. In other words, using this method, we can segment all the vessels in the OCTA image up to the capillary level as arterial and venous. By using the OCTA-AV map, all OCTA-related features previously reported in the literature such as VAD, vessel length density (VLD), vessel diameter index (VDI), vessel tortuosity (VT), and branchpoint density (BD) can be calculated separately for arterial and venous areas to check the effectiveness of the AV analysis.</p>
    <p id="Par33">For fully automated AVA segmentation using OCTA images, we have developed the AVA-Net, a U-Net-like architecture. U-Net-like architectures are commonly used for biomedical image segmentation because they produce reliable and highly accurate results with small datasets. In AVA-Net, we employed encoder blocks containing dilated convolutional operations connected to decoder blocks. We used accuracy, Dice score, and IoU metrics to assess the AVA-Net performance. The results of the cross-validation study revealed the AVA-Net performed well in AVA segmentation by achieving an accuracy of 86.33% and a mean IOU of 78.02%, and a mean Dice score of 87.65% on the validation dataset. Qualitatively AVA-Net has a good AVA segmentation performance on the validation and test dataset. However, there are some areas of incorrect segmentation as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref> by yellow rectangles. AVA-Net performance can be improved by using larger datasets collected from patients with different diseases using different OCTA devices in different fields of view.</p>
    <p id="Par34">The AVA maps generated by AVA-Net are used to define three quantitative features, AA, VA, and AVAR, which are mathematically related. These quantitative features were calculated for the healthy control, NoDR, and mild DR groups. Our results indicate that quantitative features are significantly different between the control and mild DR groups. The mean AA value of healthy eyes in the whole image is 52.6%. This is significantly increased in mild DR eyes at 3.6%. The mean AVAR value for healthy eyes increased from 1.12 to 1.20 in mild DR eyes.</p>
    <p id="Par35">For AV analysis of the vascular perfusion density in OCTA-AV maps, four different quantitative features are also derived, named T-PID, A-PID, V-PID, and AV-PIDR, which do not require any thresholding method for binarizing the OCTA-AV maps. According to Fig. <xref rid="Fig6" ref-type="fig">6</xref>, significant decreases in T-PID, A-PID, and AV-PIDR were observed in mild DR eyes when compared with healthy eyes. Compared to NoDR eyes, V-PID showed a significant decrease in mild DR eyes. Compared to healthy eyes, NoDR eyes showed no differences in T-PID as a quantitative feature without differentiation of the AV areas, which is associated with total vessels, but showed decreases in A-PID and increases in V-PID. As a result of these opposite AV changes, the AV-PIDR, which is the ratio of the A-PID to the V-PID, is a sensitive quantitative feature to distinguish healthy, NoDR, and mild DR eyes from each other. The effectiveness of differential AV analysis can be seen here. Our results show that AV-PIDR of the NoDR and mild DR groups decreased by 5.1 and 2.6%, respectively, compared to the control group with a mean AV-PIDR value of 0.944. We also observed a 2.6% increase from NoDR to mild DR groups in the whole image AV-PIDR. Bonferroni correction indicates significant changes in A-PID from control to mild DR and in AV-PIDR from control to mild DR and from NoDR to mild DR. The combination of A-PID and AV-PIDR can provide supplementary information to each other and demonstrate significant changes in all three pairs according to Bonferroni correction. Therefore, we anticipate that ensemble analysis of AVA and PID features will allow robust detection and classification of DR and other eye diseases. A limitation of this study is the limited dataset size, therefore for some subjects both eyes were included in the statistical analysis of quantitative features. There may be some correlation between right and left eyes due to genetics and environmental factors.</p>
  </sec>
  <sec id="Sec15" sec-type="conclusion">
    <title>Conclusions</title>
    <p id="Par36">A deep learning network AVA-Net has been developed for robust AVA segmentation in OCTA images. The OCTA-AV map, which preserves perfusion intensity information for improved AV analysis, could be readily generated by multiplying the OCTA images by the AVA maps. Three area features, i.e., AA, VA, and AVAR were derived from the AVA maps, while four PID features, i.e., T-PID, A-PID, V-PID, and AV-PIDR were derived from the OCTA-AV maps. The three area features can reveal significant differences between the control and mild DR but cannot separate NoDR from mild DR and control groups. The PID features, T-PID and A-PID can differentiate mild DR from control but cannot separate NoDR from control and mild DR groups. V-PID can differentiate mild DR from NoDR but cannot separate control from NoDR and mild DR groups. In contrast, the AV-PIDR can disclose significant differences among all three groups, i.e., control, NoDR, and mild DR. According to Bonferroni correction, the combination of A-PID and AV-PIDR can demonstrate significant differences among all three groups.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec16">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="43856_2023_287_MOESM1_ESM.pdf">
            <caption>
              <p>Description of Additional Supplementary Files</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="43856_2023_287_MOESM2_ESM.xlsx">
            <caption>
              <p>Supplementary Data 1</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="43856_2023_287_MOESM3_ESM.pdf">
            <caption>
              <p>Peer Review File</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM4">
          <media xlink:href="43856_2023_287_MOESM4_ESM.pdf">
            <caption>
              <p>Reporting Summary</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p>The online version contains supplementary material available at 10.1038/s43856-023-00287-9.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>This project was funded by National Eye Institute (P30 EY001792, R01 EY023522, R01 EY030101, R01EY029673, R01EY030842), Research to Prevent Blindness, and Richard and Loan Hill Endowment.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>M.A. contributed to data preparation, network design, model implementation, data processing, statistical analysis and manuscript preparation. D.L. contributed to data preparation, data processing, statistical analysis and manuscript modification. B.E. contributed to data preparation, and network design. A.K.D. contributed to data preparation, data processing, and statistical analysis. J.I.L. contributed to data preparation, ground truth approval and manuscript modification. X.Y. supervised the project and contributed to study design and manuscript preparation. All authors reviewed and approved the manuscript.</p>
  </notes>
  <notes notes-type="peer-review">
    <title>Peer review</title>
    <sec id="FPar1">
      <title>Peer review information</title>
      <p id="Par37"><italic>Communications Medicine</italic> thanks the anonymous reviewers for their contribution to the peer review of this work. <xref rid="MOESM3" ref-type="media">Peer reviewer reports</xref> are available.</p>
    </sec>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>The source data, i.e., the numerical results underlying the graphs and charts presented in the main figures, is provided as Supplementary Data <xref rid="MOESM2" ref-type="media">1</xref>. Training and test dataset images may be obtained from the corresponding author upon reasonable request.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>The deep learning architectures, AVA-Net, are publicly available on GitHub<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>.</p>
  </notes>
  <notes id="FPar2" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par38">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dashtbozorg</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Mendonça</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Campilho</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>An automatic graph-based approach for artery/vein classification in retinal images</article-title>
        <source>IEEE Trans. Image Process.</source>
        <year>2013</year>
        <volume>23</volume>
        <fpage>1073</fpage>
        <lpage>1083</lpage>
        <pub-id pub-id-type="doi">10.1109/TIP.2013.2263809</pub-id>
        <?supplied-pmid 23693131?>
        <pub-id pub-id-type="pmid">23693131</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alam</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Toslak</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lim</surname>
            <given-names>JI</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Color fundus image guided artery-vein differentiation in optical coherence tomography angiography</article-title>
        <source>Investig. Ophthalmol. Vis. Sci.</source>
        <year>2018</year>
        <volume>59</volume>
        <fpage>4953</fpage>
        <lpage>4962</lpage>
        <pub-id pub-id-type="doi">10.1167/iovs.18-24831</pub-id>
        <pub-id pub-id-type="pmid">30326063</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alam</surname>
            <given-names>MN</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Differential artery-vein analysis in quantitative retinal imaging: a review</article-title>
        <source>Quant. Imaging Med. Surg.</source>
        <year>2021</year>
        <volume>11</volume>
        <fpage>1102</fpage>
        <pub-id pub-id-type="doi">10.21037/qims-20-557</pub-id>
        <?supplied-pmid 33654680?>
        <pub-id pub-id-type="pmid">33654680</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Joshi</surname>
            <given-names>VS</given-names>
          </name>
          <name>
            <surname>Reinhardt</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Garvin</surname>
            <given-names>MK</given-names>
          </name>
          <name>
            <surname>Abramoff</surname>
            <given-names>MD</given-names>
          </name>
        </person-group>
        <article-title>Automated method for identification and artery-venous classification of vessel trees in retinal vessel networks</article-title>
        <source>PLoS one</source>
        <year>2014</year>
        <volume>9</volume>
        <fpage>e88061</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0088061</pub-id>
        <?supplied-pmid 24533066?>
        <pub-id pub-id-type="pmid">24533066</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mirsharif</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Tajeripour</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Pourreza</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Automated characterization of blood vessels as arteries and veins in retinal images</article-title>
        <source>Computerized Med. Imaging Graph.</source>
        <year>2013</year>
        <volume>37</volume>
        <fpage>607</fpage>
        <lpage>617</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compmedimag.2013.06.003</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Miri</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Amini</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Rabbani</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Kafieh</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>A comprehensive study of retinal vessel classification methods in fundus images</article-title>
        <source>J. Med. signals Sens.</source>
        <year>2017</year>
        <volume>7</volume>
        <fpage>59</fpage>
        <pub-id pub-id-type="doi">10.4103/2228-7477.205505</pub-id>
        <?supplied-pmid 28553578?>
        <pub-id pub-id-type="pmid">28553578</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Vijayakumar, V. et al. In <italic>38th annual international conference of the IEEE engineering in medicine and biology society (EMBC)</italic>. 1320–1323 (2016).</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Choi</surname>
            <given-names>JY</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Multi-categorical deep learning neural network to classify retinal images: A pilot study employing small database</article-title>
        <source>PLoS One</source>
        <year>2017</year>
        <volume>12</volume>
        <fpage>e0187336</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0187336</pub-id>
        <?supplied-pmid 29095872?>
        <pub-id pub-id-type="pmid">29095872</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Welikala</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automated arteriole and venule classification using deep learning for retinal images from the UK Biobank cohort</article-title>
        <source>Computers Biol. Med.</source>
        <year>2017</year>
        <volume>90</volume>
        <fpage>23</fpage>
        <lpage>32</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiomed.2017.09.005</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Le</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Transfer learning for automated OCTA detection of diabetic retinopathy</article-title>
        <source>Transl. Vis. Sci. Technol.</source>
        <year>2020</year>
        <volume>9</volume>
        <fpage>35</fpage>
        <lpage>35</lpage>
        <pub-id pub-id-type="doi">10.1167/tvst.9.2.35</pub-id>
        <?supplied-pmid 32855839?>
        <pub-id pub-id-type="pmid">32855839</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rossi</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Portable widefield fundus camera with high dynamic range imaging capability</article-title>
        <source>Biomed. Opt. Express</source>
        <year>2023</year>
        <volume>14</volume>
        <fpage>906</fpage>
        <lpage>917</lpage>
        <pub-id pub-id-type="doi">10.1364/BOE.481096</pub-id>
        <?supplied-pmid 36874492?>
        <pub-id pub-id-type="pmid">36874492</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ahmed</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>ADC-net: an open-source deep learning network for automated dispersion compensation in optical coherence tomography</article-title>
        <source>Front. Med.</source>
        <year>2022</year>
        <volume>9</volume>
        <fpage>864879</fpage>
        <pub-id pub-id-type="doi">10.3389/fmed.2022.864879</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yao</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Alam</surname>
            <given-names>MN</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Toslak</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Quantitative optical coherence tomography angiography: a review</article-title>
        <source>Exp. Biol. Med.</source>
        <year>2020</year>
        <volume>245</volume>
        <fpage>301</fpage>
        <lpage>312</lpage>
        <pub-id pub-id-type="doi">10.1177/1535370219899893</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Le</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Son</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Lim</surname>
            <given-names>JI</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Quantitative optical coherence tomography reveals rod photoreceptor degeneration in early diabetic retinopathy</article-title>
        <source>Retina</source>
        <year>2022</year>
        <volume>42</volume>
        <fpage>1442</fpage>
        <lpage>1449</lpage>
        <pub-id pub-id-type="doi">10.1097/IAE.0000000000003473</pub-id>
        <?supplied-pmid 35316256?>
        <pub-id pub-id-type="pmid">35316256</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chu</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Quantitative assessment of the retinal microvasculature using optical coherence tomography angiography</article-title>
        <source>J. Biomed. Opt.</source>
        <year>2016</year>
        <volume>21</volume>
        <fpage>066008</fpage>
        <pub-id pub-id-type="doi">10.1117/1.JBO.21.6.066008</pub-id>
        <?supplied-pmid 27286188?>
        <pub-id pub-id-type="pmid">27286188</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>AY</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Quantifying microvascular density and morphology in diabetic retinopathy using spectral-domain optical coherence tomography angiography</article-title>
        <source>Investig. Ophthalmol. Vis. Sci.</source>
        <year>2016</year>
        <volume>57</volume>
        <fpage>OCT362</fpage>
        <lpage>OCT370</lpage>
        <pub-id pub-id-type="doi">10.1167/iovs.15-18904</pub-id>
        <pub-id pub-id-type="pmid">27409494</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alam</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Toslak</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lim</surname>
            <given-names>JI</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>OCT feature analysis guided artery-vein differentiation in OCTA</article-title>
        <source>Biomed. Opt. Express</source>
        <year>2019</year>
        <volume>10</volume>
        <fpage>2055</fpage>
        <lpage>2066</lpage>
        <pub-id pub-id-type="doi">10.1364/BOE.10.002055</pub-id>
        <?supplied-pmid 31061771?>
        <pub-id pub-id-type="pmid">31061771</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Son</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Near infrared oximetry-guided artery–vein classification in optical coherence tomography angiography</article-title>
        <source>Exp. Biol. Med.</source>
        <year>2019</year>
        <volume>244</volume>
        <fpage>813</fpage>
        <lpage>818</lpage>
        <pub-id pub-id-type="doi">10.1177/1535370219850791</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gaier</surname>
            <given-names>ED</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Quantitative analysis of optical coherence tomographic angiography (OCT-A) in patients with non-arteritic anterior ischemic optic neuropathy (NAION) corresponds to visual function</article-title>
        <source>PLoS One</source>
        <year>2018</year>
        <volume>13</volume>
        <fpage>e0199793</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0199793</pub-id>
        <?supplied-pmid 29953490?>
        <pub-id pub-id-type="pmid">29953490</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Differentiating veins from arteries on optical coherence tomography angiography by identifying deep capillary plexus vortices</article-title>
        <source>Am. J. Ophthalmol.</source>
        <year>2019</year>
        <volume>207</volume>
        <fpage>363</fpage>
        <lpage>372</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ajo.2019.06.009</pub-id>
        <?supplied-pmid 31226248?>
        <pub-id pub-id-type="pmid">31226248</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>T-H</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Son</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Vascular morphology and blood flow signatures for differential artery-vein analysis in optical coherence tomography of the retina</article-title>
        <source>Biomed. Opt. Express</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>367</fpage>
        <lpage>379</lpage>
        <pub-id pub-id-type="doi">10.1364/BOE.413149</pub-id>
        <?supplied-pmid 33520388?>
        <pub-id pub-id-type="pmid">33520388</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Adejumo</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Depth-resolved vascular profile features for artery-vein classification in OCT and OCT angiography of human retina</article-title>
        <source>Biomed. Opt. Express</source>
        <year>2022</year>
        <volume>13</volume>
        <fpage>1121</fpage>
        <lpage>1130</lpage>
        <pub-id pub-id-type="doi">10.1364/BOE.450913</pub-id>
        <?supplied-pmid 35284164?>
        <pub-id pub-id-type="pmid">35284164</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alam</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Son</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Lim</surname>
            <given-names>JI</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>AV-Net: deep learning for fully automated artery-vein classification in optical coherence tomography angiography</article-title>
        <source>Biomed. Opt. express</source>
        <year>2020</year>
        <volume>11</volume>
        <fpage>5249</fpage>
        <lpage>5257</lpage>
        <pub-id pub-id-type="doi">10.1364/BOE.399514</pub-id>
        <?supplied-pmid 33014612?>
        <pub-id pub-id-type="pmid">33014612</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gao</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A deep learning network for classifying arteries and veins in montaged widefield OCT angiograms</article-title>
        <source>Ophthalmol. Sci.</source>
        <year>2022</year>
        <volume>2</volume>
        <fpage>100149</fpage>
        <pub-id pub-id-type="doi">10.1016/j.xops.2022.100149</pub-id>
        <?supplied-pmid 36278031?>
        <pub-id pub-id-type="pmid">36278031</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Abtahi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lim</surname>
            <given-names>JI</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>MF-AV-Net: an open-source deep learning network with multimodal fusion options for artery-vein segmentation in OCT angiography</article-title>
        <source>Biomed. Opt. Express</source>
        <year>2022</year>
        <volume>13</volume>
        <fpage>4870</fpage>
        <lpage>4888</lpage>
        <pub-id pub-id-type="doi">10.1364/BOE.468483</pub-id>
        <?supplied-pmid 36187235?>
        <pub-id pub-id-type="pmid">36187235</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kushner-Lenhoff</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>OCTA derived vessel skeleton density versus flux and their associations with systemic determinants of health</article-title>
        <source>Invest. Ophthalmol. Vis. Sci.</source>
        <year>2022</year>
        <volume>63</volume>
        <fpage>19</fpage>
        <pub-id pub-id-type="doi">10.1167/iovs.63.2.19</pub-id>
        <?supplied-pmid 35142788?>
        <pub-id pub-id-type="pmid">35142788</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Abdolahi</surname>
            <given-names>F</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Optical coherence tomography angiography-derived flux as a measure of physiological changes in retinal capillary blood flow</article-title>
        <source>Transl. Vis. Sci. Technol.</source>
        <year>2021</year>
        <volume>10</volume>
        <fpage>5</fpage>
        <pub-id pub-id-type="doi">10.1167/tvst.10.9.5</pub-id>
        <?supplied-pmid 34342607?>
        <pub-id pub-id-type="pmid">34342607</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yan</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Vision loss in optic disc drusen correlates with increased macular vessel diameter and flux and reduced peripapillary vascular density</article-title>
        <source>Am. J. Ophthalmol.</source>
        <year>2020</year>
        <volume>218</volume>
        <fpage>214</fpage>
        <lpage>224</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ajo.2020.04.019</pub-id>
        <?supplied-pmid 32360344?>
        <pub-id pub-id-type="pmid">32360344</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sampson</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Dubis</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>FK</given-names>
          </name>
          <name>
            <surname>Zawadzki</surname>
            <given-names>RJ</given-names>
          </name>
          <name>
            <surname>Sampson</surname>
            <given-names>DD</given-names>
          </name>
        </person-group>
        <article-title>Towards standardizing retinal optical coherence tomography angiography: a review</article-title>
        <source>Light: Sci. Appl.</source>
        <year>2022</year>
        <volume>11</volume>
        <fpage>1</fpage>
        <lpage>22</lpage>
        <pub-id pub-id-type="doi">10.1038/s41377-022-00740-9</pub-id>
        <pub-id pub-id-type="pmid">34974515</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Rahman, M. A. &amp; Wang, Y. In <italic>International symposium on visual computing</italic>. 234–244 (2016).</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ma</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Loss odyssey in medical image segmentation</article-title>
        <source>Med. Image Anal.</source>
        <year>2021</year>
        <volume>71</volume>
        <fpage>102035</fpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2021.102035</pub-id>
        <?supplied-pmid 33813286?>
        <pub-id pub-id-type="pmid">33813286</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Abtahi, M. et al. AVA-Net in Python using Keras. <italic>GitHub repository</italic><ext-link ext-link-type="uri" xlink:href="https://github.com/mansour2002/AVA-Net">https://github.com/mansour2002/AVA-Net</ext-link> (2022).</mixed-citation>
    </ref>
  </ref-list>
</back>
