<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr IEEE Trans Med Imaging?>
<?submitter-system nihms?>
<?submitter-canonical-name IEEE Publishing Technology?>
<?submitter-canonical-id IEEE?>
<?submitter-userid 8201177?>
<?submitter-authority myNCBI?>
<?submitter-login ieee?>
<?submitter-name IEEE Publishing Technology?>
<?domain nihpa?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">8310780</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">20511</journal-id>
    <journal-id journal-id-type="nlm-ta">IEEE Trans Med Imaging</journal-id>
    <journal-id journal-id-type="iso-abbrev">IEEE Trans Med Imaging</journal-id>
    <journal-title-group>
      <journal-title>IEEE transactions on medical imaging</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0278-0062</issn>
    <issn pub-type="epub">1558-254X</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10315989</article-id>
    <article-id pub-id-type="pmid">36374873</article-id>
    <article-id pub-id-type="doi">10.1109/TMI.2022.3221890</article-id>
    <article-id pub-id-type="manuscript">nihpa1888754</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ICAM-Reg: Interpretable Classification and Regression With Feature Attribution for Mapping Neurological Phenotypes in Individual Scans</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-7928-3394</contrib-id>
        <name>
          <surname>Bass</surname>
          <given-names>Cher</given-names>
        </name>
        <aff id="A1">School of Biomedical Engineering and Imaging Sciences, King’s College London, WC2R 2LS London, U.K., and also with Panakeia Technologies, WC2B 4BG London, U.K.</aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-8326-3468</contrib-id>
        <name>
          <surname>da Silva</surname>
          <given-names>Mariana</given-names>
        </name>
        <aff id="A2">School of Biomedical Engineering and Imaging Sciences, King’s College London, WC2R 2LS London, U.K.</aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-5753-428X</contrib-id>
        <name>
          <surname>Sudre</surname>
          <given-names>Carole</given-names>
        </name>
        <aff id="A3">School of Biomedical Engineering and Imaging Sciences, King’s College London, WC2R 2LS London, U.K., and also with the MRC Unit for Lifelong Health and Ageing, University College London, WC1E 6BT London, U.K.</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Williams</surname>
          <given-names>Logan Z. J.</given-names>
        </name>
        <aff id="A4">School of Biomedical Engineering and Imaging Sciences, King’s College London, WC2R 2LS London, U.K.</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sousa</surname>
          <given-names>Helena S.</given-names>
        </name>
        <aff id="A5">School of Biomedical Engineering and Imaging Sciences, King’s College London, WC2R 2LS London, U.K.</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tudosiu</surname>
          <given-names>Petru-Daniel</given-names>
        </name>
        <aff id="A6">School of Biomedical Engineering and Imaging Sciences, King’s College London, WC2R 2LS London, U.K.</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Alfaro-Almagro</surname>
          <given-names>Fidel</given-names>
        </name>
        <aff id="A7">Department of Clinical Neurology, Oxford Centre for Functional MRI of the Brain (FMRIB), OX3 9DU Oxford, U.K.</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fitzgibbon</surname>
          <given-names>Sean P.</given-names>
        </name>
        <aff id="A8">Wellcome Centre for Integrative Neuroimaging, University of Oxford, OX1 2JD Oxford, U.K.</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Glasser</surname>
          <given-names>Matthew F.</given-names>
        </name>
        <aff id="A9">Departments of Radiology and Neuroscience, Washington University in St. Louis, St. Louis, MO 63130 USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Smith</surname>
          <given-names>Stephen M.</given-names>
        </name>
        <aff id="A10">Department of Clinical Neurology, Oxford Centre for Functional MRI of the Brain (FMRIB), OX3 9DU Oxford, U.K.</aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-7886-3426</contrib-id>
        <name>
          <surname>Robinson</surname>
          <given-names>Emma C.</given-names>
        </name>
        <aff id="A11">School of Biomedical Engineering and Imaging Sciences, King’s College London, WC2R 2LS London, U.K.</aff>
      </contrib>
      <contrib contrib-type="author">
        <collab>Alzheimer’s Disease Neuroimaging Initiative</collab>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="CR1">
        <italic toggle="yes">Corresponding author: Cher Bass.</italic>
        <email>cher.bass@kcl.ac.uk</email>
      </corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>29</day>
      <month>6</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>03</day>
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>03</day>
      <month>7</month>
      <year>2023</year>
    </pub-date>
    <volume>42</volume>
    <issue>4</issue>
    <fpage>959</fpage>
    <lpage>970</lpage>
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link></license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P1">An important goal of medical imaging is to be able to precisely detect patterns of disease specific to individual scans; however, this is challenged in brain imaging by the degree of heterogeneity of shape and appearance. Traditional methods, based on image registration, historically fail to detect variable features of disease, as they utilise population-based analyses, suited primarily to studying group-average effects. In this paper we therefore take advantage of recent developments in generative deep learning to develop a method for simultaneous classification, or regression, and feature attribution (FA). Specifically, we explore the use of a VAE-GAN (variational autoencoder - general adversarial network) for translation called ICAM, to explicitly disentangle class relevant features, from background confounds, for improved interpretability and regression of neurological phenotypes. We validate our method on the tasks of Mini-Mental State Examination (MMSE) cognitive test score prediction for the Alzheimer’s Disease Neuroimaging Initiative (ADNI) cohort, as well as brain age prediction, for both neurodevelopment and neurodegeneration, using the developing Human Connectome Project (dHCP) and UK Biobank datasets. We show that the generated FA maps can be used to explain outlier predictions and demonstrate that the inclusion of a regression module improves the disentanglement of the latent space. Our code is freely available on GitHub <ext-link xlink:href="https://github.com/CherBass/ICAM" ext-link-type="uri">https://github.com/CherBass/ICAM</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>Brain imaging</kwd>
      <kwd>deep generative models</kwd>
      <kwd>feature attribution</kwd>
      <kwd>image-to-image translation</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>I.</label>
    <title>Introduction</title>
    <p id="P2">Brain images represent a significant resource in the development of mechanistic models of behaviour and neurological/psychiatric disease as, in principle, they capture measurable neuroanatomical traits that are heritable, present in unaffected siblings and detectable prior to disease onset [<xref rid="R1" ref-type="bibr">1</xref>]. For many complex disorders, however, these features of disease [<xref rid="R2" ref-type="bibr">2</xref>], [<xref rid="R3" ref-type="bibr">3</xref>] are subtle, variable and obscured by a back-drop of significant natural variation in brain shape and appearance [<xref rid="R4" ref-type="bibr">4</xref>], [<xref rid="R5" ref-type="bibr">5</xref>]; this makes them extremely difficult to detect.</p>
    <p id="P3">Traditional approaches for analysis of brain magnetic resonance imaging (MRI) rely on group-wise comparisons between disease and control groups, whereby they compare all images in a global average space through performing image registration to a template. Voxel-based morphometry (VBM) is one such common method [<xref rid="R6" ref-type="bibr">6</xref>], which has been used in countless studies of development, ageing and dementia [<xref rid="R7" ref-type="bibr">7</xref>], [<xref rid="R8" ref-type="bibr">8</xref>], [<xref rid="R9" ref-type="bibr">9</xref>], [<xref rid="R10" ref-type="bibr">10</xref>], [<xref rid="R11" ref-type="bibr">11</xref>]. Other techniques include traditional machine learning analysis based on comparisons of hand-engineered features, for example metrics derived from cortical regions [<xref rid="R12" ref-type="bibr">12</xref>], [<xref rid="R13" ref-type="bibr">13</xref>], [<xref rid="R14" ref-type="bibr">14</xref>], [<xref rid="R15" ref-type="bibr">15</xref>], [<xref rid="R16" ref-type="bibr">16</xref>], [<xref rid="R17" ref-type="bibr">17</xref>], [<xref rid="R18" ref-type="bibr">18</xref>], or lesion symptom mapping techniques [<xref rid="R19" ref-type="bibr">19</xref>]. More recent methods use Gaussian processes [<xref rid="R20" ref-type="bibr">20</xref>] to detect diseased brain tissue as outliers against a normative model, fit at each voxel. While these methods have significantly improved understanding of population average patterns of disease [<xref rid="R7" ref-type="bibr">7</xref>], they rely on spatial normalisation and therefore lose power at the cortex due to the impact of cortical heterogeneity [<xref rid="R4" ref-type="bibr">4</xref>], [<xref rid="R21" ref-type="bibr">21</xref>]. This also means that they are not tuned to detect features of disease specific to the individual, which are extremely important for diagnosis and prognosis.</p>
    <p id="P4">To address these limitations, recent studies have started to apply deep learning methods to brain imaging datasets. Deep learning is state-of-the-art for many image processing tasks [<xref rid="R22" ref-type="bibr">22</xref>], and has shown strong promise for brain imaging applications such as healthy tissue and lesion segmentation [<xref rid="R23" ref-type="bibr">23</xref>], [<xref rid="R24" ref-type="bibr">24</xref>], [<xref rid="R25" ref-type="bibr">25</xref>], [<xref rid="R26" ref-type="bibr">26</xref>]. Importantly, by design it can work independently of any requirement for spatial normalisation. However, deep learning methods do not, by default, return explanations of the reasoning behind their predictions, leading to them traditionally being referred to as “black box” models.</p>
    <p id="P5">More recently, several approaches have been developed to make these networks more interpretable through identifying class-relevant features for a particular input. These include post-hoc saliency based methods, designed to detect which features of a specific image contribute most strongly to a class prediction. These typically analyse the gradients or activations of the network, with respect to a given input image, and include approaches such as Gradient-weighted Class Activation Mapping (Grad-CAM) [<xref rid="R27" ref-type="bibr">27</xref>], SHAP [<xref rid="R28" ref-type="bibr">28</xref>], DeepTaylor [<xref rid="R29" ref-type="bibr">29</xref>], integrated gradients [<xref rid="R30" ref-type="bibr">30</xref>], guided backpropagation (backprop) [<xref rid="R31" ref-type="bibr">31</xref>], and Layer-wise backpropagation (LRP) [<xref rid="R32" ref-type="bibr">32</xref>]. In addition, perturbation methods such as occlusion [<xref rid="R33" ref-type="bibr">33</xref>] change or remove parts of the input image to generate heatmaps, by evaluating its effect on the prediction.</p>
    <p id="P6">Such methods have now been applied in various medical imaging applications including in MRI and Positron Emission Tomography (PET) imaging datasets for Alzheimer’s (AD) [<xref rid="R34" ref-type="bibr">34</xref>], [<xref rid="R35" ref-type="bibr">35</xref>], [<xref rid="R36" ref-type="bibr">36</xref>], [<xref rid="R37" ref-type="bibr">37</xref>] and Multiple Sclerosis (MS) [<xref rid="R38" ref-type="bibr">38</xref>] classification, and cancer detection through breast density regression [<xref rid="R39" ref-type="bibr">39</xref>]. However, while in principle, these methods can be applied to detect features from individual images, the results are typically low resolution and noisy, which makes them hard to interpret. Often this leads to studies estimating a group average to aggregate results across individuals, and boost signal to noise to make stable population-wide inferences [<xref rid="R36" ref-type="bibr">36</xref>], [<xref rid="R37" ref-type="bibr">37</xref>]. This loses individual specificity, and since these feature attribution (FA) methods often detect similar features in both healthy and disease groups, it is difficult to interpret the results.</p>
    <p id="P7">In addition, since these FA methods are applied to a CNN following training, their power is limited by the constraints of the network they are applied to. Such networks need only focus on the most consistent or discriminative features, sufficient to accurately predict each class. This is a particular issue for medical imaging where diagnosis and treatment rely on comprehensive capture of all features of disease [<xref rid="R34" ref-type="bibr">34</xref>], [<xref rid="R35" ref-type="bibr">35</xref>], [<xref rid="R36" ref-type="bibr">36</xref>], [<xref rid="R37" ref-type="bibr">37</xref>], [<xref rid="R38" ref-type="bibr">38</xref>], [<xref rid="R40" ref-type="bibr">40</xref>]. For example, when applying LRP and guided backprop to brain MRI, it was found that while they were able to detect homogeneous brain structures such as the hippocampus, they were unable to detect heterogeneous structures such as cortical folds [<xref rid="R36" ref-type="bibr">36</xref>], [<xref rid="R37" ref-type="bibr">37</xref>].</p>
    <p id="P8">For these reasons, new approaches have recently been proposed which seek holistic explanations for a phenotype through learning to translate images from one class to another [<xref rid="R40" ref-type="bibr">40</xref>], [<xref rid="R41" ref-type="bibr">41</xref>], [<xref rid="R42" ref-type="bibr">42</xref>], [<xref rid="R43" ref-type="bibr">43</xref>], [<xref rid="R44" ref-type="bibr">44</xref>], [<xref rid="R45" ref-type="bibr">45</xref>]. For example Lenis et al. [<xref rid="R43" ref-type="bibr">43</xref>] identifies salient regions of any input image by identifying the smallest feasible perturbation that would change a predictor’s score. Similarly, Schutte et al. [<xref rid="R44" ref-type="bibr">44</xref>] trains a StyleGAN [<xref rid="R46" ref-type="bibr">46</xref>] to simulate osteoarthritis in knee X-ray images and [<xref rid="R47" ref-type="bibr">47</xref>] modifies a CycleGAN [<xref rid="R45" ref-type="bibr">45</xref>] to generate the minimum pertubation required to change the disease class of retinal images. Most similar to this work is Baumgartner et al. [<xref rid="R40" ref-type="bibr">40</xref>], which uses a visual attribution (VA) GAN to translate images classed as Alzheimer’s (AD) to instead resemble Mild Cognitive Impairment (MCI). However, while this method was able to detect more features of disease relative to post-hoc methods, it was still unable to identify much of the phenotypically variable changes around the cortex [<xref rid="R48" ref-type="bibr">48</xref>].</p>
    <p id="P9">To address these problems in [<xref rid="R48" ref-type="bibr">48</xref>] we developed ICAM (Interpretable Classification via disentangled representations and feature Attribution Mapping); this improved on the state-of-the-art image-to-image translation methods (<xref rid="T2" ref-type="table">Table II</xref>) [<xref rid="R27" ref-type="bibr">27</xref>], [<xref rid="R30" ref-type="bibr">30</xref>], [<xref rid="R31" ref-type="bibr">31</xref>], [<xref rid="R33" ref-type="bibr">33</xref>], [<xref rid="R40" ref-type="bibr">40</xref>] by disentangling class-relevant <italic toggle="yes">attributes</italic> (attr) from class-irrelevant <italic toggle="yes">content</italic> features. Sharp reconstructions were then learnt through use of a Variational Autoencoder (VAE) with a discriminator loss on the decoder (Generative Adversarial Network, GAN). This not only allows classification and generation of an attribution map from the latent space, but also a more interpretable latent space that can visualise differences between and within classes. By sampling the latent space at test time to generate an FA map, we demonstrated its ability to detect meaningful brain variation pertaining to Alzheimer’s disease (<xref rid="F1" ref-type="fig">Fig. 1</xref>).</p>
    <p id="P10">While in the past translation methods have been implemented solely for classification, regression tasks are common in medical imaging, as most diseases lie on a continuous spectrum. The key contributions of this paper are therefore as follows:</p>
    <list list-type="order" id="L1">
      <list-item>
        <p id="P11">We extend ICAM [<xref rid="R48" ref-type="bibr">48</xref>] with an additional regression module to support interpretation of heterogeneous continuous phenotypes.</p>
      </list-item>
      <list-item>
        <p id="P12">Performance is validated across three different tasks: regression of healthy ageing in the UK Biobank, neurodevelopment in the developing Human Connectome Project (dHCP), and MMSE scores from ADNI.</p>
      </list-item>
      <list-item>
        <p id="P13">We demonstrate that adding a regression model improves the interpretability of the attribute latent space, and show that in this way ICAM-reg can provide explanations for subjects predicted as outliers by interpolating between the attribute latent space encoding of two subjects within and between age groups.</p>
      </list-item>
      <list-item>
        <p id="P14">We perform additional experiments to validate translation, using an independent classification network, trained on real images, to verify whether the model plausibly changes the image class.</p>
      </list-item>
    </list>
  </sec>
  <sec id="S2">
    <label>II.</label>
    <title>Related Works</title>
    <p id="P15">Over recent years, several deep generative approaches to image-to-image translation have emerged [<xref rid="R41" ref-type="bibr">41</xref>], [<xref rid="R42" ref-type="bibr">42</xref>], [<xref rid="R45" ref-type="bibr">45</xref>], [<xref rid="R49" ref-type="bibr">49</xref>], [<xref rid="R50" ref-type="bibr">50</xref>], [<xref rid="R51" ref-type="bibr">51</xref>], where these have been applied to many different domains, including medical imaging [<xref rid="R40" ref-type="bibr">40</xref>], [<xref rid="R52" ref-type="bibr">52</xref>], [<xref rid="R53" ref-type="bibr">53</xref>], [<xref rid="R54" ref-type="bibr">54</xref>]. Of these, Lee et al. [<xref rid="R42" ref-type="bibr">42</xref>], in particular, developed a domain translation network called DRIT (<xref rid="F2" ref-type="fig">Fig. 2b</xref>), which constrains translation only to features specific to a class, by encoding separate class-relevant (attribute) and class-irrelevant (content) latent spaces, and employing a discriminator.</p>
    <p id="P16">Separately, Baumgartner et al. [<xref rid="R40" ref-type="bibr">40</xref>] developed a conditional ‘visual attribution’ GAN which translated 3D MRI brain scans, classified with Alzheimer’s disease (AD), towards the appearance of scans with mild cognitive impairment (MCI): an intermediate state between healthy cognition and AD (<xref rid="F2" ref-type="fig">Fig. 2a</xref>). This generates sharp reconstructions and realistic disease maps that overlap with ground truth patterns of longitudinal atrophy. However, the approach requires image class labels to be known <italic toggle="yes">a priori</italic> and, in the absence of a latent space, it can only produce a single deterministic output for each image, which limits the modelling of more heterogeneous features.</p>
    <p id="P17">Accordingly, in our work ICAM [<xref rid="R48" ref-type="bibr">48</xref>], we extended upon the intuitions of these models to create one framework which allows simultaneous classification and feature attribution, using a more interpretable model. Compared to VA-GAN and DRIT++ [<xref rid="R40" ref-type="bibr">40</xref>], [<xref rid="R42" ref-type="bibr">42</xref>], ICAM uses 2 shared disentangled latent spaces, attribute and content, which encode for class-relevant and class-irrelevant information, respectively. The use of a shared attribute (class) latent space allows the addition of a classification layer (and in this work, also a regression layer) to the network (<xref rid="F2" ref-type="fig">Fig. 2c</xref>), which enables the network to do classification and visualisation of differences between and within classes.</p>
    <p id="P18">Other components of ICAM such as a FA map loss, L2 reconstruction loss, and a 3D attribute latent space also improve performance compared to VA-GAN and DRIT++ (as illustrated using ablation studies in [<xref rid="R48" ref-type="bibr">48</xref>]).</p>
  </sec>
  <sec id="S3">
    <label>III.</label>
    <title>Methods</title>
    <p id="P19">The goal of ICAM [<xref rid="R48" ref-type="bibr">48</xref>] is to perform classification with simultaneous feature attribution, by training a VAE-GAN to swap the classes of input images (<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>) by changing only the features which are specific to the target phenotype. In this paper, we extend the method with a regression module (‘pred’ - <xref rid="F3" ref-type="fig">Fig 3</xref>) to support prediction of continuous phenotypes.</p>
    <sec id="S4">
      <label>A.</label>
      <title>Content and Attribute Latent Spaces</title>
      <p id="P20">In ICAM, domain disentanglement is achieved through encoding two separate latent spaces: a <bold>content encoder</bold>
<inline-formula><mml:math id="M1" display="inline"><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></inline-formula> (latent space <inline-formula><mml:math id="M2" display="inline"><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>), whose objective is to encode class-irrelevant (e.g. brain shape) information, and an <bold>attribute encoder</bold>
<inline-formula><mml:math id="M3" display="inline"><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></inline-formula> (latent space <inline-formula><mml:math id="M4" display="inline"><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>), whose objective is to encode all class-relevant features of disease. In both cases, the latent spaces are shared between classes or domains (i.e. <inline-formula><mml:math id="M5" display="inline"><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mspace width="0.25em"/><mml:mo>:</mml:mo><mml:mspace width="0.25em"/><mml:mfenced open="" separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>→</mml:mo><mml:mi>C</mml:mi><mml:mo>}</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>:</mml:mo><mml:mi>y</mml:mi><mml:mo>→</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></inline-formula>). Note, in what follows, we refer to domain or class interchangeably, in which the same meaning is implied.</p>
      <p id="P21">For the <bold>content encoder</bold>
<inline-formula><mml:math id="M6" display="inline"><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></inline-formula>, class information is driven out from the latent space <inline-formula><mml:math id="M7" display="inline"><mml:mo>{</mml:mo><mml:mi>C</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula> through training of a discriminator, <inline-formula><mml:math id="M8" display="inline"><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></inline-formula>, with <bold>class adversarial content loss</bold>:
<disp-formula id="FD1"><label>(1)</label><mml:math id="M9" display="block"><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mspace width="0.15em"/><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mspace linebreak="newline"/><mml:mspace width="2.25em"/><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mspace width="0.15em"/><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula>
The goal of the content encoder <inline-formula><mml:math id="M10" display="inline"><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> is therefore to learn a representation whose domain cannot be distinguished by this discriminator (an approach first proposed by Lee et al., [<xref rid="R42" ref-type="bibr">42</xref>]). Training is also supported through L2 regularisation, to prevent explosion of gradients, and Gaussian noise (added to the last layer of the encoder) to prevent the latent space vanishing. Without this the content space goes to zero since it is the easiest way to make the space class invariant.</p>
      <p id="P22">For the <bold>attribute encoder</bold>
<inline-formula><mml:math id="M11" display="inline"><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></inline-formula>, class information is driven <italic toggle="yes">into</italic> the latent space, by appending a fully connected classification layer <inline-formula><mml:math id="M12" display="inline"><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> with binary cross entropy loss <inline-formula><mml:math id="M13" display="inline"><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msubsup></mml:math></inline-formula>. In extension from our previous work [<xref rid="R48" ref-type="bibr">48</xref>], a <bold>regression module</bold>
<inline-formula><mml:math id="M14" display="inline"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula>) is also added, using another fully connected layer, trained using a smooth L1 loss <inline-formula><mml:math id="M15" display="inline"><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>;</mml:mo><mml:mi mathvariant="italic">smooth</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math></inline-formula>. Importantly, when training regression modules, a complementary binary classification task must be run, in order to support the adversarial training of the generator (<xref rid="S5" ref-type="sec">Sec. III-B</xref>).</p>
      <p id="P23">The training of the attribute latent space is performed using variational inference, through application of a Kullback Leibler (KL) loss <inline-formula><mml:math id="M16" display="inline"><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msubsup></mml:math></inline-formula>. This places a Gaussian prior over the latent variables ensuring that the attribute latent space can be sampled, which allows translation of a single subject at test time, and the generation of mean and variance maps via the use of rejection sampling (see below). During training, the prediction modules <inline-formula><mml:math id="M17" display="inline"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M18" display="inline"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> therefore work to encourage separation of the domains within this latent space <inline-formula><mml:math id="M19" display="inline"><mml:mo>{</mml:mo><mml:mi>A</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula>, to support meaningful image translation. Further, a <bold>latent regression loss</bold> [<xref rid="R42" ref-type="bibr">42</xref>] is implemented through sampling a random attribute latent vector <inline-formula><mml:math id="M20" display="inline"><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math></inline-formula> from a Gaussian distribution, then reconstructing:
<disp-formula id="FD2"><label>(2)</label><mml:math id="M21" display="block"><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mfenced open="∥" close="∥" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>G</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:math></disp-formula>
The purpose of this loss, first proposed in DRIT++ [<xref rid="R42" ref-type="bibr">42</xref>], is to encourage an invertible mapping between the attribute latent space and the generated outputs.</p>
    </sec>
    <sec id="S5">
      <label>B.</label>
      <title>Generation and Feature Attribution</title>
      <p id="P24">Image translation and generation of FA maps is supported through the training a <bold>generator</bold>
<inline-formula><mml:math id="M22" display="inline"><mml:mo>{</mml:mo><mml:mi>G</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula>, which learns to synthesise images conditioned on both the content and attribute latent spaces <inline-formula><mml:math id="M23" display="inline"><mml:mfenced separators="|"><mml:mrow><mml:mi>G</mml:mi><mml:mspace width="0.25em"/><mml:mo>:</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>→</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>ˆ</mml:mi></mml:mover></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mfenced separators="|"><mml:mrow><mml:mi>G</mml:mi><mml:mspace width="0.25em"/><mml:mo>:</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>→</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>ˆ</mml:mi></mml:mover></mml:mrow></mml:mfenced></mml:math></inline-formula>, as well as to translate between these domains. It achieves this by swapping the content latent space: <inline-formula><mml:math id="M24" display="inline"><mml:mfenced separators="|"><mml:mrow><mml:mi>G</mml:mi><mml:mspace width="0.25em"/><mml:mo>:</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>→</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced close="" separators="|"><mml:mrow><mml:mi>G</mml:mi><mml:mspace width="0.25em"/><mml:mo>:</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>→</mml:mo></mml:mrow></mml:mfenced><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, which is made possible since this space is class invariant. Training of the generator is supported by optimisation of a <bold>domain discriminator</bold>
<inline-formula><mml:math id="M25" display="inline"><mml:mo>{</mml:mo><mml:mi>D</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula> with two losses: a) a domain adversarial loss, <inline-formula><mml:math id="M26" display="inline"><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> which seeks realistic image generation by minimising the differences between translated (fake) and real images; and b) a binary cross entropy classification loss, <inline-formula><mml:math id="M27" display="inline"><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, which seeks optimal classification of the two domains following translation. Disentanglement is further encouraged through <bold>rejection sampling</bold> of the attribute latent space during training. This checks the class of each vector randomly sampled from the attribute space (<xref rid="F4" ref-type="fig">Fig. 4</xref>) to ensure that the domain discriminator is passed a simulated image of the opposing class. This is important since the objective of ICAMs adversarial training is to encourage plausible <italic toggle="yes">translation</italic> of the images.</p>
      <p id="P25">To visualise differences between the translated images <inline-formula><mml:math id="M28" display="inline"><mml:mo>{</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula> and the original images <inline-formula><mml:math id="M29" display="inline"><mml:mo>{</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula>, we use a <bold>feature attribution map</bold>
<inline-formula><mml:math id="M30" display="inline"><mml:mo>{</mml:mo><mml:mi>M</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula>. This aims to retain only class-related differences between two images (or two locations in the attribute latent space) by subtracting the content from the translated output <inline-formula><mml:math id="M31" display="inline"><mml:mfenced separators="|"><mml:mrow><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>v</mml:mi><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>μ</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></inline-formula>. Generation is regularised through an L1 loss <inline-formula><mml:math id="M32" display="inline"><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>∥</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mspace width="0.25em"/><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> which encourages <inline-formula><mml:math id="M33" display="inline"><mml:mo>{</mml:mo><mml:mi>M</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula> to reflect a small feasible map, which leads to a realistic translated image. At test time, we generate both a mean and variance through repeated rejection sampling.</p>
      <p id="P26">Finally, to further facilitate image generation, we apply element-wise L1 and L2 loss to the reconstructed images <inline-formula><mml:math id="M34" display="inline"><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>ˆ</mml:mi></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>ˆ</mml:mi></mml:mover></mml:mrow></mml:mfenced><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1,2</mml:mn></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math></inline-formula>, and the cyclically reconstructed images <inline-formula><mml:math id="M35" display="inline"><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>ˆ</mml:mi></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>ˆ</mml:mi></mml:mover></mml:mrow></mml:mfenced><mml:mspace width="0.25em"/><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1,2</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math></inline-formula>. The cycle consistency term also allows training with unpaired images.
<disp-formula id="FD3"><label>(3)</label><mml:math id="M36" display="block"><mml:msubsup><mml:mi>L</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mfenced close="" open="["><mml:mrow><mml:msub><mml:mrow><mml:mfenced close="‖" open="‖"><mml:mrow><mml:mi>G</mml:mi><mml:mfenced><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mi>a</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mspace linebreak="newline"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mfenced close="]" open=""><mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced close="‖" open="‖"><mml:mrow><mml:mi>G</mml:mi><mml:mfenced><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mi>a</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mspace linebreak="newline"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mfenced close="" open="["><mml:mrow><mml:msub><mml:mrow><mml:mfenced close="‖" open="‖"><mml:mrow><mml:mi>G</mml:mi><mml:mfenced><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mi>a</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mspace linebreak="newline"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mfenced close="]" open=""><mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced close="‖" open="‖"><mml:mrow><mml:mi>G</mml:mi><mml:mfenced><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mi>a</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>
<disp-formula id="FD4"><label>(4)</label><mml:math id="M37" display="block"><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mfenced open="∥" close="∥" separators="|"><mml:mrow><mml:mi>G</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>μ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace linebreak="newline"/><mml:mspace width="7.25em"/><mml:mfenced open="" close="]" separators="|"><mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open="∥" close="∥" separators="|"><mml:mrow><mml:mi>G</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>μ</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mspace linebreak="newline"/><mml:mspace width="5.25em"/><mml:mspace width="0.25em"/><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mfenced open="[" close="" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mfenced open="∥" close="∥" separators="|"><mml:mrow><mml:mi>G</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>μ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mspace linebreak="newline"/><mml:mspace width="7.25em"/><mml:mfenced open="" close="]" separators="|"><mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open="∥" close="∥" separators="|"><mml:mrow><mml:mi>G</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>μ</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula>
This means the <bold>full objective function</bold> of our network is:
<disp-formula id="FD5"><label>(5)</label><mml:math id="M38" display="block"><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mi>a</mml:mi></mml:msup></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mspace width="0.15em"/><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mrow></mml:msub><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msubsup><mml:mspace linebreak="newline"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mi>a</mml:mi></mml:msup></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>;</mml:mo><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mi>a</mml:mi></mml:msup></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mi>a</mml:mi></mml:msup></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:msubsup><mml:mi>L</mml:mi><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:msubsup><mml:mspace linebreak="newline"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mi>a</mml:mi></mml:msup></mml:mrow></mml:msub><mml:msubsup><mml:mi>L</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mi>a</mml:mi></mml:msup></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi mathvariant="italic">rec</mml:mi></mml:mrow></mml:msub><mml:mfenced><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula></p>
    </sec>
    <sec id="S6">
      <label>C.</label>
      <title>Training Details</title>
      <p id="P27">ICAM is trained in a similar fashion to Lee et al. [<xref rid="R42" ref-type="bibr">42</xref>]. For each iteration, the content discriminator is updated twice, followed by the update of the encoders, generators, and domain discriminators (i.e. each training iteration uses 3 batches to perform these updates). For each update of the generator, one input is selected for each class (e.g. 2 inputs including class 0 and 1). All experiments use the following hyperparameters: learning rate for content discriminator = 0.00004, learning rate for the rest = 0.0001, Adam optimiser with betas = (0.5, 0.999), <inline-formula><mml:math id="M39" display="inline"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">rec</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, for discriminator optimisation, and <inline-formula><mml:math id="M40" display="inline"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:math></inline-formula> for generator optimisation. These parameters were optimised for a 2D data set of simulated cortical lesions, as previously described in [<xref rid="R48" ref-type="bibr">48</xref>]. Regression experiments use a network pre-trained for classification, refined with addition of the regression loss.</p>
    </sec>
  </sec>
  <sec id="S7">
    <label>IV.</label>
    <title>Results</title>
    <p id="P28">We evaluate the performance of ICAM-reg through three experiments: 1) brain age prediction (using data from UK Biobank); 2) regression of birth age (using neonatal data from the developing Human Connectome Project - dHCP); and 3) prediction of MMSE scores (using data from ADNI). We compare against VA-GAN (for ADNI and UK Biobank), and against post-hoc saliency methods (for ADNI). All experiments were trained with PyTorch [<xref rid="R55" ref-type="bibr">55</xref>] using NVIDIA TITAN GPUs. For an extensive ablation study and evaluation of the impact of changing ICAM hyperparameters please refer to [<xref rid="R48" ref-type="bibr">48</xref>] and the project GitHub page.<sup><xref rid="FN1" ref-type="fn">1</xref></sup></p>
    <sec id="S8">
      <label>A.</label>
      <title>Brain Age Prediction for the UK Biobank Cohort</title>
      <sec id="S9">
        <label>1)</label>
        <title>UK Biobank Dataset and Training:</title>
        <p id="P29">The performance of ICAM and VA-GAN for brain age prediction was validated using T1 MRI data from healthy subjects (aged 45–80 years) acquired for the UK Biobank [<xref rid="R56" ref-type="bibr">56</xref>], [<xref rid="R57" ref-type="bibr">57</xref>]. T1 image processing (see also [<xref rid="R56" ref-type="bibr">56</xref>]) involved bias correction using FAST [<xref rid="R58" ref-type="bibr">58</xref>], brain extraction using BET [<xref rid="R59" ref-type="bibr">59</xref>] and linear registration to MNI space, using FLIRT [<xref rid="R60" ref-type="bibr">60</xref>]. The input into the networks was resized to 128 × 160 × 128 voxels, and normalised in range [0, 1]. For our classification experiments we used 11,735 MRI volumes, with a ‘young’ class defined as 45–60 years (average age 54.6±3.4 years) and an ‘old’ class defined as 70–80 years (average age 73.0±2.2 years). Young subjects were separated into training, validation, and testing set sizes of: 6706, 373 and 372. Older subjects were separated into training, validation, and testing set sizes of 3856, 214 and 214.</p>
        <p id="P30">For regression we used all available subjects (21,388), where adversarial training of the classifier was supported by defining two classes at the mid-range (45–65 and 65–80); subjects corresponding to the young class (average age 57.6±4.8 years) were separated into training, validation, and testing sets with sizes: 10715, 595 and 595; subjects corresponding to the old class (average age of 70.0±3.3 years) were separated into training, validation, and testing sets with sizes: 8535, 474 and 474. Performance on FA map generation was compared against VA-GAN trained using the default parameters provided in [<xref rid="R40" ref-type="bibr">40</xref>]. Both networks were trained for 50 epochs.</p>
      </sec>
      <sec id="S10">
        <label>2)</label>
        <title>UK Biobank Results:</title>
        <p id="P31">In previous work [<xref rid="R48" ref-type="bibr">48</xref>], we compared feature attribution with ICAM and VA-GAN, and found that ICAM generated FA maps that better matched patterns of ‘ground-truth’ atrophy observed between longitudinally acquired scans (see also <xref rid="F1" ref-type="fig">Fig 1</xref>). In this work, to demonstrate more conclusively whether translation by ICAM and VA-GAN fully changes the image class, we trained an independent binary age classifier (old vs young) using the same architecture as the ICAM attribute encoder. The classifier was trained using the ‘Real’ 3D T1 MRI images (<xref rid="T1" ref-type="table">Table I</xref>, row 1), or on outputs generated by ICAM (<xref rid="T1" ref-type="table">Table I</xref>, row 2) and VA-GAN (<xref rid="T1" ref-type="table">Table I</xref>, row 3), with training and test sets kept as before. Results (<xref rid="T1" ref-type="table">Table I</xref>) show that classification with images generated by ICAM performs slightly worse than the real data (82.2% compared to 93.8%), which is to be expected in a complex 3D generation task. By contrast, VA-GAN outputs perform much worse (12.2%). Note that because VA-GAN can only translate in one direction, it has only 1 result in the table.</p>
        <p id="P32">Next, we trained ICAM-reg’s regression layer to predict ages of the MRI brain scans: resulting in a precision of of 2.20 ± 1.86 mean absolute error (MAE) (<xref rid="F7" ref-type="fig">Fig. 7</xref>). We found that the resulting FA maps explained outlier predictions well. For example in <xref rid="F5" ref-type="fig">Fig. 5 A</xref>), FA maps of two subjects, scanned at 77 years, and translated to resemble the younger age class, indicate greater age-related changes (e.g. ventricular and cortical atrophy) in subject 1 (which is predicted as older - 79) relative to subject 2 (which is predicted as younger - 73). In B) 2 subjects from the young group are directly compared by translating between them. In this case, subject 4 is predicted to be much older than their true age (predicted=56; true=47 years); whereas, subject 3 has predicted age 49, close to their true age (47). Evidence for the outlier prediction of subject 4 is presented through the translation, indicating the presence of larger ventricles, hippocampal atrophy and cortical shrinking (relative to the more typical presentation of subject 3).</p>
        <p id="P33">In addition, we investigated the improvement in separation of the model’s latent space afforded through regression (<xref rid="F8" ref-type="fig">Fig. 8</xref>), where this result is further underlined in <xref rid="F6" ref-type="fig">Fig. 6</xref>, which shows clearly that interpolation between images of two different ages smoothly translates both predicted ages and FA maps, for the generated images.</p>
        <p id="P34">Finally, since it is required by the ICAM-reg framework to train regression tasks with complementary binary classification, we investigated whether imbalancing the classification (by moving the cut-off between classes) would impact the performance and interpretability of the FA maps. We ran a smaller version of the network (output channel dimensions 13:26:52 instead of 16:32:64), for three different thresholds: one at 60 years (where the young age group is 40–60 and the old age group is 60–90); one at 65 years (where the young age group is 40–65 and the old age group is 65–90); and one at 70 years (where the young age group is 40–70 and the old age group is 70–90). Since we now had different training, validation and testing splits, we selected a subset of 100 test examples which overlapped across all experiments. Results in <xref rid="F9" ref-type="fig">Fig. 9</xref> show ICAM-reg FA maps generated for one randomly selected subject of age 66. These return very similar FA mean maps for each experiment, despite the subject belonging to a different age classification each time. Importantly, we observe similar changes to key areas associated with healthy ageing. We did find that age prediction error varied across experiments: 3.27 (threshold 60), 3.67 (threshold 65) and 3.77 MAE (threshold 70); this may reflect the use of different training splits. We therefore conclude that while threshold selection is unlikely to lead to large differences in prediction and FA map generation, it could be a hyperparameter that can be tuned.</p>
      </sec>
    </sec>
    <sec id="S11">
      <label>B.</label>
      <title>dHCP Experiments</title>
      <sec id="S12">
        <label>1)</label>
        <title>dHCP Dataset and Training:</title>
        <p id="P35">In this experiment we sought to demonstrate that ICAM-reg can work well for prediction of challenging phenotypes, and detection of focal lesions, from relatively small, heterogenous, datasets. We used 699 3D T2 MRI scans from the dHCP [<xref rid="R61" ref-type="bibr">61</xref>], [<xref rid="R62" ref-type="bibr">62</xref>]: an open data set of multimodal brain scans acquired from preterm and term neonates. Here, preterm is defined as birth prior to 37 weeks gestational age (GA), where some preterm neonates were scanned twice: at birth and at term equivalent age. The data set includes 143 preterm images (class 1, mean gestation age at birth: 31.8 ± 3.85 weeks, mean post-menstrual age at scan: 41.0 ± 1.99 weeks) and 556 term controls (class 0, mean age at birth: 40.0 ± 1.27 week, mean post-menstrual age at scan: 41.4 ± 1.74). In this experiment ICAM-reg was trained to classify between preterms and terms, and predict birth age from the term age scan (i.e. scans acquired after 37 weeks post-menstrual). Examples were split into train, validation and test sets according to a 446:55:55 split (for term subjects) and 115:14:14 split (for preterm subjects).</p>
        <p id="P36">Image pre-processing involved using diffeomorphic multimodal (T1w/T2w) registration (ANTs SyN) to estimate nonlinear transforms to a 40 week template from the extended atlas [<xref rid="R62" ref-type="bibr">62</xref>], [<xref rid="R63" ref-type="bibr">63</xref>], [<xref rid="R64" ref-type="bibr">64</xref>]. This was necessary to allow the network to train, since without this step the network was challenged by stark changes in image appearance across the cohort, caused by rapid tissue maturation, and further confounded by the relatively small and imbalanced nature of the data set. For related reasons (to preserves age-related tissue maturational differences), images were rescaled to [0,1] by normalising across the intensity range of the entire group. Images were then brain extracted (using blurred masks), and CSF, ventricles and the skull were removed in order to focus the attention of the model on brain tissue differences between the groups.</p>
        <p id="P37">ICAM-reg was pre-trained on UK Biobank data; then trained on dHCP birth age regression for a further 1000 epochs, using the same hyperparameters. Performance was compared against a baseline CNN network, trained with same architecture as <inline-formula><mml:math id="M41" display="inline"><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, using smooth L1 loss, with Adam optimiser (learning rate = 0.0001, betas = [0.5, 0.999]) for 1000 epochs.</p>
      </sec>
      <sec id="S13">
        <label>2)</label>
        <title>dHCP Results:</title>
        <p id="P38">Results are shown in <xref rid="F7" ref-type="fig">Figs. 7</xref> and <xref rid="F10" ref-type="fig">10</xref>. We report a birth age prediction MAE of 0.806 ± 0.634 for ICAM-reg vs 1.525 ± 1.160 for the baseline CNN (<xref rid="F7" ref-type="fig">Fig. 7</xref>). In addition, we report a higher correlation coefficient for ICAM-reg (Spearman correlation test, <italic toggle="yes">p</italic> &lt; 0.0001, 0.873 for ICAM-reg and 0.695 for the baseline network).</p>
        <p id="P39">For qualitative analysis we tested ICAM-reg on previously unseen images of subjects with punctate white matter lesions (PWML), which are commonly seen in preterm babies [<xref rid="R20" ref-type="bibr">20</xref>], [<xref rid="R65" ref-type="bibr">65</xref>], to test whether these would be detected in the FA maps. The results are shown in <xref rid="F10" ref-type="fig">Fig.10</xref> with yellow arrows pointing at the lesions. Quantitative analysis of the detection rates for these lesions resulted in a recall of 0.805 ± 0.078 and precision of 0.004 ± 0.004, for term subjects, and recall 0.734±0.102 and precision 0.004±0.005 for preterm subjects. Note, unlike for the ADNI and biobank results, here the FA maps were thresholded at 0.01 to remove some of the image generation noise from the calculation, and binarise the masks. Binarisation was necessary as we sought to test purely whether lesions were being detected (or not) through calculation of precision and recall scores. We tested several thresholds (range 0–0.25) and reported results with the most optimal threshold for recall-precision trade-off. These results suggest ICAM-reg consistently detects lesions in both cohorts.</p>
      </sec>
    </sec>
    <sec id="S14">
      <label>C.</label>
      <title>ADNI Experiments: Ground-Truth Evaluation of FA Maps</title>
      <p id="P40">In the final experiment, we demonstrate the performance of ICAM’s feature attribution against ground truth maps of disease progression estimated for AD to MCI conversion using the ADNI dataset, and extend [<xref rid="R48" ref-type="bibr">48</xref>] to explore modelling regression of MMSE scores. The MMSE is a test that is commonly used for the assessment of dementia by examining memory, thinking and problem-solving abilities of a patient. The score ranges between 1–30 with scores of 25–30 considered normal, 21–24 indicates mild dementia, 10–20 indicates moderate dementia, and 9 or lower indicates severe dementia.</p>
      <sec id="S15">
        <label>1)</label>
        <title>ADNI Dataset:</title>
        <p id="P41">The data used in this study was obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (<ext-link xlink:href="https://adni.loni.usc.edu/" ext-link-type="uri">adni.loni.usc.edu</ext-link>), first launched in 2003, and led by Principal Investigator Michael W. Weiner, MD [<xref rid="R66" ref-type="bibr">66</xref>]. We used 1,053 3T T1 images, pre-processed with N4 bias correction [<xref rid="R67" ref-type="bibr">67</xref>], brain extracted using Freesurfer [<xref rid="R68" ref-type="bibr">68</xref>] and rigidly registered to the MNI space using Niftyreg [<xref rid="R69" ref-type="bibr">69</xref>]. Images were normalised in range [−1, 1], and resized to 128 × 160 × 128 voxels.</p>
        <p id="P42">For our classification experiments (used for comparisons in <xref rid="T2" ref-type="table">Table II</xref>) we split the dataset into AD and MCI classes, with 257 AD and 674 MCI volumes used for training. For our regression experiments we split the dataset into AD and MCI classes, with 223 AD and 626 MCI volumes used for training. The average age of training subjects was 74.91±8.1 (for AD) and 71.97 ± 7.8 (for MCI), with average MMSE scores of 23.02 ± 2.6 (AD) and 27.75 ± 2.6 (MCI). For testing and validation, the same 122 subjects (61 each) were used for both classification and regression experiments. The average age of validation subjects was 75.88±6.8 (for AD) and 73.67±7.0 (for MCI) with mean time between scans of 2.20±0.9 years. The average validation MMSE scores were 23.72 ± 4.3 (AD) and 26.95 ± 2.8 (MCI). The average age of test subjects was 75.63 ± 7.6 (for AD) and 73.44 ± 7.6 (for MCI) with mean time between scans of 2.19 ± 1.0. The average test MMSE scores were 24.21 ± 4.1 (AD) and 26.77 ± 3.0 (MCI).</p>
      </sec>
      <sec id="S16">
        <label>2)</label>
        <title>ADNI Training:</title>
        <p id="P43">ICAM-reg experiments were performed to jointly classify AD from MCI whilst also regressing MMSE, where it is assumed that these two tasks are correlated. We compare performance against ICAM [<xref rid="R48" ref-type="bibr">48</xref>], trained purely on MCI-AD classification, and a range of baseline methods: VA-GAN [<xref rid="R40" ref-type="bibr">40</xref>], Grad-CAM, guided Grad-CAM [<xref rid="R27" ref-type="bibr">27</xref>], guided backprop [<xref rid="R31" ref-type="bibr">31</xref>], integrated gradients [<xref rid="R30" ref-type="bibr">30</xref>], occlusion [<xref rid="R33" ref-type="bibr">33</xref>] and Layer-wise Relevance Propagation (LRP) [<xref rid="R32" ref-type="bibr">32</xref>].</p>
        <p id="P44">VA-GAN was trained using default parameters and post-hoc methods were applied following training of a simple 3D ResNet with 4 down ResNet blocks, and a fully connected layer for classification of AD vs MCI. Saliency maps were then generated using the captum library [<xref rid="R70" ref-type="bibr">70</xref>], where: Grad-CAM was implemented on the last convolutional block of the ResNet (with a size of 4 × 5 × 4) and was up-sampled to the input size for visualization; integrated gradients was implemented by considering a baseline volume with constant value of 0, and the integral was computed using 200 steps; and occlusion was implemented using occlusion blocks with value 0, size 10 × 10 × 10 and stride 5.</p>
        <p id="P45">All networks (including VA-GAN) were trained for 300 epochs. Both ICAM networks were then further refined, for another 200 epochs, using updated lambdas (<inline-formula><mml:math id="M42" display="inline"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">rec</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:math></inline-formula>, and <inline-formula><mml:math id="M43" display="inline"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:math></inline-formula>). It was not possible to refine VA-GAN any further because generator and discriminator losses went to zero during training (often after 150 epochs). The baseline classifier network was trained for 50 epochs with learning rate of 0.0001, SGD with momentum of 0.9, for 50 epochs, and using a weighted BCE loss (to account for class-unbalanced training).</p>
        <p id="P46">Methods were evaluated by comparing the overlap of the proposed FA maps against ground truth, obtained by subtracting the difference between test scans (acquired before and after conversion) following rigid alignment. All ground truth maps and FA maps were masked to ensure that the returned normalised cross correlation (NCC) values reference brain tissue only. We also report the classification and regression performance of the ICAM-reg and ResNet models only (since VA-GAN does not support supervised learning).</p>
      </sec>
      <sec id="S17">
        <label>3)</label>
        <title>ADNI Results:</title>
        <p id="P47">Results comparing the NCC of the proposed FA maps with ‘ground-truth’ disease maps (<xref rid="T2" ref-type="table">Table II</xref>) show that all versions of ICAM outperform VA-GAN, and post-hoc saliency methods. We further demonstrate qualitatively in <xref rid="F1" ref-type="fig">Fig. 1</xref>, that relevant areas of brain atrophy are detected using ICAM by comparing with disease map (ground truth). Classification accuracy of ICAM-reg for the AD vs MCI prediction was 60.7%; whereas for the simple ResNet prediction was 61.7%. At the same time, regression of the MMSE score returns MMSE prediction of 2.82±2.14 mean absolute error. Importantly, we cannot compare this to VA-GAN or other FA methods, as they cannot be normally applied to regression tasks.</p>
      </sec>
    </sec>
  </sec>
  <sec id="S18">
    <label>V.</label>
    <title>Discussion</title>
    <p id="P48">In our previous work [<xref rid="R48" ref-type="bibr">48</xref>] we developed a novel framework, ICAM, for classification with feature attribution, and showed that it outperforms state-of-the-art feature attribution methods on classification tasks for individual subject feature detection. In this work, we extended ICAM to include a regression module, ICAM-reg. We then sought to test whether, when trained on a large dataset (UK Biobank), ICAM-reg could learn to disentangle its attribute latent space, so as to support meaningful interpolation between images, and generate subject-specific explanations for outlier predictions. We also demonstrated that ICAM-reg can work on much smaller and more heterogeneous datasets (dHCP, ADNI), while continuing to detect relevant features not explicitly defined during training, i.e. white matter lesions in the dHCP neonatal data; and predict clinically relevant phenotypes such as age at birth (dHCP) and cognitive test scores (ADNI).</p>
    <p id="P49">It is important to stress that for all examples ICAM-reg runs simultaneous regression <italic toggle="yes">and</italic> classification, where the choice of classifier must be complementary. This is because the backbone of the algorithm remains an image-to-image translation network that trains discriminator networks to change the class of input images. To this end, we tested the impact of selecting different thresholds for converting the regression task to a classification task (i.e. splitting the dataset into 2 groups) using the UK Biobank dataset, and have found that threshold selection can have a small impact on performance and the interpretability of the FA maps, and thus can be considered a hyperparameter that can be tuned.</p>
    <p id="P50">Nevertheless, through experiments on UK Biobank, we demonstrate that ICAM can more comprehensively translate the class of input images, relative to VA-GAN [<xref rid="R40" ref-type="bibr">40</xref>] (<xref rid="T1" ref-type="table">Table I</xref>). Further visual comparison (<xref rid="F1" ref-type="fig">Fig 1</xref>) shows that while VA-GAN is only able to slightly modify the images by changing pixel intensities in order to generate FA maps, ICAM can drastically change the input image in order to change its class, and thus also generate more reliable FA maps. <xref rid="F1" ref-type="fig">Fig. 1</xref> shows that qualitatively the pattern of atrophy detected by ICAM aligns with the ‘Real’ (ground truth) disease map, and with known patterns of brain tissue loss reported for AD, which starts in the hippocampus, and progresses from medial to lateral temporal lobes, to the parietal and frontal lobes in late stages [<xref rid="R71" ref-type="bibr">71</xref>]. A common and easily observed side-effect of tissue loss is the growth of fluid filled spaces (such as the ventricles) which is also picked up here. While there are undoubtedly interaction effects from age-related decline, the mean time interval between scans is not long (around 2 years), and these results qualitatively agree with previous papers that have attempted to disentangle pathological atrophy from healthy ageing [<xref rid="R71" ref-type="bibr">71</xref>], [<xref rid="R72" ref-type="bibr">72</xref>], [<xref rid="R73" ref-type="bibr">73</xref>]. By contrast, VA-GAN was less able to detect atrophy of the more heterogeneous regions of the cortex. Equally the best performing post-hoc saliency methods (guided-backprop and LRP) also show less sensitivity, with LRP also returning asymmetric predictions, which may reflect the inability of post-hoc methods to capture redundant features.</p>
    <p id="P51">In separate experiments, we show that brain age prediction by ICAM-reg (2.20 ± 1.86 MAE, <xref rid="F7" ref-type="fig">Fig. 7</xref>) performs highly competitively relative to other deep learning methods trained on age prediction in UK Biobank, with reported test MAE scores of 2.14±0.05 [<xref rid="R74" ref-type="bibr">74</xref>], 2.71±2.10 (female) and 2.91±2.18 (male) [<xref rid="R75" ref-type="bibr">75</xref>], and 4.006 [<xref rid="R76" ref-type="bibr">76</xref>]. Alongside the age prediction, we find that ICAM-reg can provide meaningful and individual explanations for old and young classification, as well as outlier predictions (<xref rid="F5" ref-type="fig">Figs. 5</xref>, <xref rid="F6" ref-type="fig">6</xref>). We also demonstrated that our regression model has a more interpretable latent space than our previous model [<xref rid="R48" ref-type="bibr">48</xref>], through use of a tSNE comparison (<xref rid="F8" ref-type="fig">Fig. 8</xref>), and demonstrated interpolation of the latent space between and within groups (<xref rid="F6" ref-type="fig">Fig. 6</xref>).</p>
    <p id="P52">In our dHCP experiments, we compared our regression model to a baseline CNN that has the same architecture as our attribute encoder and found that ICAM-reg performs better than the baseline CNN on birth age prediction (<xref rid="F7" ref-type="fig">Fig. 7</xref>). Despite significant class imbalance ICAM-reg’s error (<xref rid="F7" ref-type="fig">Fig. 7</xref>) is approximately consistent across the age range. This may be attributed to the fact that each forward pass through the network takes an example from each class, meaning that each class is sampled in a balanced way during training. At the same time, the model returns subject specific FA explanations of the predictions, which consistently detect punctate white matter lesions, within individuals (a known feature of preterm birth, <xref rid="F10" ref-type="fig">Fig. 10</xref>). These are detected despite stark changes in image intensity and appearance over this neonatal period. This is further demonstrated by our qualitative experiments where we computed precision and recall scores between ground truth maps and generated FA maps, and found a high recall (i.e. high rate of lesion detection) and low precision (i.e. high amount of false positives). That precision is extremely low is not surprising since ICAM-reg is trained to predict birth age, therefore the FA maps should be expected to pick up on the diffuse tissue maturation changes known to exist between the two groups rather than explicitly focusing on the PWMLs.</p>
    <p id="P53">For ADNI we show that ICAM-reg can predict cognitive scores related to Alzheimer’s (MMSE scores), and provide meaningful FA map explanations that highlight individualised patterns of brain atrophy better than baseline methods (<xref rid="F1" ref-type="fig">Fig. 1</xref>). One challenge with using longitudinal brain atrophy, as ground truth for validation, is that this also incorporates age-related changes [<xref rid="R11" ref-type="bibr">11</xref>]. This may be why NCC scores are reduced for ICAM-reg (based on MMSE) relative to ICAM (based on disease classification only).</p>
    <p id="P54">Moreover reported classification of AD versus MCI, for both ICAM-reg and the baseline ResNet does not achieve state-of-the art performance, which some studies report as high as 76% [<xref rid="R77" ref-type="bibr">77</xref>], [<xref rid="R78" ref-type="bibr">78</xref>]. While optimising classification and regression scores was not the main objective of this paper, the relatively strong performance on UK Biobank age regression suggests that results on ADNI might be improved if the confounding effects of age, sex and scanner site were removed by for example, adding additional deconfounding modules to the network. Improved performance may also be achieved through better balancing of MMSE values across the training and test sets [<xref rid="R79" ref-type="bibr">79</xref>], addressing MMSE heteroscedasticity through use of through a different loss, and inclusion of additional modalities for example T2 FLAIR or PET.</p>
    <p id="P55">Finally, there are several challenges that could still be investigated in future work. First, while ICAM has been applied to regression and binary classification problems, it has still not been tested on multi-class datasets. Second, while ICAM shows some potential for subject specific modelling of disease progression, for example conversion of progressive MCI to full AD, or projecting the neurological impact of preterm birth, considerable more effort would be required prior to clinical translation to ensure the model is unbiased and generalises across scanners and sites. Finally, it is still challenging to apply ICAM to small and diverse data sets, particularly developmental cohorts, across which tissue intensities and brain shape change very rapidly. This was addressed for the dHCP experiments in this paper by using non-linear registration to remove gross brain shape variation and thus reduce the amount of variation the network had to learn. In future, these challenges could be addressed via application of GAN augmentation techniques [<xref rid="R80" ref-type="bibr">80</xref>] to increase training data for smaller datasets, and latent space clustering strategies to further encourage disentanglement of imbalanced classes [<xref rid="R81" ref-type="bibr">81</xref>].</p>
  </sec>
</body>
<back>
  <ack id="S19">
    <title>Acknowledgment</title>
    <p id="P56">The U.K. Biobank data was accessed under Application Number 8107. ADNI data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (<ext-link xlink:href="https://adni.loni.usc.edu/" ext-link-type="uri">adni.loni.usc.edu</ext-link>). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: <ext-link xlink:href="http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf" ext-link-type="uri">http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf</ext-link>.</p>
    <p id="P57">This work was supported in part by the Alzheimer’s Disease Neuroimaging Initiative (ADNI), National Institutes of Health under Grant U01 AG024904 and in part by the Department of Defense (DOD), ADNI, under Award W81XWH-12–2-0012. The work of Cher Bass and Emma C. Robinson was supported by the Academy of Medical Sciences/the British Heart Foundation/the Government Department of Business, Energy and Industrial Strategy/the Wellcome Trust Springboard Award under Grant SBF003/1116. The work of Cher Bass, Stephen M. Smith, and Emma C. Robinson was supported by the Wellcome Collaborative Award under Grant 215573/Z/19/Z. The work of Mariana da Silva was supported by the Engineering and Physical Sciences Research Council (EPSRC) Centre for Doctoral Training in Smart Medical Imaging under Grant EP/S022104/1. The work of Logan Z. J. Williams was supported by the Commonwealth Scholarship Commission, U.K. The work of Petru-Daniel Tudosiu was supported in part by the EPSRC Research Council and in part by the EPSRC Doctoral Training Programme (DTP) under Grant EP/R513064/1.</p>
  </ack>
  <fn-group>
    <fn id="FN1">
      <label>1</label>
      <p id="P58">
        <ext-link xlink:href="https://github.com/CherBass/ICAM#ablation-and-parameter-optimisation" ext-link-type="uri">https://github.com/CherBass/ICAM#ablation-and-parameter-optimisation</ext-link>
      </p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>[1]</label>
      <mixed-citation publication-type="journal"><name><surname>Cullen</surname><given-names>H</given-names></name><etal/>, “<article-title>Polygenic risk for neuropsychiatric disease and vulnerability to abnormal deep grey matter development</article-title>,” <source>Sci. Rep</source>, vol. <volume>9</volume>, no. <issue>1</issue>, pp. <fpage>1</fpage>–<lpage>8</lpage>, <year>2019</year>.<pub-id pub-id-type="pmid">30626917</pub-id></mixed-citation>
    </ref>
    <ref id="R2">
      <label>[2]</label>
      <mixed-citation publication-type="journal"><name><surname>Iqbal</surname><given-names>K</given-names></name><etal/>, “<article-title>Subgroups of Alzheimer’s disease based on cerebrospinal fluid molecular markers</article-title>,” <source>Ann. Neurol., Off. J. Amer. Neurolog. Assoc. Child Neurol. Soc</source>., vol. <volume>58</volume>, no. <issue>5</issue>, pp. <fpage>748</fpage>–<lpage>757</lpage>, <year>2005</year>.</mixed-citation>
    </ref>
    <ref id="R3">
      <label>[3]</label>
      <mixed-citation publication-type="journal"><name><surname>Ross</surname><given-names>CA</given-names></name>, <name><surname>Margolis</surname><given-names>RL</given-names></name>, <name><surname>Reading</surname><given-names>SA</given-names></name>, <name><surname>Pletnikov</surname><given-names>M</given-names></name>, and <name><surname>Coyle</surname><given-names>JT</given-names></name>, “<article-title>Neurobiology of schizophrenia</article-title>,” <source>Neuron</source>, vol. <volume>52</volume>, no. <issue>1</issue>, pp. <fpage>139</fpage>–<lpage>153</lpage>, <year>2006</year>.<pub-id pub-id-type="pmid">17015232</pub-id></mixed-citation>
    </ref>
    <ref id="R4">
      <label>[4]</label>
      <mixed-citation publication-type="journal"><name><surname>Glasser</surname><given-names>MF</given-names></name>, “<article-title>A multi-modal parcellation of human cerebral cortex</article-title>,” <source>Nature</source>, vol. <volume>536</volume>, no. <issue>7615</issue>, pp. <fpage>171</fpage>–<lpage>178</lpage>, <year>2016</year>.<pub-id pub-id-type="pmid">27437579</pub-id></mixed-citation>
    </ref>
    <ref id="R5">
      <label>[5]</label>
      <mixed-citation publication-type="journal"><name><surname>Kong</surname><given-names>R</given-names></name><etal/>, “<article-title>Spatial topography of individual-specific cortical networks predicts human cognition, personality, and emotion</article-title>,” <source>Cerebral Cortex</source>, vol. <volume>29</volume>, no. <issue>6</issue>, pp. <fpage>2533</fpage>–<lpage>2551</lpage>, <year>2019</year>.<pub-id pub-id-type="pmid">29878084</pub-id></mixed-citation>
    </ref>
    <ref id="R6">
      <label>[6]</label>
      <mixed-citation publication-type="journal"><name><surname>Ashburner</surname><given-names>J</given-names></name> and <name><surname>Friston</surname><given-names>KJ</given-names></name>, “<article-title>Voxel-based morphometry—The methods</article-title>,” <source>NeuroImage</source>, vol. <volume>11</volume>, pp. <fpage>805</fpage>–<lpage>821</lpage>, <month>Jun</month>. <year>2000</year>.<pub-id pub-id-type="pmid">10860804</pub-id></mixed-citation>
    </ref>
    <ref id="R7">
      <label>[7]</label>
      <mixed-citation publication-type="journal"><name><surname>Matsuda</surname><given-names>H</given-names></name>, “<article-title>Voxel-based morphometry of brain MRI in normal aging and Alzheimer’s disease</article-title>,” <source>Aging Disease</source>, vol. <volume>4</volume>, no. <issue>1</issue>, p. <fpage>29</fpage>, <year>2013</year>.<pub-id pub-id-type="pmid">23423504</pub-id></mixed-citation>
    </ref>
    <ref id="R8">
      <label>[8]</label>
      <mixed-citation publication-type="journal"><name><surname>Diaz-de Grenu</surname><given-names>LZ</given-names></name><etal/>, “<article-title>A brief history of voxel-based grey matter analysis in Alzheimer’s disease</article-title>,” <source>J. Alzheimer’s Disease</source>, vol. <volume>38</volume>, no. <issue>3</issue>, pp. <fpage>647</fpage>–<lpage>659</lpage>, <year>2014</year>.<pub-id pub-id-type="pmid">24037033</pub-id></mixed-citation>
    </ref>
    <ref id="R9">
      <label>[9]</label>
      <mixed-citation publication-type="journal"><name><surname>Busatto</surname><given-names>GF</given-names></name>, <name><surname>Diniz</surname><given-names>BS</given-names></name>, and <name><surname>Zanetti</surname><given-names>MV</given-names></name>, “<article-title>Voxel-based morphometry in Alzheimer’s disease</article-title>,” <source>Expert Rev. Neurotherapeutics</source>, vol. <volume>8</volume>, no. <issue>11</issue>, pp. <fpage>1691</fpage>–<lpage>1702</lpage>, <month>Nov</month>. <year>2008</year>.</mixed-citation>
    </ref>
    <ref id="R10">
      <label>[10]</label>
      <mixed-citation publication-type="journal"><name><surname>Padilla</surname><given-names>N</given-names></name>, <name><surname>Alexandrou</surname><given-names>G</given-names></name>, <name><surname>Blennow</surname><given-names>M</given-names></name>, <name><surname>Lagercrantz</surname><given-names>H</given-names></name>, and <name><surname>Ådén</surname><given-names>U</given-names></name>, “<article-title>Brain growth gains and losses in extremely preterm infants at term</article-title>,” <source>Cerebral Cortex</source>, vol. <volume>25</volume>, no. <issue>7</issue>, pp. <fpage>1897</fpage>–<lpage>1905</lpage>, <month>Jul</month>. <year>2015</year>.<pub-id pub-id-type="pmid">24488941</pub-id></mixed-citation>
    </ref>
    <ref id="R11">
      <label>[11]</label>
      <mixed-citation publication-type="journal"><name><surname>Shiino</surname><given-names>A</given-names></name>, <name><surname>Watanabe</surname><given-names>T</given-names></name>, <name><surname>Maeda</surname><given-names>K</given-names></name>, <name><surname>Kotani</surname><given-names>E</given-names></name>, <name><surname>Akiguchi</surname><given-names>I</given-names></name>, and <name><surname>Matsuda</surname><given-names>M</given-names></name>, “<article-title>Four subgroups of Alzheimer’s disease based on patterns of atrophy using VBM and a unique pattern for early onset disease</article-title>,” <source>NeuroImage</source>, vol. <volume>33</volume>, no. <issue>1</issue>, pp. <fpage>17</fpage>–<lpage>26</lpage>, <month>Oct</month>. <year>2006</year>.<pub-id pub-id-type="pmid">16904912</pub-id></mixed-citation>
    </ref>
    <ref id="R12">
      <label>[12]</label>
      <mixed-citation publication-type="journal"><name><surname>Dimitrova</surname><given-names>R</given-names></name><etal/>, “<article-title>Preterm birth alters the development of cortical microstructure and morphology at term-equivalent age</article-title>,” <source>NeuroImage</source>, vol. <volume>243</volume>, <month>Nov</month>. <year>2021</year>, Art. no. <fpage>118488</fpage>.<pub-id pub-id-type="pmid">34419595</pub-id></mixed-citation>
    </ref>
    <ref id="R13">
      <label>[13]</label>
      <mixed-citation publication-type="journal"><name><surname>Fenchel</surname><given-names>D</given-names></name><etal/>, “<article-title>Neonatal multi-modal cortical profiles predict 18-month developmental outcomes</article-title>,” <source>Developmental Cogn. Neurosci</source>, vol. <volume>54</volume>, <month>Apr</month>. <year>2022</year>, Art. no. <fpage>101103</fpage>.</mixed-citation>
    </ref>
    <ref id="R14">
      <label>[14]</label>
      <mixed-citation publication-type="journal"><name><surname>Gaonkar</surname><given-names>B</given-names></name>, <name><surname>Shinohara</surname><given-names>RT</given-names></name>, and <name><surname>Davatzikos</surname><given-names>C</given-names></name>, “<article-title>Interpreting support vector machine models for multivariate group wise analysis in neuroimaging</article-title>,” <source>Med. Image Anal</source>, vol. <volume>24</volume>, no. <issue>1</issue>, pp. <fpage>190</fpage>–<lpage>204</lpage>, <year>2015</year>.<pub-id pub-id-type="pmid">26210913</pub-id></mixed-citation>
    </ref>
    <ref id="R15">
      <label>[15]</label>
      <mixed-citation publication-type="journal"><name><surname>Girault</surname><given-names>JB</given-names></name><etal/>, “<article-title>White matter connectomes at birth accurately predict cognitive abilities at age 2</article-title>,” <source>NeuroImage</source>, vol. <volume>192</volume>, pp. <fpage>145</fpage>–<lpage>155</lpage>, <month>Jan</month>. <year>2019</year>.<pub-id pub-id-type="pmid">30825656</pub-id></mixed-citation>
    </ref>
    <ref id="R16">
      <label>[16]</label>
      <mixed-citation publication-type="journal"><name><surname>Pandit</surname><given-names>A</given-names></name><etal/>, “<article-title>Whole-brain mapping of structural connectivity in infants reveals altered connection strength associated with growth and preterm birth</article-title>,” <source>Cerebral Cortex</source>, vol. <volume>24</volume>, no. <issue>9</issue>, pp. <fpage>2324</fpage>–<lpage>2333</lpage>, <year>2014</year>.<pub-id pub-id-type="pmid">23547135</pub-id></mixed-citation>
    </ref>
    <ref id="R17">
      <label>[17]</label>
      <mixed-citation publication-type="journal"><name><surname>Robinson</surname><given-names>EC</given-names></name>, <name><surname>Hammers</surname><given-names>A</given-names></name>, <name><surname>Ericsson</surname><given-names>A</given-names></name>, <name><surname>Edwards</surname><given-names>AD</given-names></name>, and <name><surname>Rueckert</surname></name>, “<article-title>Identifying population differences in whole-brain structural networks: A machine learning approach</article-title>,” <source>NeuroImage</source>, vol. <volume>50</volume>, no. <issue>3</issue>, pp. <fpage>910</fpage>–<lpage>919</lpage>, <year>2010</year>.<pub-id pub-id-type="pmid">20079440</pub-id></mixed-citation>
    </ref>
    <ref id="R18">
      <label>[18]</label>
      <mixed-citation publication-type="confproc"><name><surname>Robinson</surname><given-names>EC</given-names></name>, <name><surname>Valstar</surname><given-names>M</given-names></name>, <name><surname>Hammers</surname><given-names>A</given-names></name>, <name><surname>Ericsson</surname><given-names>A</given-names></name>, <name><surname>Edwards</surname><given-names>AD</given-names></name>, and <name><surname>Rueckert</surname><given-names>D</given-names></name>, “<source>Multivariate statistical analysis of whole brain structural networks obtained using probabilistic tractography</source>,” in <conf-name>Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent</conf-name>. <publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2008</year>, pp. <fpage>486</fpage>–<lpage>493</lpage>.</mixed-citation>
    </ref>
    <ref id="R19">
      <label>[19]</label>
      <mixed-citation publication-type="journal"><name><surname>Zhao</surname><given-names>L</given-names></name><etal/>, “<article-title>Strategic infarct location for post-stroke cognitive impairment: A multivariate lesion-symptom mapping study</article-title>,” <source>J. Cerebral Blood Flow Metabolism</source>, vol. <volume>38</volume>, no. <issue>8</issue>, pp. <fpage>1299</fpage>–<lpage>1311</lpage>, <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="R20">
      <label>[20]</label>
      <mixed-citation publication-type="journal"><name><surname>O’Muircheartaigh</surname><given-names>J</given-names></name><etal/>, “<article-title>Modelling brain development to detect white matter injury in term and preterm born neonates</article-title>,” <source>Brain</source>, vol. <volume>143</volume>, no. <issue>2</issue>, pp. <fpage>467</fpage>–<lpage>479</lpage>, <year>2020</year>.<pub-id pub-id-type="pmid">31942938</pub-id></mixed-citation>
    </ref>
    <ref id="R21">
      <label>[21]</label>
      <mixed-citation publication-type="journal"><name><surname>Dalca</surname><given-names>A</given-names></name>, <name><surname>Rakic</surname><given-names>M</given-names></name>, <name><surname>Guttag</surname><given-names>J</given-names></name>, and <name><surname>Sabuncu</surname><given-names>M</given-names></name>, “<article-title>Learning conditional deformable templates with convolutional networks</article-title>,” in <source>Proc. Adv. Neural Inf. Process. Syst</source>, <year>2019</year>, pp. <fpage>804</fpage>–<lpage>816</lpage>.</mixed-citation>
    </ref>
    <ref id="R22">
      <label>[22]</label>
      <mixed-citation publication-type="book"><name><surname>Goodfellow</surname><given-names>I</given-names></name>, <name><surname>Bengio</surname><given-names>Y</given-names></name>, and <name><surname>Courville</surname><given-names>A</given-names></name>, <source>Deep Learning</source>. <publisher-loc>Cambridge, MA, USA</publisher-loc>: <publisher-name>MIT Press</publisher-name>, <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="R23">
      <label>[23]</label>
      <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>H</given-names></name>, <name><surname>Dou</surname><given-names>Q</given-names></name>, <name><surname>Yu</surname><given-names>L</given-names></name>, <name><surname>Qin</surname><given-names>J</given-names></name>, and <name><surname>Heng</surname><given-names>P-A</given-names></name>, “<article-title>VoxResNet: Deep voxelwise residual networks for brain segmentation from 3D MR images</article-title>,” <source>NeuroImage</source>, vol. <volume>170</volume>, pp. <fpage>446</fpage>–<lpage>455</lpage>, <month>Apr</month>. <year>2017</year>.<pub-id pub-id-type="pmid">28445774</pub-id></mixed-citation>
    </ref>
    <ref id="R24">
      <label>[24]</label>
      <mixed-citation publication-type="confproc"><name><surname>de Brebisson</surname><given-names>A</given-names></name> and <name><surname>Montana</surname><given-names>G</given-names></name>, “<source>Deep neural networks for anatomical brain segmentation</source>,” in <conf-name>Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW)</conf-name>, <month>Jun</month>. <year>2015</year>, pp. <fpage>20</fpage>–<lpage>28</lpage>.</mixed-citation>
    </ref>
    <ref id="R25">
      <label>[25]</label>
      <mixed-citation publication-type="confproc"><name><surname>Kamnitsas</surname><given-names>K</given-names></name><etal/>, “<source>Ensembles of multiple models and architectures for robust brain tumour segmentation</source>,” in <conf-name>Proc. Int. MICCAI Brainlesion Workshop</conf-name>. <publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2017</year>, pp. <fpage>450</fpage>–<lpage>462</lpage>.</mixed-citation>
    </ref>
    <ref id="R26">
      <label>[26]</label>
      <mixed-citation publication-type="journal"><name><surname>Rajchl</surname><given-names>M</given-names></name>, <name><surname>Pawlowski</surname><given-names>N</given-names></name>, <name><surname>Rueckert</surname><given-names>D</given-names></name>, <name><surname>Matthews</surname><given-names>PM</given-names></name>, and <name><surname>Glocker</surname><given-names>B</given-names></name>, “<article-title>NeuroNet: Fast and robust reproduction of multiple brain image segmentation pipelines</article-title>,” <year>2018</year>, <source>arXiv:1806.04224</source>.</mixed-citation>
    </ref>
    <ref id="R27">
      <label>[27]</label>
      <mixed-citation publication-type="confproc"><name><surname>Selvaraju</surname><given-names>RR</given-names></name>, <name><surname>Cogswell</surname><given-names>M</given-names></name>, <name><surname>Das</surname><given-names>A</given-names></name>, <name><surname>Vedantam</surname><given-names>R</given-names></name>, <name><surname>Parikh</surname><given-names>D</given-names></name>, and <name><surname>Batra</surname><given-names>D</given-names></name>, “<source>Grad-CAM: Visual explanations from deep networks via gradient-based localization</source>,” in <conf-name>Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</conf-name>, <month>Oct</month>. <year>2017</year>, pp. <fpage>618</fpage>–<lpage>626</lpage>.</mixed-citation>
    </ref>
    <ref id="R28">
      <label>[28]</label>
      <mixed-citation publication-type="journal"><name><surname>Lundberg</surname><given-names>SM</given-names></name> and <name><surname>Lee</surname><given-names>S-I</given-names></name>, “<article-title>A unified approach to interpreting model predictions</article-title>,” in <source>Proc. Adv. Neural Inf. Process. Syst</source>, <year>2017</year>, pp. <fpage>4765</fpage>–<lpage>4774</lpage>.</mixed-citation>
    </ref>
    <ref id="R29">
      <label>[29]</label>
      <mixed-citation publication-type="confproc"><name><surname>Montavon</surname><given-names>G</given-names></name>, <name><surname>Bach</surname><given-names>S</given-names></name>, <name><surname>Binder</surname><given-names>A</given-names></name>, <name><surname>Samek</surname><given-names>W</given-names></name>, and <name><surname>Müller</surname><given-names>K-R</given-names></name>, “<source>Deep Taylor decomposition of neural networks</source>,” in <conf-name>Proc. Int. Conf. Mach. Learn. Workshop Visualizat. Deep Learn</conf-name>., <year>2016</year>, pp. <fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation>
    </ref>
    <ref id="R30">
      <label>[30]</label>
      <mixed-citation publication-type="confproc"><name><surname>Sundararajan</surname><given-names>M</given-names></name>, <name><surname>Taly</surname><given-names>A</given-names></name>, and <name><surname>Yan</surname><given-names>Q</given-names></name>, “<source>Axiomatic attribution for deep networks</source>,” in <conf-name>Proc. 34th Int. Conf. Mach. Learn</conf-name>., <year>2017</year>, pp. <fpage>3319</fpage>–<lpage>3328</lpage>.</mixed-citation>
    </ref>
    <ref id="R31">
      <label>[31]</label>
      <mixed-citation publication-type="journal"><name><surname>Springenberg</surname><given-names>JT</given-names></name>, <name><surname>Dosovitskiy</surname><given-names>A</given-names></name>, <name><surname>Brox</surname><given-names>T</given-names></name>, and <name><surname>Riedmiller</surname><given-names>M</given-names></name>, “<article-title>Striving for simplicity: The all convolutional net</article-title>,” <year>2014</year>, <source>arXiv:1412.6806</source>.</mixed-citation>
    </ref>
    <ref id="R32">
      <label>[32]</label>
      <mixed-citation publication-type="journal"><name><surname>Bach</surname><given-names>S</given-names></name>, <name><surname>Binder</surname><given-names>A</given-names></name>, <name><surname>Montavon</surname><given-names>G</given-names></name>, <name><surname>Klauschen</surname><given-names>F</given-names></name>, <name><surname>Müller</surname><given-names>K-R</given-names></name>, and <name><surname>Samek</surname><given-names>W</given-names></name>, “<article-title>On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</article-title>,” <source>PLoS ONE</source>, vol. <volume>10</volume>, no. <issue>7</issue>, <month>Jul</month>. <year>2015</year>, Art. no. <fpage>e0130140</fpage>.<pub-id pub-id-type="pmid">26161953</pub-id></mixed-citation>
    </ref>
    <ref id="R33">
      <label>[33]</label>
      <mixed-citation publication-type="confproc"><name><surname>Zeiler</surname><given-names>MD</given-names></name> and <name><surname>Fergus</surname><given-names>R</given-names></name>, “<source>Visualizing and understanding convolutional networks</source>,” in <conf-name>Proc. Eur. Conf. Comput. Vis</conf-name>. <publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2014</year>, pp. <fpage>818</fpage>–<lpage>833</lpage>.</mixed-citation>
    </ref>
    <ref id="R34">
      <label>[34]</label>
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>H</given-names></name><etal/>, “<article-title>A deep learning model for early prediction of Alzheimer’s disease dementia based on hippocampal magnetic resonance imaging data</article-title>,” <source>Alzheimer’s Dementia</source>, vol. <volume>15</volume>, no. <issue>8</issue>, pp. <fpage>1059</fpage>–<lpage>1070</lpage>, <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="R35">
      <label>[35]</label>
      <mixed-citation publication-type="journal"><name><surname>Ding</surname><given-names>Y</given-names></name>, “<article-title>A deep learning model to predict a diagnosis of Alzheimer disease by using <sup>18</sup>F-FDG pet of the brain</article-title>,” <source>Radiology</source>, vol. <volume>290</volume>, no. <issue>2</issue>, pp. <fpage>456</fpage>–<lpage>464</lpage>, <year>2018</year>.<pub-id pub-id-type="pmid">30398430</pub-id></mixed-citation>
    </ref>
    <ref id="R36">
      <label>[36]</label>
      <mixed-citation publication-type="book"><name><surname>Eitel</surname><given-names>F</given-names></name><etal/>, “<part-title>Testing the robustness of attribution methods for convolutional neural networks in MRI-based Alzheimer’s disease classification</part-title>,” in <source>Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support</source>. <publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2019</year>, pp. <fpage>3</fpage>–<lpage>11</lpage>.</mixed-citation>
    </ref>
    <ref id="R37">
      <label>[37]</label>
      <mixed-citation publication-type="journal"><name><surname>Böhle</surname><given-names>M</given-names></name>, <name><surname>Eitel</surname><given-names>F</given-names></name>, <name><surname>Weygandt</surname><given-names>M</given-names></name>, and <name><surname>Ritter</surname><given-names>K</given-names></name>, “<article-title>Layer-wise relevance propagation for explaining deep neural network decisions in MRI-based Alzheimer’s disease classification</article-title>,” <source>Frontiers Aging Neurosci</source>, vol. <volume>11</volume>, p. <fpage>194</fpage>, <month>Jul</month>. <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="R38">
      <label>[38]</label>
      <mixed-citation publication-type="journal"><name><surname>Eitel</surname><given-names>F</given-names></name><etal/>, “<article-title>Uncovering convolutional neural network decisions for diagnosing multiple sclerosis on conventional MRI using layer-wise relevance propagation</article-title>,” <source>NeuroImage, Clin</source>., vol. <volume>24</volume>, <month>Jan</month>. <year>2019</year>, Art. no. <fpage>102003</fpage>.<pub-id pub-id-type="pmid">31634822</pub-id></mixed-citation>
    </ref>
    <ref id="R39">
      <label>[39]</label>
      <mixed-citation publication-type="journal"><name><surname>van der Velden</surname><given-names>BHM</given-names></name>, <name><surname>Janse</surname><given-names>MHA</given-names></name>, <name><surname>Ragusi</surname><given-names>MAA</given-names></name>, <name><surname>Loo</surname><given-names>CE</given-names></name>, and <name><surname>Gilhuijs</surname><given-names>KGA</given-names></name>, “<article-title>Volumetric breast density estimation on MRI using explainable deep learning regression</article-title>,” <source>Sci. Rep</source>, vol. <volume>10</volume>, no. <issue>1</issue>, pp. <fpage>1</fpage>–<lpage>9</lpage>, <month>Dec</month>. <year>2020</year>.<pub-id pub-id-type="pmid">31913322</pub-id></mixed-citation>
    </ref>
    <ref id="R40">
      <label>[40]</label>
      <mixed-citation publication-type="confproc"><name><surname>Baumgartner</surname><given-names>CF</given-names></name>, <name><surname>Koch</surname><given-names>LM</given-names></name>, <name><surname>Tezcan</surname><given-names>KC</given-names></name>, <name><surname>Ang</surname><given-names>JX</given-names></name>, and <name><surname>Konukoglu</surname></name>, “<source>Visual feature attribution using Wasserstein GANs</source>,” in <conf-name>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.</conf-name>, <month>Jun</month>. <year>2018</year>, pp. <fpage>8309</fpage>–<lpage>8319</lpage>.</mixed-citation>
    </ref>
    <ref id="R41">
      <label>[41]</label>
      <mixed-citation publication-type="confproc"><name><surname>Isola</surname><given-names>P</given-names></name>, <name><surname>Zhu</surname><given-names>J-Y</given-names></name>, <name><surname>Zhou</surname><given-names>T</given-names></name>, and <name><surname>Efros</surname><given-names>AA</given-names></name>, “<source>Image-to-image translation with conditional adversarial networks</source>,” in <conf-name>Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</conf-name>, <month>Jul</month>. <year>2017</year>, pp. <fpage>1125</fpage>–<lpage>1134</lpage>.</mixed-citation>
    </ref>
    <ref id="R42">
      <label>[42]</label>
      <mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>H-Y</given-names></name><etal/>, “<article-title>DRIT++: Diverse image-to-image translation via disentangled representations</article-title>,” <year>2019</year>, <source>arXiv:1905.01270</source>.</mixed-citation>
    </ref>
    <ref id="R43">
      <label>[43]</label>
      <mixed-citation publication-type="confproc"><name><surname>Lenis</surname><given-names>D</given-names></name>, <name><surname>Major</surname><given-names>D</given-names></name>, <name><surname>Wimmer</surname><given-names>M</given-names></name>, <name><surname>Berg</surname><given-names>A</given-names></name>, <name><surname>Sluiter</surname><given-names>G</given-names></name>, and <name><surname>Bühler</surname><given-names>K</given-names></name>, “<source>Domain aware medical image classifier interpretation by counterfactual impact analysis</source>,” in <conf-name>Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent</conf-name>. <publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2020</year>, pp. <fpage>315</fpage>–<lpage>325</lpage>.</mixed-citation>
    </ref>
    <ref id="R44">
      <label>[44]</label>
      <mixed-citation publication-type="journal"><name><surname>Schutte</surname><given-names>K</given-names></name>, <name><surname>Moindrot</surname><given-names>O</given-names></name>, <name><surname>Hérent</surname><given-names>P</given-names></name>, <name><surname>Schiratti</surname><given-names>J-B</given-names></name>, and <name><surname>Jégou</surname><given-names>S</given-names></name>, “<article-title>Using StyleGAN for visual interpretability of deep learning models on medical images</article-title>,” <year>2021</year>, <source>arXiv:2101.07563</source>.</mixed-citation>
    </ref>
    <ref id="R45">
      <label>[45]</label>
      <mixed-citation publication-type="confproc"><name><surname>Zhu</surname><given-names>J-Y</given-names></name>, <name><surname>Park</surname><given-names>T</given-names></name>, <name><surname>Isola</surname><given-names>P</given-names></name>, and <name><surname>Efros</surname><given-names>AA</given-names></name>, “<source>Unpaired image-to-image translation using cycle-consistent adversarial networks</source>,” in <conf-name>Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</conf-name>, <month>Oct</month>. <year>2017</year>, pp. <fpage>2223</fpage>–<lpage>2232</lpage>.</mixed-citation>
    </ref>
    <ref id="R46">
      <label>[46]</label>
      <mixed-citation publication-type="journal"><name><surname>Karras</surname><given-names>T</given-names></name>, <name><surname>Laine</surname><given-names>S</given-names></name>, and <name><surname>Aila</surname><given-names>T</given-names></name>, “<article-title>A style-based generator architecture for generative adversarial networks</article-title>,” <source>IEEE Trans. Pattern Anal. Mach. Intell</source>, vol. <volume>43</volume>, no. <issue>12</issue>, pp. <fpage>4217</fpage>–<lpage>4228</lpage>, <month>Dec</month>. <year>2021</year>.<pub-id pub-id-type="pmid">32012000</pub-id></mixed-citation>
    </ref>
    <ref id="R47">
      <label>[47]</label>
      <mixed-citation publication-type="confproc"><name><surname>Narayanaswamy</surname><given-names>A</given-names></name><etal/>, “<source>Scientific discovery by generating counterfactuals using image translation</source>,” in <conf-name>Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent</conf-name>. <publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2020</year>, pp. <fpage>273</fpage>–<lpage>283</lpage>.</mixed-citation>
    </ref>
    <ref id="R48">
      <label>[48]</label>
      <mixed-citation publication-type="journal"><name><surname>Bass</surname><given-names>C</given-names></name>, <name><surname>da Silva</surname><given-names>M</given-names></name>, <name><surname>Sudre</surname><given-names>C</given-names></name>, <name><surname>Tudosiu</surname><given-names>P-D</given-names></name>, <name><surname>Smith</surname><given-names>S</given-names></name>, and <name><surname>Robinson</surname><given-names>E</given-names></name>, “<article-title>ICAM: Interpretable classification via disentangled representations and feature attribution mapping</article-title>,” in <source>Proc. Adv. Neural Inf. Process. Syst</source>, vol. <volume>33</volume>, <year>2020</year>, pp. <fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation>
    </ref>
    <ref id="R49">
      <label>[49]</label>
      <mixed-citation publication-type="confproc"><name><surname>Huang</surname><given-names>X</given-names></name>, <name><surname>Liu</surname><given-names>M-Y</given-names></name>, <name><surname>Belongie</surname><given-names>S</given-names></name>, and <name><surname>Kautz</surname><given-names>J</given-names></name>, “<source>Multimodal unsupervised image-to-image translation</source>,” in <conf-name>Proc. Eur. Conf. Comput. Vis. (ECCV)</conf-name>, <year>2018</year>, pp. <fpage>172</fpage>–<lpage>189</lpage>.</mixed-citation>
    </ref>
    <ref id="R50">
      <label>[50]</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>M-Y</given-names></name>, <name><surname>Breuel</surname><given-names>T</given-names></name>, and <name><surname>Kautz</surname><given-names>J</given-names></name>, “<article-title>Unsupervised image-to-image translation networks</article-title>,” in <source>Proc. Adv. Neural Inf. Process. Syst</source>, <year>2017</year>, pp. <fpage>700</fpage>–<lpage>708</lpage>.</mixed-citation>
    </ref>
    <ref id="R51">
      <label>[51]</label>
      <mixed-citation publication-type="confproc"><name><surname>Jha</surname><given-names>AH</given-names></name>, <name><surname>Anand</surname><given-names>S</given-names></name>, <name><surname>Singh</surname><given-names>M</given-names></name>, and <name><surname>Veeravasarapu</surname><given-names>V</given-names></name>, “<source>Disentangling factors of variation with cycle-consistent variational auto-encoders</source>,” in <conf-name>Proc. Eur. Conf. Comput. Vis</conf-name>. <publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2018</year>, pp. <fpage>829</fpage>–<lpage>845</lpage>.</mixed-citation>
    </ref>
    <ref id="R52">
      <label>[52]</label>
      <mixed-citation publication-type="confproc"><name><surname>Bass</surname><given-names>C</given-names></name><etal/>, “<source>Image synthesis with a convolutional capsule generative adversarial network</source>,” in <conf-name>Proc. Int. Conf. Med. Imag. Deep Learn</conf-name>., <year>2019</year>, pp. <fpage>1</fpage>–<lpage>24</lpage>.</mixed-citation>
    </ref>
    <ref id="R53">
      <label>[53]</label>
      <mixed-citation publication-type="confproc"><name><surname>Baur</surname><given-names>C</given-names></name>, <name><surname>Wiestler</surname><given-names>B</given-names></name>, <name><surname>Albarqouni</surname><given-names>S</given-names></name>, and <name><surname>Navab</surname><given-names>N</given-names></name>, “<source>Deep autoencoding models for unsupervised anomaly segmentation in brain MR images</source>,” in <conf-name>Proc. Int. MICCAI Brainlesion Workshop</conf-name>. <publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2018</year>, pp. <fpage>161</fpage>–<lpage>169</lpage>.</mixed-citation>
    </ref>
    <ref id="R54">
      <label>[54]</label>
      <mixed-citation publication-type="journal"><name><surname>Costa</surname><given-names>P</given-names></name><etal/>, “<article-title>End-to-end adversarial retinal image synthesis</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>37</volume>, no. <issue>3</issue>, pp. <fpage>781</fpage>–<lpage>791</lpage>, <month>Mar</month>. <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="R55">
      <label>[55]</label>
      <mixed-citation publication-type="book"><name><surname>Paszke</surname><given-names>A</given-names></name><etal/>, “<part-title>PyTorch: An imperative style, high-performance deep learning library</part-title>,” in <source>Advances in Neural Information Processing Systems</source>, <name><surname>Wallach</surname><given-names>H</given-names></name>, <name><surname>Larochelle</surname><given-names>H</given-names></name>, <name><surname>Beygelzimer</surname><given-names>A</given-names></name>, <name><surname>d Alche-Buc</surname><given-names>F</given-names></name>, <name><surname>Fox</surname><given-names>E</given-names></name>, and <name><surname>Garnett</surname><given-names>R</given-names></name>, Eds. <publisher-loc>Red Hook, NY, USA</publisher-loc>: <publisher-name>Curran Associates</publisher-name>, <year>2019</year>, pp. <fpage>8024</fpage>–<lpage>8035</lpage>.</mixed-citation>
    </ref>
    <ref id="R56">
      <label>[56]</label>
      <mixed-citation publication-type="journal"><name><surname>Alfaro-Almagro</surname><given-names>F</given-names></name><etal/>, “<article-title>Image processing and quality control for the first 10,000 brain imaging datasets from UK biobank</article-title>,” <source>NeuroImage</source>, vol. <volume>166</volume>, pp. <fpage>400</fpage>–<lpage>424</lpage>, <month>Jan</month>. <year>2018</year>.<pub-id pub-id-type="pmid">29079522</pub-id></mixed-citation>
    </ref>
    <ref id="R57">
      <label>[57]</label>
      <mixed-citation publication-type="journal"><name><surname>Miller</surname><given-names>KL</given-names></name><etal/>, “<article-title>Multimodal population brain imaging in the UK biobank prospective epidemiological study</article-title>,” <source>Nature Neurosci</source>, vol. <volume>19</volume>, no. <issue>11</issue>, p. <fpage>1523</fpage>, <year>2016</year>.<pub-id pub-id-type="pmid">27643430</pub-id></mixed-citation>
    </ref>
    <ref id="R58">
      <label>[58]</label>
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Brady</surname><given-names>M</given-names></name>, and <name><surname>Smith</surname><given-names>S</given-names></name>, “<article-title>Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>20</volume>, no. <issue>1</issue>, pp. <fpage>45</fpage>–<lpage>57</lpage>, <month>Jan</month>. <year>2001</year>.</mixed-citation>
    </ref>
    <ref id="R59">
      <label>[59]</label>
      <mixed-citation publication-type="journal"><name><surname>Smith</surname><given-names>SM</given-names></name>, “<article-title>Fast robust automated brain extraction</article-title>,” <source>Hum. Brain Mapping</source>, vol. <volume>17</volume>, no. <issue>3</issue>, pp. <fpage>143</fpage>–<lpage>155</lpage>, <year>2002</year>.</mixed-citation>
    </ref>
    <ref id="R60">
      <label>[60]</label>
      <mixed-citation publication-type="journal"><name><surname>Jenkinson</surname><given-names>M</given-names></name>, <name><surname>Bannister</surname><given-names>P</given-names></name>, <name><surname>Brady</surname><given-names>M</given-names></name>, and <name><surname>Smith</surname><given-names>S</given-names></name>, “<article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title>,” <source>NeuroImage</source>, vol. <volume>17</volume>, no. <issue>2</issue>, pp. <fpage>825</fpage>–<lpage>841</lpage>, <month>Oct</month>. <year>2002</year>.<pub-id pub-id-type="pmid">12377157</pub-id></mixed-citation>
    </ref>
    <ref id="R61">
      <label>[61]</label>
      <mixed-citation publication-type="journal"><name><surname>Makropoulos</surname><given-names>A</given-names></name><etal/>, “<article-title>The developing human connectome project: A minimal processing pipeline for neonatal cortical surface reconstruction</article-title>,” <source>NeuroImage</source>, vol. <volume>173</volume>, pp. <fpage>88</fpage>–<lpage>112</lpage>, <month>Jan</month>. <year>2018</year>.<pub-id pub-id-type="pmid">29409960</pub-id></mixed-citation>
    </ref>
    <ref id="R62">
      <label>[62]</label>
      <mixed-citation publication-type="journal"><name><surname>Fitzgibbon</surname><given-names>SP</given-names></name><etal/>, “<article-title>The developing human connectome project (dHCP) automated resting-state functional processing framework for newborn infants</article-title>,” <source>NeuroImage</source>, vol. <volume>223</volume>, <month>Dec</month>. <year>2020</year>, Art. no. <fpage>117303</fpage>.<pub-id pub-id-type="pmid">32866666</pub-id></mixed-citation>
    </ref>
    <ref id="R63">
      <label>[63]</label>
      <mixed-citation publication-type="journal"><name><surname>Schuh</surname><given-names>A</given-names></name><etal/>, “<article-title>Unbiased construction of a temporally consistent morphological atlas of neonatal brain development</article-title>,” <source>bioRxiv</source>, vol. <volume>15</volume>, <month>Jan</month>. <year>2018</year>, Art. no. <fpage>251512</fpage>.</mixed-citation>
    </ref>
    <ref id="R64">
      <label>[64]</label>
      <mixed-citation publication-type="journal"><name><surname>Avants</surname><given-names>BB</given-names></name>, <name><surname>Tustison</surname><given-names>NJ</given-names></name>, <name><surname>Song</surname><given-names>G</given-names></name>, <name><surname>Cook</surname><given-names>PA</given-names></name>, <name><surname>Klein</surname><given-names>A</given-names></name>, and <name><surname>Gee</surname><given-names>JC</given-names></name>, “<article-title>A reproducible evaluation of ANTs similarity metric performance in brain image registration</article-title>,” <source>NeuroImage</source>, vol. <volume>54</volume>, no. <issue>3</issue>, pp. <fpage>2033</fpage>–<lpage>2044</lpage>, <year>2011</year>.<pub-id pub-id-type="pmid">20851191</pub-id></mixed-citation>
    </ref>
    <ref id="R65">
      <label>[65]</label>
      <mixed-citation publication-type="journal"><name><surname>Tusor</surname><given-names>N</given-names></name><etal/>, “<article-title>Punctate white matter lesions associated with altered brain development and adverse motor outcome in preterm infants</article-title>,” <source>Sci. Rep</source>, vol. <volume>7</volume>, no. <issue>1</issue>, pp. <fpage>1</fpage>–<lpage>9</lpage>, <year>2017</year>.<pub-id pub-id-type="pmid">28127051</pub-id></mixed-citation>
    </ref>
    <ref id="R66">
      <label>[66]</label>
      <mixed-citation publication-type="journal"><name><surname>Jack</surname><given-names>CR</given-names></name><etal/>, “<article-title>The Alzheimer’s disease neuroimaging initiative (ADNI): MRI methods</article-title>,” <source>J. Magn. Reson. Imag., Off. J. Int. Soc. Magn. Reson. Med</source>., vol. <volume>27</volume>, no. <issue>4</issue>, pp. <fpage>685</fpage>–<lpage>691</lpage>, <year>2008</year>.</mixed-citation>
    </ref>
    <ref id="R67">
      <label>[67]</label>
      <mixed-citation publication-type="journal"><name><surname>Tustison</surname><given-names>NJ</given-names></name><etal/>, “<article-title>N4ITK: Improved N3 bias correction</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>29</volume>, no. <issue>6</issue>, pp. <fpage>1310</fpage>–<lpage>1320</lpage>, <month>Jun</month>. <year>2010</year>.</mixed-citation>
    </ref>
    <ref id="R68">
      <label>[68]</label>
      <mixed-citation publication-type="journal"><name><surname>Ségonne</surname><given-names>F</given-names></name><etal/>, “<article-title>A hybrid approach to the skull stripping problem in MRI</article-title>,” <source>NeuroImage</source>, vol. <volume>22</volume>, no. <issue>3</issue>, pp. <fpage>1060</fpage>–<lpage>1075</lpage>, <year>2004</year>.<pub-id pub-id-type="pmid">15219578</pub-id></mixed-citation>
    </ref>
    <ref id="R69">
      <label>[69]</label>
      <mixed-citation publication-type="confproc"><name><surname>Modat</surname><given-names>M</given-names></name>, <name><surname>Cardoso</surname><given-names>MJ</given-names></name>, <name><surname>Daga</surname><given-names>P</given-names></name>, <name><surname>Cash</surname><given-names>D</given-names></name>, <name><surname>Fox</surname><given-names>NC</given-names></name>, and <name><surname>Ourselin</surname><given-names>S</given-names></name>, “<source>Inverse-consistent symmetric free form deformation</source>,” in <conf-name>Proc. Int. Workshop Biomed. Image Registration</conf-name>. <publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2012</year>, pp. <fpage>79</fpage>–<lpage>88</lpage>.</mixed-citation>
    </ref>
    <ref id="R70">
      <label>[70]</label>
      <mixed-citation publication-type="webpage"><name><surname>Kokhlikyan</surname><given-names>N</given-names></name><etal/> (<year>2019</year>). <source>PyTorch Captum</source>. [<comment>Online</comment>]. <comment>Available</comment>: <comment><ext-link xlink:href="https://github.com/pytorch/captum" ext-link-type="uri">https://github.com/pytorch/captum</ext-link></comment></mixed-citation>
    </ref>
    <ref id="R71">
      <label>[71]</label>
      <mixed-citation publication-type="journal"><name><surname>Lorenzi</surname><given-names>M</given-names></name>, <name><surname>Pennec</surname><given-names>X</given-names></name>, <name><surname>Frisoni</surname><given-names>GB</given-names></name>, and <name><surname>Ayache</surname><given-names>N</given-names></name>, “<article-title>Disentangling normal aging from Alzheimer’s disease in structural magnetic resonance images</article-title>,” <source>Neurobiol. Aging</source>, vol. <volume>36</volume>, pp. <fpage>S42</fpage>–<lpage>S52</lpage>, <month>Jan</month>. <year>2015</year>.<pub-id pub-id-type="pmid">25311276</pub-id></mixed-citation>
    </ref>
    <ref id="R72">
      <label>[72]</label>
      <mixed-citation publication-type="journal"><name><surname>Sivera</surname><given-names>R</given-names></name>, <name><surname>Delingette</surname><given-names>H</given-names></name>, <name><surname>Lorenzi</surname><given-names>M</given-names></name>, <name><surname>Pennec</surname><given-names>X</given-names></name>, and <name><surname>Ayache</surname><given-names>N</given-names></name>, “<article-title>A model of brain morphological changes related to aging and Alzheimer’s disease from cross-sectional assessments</article-title>,” <source>NeuroImage</source>, vol. <volume>198</volume>, pp. <fpage>255</fpage>–<lpage>270</lpage>, <month>Sep</month>. <year>2019</year>.<pub-id pub-id-type="pmid">31121298</pub-id></mixed-citation>
    </ref>
    <ref id="R73">
      <label>[73]</label>
      <mixed-citation publication-type="journal"><name><surname>Risacher</surname><given-names>SL</given-names></name><etal/>, “<article-title>Longitudinal MRI atrophy biomarkers: Relationship to conversion in the ADNI cohort</article-title>,” <source>Neurobiol. Aging</source>, vol. <volume>31</volume>, no. <issue>8</issue>, pp. <fpage>1401</fpage>–<lpage>1418</lpage>, <year>2010</year>.<pub-id pub-id-type="pmid">20620664</pub-id></mixed-citation>
    </ref>
    <ref id="R74">
      <label>[74]</label>
      <mixed-citation publication-type="journal"><name><surname>Peng</surname><given-names>H</given-names></name>, <name><surname>Gong</surname><given-names>W</given-names></name>, <name><surname>Beckmann</surname><given-names>CF</given-names></name>, <name><surname>Vedaldi</surname><given-names>A</given-names></name>, and <name><surname>Smith</surname><given-names>SM</given-names></name>, “<article-title>Accurate brain age prediction with lightweight deep neural networks</article-title>,” <source>Med. Image Anal</source>, vol. <volume>68</volume>, <month>Feb</month>. <year>2021</year>, Art. no. <fpage>101871</fpage>.<pub-id pub-id-type="pmid">33197716</pub-id></mixed-citation>
    </ref>
    <ref id="R75">
      <label>[75]</label>
      <mixed-citation publication-type="journal"><name><surname>Dinsdale</surname><given-names>NK</given-names></name><etal/>, “<article-title>Learning patterns of the ageing brain in MRI using deep convolutional networks</article-title>,” <source>NeuroImage</source>, vol. <volume>224</volume>, <month>Jan</month>. <year>2021</year>, Art. no. <fpage>117401</fpage>.<pub-id pub-id-type="pmid">32979523</pub-id></mixed-citation>
    </ref>
    <ref id="R76">
      <label>[76]</label>
      <mixed-citation publication-type="journal"><name><surname>Jónsson</surname><given-names>BA</given-names></name><etal/>, “<article-title>Brain age prediction using deep learning uncovers associated sequence variants</article-title>,” <source>Nature Commun</source>, vol. <volume>10</volume>, no. <issue>1</issue>, pp. <fpage>1</fpage>–<lpage>10</lpage>, <year>2019</year>.<pub-id pub-id-type="pmid">30602773</pub-id></mixed-citation>
    </ref>
    <ref id="R77">
      <label>[77]</label>
      <mixed-citation publication-type="confproc"><name><surname>Senanayake</surname><given-names>U</given-names></name>, <name><surname>Sowmya</surname><given-names>A</given-names></name>, and <name><surname>Dawes</surname><given-names>L</given-names></name>, “<source>Deep fusion pipeline for mild cognitive impairment diagnosis</source>,” in <conf-name>Proc. IEEE 15th Int. Symp. Biomed. Imag. (ISBI)</conf-name>, <month>Apr</month>. <year>2018</year>, pp. <fpage>1394</fpage>–<lpage>1997</lpage>.</mixed-citation>
    </ref>
    <ref id="R78">
      <label>[78]</label>
      <mixed-citation publication-type="journal"><name><surname>Wen</surname><given-names>J</given-names></name><etal/>, “<article-title>Convolutional neural networks for classification of Alzheimer’s disease: Overview and reproducible evaluation</article-title>,” <source>Med. Image Anal</source>, vol. <volume>63</volume>, <month>Jul</month>. <year>2020</year>, Art. no. <fpage>101694</fpage>.<pub-id pub-id-type="pmid">32417716</pub-id></mixed-citation>
    </ref>
    <ref id="R79">
      <label>[79]</label>
      <mixed-citation publication-type="journal"><name><surname>Ho</surname><given-names>DE</given-names></name>, <name><surname>Imai</surname><given-names>K</given-names></name>, <name><surname>King</surname><given-names>G</given-names></name>, and <name><surname>Stuart</surname><given-names>EA</given-names></name>, “<article-title>Matching as nonparametric preprocessing for reducing model dependence in parametric causal inference</article-title>,” <source>Political Anal</source>, vol. <volume>15</volume>, no. <issue>3</issue>, pp. <fpage>199</fpage>–<lpage>236</lpage>, <year>2007</year>.</mixed-citation>
    </ref>
    <ref id="R80">
      <label>[80]</label>
      <mixed-citation publication-type="journal"><name><surname>Karras</surname><given-names>T</given-names></name>, <name><surname>Aittala</surname><given-names>M</given-names></name>, <name><surname>Hellsten</surname><given-names>J</given-names></name>, <name><surname>Laine</surname><given-names>S</given-names></name>, <name><surname>Lehtinen</surname><given-names>J</given-names></name>, and <name><surname>Aila</surname><given-names>T</given-names></name>, “<article-title>Training generative adversarial networks with limited data</article-title>,” <year>2020</year>, <source>arXiv:2006.06676</source>.</mixed-citation>
    </ref>
    <ref id="R81">
      <label>[81]</label>
      <mixed-citation publication-type="journal"><name><surname>Sohoni</surname><given-names>NS</given-names></name>, <name><surname>Dunnmon</surname><given-names>JA</given-names></name>, <name><surname>Angus</surname><given-names>G</given-names></name>, <name><surname>Gu</surname><given-names>A</given-names></name>, and <name><surname>Ré</surname><given-names>C</given-names></name>, “<article-title>No subclass left behind: Fine-grained robustness in coarse-grained classification problems</article-title>,” <year>2020</year>, <source>arXiv:2011.12945</source>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="F1">
    <label>Fig. 1.</label>
    <caption>
      <p id="P59">ADNI comparisons of Feature Attribution (FA) maps for different post-hoc and generative models. Results are visualised for one individual, scanned twice longitudinally, during which time the subject was known to convert. Here, the ‘Real’ (ground-truth) disease map was calculated by subtracting the difference between the two scans. ICAM (mean and variance maps) show good detection of regions known to be implicated in Alzheimer’s disease: the ventricles (blue arrows), cortex (green arrows), and hippocampus (pink arrows); results align much more closely with the ground truth than competing baseline methods.</p>
    </caption>
    <graphic xlink:href="nihms-1888754-f0001" position="float"/>
  </fig>
  <fig position="float" id="F2">
    <label>Fig. 2.</label>
    <caption>
      <p id="P60">Comparison of domain mapping methods. (a) VA-GAN translates images of domain <inline-formula><mml:math id="M44" display="inline"><mml:mi mathvariant="normal">x</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="M45" display="inline"><mml:mi mathvariant="normal">y</mml:mi></mml:math></inline-formula>. (b) DRIT can translate between domains <inline-formula><mml:math id="M46" display="inline"><mml:mi mathvariant="normal">x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M47" display="inline"><mml:mi mathvariant="normal">y</mml:mi></mml:math></inline-formula> through a shared content space <inline-formula><mml:math id="M48" display="inline"><mml:mi>C</mml:mi></mml:math></inline-formula>, and separate attribute spaces <inline-formula><mml:math id="M49" display="inline"><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="M50" display="inline"><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. (c) ICAM-reg uses shared content <inline-formula><mml:math id="M51" display="inline"><mml:mi>C</mml:mi></mml:math></inline-formula> and attribute <inline-formula><mml:math id="M52" display="inline"><mml:mi>A</mml:mi></mml:math></inline-formula> spaces to translate between domains, which allows classification <inline-formula><mml:math id="M53" display="inline"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> and regression <inline-formula><mml:math id="M54" display="inline"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> layers to be applied to the attribute space <inline-formula><mml:math id="M55" display="inline"><mml:mi>A</mml:mi></mml:math></inline-formula>.</p>
    </caption>
    <graphic xlink:href="nihms-1888754-f0002" position="float"/>
  </fig>
  <fig position="float" id="F3">
    <label>Fig. 3.</label>
    <caption>
      <p id="P61">Overview of method. An example of how ICAM performs classification/regression with FA map generation for 2 given input images <inline-formula><mml:math id="M56" display="inline"><mml:mi mathvariant="normal">x</mml:mi></mml:math></inline-formula> (of class 0 [brain slice without lesions] and <inline-formula><mml:math id="M57" display="inline"><mml:mi mathvariant="normal">y</mml:mi></mml:math></inline-formula> (class 1 [brain slice with simulated lesions]). Note that <inline-formula><mml:math id="M58" display="inline"><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is applied to both real and generated images, and that not all losses are plotted (see <xref rid="FD5" ref-type="disp-formula">Equation 5</xref> for full objective).</p>
    </caption>
    <graphic xlink:href="nihms-1888754-f0003" position="float"/>
  </fig>
  <fig position="float" id="F4">
    <label>Fig. 4.</label>
    <caption>
      <p id="P62">Rejection sampling during training/ testing (a) An input image is encoded into content and attribute spaces, and is passed through the classifier to identify its class (0 in this example). (b) Attribute space A is then randomly sampled until the classifier detects random vector of the opposite class. The newly sampled vector is passed to the generator along with the encoded content space to achieve translation between class 0 and 1.</p>
    </caption>
    <graphic xlink:href="nihms-1888754-f0004" position="float"/>
  </fig>
  <fig position="float" id="F5">
    <label>Fig. 5.</label>
    <caption>
      <p id="P63">UK Biobank regression: here we show two different ways in which FA maps derived from ICAM-reg can be used to explain outlier predictions. In the left box we show, FA maps resulting from translating two individuals (true age 77) towards a classification of young (using rejection sampling). In this example subject 1 is predicted as older (79) and subject 2 is predicted as younger (73); this correlates with the FA maps, which show greater age-related changes for subject 1. On the right we show FA maps derived from interpolating between two subjects within the attribute latent space. Again both have the same true age but subject 4 is predicted as much older than subject 3. The FA maps provide an explanation for this difference, showing that to translate subject 4 towards subject 3 it is necessary to fill in the ventricles and reduce cortical atrophy - all changes associated with healthy ageing.</p>
    </caption>
    <graphic xlink:href="nihms-1888754-f0005" position="float"/>
  </fig>
  <fig position="float" id="F6">
    <label>Fig. 6.</label>
    <caption>
      <p id="P64">Biobank interpolation between and within groups. Here, we show an example of interpolation of the attribute latent space, with the corresponding FA maps for each vector. We overlay the interpolated FA maps on the original image, with red maps indicating an increase pixel intensity. We first encode each image to its attribute latent space (using <inline-formula><mml:math id="M59" display="inline"><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>), and get an age prediction. We then linearly interpolate between these two spaces, and get an age prediction and FA map for each vector. We demonstrate that our ICAM-reg model can successfully achieve interpolation between and within groups (i.e. within the aged group, and between the aged and young groups). We find that we get both smoothly interpolated FA maps, and interpolated age predictions between two subjects. The green arrows point to the cortex, and blue arrows point to the ventricles.</p>
    </caption>
    <graphic xlink:href="nihms-1888754-f0006" position="float"/>
  </fig>
  <fig position="float" id="F7">
    <label>Fig. 7.</label>
    <caption>
      <p id="P65">Biobank (top) and dHCP (bottom) age prediction on the test dataset using ICAM-reg. For biobank, the age prediction error is 2.20 ± 1.86 MAE. For dHCP, the birth age prediction MAE is 0.806 ± 0.634, for ICAM-reg, and 1.525 ± 1.160, for the baseline network.</p>
    </caption>
    <graphic xlink:href="nihms-1888754-f0007" position="float"/>
  </fig>
  <fig position="float" id="F8">
    <label>Fig. 8.</label>
    <caption>
      <p id="P66">tSNE plots comparing the latent space of ICAM (left) and ICAM-reg (right). Top row shows the separation of old and young classes. Bottom row shows the distribution binned for every 5 years. In each case the results are plotted for the test subjects of each model.</p>
    </caption>
    <graphic xlink:href="nihms-1888754-f0008" position="float"/>
  </fig>
  <fig position="float" id="F9">
    <label>Fig. 9.</label>
    <caption>
      <p id="P67">FA map robustness: ICAM-reg was run three times, with three different thresholds determining the classification splits. Columns 2–4 show FA mean maps for each experiment. Maps are shown for the same subject (aged 66 years) translated from old to young. Features stay broadly consistent with similar changes to key areas associated with healthy ageing: ventricles (blue arrows), hippocampus (pink arrows), and cortex (green arrows).</p>
    </caption>
    <graphic xlink:href="nihms-1888754-f0009" position="float"/>
  </fig>
  <fig position="float" id="F10">
    <label>Fig. 10.</label>
    <caption>
      <p id="P68">dHCP results. Here we show detection of punctate white matter lesions (yellow arrows) on previously unseen images by ICAM-reg.</p>
    </caption>
    <graphic xlink:href="nihms-1888754-f0010" position="float"/>
  </fig>
  <table-wrap position="float" id="T1">
    <label>TABLE I</label>
    <caption>
      <p id="P69">Biobank Generation Experiment Comparing Accuracy Score for Classification (Young Vs Old) of Real, ICAM Generated, and VA-GAN Generated Data. Note That Because VA-GAN Can Only Do Old to Young Translation, It Has Only 1 Result in the Table</p>
    </caption>
    <table frame="hsides" rules="none">
      <colgroup span="1">
        <col align="left" valign="top" span="1"/>
        <col align="left" valign="top" span="1"/>
        <col align="left" valign="top" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">Dataset</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Accuracy - young</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Accuracy - old</th>
        </tr>
        <tr>
          <th colspan="3" align="left" valign="top" rowspan="1">
            <hr/>
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Real</td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.938</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.859</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">ICAM (translated)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.822</td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.865</bold>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">VA-GAN (translated)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.122</td>
          <td align="left" valign="top" rowspan="1" colspan="1">N/A</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T2">
    <label>TABLE II</label>
    <caption>
      <p id="P70">ADNI Experiments Comparing Baselines With ICAM. Networks Are Compared Using Normalised Cross Correlation (NCC) Between the Absolute Values of the Attribution Maps and the Ground Truth Maps. The Positive NCC (+) Compares the Ground Truth Map to the FA Map When Translating Between Class 0 (MCI) to 1 (AD), and Vice Versa for the Negative NCC (−). Values Reported Are the Mean and Standard Deviation Across the Test Subjects</p>
    </caption>
    <table frame="hsides" rules="none">
      <colgroup span="1">
        <col align="left" valign="top" span="1"/>
        <col align="left" valign="top" span="1"/>
        <col align="left" valign="top" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">Network</th>
          <th align="left" valign="top" rowspan="1" colspan="1">NCC (−)</th>
          <th align="left" valign="top" rowspan="1" colspan="1">NCC (+)</th>
        </tr>
        <tr>
          <th colspan="3" align="left" valign="top" rowspan="1">
            <hr/>
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Guided Grad-CAM [<xref rid="R27" ref-type="bibr">27</xref>]</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.244 ± 0.047</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.339 ± 0.068</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Grad-CAM [<xref rid="R27" ref-type="bibr">27</xref>]</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.321 ± 0.059</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.461 ± 0.086</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Occlusion [<xref rid="R33" ref-type="bibr">33</xref>]</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.360 ± 0.037</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.354 ± 0.057</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Integrated gradients [<xref rid="R30" ref-type="bibr">30</xref>]</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.378 ± 0.064</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.404 ± 0.059</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">LRP [<xref rid="R32" ref-type="bibr">32</xref>]</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.390 ± 0.033</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.387 ± 0.039</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Guided backprop [<xref rid="R31" ref-type="bibr">31</xref>]</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.541 ± 0.054</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.532 ± 0.052</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">VA-GAN [<xref rid="R40" ref-type="bibr">40</xref>]</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.653 ± 0.142</td>
          <td align="left" valign="top" rowspan="1" colspan="1">N/A</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">ICAM-reg</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.655 ± 0.086</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.611 ± 0.059</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">ICAM</td>
          <td align="left" valign="top" rowspan="1" colspan="1"><bold>0.683</bold> ± <bold>0.097</bold></td>
          <td align="left" valign="top" rowspan="1" colspan="1"><bold>0.652</bold> ± <bold>0.083</bold></td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
