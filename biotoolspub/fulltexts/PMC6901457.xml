<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6901457</article-id>
    <article-id pub-id-type="publisher-id">54987</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-019-54987-1</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>An Efficient hybrid filter-wrapper metaheuristic-based gene selection method for high dimensional datasets</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2461-1143</contrib-id>
        <name>
          <surname>Pirgazi</surname>
          <given-names>Jamshid</given-names>
        </name>
        <address>
          <email>j.pirgazi@znu.ac.ir</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Alimoradi</surname>
          <given-names>Mohsen</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Esmaeili Abharian</surname>
          <given-names>Tahereh</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Olyaee</surname>
          <given-names>Mohammad Hossein</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label>Faculty of Engineering, Department of Computer Engineering, University of Gonabad, Gonabad, Iran </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0417 6900</institution-id><institution-id institution-id-type="GRID">grid.449392.1</institution-id><institution>Faculty of Electronic, Computer &amp; IT Department of Computer, </institution><institution>Qazvin Islamic Azad University, </institution></institution-wrap>Qazvin, Iran </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>9</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>9</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>9</volume>
    <elocation-id>18580</elocation-id>
    <history>
      <date date-type="received">
        <day>5</day>
        <month>9</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>22</day>
        <month>11</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Feature selection problem is one of the most significant issues in data classification. The purpose of feature selection is selection of the least number of features in order to increase accuracy and decrease the cost of data classification. In recent years, due to appearance of high-dimensional datasets with low number of samples, classification models have encountered over-fitting problem. Therefore, the need for feature selection methods that are used to remove the extensions and irrelevant features is felt. Recently, although, various methods have been proposed for selecting the optimal subset of features with high precision, these methods have encountered some problems such as instability, high convergence time, selection of a semi-optimal solution as the final result. In other words, they have not been able to fully extract the effective features. In this paper, a hybrid method based on the IWSSr method and Shuffled Frog Leaping Algorithm (SFLA) is proposed to select effective features in a large-scale gene dataset. The proposed algorithm is implemented in two phases: filtering and wrapping. In the filter phase, the Relief method is used for weighting features. Then, in the wrapping phase, by using the SFLA and the IWSSr algorithms, the search for effective features in a feature-rich area is performed. The proposed method is evaluated by using some standard gene expression datasets. The experimental results approve that the proposed approach in comparison to similar methods, has been achieved a more compact set of features along with high accuracy. The source code and testing datasets are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jimy2020/SFLA_IWSSr-Feature-Selection">https://github.com/jimy2020/SFLA_IWSSr-Feature-Selection</ext-link>.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Machine learning</kwd>
      <kwd>Microarrays</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par2">The basic issue about big data is a large number of features. Among the features available, only a few of them will be useful to distinguish samples that belong to different classes and many of the features are irrelevant, noise, or redundant. Irrelevant features do not necessarily lead to noise generation in big data analysis; they result in increasing the dimensions of the dataset and computational complexity in clustering and classification operations, and consequently they decrease the rate of classification accuracy. Therefore, it is necessary to select the appropriate features. In feature selection, the redundant features are usually removed from dataset because there is a subset of other features that can provide the information that is provided by these redundant features. On the other hand, noise features that do not provide any information about labels should also be removed because they will reduce the efficiency of the algorithm. Therefore, only relevant features which consist of significant information about given dataset will remain<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Consequently, a method for identifying diverse features, calculating relationships between features and selecting relevant features is needed through a huge amount of data.</p>
    <p id="Par3">For a dataset containing <italic>N</italic> number of features, there are 2<sup><italic>N</italic></sup> number of candidate subsets. The purpose of designing different feature selection methods has always been to find the most compressed subset with the highest precision among the candidate subsets. Considering the wide scope of possible solutions and increasing the size of this set of responses due to increment of the number of features exponentially, finding the best subset of N (medium or large) features is extremely costly. Computational complexity of selecting features is another major challenge for researchers<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>.</p>
    <p id="Par4">Different methods proposed for selecting a subset of features, have encountered some problems such as instability, high convergence time and falling in local optima as a final result, etc. Despite the success they have gained, they have not been able to extract the most effective features.</p>
    <p id="Par5">The feature selection methods are generally divided into four categories: filter methods, wrapper methods, hybrid methods and embedded methods. Each of these methods is described in detail<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>.</p>
    <p id="Par6">The filter methods for selection a subset of proper features use intrinsic and statistical characteristics of the features and they are independent of any learning algorithm. In these methods, weight is assigned to each feature based on the degree of relevance of features to class labels; correlation criteria and information theory-based criteria are used for weighting features usually. Due to the need for less computations, filter methods are effective for high-dimensional datasets, but they do not have the proper accuracy<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. The filter methods are divided into two groups of univariate and multivariate. In univariate methods, relevance of only one feature is measured according to the evaluation criterion. In these methods, dependencies between features do not play a role in the process of feature selection. The methods are such as: t-statistics (TS)<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>, Signal-to-Noise Ratio (SNR)<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>, and Pearson Correlation coefficient (PC)<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> and F-Test (FT)<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. In multivariate methods, the relationship between the features is considered. This makes these methods slower than univariate methods. The methods are such as: minimum Redundancy Maximum Relevance (mRMR)<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, Correlation based Feature Selection (CFS)<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, Fast Correlation Based Filter (FCBF)<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> and Mutual Information Feature Selection (MIFS)<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, Max-Relevance-Max-Distance (MRMD)<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>, Analysis of variance (ANOVA)<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> and F-Score<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. In these methods, the features are sorted based on their weights, and the features that have higher weights are selected as relevant features.</p>
    <p id="Par7">MRMD method considers distance between two kinds of features. This method is based on distance function to measure the independence of every feature. The higher the distance, the more the independence. Therefore, Pearson’s correlation coefficient is used to measure the relevance between features. Distance functions such Euclidean distance, Cosine distance and Tanimoto distance are exploited to calculate the redundancy.</p>
    <p id="Par8">In ANOVA, a method is proposed to improve the prediction accuracy of mitochondrial proteins of the malaria parasite. In this method, firstly, the protein samples are formulated using the <italic>g</italic>-gap dipeptide composition. Then, Analysis of variance is proposed to select the best subset of features. Finally, the support vector machine (SVM) is used to perform the prediction.</p>
    <p id="Par9">The most important defect of filter algorithms is the lack of utilization of the classification accuracy in selection of a subset of features. To solve this problem, new methods called wrapper methods are proposed. Wrapper methods use learning algorithms and a classifier to find a subset of features. In these methods, the learning model has the tasks of searching in the space of primary features and selecting the subset of the candidate features.</p>
    <p id="Par10">Also, the classifier is used to estimate the performance of the subset of the selected candidate features. Compared to the filter methods, the wrapper methods have higher computational costs and they are not suitable for high-dimensional datasets; however, they are more successful in finding the subset of effective features and the high accuracy of selecting a subset of features using these methods is noticeable<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. Many of wrapper methods have used heuristic search algorithms to find a subset of features. These methods start with a randomly generated solution, and in each iteration they are one step closer to the best subset of the solution. The evolutionary algorithms used in wrapper methods include Genetic Algorithm<sup><xref ref-type="bibr" rid="CR17">17</xref>–<xref ref-type="bibr" rid="CR19">19</xref></sup>, Simulated Annealing algorithm<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR21">21</xref></sup>, Ant Colony Optimization algorithm<sup><xref ref-type="bibr" rid="CR22">22</xref>–<xref ref-type="bibr" rid="CR24">24</xref></sup>, Shuffled Frog Leaping Algorithm<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>, Particle Swarm Optimization algorithm<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>, Binary Wolf Search Algorithm<sup><xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup> and so on.</p>
    <p id="Par11">Some methods use exhaustive searches. In<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, in order to select a subset of features, it first starts with a complete set of features, and then some of the features are removed by the first depth method. In<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, the features are selected using the beam search. This method arranges the features in a queue based on importance, and then all possible states are evaluated using beam searches. The main drawback of these methods is their computational complexity. Heuristic methods were proposed to solve this problem. Sequential feature selection methods such as Sequential Backward Selection (SBS)<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> and Sequential Forward Selection (SFS)<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, greedy methods such as hill-climbing<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, Bayesian search methods such as Bayesian features selection<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, meta-heuristic methods such as Ant Colony Optimization algorithm<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, and Genetic Algorithm<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, etc. are some methods that use heuristic search.</p>
    <p id="Par12">Another category of feature selection methods is Hybrid methods that combine filter and wrapper methods. So in the first step, based on a filter method, some features are selected based on importance. Then, in the selected features space, a wrapper method is applied to select the effective features<sup><xref ref-type="bibr" rid="CR37">37</xref>–<xref ref-type="bibr" rid="CR39">39</xref></sup>. In<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>, Incremental Wrapper Subset Selection (IWSS) is presented. In this method, after the weight of the features in the filter phase is calculated, the incremental algorithm is used to select the subset of features. First, the feature subset is empty. In the first iteration, the features are added with greater weights to the subset of features and the classifier is created based on the features and the dataset. The accuracy rate of classifier is stored as the best result. In next iterations, each time a feature with more weight is added to the subset, again the classifier is trained. If the recognition rate of the classifier is better than the one stored, the added feature is considered as a relevant feature and it is retained in the subset, otherwise the feature is removed from the subset. In<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>, the hybrid local search strategy embedded in the particle swarm optimization algorithm has been used to select relevant features. The purpose of the local search in this method is to optimize the particle swarm to select distinctive features based on their correlation information.</p>
    <p id="Par13">In<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>, a hybrid approach based on the Greedy Randomized Adaptive Search Procedure is proposed. In the first step, by using a filter method, the process of ranking the features is done, and features that have high degrees of relation with class labels are more weighty and less important features are less weighty. In the second stage, in the wrapper method, GRASP method is used to find the best subset. In the GRASP method, a subset of features is randomly selected based on their weights. Then in the next step by using the IWSS, SFS, IWSS, IWSSr, Hill-Climbing, and Best Agglomerative Ranked Subset (BARS)<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> methods; redundant and irrelevant features are removed. Use of an improvement phase is also considered in FICA<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. In this paper, at the filter step, features weighting is performed. Then, in the wrapper phase, using the Fuzzy Imperialist Competitive Algorithm (FICA) and the IWSSr algorithm, searching for effective features in the weighted feature space is done. In the other work, by using mutual information and adaptive genetic algorithm, gene expression data are classified<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. In this method, the features are ranked base on maximizing the mutual information and then, by using the adaptive genetic algorithm, the optimal subset of features is selected. In<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>, an effective hybrid gene selection method based on ReliefF and Ant colony optimization (ACO) algorithm for tumor classification is proposed. At first, ReliefF is used to estimate the weights of features according to how well their values distinguish between close instances. Then a new pruning rule based on ACO is designed to reduce dimensionality and obtain a new candidate subset with the smaller number of genes.</p>
    <p id="Par14">A two-step feature selection is proposed to exclude redundant and noise information for identifying origin of replication in Saccharomyces cerevisiae. In this method, at first, the weight of the features is calculated based on the F-score technique. Then, the MRMR technique is used to maximize the correlation between features and class labels while minimize the correlation between features and features<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>.</p>
    <p id="Par15">In the embedded methods, selecting the features subset is considered as a part of the model construction. This kind of methods can be considered as a search in the feature and model space; such as Adaboost<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>, random forest, and decision tree<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. SVM-RFE is also one of the embedded methods<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>. In this method, the algorithm starts with a set containing all features. In each iteration, the weight vector coefficients w is used to evaluate the features. Each element of this vector corresponds to a feature. In this case, the feature with the lowest score, ie, c<sub>i</sub> = (w<sub>i</sub>)<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, is removed. These weights indicate the relation of each feature with class label. Another algorithm proposed in this field is the KP-SVM algorithm<sup><xref ref-type="bibr" rid="CR50">50</xref>,<xref ref-type="bibr" rid="CR51">51</xref></sup>. The algorithm tries to find the appropriate features by updating the parameter σ in the RBF kernel.</p>
    <p id="Par16">In this paper, a hybrid method is proposed for selecting features in high dimensional datasets. In the proposed method, in the filter phase, the Relief method is used for weighting the features. Then, in the wrapper step, by using the SFLA and the IWSSr algorithm<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>, the search is performed to find the best subset of the features. The proposed method is evaluated with ten standard gene expression datasets. The results of the experiments confirm the effectiveness of the proposed approach in comparison with similar methods, in terms of Accuracy, Specificity, Sensitivity, Balance Rate and accessing to a subset of more compact features. The rest of the paper is organized as follows. Section 2 and 3 present an overview of the SFLA and IWSSr approaches and Section 4 describes the phases of the proposed method in detail. Section 5 provides the results of the method in the <italic>gene</italic> datasets. Finally, Section 6 summarizes the results.</p>
  </sec>
  <sec id="Sec2">
    <title>An Overview of the SFLA</title>
    <p id="Par17">SFLA is a new population-based metaheuristic optimization method that imitates the memetic evolution of a group of frogs when looking for a place with the maximum amount of available food. The SFLA has both definite and random strategies in finding the optimal response. The definite strategy allows the algorithm to use surface-level information efficiently in order to guide heuristic search. Random elements control the flexibility and power of the search pattern in the proposed method.</p>
    <p id="Par18">Inthis method, each frog is considered as a solution to the problem and a bunch of frogs forms a population that moves in order to reach a specific target. During the process of reaching the optimal answer, the population is divided into a number of subsets. The effects of the frogs in each subgroup modify the decision variables. After a certain number of evolutions, information is transmitted between the frogs during the process of combining subsets and forming a new population and a targeted search is carried out to determine the optimal answer. This trend continues until certain convergence conditions are established<sup><xref ref-type="bibr" rid="CR53">53</xref>,<xref ref-type="bibr" rid="CR54">54</xref></sup>.</p>
    <p id="Par19">In the SFLA, a primitive population of <italic>sfla_p</italic> frogs is randomly generated from possible answers. The position or situation of a frog is a possible solution to the problem. These frogs are implemented by vectors and structures to indicate the variables or problem solutions. In the algorithm, the entire initial population is first divided into sfla_m groups called memplex. Different memplexes that have <italic>sfla_n</italic> frogs are bunch of frogs that are individually searching for a solution in the search space. In each memplex, a submemplex is created to avoid falling in local optima<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. Each submemplex consists of <italic>sfla_q</italic> frogs and the frogs are selected randomly based on the following probability function:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{j}=\frac{2({\rm{sfla}}\_{\rm{n}}+1-{\rm{j}})}{{\rm{sfla}}\_{\rm{n}}\,({\rm{sfla}}\_{\rm{n}}+1)},\,j=1,2,\ldots ,sfla\_n$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">sfla</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="normal">n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">sfla</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="normal">n</mml:mi><mml:mspace width=".25em"/><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">sfla</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="normal">n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width=".25em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mo>_</mml:mo><mml:mi>n</mml:mi></mml:math><graphic xlink:href="41598_2019_54987_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>Where <italic>P</italic><sub><italic>j</italic></sub> is the probability of choosing jth frog for selection and <italic>sfla_n</italic> is the number of frogs in the memplex. Since in each memplex the frogs are sorted according to a descending order of fitness, by decreasing the fitness value, the probability of selecting frogs is lowered. Therefore, a better-positioned frog in the search space will have a greater chance of choosing as a member of the submemplex. In each submemplex, the worst frog (<italic>P</italic><sub><italic>w</italic></sub>), performs leaping based on its own experiences and the position of best frog in memplex (<italic>P</italic><sub><italic>b</italic></sub>). Therefore, the worst frog is first selected from the submemplex. The leaping step size for frog <italic>P</italic><sub><italic>w</italic></sub> is as follows:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\rm{S}}}_{{\rm{B}}}=\{\begin{array}{cc}\min \{{\rm{int}}({\rm{rand}}.[{{\rm{P}}}_{{\rm{b}}}-{{\rm{P}}}_{{\rm{w}}}]).\,{{\rm{S}}}_{{\rm{\max }}}\} &amp; {\rm{for}}\,{\rm{a}}\,{\rm{positive}}\,{\rm{step}}\\ \max \{{\rm{int}}({\rm{rand}}.[{{\rm{P}}}_{{\rm{b}}}-{{\rm{P}}}_{{\rm{w}}}]).-{{\rm{S}}}_{{\rm{\max }}}\} &amp; {\rm{for}}\,{\rm{a}}\,{\rm{nagative}}\,{\rm{step}}\end{array}$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">B</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>min</mml:mi><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mi mathvariant="normal">int</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">rand</mml:mi><mml:mo>.</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mspace width=".25em"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">for</mml:mi><mml:mspace width=".25em"/><mml:mi mathvariant="normal">a</mml:mi><mml:mspace width=".25em"/><mml:mi mathvariant="normal">positive</mml:mi><mml:mspace width=".25em"/><mml:mi mathvariant="normal">step</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>max</mml:mi><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mi mathvariant="normal">int</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">rand</mml:mi><mml:mo>.</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">for</mml:mi><mml:mspace width=".25em"/><mml:mi mathvariant="normal">a</mml:mi><mml:mspace width=".25em"/><mml:mi mathvariant="normal">nagative</mml:mi><mml:mspace width=".25em"/><mml:mi mathvariant="normal">step</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41598_2019_54987_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>Where <italic>rand</italic> is a random number in the range [0,1] and <italic>S_max</italic> is the maximum leap length. In the next step, the worst frog position is edited by the following equation:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\rm{P}}}_{{\rm{w}}}^{\text{'}}={{\rm{P}}}_{{\rm{w}}}+{{\rm{S}}}_{{\rm{B}}}$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow><mml:mrow><mml:mo>'</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">B</mml:mi></mml:mrow></mml:msub></mml:math><graphic xlink:href="41598_2019_54987_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
    <p id="Par20">If the new frog (<inline-formula id="IEq1"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{w}^{\text{'}}$$\end{document}</tex-math><mml:math id="M8"><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo>'</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2019_54987_Article_IEq1.gif"/></alternatives></inline-formula>) is better than the original frog, this frog is replaced with the original frog, otherwise the <italic>P</italic><sub><italic>w</italic></sub> frog is edited according to the best frog of the total population (<italic>P</italic><sub><italic>G</italic></sub>) according to the following:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\rm{S}}}_{{\rm{G}}}=\{\begin{array}{cc}\min \{{\rm{int}}({\rm{rand}}.[{{\rm{P}}}_{{\rm{G}}}-{{\rm{P}}}_{{\rm{w}}}]).\,{{\rm{S}}}_{{\rm{\max }}}\} &amp; {\rm{for}}\,{\rm{a}}\,{\rm{positive}}\,{\rm{step}}\\ \max \{{\rm{int}}({\rm{rand}}.[{{\rm{P}}}_{{\rm{G}}}-{{\rm{P}}}_{{\rm{w}}}]).-{{\rm{S}}}_{{\rm{\max }}}\} &amp; {\rm{for}}\,{\rm{a}}\,{\rm{nagative}}\,{\rm{step}}\end{array}$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>min</mml:mi><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mi mathvariant="normal">int</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">rand</mml:mi><mml:mo>.</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mspace width=".25em"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">for</mml:mi><mml:mspace width=".25em"/><mml:mi mathvariant="normal">a</mml:mi><mml:mspace width=".25em"/><mml:mi mathvariant="normal">positive</mml:mi><mml:mspace width=".25em"/><mml:mi mathvariant="normal">step</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>max</mml:mi><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mi mathvariant="normal">int</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">rand</mml:mi><mml:mo>.</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">for</mml:mi><mml:mspace width=".25em"/><mml:mi mathvariant="normal">a</mml:mi><mml:mspace width=".25em"/><mml:mi mathvariant="normal">nagative</mml:mi><mml:mspace width=".25em"/><mml:mi mathvariant="normal">step</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41598_2019_54987_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P^{\prime\prime} }_{w}={P}_{w}+{S}_{G}$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:msub><mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mo>″</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:math><graphic xlink:href="41598_2019_54987_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
    <p id="Par21">Similar to the previous one, if the <inline-formula id="IEq2"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P^{\prime\prime} }_{w}$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mo>″</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2019_54987_Article_IEq2.gif"/></alternatives></inline-formula> frog is better than the original frog (<italic>P</italic><sub><italic>w</italic></sub>), this frog is replaced with the <inline-formula id="IEq3"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P^{\prime\prime} }_{w}$$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mo>″</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2019_54987_Article_IEq3.gif"/></alternatives></inline-formula> frog and if neither of these is satisfied, a new random frog is replaced with the worst frog of submemplex. After the <italic>IT</italic><sub><italic>mem</italic></sub> steps of dividing memplex into submemplexes, again all the frogs are combined and re-divided into <italic>sfla_m</italic> memplexes. This operation continues to meet the end conditions of the program. The pseudo code of SFLA is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Based on this algorithm, the worst frog can leap toward the best frog. By repeating this process, gradually the average fitness of the frog population increases during the evolutionary stages and converges to a certain degree. With respect to this process, <italic>P</italic><sub><italic>G</italic></sub> and <italic>P</italic><sub><italic>w</italic></sub> are changed in each iteration and the value of fitness increases to converge to the desired response<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>.<fig id="Fig1"><label>Figure 1</label><caption><p>Pseudo code of SFLA.</p></caption><graphic xlink:href="41598_2019_54987_Fig1_HTML" id="d29e1137"/></fig></p>
  </sec>
  <sec id="Sec3">
    <title>An Overview of IWSSr Algorithm</title>
    <p id="Par22">IWSSr algorithm<sup><xref ref-type="bibr" rid="CR52">52</xref></sup> that is an extension of IWSS algorithm, is one of the wrapper-based features subset selection algorithms. In this method, first, in the filter phase, the relevance of each feature with the class labels is calculated and a weight is assigned to each feature. In IWSSr, the SU criterion is used for weighting features. SU is a nonlinear information theory based criterion. This criterion evaluates each feature independently and it assigns to each feature a number in the range [0,1] indicating the weight of each feature based on its relevance to class labels. A large number indicates the high importance of the feature. This criterion is calculated as follows:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S{U}_{i,c}({F}_{i},C)=2\frac{H({F}_{i})-H({F}_{i}|C)}{H({F}_{i})-H(C)}$$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mi>S</mml:mi><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41598_2019_54987_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>Where <italic>C</italic> is the class label, <italic>F</italic><sub><italic>i</italic></sub> represents ith feature and <italic>H</italic> indicates entropy. In the following, at wrapper phase, the features are arranged in descending order by weights. Then an incremental mechanism is used to select a subset of features. Figure <xref rid="Fig2" ref-type="fig">2</xref> shows the pseudo code of IWSSr algorithm. In this algorithm, <italic>S</italic> is the subset of selected features. At first, the candidate subset is empty and in first iteration, the feature that has the highest score is added to the candidate subset.<fig id="Fig2"><label>Figure 2</label><caption><p>Pseudo code of IWSSr.</p></caption><graphic xlink:href="41598_2019_54987_Fig2_HTML" id="d29e1282"/></fig></p>
    <p id="Par23">Then a classifier is trained based on the candidate subset and the existing training data. The classification accuracy is maintained as the best result. The next step is done in two phases. In the first phase, a feature with a high score that has not been evaluated yet, is replaced with each feature in the candidate subset. After each replacement, a new classifier is trained by using the obtained subset. then the classification accuracy is calculated. If the addition of a new feature causes increase in classification accuracy compared to the previous subset, the result is maintained as the best. In this way, the dependence of this feature with all previous selected features is measured and if it does not depend on any of the selected features, it will be added to the candidate subset.</p>
    <p id="Par24">In the second phase, the feature that is under review (the feature that was replaced with the features in the selected subset in the first phase) is added to the selected subset <italic>S</italic> (which was obtained in the previous stage) and a new classifier is trained based on the new subset and the classification accuracy is calculated. If the accuracy of the subset is higher than the accuracy of the candidate subset of the first phase, it is maintained as the best result. After the first and second phases, if we have achieved a better subset in each of these phases, the optimal subset is selected as the subset of this iteration and the feature is applied to the selected subset.</p>
  </sec>
  <sec id="Sec4" sec-type="materials|methods">
    <title>Materials and Methods</title>
    <p id="Par25">The proposed algorithm is a feature selection system called IWSSr and Shuffled Frog Leaping Algorithm (IWSSr-SFLA). In this paper, a hybrid method is proposed for selecting features in high dimensional datasets. In the proposed method in the filter phase, the Relief method is used for weighting the features. Then, in the wrapping phase, by using the combination of Shuffled Frog Leaping Algorithm and the IWSSr algorithm, the search is performed to find the best subset of features.</p>
    <p id="Par26">In the first phase, the Relief method, estimates the quality of features according to how well their values distinguish between instances that are near to each other. The Relief method calculates the correlation between features found by nearest-neighbor algorithm. Its output is a set containing weights of features<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>. It arranges the set in descending order. Figure <xref rid="Fig3" ref-type="fig">3</xref> shows the general scheme of the Relief algorithm<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>.<fig id="Fig3"><label>Figure 3</label><caption><p>The general scheme of the Relief algorithm.</p></caption><graphic xlink:href="41598_2019_54987_Fig3_HTML" id="d29e1316"/></fig></p>
    <p id="Par27">As we can see in Fig. <xref rid="Fig3" ref-type="fig">3</xref>, at first, one sample is randomly selected, then its two neighbors are searched. One neighbor along with selected sample are in a same class and the other neighbor is in a different class. Function Diff(A,R,H) calculates the difference between the values of the feature <italic>A</italic> and the first neighbor, and Diff(A,R,M)) calculates the difference between the values of the feature <italic>A</italic> and the second neighbor. then the weight of each feature is updated. For discrete features the difference is 1 (when the values are different) and 0 (when the values are the same). For continuous features, the difference is the normalized value of the real difference of two values of feature, in the range of [0,1]. The Relief algorithm works well for noisy or correlated features. It depends on the number of features and the number of samples in the dataset. It is noticeable to point that the time complexity of the algorithm is linear.</p>
    <p id="Par28">In the wrapping phase, a primary population of frogs is initially created, each containing a subset of the features. In order to find the best subset for a more efficient classification, the primary population should be trained. After some learning phases, the best frog (which is closest to the target) is selected as a solution. At each training phase, the entire population is first divided into a number of memplexes.</p>
    <p id="Par29">In each memplex, a submemplex is selected and in this category the worst frog is initially trained or leaped towards the best frog of the memplex. If the better frog is created, this frog is replaced with the worst frog. Otherwise, the worst frog will be leaped according to the best frog of the entire set. This time, As the previous stage, if the frog is improved, it is replaced, and if not, a new frog is created. After creating the new frog randomly, the replacement of the new frog is done if its fitness is better than the original frog, otherwise the original frog is remaining unchanged. The division of the memplexes into submemplexes is repeated <italic>IT</italic><sub><italic>mem</italic></sub> times. After completing the learning phases, the whole set and the best frog get closer to the goal.</p>
    <sec id="Sec5">
      <title>Initial population creation</title>
      <p id="Par30">In the proposed algorithm, an Initial population with the number of <italic>sfla_p</italic> frogs is initially created randomly. Each frog has a subset of features for classifying data. Therefore any of the frogs will be a solution to the problem. In the initial population, a random percentage of the features are selected based on the weights assigned to them in the filtering phase. Due to random weighted selection, high weight features are more likely to be selected. Figure <xref rid="Fig4" ref-type="fig">4</xref> shows how to create the frogs in the proposed algorithm.<fig id="Fig4"><label>Figure 4</label><caption><p>Create frogs in the proposed algorithm.</p></caption><graphic xlink:href="41598_2019_54987_Fig4_HTML" id="d29e1357"/></fig></p>
    </sec>
    <sec id="Sec6">
      <title>Evaluation of the initial population</title>
      <p id="Par31">After selecting the features for each frog, the reduntant features of each frog are removed by using the IWSSr algorithm and after applying this algorithm, the cost of each frog is calculated. The initial population is evaluated using a quality check function. The frog, which includes more relevant features, earns a higher value of fitness.<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\rm{F}}=(\frac{{\rm{TP}}}{{\rm{TP}}+{\rm{FN}}}+\frac{{\rm{TN}}}{{\rm{TN}}+{\rm{FP}}})/2$$\end{document}</tex-math><mml:math id="M20" display="block"><mml:mi mathvariant="normal">F</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FN</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">TN</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FP</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:math><graphic xlink:href="41598_2019_54987_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>Where <italic>TN</italic> is the number of negative samples which are correctly classified. <italic>FN</italic> is the number of positive samples identified as negative samples. <italic>TP</italic> is the number of positive samples which are correctly classified. <italic>FP</italic> is the number of negative samples identified as positive.</p>
    </sec>
    <sec id="Sec7">
      <title>Termination conditions of the program</title>
      <p id="Par32">The termination conditions refer to the user-defined conditions. The conditions can be a user-defined constant number of iterations for training, reaching the maximum percentage of diagnosis or not changing the entire population. In the experiments, after <italic>IT</italic><sub><italic>max</italic></sub> iterations, the learning process is terminated.</p>
    </sec>
    <sec id="Sec8">
      <title>Division of memplexes into submemplexes</title>
      <p id="Par33">In each memplex which has <italic>sfla_n</italic> frogs, a submemplex is created that contains <italic>sfla_q</italic> frogs. To do this, frogs of memplex are sorted by descending value of fitness. The probability of choosing each frog in submemplex is calculated based on Eq. (<xref rid="Equ1" ref-type="">1</xref>). Therefore, the submemplex is created based on the fitness of each frog.</p>
    </sec>
    <sec id="Sec9">
      <title>Leap or improve the frog</title>
      <p id="Par34">After each submemplex creation, the worst frog position (<italic>P</italic><sub><italic>w</italic></sub>) is edited based on the position of the best frog of the memplex (<italic>P</italic><sub><italic>b</italic></sub>) (or the best frog of the total population (<italic>P</italic><sub><italic>G</italic></sub>)). This edition is called leaping. Therefore, the leaping in the SLFA is an operation in which, the frog with a lower fitness can be improved according to a frog which has better fitness. The leaping action can vary depending on different issues. The improvement phase of the worst frog which is indicated by the IWF as shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>, is illustrated as a flowchart in Fig. <xref rid="Fig6" ref-type="fig">6</xref>.<fig id="Fig5"><label>Figure 5</label><caption><p>Pseudo code of the proposed hybrid algorithm.</p></caption><graphic xlink:href="41598_2019_54987_Fig5_HTML" id="d29e1484"/></fig><fig id="Fig6"><label>Figure 6</label><caption><p>Leap algorithm for worst frog Improvement (F<sub>w</sub>) by the help of better frog (F<sub>b</sub>) (IWF).</p></caption><graphic xlink:href="41598_2019_54987_Fig6_HTML" id="d29e1499"/></fig></p>
      <p id="Par35">To improve the worst frog (<italic>P</italic><sub><italic>w</italic></sub>) according to better frog in the memplex (<italic>P</italic><sub><italic>b</italic></sub>), at first, the number of features that are removed from or added to the frog is calculated using the following equation:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\rm{S}}}_{{\rm{b}}}=\{\begin{array}{cc}\min \{{\rm{int}}({\rm{rand}}[{{\rm{SP}}}_{{\rm{b}}}-{{\rm{SP}}}_{{\rm{w}}}]),{{\rm{S}}}_{{\rm{\max }}}\} &amp; {\rm{if}}\,{{\rm{SP}}}_{{\rm{b}}} &gt; {{\rm{SP}}}_{{\rm{w}}}\\ \max \{{\rm{int}}({\rm{rand}}[{{\rm{SP}}}_{{\rm{b}}}-{{\rm{SP}}}_{{\rm{w}}}]),-{{\rm{S}}}_{{\rm{\max }}}\} &amp; {\rm{else}}\end{array}$$\end{document}</tex-math><mml:math id="M22" display="block"><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>min</mml:mi><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mi mathvariant="normal">int</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">rand</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">SP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">SP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">if</mml:mi><mml:mspace width=".25em"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">SP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">SP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>max</mml:mi><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mi mathvariant="normal">int</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">rand</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">SP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">SP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">else</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41598_2019_54987_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>Where <italic>SP</italic><sub><italic>w</italic></sub> and <italic>SP</italic><sub><italic>b</italic></sub> are the number of features in the worst and better frogs respectively. <italic>rand</italic> is a random number in the range of [0,1] and <italic>S</italic><sub><italic>max</italic></sub> is the maximum number of feature changes allowed. In order to make changes in the worst frog, at first, according to the SU criterion, the features of the worst and better frogs are arranged. Then, if <italic>S</italic><sub><italic>b</italic></sub> is a positive number, then <italic>S</italic><sub><italic>b</italic></sub> features are randomly added to the worst frog from the better frog. In this case, the features that have high weights are more likely to be selected. Similarly, if <italic>S</italic><sub><italic>b</italic></sub>is negative, then <italic>S</italic><sub><italic>b</italic></sub> features are randomly deleted from the worst frog. In this case, features that are less weighted are more likely to be selected. In the next step, by using the IWSSr algorithm, the reduntant features of the worst frogs are removed.</p>
    </sec>
  </sec>
  <sec id="Sec10">
    <title>Results and Discussions</title>
    <sec id="Sec11">
      <title>Datasets</title>
      <p id="Par36">In order to evaluate the proposed method, the experiments are performed by MATLAB software on ten gene expression datasets. Summary of the datasets are given in Table <xref rid="Tab1" ref-type="table">1</xref>. Each dataset is descripted as follows:<table-wrap id="Tab1"><label>Table 1</label><caption><p>Microarray data sets used in the experiments.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Data set</th><th>Original Data</th><th>Training Data</th><th>Independent Data</th><th>#Gene</th><th>#Classes</th><th>#class1</th><th>#class2</th></tr></thead><tbody><tr><td>Colon</td><td>62</td><td>50</td><td>12</td><td>2000</td><td>2</td><td>40</td><td>22</td></tr><tr><td>Arcene</td><td>100</td><td>80</td><td>20</td><td>10000</td><td>2</td><td>44</td><td>56</td></tr><tr><td>Prostate1</td><td>88</td><td>71</td><td>17</td><td>12625</td><td>2</td><td>38</td><td>50</td></tr><tr><td>DLBCL</td><td>77</td><td>61</td><td>16</td><td>11226</td><td>2</td><td>58</td><td>19</td></tr><tr><td>Lung</td><td>181</td><td>145</td><td>36</td><td>12533</td><td>2</td><td>150</td><td>31</td></tr><tr><td>Dorothea</td><td>800</td><td>640</td><td>160</td><td>100000</td><td>2</td><td>610</td><td>190</td></tr><tr><td>Prostate</td><td>136</td><td>109</td><td>27</td><td>12600</td><td>2</td><td>77</td><td>59</td></tr><tr><td>CNS</td><td>60</td><td>48</td><td>12</td><td>7129</td><td>2</td><td>21</td><td>39</td></tr><tr><td>Leukemia</td><td>72</td><td>58</td><td>14</td><td>7129</td><td>2</td><td>47</td><td>25</td></tr><tr><td>Breast</td><td>97</td><td>78</td><td>19</td><td>24481</td><td>2</td><td>51</td><td>46</td></tr></tbody></table></table-wrap></p>
      <p id="Par37">Prostate dataset: This dataset contains 12600 genes for 136 samples. 77 samples include prostate tumor and 59 samples are normal<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. Colon dataset: This dataset contains 2000 genes and 62 samples. 40 samples contain colon cancer and 22 samples are normal<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. Central Nervous System dataset (CNS): This dataset contains 7129 genes and 60 samples. The dataset includes 21 benign samples and 39 malignant samples<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>. Dfiuse Large b-cell lymphoma dataset (DLBCL): This dataset contains 11226 genes for 77 samples. 58 samples including lymphoma tissue, are large cell B, and 19 samples of lymphoma tissue are Follicular lymphoma<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>. Dorothea dataset: This dataset contains 100,000 features and 800 samples. 190 samples are positive and 610 are negative<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. Leukemia dataset: This dataset contains 7129 genes and 72 samples. Diseases of the leukemia collection are divided into two categories of Acute Lymphoblastic Leukemia (ALL) and Acute Myelogenous Leukemia (AML). The dataset consists of 47 ALL samples and 25 AML samples<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>. Arcene dataset: This dataset contains 10,000 genes and 100 samples. This dataset consists of 56 cancer samples and 44 normal samples<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. Lung cancer: Gene expression dataset for lung cancer classification between two classes: adenocarcinoma (ADCA); malignant pleural mesothe-lioma (MPM). The lung dataset contains 181 tissue samples (150 ADCA and 31 MPM). Each sample is described by 12533 genes<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>. Breast cancer: Patients outcome prediction for breast cancer. The training data contains 97 patient samples, 46 of which are from patients who had developed distance metastases within 5 years (labelled as “relapse”), the rest 51 samples are from patients who remained healthy from the disease after their initial diagnosis for interval of at least 5 years (labelled as “non-relapse”). In this data,the number of genes are 24481<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. Prostate1 dataset: This dataset contains expression levels of 12625 genes taken over 88 samples. (38 normal samples and 50 abnormal)<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>.</p>
    </sec>
    <sec id="Sec12">
      <title>Performance metrics</title>
      <p id="Par38">To compare the results of the proposed method, seven hybrid methods LFS, IWSS, IWSSr, BARS, GRASP, SVM-RFE and FICA and three filter methods FCBF<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, F-Score and PCA<sup><xref ref-type="bibr" rid="CR51">51</xref></sup> have been used. The PCA method has been proposed for high-dimensional datasets in recent years. To demonstrate the performance of the proposed method some metrics such as, the number of features obtained, the number of evaluations performed to reach the final subset, accuracy, specificity, sensitivity, and balance rate according to the following formula are measured<sup><xref ref-type="bibr" rid="CR64">64</xref>,<xref ref-type="bibr" rid="CR65">65</xref></sup>. The number of evaluations indicates the number of subsets tested to reach the final subset.<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\rm{Accuracy}}=\frac{TP+TN}{TP+TN+FP+FN}$$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mi mathvariant="normal">Accuracy</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41598_2019_54987_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\rm{Specificity}}=\frac{TN}{TN+FP}$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mi mathvariant="normal">Specificity</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41598_2019_54987_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\rm{Sensitivity}}=\frac{TP\,}{TP+FN}$$\end{document}</tex-math><mml:math id="M28" display="block"><mml:mi mathvariant="normal">Sensitivity</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mspace width=".25em"/></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41598_2019_54987_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\rm{BR}}=\frac{{\rm{Specificity}}+{\rm{Sensitivity}}\,}{2}$$\end{document}</tex-math><mml:math id="M30" display="block"><mml:mi mathvariant="normal">BR</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Specificity</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Sensitivity</mml:mi><mml:mspace width=".25em"/></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:math><graphic xlink:href="41598_2019_54987_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par39">The classifier used in the proposed method is support vector machine and in the methods to be compared, Bayesian classifier is used.</p>
      <p id="Par40">When using feature selection methods, it is important to make sure that there is no overlap between the training and test data. Cross validation is an approach that puts data into categories effectively to evaluate feature selection and classification methods. In this approach, the efficiency of the proposed methods is evaluated on the basis of a number of categories derived from the original data. At first, the whole samples of a dataset are randomly divided into k categories for training and testing purposes. In k steps, (k-1) batches are used for model training and one batch is used for testing. At each step, the features and parameters used to test the model are obtained from the training stage and with the help of samples in the training categories. Finally, the efficiency of the proposed method is obtained based on the k outputs of the training and testing phases<sup><xref ref-type="bibr" rid="CR66">66</xref>,<xref ref-type="bibr" rid="CR67">67</xref></sup>.</p>
      <p id="Par41">In this paper, Cross Validation (CV) method is used to train and then test the support vector machine classifier based on selected features to determine the percentage of recognition of test data, where k = 10. Since in the 10-fold CV method, the samples are randomly divided into 10 categories, the results depend on how the samples are grouped. To solve this problem, the samples are randomly divided into 10 groups 10 times.</p>
      <p id="Par42">The final number of features is equal to the average of selected features and other criteria are equal to the average of the criteria in selected subset after 10 times execution of proposed method. The performance criteria of the proposed method is also obtained based on the average of 10-fold CV repetitions.</p>
      <p id="Par43">The initial value of hyper parameters of the proposed method is given in Table <xref rid="Tab2" ref-type="table">2</xref>. All hyper parameters are selected based on multiple tests and they are identical in all datasets. To determine the value of hyper parameters, the Random search method is used. For this purpose, a set of hyper parameters is chosen and the model is built based on training data and then it is evaluated based on evaluation data. This process is repeated with other hyper parameters. The hyper parameters that report the best accuracy are selected. In this paper, Population size is set from 80 to 120, Number of memplexes is set from 8 to 12, Population size of submemplexes is set from 3 to 6. The maximum leap length allowed to change (S<sub>max</sub>) is set from 3 to 8.<table-wrap id="Tab2"><label>Table 2</label><caption><p>SFLA parameters used in the problem.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Parameter</th><th>Value</th><th>Comments</th></tr></thead><tbody><tr><td><italic>sfla_p</italic></td><td>100</td><td><italic>Population size</italic></td></tr><tr><td><italic>sfla_m</italic></td><td>10</td><td>Number of memplexes</td></tr><tr><td><italic>sfla_n</italic></td><td>10</td><td>Population size of each memplex</td></tr><tr><td><italic>sfla_q</italic></td><td>4</td><td>Population size of submemplexes</td></tr><tr><td><italic>IT</italic><sub><italic>max</italic></sub></td><td>40</td><td>Total Iteration number</td></tr><tr><td><italic>IT</italic><sub><italic>mem</italic></sub></td><td>10</td><td>The number of replications of the division of memplexes into submemplexes</td></tr><tr><td><italic>S</italic><sub><italic>max</italic></sub></td><td>5</td><td>The maximum leap length allowed to change</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec13">
      <title>Experimental results</title>
      <p id="Par44">In Tables <xref rid="Tab3" ref-type="table">3</xref> and <xref rid="Tab4" ref-type="table">4</xref>, the results of the implementation of the proposed method have been shown along with comparative methods. In this following tables, acc refers to the accuracy and atts refers to the attribute. According to Table <xref rid="Tab3" ref-type="table">3</xref>, the results approve that the BARS method has fewer features and better accuracy than other methods.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Result of feature selection algorithm.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">DataSet</th><th colspan="2">IWSS</th><th colspan="2">IWSSr</th><th colspan="2">LFS</th><th colspan="2">BARS</th><th colspan="2">FCBF</th><th colspan="2">PCA</th></tr><tr><th>Acc</th><th>Atts</th><th>Acc</th><th>Atts</th><th>Acc</th><th>Atts</th><th>Acc</th><th>Atts</th><th>Acc</th><th>Atts</th><th>Acc</th><th>Atts</th></tr></thead><tbody><tr><td>Colon</td><td>80.65</td><td>3.8</td><td>83.87</td><td>2.8</td><td>80.80</td><td>4.1</td><td>85.70</td><td>3.0</td><td>77.40</td><td>14.6</td><td>72.50</td><td>28.9</td></tr><tr><td>Arcene</td><td>70.00</td><td>13.4</td><td>72.00</td><td>6.2</td><td>73.00</td><td>4.5</td><td>74.00</td><td>4.9</td><td>70.00</td><td>34.2</td><td>—</td><td>—</td></tr><tr><td>Prostate1</td><td>76.23</td><td>12.8</td><td>77.42</td><td>8.3</td><td>73.12</td><td>3.6</td><td>85.34</td><td>4.1</td><td>63.12</td><td>32.4</td><td>59.12</td><td>37.1</td></tr><tr><td>DLBCL</td><td>83.11</td><td>3.2</td><td>81.23</td><td>2.7</td><td>88.67</td><td>4.1</td><td>75.21</td><td>2.8</td><td>96.45</td><td>56.2</td><td>68.11</td><td>42.7</td></tr><tr><td>Lung</td><td>97.20</td><td>2.7</td><td>97.20</td><td>2.4</td><td>93.60</td><td>2.5</td><td>98.30</td><td>3.0</td><td>99.40</td><td>115.2</td><td>85.61</td><td>125.2</td></tr><tr><td>Dorothea</td><td>93.50</td><td>7.4</td><td>92.90</td><td>6.3</td><td>90.30</td><td>5.5</td><td>93.80</td><td>7.3</td><td>92.60</td><td>92.8</td><td>—</td><td>—</td></tr><tr><td>Prostate</td><td>77.90</td><td>11.1</td><td>78.70</td><td>7.0</td><td>75.40</td><td>4.5</td><td>86.80</td><td>3.7</td><td>61.30</td><td>35.8</td><td>57.35</td><td>36.6</td></tr><tr><td>CNS</td><td>85.21</td><td>3.2</td><td>86.10</td><td>3.1</td><td>83.23</td><td>3.4</td><td>89.12</td><td>2.8</td><td>93.24</td><td>42.2</td><td>77.32</td><td>44.1</td></tr><tr><td>Leukemia</td><td>87.50</td><td>2.5</td><td>87.50</td><td>3.0</td><td>93.00</td><td>3.2</td><td>90.50</td><td>2.3</td><td>95.80</td><td>45.8</td><td>79.10</td><td>53.8</td></tr><tr><td>Breast</td><td>69.21</td><td>11.1</td><td>70.21</td><td>9.2</td><td>70.43</td><td>10.1</td><td>72.81</td><td>9.34</td><td>69.43</td><td>107.3</td><td>63.10</td><td>96.3</td></tr><tr><td><bold>Mean</bold></td><td><bold>82.05</bold></td><td><bold>7.122</bold></td><td><bold>82.71</bold></td><td><bold>5.1</bold></td><td><bold>82.15</bold></td><td><bold>4.55</bold></td><td><bold>85.15</bold></td><td><bold>4.32</bold></td><td><bold>81.87</bold></td><td><bold>57.65</bold></td><td><bold>70.27</bold></td><td><bold>58.09</bold></td></tr></tbody></table></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>Comparison of proposed method with GRASP and FICA.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2"/><th colspan="2">Grasp + HC</th><th colspan="2">Grasp + IWSS</th><th colspan="2">Grasp + IWSSr</th><th colspan="2">Grasp + BARS</th><th colspan="2">Grasp + SFS</th><th colspan="2">FICA + IWSSr</th><th colspan="2">F-Score</th><th colspan="2">SVM-RFE</th><th colspan="2">Proposed method</th></tr><tr><th>Acc</th><th>Atts</th><th>Acc</th><th>Atts</th><th>Acc</th><th>Atts</th><th>Acc</th><th>Atts</th><th>Acc</th><th>Atts</th><th>Acc</th><th>Atts</th><th>Acc</th><th>Atts</th><th>Acc</th><th>Atts</th><th>Acc</th><th>Atts</th></tr></thead><tbody><tr><td>Colon</td><td>81.10</td><td>3.0</td><td>79.60</td><td>3.4</td><td>82.20</td><td>3.1</td><td>80.00</td><td>2.9</td><td>80.00</td><td>3.5</td><td>93.60</td><td>4.5</td><td>83.74</td><td>55</td><td>93.70</td><td>9.8</td><td>94.72</td><td>5.3</td></tr><tr><td>Arcene</td><td>80.00</td><td>5.7</td><td>79.30</td><td>6.0</td><td>78.50</td><td>5.7</td><td>79.00</td><td>5.2</td><td>79.30</td><td>6.3</td><td>93.40</td><td>7.1</td><td>73.25</td><td>110</td><td>89.11</td><td>13.5</td><td>95.16</td><td>8.5</td></tr><tr><td>Prostate1</td><td>80.45</td><td>4.3</td><td>79.12</td><td>4.1</td><td>78.49</td><td>3.7</td><td>81.12</td><td>4.7</td><td>78.43</td><td>6.3</td><td>—</td><td>—</td><td>68.74</td><td>105</td><td>82.71</td><td>17.2</td><td>88.52</td><td>8.2</td></tr><tr><td>DLBCL</td><td>85.65</td><td>2.1</td><td>84.60</td><td>2.2</td><td>85.61</td><td>2.1</td><td>89.11</td><td>2.2</td><td>85.70</td><td>2.4</td><td>99.10</td><td>4.5</td><td>93.11</td><td>100</td><td>95.23</td><td>15.7</td><td>99.21</td><td>6.8</td></tr><tr><td>Lung</td><td>95.60</td><td>2.2</td><td>95.08</td><td>2.2</td><td>95.70</td><td>2.4</td><td>96.02</td><td>2.3</td><td>96.20</td><td>2.4</td><td>98.90</td><td>3</td><td>82.16</td><td>105</td><td>98.73</td><td>9.4</td><td>99.16</td><td>5.6</td></tr><tr><td>Dorothea</td><td>93.30</td><td>3.7</td><td>93.30</td><td>4.2</td><td>92.90</td><td>3.8</td><td>93.50</td><td>5.0</td><td>93.20</td><td>4.4</td><td>75.80</td><td>3</td><td>76.24</td><td>310</td><td>84.32</td><td>21.7</td><td>91.43</td><td>7.2</td></tr><tr><td>Prostate</td><td>77.80</td><td>5.0</td><td>78.60</td><td>5.7</td><td>77.50</td><td>4.6</td><td>78.60</td><td>5.1</td><td>78.10</td><td>5.6</td><td>92.40</td><td>4.4</td><td>54.33</td><td>250</td><td>92.20</td><td>14.4</td><td>94.18</td><td>7.8</td></tr><tr><td>CNS</td><td>91.46</td><td>2.6</td><td>93.12</td><td>2.8</td><td>87.32</td><td>2.8</td><td>92.14</td><td>3.1</td><td>91.12</td><td>3.1</td><td>—</td><td>—</td><td>66.53</td><td>90</td><td>76.96</td><td>16.3</td><td>95.64</td><td>6.7</td></tr><tr><td>Leukemia</td><td>92.60</td><td>2.7</td><td>93.70</td><td>2.7</td><td>91.60</td><td>2.8</td><td>93.30</td><td>2.8</td><td>93.60</td><td>3.3</td><td>99.60</td><td>1.8</td><td>75.57</td><td>70</td><td>100.00</td><td>8.6</td><td>99.62</td><td>5.2</td></tr><tr><td>Breast</td><td>79.63</td><td>4.3</td><td>80.11</td><td>3.1</td><td>78.38</td><td>3.5</td><td>81.24</td><td>2.7</td><td>80.91</td><td>3.6</td><td>—</td><td>—</td><td>73.82</td><td>120</td><td>86.09</td><td>17.3</td><td>88.17</td><td>10.2</td></tr><tr><td>Mean</td><td>85.75</td><td>3.56</td><td>85.65</td><td>3.64</td><td>84.82</td><td>3.45</td><td>86.40</td><td>3.60</td><td>85.65</td><td>4.09</td><td>—</td><td>—</td><td>74.74</td><td>131.5</td><td>89.90</td><td>14.39</td><td>93.34</td><td>7.12</td></tr></tbody></table></table-wrap></p>
      <p id="Par45">The main idea behind this approach is based on relevancy and redundancy; so the features are added to the selected set that have better information for the classification of the data. The results show that the LFS<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> method has fewer features, but does not have good accuracy. Due to the use of only 100 filtered features to select the subset of features in the wrapper phase, the relationship between the features cannot be considered. The IWSS and IWSSr are wrapper methods. Although the IWSS method finds the subset fast because of relying on the univariate ranking of features, does not consider the relationship between the features. It often fails to find redundant features and the average number of features found by this method is high. In the IWSSr method, in each step of the implementation, the dependence of the assessed feature with all of the features in the selected subset is examined.</p>
      <p id="Par46">Therefore, in addition to the high accuracy, it finds a subset of more compact features in comparison with the IWSS method. However, this method requires a high evaluation time compared to similar methods and runs slow on high dimensional datasets. FCBF and PCA methods are filter-based. These methods only consider the linear relationship between features to find irrelevant features, so they cannot remove the redundant features, and the number of features found in these methods is high.</p>
      <p id="Par47">In Table <xref rid="Tab4" ref-type="table">4</xref> the proposed method is compared with Grasp, IFCA, F-score and SVM-RFE. In the Grasp method, after finding the candidate subsets, in the local search phase, the methods of IWSS, IWSSr, SFS, BARS, and Hill Climing are used separately to select the best subset of features. The BARS method selects the best subset of features using a combination of candidate subsets of features and removing the redundant features. The GRASP method, using a two-step algorithm as well as the application of various techniques in the improving phase section, has made progresses in comparison with other methods. However, it is less efficient than the proposed method and FICA. FICA method, because of using the IWSSr method, considers the relationships between features. The Fuzzy Imperialist Competitive Algorithm has been able to remove redundant features properly.</p>
      <p id="Par48">Additionally, the fuzzy influence of imperialist in colonies and the distribution of relevant features of imperialists in the colonial subsets leads to select the subset of optimal features with high-performance. Although this method finds a subset of more compact features than the proposed method, the results show that the accuracy of this method is competitive with the proposed method.</p>
      <p id="Par49">The F-score method is usually utilized to compute the degree of difference between two sets of real numbers. The larger the F value, the better the predictive ability of the feature<sup><xref ref-type="bibr" rid="CR68">68</xref></sup>. In this study, F value for all features is calculated in the datasets. Then, the 55 high F values are selected for classification using SVM. Although this method is simple, it’s detection rate is lower than the proposed method. This method does not indicate mutual information of features. In other words, F-score reveals the discriminative power of each feature independently from other features. Also, the number of selected features in this method is much higher than the other methods.</p>
      <p id="Par50">The SVM-RFE method (Support Vector Machine based on Recursive Feature Elimination) ranks the genes by training a SVM model and selects important genes using recursive feature elimination strategy. In this method, RFE is applied for eliminating unimportant features<sup><xref ref-type="bibr" rid="CR69">69</xref></sup>. Therefore, firstly, the SVM training using initial set of features is performed and the weight is assigned to each feature. Then, these absolute weights are sorted in descending order. Finally, the less weighted features are deleted. The results show that accuracy rate of this method is appropriate, but, the main problem of SVM-RFE is its time complexity, especially when the dimensionality of input data is extremely high. Furthermore, the number of selected genes in this method is higher than the other methods.</p>
      <p id="Par51">The results show that the accuracy of the proposed method in all datasets except Dorothea, is better than other methods. First, it is able to remove irrelevant features in the filter phase, then it removes the redundant features from the subset of features using the hybrid of the SFLA and IWSSr. In this method, due to the improvement of worst frogs, based on better frogs in the memplex and the best frog in the whole set, the redundant and irrelevant features of the frogs are removed and the relevant and useful features are added to the frogs. Removing and adding features are done based on their importance and their relationship with each other. Therefore, the selected feature set is more compact in the best frog and includes relevant features. The results show that in 8 datasets of 10 datasets used, an accuracy of 90% and in 3 datasets, a high accuracy of 98% is achieved. In 10 datasets used in the proposed method the average accuracy of 93.34% is obtained that is better than what obtained from other methods. Additionally, the average of selected features is 7.12, that can be compared to other methods.</p>
      <p id="Par52">In order to better evaluation, in this study, each dataset is divided into two datasets; a training dataset and an independent dataset. 80% of the original data is chosen randomly for the training dataset and 20% for the independent dataset. For this purpose, the training dataset is used to train, evaluate and justify the proposed method, and the independent dataset is applied for final performance evaluation of the proposed method. The samples are randomly divided into 2 groups 10 times and the results are averaged over 10 times. The results of these experiments are shown in Table <xref rid="Tab5" ref-type="table">5</xref>. The results approve that the proposed method is robust and it has high accuracy rate. Therefore, the method can be used to classify gene expression data with high accuracy.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Performance results of proposed method in training and independent data.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2"/><th colspan="4">Training data</th><th colspan="4">Independent data</th></tr><tr><th>Accuracy</th><th>Sensitivity</th><th>Specificity</th><th>Balance rates</th><th>Accuracy</th><th>Sensitivity</th><th>Specificity</th><th>Balance rates</th></tr></thead><tbody><tr><td>Colon</td><td>94.50</td><td>95.87</td><td>86.11</td><td>90.99</td><td>93.33</td><td>95.00</td><td>90.00</td><td>92.50</td></tr><tr><td>Arcene</td><td>94.75</td><td>92.57</td><td>96.44</td><td>94.50</td><td>94.00</td><td>92.21</td><td>95.45</td><td>93.83</td></tr><tr><td>Prostate1</td><td>88.87</td><td>86.77</td><td>90.50</td><td>88.63</td><td>88.23</td><td>87.77</td><td>91.00</td><td>89.38</td></tr><tr><td>DLBCL</td><td>99.50</td><td>98.89</td><td>94.81</td><td>96.85</td><td>98.12</td><td>98.33</td><td>95.00</td><td>96.66</td></tr><tr><td>Lung</td><td>99.13</td><td>99.58</td><td>96.80</td><td>98.19</td><td>99.16</td><td>99.66</td><td>96.66</td><td>98.16</td></tr><tr><td>Dorothea</td><td>91.25</td><td>93.70</td><td>90.26</td><td>91.98</td><td>90.37</td><td>91.31</td><td>89.47</td><td>90.39</td></tr><tr><td>Prostate</td><td>94.18</td><td>98.36</td><td>90.45</td><td>94.41</td><td>94.44</td><td>96.25</td><td>91.81</td><td>94.03</td></tr><tr><td>CNS</td><td>95.31</td><td>90.54</td><td>97.21</td><td>93.88</td><td>94.99</td><td>92.50</td><td>96.25</td><td>94.37</td></tr><tr><td>Leukemia</td><td>99.34</td><td>100.00</td><td>97.29</td><td>98.64</td><td>98.57</td><td>99.00</td><td>97.50</td><td>98.25</td></tr><tr><td>Breast</td><td>88.12</td><td>88.23</td><td>87.95</td><td>88.09</td><td>87.89</td><td>89.00</td><td>85.55</td><td>87.27</td></tr></tbody></table></table-wrap></p>
      <p id="Par53">In addition, a more detailed analysis of the proposed method, focusing on the features selected, shows some interesting aspects. Figure <xref rid="Fig7" ref-type="fig">7</xref> shows value of the selected features for all samples in some datasets. The proposed method has selected features whose values are less overlapping in the two classes. So these features have distinguished the patterns of two classes even better. It shows that the proposed method has selected appropriate features properly based on the available information. Also, the features in the negative class, especially in the DLBCL and Colon Datasets, have less variance. This property may be important in this regard that in the test and not seen samples, the value of the features is also in the range shown in the Fig. <xref rid="Fig7" ref-type="fig">7</xref>. Therefore, the error rate in this class can be less in comparison with other class. However, the value of features in the positive class has more variance. This causes the test data to deviate more than the mean, and the error rate in this class increases. Therefore, feature selection methods should select features that have a high classification accuracy on the test and training data.<fig id="Fig7"><label>Figure 7</label><caption><p>Distribution of selected feature values using the proposed method.</p></caption><graphic xlink:href="41598_2019_54987_Fig7_HTML" id="d29e4057"/></fig></p>
      <p id="Par54">To study the process of convergence of the algorithm, the mean accuracy of the method on the datasets in 40 iterations is shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>. As you can see, the learning process is going fast at the beginning, on average in step 20, the algorithm has converged on most datasets, and the accuracy has not increased from this iteration.<fig id="Fig8"><label>Figure 8</label><caption><p>Mean accuracy of frog’s populations in the 40 iterations of training.</p></caption><graphic xlink:href="41598_2019_54987_Fig8_HTML" id="d29e4072"/></fig></p>
      <p id="Par55">Moreover, the minimum, maximum and average number of iterations to achieve convergence of the proposed algorithm using the datasets are shown in Table <xref rid="Tab6" ref-type="table">6</xref>. The Arcene dataset with an average of 7.8 iterations has the lowest convergence time and Breast dataset with an average of 9.5 iterations has the highest convergence time. Overall, the average number of iterations required for all datasets is 12. 38 reps.<table-wrap id="Tab6"><label>Table 6</label><caption><p>minimum, maximum and average number of iterations performed by the proposed algorithm.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th>Minimum number of iterations</th><th>average number of iterations</th><th>Maximum number of iterations</th><th>Average accuracy</th></tr></thead><tbody><tr><td>Colon</td><td>12</td><td>13.9</td><td>18</td><td>94.72</td></tr><tr><td>Arcene</td><td>4</td><td>70.8</td><td>9</td><td>95.16</td></tr><tr><td>Prostate1</td><td>6</td><td>9.40</td><td>15</td><td>88.52</td></tr><tr><td>DLBCL</td><td>9</td><td>14.7</td><td>16</td><td>99.21</td></tr><tr><td>Lung</td><td>5</td><td>80.7</td><td>10</td><td>99.16</td></tr><tr><td>Dorothea</td><td>9</td><td>11.4</td><td>14</td><td>91.43</td></tr><tr><td>Prostate</td><td>17</td><td>22.2</td><td>30</td><td>94.18</td></tr><tr><td>CNS</td><td>8</td><td>9.80</td><td>21</td><td>95.64</td></tr><tr><td>Leukemia</td><td>11</td><td>16.4</td><td>19</td><td>99.62</td></tr><tr><td>Breast</td><td>12</td><td>9.50</td><td>22</td><td>88.17</td></tr><tr><td>Average</td><td>9.3</td><td>12.38</td><td>17.4</td><td>93.34</td></tr></tbody></table></table-wrap></p>
      <p id="Par56">By checking the number of samples in two classes of data it is clear that the number of data for two classes in Colon, DLBCL, Lung, CNS and Leukemia datasets is not balanced. In this type of data, the method cannot be evaluated only based on the “precision” criterion. Because the method may be biased to the majority class. In order to better evaluate, the accuracy, specificity, sensitivity and balance rates of proposed method in the mentioned datasets are shown in Fig. <xref rid="Fig9" ref-type="fig">9</xref>. Obviously, the proposed method has classified the class with more samples properly. However, the class with fewer samples has been classified with fewer classification rate. Due to the low number of samples in the class for correct learning, the classification operation is justifiable. Generally, all the criteria except Specificity in Colon dataset is higher than 90%. The results of the Fig. <xref rid="Fig9" ref-type="fig">9</xref> show that the performance of proposed method in the classification of unbalanced data is also acceptable.<fig id="Fig9"><label>Figure 9</label><caption><p>Comparing the performance criterion (accuracy, Specificity, Sensitivity and Balanced Rate) of proposed method.</p></caption><graphic xlink:href="41598_2019_54987_Fig9_HTML" id="d29e4301"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec14" sec-type="conclusion">
    <title>Conclusion</title>
    <p id="Par57">In this paper, a two-step hybrid algorithm based on Shuffled Frog Leaping Algorithm is proposed. This method uses the advantages of filter and wrapping methods for selecting efficient features. In the filter phase of the proposed method, the Relief method is used for weighting the features of the dataset. Then, in wrapping phase, in the weighted space, by using the Shuffled Frog Leaping Algorithm and the IWSSr algorithm, the search is performed to find the effective and relevant features. In the phase of modifying frogs, removing and adding features are based on their importance and weight. Therefore, the proposed method detects the relationship between the features properly and removes the redundant and irrelevant features from the selected feature set. The proposed method is evaluated using ten gene standard datasets. The experimental results of the proposed algorithm approve that it has the highest accuracy (an average of 93.34%) in comparison with similar methods. Also, the number of features found in each dataset with an average of 7.12 causes high efficiency and a subset of compressed features is achieved.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>J.P., M.A. and T.E.A. designed the research, J.P. and M.A. collected data, J.P., T.E.A. and M.H.O. wrote and performed computer programs, J.P., M.A., T.E.A. and M.H.O. analyzed and interpreted the results, M.A. and T.E.A. wrote the first version of the manuscript, J.P., M.A. and M.H.O. revised and edited the manuscript.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par58">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <mixed-citation publication-type="other">Liu, H. &amp; Motoda, H. <italic>Feature selection for knowledge discovery and data mining</italic>. 454 (Springer Science &amp; Business Media, 2012).</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>A review of matched-pairs feature selection methods for gene expression data analysis</article-title>
        <source>Computational and structural biotechnology journal</source>
        <year>2018</year>
        <volume>16</volume>
        <fpage>88</fpage>
        <lpage>97</lpage>
        <pub-id pub-id-type="doi">10.1016/j.csbj.2018.02.005</pub-id>
        <?supplied-pmid 30275937?>
        <pub-id pub-id-type="pmid">30275937</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <mixed-citation publication-type="other">Hira, Z. M. &amp; Gillies, D. F. A review of feature selection and feature extraction methods applied on microarray data. <italic>Advances in bioinformatics</italic> (2015).</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>The cross-entropy based multi-filter ensemble method for gene selection</article-title>
        <source>Genes</source>
        <year>2018</year>
        <volume>9</volume>
        <fpage>258</fpage>
        <pub-id pub-id-type="doi">10.3390/genes9050258</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Speed, T. <italic>Statistical analysis of gene expression microarray data</italic>. (Chapman and Hall/CRC, 2003).</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Golub</surname>
            <given-names>TR</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Molecular classification of cancer: class discovery and class prediction by gene expression monitoring</article-title>
        <source>science</source>
        <year>1999</year>
        <volume>286</volume>
        <fpage>531</fpage>
        <lpage>537</lpage>
        <pub-id pub-id-type="doi">10.1126/science.286.5439.531</pub-id>
        <pub-id pub-id-type="pmid">10521349</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Leung, Y., Chang, C., Hung, Y. &amp; Fung, P. In <italic>2006 International Conference of the IEEE Engineering in Medicine and Biology Society</italic>. 5846–5849 (IEEE).</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ding</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Minimum redundancy feature selection from microarray gene expression data</article-title>
        <source>Journal of bioinformatics and computational biology</source>
        <year>2005</year>
        <volume>3</volume>
        <fpage>185</fpage>
        <lpage>205</lpage>
        <pub-id pub-id-type="doi">10.1142/S0219720005001004</pub-id>
        <?supplied-pmid 15852500?>
        <pub-id pub-id-type="pmid">15852500</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Peng, H., Long, F. &amp; Ding, C. Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy. <italic>IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</italic>, 1226–1238 (2005).</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Hall, M. A. Correlation-based feature selection for machine learning. (1999).</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Yu, L. &amp; Liu, H. In <italic>Proceedings of the 20th international conference on machine learning (ICML-03)</italic>. 856–863 (2003).</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Battiti</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Using mutual information for selecting features in supervised neural net learning</article-title>
        <source>IEEE Transactions on neural networks</source>
        <year>1994</year>
        <volume>5</volume>
        <fpage>537</fpage>
        <lpage>550</lpage>
        <pub-id pub-id-type="doi">10.1109/72.298224</pub-id>
        <?supplied-pmid 18267827?>
        <pub-id pub-id-type="pmid">18267827</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zou</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>A novel features ranking metric with application to scalable visual and bioinformatics data classification</article-title>
        <source>Neurocomputing</source>
        <year>2016</year>
        <volume>173</volume>
        <fpage>346</fpage>
        <lpage>354</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neucom.2014.12.123</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ding</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Identification of mitochondrial proteins of malaria parasite using analysis of variance</article-title>
        <source>Amino acids</source>
        <year>2015</year>
        <volume>47</volume>
        <fpage>329</fpage>
        <lpage>333</lpage>
        <pub-id pub-id-type="doi">10.1007/s00726-014-1862-4</pub-id>
        <?supplied-pmid 25385313?>
        <pub-id pub-id-type="pmid">25385313</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ou</surname>
            <given-names>Y-Y</given-names>
          </name>
        </person-group>
        <article-title>Identifying the molecular functions of electron transport proteins using radial basis function networks and biochemical properties</article-title>
        <source>Journal of Molecular Graphics and Modelling</source>
        <year>2017</year>
        <volume>73</volume>
        <fpage>166</fpage>
        <lpage>178</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jmgm.2017.01.003</pub-id>
        <?supplied-pmid 28285094?>
        <pub-id pub-id-type="pmid">28285094</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Brankovic, A., Hosseini, M. &amp; Piroddi, L. A distributed feature selection algorithm based on distance correlation with an application to microarrays. <italic>IEEE/ACM transactions on computational biology and bioinformatics</italic> (2018).</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>Y-L</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>C-Y</given-names>
          </name>
          <name>
            <surname>Hor</surname>
            <given-names>M-K</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>P-F</given-names>
          </name>
        </person-group>
        <article-title>Feature selection using genetic algorithm and cluster validation</article-title>
        <source>Expert Systems with Applications</source>
        <year>2011</year>
        <volume>38</volume>
        <fpage>2727</fpage>
        <lpage>2732</lpage>
        <pub-id pub-id-type="doi">10.1016/j.eswa.2010.08.062</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Benitez, I. P., Sison, A. M. &amp; Medina, R. P. In <italic>2018 IEEE Symposium on Computer Applications &amp; Industrial Electronics (ISCAIE)</italic>. 238–243 (IEEE).</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Yang, J. &amp; Honavar, V. In Feature extraction, construction and selection 117–136 (Springer, 1998).</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jeong</surname>
            <given-names>I-S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A Feature Selection Approach Based on Simulated Annealing for Detecting Various Denial of Service Attacks</article-title>
        <source>Software Networking</source>
        <year>2018</year>
        <volume>2018</volume>
        <fpage>173</fpage>
        <lpage>190</lpage>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Debuse</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Rayward-Smith</surname>
            <given-names>VJ</given-names>
          </name>
        </person-group>
        <article-title>Feature subset selection within a simulated annealing data mining algorithm</article-title>
        <source>Journal of Intelligent Information Systems</source>
        <year>1997</year>
        <volume>9</volume>
        <fpage>57</fpage>
        <lpage>81</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1008641220268</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sivagaminathan</surname>
            <given-names>RK</given-names>
          </name>
          <name>
            <surname>Ramakrishnan</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>A hybrid approach for feature subset selection using neural networks and ant colony optimization</article-title>
        <source>Expert systems with applications</source>
        <year>2007</year>
        <volume>33</volume>
        <fpage>49</fpage>
        <lpage>60</lpage>
        <pub-id pub-id-type="doi">10.1016/j.eswa.2006.04.010</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kabir</surname>
            <given-names>MM</given-names>
          </name>
          <name>
            <surname>Shahjahan</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Murase</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>A new hybrid ant colony optimization algorithm for feature selection</article-title>
        <source>Expert Systems with Applications</source>
        <year>2012</year>
        <volume>39</volume>
        <fpage>3747</fpage>
        <lpage>3763</lpage>
        <pub-id pub-id-type="doi">10.1016/j.eswa.2011.09.073</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Deriche, M. In <italic>2009 6th International Multi-Conference on Systems, Signals and Devices</italic>. 1–4 (IEEE).</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Feature Selection for Optimized High-Dimensional Biomedical Data Using an Improved Shuffled Frog Leaping Algorithm</article-title>
        <source>IEEE/ACM transactions on computational biology and bioinformatics</source>
        <year>2016</year>
        <volume>15</volume>
        <fpage>1765</fpage>
        <lpage>1773</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2016.2602263</pub-id>
        <?supplied-pmid 28113635?>
        <pub-id pub-id-type="pmid">28113635</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pirgazi</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Khanteymoori</surname>
            <given-names>AR</given-names>
          </name>
        </person-group>
        <article-title>SFLA based gene selection approach for improving cancer classification accuracy</article-title>
        <source>AUT Journal of Modeling and Simulation</source>
        <year>2015</year>
        <volume>47</volume>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xue</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Browne</surname>
            <given-names>WN</given-names>
          </name>
        </person-group>
        <article-title>Particle swarm optimization for feature selection in classification: A multi-objective approach</article-title>
        <source>IEEE transactions on cybernetics</source>
        <year>2012</year>
        <volume>43</volume>
        <fpage>1656</fpage>
        <lpage>1671</lpage>
        <pub-id pub-id-type="doi">10.1109/TSMCB.2012.2227469</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Chakraborty, B. In <italic>2008 3rd international conference on intelligent system and knowledge engineering</italic>. 1038–1042 (IEEE).</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Fong</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>RK</given-names>
          </name>
          <name>
            <surname>Millham</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>KK</given-names>
          </name>
        </person-group>
        <article-title>Elitist binary wolf search algorithm for heuristic feature selection in high-dimensional bioinformatics datasets</article-title>
        <source>Scientific reports</source>
        <year>2017</year>
        <volume>7</volume>
        <fpage>4354</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-017-04037-5</pub-id>
        <?supplied-pmid 28659577?>
        <pub-id pub-id-type="pmid">28659577</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Too</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Abdullah</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mohd Saad</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Mohd Ali</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Tee</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>A New Competitive Binary Grey Wolf Optimizer to Solve the Feature Selection Problem in EMG Signals Classification</article-title>
        <source>Computers</source>
        <year>2018</year>
        <volume>7</volume>
        <fpage>58</fpage>
        <pub-id pub-id-type="doi">10.3390/computers7040058</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Narendra, P. M. &amp; Fukunaga, K. A branch and bound algorithm for feature subset selection. <italic>IEEE Transactions on computers</italic>, 917–922 (1977).</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Doak, J. E. <italic>Intrusion detection: The application of feature selection, a comparison of algorithms, and the application of a wide area network analyzer</italic>. (U. of Calif., Davis, 1992).</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cotter</surname>
            <given-names>SF</given-names>
          </name>
          <name>
            <surname>Kreutz-Delgado</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Rao</surname>
            <given-names>BD</given-names>
          </name>
        </person-group>
        <article-title>Backward sequential elimination for sparse vector subset selection</article-title>
        <source>Signal Processing</source>
        <year>2001</year>
        <volume>81</volume>
        <fpage>1849</fpage>
        <lpage>1864</lpage>
        <pub-id pub-id-type="doi">10.1016/S0165-1684(01)00064-0</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Almuallim</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Dietterich</surname>
            <given-names>TG</given-names>
          </name>
        </person-group>
        <article-title>Learning boolean concepts in the presence of many irrelevant features</article-title>
        <source>Artificial Intelligence</source>
        <year>1994</year>
        <volume>69</volume>
        <fpage>279</fpage>
        <lpage>305</lpage>
        <pub-id pub-id-type="doi">10.1016/0004-3702(94)90084-1</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Caruana, R. &amp; Freitag, D. In <italic>Machine Learning Proceedings 1994</italic> 28–36 (Elsevier, 1994).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mitchell</surname>
            <given-names>TJ</given-names>
          </name>
          <name>
            <surname>Beauchamp</surname>
            <given-names>JJ</given-names>
          </name>
        </person-group>
        <article-title>Bayesian variable selection in linear regression</article-title>
        <source>Journal of the American Statistical Association</source>
        <year>1988</year>
        <volume>83</volume>
        <fpage>1023</fpage>
        <lpage>1032</lpage>
        <pub-id pub-id-type="doi">10.1080/01621459.1988.10478694</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bermejo</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Gámez</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Puerta</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <article-title>A GRASP algorithm for fast hybrid (filter-wrapper) feature subset selection in high-dimensional datasets</article-title>
        <source>Pattern Recognition Letters</source>
        <year>2011</year>
        <volume>32</volume>
        <fpage>701</fpage>
        <lpage>711</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patrec.2010.12.016</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brahim</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Limam</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>A hybrid feature selection method based on instance learning and cooperative subset search</article-title>
        <source>Pattern Recognition Letters</source>
        <year>2016</year>
        <volume>69</volume>
        <fpage>28</fpage>
        <lpage>34</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patrec.2015.10.005</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shukla</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Vardhan</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>A hybrid framework for optimal feature subset selection</article-title>
        <source>Journal of Intelligent &amp; Fuzzy Systems</source>
        <year>2019</year>
        <volume>36</volume>
        <fpage>2247</fpage>
        <lpage>2259</lpage>
        <pub-id pub-id-type="doi">10.3233/JIFS-169936</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ruiz</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Riquelme</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Aguilar-Ruiz</surname>
            <given-names>JS</given-names>
          </name>
        </person-group>
        <article-title>Incremental wrapper-based gene selection from microarray data for cancer classification</article-title>
        <source>Pattern Recognition</source>
        <year>2006</year>
        <volume>39</volume>
        <fpage>2383</fpage>
        <lpage>2392</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patcog.2005.11.001</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Moradi</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Gholampour</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>A hybrid particle swarm optimization for feature subset selection by integrating a novel local search strategy</article-title>
        <source>Applied Soft Computing</source>
        <year>2016</year>
        <volume>43</volume>
        <fpage>117</fpage>
        <lpage>130</lpage>
        <pub-id pub-id-type="doi">10.1016/j.asoc.2016.01.044</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Ruiz, R., Riquelme, J. C. &amp; Aguilar-Ruiz, J. S. <italic>In New Challenges for Feature Selection in Data Mining and Knowledge Discovery</italic>. 148–162 (2008).</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Moradkhani</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Amiri</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Javaherian</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Safari</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>A hybrid algorithm for feature subset selection in high-dimensional datasets using FICA and IWSSr algorithm</article-title>
        <source>Applied Soft Computing</source>
        <year>2015</year>
        <volume>35</volume>
        <fpage>123</fpage>
        <lpage>135</lpage>
        <pub-id pub-id-type="doi">10.1016/j.asoc.2015.03.049</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lu</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A hybrid feature selection algorithm for gene expression data classification</article-title>
        <source>Neurocomputing</source>
        <year>2017</year>
        <volume>256</volume>
        <fpage>56</fpage>
        <lpage>62</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neucom.2016.07.080</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sun</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Kong</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhai</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>A Hybrid Gene Selection Method Based on ReliefF and Ant Colony Optimization Algorithm for Tumor Classification</article-title>
        <source>Scientific Reports</source>
        <year>2019</year>
        <volume>9</volume>
        <fpage>8978</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-019-45223-x</pub-id>
        <?supplied-pmid 31222027?>
        <pub-id pub-id-type="pmid">31222027</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dao</surname>
            <given-names>F-Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Identify origin of replication in Saccharomyces cerevisiae using two-step feature selection technique</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>35</volume>
        <fpage>2075</fpage>
        <lpage>2083</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty943</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>AdaBoost for feature selection, classification and its relation with SVM, a review</article-title>
        <source>Physics Procedia</source>
        <year>2012</year>
        <volume>25</volume>
        <fpage>800</fpage>
        <lpage>807</lpage>
        <pub-id pub-id-type="doi">10.1016/j.phpro.2012.03.160</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ram</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Najafi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Shakeri</surname>
            <given-names>MT</given-names>
          </name>
        </person-group>
        <article-title>Classification and biomarker genes selection for cancer gene expression data using random forest</article-title>
        <source>Iranian journal of pathology</source>
        <year>2017</year>
        <volume>12</volume>
        <fpage>339</fpage>
        <pub-id pub-id-type="pmid">29563929</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guyon</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Weston</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Barnhill</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Vapnik</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Gene selection for cancer classification using support vector machines</article-title>
        <source>Machine learning</source>
        <year>2002</year>
        <volume>46</volume>
        <fpage>389</fpage>
        <lpage>422</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1012487302797</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>S. Maldonado</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Weber</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Basak</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Simultaneous feature selection and classification using kernel-penalized support vector machines</article-title>
        <source>Information Sciences</source>
        <year>2011</year>
        <volume>181</volume>
        <fpage>115</fpage>
        <lpage>128</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ins.2010.08.047</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">Hall, M. A. Correlation-based feature selection of discrete and numeric class machine learning. (2000).</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other">Bermejo, P., Gámez, J. A. &amp; Puerta, J. M. In <italic>2009 IEEE Symposium on Computational Intelligence and Data Mining</italic>. 367–374 (IEEE).</mixed-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Eusuff</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lansey</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Pasha</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Shuffled frog-leaping algorithm: a memetic meta-heuristic for discrete optimization</article-title>
        <source>Engineering optimization</source>
        <year>2006</year>
        <volume>38</volume>
        <fpage>129</fpage>
        <lpage>154</lpage>
        <pub-id pub-id-type="doi">10.1080/03052150500384759</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bhattacharjee</surname>
            <given-names>KK</given-names>
          </name>
          <name>
            <surname>Sarmah</surname>
            <given-names>SP</given-names>
          </name>
        </person-group>
        <article-title>Shuffled frog leaping algorithm and its application to 0/1 knapsack problem</article-title>
        <source>Applied soft computing</source>
        <year>2014</year>
        <volume>19</volume>
        <fpage>252</fpage>
        <lpage>263</lpage>
        <pub-id pub-id-type="doi">10.1016/j.asoc.2014.02.010</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <mixed-citation publication-type="other">Pirgazi, J., Khanteymoori, A. R. &amp; Amiri, A. In <italic>2015</italic> IEEE <italic>International Conference on Fuzzy Systems (FUZZ-IEEE)</italic>. 1–5 (IEEE).</mixed-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Robnik-Šikonja</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kononenko</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Theoretical and empirical analysis of ReliefF and RReliefF</article-title>
        <source>Machine learning</source>
        <year>2003</year>
        <volume>53</volume>
        <fpage>23</fpage>
        <lpage>69</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1025667309714</pub-id>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bolón-Canedo</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Sánchez-Marono</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Alonso-Betanzos</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Benítez</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Herrera</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>A review of microarray datasets and applied feature selection methods</article-title>
        <source>Information Sciences</source>
        <year>2014</year>
        <volume>282</volume>
        <fpage>111</fpage>
        <lpage>135</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ins.2014.05.042</pub-id>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alon</surname>
            <given-names>U</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays</article-title>
        <source>Proceedings of the National Academy of Sciences</source>
        <year>1999</year>
        <volume>96</volume>
        <fpage>6745</fpage>
        <lpage>6750</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.96.12.6745</pub-id>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pomeroy</surname>
            <given-names>SL</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Prediction of central nervous system embryonal tumor outcome based on gene expression</article-title>
        <source>Nature</source>
        <year>2002</year>
        <volume>415</volume>
        <fpage>436</fpage>
        <pub-id pub-id-type="doi">10.1038/415436a</pub-id>
        <?supplied-pmid 11807556?>
        <pub-id pub-id-type="pmid">11807556</pub-id>
      </element-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shipp</surname>
            <given-names>MA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Diffuse large B-cell lymphoma outcome prediction by gene-expression profiling and supervised machine learning</article-title>
        <source>Nature medicine</source>
        <year>2002</year>
        <volume>8</volume>
        <fpage>68</fpage>
        <pub-id pub-id-type="doi">10.1038/nm0102-68</pub-id>
        <?supplied-pmid 11786909?>
        <pub-id pub-id-type="pmid">11786909</pub-id>
      </element-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stuart</surname>
            <given-names>RO</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>In silico dissection of cell-type-associated patterns of gene expression in prostate cancer</article-title>
        <source>Proceedings of the National Academy of Sciences</source>
        <year>2004</year>
        <volume>101</volume>
        <fpage>615</fpage>
        <lpage>620</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.2536479100</pub-id>
      </element-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gordon</surname>
            <given-names>GJ</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Translation of microarray data into clinically relevant cancer diagnostic tests using gene expression ratios in lung cancer and mesothelioma</article-title>
        <source>Cancer research</source>
        <year>2002</year>
        <volume>62</volume>
        <fpage>4963</fpage>
        <lpage>4967</lpage>
        <?supplied-pmid 12208747?>
        <pub-id pub-id-type="pmid">12208747</pub-id>
      </element-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Singh</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Gene expression correlates of clinical prostate cancer behavior</article-title>
        <source>Cancer cell</source>
        <year>2002</year>
        <volume>1</volume>
        <fpage>203</fpage>
        <lpage>209</lpage>
        <pub-id pub-id-type="doi">10.1016/S1535-6108(02)00030-2</pub-id>
        <?supplied-pmid 12086878?>
        <pub-id pub-id-type="pmid">12086878</pub-id>
      </element-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ho</surname>
            <given-names>Q-T</given-names>
          </name>
          <name>
            <surname>Ou</surname>
            <given-names>Y-Y</given-names>
          </name>
        </person-group>
        <article-title>Classifying the molecular functions of Rab GTPases in membrane trafficking using deep convolutional neural networks</article-title>
        <source>Analytical biochemistry</source>
        <year>2018</year>
        <volume>555</volume>
        <fpage>33</fpage>
        <lpage>41</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ab.2018.06.011</pub-id>
        <?supplied-pmid 29908156?>
        <pub-id pub-id-type="pmid">29908156</pub-id>
      </element-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Le</surname>
            <given-names>NQK</given-names>
          </name>
          <name>
            <surname>Huynh</surname>
            <given-names>T-T</given-names>
          </name>
          <name>
            <surname>Yapp</surname>
            <given-names>EKY</given-names>
          </name>
          <name>
            <surname>Yeh</surname>
            <given-names>H-Y</given-names>
          </name>
        </person-group>
        <article-title>Identification of clathrin proteins by incorporating hyperparameter optimization in deep learning and PSSM profiles</article-title>
        <source>Computer Methods and Programs in Biomedicine</source>
        <year>2019</year>
        <volume>177</volume>
        <fpage>81</fpage>
        <lpage>88</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cmpb.2019.05.016</pub-id>
        <?supplied-pmid 31319963?>
        <pub-id pub-id-type="pmid">31319963</pub-id>
      </element-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jung</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>AK-fold averaging cross-validation procedure</article-title>
        <source>Journal of nonparametric statistics</source>
        <year>2015</year>
        <volume>27</volume>
        <fpage>167</fpage>
        <lpage>179</lpage>
        <pub-id pub-id-type="doi">10.1080/10485252.2015.1010532</pub-id>
        <?supplied-pmid 27630515?>
        <pub-id pub-id-type="pmid">27630515</pub-id>
      </element-citation>
    </ref>
    <ref id="CR67">
      <label>67.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Arlot</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Celisse</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>A survey of cross-validation procedures for model selection</article-title>
        <source>Statistics surveys</source>
        <year>2010</year>
        <volume>4</volume>
        <fpage>40</fpage>
        <lpage>79</lpage>
        <pub-id pub-id-type="doi">10.1214/09-SS054</pub-id>
      </element-citation>
    </ref>
    <ref id="CR68">
      <label>68.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ou</surname>
            <given-names>Y-Y</given-names>
          </name>
        </person-group>
        <article-title>Prediction of FAD binding sites in electron transport proteins according to efficient radial basis function networks and significant amino acid pairs</article-title>
        <source>BMC bioinformatics</source>
        <year>2016</year>
        <volume>17</volume>
        <fpage>298</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-016-1163-x</pub-id>
        <?supplied-pmid 27475771?>
        <pub-id pub-id-type="pmid">27475771</pub-id>
      </element-citation>
    </ref>
    <ref id="CR69">
      <label>69.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Efficient feature selection and classification for microarray data</article-title>
        <source>PloS one</source>
        <year>2018</year>
        <volume>13</volume>
        <fpage>e0202167</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0202167</pub-id>
        <?supplied-pmid 30125332?>
        <pub-id pub-id-type="pmid">30125332</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
