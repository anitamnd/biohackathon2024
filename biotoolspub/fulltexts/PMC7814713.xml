<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Med Imaging</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Med Imaging</journal-id>
    <journal-title-group>
      <journal-title>BMC Medical Imaging</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2342</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7814713</article-id>
    <article-id pub-id-type="publisher-id">543</article-id>
    <article-id pub-id-type="doi">10.1186/s12880-020-00543-7</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MIScnn: a framework for medical image segmentation with convolutional neural networks and deep learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0838-9885</contrib-id>
        <name>
          <surname>Müller</surname>
          <given-names>Dominik</given-names>
        </name>
        <address>
          <email>dominik.mueller@informatik.uni-augsburg.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2857-7122</contrib-id>
        <name>
          <surname>Kramer</surname>
          <given-names>Frank</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="GRID">grid.7307.3</institution-id><institution-id institution-id-type="ISNI">0000 0001 2108 9006</institution-id><institution>IT-Infrastructure for Translational Medical Research, Faculty of Applied Computer Science, Faculty of Medicine, </institution><institution>University of Augsburg, </institution></institution-wrap>Augsburg, Germany </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>18</day>
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>18</day>
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>21</volume>
    <elocation-id>12</elocation-id>
    <history>
      <date date-type="received">
        <day>4</day>
        <month>8</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>25</day>
        <month>12</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">The increased availability and usage of modern medical imaging induced a strong need for automatic medical image segmentation. Still, current image segmentation platforms do not provide the required functionalities for plain setup of medical image segmentation pipelines. Already implemented pipelines are commonly standalone software, optimized on a specific public data set. Therefore, this paper introduces the open-source Python library MIScnn.</p>
      </sec>
      <sec>
        <title>Implementation</title>
        <p id="Par2">The aim of MIScnn is to provide an intuitive API allowing fast building of medical image segmentation pipelines including data I/O, preprocessing, data augmentation, patch-wise analysis, metrics, a library with state-of-the-art deep learning models and model utilization like training, prediction, as well as fully automatic evaluation (e.g. cross-validation). Similarly, high configurability and multiple open interfaces allow full pipeline customization.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par3">Running a cross-validation with MIScnn on the Kidney Tumor Segmentation Challenge 2019 data set (multi-class semantic segmentation with 300 CT scans) resulted into a powerful predictor based on the standard 3D U-Net model.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par4">With this experiment, we could show that the MIScnn framework enables researchers to rapidly set up a complete medical image segmentation pipeline by using just a few lines of code. The source code for MIScnn is available in the Git repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/frankkramer-lab/MIScnn">https://github.com/frankkramer-lab/MIScnn</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Medical image analysis</kwd>
      <kwd>Computer aided diagnosis</kwd>
      <kwd>Biomedical image segmentation</kwd>
      <kwd>U-Net</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Open-source framework</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002347</institution-id>
            <institution>Bundesministerium für Bildung und Forschung</institution>
          </institution-wrap>
        </funding-source>
        <award-id>01ZZ1804E</award-id>
        <principal-award-recipient>
          <name>
            <surname>Kramer</surname>
            <given-names>Frank</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Projekt DEAL</institution>
        </funding-source>
      </award-group>
      <open-access>
        <p>Open Access funding enabled and organized by Projekt DEAL.</p>
      </open-access>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par15">Medical imaging became a standard in diagnosis and medical intervention for the visual representation of the functionality of organs and tissues. Through the increased availability and usage of modern medical imaging like Magnetic Resonance Imaging (MRI) or Computed Tomography (CT), the need for automated processing of scanned imaging data is quite strong [<xref ref-type="bibr" rid="CR1">1</xref>]. Currently, the evaluation of medical images is a manual process performed by physicians. Larger numbers of slices require the inspection of even more image material by doctors, especially regarding the increased usage of high-resolution medical imaging. In order to shorten the time-consuming inspection and evaluation process, an automatic pre-segmentation of abnormal features in medical images would be required.</p>
    <p id="Par16">Image segmentation is a popular sub-field of image processing within computer science [<xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR7">7</xref>]. The aim of semantic segmentation is to identify common features in an input image by learning and then labeling each pixel in an image with a class (e.g. background, kidney or tumor). There is a wide range of algorithms to solve segmentation problems. However, state-of-the-art accuracy was accomplished by convolutional neural networks and deep learning models [<xref ref-type="bibr" rid="CR7">7</xref>–<xref ref-type="bibr" rid="CR11">11</xref>], which are used extensively today. Furthermore, the newest convolutional neural networks are able to exploit local and global features in images [<xref ref-type="bibr" rid="CR12">12</xref>–<xref ref-type="bibr" rid="CR14">14</xref>] and they can be trained to use 3D image information as well [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>]. In recent years, medical image segmentation models with a convolutional neural network architecture have become quite powerful and achieved similar results performance-wise as radiologists [<xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR17">17</xref>]. Nevertheless, these models have been standalone applications with optimized architectures, preprocessing procedures, data augmentations and metrics specific for their data set and corresponding segmentation problem [<xref ref-type="bibr" rid="CR14">14</xref>]. Also, the performance of such optimized pipelines varies drastically between different medical conditions. However, even for the same medical condition, evaluation and comparisons of these models are a persistent challenge due to the variety of the size, shape, localization and distinctness of different data sets. In order to objectively compare two segmentation model architectures from the sea of one-use standalone pipelines, each specific for a single public data set, it would be required to implement a complete custom pipeline with preprocessing, data augmentation and batch creation. Frameworks for general image segmentation pipeline building can not be fully utilized. The reason for this are their missing medical image I/O interfaces, their preprocessing methods, as well as their lack of handling highly unbalanced class distributions, which is standard in medical imaging. Recently developed medical image segmentation platforms, like NiftyNet [<xref ref-type="bibr" rid="CR18">18</xref>], are powerful tools and an excellent first step for standardized medical image segmentation pipelines. However, they are designed more like configurable software instead of frameworks. They lack modular pipeline blocks to offer researchers the opportunity for easy customization and to help developing their own software for their specific segmentation problems.</p>
    <p id="Par17">In this work, we push towards constructing an intuitive and easy-to-use framework for fast setup of state-of-the-art convolutional neural network and deep learning models for medical image segmentation. The aim of our framework Medical Image Segmentation with Convolutional Neural Networks (MIScnn) is to provide a complete pipeline for preprocessing, data augmentation, patch slicing and batch creation steps in order to start straightforward with training and predicting on diverse medical imaging data. Instead of being fixated on one model architecture, MIScnn allows not only fast switching between multiple modern convolutional neural network models, but it also provides the possibility to easily add custom model architectures. Additionally, it facilitates a simple deployment and fast usage of new deep learning models for medical image segmentation. Still, MIScnn is highly configurable to adjust hyperparameters, general training parameters, preprocessing procedures, as well as include or exclude data augmentations and evaluation techniques.</p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <p id="Par18">The open-source Python library MIScnn is a framework to setup medical image segmentation pipelines with convolutional neural networks and deep learning models. MIScnn is providing several core features, which are also illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>:<list list-type="bullet"><list-item><p id="Par19">2D/3D medical image segmentation for binary and multi-class problems</p></list-item><list-item><p id="Par20">Data I/O, preprocessing and data augmentation for biomedical images</p></list-item><list-item><p id="Par21">Patch-wise and full image analysis</p></list-item><list-item><p id="Par22">State-of-the-art deep learning model and metric library</p></list-item><list-item><p id="Par23">Intuitive and fast model utilization (training, prediction)</p></list-item><list-item><p id="Par24">Multiple automatic evaluation techniques (e.g. cross-validation)</p></list-item><list-item><p id="Par25">Custom model, data I/O, pre-/postprocessing and metric support</p></list-item></list><fig id="Fig1"><label>Fig. 1</label><caption><p>Flowchart diagram of the MIScnn pipeline starting with the data I/O and ending with a deep learning model</p></caption><graphic xlink:href="12880_2020_543_Fig1_HTML" id="MO1"/></fig></p>
    <sec id="Sec3">
      <title>Data input</title>
      <sec id="Sec4">
        <title>NIfTI data I/O interface</title>
        <p id="Par26">MIScnn provides a data I/O interface for the Neuroimaging Informatics Technology Initiative (NifTI) [<xref ref-type="bibr" rid="CR19">19</xref>] file format for loading Magnetic Resonance Imaging and Computed Tomography data into the framework. This format was initially created to speed up the development and enhance the utility of informatics tools related to neuroimaging. Still, it is now commonly used for sharing public and anonymous MRI and CT data sets, not only for brain imaging, but also for all kinds of human 3D imaging. A NIfTI file contains the 3D image matrix and diverse metadata, like the thickness of the MRI slices.</p>
      </sec>
      <sec id="Sec5">
        <title>Custom data I/O interface</title>
        <p id="Par27">Next to the implemented NIfTI I/O interface, MIScnn allows the usage of custom data I/O interfaces for other imaging data formats. This open interface enables MIScnn to handle specific biomedical imaging features (e.g. MRI slice thickness), and therefore it avoids losing these feature information by a format conversion requirement. A custom I/O interface must be committed to the preprocessing function and it has to return the medical image as a 2D or 3D matrix for integration in the workflow. It is advised to add format specific preprocessing procedures (e.g. MRI slice thickness normalization) in the format specific I/O interface, before returning the image matrix into the pipeline.</p>
      </sec>
    </sec>
    <sec id="Sec6">
      <title>Preprocessing</title>
      <sec id="Sec7">
        <title>Pixel intensity normalization</title>
        <p id="Par28">Inconsistent signal intensity ranges of images can drastically influence the performance of segmentation methods [<xref ref-type="bibr" rid="CR20">20</xref>, <xref ref-type="bibr" rid="CR21">21</xref>]. The signal ranges of biomedical imaging data are highly varying between data sets due to different image formats, diverse hardware/instruments (e.g. different scanners), technical discrepancies, and simply biological variation [<xref ref-type="bibr" rid="CR10">10</xref>]. Additionally, the machine learning algorithms behind image segmentation usually perform better on features which follow a normal distribution. In order to achieve dynamic signal intensity range consistency, it is advisable to scale and standardize imaging data. The signal intensity scaling projects the original value range to a predefined range usually between [0, 1] or [− 1, 1], whereas standardization centers the values close to a normal distribution by computing a Z-Score normalization. MIScnn can be configured to include or exclude pixel intensity scaling or standardization on the medical imaging data in the pipeline.</p>
      </sec>
      <sec id="Sec8">
        <title>Clipping</title>
        <p id="Par29">Similar to pixel intensity normalization, it is also common to clip pixel intensities to a certain range. Intensity values outside of this range will be clipped to the minimum or maximum range value. Especially in computer tomography images, pixel intensity values are expected to be identical for the same organs or tissue types even in different scanners [<xref ref-type="bibr" rid="CR22">22</xref>]. This can be exploited through organ-specific pixel intensity clipping.</p>
      </sec>
      <sec id="Sec9">
        <title>Resampling</title>
        <p id="Par30">The resampling technique is used to modify the width and/or height of images. This results into a new image with a modified number of pixels. Magnetic resonance or computer tomography scans can have different slice thickness. However, training neural network models requires the images to have the same slice thickness or voxel spacing. This can be accomplished through resampling. Additionally, downsampling images reduces the required GPU memory for training and prediction.</p>
      </sec>
      <sec id="Sec10">
        <title>One hot encoding</title>
        <p id="Par31">MIScnn is able to handle binary (background/cancer) as well multi-class (background/kidney/liver/lungs) segmentation problems. The representation of a binary segmentation is being made quite simple by using a variable with two states, zero and one. But for the processing of multiple categorical segmentation labels in machine learning algorithms, like deep learning models, it is required to convert the classes into a more mathematical representation. This can be achieved with the One Hot encoding method by creating a single binary variable for each segmentation class. MIScnn automatically One Hot encodes segmentation labels with more than two classes.</p>
      </sec>
    </sec>
    <sec id="Sec11">
      <title>Patch-wise and full image analysis</title>
      <p id="Par32">Depending on the resolution of medical images, the available GPU hardware plays a large role in 3D segmentation analysis. Currently, it is not possible to fully fit high-resolution MRIs with an example size of 400 × 512 × 512 into state-of-the-art convolutional neural network models due to the enormous GPU memory requirements. Therefore, the 3D medical imaging data can be either sliced into smaller cuboid patches or analyzed slice-by-slice, similar to a set of 2D images [<xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR23">23</xref>]. In order to fully use the information of all three axis, MIScnn slices 3D medical images into patches with a configurable size (e.g. 128 × 128 × 128) by default. Depending on the model architecture, these patches can fit into GPUs with RAM sizes of 4–24 GB, which are commonly used in research. Nevertheless, the slice-by-slice 2D analysis, as well as the 3D patch analysis is supported and can be used in MIScnn. It is also possible to configure the usage of full 3D images in case of analyzing uncommonly small medical images or having a large GPU cluster. By default, 2D medical images are fitted completely into the convolutional neural network and deep learning models. Still, a 2D patch-wise approach for large resolution images can be also applied.</p>
    </sec>
    <sec id="Sec12">
      <title>Data augmentation for training</title>
      <p id="Par33">In the machine learning field, data augmentation covers the artificially increase of training data. Especially in medical imaging, commonly only a small number of samples or images of a studied medical condition is available for training [<xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR24">24</xref>–<xref ref-type="bibr" rid="CR27">27</xref>]. Thus, an image can be modified with multiple techniques, like shifting, to expand the number of plausible examples for training. The aim is to create reasonable variations of the desired pattern in order to avoid overfitting in small data sets [<xref ref-type="bibr" rid="CR26">26</xref>].</p>
      <p id="Par34">For state-of-the-art data augmentation, MIScnn integrated the batchgenerators package from the Division of Medical Image Computing at the German Cancer Research Center (DKFZ) [<xref ref-type="bibr" rid="CR28">28</xref>]. It offers various data augmentation techniques and was used by the winners of the latest medical image processing challenges [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR22">22</xref>, <xref ref-type="bibr" rid="CR29">29</xref>]. It supports spatial translations, rotations, scaling, elastic deformations, brightness, contrast, gamma and noise augmentations like Gaussian noise.</p>
    </sec>
    <sec id="Sec13">
      <title>Sampling and batch generation</title>
      <sec id="Sec14">
        <title>Skipping blank patches</title>
        <p id="Par35">The known problem in medical images of the large unbalance between the relevant segments and the background results into an extensive amount of parts purely labeled as background and without any learning information [<xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR24">24</xref>]. Especially after data augmentation, there is no benefit to multiply these blank parts or patches [<xref ref-type="bibr" rid="CR30">30</xref>]. Therefore, in the patch-wise model training, all patches, which are completely labeled as background, can be excluded in order to avoid wasting time on unnecessary fitting.</p>
      </sec>
      <sec id="Sec15">
        <title>Batch management</title>
        <p id="Par36">After the data preprocessing and the optional data augmentation for training, sets of full images or patches are bundled into batches. One batch contains a number of prepared images which are processed in a single step by the model and GPU. Sequential for each batch or processing step, the neural network updates its internal weights accordingly with the predefined learning rate. The possible number of images inside a single batch highly depends on the available GPU memory and has to be configured properly in MIScnn. Every batch is saved to disk in order to allow fast repeated access during the training process. This approach drastically reduces the computing time due to the avoidance of unnecessary repeated preprocessing of the batches. Nevertheless, this approach is not ideal for extremely large data sets or for researchers without the required disk space. In order to bypass this problem, MIScnn also supports “on-the-fly” generation of the next batch in memory during runtime.</p>
      </sec>
      <sec id="Sec16">
        <title>Batch shuffling</title>
        <p id="Par37">During model training, the order of batches, which are going to be fitted and processed, is shuffled at the end of each epoch. This method reduces the variance of the neural network during fitting over an epoch and lowers the risk of overfitting. Still, it must be noted, that only the processing sequence of the batches is shuffled and the data itself is not sorted into a new batch order.</p>
      </sec>
      <sec id="Sec17">
        <title>Multi-CPU and -GPU support</title>
        <p id="Par38">MIScnn also supports the usage of multiple GPUs and parallel CPU batch loading next to the GPU computing. Particularly, the storage of already prepared batches on disk enables a fast and parallelizable processing with CPU as well as GPU clusters by eliminating the risk of batch preprocessing bottlenecks.</p>
      </sec>
    </sec>
    <sec id="Sec18">
      <title>Deep learning model creation</title>
      <sec id="Sec19">
        <title>Model architecture</title>
        <p id="Par39">The selection of a deep learning or convolutional neural network model is the most important step in a medical image segmentation pipeline. There is a variety of model architectures and each has different strengths and weaknesses [<xref ref-type="bibr" rid="CR12">12</xref>, <xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR31">31</xref>–<xref ref-type="bibr" rid="CR37">37</xref>]. MIScnn features an open model interface to load and switch between provided state-of-the-art convolutional neural network models like the popular U-Net model [<xref ref-type="bibr" rid="CR12">12</xref>]. Models are represented with the open-source neural network library Keras [<xref ref-type="bibr" rid="CR38">38</xref>] which provides a user-friendly API for commonly used neural-network building blocks on top of TensorFlow [<xref ref-type="bibr" rid="CR39">39</xref>]. The already implemented models are highly configurable by definable number of neurons, custom input sizes, optional dropout and batch normalization layers or enhanced architecture versions like the Optimized High Resolution Dense-U-Net model [<xref ref-type="bibr" rid="CR15">15</xref>]. Additionally, MIScnn offers architectures for 3D, as well as 2D medical image segmentation. This model selection process is visualized in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. Besides the flexibility in switching between already implemented models, the open model interface enables the ability for custom deep learning model implementations and simple integrating these custom models into the MIScnn pipeline.<fig id="Fig2"><label>Fig. 2</label><caption><p>Flowchart visualization of the deep learning model creation and architecture selection process</p></caption><graphic xlink:href="12880_2020_543_Fig2_HTML" id="MO2"/></fig></p>
      </sec>
      <sec id="Sec20">
        <title>Metrics</title>
        <p id="Par40">MIScnn offers a large quantity of various metrics which can be used as loss function for training or for evaluation in figures and manual performance analysis. The Dice coefficient, also known as the Dice similarity index, is one of the most popular metrics for medical image segmentation. It scores the similarity between the predicted segmentation and the ground truth. However, it also penalizes false positives comparable to the precision metric. Depending on the segmentation classes (binary or multi-class), there is a simple and class-wise Dice coefficient implementation. Whereas the simple implementation only accumulates the overall number of correct and false predictions, the class-wise implementation accounts the prediction performance for each segmentation class which is strongly recommended for commonly class-unbalanced medical images. Another popular supported metric is the Jaccard Index. Even though it is similar to the Dice coefficient, it does not only emphasize on precise segmentation. However, it also penalizes under- and over-segmentation. Still, MIScnn uses the Tversky loss [<xref ref-type="bibr" rid="CR40">40</xref>] for training. Comparable to the Dice coefficient, the Tversky loss function addresses data imbalance. Even so, it achieves a much better trade-off between precision and recall. Thus, the Tversky loss function ensures good performance on binary, as well as multi-class segmentation. Additionally, all standard metrics which are included in Keras, like accuracy or cross-entropy, can be used in MIScnn. Next to the already implemented metrics or loss functions, MIScnn offers the integration of custom metrics for training and evaluation. A custom metric can be implemented as defined in Keras, and simply be passed to the deep learning model.</p>
      </sec>
    </sec>
    <sec id="Sec21">
      <title>Model utilization</title>
      <p id="Par41">With the initialized deep learning model and the fully preprocessed data, the model can now be used for training on the data to fit model weights or for prediction by using an already fitted model. Alternatively, the model can perform an evaluation, as well, by running a cross-validation for example, with multiple training and prediction calls. The model API allows saving and loading models in order to subsequently reuse already fitted models for prediction or for sharing pre-trained models.</p>
      <sec id="Sec22">
        <title>Training</title>
        <p id="Par42">In the process of training a convolutional neural network or deep learning model, diverse settings have to be configured. At this point in the pipeline, the data augmentation options of the data set, which have a large influence on the training in medical image segmentation, must be already defined. Sequentially, the batch management configuration covered the settings for the batch size, and also the batch shuffling at the end of each epoch. Therefore, only the learning rate and the number of epochs are required to be adjusted before running the training process. The learning rate of a neural network model is defined as the extend in which the old weights of the neural network model are updated in each iteration or epoch. In contrast, the number of epochs defines how many times the complete data set will be fitted into the model. Sequentially, the resulting fitted model can be saved to disk.</p>
        <p id="Par43">During the training, the underlying Keras framework gives insights into the current model performance with the predefined metrics, as well as the remaining fitting time. Additionally, MIScnn offers the usage of a fitting-evaluation callback functionality in which the fitting scores and metrics are stored into a tab separated file or directly plotted as a figure.</p>
      </sec>
      <sec id="Sec23">
        <title>Prediction</title>
        <p id="Par44">For the segmentation prediction, an already fitted neural network model can be directly used after training or it can be loaded from file. The model predicts for every pixel a Sigmoid value for each class. The Sigmoid value represents a probability estimation of this pixel for the associated label. Sequentially, the argmax of the One Hot encoded class are identified for multi-class segmentation problems and then converted back to a single result variable containing the class with the highest Sigmoid value.</p>
        <p id="Par45">When using the overlapping patch-wise analysis approach during the training, MIScnn supports two methods for patches in the prediction. Either the prediction process plainly creates distinct patches and treats the overlapping patches during the training as purely data augmentation, or overlapping patches are created for prediction. Due to the lack of prediction power at patch edges, computing a second prediction for edge pixels in patches, by using an overlap, is a commonly used approach. In the following merge of patches back to the original medical image shape, a merging strategy for the pixels is required, in the overlapping part of two patches and with multiple predictions. By default, MIScnn calculates the mean between the predicted Sigmoid values for each class in every overlapping pixel.</p>
        <p id="Par46">The resulting image matrix with the segmentation prediction, which has the identical shape as the original medical image, is saved into a file structure according to the provided data I/O interface. By default, using the NIfTI data I/O interface, the predicted segmentation matrix is saved in NIfTI format without any additional metadata.</p>
      </sec>
      <sec id="Sec24">
        <title>Evaluation</title>
        <p id="Par47">MIScnn supports multiple automatic evaluation techniques to investigate medical image segmentation performance: k-fold cross-validation, leave-one-out cross-validation, percentage-split validation, hold-out sets for testing (data set split into test and train set with a given percentage) and detailed validation in which it can be specified which images should be used for training and testing. Except for the detailed validation, all other evaluation techniques use random sampling to create training and testing data sets. During the evaluation, the predefined metrics and loss function for the model are automatically plotted in figures and saved in tab separated files for possible further analysis. Next to the performance metrics, the pixel value range and segmentation class frequency of medical images can be analyzed in the MIScnn evaluation. Also, the resulting prediction can be compared directly next to the ground truth by creation image visualizations with segmentation overlays. For 3D images, like MRIs, the slices with the segmentation overlays are automatically visualized in the Graphics Interchange Format (GIF).</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec25">
    <title>Results</title>
    <p id="Par48">Here, we analyze and evaluate data from the Kidney Tumor Segmentation Challenge 2019 using MIScnn. The main idea for this experiment is to demonstrate the ‘out-of-the-box’ performance of MIScnn without thorough and time-consuming optimization on the data set or on the medical abnormality. All results were obtained using the scripts shown in the <xref rid="Sec220" ref-type="sec">Appendix</xref>.</p>
    <sec id="Sec26">
      <title>Kidney Tumor Segmentation Challenge 2019 (KiTS19)</title>
      <p id="Par49">With more than 400,000 kidney cancer diagnoses worldwide in 2018, kidney cancer is under the top 10 most common cancer types in men and under the top 15 in woman [<xref ref-type="bibr" rid="CR41">41</xref>]. The development of advanced tumor visualization techniques is highly important for efficient surgical planning. Due to the variety in kidney and kidney tumor morphology, the automatic image segmentation is challenging but of great interest [<xref ref-type="bibr" rid="CR29">29</xref>].</p>
      <p id="Par50">The goal of the KiTS19 challenge is the development of reliable and unbiased kidney and kidney tumor semantic segmentation methods [<xref ref-type="bibr" rid="CR29">29</xref>]. Therefore, the challenge built a data set for arterial phase abdominal CT scan of 300 kidney cancer patients [<xref ref-type="bibr" rid="CR29">29</xref>]. The original scans have an image resolution of 512 × 512 and on average 216 slices (highest slice number is 1059). For all CT scans, a ground truth semantic segmentation was created by experts. This semantic segmentation labeled each pixel with one of three classes: Background, kidney or tumor. An example CT scan including annotation is shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. 210 of these CT scans with the ground truth segmentation were published during the training phase of the challenge, whereas 90 CT scans without published ground truth were released afterwards in the submission phase. The submitted user predictions for these 90 CT scans will be objectively evaluated and the user models ranked according to their performance. The CT scans were provided in NIfTI format in original resolution and also in interpolated resolution with slice thickness normalization.<fig id="Fig3"><label>Fig. 3</label><caption><p>Computed Tomography image from the Kidney Tumor Segmentation Challenge 2019 data set showing the ground truth segmentation of both kidneys (red) and tumor (blue) [<xref ref-type="bibr" rid="CR29">29</xref>]</p></caption><graphic xlink:href="12880_2020_543_Fig3_HTML" id="MO3"/></fig></p>
    </sec>
    <sec id="Sec27">
      <title>Validation on the KiTS19 data set with MIScnn</title>
      <p id="Par51">For the evaluation of the MIScnn framework usability and data augmentation quality, a subset of 120 CT scans with slice thickness normalization were retrieved from the KiTS19 data set. An automatic threefold cross-validation was run on this KiTS19 subset with MIScnn. In order to reduce the overfitting risk, the cross-validation testing sets had no influence on the fitting process and were not used for any automatic hyper parameter tuning.</p>
      <sec id="Sec28">
        <title>MIScnn configurations</title>
        <p id="Par52">The MIScnn pipeline was configured to perform a multi-class, patch-wise analysis with 80 × 160 × 160 patches and a batch size of 2. The pixel value normalization by Z-Score, clipping to the range − 79 and 304, as well as resampling to the voxel spacing 3.22 × 1.62 × 1.62.</p>
        <p id="Par53">For data augmentation, all implemented techniques were used. This includes creating patches through random cropping, scaling, rotations, elastic deformations, mirroring, brightness, contrast, gamma and Gaussian noise augmentations. For prediction, overlapping patches were created with an overlap size of 40 × 80 × 80 in x, y, z directions. The standard 3D U-Net with batch normalization layers were used as deep learning and convolutional neural network model. The training was performed using the Tversky loss for 1000 epochs with a starting learning rate of 1E−4 and batch shuffling after each epoch.</p>
        <p id="Par54">The cross-validation was run on two Nvidia Quadro P6000 (24 GB memory each), using 48 GB memory and taking 58 h.</p>
      </sec>
      <sec id="Sec29">
        <title>Results</title>
        <p id="Par55">With the MIScnn pipeline, it was possible to successfully set up a complete, working medical image multi-class segmentation pipeline. The threefold cross-validation of 120 CT scans for kidney and tumor segmentation were evaluated through several metrics: Tversky loss, soft Dice coefficient, class-wise Dice coefficient, as well as the sum of categorical cross-entropy and soft Dice coefficient. These scores were computed during the fitting itself, as well as for the prediction with the fitted model. For each cross-validation fold, the training and predictions scores are visualized in Fig. <xref rid="Fig4" ref-type="fig">4</xref> and sum up in Table <xref rid="Tab1" ref-type="table">1</xref>.<fig id="Fig4"><label>Fig. 4</label><caption><p>Performance evaluation of the standard 3D U-Net model for kidney and tumor prediction with a threefold cross-validation on the 120 CT data set from the KiTS19 challenge. Left: Tversky loss against epochs illustrating loss development during training for the corresponding test and train data sets. Each point represents the average Tversky loss between the cross-validation folds. Center: Class-wise Dice coefficient against epochs illustrating soft Dice similarity coefficient development during training for the corresponding test and train data sets. Each point represents the average soft Dice similarity coefficient between the cross-validation folds. Right: Dice similarity coefficient distribution for the kidney and tumor for all samples of the cross-validation</p></caption><graphic xlink:href="12880_2020_543_Fig4_HTML" id="MO4"/></fig><table-wrap id="Tab1"><label>Table 1</label><caption><p>Performance results of the threefold cross-validation for tumor and kidney segmentation with a standard 3D U-Net model on 120 CT scans from the KiTS19 challenge</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Metric</th><th align="left">Training</th><th align="left">Validation</th></tr></thead><tbody><tr><td align="left">Tversky loss</td><td char="." align="char">0.3672</td><td char="." align="char">0.4609</td></tr><tr><td align="left">Soft Dice similarity coefficient</td><td char="." align="char">0.8776</td><td char="." align="char">0.8235</td></tr><tr><td align="left">Categorical cross-entropy</td><td char="." align="char">− 0.8584</td><td char="." align="char">− 0.7899</td></tr><tr><td align="left">Dice similarity coefficient: background</td><td char="." align="char">–</td><td char="." align="char">0.9994</td></tr><tr><td align="left">Dice similarity coefficient: kidney</td><td char="." align="char">–</td><td char="." align="char">0.9319</td></tr><tr><td align="left">Dice similarity coefficient: tumor</td><td char="." align="char">–</td><td char="." align="char">0.6750</td></tr></tbody></table><table-wrap-foot><p>Each metric is computed between the provided ground truth and our model prediction and then averaged between the three folds</p></table-wrap-foot></table-wrap></p>
        <p id="Par56">The fitted model achieved a very strong performance for kidney segmentation. The kidney Dice coefficient had a median around 0.9544. The tumor segmentation prediction showed a considerably high but weaker performance than the kidney with a median around 0.7912.</p>
        <p id="Par57">Besides the computed metrics, MIScnn created segmentation visualizations for manual comparison between ground truth and prediction. As illustrated in Fig. <xref rid="Fig5" ref-type="fig">5</xref>, the predicted semantic segmentation of kidney and tumors is highly accurate.<fig id="Fig5"><label>Fig. 5</label><caption><p>Computed Tomography scans of kidney tumors from the Kidney Tumor Segmentation Challenge 2019 data set showing the kidney (red) and tumor (blue) segmentation as overlays. The images show the segmentation differences between the ground truth provided by the KiTS19 challenge and the prediction from the standard 3D U-Net models of our threefold cross-validation</p></caption><graphic xlink:href="12880_2020_543_Fig5_HTML" id="MO5"/></fig></p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec30">
    <title>Discussion</title>
    <sec id="Sec31">
      <title>MIScnn framework</title>
      <p id="Par58">With excellent performing convolutional neural network and deep learning models like the U-Net, the urge to move automatic medical image segmentation from the research labs into practical application in clinics is uprising. Still, the landscape of standalone pipelines of top performing models, designed only for a single specific public data set, handicaps this progress. The goal of MIScnn is to provide a high-level API to setup a medical image segmentation pipeline with preprocessing, data augmentation, model architecture selection and model utilization. MIScnn offers a highly configurable and open-source pipeline with several interfaces for custom deep learning models, image formats or fitting metrics. The modular structure of MIScnn allows a medical image segmentation novice to setup a functional pipeline for a custom data set in just a few lines of code. Additionally, switchable models and an automatic evaluation functionality allow robust and unbiased comparisons between deep learning models. A universal framework for medical image segmentation, following the Python philosophy of simple and intuitive modules, is an important step in contributing to practical application development.</p>
    </sec>
    <sec id="Sec32">
      <title>Use case: Kidney Tumor Segmentation Challenge</title>
      <p id="Par59">In order to show the reliability of MIScnn, a pipeline was setup for kidney tumors segmentation on a CT image data set. The popular and state-of-the-art standard U-Net were used as deep learning model with up-to-date data augmentation. Its predictive power was very impressive in the context of using only the standard U-Net architecture with mostly default hyperparameters. In the medical perspective, through the variety in kidney tumor morphology, which is one of the reasons for the KiTS19 challenge, the weaker tumor results are quite reasonable [<xref ref-type="bibr" rid="CR29">29</xref>]. Also, the models were trained with only 38% of the original KiTS19 data set due to 80 images for training and 40 for testing were randomly selected. The remaining 90 CTs were excluded in order to reduce run time in the cross-validation. Nevertheless, it was possible to build a powerful pipeline for kidney tumor segmentation with MIScnn resulting into a model with high performance, which is directly comparable with modern, optimized, standalone pipelines [<xref ref-type="bibr" rid="CR12">12</xref>, <xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR32">32</xref>]. We proved that with just a few lines of codes using the MIScnn framework, it was possible to successfully build a powerful pipeline for medical image segmentation. Additionally, fast switching the model to a more precise architecture for high resolution images, like the Dense U-Net model, would probably result into an even better performance [<xref ref-type="bibr" rid="CR15">15</xref>]. However, this gain would go hand in hand with an increased fitting time and higher GPU memory requirement, which was not possible with our current sharing schedule for GPU hardware. Nevertheless, the possibility of swift switching between models to compare their performance on a data set is a promising step forward in the field of medical image segmentation.</p>
    </sec>
    <sec id="Sec33">
      <title>Road map and future direction</title>
      <p id="Par60">The active MIScnn development is currently focused on multiple key features: Adding further data I/O interfaces for the most common medical image formats like DICOM, extend preprocessing and data augmentation methods, implement more efficient patch skipping techniques instead of excluding every blank patch (e.g. denoising patch skipping) and implementation of an open interface for custom preprocessing techniques for specific image types like MRIs. Next to the planned feature implementations, the MIScnn road map includes the model library extension with more state-of-the-art deep learning models for medical image segmentation. Additionally, an objective comparison of the U-Net model version variety is outlined to get more insights on different model performances with the same pipeline. Community contributions in terms of implementations or critique are welcomed and can be included after evaluation. Currently, MIScnn already offers a robust pipeline for medical image segmentation, nonetheless, it will still be regularly updated and extended in the future.</p>
    </sec>
    <sec id="Sec34">
      <title>MIScnn availability</title>
      <p id="Par61">The MIScnn framework can be directly installed as a Python library using pip install miscnn. Additionally, the source code is available in the Git repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/frankkramer-lab/MIScnn">https://github.com/frankkramer-lab/MIScnn</ext-link>. MIScnn is licensed under the open-source GNU General Public License Version 3. The code of the cross-validation experiment for the Kidney Tumor Segmentation Challenge is available as a Jupyter Notebook in the official Git repository.</p>
    </sec>
  </sec>
  <sec id="Sec35">
    <title>Conclusions</title>
    <p id="Par62">In this paper, we have introduced the open-source Python library MIScnn: A framework for medical image segmentation with convolutional neural networks and deep learning. The intuitive API allows fast building medical image segmentation pipelines including data I/O, preprocessing, data augmentation, patch-wise analysis, metrics, a library with state-of-the-art deep learning models and model utilization like training, prediction, as well as fully automatic evaluation (e.g. cross-validation). High configurability and multiple open interfaces allow users to fully customize the pipeline. This framework enables researchers to rapidly set up a complete medical image segmentation pipeline by using just a few lines of code. We proved the MIScnn functionality by running an automatic cross-validation on the Kidney Tumor Segmentation Challenge 2019 CT data set resulting into a powerful predictor. We hope that it will help migrating medical image segmentation from the research labs into practical applications.</p>
    <sec id="Sec36">
      <title>Availability and requirements</title>
      <p id="Par63">
        <list list-type="bullet">
          <list-item>
            <p id="Par64"><bold>Project name:</bold> MIScnn</p>
          </list-item>
          <list-item>
            <p id="Par65">
              <bold>Project home page:</bold>
              <ext-link ext-link-type="uri" xlink:href="https://github.com/frankkramer-lab/MIScnn">https://github.com/frankkramer-lab/MIScnn</ext-link>
            </p>
          </list-item>
          <list-item>
            <p id="Par66"><bold>Operating system(s):</bold> Platform independent</p>
          </list-item>
          <list-item>
            <p id="Par67"><bold>Programming language:</bold> Python</p>
          </list-item>
          <list-item>
            <p id="Par68"><bold>Other requirements:</bold> Python 3.6; Tensorflow 2.2.0</p>
          </list-item>
          <list-item>
            <p id="Par69"><bold>License:</bold> GPL-3.0 License</p>
          </list-item>
          <list-item>
            <p id="Par70"><bold>Any restrictions to use by non-academics:</bold> None</p>
          </list-item>
        </list>
      </p>
    </sec>
  </sec>
</body>
<back>
  <app-group>
    <app id="App1">
      <sec id="Sec220">
        <title>Appendix</title>
        <p id="Par74">Please check the Appendix document in the below mentioned path <list list-type="bullet"><list-item><p id="Par75">Direct URL: <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/3962097">https://zenodo.org/record/3962097</ext-link></p></list-item><list-item><p id="Par76">DOI URL: 10.5281/zenodo.3962097</p></list-item></list></p>
      </sec>
    </app>
  </app-group>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>MIScnn</term>
        <def>
          <p id="Par5">Medical Image Segmentation with Convolutional Neural Networks</p>
        </def>
      </def-item>
      <def-item>
        <term>CT</term>
        <def>
          <p id="Par6">Computed tomography</p>
        </def>
      </def-item>
      <def-item>
        <term>I/O</term>
        <def>
          <p id="Par7">Input/output</p>
        </def>
      </def-item>
      <def-item>
        <term>MRI</term>
        <def>
          <p id="Par8">Magnetic Resonance Imaging</p>
        </def>
      </def-item>
      <def-item>
        <term>NifTI</term>
        <def>
          <p id="Par9">Neuroimaging Informatics Technology Initiative</p>
        </def>
      </def-item>
      <def-item>
        <term>GPU</term>
        <def>
          <p id="Par10">Graphics processing unit</p>
        </def>
      </def-item>
      <def-item>
        <term>RAM</term>
        <def>
          <p id="Par11">Random Access Memory</p>
        </def>
      </def-item>
      <def-item>
        <term>CPU</term>
        <def>
          <p id="Par12">Central Processing Unit</p>
        </def>
      </def-item>
      <def-item>
        <term>API</term>
        <def>
          <p id="Par13">Application programming interface</p>
        </def>
      </def-item>
      <def-item>
        <term>KiTS19</term>
        <def>
          <p id="Par14">Kidney Tumor Segmentation Challenge 2019</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We want to thank Bernhard Bauer and Fabian Rabe for sharing their GPU hardware (2x Nvidia Quadro P6000) with us which was used for this work. We also want to thank Dennis Klonnek, Florian Auer, Barbara Forastefano, Edmund Müller and Iñaki Soto Rey for their useful comments.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>FK was in charge of coordination, review, and correction of the manuscript. DM contributed to the conception and design of this work, its data analysis and interpretation, and was in charge for draft and revise the manuscript. All the authors are accountable for the integrity of this work. Both authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Open Access funding enabled and organized by Projekt DEAL. This work has been supported by the German Ministry of Education and Research (BMBF) Grant 01ZZ1804E.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The KiTS19 dataset analyzed in this article is available in the GitHub repository, <ext-link ext-link-type="uri" xlink:href="https://github.com/neheller/kits19">https://github.com/neheller/kits19</ext-link>. All data generated and analyzed during this study is available in the zenodo repository, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3962097">https://doi.org/10.5281/zenodo.3962097</ext-link>. The code for this article was implemented in Python (platform independent) and is available under the GPL-3.0 License at the following GitHub repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/frankkramer-lab/MIScnn">https://github.com/frankkramer-lab/MIScnn</ext-link>.</p>
  </notes>
  <notes id="FPar1">
    <title>Ethics approval and consent to participate</title>
    <p id="Par71">Not applicable.</p>
  </notes>
  <notes id="FPar2">
    <title>Consent for publication</title>
    <p id="Par72">Not applicable.</p>
  </notes>
  <notes id="FPar3" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par73">The authors declare no conflicts of interest.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Aggarwal</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Vig</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Bhadoria</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Dethe</surname>
            <given-names>CG</given-names>
          </name>
        </person-group>
        <article-title>Role of segmentation in medical imaging: a comparative study</article-title>
        <source>Int J Comput Appl</source>
        <year>2011</year>
        <volume>29</volume>
        <fpage>54</fpage>
        <lpage>61</lpage>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gibelli</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Cellina</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gibelli</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Oliva</surname>
            <given-names>AG</given-names>
          </name>
          <name>
            <surname>Termine</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Pucciarelli</surname>
            <given-names>V</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Assessing symmetry of zygomatic bone through three-dimensional segmentation on computed tomography scan and “mirroring” procedure: a contribution for reconstructive maxillofacial surgery</article-title>
        <source>J Cranio-Maxillofac Surg</source>
        <year>2018</year>
        <volume>46</volume>
        <fpage>600</fpage>
        <lpage>604</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jcms.2018.02.012</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cellina</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gibelli</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Cappella</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Toluian</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Pittino</surname>
            <given-names>CV</given-names>
          </name>
          <name>
            <surname>Carlo</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Segmentation procedures for the assessment of paranasal sinuses volumes</article-title>
        <source>Neuroradiol J.</source>
        <year>2020</year>
        <pub-id pub-id-type="doi">10.1177/1971400920946635</pub-id>
        <?supplied-pmid 32757847?>
        <pub-id pub-id-type="pmid">32757847</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Scott</surname>
            <given-names>MR</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Brain SegNet: 3D local refinement network for brain lesion segmentation</article-title>
        <source>BMC Med Imaging</source>
        <year>2020</year>
        <volume>20</volume>
        <fpage>17</fpage>
        <pub-id pub-id-type="doi">10.1186/s12880-020-0409-2</pub-id>
        <?supplied-pmid 32046685?>
        <pub-id pub-id-type="pmid">32046685</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sun</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ti</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A potential field segmentation based method for tumor segmentation on multi-parametric MRI of glioma cancer patients</article-title>
        <source>BMC Med Imaging</source>
        <year>2019</year>
        <volume>19</volume>
        <fpage>48</fpage>
        <pub-id pub-id-type="doi">10.1186/s12880-019-0348-y</pub-id>
        <?supplied-pmid 31208349?>
        <pub-id pub-id-type="pmid">31208349</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Claudia</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Farida</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Guy</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Marie-Claude</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Carl-Eric</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Quantitative evaluation of an automatic segmentation method for 3D reconstruction of intervertebral scoliotic disks from MR images</article-title>
        <source>BMC Med Imaging</source>
        <year>2012</year>
        <volume>12</volume>
        <fpage>26</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2342-12-26</pub-id>
        <?supplied-pmid 22856667?>
        <pub-id pub-id-type="pmid">22856667</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Georgiou</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Lew</surname>
            <given-names>MS</given-names>
          </name>
        </person-group>
        <article-title>A review of semantic segmentation using deep neural networks</article-title>
        <source>Int J Multimed Inf Retr</source>
        <year>2018</year>
        <volume>7</volume>
        <fpage>87</fpage>
        <lpage>93</lpage>
        <pub-id pub-id-type="doi">10.1007/s13735-017-0141-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Anwar</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Majid</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Qayyum</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Awais</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Alnowami</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>MK</given-names>
          </name>
        </person-group>
        <article-title>Medical image analysis using convolutional neural networks: a review</article-title>
        <source>J Med Syst.</source>
        <year>2018</year>
        <volume>42</volume>
        <fpage>226</fpage>
        <pub-id pub-id-type="doi">10.1007/s10916-018-1088-1</pub-id>
        <?supplied-pmid 30298337?>
        <pub-id pub-id-type="pmid">30298337</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>A perspective on deep imaging</article-title>
        <source>IEEE Access</source>
        <year>2016</year>
        <volume>4</volume>
        <fpage>8914</fpage>
        <lpage>8924</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2016.2624938</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Litjens</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Kooi</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Bejnordi</surname>
            <given-names>BE</given-names>
          </name>
          <name>
            <surname>Setio</surname>
            <given-names>AAA</given-names>
          </name>
          <name>
            <surname>Ciompi</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Ghafoorian</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A survey on deep learning in medical image analysis</article-title>
        <source>Med Image Anal</source>
        <year>2012</year>
        <volume>2017</volume>
        <issue>42</issue>
        <fpage>60</fpage>
        <lpage>88</lpage>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shen</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Suk</surname>
            <given-names>H-I</given-names>
          </name>
        </person-group>
        <article-title>Deep learning in medical image analysis</article-title>
        <source>Annu Rev Biomed Eng</source>
        <year>2017</year>
        <volume>19</volume>
        <fpage>221</fpage>
        <lpage>248</lpage>
        <pub-id pub-id-type="doi">10.1146/annurev-bioeng-071516-044442</pub-id>
        <?supplied-pmid 5479722?>
        <pub-id pub-id-type="pmid">28301734</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ronneberger</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Fischer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Brox</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>U-Net: convolutional networks for biomedical image segmentation</article-title>
        <source>Lect Notes Comput Sci (including SubserLect Notes ArtifIntellLect Notes Bioinformatics)</source>
        <year>2015</year>
        <volume>9351</volume>
        <fpage>234</fpage>
        <lpage>241</lpage>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Zhou Z, Siddiquee MMR, Tajbakhsh N, Liang J. UNet++: a nested U-Net architecture for medical image segmentation. 2018. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1807.10165">http://arxiv.org/abs/1807.10165</ext-link>. Accessed 19 Jul 2019.</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Isensee F, Petersen J, Klein A, Zimmerer D, Jaeger PF, Kohl S, et al. nnU-Net: self-adapting framework for U-Net-based medical image segmentation. 2018. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1809.10486">http://arxiv.org/abs/1809.10486</ext-link>. Accessed 19 Jul 2019.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kolařík</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Burget</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Uher</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Říha</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Dutta</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Optimized high resolution 3D dense-U-Net network for brain and spine segmentation</article-title>
        <source>Appl Sci</source>
        <year>2019</year>
        <volume>9</volume>
        <fpage>404</fpage>
        <pub-id pub-id-type="doi">10.3390/app9030404</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Çiçek</surname>
            <given-names>Ö</given-names>
          </name>
          <name>
            <surname>Abdulkadir</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lienkamp</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Brox</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Ronneberger</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>3D U-net: learning dense volumetric segmentation from sparse annotation</article-title>
        <source>Lect Notes Comput Sci</source>
        <year>2016</year>
        <volume>9901</volume>
        <fpage>424</fpage>
        <lpage>432</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-319-46723-8_49</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Lee K, Zung J, Li P, Jain V, Seung HS. Superhuman accuracy on the SNEMI3D connectomics challenge. 2017; Nips:1–11. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1706.00120">http://arxiv.org/abs/1706.00120</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gibson</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Sudre</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Fidon</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Shakir</surname>
            <given-names>DI</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>NiftyNet: a deep-learning platform for medical imaging</article-title>
        <source>Comput Methods Programs Biomed</source>
        <year>2018</year>
        <volume>158</volume>
        <fpage>113</fpage>
        <lpage>122</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cmpb.2018.01.025</pub-id>
        <?supplied-pmid 29544777?>
        <pub-id pub-id-type="pmid">29544777</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Neuroimaging Informatics Technology Initiative. <ext-link ext-link-type="uri" xlink:href="https://nifti.nimh.nih.gov/background">https://nifti.nimh.nih.gov/background</ext-link>. Accessed 19 Jul 2019.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Roy S, Carass A, Prince JL. Patch based intensity normalization of brain MR images. In: Proceedings—international symposium on biomedical imaging. 2013.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Nyú LG, Udupa JK. On standardizing the MR image intensity scale. Magn Reson Med. 1999;42:1072–81. <pub-id pub-id-type="doi">10.1002/(SICI)1522-2594(199912)42:6&lt;1072::AID-MRM11&gt;3.0.CO;2-M</pub-id></mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Isensee F, Maier-Hein KH. An attempt at beating the 3D U-Net. 2019;1–7. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1908.02182">http://arxiv.org/abs/1908.02182</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Van Den</surname>
            <given-names>HA</given-names>
          </name>
          <name>
            <surname>Reid</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Efficient piecewise training of deep structured models for semantic segmentation</article-title>
        <source>Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit</source>
        <year>2016</year>
        <volume>2016</volume>
        <fpage>3194</fpage>
        <lpage>3203</lpage>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hussain</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Gimenez</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Yi</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Rubin</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Differential data augmentation techniques for medical imaging classification tasks</article-title>
        <source>Annu Symp Proc AMIA Symp</source>
        <year>2017</year>
        <volume>2017</volume>
        <fpage>979</fpage>
        <lpage>984</lpage>
        <?supplied-pmid 29854165?>
        <pub-id pub-id-type="pmid">29854165</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Eaton-rosen Z, Bragman F. Improving data augmentation for medical image segmentation. Midl. 2018; 1–3.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Perez L, Wang J. The effectiveness of data augmentation in image classification using deep learning. 2017. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1712.04621">http://arxiv.org/abs/1712.04621</ext-link>. Accessed 23 Jul 2019.</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Taylor L, Nitschke G. Improving deep learning using generic data augmentation. 2017. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1708.06020">http://arxiv.org/abs/1708.06020</ext-link>. Accessed 23 Jul 2019.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Isensee F, Jäger P, Wasserthal J, Zimmerer D, Petersen J, Kohl S, et al. batchgenerators—a python framework for data augmentation. 2020. 10.5281/zenodo.3632567.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Heller N, Sathianathen N, Kalapara A, Walczak E, Moore K, Kaluzniak H, et al. The KiTS19 challenge data: 300 kidney tumor cases with clinical context, CT semantic segmentations, and surgical outcomes. 2019. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1904.00445">http://arxiv.org/abs/1904.00445</ext-link>. Accessed 19 Jul 2019.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Coupé</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Manjón</surname>
            <given-names>JV</given-names>
          </name>
          <name>
            <surname>Fonov</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Pruessner</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Robles</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Collins</surname>
            <given-names>DL</given-names>
          </name>
        </person-group>
        <article-title>Patch-based segmentation using expert priors: application to hippocampus and ventricle segmentation</article-title>
        <source>Neuroimage</source>
        <year>2011</year>
        <volume>54</volume>
        <fpage>940</fpage>
        <lpage>954</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.09.018</pub-id>
        <?supplied-pmid 20851199?>
        <pub-id pub-id-type="pmid">20851199</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Ourselin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Vercauteren</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Automatic brain tumor segmentation using cascaded anisotropic convolutional neural networks</article-title>
        <source>Lect Notes Comput Sci (including SubserLect Notes ArtifIntellLect Notes Bioinformatics)</source>
        <year>2018</year>
        <volume>10670</volume>
        <fpage>178</fpage>
        <lpage>190</lpage>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Road extraction by deep residual U-Net</article-title>
        <source>IEEE Geosci Remote Sens Lett</source>
        <year>2018</year>
        <volume>15</volume>
        <fpage>749</fpage>
        <lpage>753</lpage>
        <pub-id pub-id-type="doi">10.1109/LGRS.2018.2802944</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Iglovikov V, Shvets A. TernausNet: U-Net with VGG11 encoder pre-trained on ImageNet for image segmentation. 2018. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1801.05746">http://arxiv.org/abs/1801.05746</ext-link>. Accessed 19 Jul 2019.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Ibtehaz N, Rahman MS. MultiResUNet : Rethinking the U-Net architecture for multimodal biomedical image segmentation. 2019. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1902.04049">http://arxiv.org/abs/1902.04049</ext-link>. Accessed 19 Jul 2019.</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kamnitsas</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Bai</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Ferrante</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>McDonagh</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sinclair</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pawlowski</surname>
            <given-names>N</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Ensembles of multiple models and architectures for robust brain tumour segmentation</article-title>
        <source>Lect Notes Comput Sci (including SubserLect Notes ArtifIntellLect Notes Bioinformatics)</source>
        <year>2018</year>
        <volume>10670</volume>
        <fpage>450</fpage>
        <lpage>462</lpage>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Valverde</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Salem</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Cabezas</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pareto</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Vilanova</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Ramió-Torrentà</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>One-shot domain adaptation in multiple sclerosis lesion segmentation using convolutional neural networks</article-title>
        <source>NeuroImage Clin</source>
        <year>2018</year>
        <volume>2019</volume>
        <issue>21</issue>
        <fpage>101638</fpage>
        <pub-id pub-id-type="doi">10.1016/j.nicl.2018.101638</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brosch</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>LYW</given-names>
          </name>
          <name>
            <surname>Yoo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>DKB</given-names>
          </name>
          <name>
            <surname>Traboulsee</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tam</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Deep 3D convolutional encoder networks with shortcuts for multiscale feature integration applied to multiple sclerosis lesion segmentation</article-title>
        <source>IEEE Trans Med Imaging</source>
        <year>2016</year>
        <volume>35</volume>
        <fpage>1229</fpage>
        <lpage>1239</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2016.2528821</pub-id>
        <pub-id pub-id-type="pmid">26886978</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Chollet, François, others. Keras. 2015. <ext-link ext-link-type="uri" xlink:href="https://keras.io">https://keras.io</ext-link>. Accessed 19 Jul 2019.</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, et al. TensorFlow: large-scale machine learning on heterogeneous systems. 2015. <ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/">https://www.tensorflow.org/</ext-link>. Accessed 19 Jul 2019.</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Seyed SSM, Erdogmus D, Gholipour A, Salehi SSM, Erdogmus D, Gholipour A. Tversky loss function for image segmentation using 3D fully convolutional deep networks. In: Lecture notes in computer science. Springer; 2017. p. 379–87. 10.1007/978-3-319-67389-9_44.</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ferlay</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Colombet</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Soerjomataram</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Mathers</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Parkin</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Piñeros</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Estimating the global cancer incidence and mortality in 2018: GLOBOCAN sources and methods</article-title>
        <source>Int J Cancer</source>
        <year>2018</year>
        <pub-id pub-id-type="doi">10.1002/ijc.31937</pub-id>
        <?supplied-pmid 30350310?>
        <pub-id pub-id-type="pmid">30350310</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
