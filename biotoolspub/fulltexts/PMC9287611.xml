<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_ISCI104644 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEfx1 jpg ?>
<?FILEmmc1 zip ?>
<?FILEsi1 gif ?>
<?FILEsi2 gif ?>
<?FILEsi3 gif ?>
<?FILEsi4 gif ?>
<?FILEsi5 gif ?>
<?FILEsi6 gif ?>
<?FILEsi7 gif ?>
<?FILEsi8 gif ?>
<?FILEsi9 gif ?>
<?FILEsi10 gif ?>
<?FILEsi11 gif ?>
<?FILEsi12 gif ?>
<?FILEsi13 gif ?>
<?FILEsi14 gif ?>
<?FILEsi15 gif ?>
<?FILEsi16 gif ?>
<?FILEsi17 gif ?>
<?FILEsi18 gif ?>
<?FILEsi19 gif ?>
<?FILEsi20 gif ?>
<?FILEsi21 gif ?>
<?FILEsi22 gif ?>
<?FILEsi23 gif ?>
<?FILEsi24 gif ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">iScience</journal-id>
    <journal-id journal-id-type="iso-abbrev">iScience</journal-id>
    <journal-title-group>
      <journal-title>iScience</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2589-0042</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9287611</article-id>
    <article-id pub-id-type="pii">S2589-0042(22)00916-6</article-id>
    <article-id pub-id-type="doi">10.1016/j.isci.2022.104644</article-id>
    <article-id pub-id-type="publisher-id">104644</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>motilitAI: A machine learning framework for automatic prediction of human sperm motility</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au1">
        <name>
          <surname>Ottl</surname>
          <given-names>Sandra</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au2">
        <name>
          <surname>Amiriparian</surname>
          <given-names>Shahin</given-names>
        </name>
        <email>shahin.amiriparian@informatik.uni-augsburg.de</email>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="fn1" ref-type="fn">3</xref>
        <xref rid="cor1" ref-type="corresp">∗</xref>
      </contrib>
      <contrib contrib-type="author" id="au3">
        <name>
          <surname>Gerczuk</surname>
          <given-names>Maurice</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au4">
        <name>
          <surname>Schuller</surname>
          <given-names>Björn W.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="aff2" ref-type="aff">2</xref>
      </contrib>
      <aff id="aff1"><label>1</label>Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany</aff>
      <aff id="aff2"><label>2</label>GLAM – Group on Language, Audio, and Music, Imperial College London, UK</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>∗</label>Corresponding author <email>shahin.amiriparian@informatik.uni-augsburg.de</email></corresp>
      <fn id="fn1">
        <label>3</label>
        <p id="ntpara0010">Lead contact</p>
      </fn>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>20</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <day>19</day>
      <month>8</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>20</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <volume>25</volume>
    <issue>8</issue>
    <elocation-id>104644</elocation-id>
    <history>
      <date date-type="received">
        <day>2</day>
        <month>12</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>11</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>15</day>
        <month>6</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder/>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="abs0010">
      <title>Summary</title>
      <p>In this article, human semen samples from the Visem dataset are automatically assessed with machine learning methods for their quality with respect to sperm motility. Several regression models are trained to automatically predict the percentage (0–100) of progressive, non-progressive, and immotile spermatozoa. The videos are adopted for unsupervised tracking and two different feature extraction methods—in particular custom movement statistics and displacement features. We train multiple neural networks and support vector regression models on the extracted features. Best results are achieved using a linear Support Vector Regressor with an aggregated and quantized representation of individual displacement features of each sperm cell. Compared to the best submission of the Medico Multimedia for Medicine challenge, which used the same dataset and splits, the mean absolute error (MAE) could be reduced from 8.83 to 7.31. We provide the source code for our experiments on GitHub (Code available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/EIHW/motilitAI" id="intref0010">https://github.com/EIHW/motilitAI</ext-link>).</p>
    </abstract>
    <abstract abstract-type="graphical" id="abs0015">
      <title>Graphical abstract</title>
      <fig id="undfig1" position="anchor">
        <graphic xlink:href="fx1"/>
      </fig>
    </abstract>
    <abstract abstract-type="author-highlights" id="abs0020">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="ulist0010">
          <list-item id="u0010">
            <label>•</label>
            <p id="p0010">Improvements to state of the art in automatic human sperm motility prediction</p>
          </list-item>
          <list-item id="u0015">
            <label>•</label>
            <p id="p0015">Unsupervised feature quantization used with off-the-shelf tracking algorithms</p>
          </list-item>
          <list-item id="u0020">
            <label>•</label>
            <p id="p0020">Framework publicly available on GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/EIHW/motilitAI" id="intref0015">https://github.com/EIHW/motilitAI</ext-link></p>
          </list-item>
        </list>
      </p>
    </abstract>
    <abstract abstract-type="teaser" id="abs0025">
      <p>Health sciences; Medicine; Reproductive medicine; Bioinformatics; Biocomputational method; Artificial intelligence</p>
    </abstract>
    <kwd-group id="kwrds0010">
      <title>Subject areas</title>
      <kwd>Health sciences</kwd>
      <kwd>Medicine</kwd>
      <kwd>Reproductive medicine</kwd>
      <kwd>Bioinformatics</kwd>
      <kwd>Biocomputational method</kwd>
      <kwd>Artificial intelligence</kwd>
    </kwd-group>
  </article-meta>
  <notes>
    <p id="misc0010">Published: August 19, 2022</p>
  </notes>
</front>
<body>
  <sec id="sec1">
    <title>Introduction</title>
    <p id="p0045">eHealth or “the use of information and communications technology in support of health and health-related fields” (<xref rid="bib47" ref-type="bibr">World Health Organization, 2017</xref>) has been a prioritized item on the agenda of the World Health Organization (WHO) since 2005 (<xref rid="bib44" ref-type="bibr">World Health Organization, 2005a</xref>). From then until 2016, the percentage of WHO member states that have a national eHealth policy in place has risen to <inline-formula><mml:math id="M1" altimg="si1.gif"><mml:mrow><mml:mn>58</mml:mn><mml:mspace width="0.25em"/><mml:mtext>%</mml:mtext></mml:mrow></mml:math></inline-formula> (<xref rid="bib46" ref-type="bibr">World Health Organization, 2016</xref>). eHealth is further considered an important factor for improving both the quality and availability of affordable health care, moving countries closer toward achieving universal health coverage (<xref rid="bib45" ref-type="bibr">World Health Organization, 2005b</xref>).</p>
    <p id="p0050">One such issue can, for example, be found with fertility-related problems (<xref rid="bib48" ref-type="bibr">Yee et al., 2013</xref>). Across the globe, approximately 8%–12% of couples are affected by infertility (<xref rid="bib37" ref-type="bibr">Stephen and Chandra, 1998</xref>; <xref rid="bib26" ref-type="bibr">Kumar and Singh, 2015</xref>) which is defined as the inability to achieve a clinical pregnancy after 12 or more months of regular unprotected sexual intercourse (<xref rid="bib49" ref-type="bibr">Zegers-Hochschild et al., 2009</xref>; <xref rid="bib34" ref-type="bibr">Practice Committee of the American Society for Reproductive Medicine, 2008</xref>). The issue can be a result of both male and female factor infertility (<xref rid="bib26" ref-type="bibr">Kumar and Singh, 2015</xref>). In males, infertility is often related to deficiencies in sperm quality measured by characteristics and reference values defined by the WHO (<xref rid="bib9" ref-type="bibr">Cooper et al., 2010</xref>). The attributes most strongly associated with fertility can be found in the concentration, motility, and morphology of sperm (<xref rid="bib26" ref-type="bibr">Kumar and Singh, 2015</xref>). The analysis of these characteristics can serve as a valuable baseline for diagnosis and treatment of patients but requires either specialized, expensive medical equipment or manual annotation by trained medical staff (<xref rid="bib11" ref-type="bibr">David et al., 1981</xref>; <xref rid="bib33" ref-type="bibr">Mortimer et al., 2015</xref>). In this respect, machine learning approaches that use video recordings of semen (the seminal fluid which contains the sperm) samples to detect morphology and motility of the included spermatozoa could assist physicians in their work. To work toward this goal, the <italic>Visem</italic> dataset (<xref rid="bib15" ref-type="bibr">Haugen et al., 2019</xref>) collected and released by the <italic>Simula Research Laboratory</italic> contains microscopic recordings of semen samples which are additionally annotated with regards to the mentioned characteristics of spermatozoa quality. In our work, we make use of the <italic>Visem</italic> dataset. It is a freely available dataset on which a range of state-of-the-art machine learning methods have been applied facilitating a better comparison of our framework’s efficacy with a wide variety of methods.</p>
    <p id="p0055">For this paper, a novel combination of unsupervised tracking, feature extraction and quantization methods, and machine learning models is investigated to perform automatic analysis of the motility of recorded spermatozoa cells. Motility means observing the speed and way of movement of sperm, i.e., if they travel on a straight path or in a circle. Furthermore, before extracting features, the data from the <italic>Visem</italic> dataset are preprocessed to minimize the negative impact that might come from blurred camera settings and numerous cuts within each video. The effectiveness of the applied feature extraction methods and machine learning models are compared to the approaches provided by the data organizers and state-of-the-art deep learning-based methodologies from various research groups. These contributions have been submitted to the <italic>Medico: Multimedia for Medicine</italic> sub-challenge (<xref rid="bib17" ref-type="bibr">Hicks et al., 2019b</xref>) that was part of the 2019 edition of the Medieval challenge.</p>
    <p id="p0060">The remainder of this paper is structured as follows. The proceeding section reviews the related work on computer-aided sperm analysis (CASA), and more specifically on automated prediction of sperm motility. In “sec:dataset”, we describe the dataset. “sec:expset” follows with the illustration of our approach, including preprocessing, particle tracking algorithms, feature extraction, and machine learning models. All accomplished results are listed in “sec:results” and discussed in “sec:discussion”. Finally, we give a conclusion and suggestions for future work in “sec:conclusion”.</p>
    <sec id="sec1.1">
      <title>Related work</title>
      <p id="p0065">Sperm motility characteristics have been defined in the official WHO lab manual (<xref rid="bib43" ref-type="bibr">World Health Organization, 1999</xref>). The motility of sperm cells can further be analyzed by classifying them according to multiple categories (<xref rid="bib7" ref-type="bibr">Björndahl et al., 2010</xref>). Spermatozoa can either be immotile or motile, where for motile sperm, further categorization can be applied. A particular cell is motile if its tail is beating. The beating of the tail alone, however, does not translate to effective movement. Therefore, motile sperm cells are additionally grouped according to the progressivity of their movement. Non-progressive spermatozoa beat their tails without any net gain in space, whereas progressive cells do gain space in the process (<xref rid="bib7" ref-type="bibr">Björndahl et al., 2010</xref>).</p>
      <p id="p0070">Traditionally, these characteristics had to be assessed manually by trained clinical staff (<xref rid="bib33" ref-type="bibr">Mortimer et al., 2015</xref>; <xref rid="bib11" ref-type="bibr">David et al., 1981</xref>), but advancements in computational hardware led to the introductions of CASA (<xref rid="bib29" ref-type="bibr">Mortimer, 1990</xref>). CASA works well for many non-human species (<xref rid="bib21" ref-type="bibr">Van der Horst et al., 2009</xref>; <xref rid="bib28" ref-type="bibr">Lueders et al., 2012</xref>), but has traditionally struggled with the accurate assessment of male fertility characteristics from microscopic video recordings of human sperm cells (<xref rid="bib7" ref-type="bibr">Björndahl et al., 2010</xref>). This discrepancy is caused both by biological as well as technical limitations (<xref rid="bib30" ref-type="bibr">Mortimer, 1994</xref>; <xref rid="bib32" ref-type="bibr">Mortimer and Mortimer, 1998</xref>; <xref rid="bib31" ref-type="bibr">Mortimer et al., 1995</xref>). First of all, from a biological perspective, human sperm has many characteristics that are detrimental to automatic analysis, such as high amounts of debris particles, generally lower sperm motility and concentration, and many dead spermatozoa which are often also clumped together (<xref rid="bib33" ref-type="bibr">Mortimer et al., 2015</xref>). As a consequence, while progressive movement can be detected quite accurately, non-progressive motile sperm cells are very hard to automatically differentiate from drifting debris or dead spermatozoa (<xref rid="bib31" ref-type="bibr">Mortimer et al., 1995</xref>). Furthermore, the clumping of alive cells with debris or dead spermatozoa can negatively affect the automatic tracking, leading to missing and interrupted tracks (<xref rid="bib33" ref-type="bibr">Mortimer et al., 2015</xref>). Morphology is especially hard to assess by commercial CASA systems, as accurate analysis is only possible for sperm heads (<xref rid="bib32" ref-type="bibr">Mortimer and Mortimer, 1998</xref>). Especially for motility, the CASA systems base their analysis on computing various kinematic statistics about each sperm track and then using those for determining progressive and non-progressive motility based on agreed-upon rules and thresholds (<xref rid="bib33" ref-type="bibr">Mortimer et al., 2015</xref>). Therefore, advancements made for these systems are mainly aimed at mitigating the problems and limitations that arise from the general quality of human sperm, such as eliminating drift, recovering sperm tracks through collision, or detecting cells that are clumped together (<xref rid="bib33" ref-type="bibr">Mortimer et al., 2015</xref>). <xref rid="bib41" ref-type="bibr">Urbano et al. (2016)</xref>, for example, implemented a robust multi-target sperm tracking algorithm that is able to effectively deal with collisions based on the joint probabilistic data association filter (<xref rid="bib6" ref-type="bibr">Bar-Shalom et al., 2009</xref>). <xref rid="bib20" ref-type="bibr">Hidayatullah et al. (2021)</xref> have proposed a machine learning framework for the prediction of bull sperm motility using a Support Vector Machine (SVM) classifier combined with three CASA parameters: curvilinear velocity, straight-line velocity, and linearity. The authors have demonstrated the efficacy of their approach and indicated that their method could be utilized for examining human sperm (<xref rid="bib20" ref-type="bibr">Hidayatullah et al., 2021</xref>). Apart from the video-based CASA systems, signal processing-based machines, such as the SQA-V Gold Semen Analyzer (SQA-Vision – The Ultimate Automated Semen Analysis Solution for Hospitals, Reproductive Centers, Free Standing Labs, and Cryobanks, available at <ext-link ext-link-type="uri" xlink:href="http://mes-global.com/analyzers" id="intref0020">http://mes-global.com/analyzers</ext-link>), exist that provide more accurate results but are expensive, prohibiting their use in developing countries. Many CASA systems used in research and medical applications are closed-source, proprietary software, or integrated hardware-software solutions. However, recently, developments toward the introduction of open-source alternatives into the field have been made, e. g., with openCASA (<xref rid="bib2" ref-type="bibr">Alquézar-Baeta et al., 2019</xref>). Furthermore, applications that solve individual parts of the automatic sperm analysis task can be found with particle tracking software, such as <italic>Trackpy</italic> (<xref rid="bib1" ref-type="bibr">Allan et al., 2019</xref>), or motility analysis toolkits for inference of cell state (<xref rid="bib25" ref-type="bibr">Kimmel et al., 2018</xref>).</p>
      <p id="p0075">The advancements in the field of machine learning, especially deep learning (DL) for image analysis, also made an impact on the field, leading to new possibilities for micro cinematographic approaches. Recently, <xref rid="bib42" ref-type="bibr">Valiuškaitė et al. (2020)</xref> have applied region-based convolutional neural networks (RCNNs) to evaluate sperm head motility in human semen videos. In particular, the authors first applied a Faster R-CNN—with ImageNet (<xref rid="bib22" ref-type="bibr">Huang et al., 2017</xref>; <xref rid="bib12" ref-type="bibr">Deng et al., 2009</xref>) pre-trained convolutional neural networks (CNNs)—for sperm head segmentation and then used a heuristic algorithm for sperm motility calculation. Compared to the above methods, the approach taken in our work does not focus on achieving the best possible tracking accuracy. Rather, we show that through the use of unsupervised feature learning and quantization, noisy or inaccurate sperm tracks can still perform well in downstream motility prediction tasks. The tracking methods used in our experiments work off-the-shelf, i.e., they are not adapted to the particularities of a specific dataset, as would be the case when using deep neural networks (DNNs) that are trained on the database at hand. In 2019, the Medico Multimedia for Medicine challenge (<xref rid="bib17" ref-type="bibr">Hicks et al., 2019b</xref>) presented researchers with the opportunity to develop automatic analysis systems for the assessment of human semen quality. The challenge dataset, <italic>Visem</italic>, contains 85 video recordings of semen samples which are annotated with regard to morphology and motility of the recorded sperm cells on a per-video basis. While there are only a handful of challenge submissions (<xref rid="bib18" ref-type="bibr">Hicks et al., 2019c</xref>; <xref rid="bib38" ref-type="bibr">Thambawita et al., 2019a</xref>, <xref rid="bib39" ref-type="bibr">b</xref>), they all used current deep learning approaches and showed that video-based analysis can provide insight into important characteristics of spermatozoa health. For the task of motility prediction, their CNN-based models could improve significantly over both a ZeroR baseline as well as models based on traditional image features and regression algorithms (<xref rid="bib18" ref-type="bibr">Hicks et al., 2019c</xref>, <xref rid="bib16" ref-type="bibr">a</xref>).</p>
      <p id="p0080">More related to the methodology applied in this paper, a feature representation of textual documents from the field of Natural Language Processing, namely the Bag-of-Words model, has recently been applied to other domains. One such example can be found in the study by (<xref rid="bib5" ref-type="bibr">Amiriparian et al., 2018</xref>; <xref rid="bib3" ref-type="bibr">Amiriparian, 2019</xref>), where deep feature vectors are aggregated and quantized in an unsupervised fashion to form noise-robust feature representations for a number of audio analysis tasks. Similarly, <xref rid="bib4" ref-type="bibr">Amiriparian et al. (2017)</xref> applied Bags-of-Deep-Features for the task of video sentiment analysis. In this work, a similar model is employed to generate feature representations for entire sperm samples from individual per-track movement statistics.</p>
    </sec>
  </sec>
  <sec id="sec2">
    <title>Results</title>
    <sec id="sec2.1">
      <title>Dataset</title>
      <p id="p0085">The data used for the experiments come from the so-called <italic>Visem dataset</italic> (<xref rid="bib15" ref-type="bibr">Haugen et al., 2019</xref>) collected by the <italic>Simula Research</italic> Laboratory (Visem dataset available at: <ext-link ext-link-type="uri" xlink:href="https://datasets.simula.no/visem/" id="intref0025">https://datasets.simula.no/visem/</ext-link>). This dataset consists of 85 videos of live spermatozoa from men aged 18 years or older. Each video has a resolution of <inline-formula><mml:math id="M2" altimg="si2.gif"><mml:mrow><mml:mn>640</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>480</mml:mn></mml:mrow></mml:math></inline-formula> pixels, captured with 400x magnification using an Olympus C×31 microscope and runs at 50 frames-per-second. The name of each video file is composed of the patient ID, the date of recording, a short optional description, and the code of the assessing person (e. g. <sc>1_09.09.02_SSW</sc>). Each sample is annotated with both motility and morphology characteristics. For motility, the percentages (0–100) of progressive, non-progressive, and immotile particles are given. These values form the ground truth for the experiments conducted in this paper.</p>
      <p id="p0090">Further, the dataset includes the results of a standard semen analysis and a set of sperm characteristics, i.e., the level of sex hormones measured in the blood of the participants, levels of fatty acids in spermatozoa or of phospholipids (measured in the blood). Besides, general, anonymized study participant-related data such as age, abstinence time, and body mass index (BMI) are given by the sixth <italic>csv</italic>-file. Additionally, WHO analysis data, e.g., the ground truth for sperm quality assessment could be accessed.</p>
      <p id="p0095">The Medico Multimedia for Medicine challenge’s provided subject independent 3-fold cross-validation setup (<xref rid="bib17" ref-type="bibr">Hicks et al., 2019b</xref>) was adopted for our experiments.</p>
    </sec>
    <sec id="sec2.2">
      <title>Approach and experimental settings</title>
      <p id="p0100">The different aspects of the overall approach of this paper are depicted in <xref rid="fig1" ref-type="fig">Figure 1</xref>. The videos from the dataset are preprocessed and subsequently tracking is applied to them to extract features. These features, i.e., Mean Squared Displacement (MSD) and movement statistics, are aggregated using Bag-of-Words (BoW) and, in the case of MSD, their mean values. We make use of all of the video material, collecting detected tracks and displacement features along with the whole duration of every clip, before aggregating them to video-level feature representations. Afterward, different models, i.e., a linear Support Vector Regressor (SVR), Multilayer Perceptron (MLP) regressor, CNN, and long short-term memory (LSTM) network, are trained on those features to predict motility. We systematically evaluate all combinations of feature extraction and machine learning models, as far as applicable, i.e., the BoW features that form an aggregated, sparse representation of entire video samples are not combined with recurrent neural networks (RNNs) or CNNs.<fig id="fig1"><label>Figure 1</label><caption><p>Our proposed framework for motility predictions consists of the following steps</p><p>First, preprocessing is applied to the videos after which the spermatozoa are tracked. From these tracks, features are extracted in the form of <italic>custom movement statistics (CMS)</italic> and <italic>mean squared displacement (MSD)</italic>. Finally, we aggregate those features and use them to train different networks for motility prediction.</p></caption><graphic xlink:href="gr1"/></fig></p>
      <sec id="sec2.3">
        <title>Tracking</title>
        <p id="p0105">To achieve spermatozoa tracking, two different approaches are pursued. On the one hand, sparse optical flow with the <italic>Lucas-Kanade</italic> algorithm is applied for this purpose, see “<xref rid="sec2.4" ref-type="sec">sparse optical flow with Lucas-Kanade algorithm</xref>”. On the other hand, the <xref rid="sec2.5" ref-type="sec">Crocker-Grier algorithm</xref> that is used in the so-called <italic>Trackpy</italic> tool is a second method to track sperm, as can be seen, in “<xref rid="sec2.5" ref-type="sec">Crocker-Grier algorithm</xref>”. It should be noted that both of these algorithms are quite old and not tuned to the particularities of tracking spermatozoa. However, our work on the problem focuses on harnessing unsupervised representation learning to extract useful and performant features even from noise or imperfect sperm tracks.</p>
      </sec>
      <sec id="sec2.4">
        <title>Sparse optical flow with Lucas-Kanade algorithm</title>
        <p id="p0110">The Lucas-Kanade method falls into the latter category as a differential approach for estimating sparse optical flow (<xref rid="bib27" ref-type="bibr">Lucas and Kanade, 1981</xref>). A basic assumption made for computing optical flow is that the brightness of the image is constant across all recorded frames, i.e., pixel intensities are merely translated according to their respective velocities between consecutive video images (<xref rid="bib13" ref-type="bibr">Fleet and Weiss, 2006</xref>). Although this assumption rarely holds for real-world video sequences, it nevertheless works well in practice to estimate optical flow (<xref rid="bib13" ref-type="bibr">Fleet and Weiss, 2006</xref>). The Lucas-Kanade method introduces the additional constraint that the optical flow is constant for any small subspace of the image. Together with Tomasi (<xref rid="bib40" ref-type="bibr">Tomasi and Kanade, 1991</xref>), Kanade improved this tracking algorithm by detecting good image patches from the eigenvalues of the gradient matrix based on certain thresholds. Shi and Tomasi finally also introduced a method of filtering out bad features, by comparing affine compensated tracked image patches between non-consecutive frames, the assumption being that translation should be enough to account for dissimilarities in image patches along a detected track (<xref rid="bib36" ref-type="bibr">Shi and Tomasi, 1994</xref>).</p>
        <p id="p0115">Implementations of all the components used in this tracking algorithm are available in the open-source computer vision library OpenCV (Lucas-Kanade Tracker: <ext-link ext-link-type="uri" xlink:href="https://github.com/opencv/opencv/blob/master/samples/python/lk_track.py" id="intref0030">https://github.com/opencv/opencv/blob/master/samples/python/lk_track.py</ext-link>) (<xref rid="bib8" ref-type="bibr">Bradski, 2000</xref>). To achieve better results in detecting sperm particles and their positions over time, different values for the feature detection hyperparameters <italic>maxCorners</italic>, <italic>minDistance</italic>, and <italic>blockSize</italic> are optimized to smaller values of 100, 10, and 10, respectively. An example of the sperm tracks detected by this method is visualized in “Microscopic recording of a sperm sample contained in the Visem dataset (<xref rid="fig2" ref-type="fig">Figure 2</xref>A). <xref rid="fig2" ref-type="fig">Figure 2</xref>B depicts spermatozoa tracks as detected by the Lukas- Kanade method—one of two unsupervised tracking algorithms utilized in this work.”<fig id="fig2"><label>Figure 2</label><caption><p>An example frame from the Visem dataset and the result of automated sperm cell tracking</p><p>(A) Microscopic recording of a sperm sample contained in the Visem dataset.</p><p>(B) depicts spermatozoa tracks as detected by the Lukas-Kanade method—one of two unsupervised tracking algorithms utilized in this work.</p></caption><graphic xlink:href="gr2"/></fig></p>
        <p id="p0120">However, besides identifying suitable parameter values for tracking the sperm cells, it is necessary to extract information about the sperm’s position over time to be able to compute different statistics, such as a certain sperm’s speed over a specific time interval. For this purpose, different information on every tracked sperm particle had to be computed and stored. These data include the number of the first and the last frame of a sperm’s track, the position of the sperm in every frame of the track, and the distance the sperm has moved in total. With the information stored about all sperm particles in a certain video, features describing a particular spermatozoon’s movement can be extracted. The particularities of these features will be discussed in “<xref rid="sec2.6" ref-type="sec">custom movement statistics</xref>”.</p>
      </sec>
      <sec id="sec2.5">
        <title>Crocker-Grier algorithm</title>
        <p id="p0125">The second tracking method employed for tracking sperm particles in the videos of the <italic>Visem</italic> dataset comes in the form of the Crocker-Grier algorithm for microscopic particle tracking and analysis in colloidal studies (<xref rid="bib10" ref-type="bibr">Crocker and Grier, 1996</xref>). Therefore, the target application of this approach is more closely related to the task of spermatozoa tracking from video recordings. The algorithm can track colloidal spheres—Gaussian-like blobs of a certain total brightness—across frames of microscopic video recordings of particles, and consists of a number of distinct consecutive steps. First of all, geometric distortion, non-uniform contrast, and noise are alleviated by spatial warping, background removal, and filtering, respectively (<xref rid="bib10" ref-type="bibr">Crocker and Grier, 1996</xref>; <xref rid="bib23" ref-type="bibr">Jain, 1989</xref>; <xref rid="bib35" ref-type="bibr">Pratt, 2013</xref>). After these preprocessing steps, candidate particle centroids can be detected by finding local brightness maxima (<xref rid="bib10" ref-type="bibr">Crocker and Grier, 1996</xref>), e.g., computed by grayscale dilation (<xref rid="bib23" ref-type="bibr">Jain, 1989</xref>). These maxima are further filtered by considering only those in the upper 30th percentile of brightness (<xref rid="bib10" ref-type="bibr">Crocker and Grier, 1996</xref>), and refined according to the brightness-weighted centroids in their immediate vicinity. Afterward, particle positions can be linked probabilistically considering the dynamics of Brownian particle diffusion (<xref rid="bib10" ref-type="bibr">Crocker and Grier, 1996</xref>), <inline-formula><mml:math id="M3" altimg="si3.gif"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="badbreak">/</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. At this stage, tracks can also be interrupted or terminated if, for example, particles leave the video frame. However, past locations are kept in memory so that relinking is possible, should the particle reappear.</p>
        <p id="p0130">The open-source python library Trackpy (Trackpy tool: <ext-link ext-link-type="uri" xlink:href="https://github.com/soft-matter/trackpy" id="intref0035">https://github.com/soft-matter/trackpy</ext-link>) (<xref rid="bib1" ref-type="bibr">Allan et al., 2019</xref>) provides an implementation of this algorithm and additional tools to process and extract features from particle tracks. Parameters regarding the location and linking of particles into trajectories had to be adjusted in order to improve the tracking accuracy. To reduce the hyperparameter space of our experimental pipeline, we chose to find these parameters manually by qualitative analysis of a few samples of the <italic>Visem</italic> dataset. First, an estimate of 11 pixels for the size and a minimum mass of 900 spermatozoa heads is found to lead to accurate detection of sperm cells. For linking the locations, a maximum travel distance of five pixels per frame resulted in consistent, uninterrupted tracking. Furthermore, a maximum of three frame-positions are kept in memory for cells that disappeared. Some of the detected trajectories (<inline-formula><mml:math id="M4" altimg="si4.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">&lt;</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula> frames) are too short for analysis and are therefore filtered out. As some videos contain camera drift, <italic>Trackpy</italic>’s built-in drift subtraction is applied. The drift-subtracted and filtered tracks for each video scene then served as basis for feature extraction, as will be explained in “<xref rid="sec2.5.1" ref-type="sec">feature extraction</xref>”.</p>
      </sec>
    </sec>
    <sec id="sec2.5.1">
      <title>Feature extraction</title>
      <p id="p0135">For the task of predicting motility statistics for input semen samples, features are extracted based on the spermatozoa tracks obtained with the methods described in “<xref rid="sec2.3" ref-type="sec">Tracking</xref>”. Three feature descriptors are considered in the experiments. Custom movement statistics are computed from the tracks generated with the basic <italic>Lucas-Kanade tracker</italic>, mean squared displacement vectors are extracted directly with <italic>Trackpy</italic>, and finally, a range of more involved and computationally heavier particle motility statistics is created.</p>
      <sec id="sec2.6">
        <title>Custom movement statistics</title>
        <p id="p0140">The first feature representation chosen for performing motility prediction on the dataset is constructed by computing a set of statistics from the tracks detected with the adapted <italic>Lucas-K. tracker</italic>. Based on the nature of the task at hand for which it is important to differentiate between progressive and non-progressive movement of sperm cells, both the total amount of movement by spermatozoa in particular time frames as well as the actual distances covered by them are of interest. The first aspect can be calculated for a specific window by accumulating the number of pixels a particular cell moved between each consecutive video frame while the second metric looks at the Euclidean distance between the positions of the cell at the start and end of the time window. These calculations can then be carried out for sliding windows of different sizes, and statistical functionals can be applied to their results. This leads to feature vectors of fixed length for each sperm track found by the Lucas-Kanade tracking algorithm. Specifically, these window sizes are used (measured in number of frames): 5, 10, 20, 50, 80, 100, 150, 200, 250, 300, 400, 500, 750, and 1000. After computing both metrics as described above for the whole sample by sliding each of the windows over a particular track with a hop size of one frame, mean, maximum, and minimum functionals are applied to the resulting series of motility calculations. Two additional features are computed as the total distance covered by a single sperm cell during the whole video sample and its average speed in pixels moved per frame. In total, the approach leads to numerical feature vectors of size 14 × 2 × 3 + 2 = 86 for each detected sperm track. Before being applicable to the task of motility analysis on a per-sample basis, these vectors can be further processed and aggregated per video clip. Here, two possibilities are explored. First, feature vectors of a single video sample are reduced by their feature-wise mean. Secondly, a BoW approach is applied to the vectors that both quantizes and summarizes them in an unsupervised manner.</p>
      </sec>
      <sec id="sec2.6.1">
        <title>Displacement features</title>
        <p id="p0145">A common statistical measure that is employed to characterize the random movement of particles can be found with the MSD (<xref rid="bib14" ref-type="bibr">Frenkel and Smit, 2001</xref>). It can be used to describe the explorative behavior of particles in a system, i.e., if movement is restricted to diffusion or affected by some sort of force. The displacement of a single particle <italic>j</italic> is defined as the distance it traveled in a particular time frame of duration <italic>l</italic> (lag-time) <italic>t</italic><sub><italic>i</italic></sub> to <italic>t</italic><sub><italic>i</italic> + <italic>l</italic></sub> measured as the square of the Euclidean distance between its positions at the start (<italic>x</italic><sub><italic>j</italic></sub>(<italic>t</italic><sub><italic>i</italic></sub>)) and end (<italic>x</italic><sub><italic>j</italic></sub>(<italic>t</italic><sub><italic>i + l</italic></sub>)) of the frame. For a set of <italic>N</italic> particles, the <italic>ensemble mean</italic> displacement for a specific time interval can then be computed as:<disp-formula id="fd1"><label>(Equation 1)</label><mml:math id="M5" altimg="si5.gif"><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mtext>.</mml:mtext></mml:mrow></mml:math></disp-formula></p>
        <p id="p0150">When observing a longer period of time (<italic>T</italic><sub>0</sub> to <italic>T</italic><sub>1</sub>), an average of MSD can further be computed from sliding windows of particular lag times over the whole segment. This can be done for each individual particle (mean squared displacement of each particle (imsd)) or again as an average for all of the particles (ensemble mean squared displacement of all particles (emsd)). Finally, computing these displacement values for a range of different lag times can capture more detailed information about particle movement. For the application of automated sperm motility analysis, mean squared displacement of spermatozoa in a given sample for different sized time windows could give insight into the amount of progressive and non-progressive motility. Given enough time, a progressive sperm cell would travel across a larger distance, whereas a sperm that is merely moving in place would display the same amount of displacement for both short and long time frames. <italic>Trackpy</italic> provides interfaces to compute both <italic>imsd</italic> and <italic>emsd</italic> for a range of increasing time frames. Specifically, it considers lag-times up to a user definable maximum that are increased in framewise step sizes, i.e., in the case of the <italic>Visem</italic> dataset that is recorded at 50 fps, the consecutive window sizes grow by <inline-formula><mml:math id="M6" altimg="si6.gif"><mml:mrow><mml:mn>20</mml:mn><mml:mspace width="0.25em"/><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula>, each. When considering a maximum lag-time of 10 s for example, 500 mean squared displacement values are computed from the sperm tracks. As <italic>emsd</italic> is computed as an aggregated measurement for all sperm cells of a given sample in a particular time frame, it can be directly used as input for the machine learning algorithms described in “<xref rid="sec2.8" ref-type="sec">regression models</xref>”. Also, <italic>imsd</italic> feature vectors, which are extracted on a per-track basis, can be further quantized and aggregated using the Bag-of-Words framework described in “<xref rid="sec2.7" ref-type="sec">Bag-of-Words</xref>” to form a clip level representation. In this article, three different combinations of window and hop sizes are considered for the extraction of <italic>emsd</italic> feature vectors: a window size of 2 s with a 1 s hop and 10 s windows with either 1 s or 5 s hops. Based on the motility prediction performance achieved using the different <italic>emsd</italic> feature configurations, hop and window sizes for <italic>imsd</italic> prediction are chosen.</p>
      </sec>
    </sec>
    <sec id="sec2.7">
      <title>Bag-of-Words</title>
      <p id="p0155">The use of unsupervised tracking algorithms allows the extraction of useful features on a more granular, per-spermatozoon basis. As the sperm cell count varies heavily between the different samples in the <italic>Visem</italic> dataset and annotations are further only available on a per-sample level, a type of feature aggregation mechanism has to be implemented to leverage per-cell information. In “<xref rid="sec2.19" ref-type="sec">Displacement Features</xref>”, regarding the mean displacement of all spermatozoa during a given time frame of a specific recording has been introduced as a first, baseline method for this problem. However, simply averaging the displacement of all cells might lead to the loss of more granular information. For this reason, a histogram representation based on the famous BoW model extended to be used with arbitrary numerical input features will be employed. For the experiments, the input feature vectors belonging to individual sperm cell tracks are first standardized to zero mean and unit variance before a random subset is chosen to form a codebook. Afterward, a fixed number of the top nearest vectors from the codebook is computed for each input feature vector. Aggregated over all sperm tracks belonging to a given sample recording, the counts of these assigned vectors form a histogram representation which is further processed by term frequencyinverse document frequency (tf-idf). Furthermore, the number of codebook vectors <italic>N</italic> and assigned vectors <italic>a</italic> is optimized by evaluating all combinations of <inline-formula><mml:math id="M7" altimg="si7.gif"><mml:mrow><mml:mi>N</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>2</mml:mn><mml:mspace width="0.25em"/><mml:mn>500,5</mml:mn><mml:mspace width="0.25em"/><mml:mn>000,10</mml:mn><mml:mspace width="0.25em"/><mml:mn>000</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M8" altimg="si8.gif"><mml:mrow><mml:mi>a</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>1,10,50,100,200,500</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> on the given data using different machine learning models.</p>
    </sec>
    <sec id="sec2.8">
      <title>Regression models</title>
      <p id="p0160">The features that are described in “<xref rid="sec2.5.1" ref-type="sec">feature extraction</xref>” are used as input for various machine learning approaches. The extraction methods described above lead to variable numbers of feature vectors for each original video sample, e. g., displacement vectors are extracted for overlapping windows. To enable comparisons between all implemented approaches and the methods applied by the participants of the Medico challenge, the predictions of each model are mean aggregated on a per-sample basis, i. e., each model produces a single prediction for each of the 85 patients contained in the <italic>Visem</italic> dataset. The models are outlined in the following. As metrics, we utilize both the MAE as well as the root-mean-square error (RMSE) which is more sensitive to outliers. The metrics are computed as follows:<disp-formula id="fd2"><label>(Equation 2)</label><mml:math id="M9" altimg="si9.gif"><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>E</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="fd3"><label>(Equation 3)</label><mml:math id="M10" altimg="si10.gif"><mml:mrow><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula></p>
      <p id="p0165">We compute these metrics for each of the three challenge folds and present the mean of their values.</p>
      <sec id="sec2.9">
        <title>Linear support vector regressor</title>
        <p id="p0170">The first method to predict the motility of spermatozoa is a linear SVR. Here, different scaling options, i. e., <italic>StandardScaler</italic>, <italic>MinMaxScaler</italic>, and no scaler, are tested. Five distinct complexity values <italic>c</italic> equally distributed between <inline-formula><mml:math id="M11" altimg="si11.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M12" altimg="si12.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> are evaluated. The best value for this is found by the MAE obtained in an internal 5-fold cross-validation on each fold’s training data.</p>
      </sec>
      <sec id="sec2.10">
        <title>Multilayer perceptron</title>
        <p id="p0175">The architecture of the MLP model contains multiple fully connected layers with batch normalization applied before the activation function. The model is trained with the <italic>Adam</italic> optimizer in order to minimize the mean squared error (MSE) and an additional L2 weight regularization term. Exponential linear unit (ELU) and rectified linear unit (ReLU) are evaluated as choices for the activation functions of the layers. A random search is performed over different parameters, including learning rate, number of layers and units per layer, batch size, and dropout that are listed in <xref rid="tbl1" ref-type="table">Table 1</xref>. The best parameters are determined by the MAE achieved on the random 20% validation splits of each fold’s training data.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>All hyperparameters and their values that are optimized for the different machine learning models</p></caption><table frame="hsides" rules="groups"><thead><tr><th>hyper-parameter</th><th>MLP</th><th>RNN</th><th>CNN</th></tr></thead><tbody><tr><td>batch size</td><td>16, 32, 64</td><td>16, 32, 64</td><td>16, 32, 64</td></tr><tr><td>Dropout</td><td>0.2, 0.4</td><td>0.2, 0.4</td><td>0.2, 0.4</td></tr><tr><td>kernel regularizer</td><td>10<sup>−4</sup>, 10<sup>−3</sup>, 10<sup>−2</sup></td><td>10<sup>−4</sup>, 10<sup>−3</sup>, 10<sup>−2</sup></td><td>10<sup>−4</sup>, 10<sup>−3</sup>, 10<sup>−2</sup></td></tr><tr><td>activation dense</td><td>ELU, ReLU</td><td>ELU, ReLU</td><td>ELU, ReLU</td></tr><tr><td>number of layers</td><td>2, 4, 8</td><td>2, 4, 8</td><td>2, 4, 8</td></tr><tr><td>learning rate</td><td>10<sup>−4</sup>, 10<sup>−3</sup>, 10<sup>−2</sup></td><td>10<sup>−4</sup>, 10<sup>−3</sup>, 10<sup>−2</sup></td><td>10<sup>−4</sup>, 10<sup>−3</sup>, 10<sup>−2</sup></td></tr><tr><td>no. of units/filters</td><td>256, 512, 1024</td><td>32, 64</td><td>64, 128, 256</td></tr><tr><td>cell type</td><td>–</td><td>–</td><td>GRU, LSTM</td></tr><tr><td>recurrent dropout</td><td>–</td><td>–</td><td>0, 0.2, 0.4</td></tr><tr><td>bidirectional</td><td>–</td><td>–</td><td>true, false</td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="sec2.11">
        <title>Convolutional neural network</title>
        <p id="p0180">Another method used for the prediction of motility of spermatozoa is a 1-dimensional CNN. Its model architecture is constructed by multiple convolutional blocks which are stacked on top of each other. Each convolutional block consists of the following parts. First, a 1-dimensional convolutional layer with a kernel size of three and stride of one extracts features from the input. Batch normalization is then applied before the non-linear activation function. Afterward, the output is max-pooled and neurons are randomly dropped out to prevent overfitting. Furthermore, the number of filters in the convolutional layer is doubled for each consecutive block. After the last block, a fully connected layer with linear activation predicts the three target values for the regression problems. <xref rid="fig3" ref-type="fig">Figure 3</xref> depicts an example of such a CNN with 32 filters in the first layer and three convolutional blocks. The model is trained with the <italic>Adam</italic> optimizer to minimize the MSE and an additional L2 weight regularization term. In order to optimize both model architecture and training settings, a random parameter search is performed. Here, the learning rate of the <italic>Adam</italic> optimizer, different functions for the activation, the number of layers, filters and batch size, dropout, and kernel regularizer are adjusted, as can be seen in “All hyperparameters and their values that are optimized for the different machine learning models.”. 50 different combinations of those parameters are tested and the best one is chosen according to validation MAE. The network is trained for an indefinite number of epochs, stopping early if the validation MAE has not increased for 100 epochs.<fig id="fig3"><label>Figure 3</label><caption><p>This figure shows the architecture of the model used for the CNN</p><p>Three similar blocks of layers with an increasing number of output filters are stacked consecutively. Finally, the output of the last block is fed into a fully connected layer with output neurons for each of the predicted motility characteristics <italic>percentage of immotile sperm</italic>, <italic>percentage of progressive motility</italic>, and <italic>percentage of non-progressive motility</italic>.</p></caption><graphic xlink:href="gr3"/></fig></p>
      </sec>
      <sec id="sec2.12">
        <title>Recurrent neural network</title>
        <p id="p0185">The model architecture of the considered recurrent neural network (RNN) consists of multiple recurrent layers. Each of those layers contains either a gated recurrent unit (GRU) or LSTM cell. Bidirectional variants where the input is processed both in the forward and backward direction are also tested. Dropout is applied both within (between time steps) and after each recurrent layer. Both GRUs and LSTMs use hyperbolic tangent activation (<italic>hyperbolic tangent (tanh)</italic>) for their recurrent layers and <italic>sigmoid</italic> as their gate activation functions. In order to minimize the MSE and an additional L2 weight regularization term, the model is trained with the <italic>RMSProp</italic> optimizer. Parameters such as learning rate, number of layers and recurrent units, and batch size are optimized, as listed in <xref rid="tbl1" ref-type="table">Table 1</xref>.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec2.12.1">
    <title>Evaluation</title>
    <p id="p0190">Motility of spermatozoa is predicted by a linear SVR (“Linear Support Vector Regressor”), an MLP (“Multilayer Perceptron”), a CNN (“Convolutional Neural Network”), and an LSTM (“Long Short-term Memory network”), where all models are trained on <italic>emsd</italic> features extracted with <italic>Trackpy</italic>. Moreover, motility prediction is achieved by BoW with a linear SVR (“Bag-of-Words with Support Vector Regressor”) and with an MLP regressor (“Bag-of-Words with Multilayer Perceptron”), both trained on <italic>imsd</italic> features extracted with the help of <italic>Trackpy</italic>, and on features created by computations on a set of statistics from tracks detected with the customized <italic>Lucas-Kanade tracker</italic>. We did not train any CNN or RNN models on the BoWs, as they are sparse quantizations of entire video samples, thus containing neither structural nor temporal information that could be exploited by those types of neural networks. Since BoWs are sparse quantizations of entire video samples, they contain neither structural nor temporal information. For this reason, we refrained to train CNN or RNN models on BoW features.</p>
    <p id="p0195">MAE and RMSE results for both validation and evaluation are outlined in <xref rid="tbl2" ref-type="table">Tables 2</xref> and <xref rid="tbl3" ref-type="table">3</xref>. However, for purposes of readability, in the text, we mainly remark on MAE results achieved on evaluation using the same parameters found for the best results on validation. As described in “sec:dataset”, we used the 3-fold cross-validation setup of the Medico Multimedia for Medicine challenge. Therefore, the reported MAEs and RMSEs are mean values computed over 3-folds.<table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>Mean absolute error (MAE) and root-mean-square error (RMSE) results of proposed experiments using four machine learning models on <italic>emsd</italic> features</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">metric</th><th rowspan="2">hop</th><th rowspan="2">window</th><th colspan="2">SVR<hr/></th><th colspan="2">MLP<hr/></th><th colspan="2">CNN<hr/></th><th colspan="2">RNN<hr/></th></tr><tr><th>val</th><th>eval</th><th>val</th><th>eval</th><th>val</th><th>eval</th><th>val</th><th>eval</th></tr></thead><tbody><tr><td rowspan="3">MAE</td><td>1s</td><td>2s</td><td>11.13</td><td>10.91</td><td>6.49</td><td>11.56</td><td>6.29</td><td>10.48</td><td>6.55</td><td>12.97</td></tr><tr><td>1s</td><td>10s</td><td>10.16</td><td>8.36</td><td>5.19</td><td>8.83</td><td><bold>5.03</bold></td><td><bold>8.44</bold></td><td>6.59</td><td>8.49</td></tr><tr><td>5s</td><td>10s</td><td>10.12</td><td>8.60</td><td>5.26</td><td>8.13</td><td>7.21</td><td>8.74</td><td>6.45</td><td>8.13</td></tr><tr><td rowspan="3">RMSE</td><td>1s</td><td>2s</td><td>14.02</td><td>14.30</td><td>8.04</td><td>15.13</td><td>8.21</td><td>14.17</td><td>10.27</td><td>16.17</td></tr><tr><td>1s</td><td>10s</td><td>12.87</td><td>11.06</td><td>6.84</td><td>11.48</td><td><bold>6.55</bold></td><td><bold>10.82</bold></td><td>9.18</td><td>11.41</td></tr><tr><td>5s</td><td>10s</td><td>12.85</td><td>11.56</td><td>6.95</td><td>10.56</td><td>7.21</td><td>11.31</td><td>9.49</td><td>10.79</td></tr></tbody></table><table-wrap-foot><fn><p>Three different <italic>hop</italic> and <italic>window size</italic> combinations are evaluated for the extraction of the <italic>emsd</italic> feature vectors. <italic>hop</italic> size: refers to the difference between two adjacent window centers. For example, for a hop size of 1 s and a window size of 10 s two adjacent widows are for 90% overlapped. <italic>val</italic>: results on the (val)idation set of the data. eval: results on the unseen (eval)uation set of the data. Best results for each metric are highlighted in gray shading.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="tbl3"><label>Table 3</label><caption><p>Mean absolute error (MAE) results of proposed experiments using a BoW with SVR and MLP on <italic>custom movement statistics</italic> (<italic>CMS</italic>) and <italic>mean squared displacement</italic> (<italic>MSD</italic>) features</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">codebook size</th><th rowspan="2">assigned vectors</th><th colspan="2">SVR + CMS<hr/></th><th colspan="2">SVR + msd<hr/></th><th colspan="2">MLP + CMS<hr/></th><th colspan="2">MLP + msd<hr/></th></tr><tr><th>val</th><th>eval</th><th>val</th><th>eval</th><th>val</th><th>eval</th><th>val</th><th>eval</th></tr></thead><tbody><tr><td rowspan="6">2 500</td><td>1</td><td>8.34</td><td>8.00</td><td>7.71</td><td>7.55</td><td>7.63</td><td>8.11</td><td>6.70</td><td>8.01</td></tr><tr><td>10</td><td>8.31</td><td>7.91</td><td>7.85</td><td>7.73</td><td>7.37</td><td>8.37</td><td>6.19</td><td>8.74</td></tr><tr><td>50</td><td>8.33</td><td>8.03</td><td>8.10</td><td>8.01</td><td>7.52</td><td>8.50</td><td>6.29</td><td>7.95</td></tr><tr><td>100</td><td>8.28</td><td>8.00</td><td>8.18</td><td>8.06</td><td>7.18</td><td>8.64</td><td>6.47</td><td>8.35</td></tr><tr><td>200</td><td>8.23</td><td>8.05</td><td>8.26</td><td>8.09</td><td>7.15</td><td>8.39</td><td>6.41</td><td>8.16</td></tr><tr><td>500</td><td>8.26</td><td>8.31</td><td>8.27</td><td>8.03</td><td>6.75</td><td>8.11</td><td><bold>5.91</bold></td><td><bold>7.85</bold></td></tr><tr><td rowspan="6">5 000</td><td>1</td><td>8.42</td><td>8.26</td><td>7.87</td><td>7.81</td><td>7.99</td><td>8.29</td><td>6.70</td><td>8.04</td></tr><tr><td>10</td><td>8.22</td><td>7.93</td><td>7.69</td><td>7.43</td><td>7.12</td><td>8.73</td><td>6.65</td><td>8.54</td></tr><tr><td>50</td><td>8.38</td><td>8.07</td><td>8.05</td><td>7.99</td><td>7.50</td><td>8.42</td><td>6.30</td><td>8.18</td></tr><tr><td>100</td><td>8.34</td><td>8.03</td><td>8.11</td><td>8.03</td><td>7.39</td><td>8.40</td><td>6.14</td><td>8.54</td></tr><tr><td>200</td><td>8.29</td><td>8.00</td><td>8.19</td><td>8.07</td><td>7.31</td><td>8.40</td><td>6.35</td><td>8.68</td></tr><tr><td>500</td><td>8.22</td><td>8.08</td><td>8.28</td><td>8.10</td><td>7.05</td><td>7.54</td><td>6.23</td><td>8.40</td></tr><tr><td rowspan="6">10 000</td><td>1</td><td>8.56</td><td>8.73</td><td>8.08</td><td>8.18</td><td>7.85</td><td>7.92</td><td>7.03</td><td>8.11</td></tr><tr><td>10</td><td>8.19</td><td>7.86</td><td><bold>7.56</bold></td><td><bold>7.31</bold></td><td>7.41</td><td>8.17</td><td>6.47</td><td>8.27</td></tr><tr><td>50</td><td>8.40</td><td>7.98</td><td>7.92</td><td>7.86</td><td>7.41</td><td>8.17</td><td>6.27</td><td>8.07</td></tr><tr><td>100</td><td>8.38</td><td>8.07</td><td>8.05</td><td>7.99</td><td>7.42</td><td>7.95</td><td>6.12</td><td>8.04</td></tr><tr><td>200</td><td>8.34</td><td>8.03</td><td>8.11</td><td>8.03</td><td>7.26</td><td>8.26</td><td>6.28</td><td>8.19</td></tr><tr><td>500</td><td>8.27</td><td>8.03</td><td>8.23</td><td>8.09</td><td>7.13</td><td>7.67</td><td>6.34</td><td>7.91</td></tr></tbody></table><table-wrap-foot><fn><p>18 different <italic>codebook sizes</italic> and number of <italic>assigned vectors</italic> combinations, all with a window size of 5 s are evaluated. Best results for each codebook size are highlighted in gray shading.</p></fn></table-wrap-foot></table-wrap></p>
    <sec id="sec2.13">
      <title>Linear support vector regressor</title>
      <p id="p0200">The first set of results comes from training a linear SVR on the <italic>emsd</italic> feature vectors extracted with <italic>Trackpy</italic>. As described in “<xref rid="sec2.19" ref-type="sec">displacement features</xref>”, three different combinations of window and hop size are evaluated for feature extraction. Training and optimization of the regressor is further done as outlined in “Linear Support Vector Regressor”. <xref rid="tbl2" ref-type="table">Table 2</xref> shows both MAEs and RMSEs achieved during validation and evaluation using 3-fold cross-validation. It is apparent from both the validation and evaluation results that choosing only a small window size of 2 s for computing the displacement statistics leads to feature representations that lack useful information for predicting motility characteristics of the sperm cells for each sample. For the configurations, using a larger window size of 10 s and a larger hop size of 5 s leads to slightly better results during validation but decreased evaluation performance. Considering the best validation result, a minimum MAE of 8.60 is obtained on evaluation with the SVR trained on <italic>emsd</italic> features. Measured against the state of the art, this result shows a relative improvement of 2.6% (<xref rid="bib39" ref-type="bibr">Thambawita et al., 2019b</xref>).</p>
    </sec>
    <sec id="sec2.14">
      <title>Multilayer perceptron</title>
      <p id="p0205">Secondly, an MLP is trained on the <italic>emsd</italic> feature vectors that have been extracted with <italic>Trackpy</italic>. Again, following the procedure described in “<xref rid="sec2.19" ref-type="sec">Displacement features</xref>”, features are extracted by three combinations of window and hop size. In “Multilayer Perceptron”, it is shown how the network is trained and optimized. Best results are achieved with a learning rate of <inline-formula><mml:math id="M13" altimg="si13.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, a batch size of 16, and a dropout of 0.2. For a window size of 2 s and 1 s hop, best results are obtained with the ReLU activation function, eight layers, a factor of <inline-formula><mml:math id="M14" altimg="si14.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for the L2 weight regularization, and <inline-formula><mml:math id="M15" altimg="si15.gif"><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="0.25em"/><mml:mn>024</mml:mn></mml:mrow></mml:math></inline-formula> units on each layer. The model trained on features extracted with a window size of 10 s and 1 and 5 s hop is performing best for choosing ELU as activation function, four layers, <inline-formula><mml:math id="M16" altimg="si16.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> as factor for the L2 weight regularization, and 512 units on each layer. In <xref rid="tbl2" ref-type="table">Table 2</xref>, an overview of MAE and RMSE results coming from validation and evaluation with 3-fold cross-validation is given. Same as with the SVR (see “<xref rid="sec2.13" ref-type="sec">linear support vector regressor</xref>”), choosing a window size of 2 s for the computation of displacement statistics performs the worst. For choosing a larger window size of 10 s, validation performance is somewhat better for applying a hop size of 1 s than 5 s. However, the larger hop size of 5 s is performing slightly better on evaluation. The minimum MAE value of 8.83 is achieved by the MLP trained on displacement features, which is as good as the findings of state of the art (SOTA) (<xref rid="bib39" ref-type="bibr">Thambawita et al., 2019b</xref>).</p>
    </sec>
    <sec id="sec2.15">
      <title>Convolutional neural network</title>
      <p id="p0210">Features extracted with the help of the <italic>Trackpy</italic> tool are used for a third set of experiments, this time applying a CNN. Here, features are extracted by three combinations of window and hop size, as can be read in “Displacement Features”. How the network is trained and optimized is explained in “Convolutional Neural Network”. Best results are achieved with a learning rate of <inline-formula><mml:math id="M17" altimg="si13.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, ELU as activation function, and a factor of <inline-formula><mml:math id="M18" altimg="si13.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for the L2 weight regularization. Training the network on features extracted with a window size of 2 s and 1 s hop performed best with four layers, starting with 32 filters for the first layer, a batch size of 64, and a dropout of <inline-formula><mml:math id="M19" altimg="si13.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Best results with a window size of 10 s and hop size of 1 and 5 s are for a number of eight layers, starting with 32 filters for the first layer, a batch size of 32, and a dropout of 0.4. As can be observed in <xref rid="tbl2" ref-type="table">Table 2</xref>, using the <italic>emsd</italic> features obtained with 1 s overlapping 2 s segments of one video is not quite keeping pace with the best results of previous papers on this kind of experiment. <italic>emsd</italic> features with overlapping 10 s parts of the videos with a hop size of 1 and 5 s are more promising. Experiments with 1 s hop are achieving best results for MAE and RMSE values on validation and evaluation. Going by the best validation result, the minimum MAE is at 8.44 for training a CNN on <italic>emsd</italic> features. These results show a relative improvement of 4.4% against state-of-the-art results by <xref rid="bib39" ref-type="bibr">Thambawita et al. (2019b)</xref>. Moreover, the CNN performs slightly better than the previous models—SVR achieving 8.60 MAE, see “<xref rid="sec2.13" ref-type="sec">linear support vector regressor</xref>”, and MLP resulting in 8.83 MAE, see “<xref rid="sec2.14" ref-type="sec">Multilayer perceptron</xref>”. As the improvements are only marginal at best, it is questionable if structural dependencies which could be exploited by the CNN can be found in the <italic>emsd</italic> feature vectors.</p>
    </sec>
    <sec id="sec2.16">
      <title>Recurrent neural network</title>
      <p id="p0215">A fourth set of experiments is done with training an RNN on the features extracted with the <italic>Trackpy</italic> tool. As described in “<xref rid="sec2.19" ref-type="sec">displacement features</xref>”, three different combinations of window and hop size are assessed for feature extraction. Now, contrary to the other experiments, the feature sequences are formed from the vectors extracted from consecutive overlapping windows and the RNN uses all of the information in the sequence to make a prediction. The network is trained and optimized according to “Recurrent Neural Network”. Best results are obtained with bidirectional LSTM cells with a number of 256 recurrent units on each layer. Further, applying dropout to the activations in the recurrent layers decreases performance in all cases. For training this network on features of 2 and 10 s windows and 1 s hop, the best hyperparameters are a learning rate of <inline-formula><mml:math id="M20" altimg="si16.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, a number of two layers, a batch size of 16, a dropout of 0.4, and a factor of <inline-formula><mml:math id="M21" altimg="si13.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for the L2 weight regularization. With a window size of 10 s and 5 s hop, best performance is achieved with a learning rate of <inline-formula><mml:math id="M22" altimg="si14.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, a number of four layers, a batch size of 64, a dropout of 0.2, and a factor of <inline-formula><mml:math id="M23" altimg="si14.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for the L2 weight regularization. Validation and evaluation with 3-fold cross-validation scored the MAE and RMSE values displayed in <xref rid="tbl2" ref-type="table">Table 2</xref>. Choosing a window size of 10 s and a hop size of 5 s achieves evaluation results that are slightly better than state-of-the-art results. The minimum MAE is at 8.13 MAE for evaluation, a relative improvement of 7.8% against state-of-the-art results by <xref rid="bib39" ref-type="bibr">Thambawita et al. (2019b)</xref>, as well as 3.6% against previous experiments, for example those using a CNN on the same <italic>emsd</italic> features, cf. “Convolutional Neural Network”. Considering the temporal dependencies within sequences of <italic>emsd</italic> vectors therefore seems to improve on regression performance if ever so slightly. Furthermore, the RNN experiments enforce the notion that <italic>emsd</italic> features computed over longer time intervals contain more information regarding the motility of sperm cells, as even when taking a sequence of shorter frames into account as a whole, results are better with the greater window size.</p>
    </sec>
    <sec id="sec2.17">
      <title>Bag-of-Words with support vector regressor</title>
      <p id="p0220">A possible drawback of the initial experiments with <italic>emsd</italic> features might be that they aggregate information about the movement across all spermatozoa in a given sample in a very primitive fashion with mean values. Therefore, the experiments with unsupervised feature quantization and aggregation via BoW of single-spermatozoon-based features investigate a more sophisticated approach of analyzing a variable number of sperm cells.</p>
      <p id="p0225">Here, the prediction of the motility of spermatozoa is accomplished by generating BoWs from the features described in “<xref rid="sec2.5.1" ref-type="sec">feature extraction</xref>” that serve as input for training an SVR.</p>
      <sec id="sec2.18">
        <title>Custom movement statistics features</title>
        <p id="p0230">In the first set of experiments for predicting motility using a BoW, the BoW is generated from movement statistics coming from the adapted <italic>Lucas-Kanade tracker</italic>, as discussed in “Custom Movement Statistics”. As shown in “<xref rid="sec2.7" ref-type="sec">Bag-of-Words</xref>”, for training and optimization of the model, codebook sizes of 2500, 5000, and 10000, assigning 1, 10, 50, 100, 200, and 500 vectors, and complexity values between <inline-formula><mml:math id="M24" altimg="si11.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M25" altimg="si12.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> are considered. Best results are detected with a complexity of 10. Validation and evaluation achieved MAE results are shown in <xref rid="tbl3" ref-type="table">Table 3</xref>. Choosing any of the investigated combinations of codebook size and the number of assigned vectors leads to evaluation results that are slightly better than state-of-the-art results by <xref rid="bib39" ref-type="bibr">Thambawita et al. (2019b)</xref>. The best result on the validation, achieved with a codebook size of <inline-formula><mml:math id="M26" altimg="si17.gif"><mml:mrow><mml:mn>10</mml:mn><mml:mspace width="0.25em"/><mml:mn>000</mml:mn></mml:mrow></mml:math></inline-formula> and 10 assigned vectors is 7.86 MAE on evaluation, a relative improvement of 10.9% against state-of-the-art results, and 6.8% against the best result of previous experiments with a CNN cf. “Convolutional Neural Network”. These results suggest the superiority of the BoW approach to simple mean aggregation. Tuning the BoW hyperparameters shows that choosing a smaller number of codebook vectors to assign to each input sample leads to improved results. Furthermore, a marginal performance gain can be achieved with larger codebooks.</p>
      </sec>
      <sec id="sec2.19">
        <title>Displacement features</title>
        <p id="p0235">The same model as in the preceding part, the BoW with a linear SVR, is now trained on <italic>imsd</italic> features extracted with <italic>Trackpy</italic>, described in “<xref rid="sec2.19" ref-type="sec">displacement features</xref>”. As outlined in “Bag-of-Words”, codebook sizes of 2500, 5000, and 10000, assigning 1, 10, 50, 100, 200, and 500 vectors and complexity values between <inline-formula><mml:math id="M27" altimg="si11.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M28" altimg="si12.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> are tested to extract features in order to train and optimize the model. A complexity of <inline-formula><mml:math id="M29" altimg="si12.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> showed the best results. In <xref rid="tbl3" ref-type="table">Table 3</xref>, the validation and evaluation for MAE values are reported. Evaluation results for any tested combination of codebook size and number of assigned vectors are better than or at least equally good as all previous experiments in this article and state-of-the-art results. A codebook size of 10000 and 10 assigned vectors achieves the overall minimum evaluation MAE of 7.31. This is a relative improvement of over 17.2% compared to the best submission (<xref rid="bib39" ref-type="bibr">Thambawita et al., 2019b</xref>) and is outperforming the results provided in “<xref rid="sec2.21" ref-type="sec">Custom movement statistics features</xref>”.</p>
        <p id="p0240">The <italic>imsd</italic> features extracted with <italic>Trackpy</italic> therefore serve as a more powerful basis for feature creation than the custom statistics generated from movement tracks. The observations about codebook sizes and number of assigned vectors also hold for this set of experiments, with larger codebooks and fewer vector assignments leading to the best results.</p>
      </sec>
    </sec>
    <sec id="sec2.20">
      <title>Bag-of-Words with multilayer perceptron</title>
      <p id="p0245">Motility prediction of spermatozoa is additionally achieved by training a BoW with an MLP on features created with both <italic>Trackpy</italic> and calculations coming from the adapted <italic>Lucas-Kanade tracker</italic>.</p>
      <sec id="sec2.21">
        <title>Custom movement statistics features</title>
        <p id="p0250">Experiments for this model are started by training the BoW with an MLP regressor on custom movement statistics features created with the help of the tailored <italic>Lucas-Kanade tracker</italic> that has been explained in “Custom Movement Statistics”. Codebook sizes of 2500, 5000, and 10000 and 1, 10, 50, 100, 200, and 500 assigned vectors are tested for feature extraction, so that the model can be optimized, see “<xref rid="sec2.7" ref-type="sec">Bag-of-Words</xref>”. For further optimization, various values for different hyperparameters are assessed as shown in “<xref rid="sec2.14" ref-type="sec">multilayer perceptron</xref>”. Best results are accomplished with a learning rate of <inline-formula><mml:math id="M30" altimg="si13.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, a batch size of 64, a dropout of 0.4, two layers, and <inline-formula><mml:math id="M31" altimg="si15.gif"><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="0.25em"/><mml:mn>024</mml:mn></mml:mrow></mml:math></inline-formula> units per layer. ReLU is the best performing activation function and a factor of <inline-formula><mml:math id="M32" altimg="si13.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> proved best for the L2 weight regularization. MAE results for validation and evaluation are listed in <xref rid="tbl3" ref-type="table">Table 3</xref>. The best MAE results for evaluation of 8.11 MAE are achieved for a codebook size of 2500 and 500 assigned vectors.</p>
      </sec>
      <sec id="sec2.22">
        <title>Displacement features</title>
        <p id="p0255">The same model of a BoW with an MLP regressor as in the preceding part in this section is additionally trained on displacement features extracted with <italic>Trackpy</italic>, as shown in “<xref rid="sec2.19" ref-type="sec">displacement features</xref>”. As described in “<xref rid="sec2.7" ref-type="sec">Bag-of-Words</xref>”, codebook sizes of 2500, 5000, and 10000 and 1, 10, 50, 100, 200, and 500 assigned vectors are evaluated for feature extraction. This model is further trained and optimized according to “<xref rid="sec2.14" ref-type="sec">Multilayer perceptron</xref>”, obtaining best results with a learning rate of <inline-formula><mml:math id="M33" altimg="si13.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, a batch size of 16, a dropout of 0.2, four layers, and 256 units in each of those layers. Here, ELU activation function and a factor of <inline-formula><mml:math id="M34" altimg="si13.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for the L2 weight regularization performed best. <xref rid="tbl3" ref-type="table">Table 3</xref> illustrates MAE results for the validation and evaluation. A codebook size of 2500 and assigning 500 vectors achieves 7.85 MAE on evaluation.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec3">
    <title>Discussion</title>
    <p id="p0260">The large number of experiments conducted and evaluated in this article (summarised in <xref rid="tbl4" ref-type="table">Table 4</xref>) additionally requires a high-level overview which discusses individual strengths and weaknesses. For predicting motility, almost every experiment in this article improved upon the state of the art (<xref rid="bib39" ref-type="bibr">Thambawita et al., 2019b</xref>) (The MAE provided by <xref rid="bib39" ref-type="bibr">Thambawita et al. (2019b)</xref> was at the time of writing this manuscript the best available MAE result. However, it should be noted that we did not find a peer-reviewed publication of this paper. The best (peer-reviewed) published MAE value for the task of sperm motility prediction based on the <italic>Visem</italic> dataset is provided by <xref rid="bib19" ref-type="bibr">Hicks et al. (2019d)</xref>.) which is already better than the ZeroR baseline. The best results of every investigated combination of feature representation and machine learning algorithm are displayed in <xref rid="fig4" ref-type="fig">Figure 4</xref>. Using <italic>emsd</italic> feature vectors extracted from overlapping windows of the input videos already leads to better results with almost every machine learning model. Comparing the different algorithms for this feature type shows that more involved neural network architectures, i. e., CNNs and RNNs, are able to extract additional information from the features, by either considering structural dependencies within a single vector (CNN) or exploiting long(er) term temporal dependencies between consecutive <italic>emsd</italic> measurements (RNN)—with the latter leading to the best results with these kinds of features. However, an MLP trained on <italic>emsd</italic> vectors is inferior to the more robust SVR. The remaining three types of features all aggregate sperm-level information into subject-level sparse feature representations via BoW. Therefore, they are no longer suitable candidates for the CNN and RNN models. Overall, the BoW methodology still leads to stronger results considering the simpler machine learning strategies applied in those experiments. Furthermore, the strongest feature representations can be extracted by constructing BoWs from <italic>imsd</italic> vectors with the overall best MAE of 7.31. This result is further significantly better than state-of-the-art results at <inline-formula><mml:math id="M35" altimg="si18.gif"><mml:mrow><mml:mi>p</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">&lt;</mml:mo><mml:mo>.</mml:mo><mml:mn>01</mml:mn></mml:mrow></mml:math></inline-formula> measured by a one-tailed T-Test than by <xref rid="bib18" ref-type="bibr">Hicks et al. (2019c)</xref> at <inline-formula><mml:math id="M36" altimg="si19.gif"><mml:mrow><mml:mi>p</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">&lt;</mml:mo><mml:mo>.</mml:mo><mml:mn>05</mml:mn></mml:mrow></mml:math></inline-formula>. For these experiments, the SVR outperformed the MLP and therefore, the latter is not applied in the very last experiment. This observation can be explained by the small size of the training dataset in the BoW experiments where features are aggregated per patient. As deep learning models generally require larger amounts of data to perform well, this circumstance might have prevented the MLP from achieving better results.<table-wrap position="float" id="tbl4"><label>Table 4</label><caption><p>Mean absolute error (MAE) and root-mean-square error (RMSE) results of proposed experiments using eight different machine learning models with <italic>SVR</italic>, <italic>MLP</italic>, <italic>CNN</italic>, and <italic>RNN</italic> on <italic>emsd</italic>, <italic>imsd</italic>, and <italic>custom movement statistics</italic> (<italic>CMS</italic>) features</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" rowspan="2"/><th colspan="4">emsd<hr/></th><th colspan="2">CMS<hr/></th><th colspan="2">imsd<hr/></th><th rowspan="2">SOTA</th></tr><tr><th>SVR</th><th>MLP</th><th>CNN</th><th>RNN</th><th>SVR</th><th>MLP</th><th>SVR</th><th>MLP</th></tr></thead><tbody><tr><td rowspan="2">MAE</td><td>val</td><td>10.12</td><td>5.19</td><td>5.03</td><td>6.45</td><td>8.19</td><td>6.75</td><td>7.56</td><td>5.91</td><td>–</td></tr><tr><td>eval</td><td>8.60</td><td>8.83</td><td>8.44</td><td>8.13</td><td>7.86</td><td>8.11</td><td><bold>7.31</bold></td><td>7.85</td><td>8.83</td></tr><tr><td rowspan="2">RMSE</td><td>val</td><td>12.85</td><td>6.84</td><td>6.55</td><td>9.18</td><td>10.16</td><td>9.04</td><td>9.39</td><td>8.74</td><td>–</td></tr><tr><td>eval</td><td>11.56</td><td>11.48</td><td>10.82</td><td>11.41</td><td>10.38</td><td>10.81</td><td>9.56</td><td>10.49</td><td>12.05</td></tr></tbody></table><table-wrap-foot><fn><p>The best overall result is highlighted in bold.</p></fn></table-wrap-foot></table-wrap><fig id="fig4"><label>Figure 4</label><caption><p>Best results for motility prediction and state of the art (<xref rid="bib39" ref-type="bibr">Thambawita et al., 2019b</xref>)</p><p>The lower the mean absolute error (MAE) the better, showing that SVR models achieve better results than the other models for every type of features. BoWs outperform models trained on emsd features.</p></caption><graphic xlink:href="gr4"/></fig></p>
    <p id="p0265">For our best model, an SVR trained on quantized, BoW representations learn from per-sperm displacement measurements, we further perform an analysis on the individual dimensions of motility—progressive and non-progressive motility as well as percentage of immotile sperm cells. <xref rid="fig5" ref-type="fig">Figure 5</xref> plots the model’s predictions for each motility metric against its true value. It is clear that progressive motility of sperm cells is most easily detected by our framework, achieving a coefficient of determination of <inline-formula><mml:math id="M37" altimg="si20.gif"><mml:mrow><mml:mn>74</mml:mn><mml:mtext>%</mml:mtext></mml:mrow></mml:math></inline-formula>. For the other two dimensions, our model did not perform as well. Detection of non-progressive motility is closer to a mean prediction baseline than the actual distribution, achieving an R2 score of merely <inline-formula><mml:math id="M38" altimg="si21.gif"><mml:mrow><mml:mn>26</mml:mn><mml:mtext>%</mml:mtext></mml:mrow></mml:math></inline-formula>. However, this is the dimension exhibiting the lowest amount of variance in the dataset, making it harder for the models to learn important discriminating features about the underlying data distribution. Lastly, for most patients, the amount of immotile sperm cells in their samples was below <inline-formula><mml:math id="M39" altimg="si22.gif"><mml:mrow><mml:mn>40</mml:mn><mml:mtext>%</mml:mtext></mml:mrow></mml:math></inline-formula>, leading to a condensed data distribution. Nonetheless, immotile sperm cells were still quite reliably detected by our framework, yielding an R2 score of <inline-formula><mml:math id="M40" altimg="si23.gif"><mml:mrow><mml:mn>66</mml:mn><mml:mtext>%</mml:mtext></mml:mrow></mml:math></inline-formula>.<fig id="fig5"><label>Figure 5</label><caption><p>Visualization of our best model’s predictions for the three dimensions of sperm-motility—percentage of progressively motile (A), non-progressively motile (B), and immotile spermatozoa (C)</p><p>The green line represents a linear regression fitted to relate the model’s predictions to the manually annotated ground-truth motility labels. The shaded margin around the regression line visualizes a 95% confidence interval obtained by bootstrapping. A red identity line is further added to the plot for easier assessment of the overall performance. Finally, coefficients of determination (R2 scores) are given for each result. For a detailed account about the error analysis please refer to “Discussion”.</p></caption><graphic xlink:href="gr5"/></fig></p>
  </sec>
  <sec id="sec3.1">
    <title>Limitations of the study</title>
    <p id="p0270">When looking at the computational effort required to run our motility prediction pipeline, real-time analysis (at least 30 fps) is not yet possible. The main bottleneck can be found with the extraction of the sperm tracks which was performed for the entirety of each video. Here, especially the Crocker-Grier algorithm is quite slow, requiring a runtime longer than each video’s duration for tracking the spermatozoa. For example, extracting tracks and displacement features for a 20 s video clip with a sliding window of 10 s with 5 s hop, takes around 22 s on a desktop Intel i9 processor (8 cores, 16 threads). The rest of the pipeline can afterward be run in under 2 s. An investigation into more efficient tracking algorithms is required for improving the pipeline’s performance.</p>
  </sec>
  <sec id="sec3.2">
    <title>Conclusion and future work</title>
    <p id="p0275">In this article, the task of automatic sperm quality assessment from microscopic video recordings is addressed by applying a framework of unsupervised tracking, feature quantization, and machine learning. The publicly available <italic>Visem</italic> dataset served as the basis for predicting the motility of spermatozoa. Two different tracking algorithms are utilized in order to enable extraction of features on a per-sperm cell basis. The features are then quantized and aggregated with a BoW approach and used as input for machine learning models. All methods herein achieved improvements for motility prediction over the submissions to the Medico Multimedia for Medicine challenge. The overall best results are achieved by unsupervised tracking of sperm cells with the Crocker-Grier (<xref rid="bib10" ref-type="bibr">Crocker and Grier, 1996</xref>) algorithm, extracting <italic>imsd</italic> features for each detected track and aggregating those features into a histogram representation using BoW. With this feature representation, a linear SVR improved the mean (3-fold) MAE from 8.83 to 7.31, a decrease of over <inline-formula><mml:math id="M41" altimg="si24.gif"><mml:mrow><mml:mn>17</mml:mn><mml:mspace width="0.25em"/><mml:mtext>%</mml:mtext></mml:mrow></mml:math></inline-formula>. The results further show that the unsupervised feature quantization helps to achieve more consistent and robust results, regardless of which feature representation is chosen as input. For future work, the presented framework can be extended and improved upon by pursuing a number of additional research directions. First of all, other methods of feature extraction from sperm tracks can be explored. During the experiments in this article, a more involved and computationally heavy set of features in the form of sperm motility parameters, such as curve linear velocities and coefficients obtained from regression analysis, are evaluated. Combined with the BoW feature quantization, however, these are less successful than the simpler <italic>imsd</italic> vectors. More interesting could be to integrate unsupervised representation learning into the process. A direct approach, for example, could train an autoencoder directly on the video content. Considering the noisy nature of the sperm sample recordings which contain lots of debris and background contrast variation, and furthermore exhibit very sparse motion characteristics, this seems hardly feasible with state-of-the-art deep learning methods. Instead, convolutional and recurrent autoencoders could be applied to suitable transformations of the detected tracks, as has already been done for single tracks of myogenic cells (<xref rid="bib24" ref-type="bibr">Kimmel et al., 2019</xref>). Here, all tracks could be considered together or individually in an unsupervised training procedure. Using <sc>motilitAI</sc>, our low-resource AI-based method for automatic sperm motility recognition, we hope for its integration in digital microscopes and making our solution reachable for everyone at low cost.</p>
  </sec>
  <sec id="sec5">
    <title>STAR★Methods</title>
    <sec id="sec5.1">
      <title>Key resources table</title>
      <p id="p0285">
        <table-wrap position="float" id="undtbl1">
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th>REAGENT or RESOURCE</th>
                <th>SOURCE</th>
                <th>IDENTIFIER</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td colspan="3">
                  <bold>Deposited data</bold>
                </td>
              </tr>
              <tr>
                <td colspan="3">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td>VISEM</td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/2640506" id="intref0040">https://zenodo.org/record/2640506</ext-link>
                </td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.2640506" id="intref0045">https://doi.org/10.5281/zenodo.2640506</ext-link>
                </td>
              </tr>
              <tr>
                <td colspan="3">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td colspan="3">
                  <bold>Software and algorithms</bold>
                </td>
              </tr>
              <tr>
                <td colspan="3">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td>motiliAI</td>
                <td>
                  <ext-link ext-link-type="uri" xlink:href="https://github.com/EIHW/motilitAI" id="intref0050">https://github.com/EIHW/motilitAI</ext-link>
                </td>
                <td/>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
    </sec>
    <sec id="sec5.2">
      <title>Resource availability</title>
      <sec id="sec5.2.1">
        <title>Lead contact</title>
        <p id="p0290">Further information and requests for resources should be directed to and will be fulfilled by the lead contact, Shahin Amiriparian.</p>
      </sec>
      <sec id="sec5.2.2">
        <title>Materials availability</title>
        <p id="p0295">This study did not generate new unique reagents.</p>
      </sec>
    </sec>
    <sec id="sec5.3">
      <title>Method details</title>
      <p id="p0300">The whole machine learning pipeline reported in this article can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/EIHW/motilitAI" id="intref0055">https://github.com/EIHW/motilitAI</ext-link>. A snapshot of the code is further included as supplemental material <xref rid="mmc1" ref-type="supplementary-material">Data S1</xref>.</p>
    </sec>
    <sec id="sec5.4">
      <title>Quantification and statistical analysis</title>
      <p id="p0305">
        <list list-type="simple" id="ulist0020">
          <list-item id="u0040">
            <label>•</label>
            <p id="p0310">Samples of 85 participants have been used for machine learning methods</p>
          </list-item>
          <list-item id="u0045">
            <label>•</label>
            <p id="p0315">Statistical significance over baseline results was determined via three-fold cross-validation and Student’s <italic>t</italic> test.</p>
          </list-item>
        </list>
      </p>
    </sec>
  </sec>
</body>
<back>
  <ref-list id="cebib0010">
    <title>References</title>
    <ref id="bib1">
      <element-citation publication-type="other" id="sref1">
        <person-group person-group-type="author">
          <name>
            <surname>Allan</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>van der Wel</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Keim</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Caswell</surname>
            <given-names>T.A.</given-names>
          </name>
          <name>
            <surname>Wieker</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Verweij</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Reid</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Thierry Grueter</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Ramos</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Apiszcz</surname>
            <given-names>Z.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Soft-Matter/Trackpy: Trackpy v0.4.2</article-title>
        <pub-id pub-id-type="doi">10.5281/zenodo.3492186</pub-id>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="bib2">
      <element-citation publication-type="journal" id="sref2">
        <person-group person-group-type="author">
          <name>
            <surname>Alquézar-Baeta</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Gimeno-Martos</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Miguel-Jiménez</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Santolaria</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Yániz</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Palacín</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Casao</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Cebrián-Pérez</surname>
            <given-names>J.Á.</given-names>
          </name>
          <name>
            <surname>Muiño-Blanco</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Pérez-Pé</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Opencasa: a new open-source and scalable tool for sperm quality analysis</article-title>
        <source>PLoS Comput. Biol.</source>
        <year>2019</year>
        <fpage>1</fpage>
        <lpage>18</lpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006691</pub-id>
      </element-citation>
    </ref>
    <ref id="bib3">
      <element-citation publication-type="journal" id="sref3">
        <person-group person-group-type="author">
          <name>
            <surname>Amiriparian</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Deep Representation Learning Techniques for Audio Signal Processing</article-title>
        <comment>Ph.D. Thesis</comment>
        <source>Technische Universität München</source>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="bib4">
      <element-citation publication-type="book" id="sref4">
        <person-group person-group-type="author">
          <name>
            <surname>Amiriparian</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Cummins</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Ottl</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Gerczuk</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Schuller</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <part-title>Sentiment analysis using image-based deep spectrum features</part-title>
        <source>Proceedings of the 7th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)</source>
        <year>2017</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>26</fpage>
        <lpage>29</lpage>
      </element-citation>
    </ref>
    <ref id="bib5">
      <element-citation publication-type="book" id="sref5">
        <person-group person-group-type="author">
          <name>
            <surname>Amiriparian</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Gerczuk</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ottl</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Cummins</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Pugachevskiy</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Schuller</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <part-title>Bag-of-deep-features: noise-robust deep feature representations for audio analysis</part-title>
        <source>Proceedings of the International Joint Conference on Neural Networks (IJCNN)</source>
        <year>2018</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>1</fpage>
        <lpage>7</lpage>
      </element-citation>
    </ref>
    <ref id="bib6">
      <element-citation publication-type="book" id="sref6">
        <person-group person-group-type="author">
          <name>
            <surname>Bar-Shalom</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Daum</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>The probabilistic data association filter</part-title>
        <source>Control Systems Magazine</source>
        <year>2009</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>82</fpage>
        <lpage>100</lpage>
      </element-citation>
    </ref>
    <ref id="bib7">
      <element-citation publication-type="book" id="sref7">
        <person-group person-group-type="author">
          <name>
            <surname>Björndahl</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Mortimer</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Barratt</surname>
            <given-names>C.L.</given-names>
          </name>
          <name>
            <surname>Castilla</surname>
            <given-names>J.A.</given-names>
          </name>
          <name>
            <surname>Menkveld</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Kvist</surname>
            <given-names>U.</given-names>
          </name>
          <name>
            <surname>Alvarez</surname>
            <given-names>J.G.</given-names>
          </name>
          <name>
            <surname>Haugen</surname>
            <given-names>T.B.</given-names>
          </name>
        </person-group>
        <part-title>A Practical Guide to Basic Laboratory Andrology</part-title>
        <year>2010</year>
        <publisher-name>Cambridge University Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib8">
      <element-citation publication-type="book" id="sref8">
        <person-group person-group-type="author">
          <name>
            <surname>Bradski</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <part-title>The OpenCV Library</part-title>
        <year>2000</year>
        <publisher-name>Dr. Dobb’s Journal of Software Tools</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib9">
      <element-citation publication-type="journal" id="sref9">
        <person-group person-group-type="author">
          <name>
            <surname>Cooper</surname>
            <given-names>T.G.</given-names>
          </name>
          <name>
            <surname>Noonan</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Von Eckardstein</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Auger</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Baker</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Behre</surname>
            <given-names>H.M.</given-names>
          </name>
          <name>
            <surname>Haugen</surname>
            <given-names>T.B.</given-names>
          </name>
          <name>
            <surname>Kruger</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Mbizvo</surname>
            <given-names>M.T.</given-names>
          </name>
          <name>
            <surname>Vogelsong</surname>
            <given-names>K.M.</given-names>
          </name>
        </person-group>
        <article-title>World health organization reference values for human semen characteristics</article-title>
        <source>Hum. Reprod. Update</source>
        <year>2010</year>
        <fpage>231</fpage>
        <lpage>245</lpage>
        <pub-id pub-id-type="pmid">19934213</pub-id>
      </element-citation>
    </ref>
    <ref id="bib10">
      <element-citation publication-type="journal" id="sref10">
        <person-group person-group-type="author">
          <name>
            <surname>Crocker</surname>
            <given-names>J.C.</given-names>
          </name>
          <name>
            <surname>Grier</surname>
            <given-names>D.G.</given-names>
          </name>
        </person-group>
        <article-title>Methods of digital video microscopy for colloidal studies</article-title>
        <source>J. Colloid Interface Sci.</source>
        <year>1996</year>
        <fpage>298</fpage>
        <lpage>310</lpage>
      </element-citation>
    </ref>
    <ref id="bib11">
      <element-citation publication-type="journal" id="sref11">
        <person-group person-group-type="author">
          <name>
            <surname>David</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Serres</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Jouannet</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Kinematics of human spermatozoa</article-title>
        <source>Gamete Res.</source>
        <year>1981</year>
        <fpage>83</fpage>
        <lpage>95</lpage>
      </element-citation>
    </ref>
    <ref id="bib12">
      <element-citation publication-type="book" id="sref12">
        <person-group person-group-type="author">
          <name>
            <surname>Deng</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Dong</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Socher</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>L.J.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Fei-Fei</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <part-title>Imagenet: a large-scale hierarchical image database</part-title>
        <source>Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</source>
        <year>2009</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>248</fpage>
        <lpage>255</lpage>
      </element-citation>
    </ref>
    <ref id="bib13">
      <element-citation publication-type="book" id="sref13">
        <person-group person-group-type="author">
          <name>
            <surname>Fleet</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Weiss</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <part-title>Optical flow estimation</part-title>
        <source>Handbook of Mathematical Models in Computer Vision</source>
        <year>2006</year>
        <publisher-name>Springer</publisher-name>
        <fpage>237</fpage>
        <lpage>257</lpage>
      </element-citation>
    </ref>
    <ref id="bib14">
      <element-citation publication-type="book" id="sref14">
        <person-group person-group-type="author">
          <name>
            <surname>Frenkel</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Smit</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <series>Understanding Molecular Simulation: From Algorithms to Applications</series>
        <volume>volume 1</volume>
        <year>2001</year>
        <publisher-name>Elsevier</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib15">
      <element-citation publication-type="book" id="sref15">
        <person-group person-group-type="author">
          <name>
            <surname>Haugen</surname>
            <given-names>T.B.</given-names>
          </name>
          <name>
            <surname>Hicks</surname>
            <given-names>S.A.</given-names>
          </name>
          <name>
            <surname>Andersen</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Witczak</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Hammer</surname>
            <given-names>H.L.</given-names>
          </name>
          <name>
            <surname>Borgli</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Halvorsen</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Riegler</surname>
            <given-names>M.A.</given-names>
          </name>
        </person-group>
        <part-title>Visem: a multimodal video dataset of human spermatozoa</part-title>
        <source>Proceedings of the 10th ACM on Multimedia Systems Conference, ACM</source>
        <year>2019</year>
        <pub-id pub-id-type="doi">10.1145/3304109.3325814</pub-id>
      </element-citation>
    </ref>
    <ref id="bib16">
      <element-citation publication-type="book" id="sref16">
        <person-group person-group-type="author">
          <name>
            <surname>Hicks</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Halvorsen</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Haugen</surname>
            <given-names>T.B.</given-names>
          </name>
          <name>
            <surname>Andersen</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Witczak</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Pogorelov</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Hammer</surname>
            <given-names>H.L.</given-names>
          </name>
          <name>
            <surname>D-T</surname>
            <given-names>D.N.</given-names>
          </name>
          <name>
            <surname>Lux</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Riegler</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <part-title>Predicting sperm motility and morphology using deep learning and handcrafted features</part-title>
        <source>Proceedings of the CEUR Workshop on Multimedia Benchmark Workshop</source>
        <year>2019</year>
        <publisher-name>MediaEval</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib17">
      <element-citation publication-type="book" id="sref17">
        <person-group person-group-type="author">
          <name>
            <surname>Hicks</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Halvorsen</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Haugen</surname>
            <given-names>T.B.</given-names>
          </name>
          <name>
            <surname>Andersen</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Witczak</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Pogorelov</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Hammer</surname>
            <given-names>H.L.</given-names>
          </name>
          <name>
            <surname>Dang-Nguyen</surname>
            <given-names>D.T.</given-names>
          </name>
          <name>
            <surname>Lux</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Riegler</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <part-title>Medico multimedia task at mediaeval 2019</part-title>
        <source>Proceedings of the CEUR Workshop on Multimedia Benchmark Workshop</source>
        <year>2019</year>
        <publisher-name>MediaEval</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib18">
      <element-citation publication-type="book" id="sref18">
        <person-group person-group-type="author">
          <name>
            <surname>Hicks</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Haugen</surname>
            <given-names>T.B.</given-names>
          </name>
          <name>
            <surname>Halvorsen</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Riegler</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <part-title>Using deep learning to predict motility and morphology of human sperm</part-title>
        <source>Proceedings of the CEUR Workshop on Multimedia Benchmark Workshop</source>
        <year>2019</year>
        <publisher-name>MediaEval</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib19">
      <element-citation publication-type="journal" id="sref19">
        <person-group person-group-type="author">
          <name>
            <surname>Hicks</surname>
            <given-names>S.A.</given-names>
          </name>
          <name>
            <surname>Andersen</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Witczak</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Thambawita</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Halvorsen</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Hammer</surname>
            <given-names>H.L.</given-names>
          </name>
          <name>
            <surname>Haugen</surname>
            <given-names>T.B.</given-names>
          </name>
          <name>
            <surname>Riegler</surname>
            <given-names>M.A.</given-names>
          </name>
        </person-group>
        <article-title>Machine learning-based analysis of sperm videos and participant data for male fertility prediction</article-title>
        <source>Sci. Rep.</source>
        <year>2019</year>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="pmid">30626917</pub-id>
      </element-citation>
    </ref>
    <ref id="bib20">
      <element-citation publication-type="journal" id="sref20">
        <person-group person-group-type="author">
          <name>
            <surname>Hidayatullah</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Mengko</surname>
            <given-names>T.L.E.R.</given-names>
          </name>
          <name>
            <surname>Munir</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Barlian</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Bull sperm tracking and machine learning-based motility classification</article-title>
        <source>IEEE Access</source>
        <volume>9</volume>
        <year>2021</year>
        <fpage>61159</fpage>
        <lpage>61170</lpage>
        <pub-id pub-id-type="doi">10.1109/access.2021.3074127</pub-id>
      </element-citation>
    </ref>
    <ref id="bib21">
      <element-citation publication-type="journal" id="sref21">
        <person-group person-group-type="author">
          <name>
            <surname>Van der Horst</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Kitchin</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Van der Horst</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Atherton</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>The effect of the breeding season, cryopreservation and physiological extender on selected sperm and semen parameters of four ferret species: implications for captive breeding in the endangered black-footed ferret</article-title>
        <source>Reprod. Fertil. Dev.</source>
        <year>2009</year>
        <fpage>351</fpage>
        <lpage>363</lpage>
        <pub-id pub-id-type="pmid">19210927</pub-id>
      </element-citation>
    </ref>
    <ref id="bib22">
      <element-citation publication-type="book" id="sref22">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Van Der Maaten</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Weinberger</surname>
            <given-names>K.Q.</given-names>
          </name>
        </person-group>
        <part-title>Densely connected convolutional networks</part-title>
        <source>Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</source>
        <year>2017</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>4700</fpage>
        <lpage>4708</lpage>
      </element-citation>
    </ref>
    <ref id="bib23">
      <element-citation publication-type="book" id="sref23">
        <person-group person-group-type="author">
          <name>
            <surname>Jain</surname>
            <given-names>A.K.</given-names>
          </name>
        </person-group>
        <part-title>Fundamentals of Digital Image Processing</part-title>
        <year>1989</year>
        <publisher-name>Prentice Hall</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib24">
      <element-citation publication-type="journal" id="sref24">
        <person-group person-group-type="author">
          <name>
            <surname>Kimmel</surname>
            <given-names>J.C.</given-names>
          </name>
          <name>
            <surname>Brack</surname>
            <given-names>A.S.</given-names>
          </name>
          <name>
            <surname>Marshall</surname>
            <given-names>W.F.</given-names>
          </name>
        </person-group>
        <article-title>Deep convolutional and recurrent neural networks for cell motility discrimination and prediction</article-title>
        <source>IEEE/ACM Trans. Comput. Biol. Bioinform.</source>
        <volume>18</volume>
        <year>2019</year>
        <fpage>562</fpage>
        <lpage>574</lpage>
      </element-citation>
    </ref>
    <ref id="bib25">
      <element-citation publication-type="journal" id="sref25">
        <person-group person-group-type="author">
          <name>
            <surname>Kimmel</surname>
            <given-names>J.C.</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>A.Y.</given-names>
          </name>
          <name>
            <surname>Brack</surname>
            <given-names>A.S.</given-names>
          </name>
          <name>
            <surname>Marshall</surname>
            <given-names>W.F.</given-names>
          </name>
        </person-group>
        <article-title>Inferring cell state by quantitative motility analysis reveals a dynamic state system and broken detailed balance</article-title>
        <source>PLoS Comput. Biol.</source>
        <year>2018</year>
        <fpage>1</fpage>
        <lpage>29</lpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1005927</pub-id>
      </element-citation>
    </ref>
    <ref id="bib26">
      <element-citation publication-type="journal" id="sref26">
        <person-group person-group-type="author">
          <name>
            <surname>Kumar</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>A.K.</given-names>
          </name>
        </person-group>
        <article-title>Trends of male factor infertility, an important cause of infertility: a review of literature</article-title>
        <source>J. Hum. Reprod. Sci.</source>
        <year>2015</year>
        <fpage>191</fpage>
        <pub-id pub-id-type="pmid">26752853</pub-id>
      </element-citation>
    </ref>
    <ref id="bib27">
      <element-citation publication-type="book" id="sref27">
        <person-group person-group-type="author">
          <name>
            <surname>Lucas</surname>
            <given-names>B.D.</given-names>
          </name>
          <name>
            <surname>Kanade</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <part-title>An Iterative Image Registration Technique with an Application to Stereo Vision</part-title>
        <year>1981</year>
      </element-citation>
    </ref>
    <ref id="bib28">
      <element-citation publication-type="journal" id="sref28">
        <person-group person-group-type="author">
          <name>
            <surname>Lueders</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Luther</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Scheepers</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Van der Horst</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Improved semen collection method for wild felids: urethral catheterization yields high sperm quality in african lions (panthera leo)</article-title>
        <source>Theriogenology</source>
        <year>2012</year>
        <fpage>696</fpage>
        <lpage>701</lpage>
        <pub-id pub-id-type="pmid">22538007</pub-id>
      </element-citation>
    </ref>
    <ref id="bib29">
      <element-citation publication-type="book" id="sref29">
        <person-group person-group-type="author">
          <name>
            <surname>Mortimer</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <part-title>Objective analysis of sperm motility and kinematics</part-title>
        <source>Handbook of the Laboratory Diagnosis and Treatment of Infertility</source>
        <year>1990</year>
        <publisher-name>CRC Press, Inc.</publisher-name>
        <fpage>97</fpage>
        <lpage>133</lpage>
      </element-citation>
    </ref>
    <ref id="bib30">
      <element-citation publication-type="book" id="sref30">
        <person-group person-group-type="author">
          <name>
            <surname>Mortimer</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <part-title>Practical Laboratory Andrology</part-title>
        <year>1994</year>
        <publisher-name>Oxford University Press on Demand</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib31">
      <element-citation publication-type="book" id="sref31">
        <person-group person-group-type="author">
          <name>
            <surname>Mortimer</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Aitken</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Mortimer</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Pacey</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>Workshop Report: Clinical Casa – the Quest for Consensus</part-title>
        <year>1995</year>
        <publisher-name>CSIRO</publisher-name>
        <fpage>951</fpage>
        <lpage>959</lpage>
      </element-citation>
    </ref>
    <ref id="bib32">
      <element-citation publication-type="book" id="sref32">
        <person-group person-group-type="author">
          <name>
            <surname>Mortimer</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Mortimer</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <part-title>Value and reliability of casa systems</part-title>
        <source>Studies in Profertility Series</source>
        <year>1998</year>
        <publisher-name>The Parthenon Publishing Group Limited</publisher-name>
        <fpage>73</fpage>
        <lpage>90</lpage>
      </element-citation>
    </ref>
    <ref id="bib33">
      <element-citation publication-type="journal" id="sref33">
        <person-group person-group-type="author">
          <name>
            <surname>Mortimer</surname>
            <given-names>S.T.</given-names>
          </name>
          <name>
            <surname>van der Horst</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Mortimer</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>The future of computer-aided sperm analysis</article-title>
        <source>Asian J. Androl.</source>
        <year>2015</year>
        <fpage>545</fpage>
        <pub-id pub-id-type="pmid">25926614</pub-id>
      </element-citation>
    </ref>
    <ref id="bib34">
      <element-citation publication-type="journal" id="sref34">
        <person-group person-group-type="author">
          <collab>Practice Committee of the American Society for Reproductive Medicine</collab>
        </person-group>
        <article-title>Definitions of infertility and recurrent pregnancy loss</article-title>
        <source>Fertil. Steril.</source>
        <year>2008</year>
        <fpage>1603</fpage>
        <pub-id pub-id-type="pmid">18485348</pub-id>
      </element-citation>
    </ref>
    <ref id="bib35">
      <element-citation publication-type="book" id="sref35">
        <person-group person-group-type="author">
          <name>
            <surname>Pratt</surname>
            <given-names>W.K.</given-names>
          </name>
        </person-group>
        <part-title>Introduction to Digital Image Processing</part-title>
        <year>2013</year>
        <publisher-name>CRC press</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib36">
      <element-citation publication-type="book" id="sref36">
        <person-group person-group-type="author">
          <name>
            <surname>Shi</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Tomasi</surname>
          </name>
        </person-group>
        <part-title>Good features to track</part-title>
        <source>Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</source>
        <year>1994</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>593</fpage>
        <lpage>600</lpage>
      </element-citation>
    </ref>
    <ref id="bib37">
      <element-citation publication-type="journal" id="sref37">
        <person-group person-group-type="author">
          <name>
            <surname>Stephen</surname>
            <given-names>E.H.</given-names>
          </name>
          <name>
            <surname>Chandra</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Updated projections of infertility in the United States: 1995–2025</article-title>
        <source>Fertil. Steril.</source>
        <year>1998</year>
        <fpage>30</fpage>
        <lpage>34</lpage>
        <pub-id pub-id-type="pmid">9660416</pub-id>
      </element-citation>
    </ref>
    <ref id="bib38">
      <element-citation publication-type="book" id="sref38">
        <person-group person-group-type="author">
          <name>
            <surname>Thambawita</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Halvorsen</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Hammer</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Riegler</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Haugen</surname>
            <given-names>T.B.</given-names>
          </name>
        </person-group>
        <part-title>Extracting temporal features into a spatial domain using autoencoders for sperm video analysis</part-title>
        <source>CEUR Workshop Proceedings</source>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="bib39">
      <element-citation publication-type="book" id="sref39">
        <person-group person-group-type="author">
          <name>
            <surname>Thambawita</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Halvorsen</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Hammer</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Riegler</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Haugen</surname>
            <given-names>T.B.</given-names>
          </name>
        </person-group>
        <part-title>Stacked dense optical flows and dropout layers to predict sperm motility and morphology</part-title>
        <source>CEUR Workshop Proceedings</source>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="bib40">
      <element-citation publication-type="book" id="sref40">
        <person-group person-group-type="author">
          <name>
            <surname>Tomasi</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Kanade</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <part-title>Detection and Tracking of Point Features, School of Computer Science</part-title>
        <year>1991</year>
        <publisher-name>Carnegie Mellon University</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib41">
      <element-citation publication-type="book" id="sref41">
        <person-group person-group-type="author">
          <name>
            <surname>Urbano</surname>
            <given-names>L.F.</given-names>
          </name>
          <name>
            <surname>Masson</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>VerMilyea</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kam</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <part-title>Automatic tracking and motility analysis of human sperm in time-lapse images</part-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <year>2016</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>792</fpage>
        <lpage>801</lpage>
      </element-citation>
    </ref>
    <ref id="bib42">
      <element-citation publication-type="journal" id="sref42">
        <person-group person-group-type="author">
          <name>
            <surname>Valiuškaitė</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Raudonis</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Maskeliūnas</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Damaševičius</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Krilavičius</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning based evaluation of spermatozoid motility for artificial insemination</article-title>
        <source>Sensors</source>
        <volume>21</volume>
        <year>2020</year>
        <fpage>72</fpage>
        <pub-id pub-id-type="doi">10.3390/s21010072</pub-id>
      </element-citation>
    </ref>
    <ref id="bib43">
      <element-citation publication-type="book" id="sref43">
        <person-group person-group-type="author">
          <collab>World Health Organization</collab>
        </person-group>
        <part-title>WHO Laboratory Manual for the Examination of Human Semen and Sperm-Cervical Mucus Interaction</part-title>
        <year>1999</year>
        <publisher-name>Cambridge University Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib44">
      <element-citation publication-type="journal" id="sref44">
        <person-group person-group-type="author">
          <collab>World Health Organization</collab>
        </person-group>
        <article-title>ehealth</article-title>
        <source>Resolution</source>
        <year>2005</year>
        <fpage>16</fpage>
        <lpage>25</lpage>
      </element-citation>
    </ref>
    <ref id="bib45">
      <element-citation publication-type="journal" id="sref45">
        <person-group person-group-type="author">
          <collab>World Health Organization</collab>
        </person-group>
        <article-title>Sustainable health financing, universal coverage and social health insurance</article-title>
        <source>Resolution</source>
        <volume>58</volume>
        <year>2005</year>
        <fpage>33</fpage>
      </element-citation>
    </ref>
    <ref id="bib46">
      <element-citation publication-type="book" id="sref46">
        <person-group person-group-type="author">
          <collab>World Health Organization</collab>
        </person-group>
        <series>Atlas of EHealth Country Profiles: The Use of EHealth in Support of Universal Health Coverage: Based on the Findings of the Third Global Survery on EHealth 2015</series>
        <volume>volume 3</volume>
        <year>2016</year>
        <publisher-name>World Health Organization</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib47">
      <element-citation publication-type="book" id="sref47">
        <person-group person-group-type="author">
          <collab>World Health Organization</collab>
        </person-group>
        <part-title>Global Diffusion of eHealth: Making Universal Health Coverage Achievable: Report of the Third Global Survey on eHealth</part-title>
        <year>2017</year>
        <publisher-name>World Health Organization</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib48">
      <element-citation publication-type="journal" id="sref48">
        <person-group person-group-type="author">
          <name>
            <surname>Yee</surname>
            <given-names>W.K.</given-names>
          </name>
          <name>
            <surname>Sutton</surname>
            <given-names>K.L.</given-names>
          </name>
          <name>
            <surname>Dowling</surname>
            <given-names>D.K.</given-names>
          </name>
        </person-group>
        <article-title>In vivo male fertility is affected by naturally occurring mitochondrial haplotypes</article-title>
        <source>Curr. Biol.</source>
        <volume>23</volume>
        <year>2013</year>
        <fpage>R55</fpage>
        <lpage>R56</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cub.2012.12.002</pub-id>
        <pub-id pub-id-type="pmid">23347935</pub-id>
      </element-citation>
    </ref>
    <ref id="bib49">
      <element-citation publication-type="journal" id="sref49">
        <person-group person-group-type="author">
          <name>
            <surname>Zegers-Hochschild</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Adamson</surname>
            <given-names>G.D.</given-names>
          </name>
          <name>
            <surname>de Mouzon</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ishihara</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Mansour</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Nygren</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Sullivan</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>van der Poel</surname>
            <given-names>S.</given-names>
          </name>
          <collab>ICMART</collab>
          <collab>WHO</collab>
        </person-group>
        <article-title>International committee for monitoring assisted reproductive technology (ICMART) and the world health organization (WHO) revised glossary of art terminology</article-title>
        <source>Hum. Reprod.</source>
        <year>2009</year>
        <fpage>2683</fpage>
        <lpage>2687</lpage>
        <pub-id pub-id-type="pmid">19801627</pub-id>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="appsec2" sec-type="supplementary-material">
    <title>Supplemental information</title>
    <p id="p0340">
      <supplementary-material content-type="local-data" id="mmc1">
        <caption>
          <title>Document S1. Data S1</title>
        </caption>
        <media xlink:href="mmc1.zip"/>
      </supplementary-material>
    </p>
  </sec>
  <sec sec-type="data-availability" id="da0010">
    <title>Data and code availability</title>
    <p id="p0025">
      <list list-type="simple" id="ulist0015">
        <list-item id="u0025">
          <label>•</label>
          <p id="p0030">Data: This paper analyzes existing, publicly available data. These accession numbers for the datasets are listed in the <xref rid="sec5.1" ref-type="sec">key resources table</xref>.</p>
        </list-item>
        <list-item id="u0030">
          <label>•</label>
          <p id="p0035">Code: All original code is available in this paper’s <xref rid="mmc1" ref-type="supplementary-material">supplemental information</xref>.</p>
        </list-item>
        <list-item id="u0035">
          <label>•</label>
          <p id="p0040">Any additional information required to reanalyze the data reported in this paper is available from the <xref rid="sec5.2.1" ref-type="sec">lead contact</xref> upon request.</p>
        </list-item>
      </list>
    </p>
  </sec>
  <ack id="ack0010">
    <title>Acknowledgment</title>
    <p id="p0320">This research was partially supported by <funding-source id="gs1">Zentrales Innovationsprogramm Mittelstand</funding-source> (ZIM) under grant agreement No. 16KN069455 (KIRun), <funding-source id="gs2"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source> (DFG) under grant agreement No. 421613952 (ParaStiChaD), and SCHU2508/12-1 (Leader Humor).</p>
    <sec id="sec6">
      <title>Author contributions</title>
      <p id="p0325">Conceptualization, S.O. and S.A.; Methodology, S.O. and S.A.; Software, S.O. and M.G.; Investigation, S.O.; Writing – Original Draft, S.O., S.A., and M.G.; Writing – Review &amp; Editing, B.S.; Supervision, S.A. and B.S.</p>
    </sec>
    <sec sec-type="COI-statement" id="sec7">
      <title>Declaration of interests</title>
      <p id="p0330">The authors declare no competing interests.</p>
    </sec>
  </ack>
  <fn-group>
    <fn id="appsec1" fn-type="supplementary-material">
      <p id="p0335">Supplemental information can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.isci.2022.104644" id="intref0060">https://doi.org/10.1016/j.isci.2022.104644</ext-link>.</p>
    </fn>
  </fn-group>
</back>
