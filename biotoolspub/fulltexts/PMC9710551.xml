<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9710551</article-id>
    <article-id pub-id-type="pmid">36227117</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btac678</article-id>
    <article-id pub-id-type="publisher-id">btac678</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Sequence Analysis</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>E-SNPs&amp;GO: embedding of protein sequence and function improves the annotation of human pathogenic variants</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Manfredi</surname>
          <given-names>Matteo</given-names>
        </name>
        <xref rid="btac678-FM1" ref-type="author-notes"/>
        <aff><institution>Biocomputing Group, Department of Pharmacy and Biotechnology, University of Bologna</institution>, Bologna 40126, <country country="IT">Italy</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-7359-0633</contrib-id>
        <name>
          <surname>Savojardo</surname>
          <given-names>Castrense</given-names>
        </name>
        <xref rid="btac678-FM1" ref-type="author-notes"/>
        <aff><institution>Biocomputing Group, Department of Pharmacy and Biotechnology, University of Bologna</institution>, Bologna 40126, <country country="IT">Italy</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0274-5669</contrib-id>
        <name>
          <surname>Martelli</surname>
          <given-names>Pier Luigi</given-names>
        </name>
        <xref rid="btac678-cor1" ref-type="corresp"/>
        <!--pierluigi.martelli@unibo.it-->
        <aff><institution>Biocomputing Group, Department of Pharmacy and Biotechnology, University of Bologna</institution>, Bologna 40126, <country country="IT">Italy</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-7462-7039</contrib-id>
        <name>
          <surname>Casadio</surname>
          <given-names>Rita</given-names>
        </name>
        <aff><institution>Biocomputing Group, Department of Pharmacy and Biotechnology, University of Bologna</institution>, Bologna 40126, <country country="IT">Italy</country></aff>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Boeva</surname>
          <given-names>Valentina</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btac678-cor1">To whom correspondence should be addressed. Email: <email>pierluigi.martelli@unibo.it</email></corresp>
      <fn id="btac678-FM1">
        <p>The authors wish it to be known that, in their opinion, Matteo Manfredi and Castrense Savojardo should be regarded as Joint First Authors.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="collection">
      <day>01</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-10-13">
      <day>13</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <volume>38</volume>
    <issue>23</issue>
    <fpage>5168</fpage>
    <lpage>5174</lpage>
    <history>
      <date date-type="received">
        <day>16</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>14</day>
        <month>9</month>
        <year>2022</year>
      </date>
      <date date-type="editorial-decision">
        <day>06</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>10</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="corrected-typeset">
        <day>26</day>
        <month>10</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btac678.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>The advent of massive DNA sequencing technologies is producing a huge number of human single-nucleotide polymorphisms occurring in protein-coding regions and possibly changing their sequences. Discriminating harmful protein variations from neutral ones is one of the crucial challenges in precision medicine. Computational tools based on artificial intelligence provide models for protein sequence encoding, bypassing database searches for evolutionary information. We leverage the new encoding schemes for an efficient annotation of protein variants.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>E-SNPs&amp;GO is a novel method that, given an input protein sequence and a single amino acid variation, can predict whether the variation is related to diseases or not. The proposed method adopts an input encoding completely based on protein language models and embedding techniques, specifically devised to encode protein sequences and GO functional annotations. We trained our model on a newly generated dataset of 101 146 human protein single amino acid variants in 13 661 proteins, derived from public resources. When tested on a blind set comprising 10 266 variants, our method well compares to recent approaches released in literature for the same task, reaching a Matthews Correlation Coefficient score of 0.72. We propose E-SNPs&amp;GO as a suitable, efficient and accurate large-scale annotator of protein variant datasets.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The method is available as a webserver at <ext-link xlink:href="https://esnpsandgo.biocomp.unibo.it" ext-link-type="uri">https://esnpsandgo.biocomp.unibo.it</ext-link>. Datasets and predictions are available at <ext-link xlink:href="https://esnpsandgo.biocomp.unibo.it/datasets" ext-link-type="uri">https://esnpsandgo.biocomp.unibo.it/datasets</ext-link>.</p>
      </sec>
      <sec id="s4a">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>PRIN 2017</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2017483NH8</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Italian Ministry of University and Research</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="7"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Single-nucleotide polymorphisms (SNPs) are major sources of human evolution. In many cases, these variations can be directly associated with the onset of genetic diseases. Specifically, SNPs occurring in protein-coding regions often lead to observable changes in the protein residue sequence. Single amino acid variations (SAVs) may have an impact at different levels, hampering protein structure, function, stability, localization and interaction with other proteins and/or nucleotides, hence setting the basis for the onset of pathologic conditions (<xref rid="btac678-B21" ref-type="bibr">Lappalainen and MacArthur, 2021</xref>; <xref rid="btac678-B50" ref-type="bibr">Vihinen, 2021</xref> and references therein).</p>
    <p>Public databases, such as HUMSAVAR (<xref rid="btac678-B47" ref-type="bibr">The UniProt Consortium, 2021</xref>) and ClinVar (<xref rid="btac678-B20" ref-type="bibr">Landrum <italic toggle="yes">et al.</italic>, 2018</xref>), store a compendium of known SAVs and provide, when available, information about the variant clinical significance. However, clear associations to diseases are still unknown for many SAVs, which substantially remain of Uncertain Significance (US). Therefore, SAV annotation is an issue, and effective computational tools are needed to provide large-scale annotation of uncharacterized human variation data.</p>
    <p>In the past years, several computational approaches have been implemented, with the aim of annotating whether a protein variation is or not disease associated (<xref rid="btac678-B1" ref-type="bibr">Adzhubei <italic toggle="yes">et al.</italic>, 2010</xref>; <xref rid="btac678-B8" ref-type="bibr">Calabrese <italic toggle="yes">et al.</italic>, 2009</xref>; <xref rid="btac678-B9" ref-type="bibr">Carter <italic toggle="yes">et al.</italic>, 2013</xref>; <xref rid="btac678-B11" ref-type="bibr">Choi <italic toggle="yes">et al.</italic>, 2012</xref>; <xref rid="btac678-B18" ref-type="bibr">Jagadeesh <italic toggle="yes">et al.</italic>, 2016</xref>; <xref rid="btac678-B22" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2009</xref>; <xref rid="btac678-B28" ref-type="bibr">Ng and Henikoff, 2001</xref>; <xref rid="btac678-B30" ref-type="bibr">Niroula <italic toggle="yes">et al.</italic>, 2015</xref>; <xref rid="btac678-B33" ref-type="bibr">Pejaver <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac678-B35" ref-type="bibr">Raimondi <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btac678-B37" ref-type="bibr">Schwarz <italic toggle="yes">et al.</italic>, 2010</xref>; <xref rid="btac678-B52" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2022</xref>). Methods like SIFT (<xref rid="btac678-B28" ref-type="bibr">Ng and Henikoff, 2001</xref>) or PROVEAN (<xref rid="btac678-B11" ref-type="bibr">Choi <italic toggle="yes">et al.</italic>, 2012</xref>) are based on the conservation analysis in multiple sequence alignments. More complex approaches stand on different types of machine-learning frameworks. These include neural networks (<xref rid="btac678-B33" ref-type="bibr">Pejaver <italic toggle="yes">et al.</italic>, 2020</xref>), random forests (<xref rid="btac678-B9" ref-type="bibr">Carter <italic toggle="yes">et al.</italic>, 2013</xref>; <xref rid="btac678-B22" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2009</xref>; <xref rid="btac678-B30" ref-type="bibr">Niroula <italic toggle="yes">et al.</italic>, 2015</xref>; <xref rid="btac678-B35" ref-type="bibr">Raimondi <italic toggle="yes">et al.</italic>, 2017</xref>), gradient tree boosting (<xref rid="btac678-B18" ref-type="bibr">Jagadeesh <italic toggle="yes">et al.</italic>, 2016</xref>; <xref rid="btac678-B52" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2022</xref>), support vector machines (SVMs) (<xref rid="btac678-B8" ref-type="bibr">Calabrese <italic toggle="yes">et al.</italic>, 2009</xref>) and naive Bayes classifiers (<xref rid="btac678-B1" ref-type="bibr">Adzhubei <italic toggle="yes">et al.</italic>, 2010</xref>; <xref rid="btac678-B37" ref-type="bibr">Schwarz <italic toggle="yes">et al.</italic>, 2010</xref>). Each method is trained/tested on different datasets of SAVs, either extracted directly from public resources like HUMSAVAR (<xref rid="btac678-B47" ref-type="bibr">The UniProt Consortium, 2021</xref>) and/or ClinVar (<xref rid="btac678-B20" ref-type="bibr">Landrum <italic toggle="yes">et al.</italic>, 2018</xref>), or taking advantage of pre-compiled datasets of variations, like VariBench (<xref rid="btac678-B27" ref-type="bibr">Nair and Vihinen, 2013</xref>). Different types of descriptors extract salient features of the protein sequence and/or the local sequence context surrounding the variant position, including physicochemical properties, sequence profiles, conservation scores, predicted structural motifs and functional annotations. SNPs&amp;GO (<xref rid="btac678-B8" ref-type="bibr">Calabrese <italic toggle="yes">et al.</italic>, 2009</xref>) firstly recognized the importance of functional annotations for the prediction of variant pathogenicity and introduced the LGO feature, a score of association between Gene Ontology (GO) (Ashburner <italic toggle="yes">et al.</italic>, 2000) annotations and the variant pathogenicity. The incorporation of the LGO feature significantly improved the prediction performance of SNPs&amp;GO (<xref rid="btac678-B8" ref-type="bibr">Calabrese <italic toggle="yes">et al.</italic>, 2009</xref>).</p>
    <p>Recent developments in the field of deep learning focus on the definition of new ways of representing protein sequences. Large-scale protein language models (PLMs) are inspired and derived from the natural language processing (NLP) field (<xref rid="btac678-B31" ref-type="bibr">Ofer <italic toggle="yes">et al</italic>, 2021</xref>). They learn numerical vector representations of protein sequences, containing important features that are reflected in the evolutionary conservation and in the sequence syntax (<xref rid="btac678-B7" ref-type="bibr">Bepler and Berger, 2021</xref>). These numerical vectors are then adopted to encode protein sequence and/or individual residues in place of canonical, hand-crafted features, such as physicochemical properties or evolutionary information. These distributed protein representations emerge from the application of learning models trained on large databases of sequence data (<xref rid="btac678-B7" ref-type="bibr">Bepler and Berger, 2021</xref>; <xref rid="btac678-B31" ref-type="bibr">Ofer <italic toggle="yes">et al.</italic>, 2021</xref>).</p>
    <p>Successful PLMs are routinely trained on databases composed of hundreds of millions of unique sequences with hundreds of billions of residues. Training is computationally demanding, routinely requiring weeks or months of computations on high-performance Tensor Processing Units (TPUs) and/or Graphical Processing Units (GPUs) (<xref rid="btac678-B14" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac678-B36" ref-type="bibr">Rives <italic toggle="yes">et al.</italic>, 2021</xref>). However, the advantage is that most of the computational cost is concentrated on the training phase, and once models are trained they can be adopted to embed new sequences with limited resources in terms of time, memory and computational power.</p>
    <p>Embeddings obtained with language models have been recently employed for many different applications with great success, including the prediction of protein function and localization (<xref rid="btac678-B23" ref-type="bibr">Littmann <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac678-B40" ref-type="bibr">Stärk <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac678-B46" ref-type="bibr">Teufel <italic toggle="yes">et al.</italic> 2022</xref>), of protein contact maps (<xref rid="btac678-B39" ref-type="bibr">Singh <italic toggle="yes">et al.</italic>, 2022</xref>) and binding sites (<xref rid="btac678-B24" ref-type="bibr">Mahbub and Bayzid, 2022</xref>).</p>
    <p>Several pre-trained language models currently exist in the literature (<xref rid="btac678-B2" ref-type="bibr">Alley <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac678-B4" ref-type="bibr">Asgari and Mofrad, 2015</xref>; <xref rid="btac678-B14" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac678-B16" ref-type="bibr">Heinzinger <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac678-B36" ref-type="bibr">Rives <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac678-B44" ref-type="bibr">Strodthoff <italic toggle="yes">et al.</italic>, 2020</xref>), mainly differing in their specific architectures [autoregressive, bidirectional, masked; see for review <xref rid="btac678-B7" ref-type="bibr">Bepler and Berger (2021)</xref>] and in the datasets adopted for training.</p>
    <p>Not limited to the encoding of protein sequence data, embedding techniques are also applied to model the relationships existing within more complex structures, such as graphs, networks, or biological ontologies (<xref rid="btac678-B13" ref-type="bibr">Edera <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btac678-B15" ref-type="bibr">Grover and Leskovec, 2016</xref>; <xref rid="btac678-B19" ref-type="bibr">Kandathil <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btac678-B34" ref-type="bibr">Perozzi <italic toggle="yes">et al.</italic>, 2014</xref>; <xref rid="btac678-B53" ref-type="bibr">Zhong <italic toggle="yes">et al.</italic>, 2019</xref>).</p>
    <p>In this article, we attempt to fully exploit the power of language models and embeddings for the prediction of variant pathogenicity from the human protein sequence. On the methodological side, two major contributions can be highlighted. Firstly, we adopt two different and complementary embedding procedures, ProtT5 (<xref rid="btac678-B14" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>) and ESM-1v (<xref rid="btac678-B26" ref-type="bibr">Meier <italic toggle="yes">et al.</italic>, 2021</xref>), to directly encode an input variation without introducing any hand-crafted feature as previously done. Secondly, leveraging the idea introduced in SNPs&amp;GO (<xref rid="btac678-B8" ref-type="bibr">Calabrese <italic toggle="yes">et al.</italic>, 2009</xref>), we explore a new way of encoding functional annotations by adopting a model called Anc2Vec (<xref rid="btac678-B13" ref-type="bibr">Edera <italic toggle="yes">et al.</italic>, 2022</xref>), specifically designed for the embedding of GO terms (<xref rid="btac678-B5" ref-type="bibr">Ashburner <italic toggle="yes">et al.</italic>, 2000</xref>).</p>
    <p>We trained an SVM using the above input encoding on a newly generated dataset of 101 146 human disease-related and benign variations obtained from the rational merging of data deposited in two databases, HUMSAVAR (<xref rid="btac678-B47" ref-type="bibr">The UniProt Consortium, 2021</xref>) and ClinVar (<xref rid="btac678-B20" ref-type="bibr">Landrum <italic toggle="yes">et al.</italic>, 2018</xref>). The method is tested on an independent, non-redundant blind set comprising 10 266 variations, adopting stringent homology reduction and evaluation procedures. Results obtained in a comparative benchmark and including one of the most recent and effective methods (<xref rid="btac678-B33" ref-type="bibr">Pejaver <italic toggle="yes">et al.</italic>, 2020</xref>), demonstrate that our model performs at the level or even better than the state-of-the-art (when available for comparison) reaching a Matthews Correlation Coefficient (MCC) of 0.72. Based on an input encoding derived solely from embedding models, our method is fast: this makes it suitable for large-scale annotation of human pathogenic variants.</p>
    <p>We release our tool as a webserver at <ext-link xlink:href="https://esnpsandgo.biocomp.unibo.it" ext-link-type="uri">https://esnpsandgo.biocomp.unibo.it</ext-link>.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Dataset</title>
      <p>We obtained the dataset of SAVs by merging information extracted from two resources: HUMSAVAR (accessed on August 4, 2021), listing all missense variants annotated in human UniProt/SwissProt (<xref rid="btac678-B47" ref-type="bibr">The UniProt Consortium, 2021</xref>) entries, and ClinVar (accessed on March 29, 2021), the NCBI resource of relationships among human variations and disease phenotypes (<xref rid="btac678-B20" ref-type="bibr">Landrum <italic toggle="yes">et al.</italic>, 2018</xref>).</p>
      <p>Both databases classify the effect of SAVs into different classes: Pathogenic or Likely Pathogenic (P/LP), Benign or Likely Benign (B/LB) and of US. We retained only P/LP SAVs clearly associated with the diseases catalogued in OMIM (<xref rid="btac678-B3" ref-type="bibr">Amberger <italic toggle="yes">et al.</italic>, 2019</xref>) or in MONDO (<xref rid="btac678-B38" ref-type="bibr">Shefchek <italic toggle="yes">et al.</italic>, 2020</xref>). We collected also all the B/LB variations and excluded SAVs labelled as US, somatic, or with contrasting annotations of the effect.</p>
      <p>Overall, the dataset consists of 13 661 protein sequences endowed with 111 412 SAVs, including 43 895 P/LP SAVs in 3603 proteins and 67 517 B/LB SAVs in 13 229 proteins (<xref rid="btac678-T1" ref-type="table">Table 1</xref>, last row).</p>
      <table-wrap position="float" id="btac678-T1">
        <label>Table 1.</label>
        <caption>
          <p>The dataset of SAVs adopted in this study</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Dataset</th>
              <th rowspan="1" colspan="1">No. of pathogenic SAVs</th>
              <th rowspan="1" colspan="1">No. of neutral SAVs</th>
              <th rowspan="1" colspan="1">No. of proteins</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Training set</td>
              <td align="char" char="." rowspan="1" colspan="1">39 812</td>
              <td align="char" char="." rowspan="1" colspan="1">61 334</td>
              <td align="char" char="." rowspan="1" colspan="1">12 347</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Blind test set</td>
              <td align="char" char="." rowspan="1" colspan="1">4083</td>
              <td align="char" char="." rowspan="1" colspan="1">6183</td>
              <td align="char" char="." rowspan="1" colspan="1">1314</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Total</td>
              <td align="char" char="." rowspan="1" colspan="1">43 895</td>
              <td align="char" char="." rowspan="1" colspan="1">67 517</td>
              <td align="char" char="." rowspan="1" colspan="1">13 661</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>For all proteins in the dataset, we extracted GO (Ashburner <italic toggle="yes">et al.</italic>, 2000) annotations from the corresponding entry in UniProt. Overall, our dataset is annotated with 17 076 GO terms, including 11 476 Biological Process (BP), 3955 Molecular Function (MF) and 1645 Cellular Component (CC). The complete dataset is available at <ext-link xlink:href="https://esnpsandgo.biocomp.unibo.it/datasets" ext-link-type="uri">https://esnpsandgo.biocomp.unibo.it/datasets</ext-link>.</p>
      <sec>
        <label>2.1.1</label>
        <title>Cross-validation procedure and generation of the blind test set</title>
        <p>To avoid biases between training and testing sets, we adopted a stringent clustering procedure to generate cross-validation sets. Firstly, we clustered protein sequences with the MMseqs2 program (<xref rid="btac678-B41" ref-type="bibr">Steinegger and Söding, 2017</xref>), by constraining a minimum sequence identity of 25% over a pairwise alignment coverage of at least 40%. We used a connected component clustering strategy so that if two proteins are clustered with a third one, they both end up in the same set. In this way, we limit sequence redundancy between training and testing sets, enabling a fair evaluation of the results. We selected 10% of the data to construct the blind test set for assessing the generalization performance of our approach and for benchmarking it with other popular methods available. The remaining 90% of the dataset was further split into 10 equally distributed subsets that were used in a 10-fold cross-validation procedure for optimizing the input encoding and for fixing the model hyperparameters. We also tried a 20–80% split (20% of the data for the blind test set and 80% for training with the 10-fold cross-validation procedure) and obtained a very similar performance. For this reason, we list results corresponding to the 10% blind test. When performing cross-validation, we took care of preserving the balancing of positive and negative examples in each subset (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>).</p>
        <p>It is worth noticing that the blind test can share similarity with proteins included in the training sets of the other benchmarked methods.</p>
      </sec>
    </sec>
    <sec>
      <title>2.2 General overview of the approach</title>
      <p><xref rid="btac678-F1" ref-type="fig">Figure 1</xref> depicts the architecture of E-SNPs&amp;GO, including three major blocks: an Input encoding, a Predictor and an Output. The input consists of a human protein sequence and a SAV occurring at a specific position along the sequence. In the input encoding phase, the sequence and its variant are embedded with two different procedures, ESM-1v (<xref rid="btac678-B26" ref-type="bibr">Meier <italic toggle="yes">et al.</italic>, 2021</xref>) and ProtT5 (<xref rid="btac678-B14" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>), generating for each sequence 1280 and 1024 features, respectively. In order to embed the functional protein annotation of the wild-type protein, we adopt Anc2Vec (<xref rid="btac678-B13" ref-type="bibr">Edera <italic toggle="yes">et al.</italic>, 2022</xref>), computing three sets of 200 features corresponding to the different subontologies.</p>
      <fig position="float" id="btac678-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>General overview of the architecture of E-SNPs&amp;GO. Inputs (wild-type sequence, variation and variation position) are in yellow. The architecture includes three major blocks: an Input encoding, a Predictor and an Output. During the Input encoding, three embedding models are adopted to generate vector representations. The wild-type sequence (green) and the variant sequence (red) are modelled with ESM-1v (<xref rid="btac678-B26" ref-type="bibr">Meier <italic toggle="yes">et al.</italic>, 2021</xref>) and ProtT5 (<xref rid="btac678-B14" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>). The GO functional annotations (blue MF, purple CC and pink BP) are modelled with Anc2Vec (<xref rid="btac678-B13" ref-type="bibr">Edera <italic toggle="yes">et al.</italic>, 2022</xref>). The vectors within the dashed box (marked with different colors), representing the variation position and the averaged (Avg) GO terms of the wild-type sequence, are then concatenated together to obtain a final representation consisting of 1280×2 + 1024×2 + 200×3 = 5208 features. This vector is fed to the Predictor, which includes a PCA to reduce the input dimensionality (from 5208 to 2400) and a SVM providing as a final output a binary classification into B/LB (negative class, Score &lt;0) or P/LP (positive class, Score ≥0). We apply an Isotonic Regression (Calibration) to obtain a calibrated probability (A color version of this figure appears in the online version of this article.)</p>
        </caption>
        <graphic xlink:href="btac678f1" position="float"/>
      </fig>
      <p>In the predictor, the vector representation generated in the input encoding is then processed using a principal component analysis (PCA), which reduces the dimensionality of the input from 5208 features to 2400. The output feeds a SVM classifier performing the final labelling as Pathogenic (P/LP) or Benign (B/LB). A given input variant is predicted as pathogenic when the SVM output score ≥0, benign otherwise. A final calibration step allows to convert scores into probabilities for a variant to be pathogenic. Details of the methods included in E-SNPs&amp;GO, are listed in the following sections.</p>
    </sec>
    <sec>
      <title>2.3 Input encoding: embeddings of protein sequence, its variant and GO terms</title>
      <sec>
        <label>2.3.1</label>
        <title>Transformers for embedding of protein sequences and their variants</title>
        <p>Several prominent language models and corresponding embedding generation schemes in NLP are available, and some of these have been adapted to protein sequences to perform specific prediction tasks (<xref rid="btac678-B7" ref-type="bibr">Bepler and Berger, 2021</xref>). Large-scale PLMs aim at learning a numerical vector representation that allows reconstructing the input sequence.</p>
        <p>Among PLMs, transformer-based models (<xref rid="btac678-B48" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic>, 2017</xref>) aim to solve the problem of efficiently capturing long-distance interactions in the sequence. Transformers are architectures that include a self-attention mechanism to extract the context information from the whole sequence (<xref rid="btac678-B48" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic>, 2017</xref>). In general, a transformer language model builds on top of an encoder–decoder architecture. However, the different transformer-based PLMs only utilize either the encoder or the decoder part. In this respect, transformer-based PLMs can be classified in three different categories: (i) encoder-only models use only the encoder part of the transformer accessing the whole input sequence and are trained to reconstruct a somewhat corrupted version of the input (e.g. masking random positions along the sequence); (ii) decoder-only models (also called autoregressive models) use only the decoder part accessing, at each position, all the residues placed before the current one in the sequence and are usually trained to predict the next residue in the sequence; (iii) sequence-to-sequence models use both the encoder and the decoder and are trained to reconstruct a masked input sequence (<xref rid="btac678-B48" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic>, 2017</xref>).</p>
        <p>The learned representation captures important features of the proteins, including physicochemical, structural, functional and evolutionary features (<xref rid="btac678-B7" ref-type="bibr">Bepler and Berger, 2021</xref>; <xref rid="btac678-B31" ref-type="bibr">Ofer <italic toggle="yes">et al.</italic>, 2021</xref>). By transfer learning, the embedded schemes are provided as input to Predictor block (<xref rid="btac678-F1" ref-type="fig">Fig. 1</xref>).</p>
        <p>In this article, we adopt two different protein embedding schemes, based on two different transformers models: ESM-1v (<xref rid="btac678-B26" ref-type="bibr">Meier <italic toggle="yes">et al.</italic>, 2021</xref>), an encoder-only model, and ProtT5 (<xref rid="btac678-B14" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>), a sequence-to-sequence model. The major difference stands in the volume of the sequence datasets used for generating the embedding schemes and in the adoption of different training procedures. ESM-1v was trained on a single run using a dataset of 98 million unique sequences extracted from UniRef90 (<xref rid="btac678-B45" ref-type="bibr">Suzek <italic toggle="yes">et al.</italic>, 2015</xref>). ESM-1v releases five models generated by training with five different random seeds (<xref rid="btac678-B26" ref-type="bibr">Meier <italic toggle="yes">et al.</italic>, 2021</xref>). Apparently, only a small difference in performance is obtained when the ensemble is compared to a single model (<xref rid="btac678-B26" ref-type="bibr">Meier <italic toggle="yes">et al.</italic>, 2021</xref>). Therefore, to reduce the computational cost, we adopted only one model (the first one). ProtT5 (version XL U50) was trained using a two-step procedure: in a first pass, training was performed using the large BFD database (<xref rid="btac678-B43" ref-type="bibr">Steinegger <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac678-B42" ref-type="bibr">Steinegger and Söding, 2018</xref>), comprising the whole UniProt as well as protein sequences translated from multiple metagenomic sequencing projects, and consisting of about 2.1 billion unique sequences. In the second pass, a fine-tuning of the model was obtained using a smaller database derived from UniRef50 (<xref rid="btac678-B45" ref-type="bibr">Suzek <italic toggle="yes">et al.</italic>, 2015</xref>) and including 45 million unique sequences.</p>
      </sec>
      <sec>
        <label>2.3.2</label>
        <title>Embedding of biological ontologies</title>
        <p>The concept of embedding can be generalized to any kind of data with different underlying structures, such as graphs or networks (<xref rid="btac678-B15" ref-type="bibr">Grover and Leskovec, 2016</xref>; <xref rid="btac678-B34" ref-type="bibr">Perozzi <italic toggle="yes">et al.</italic>, 2014</xref>). In particular, several embedding models have been defined to provide a numerical representation of nodes in ontologies (<xref rid="btac678-B10" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac678-B53" ref-type="bibr">Zhong <italic toggle="yes">et al.</italic>, 2019</xref>). Here, we adopt Anc2Vec (<xref rid="btac678-B13" ref-type="bibr">Edera <italic toggle="yes">et al.</italic>, 2022</xref>), a method that learns a vector representation for GO terms, by preserving ancestor relationships.</p>
        <p>Because the embedding is not context-dependent, we precompute the vector representation for each possible GO term.</p>
      </sec>
    </sec>
    <sec>
      <title>2.4 Predictor</title>
      <sec>
        <label>2.4.1</label>
        <title>Predictor input</title>
        <p>For encoding variations, we firstly perform a full-sequence generation of embeddings using both the ESM-1v (<xref rid="btac678-B26" ref-type="bibr">Meier <italic toggle="yes">et al.</italic>, 2021</xref>) and the ProtT5 XL U50 (<xref rid="btac678-B14" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>) models. Given a protein sequence with <italic toggle="yes">L</italic> residues, this provides protein encodings of dimensions <italic toggle="yes">L</italic>×1280 and <italic toggle="yes">L</italic>×1024, respectively. Sequence embeddings are carried out independently on both the wild-type and the variant sequence.</p>
        <p>For a variation at position <italic toggle="yes">i</italic> in a protein sequence, we compute a vector of 4608 features, including:
</p>
        <list list-type="bullet">
          <list-item>
            <p>1280 features corresponding to ESM-1v embedding in position <italic toggle="yes">i</italic> of the variated sequence.</p>
          </list-item>
          <list-item>
            <p>1280 features corresponding to ESM-1v embedding in position <italic toggle="yes">i</italic> of the wild-type sequence.</p>
          </list-item>
          <list-item>
            <p>1024 features corresponding to ProtT5 (version XL U50) embedding in position <italic toggle="yes">i</italic> of the variated sequence.</p>
          </list-item>
          <list-item>
            <p>1024 features corresponding to ProtT5 (version XL U50) embedding in position <italic toggle="yes">i</italic> of the wild-type sequence.</p>
          </list-item>
        </list>
        <p>The ESM-1v embedding model constrains the maximal protein length (<italic toggle="yes">L</italic>) to 1024 residues. For this reason, variations occurring on longer sequences were encoded using a 201 long sequence window centred on the variant position.</p>
        <p>After this step, we extract all the GO terms annotated in the UniProt entry of the wild-type protein carrying the variation. Potential term redundancy is removed by retaining only leaf terms. Terms from the three different GO sub-ontologies (MF, CC and BP) are processed independently. Each annotated GO term is then embedded as a vector of 200 features using the Anc2Vec model (<xref rid="btac678-B13" ref-type="bibr">Edera <italic toggle="yes">et al.</italic>, 2022</xref>). To obtain a single vector representation independent of the number of terms of a given protein, we average all the vector encodings (<xref rid="btac678-F1" ref-type="fig">Fig. 1</xref>). Three final average vectors, one for each GO sub-ontology, are concatenated obtaining a protein function encoding of 600 components.</p>
        <p>The final variation encoding comprises 5208 features, obtained by merging the local positional embedding (4608 features from ESM-1V + ProtT5 XL U50) described above and the Anc2Vec functional encoding (600 features). Eventually, we encode the different embeddings separately (see Section 3 and <xref rid="btac678-T2" ref-type="table">Table 2</xref>).</p>
        <table-wrap position="float" id="btac678-T2">
          <label>Table 2.</label>
          <caption>
            <p>Performance of different embedding schemes</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Input encoding</th>
                <th rowspan="1" colspan="1"><italic toggle="yes">Q</italic><sub>2</sub> (%)</th>
                <th rowspan="1" colspan="1">Precision (%)</th>
                <th rowspan="1" colspan="1">Recall (%)</th>
                <th rowspan="1" colspan="1"><italic toggle="yes">F</italic>1-score (%)</th>
                <th rowspan="1" colspan="1">ROC-AUC (%)</th>
                <th rowspan="1" colspan="1">MCC</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">ESM-1v</td>
                <td rowspan="1" colspan="1">82.4 (±1.5)</td>
                <td rowspan="1" colspan="1">80.4 (±2.6)</td>
                <td rowspan="1" colspan="1">77.0 (±2.8)</td>
                <td rowspan="1" colspan="1">78.6 (±1.9)</td>
                <td rowspan="1" colspan="1">81.6 (±1.5)</td>
                <td rowspan="1" colspan="1">0.64 (±0.03)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ESM-1v+GO</td>
                <td rowspan="1" colspan="1">83.3 (±1.4)</td>
                <td rowspan="1" colspan="1">81.7 (±2.5)</td>
                <td rowspan="1" colspan="1">78.1 (±2.7)</td>
                <td rowspan="1" colspan="1">79.8 (±1.8)</td>
                <td rowspan="1" colspan="1">82.6 (±1.4)</td>
                <td rowspan="1" colspan="1">0.66 (±0.03)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ProtT5</td>
                <td rowspan="1" colspan="1">83.0 (±1.3)</td>
                <td rowspan="1" colspan="1">79.8 (±1.9)</td>
                <td rowspan="1" colspan="1">80.0 (±2.8)</td>
                <td rowspan="1" colspan="1">79.9 (±1.7)</td>
                <td rowspan="1" colspan="1">82.6 (±1.4)</td>
                <td rowspan="1" colspan="1">0.65 (±0.03)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ProtT5+GO</td>
                <td rowspan="1" colspan="1">83.7 (±1.1)</td>
                <td rowspan="1" colspan="1">81.8 (±1.9)</td>
                <td rowspan="1" colspan="1">79.2 (±2.5)</td>
                <td rowspan="1" colspan="1">80.5 (±1.5)</td>
                <td rowspan="1" colspan="1">83.1 (±1.3)</td>
                <td rowspan="1" colspan="1">0.67 (±0.02)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ESM-1v+ProtT5</td>
                <td rowspan="1" colspan="1">83.6 (±1.4)</td>
                <td rowspan="1" colspan="1">81.8 (±2.3)</td>
                <td rowspan="1" colspan="1">78.6 (±2.9)</td>
                <td rowspan="1" colspan="1">80.1 (±1.8)</td>
                <td rowspan="1" colspan="1">82.9 (±1.5)</td>
                <td rowspan="1" colspan="1">0.66 (±0.03)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ESM-1v+ProtT5+GO(-PCA)</td>
                <td rowspan="1" colspan="1">83.1 (±0.8)</td>
                <td rowspan="1" colspan="1">81.0 (±1.4)</td>
                <td rowspan="1" colspan="1">78.0 (±1.5)</td>
                <td rowspan="1" colspan="1">79.4 (±1.1)</td>
                <td rowspan="1" colspan="1">82.8 (±0.8)</td>
                <td rowspan="1" colspan="1">0.66 (±0.02)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ESM-1v+ProtT5+GO(+PCA)</td>
                <td rowspan="1" colspan="1">85.1 (±0.9)</td>
                <td rowspan="1" colspan="1">82.4 (±1.7)</td>
                <td rowspan="1" colspan="1">79.1 (±1.7)</td>
                <td rowspan="1" colspan="1">80.7 (±1.1)</td>
                <td rowspan="1" colspan="1">84.1 (±0.9)</td>
                <td rowspan="1" colspan="1">0.69 (±0.02)</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn1">
              <p><italic toggle="yes">Note</italic>: We adopted a 10-fold cross-validation on a training set comprising 101 146 human variations (<xref rid="btac678-T1" ref-type="table">Table 1</xref>) for testing the effect of different input encodings on the performances of the method. Standard deviation (between brackets) is computed over the 10 cross-validation sets and scoring indexes (defined in Section 2.6) are average values.</p>
            </fn>
            <fn id="tblfn2">
              <p>ESM-1v (2×1280 = 2560 features).</p>
            </fn>
            <fn id="tblfn3">
              <p>ESM-1v + GO (2×1280 + 3×200 = 3160 features).</p>
            </fn>
            <fn id="tblfn4">
              <p>ProtT5 (2×1024 = 2048 features).</p>
            </fn>
            <fn id="tblfn5">
              <p>ProtT5 + GO (2×1024 + 3×200 = 2648 features).</p>
            </fn>
            <fn id="tblfn6">
              <p>ESM-1v + ProtT5 (2×1280 + 2×1024 = 4608 features).</p>
            </fn>
            <fn id="tblfn7">
              <p>ESM-1v + ProtT5 + GO (−PCA) (2×1280 + 2×1024 + 3×200 = 5208 features), no PCA used.</p>
            </fn>
            <fn id="tblfn8">
              <p>ESM-1v + ProtT5 + GO (+PCA) (2×1280 + 2×1024 + 3×200 = 5208 features), PCA used to reduce dimensionality.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec>
        <label>2.4.2</label>
        <title>Model selection and implementation</title>
        <p>The predictor includes two cascading components (<xref rid="btac678-F1" ref-type="fig">Fig. 1</xref>): a PCA for reducing the dimensionality of the input features and a binary SVM with a Radial Basis Function (RBF) kernel, which performs the variant classification into pathogenic or not. We optimized the hyperparameters of both methods (such as the number of components of PCA, the SVM cost parameter C and the gamma coefficient of the RBF kernel) with a grid search procedure. A complete list of hyperparameters tested and their optimal values are available in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>.</p>
        <p>It is worth clarifying that, during both cross-validation and blind testing, the execution of the PCA step is always computed on the training set and then applied for projecting vectors of the testing set in the reduced space.</p>
        <p>All methods are implemented in Python3 using the scikit-learn library (<xref rid="btac678-B32" ref-type="bibr">Pedregosa <italic toggle="yes">et al.</italic>, 2011</xref>). ESM-1v and ProtT5 embeddings are computed with the bio-embeddings package (<xref rid="btac678-B12" ref-type="bibr">Dallago <italic toggle="yes">et al.</italic>, 2021</xref>).</p>
        <p>The complete machine-learning workflow is compliant with the DOME recommendation checklist (<xref rid="btac678-B51" ref-type="bibr">Walsh <italic toggle="yes">et al.</italic>, 2021</xref>), as reported in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S3</xref>.</p>
      </sec>
    </sec>
    <sec>
      <title>2.5 Output</title>
      <p>The SVM adopted for classification computes a decision function that represents the distance of the point mapping the input from the discrimination boundary. We use this value to estimate the reliability of the prediction, in terms of the probability of the input variation to be pathogenic (<xref rid="btac678-F1" ref-type="fig">Fig. 1</xref>).</p>
      <p>In a perfectly calibrated method, when a set of predictions scored with probability <italic toggle="yes">P</italic> is tested on real data, we expect that the fraction of true positives is exactly <italic toggle="yes">P</italic>. In this work, we adopt a procedure previously described (Benevenuta <italic toggle="yes">et al.</italic>, 2021) to obtain a calibrated probability that we provide in output alongside the predicted class. In particular, we fit an Isotonic Regression (<xref rid="btac678-B29" ref-type="bibr">Niculescu-Mizil and Caruana, 2005</xref>) in cross-validation and we use it to obtain a probability score on the blind test. <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S1</xref> shows that E-SNPs&amp;GO output probabilities are very close to being perfectly calibrated, more than other popular methods.</p>
      <p>Keeping as a reference the probability of being P/PL, the probability score (<italic toggle="yes">P</italic><sub>P/PL</sub>) gives an integer Reliability Index from 0 (random prediction) to 10 (certain prediction) using the formula:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mi>RI</mml:mi><mml:mo>=</mml:mo><mml:mi>round</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>20</mml:mn><mml:mo>×</mml:mo><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mi>L</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula></p>
    </sec>
    <sec>
      <title>2.6 Scoring indexes</title>
      <p>We assess the performance with the following scores. P/LP variations are assumed to be the positive class, B/LB variations are the negative class. In what follows, TP, TN, FP and FN are true positive, true negative, false positive and false negative predictions, respectively.</p>
      <p>We compute the following scoring measures:
</p>
      <list list-type="bullet">
        <list-item>
          <p>Accuracy (<italic toggle="yes">Q</italic><sub>2</sub>):
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">TN</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FN</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        </list-item>
        <list-item>
          <p>Precision:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mi mathvariant="normal">Precision</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FP</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        </list-item>
        <list-item>
          <p>Recall:
<disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mi mathvariant="normal">Recall</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FN</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        </list-item>
        <list-item>
          <p><italic toggle="yes">F</italic>1-score, the harmonic mean of precision and recall:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">Precision</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">Recall</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Precision</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Recall</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        </list-item>
        <list-item>
          <p>Area under the receiver operating characteristic curve (ROC-AUC).</p>
        </list-item>
        <list-item>
          <p>MCC:
<disp-formula id="E6"><label>(6)</label><mml:math id="M6" display="block" overflow="scroll"><mml:mi mathvariant="normal">MCC</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">TN</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal">FP</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">FN</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mo>(</mml:mo><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FP</mml:mi><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FN</mml:mi><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="normal">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FP</mml:mi><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="normal">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FN</mml:mi><mml:mo>)</mml:mo></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        </list-item>
      </list>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Assessing the contribution of different input encodings</title>
      <p>To select the optimal input encoding, we performed different experiments to test various combinations of input features. To this aim, we trained in cross-validation several independent SVM+PCA models using different input features and using the MCC to score and select the optimal model.</p>
      <p>GO terms provide global protein information. Their embedding does not consider the specific variant position. If the prediction is run considering only averaged embedded GO terms vector (<xref rid="btac678-F1" ref-type="fig">Fig. 1</xref>), the predictor performance is very low (MCC = 0.27, data not shown). Different input encodings, corresponding to different predictors, perform differently (<xref rid="btac678-T2" ref-type="table">Table 2</xref>). The inclusion of GO embeddings in the final input is always beneficial, improving MCC by 2 or 3 percentage points in all cases (compare ESM-1v, ProtT5 and ESM-1v+ProtT5 with or without GO, respectively in <xref rid="btac678-T2" ref-type="table">Table 2</xref>). Considering the two protein sequence embeddings, ProtT5 outperforms ESM-1v both with and without the additional GO information. Most notably, the model trained on data from ProtT5 alone is the most balanced, reaching equal precision and recall. Finally, the concatenation of both sequence encodings and the GO embedding provides the best performance (MCC = 0.69), leading to an increase in precision without a corresponding decrease in recall.</p>
      <p>Based on these results, we select the model trained with ESM-1v+ProtT5+GO as the optimal one.</p>
    </sec>
    <sec>
      <title>3.2 Benchmark on the blind test set</title>
      <p>We test our method adopting both a 10-fold cross-validation procedure and an independent blind test set constructed to be non-redundant with respect to the training dataset (see Section 2.1). <xref rid="btac678-T3" ref-type="table">Table 3</xref> lists the results. E-SNPs&amp;GO obtains similar results in cross-validation and blind test, making it very robust to generalization. Concerning individual indexes, our method seems to be slightly more precise than sensitive (compare Precision and Recall).</p>
      <table-wrap position="float" id="btac678-T3">
        <label>Table 3.</label>
        <caption>
          <p>Benchmark of our and other top scoring methods available in literature</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Input encoding</th>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"><italic toggle="yes">Q</italic><sub>2</sub> (%)</th>
              <th rowspan="1" colspan="1">Precision (%)</th>
              <th rowspan="1" colspan="1">Recall (%)</th>
              <th rowspan="1" colspan="1"><italic toggle="yes">F</italic>1-score (%)</th>
              <th rowspan="1" colspan="1">ROC-AUC (%)</th>
              <th rowspan="1" colspan="1">MCC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">E-SNPs&amp;GO<xref rid="tblfn10" ref-type="table-fn"><sup>a</sup></xref></td>
              <td rowspan="1" colspan="1">Cross-validation</td>
              <td rowspan="1" colspan="1">85.1 (±0.9)</td>
              <td rowspan="1" colspan="1">82.4 (±1.7)</td>
              <td rowspan="1" colspan="1">79.1 (±1.7)</td>
              <td rowspan="1" colspan="1">80.7 (±1.1)</td>
              <td rowspan="1" colspan="1">84.1 (±0.9)</td>
              <td rowspan="1" colspan="1">0.69 (±0.018)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">E-SNPs&amp;GO<xref rid="tblfn10" ref-type="table-fn"><sup>a</sup></xref></td>
              <td rowspan="1" colspan="1">Blind test set</td>
              <td rowspan="1" colspan="1">86.8</td>
              <td rowspan="1" colspan="1">85.7</td>
              <td rowspan="1" colspan="1">80.1</td>
              <td rowspan="1" colspan="1">82.8</td>
              <td rowspan="1" colspan="1">85.6</td>
              <td rowspan="1" colspan="1">0.72</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SNPs&amp;GO<xref rid="tblfn10" ref-type="table-fn"><sup>a</sup></xref></td>
              <td rowspan="1" colspan="1">Blind test set</td>
              <td rowspan="1" colspan="1">79.8</td>
              <td rowspan="1" colspan="1">84.8</td>
              <td rowspan="1" colspan="1">63.2</td>
              <td rowspan="1" colspan="1">72.4</td>
              <td rowspan="1" colspan="1">77.5</td>
              <td rowspan="1" colspan="1">0.58</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MutPred2.0<xref rid="tblfn11" ref-type="table-fn"><sup>b</sup></xref></td>
              <td rowspan="1" colspan="1">Blind test set</td>
              <td rowspan="1" colspan="1">85.6</td>
              <td rowspan="1" colspan="1">78.6</td>
              <td rowspan="1" colspan="1">87.7</td>
              <td rowspan="1" colspan="1">82.9</td>
              <td rowspan="1" colspan="1">85.9</td>
              <td rowspan="1" colspan="1">0.71</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PROVEAN<xref rid="tblfn12" ref-type="table-fn"><sup>c</sup></xref></td>
              <td rowspan="1" colspan="1">Blind test set</td>
              <td rowspan="1" colspan="1">78.2</td>
              <td rowspan="1" colspan="1">68.7</td>
              <td rowspan="1" colspan="1">83.0</td>
              <td rowspan="1" colspan="1">75.2</td>
              <td rowspan="1" colspan="1">79.0</td>
              <td rowspan="1" colspan="1">0.57</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SIFT<xref rid="tblfn13" ref-type="table-fn"><sup>d</sup></xref></td>
              <td rowspan="1" colspan="1">Blind test set</td>
              <td rowspan="1" colspan="1">74.4</td>
              <td rowspan="1" colspan="1">62.7</td>
              <td rowspan="1" colspan="1">88.0</td>
              <td rowspan="1" colspan="1">73.2</td>
              <td rowspan="1" colspan="1">76.7</td>
              <td rowspan="1" colspan="1">0.53</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PolyPhen-2<xref rid="tblfn14" ref-type="table-fn"><sup>e</sup></xref></td>
              <td rowspan="1" colspan="1">Blind test set</td>
              <td rowspan="1" colspan="1">72.3</td>
              <td rowspan="1" colspan="1">60.6</td>
              <td rowspan="1" colspan="1">89.5</td>
              <td rowspan="1" colspan="1">72.2</td>
              <td rowspan="1" colspan="1">75.1</td>
              <td rowspan="1" colspan="1">0.50</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn9">
            <p><italic toggle="yes">Note</italic>: The benchmark is performed on a test set comprising 10 266 human variations (<xref rid="btac678-T1" ref-type="table">Table 1</xref>, 10% of the total number of SAVs) that is blind with respect to our training set. It could be redundant with respect to the training sets of other methods, leading to a possible overestimation of their performances. We also report our performances in cross-validation for comparison. We increased the size of the blind test set up to 20% of the number of SAVs and the E-SNPs&amp;GO MCC score values were negligibly affected (0.5%, data not shown).</p>
          </fn>
          <fn id="tblfn10">
            <label>a</label>
            <p>E-SNPs&amp;GO: this article; SNPs&amp;GO (<xref rid="btac678-B8" ref-type="bibr">Calabrese <italic toggle="yes">et al.</italic>, 2009</xref>).</p>
          </fn>
          <fn id="tblfn11">
            <label>b</label>
            <p>MutPred2.0 (<xref rid="btac678-B33" ref-type="bibr">Pejaver <italic toggle="yes">et al.</italic>, 2020</xref>).</p>
          </fn>
          <fn id="tblfn12">
            <label>c</label>
            <p>PROVEAN (<xref rid="btac678-B11" ref-type="bibr">Choi <italic toggle="yes">et al.</italic>, 2012</xref>).</p>
          </fn>
          <fn id="tblfn13">
            <label>d</label>
            <p>SIFT (<xref rid="btac678-B28" ref-type="bibr">Ng and Henikoff, 2001</xref>).</p>
          </fn>
          <fn id="tblfn14">
            <label>e</label>
            <p>PolyPhen-2 (<xref rid="btac678-B1" ref-type="bibr">Adzhubei <italic toggle="yes">et al.</italic>, 2010</xref>).</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p><xref rid="btac678-T3" ref-type="table">Table 3</xref> includes also a comparative benchmark of our method with other state-of-the-art tools, including our SNPs&amp;GO (<xref rid="btac678-B8" ref-type="bibr">Calabrese <italic toggle="yes">et al.</italic>, 2009</xref>), SIFT (<xref rid="btac678-B28" ref-type="bibr">Ng and Henikoff, 2001</xref>), PolyPhen-2 (<xref rid="btac678-B1" ref-type="bibr">Adzhubei <italic toggle="yes">et al.</italic>, 2010</xref>), PROVEAN (<xref rid="btac678-B11" ref-type="bibr">Choi <italic toggle="yes">et al.</italic>, 2012</xref>) and MutPred2 (<xref rid="btac678-B33" ref-type="bibr">Pejaver <italic toggle="yes">et al.</italic>, 2020</xref>), one of the most recent and best-performing approaches in the field. Methods are scored adopting our blind test set (Section 2.1), ensuring a fair evaluation of the performance of our method. However, this does not completely exclude the presence of biases in the evaluation of the other tools (with the exception of our SNPs&amp;GO), since variations included in our blind test may be present in the respective training sets, leading to potential overestimation of their performance.</p>
      <p>In <xref rid="btac678-T3" ref-type="table">Table 3</xref>, it appears that in this benchmark our method is performing at the state-of-the-art. Among tested approaches, PROVEAN, SIFT and PolyPhen-2, reporting MCCs of 0.57, 0.53 and 0.50, respectively, are scoring lower than our previous SNPs&amp;GO (that achieves an MCC of 0.58). Our E-SNPs&amp;GO and MutPred2, score with significantly higher MCC values of 0.72 and 0.71, respectively. Noticeably the embedding procedure seems to grasp all the properties extracted by an ensemble of different predictors of functional, structural and physicochemical properties, such as the one used by MutPred2 (including over 50 tools). Looking at individual scoring measures, MutPred2 appears more sensitive while our method reports a higher precision.</p>
      <p>A detailed ablation study performed to evaluate the effect of the GO terms on the prediction scores (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S4</xref>), indicates that the CC sub-ontology slightly outperforms the others.</p>
    </sec>
    <sec>
      <title>3.3 Prediction of variants of uncertain significance</title>
      <p>We tested E-SNPs&amp;GO on a dataset of 2588 proteins annotated with 9165 variants of uncertain significance (VUS) extracted from HUMSAVAR (accessed on May 12, 2022). Given that they are uncertain, we cannot assess our performances on this dataset. However, we can sample our predicted annotation in terms of probability and reliability [Equation (6)]. Setting as a reference the probability of being P/LP, <xref rid="btac678-F2" ref-type="fig">Figure 2</xref> shows the distribution of E-SNPs&amp;GO predictions over the whole VUS set as a function of probability and reliability index. A total of 4537 variations are P/LP (pathogenicity probability ≥0.5), while 4628 are B/LB (pathogenicity probability &lt;0.5). The reliability index increases as the probability goes towards 1 or 0 for P/LP and B/LB predictions, respectively [Equation (6)]. In the dataset, 3210 P/LP and 2908 B/LB predictions score with a reliability [RI, Equation (6)] ≥6, accounting for the 67% of VUS. The remaining 33% is predicted with RI lower than 6. For further validation, VUS predictions are available at <ext-link xlink:href="https://esnpsandgo.biocomp.unibo.it/datasets" ext-link-type="uri">https://esnpsandgo.biocomp.unibo.it/datasets</ext-link>.</p>
      <fig position="float" id="btac678-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>Distribution of predicted pathogenicity probabilities for the dataset of VUS. The value 0.5 discriminates between B/LB and P/LP prediction. Probability values close to either 0 or 1 correspond to prediction with a high reliability index [Equation (1)]</p>
        </caption>
        <graphic xlink:href="btac678f2" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.4 E-SNP&amp;GO web server</title>
      <p>E-SNPs&amp;GO web server is available at <ext-link xlink:href="https://esnpsandgo.biocomp.unibo.it" ext-link-type="uri">https://esnpsandgo.biocomp.unibo.it</ext-link>. The server allows users to submit up to 1000 variations per single job. Upon job completion, the results can be visualized on the web page and downloaded in either a tab-separated or a JSON file.</p>
      <p>We measured the average E-SNPs&amp;GO runtime by submitting 100 different jobs each including 1000 variations randomly selected from the blind test set. In order to estimate the real execution time for the end user, this experiment was performed in the machine hosting the web server, equipped with one AMD EPYC 7301 CPU with 12 cores, 48 GB of RAM and no GPU available. On average, we obtain a running time of 12.4 ± 4.4 s per variation, when submitting the maximum allowed number of variations per job (1000 variations). This highlights a significant improvement over time-consuming approaches using canonical features such as evolutionary information extracted from multiple sequence alignments.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusions</title>
    <p>We introduce E-SNPs&amp;GO, a method based on language models for annotating whether a single-nucleotide variation is or is not P/LP. We adopt two different protein embedding procedures based on transformers, ESM-1v (<xref rid="btac678-B26" ref-type="bibr">Meier <italic toggle="yes">et al.</italic>, 2021</xref>) and ProtT5 (<xref rid="btac678-B14" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>). Both embedding methods have been developed and tested on protein variant related problems, such as deep mutational scanning (<xref rid="btac678-B25" ref-type="bibr">Marquet <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac678-B26" ref-type="bibr">Meier <italic toggle="yes">et al.</italic>, 2021</xref>). Here, we address the problem of annotating pathogenic versus benign variations. To this aim, we also add an embedding scheme for functional annotations of wild-type proteins, Anc2Vec (<xref rid="btac678-B13" ref-type="bibr">Edera <italic toggle="yes">et al.</italic>, 2022</xref>), a method that learns a vector representation for GO terms by preserving ancestor relationships. When benchmarked towards state-of-the-art methods available, E-SNPs&amp;GO well compares to the recently developed MutPred2.0 (<xref rid="btac678-B33" ref-type="bibr">Pejaver <italic toggle="yes">et al.</italic>, 2020</xref>), which includes as input sequence features derived from some 50 predictors and outperforms previously published methods. Evidently, protein language models learn all the relevant information that can be eventually introduced as input by predictors addressing different tasks.</p>
    <p>We prove that embedding models overpass the problem of having as input thousands of different features in order to collect all the relevant features for a reliable annotation of the human pathogenic variations.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by PRIN 2017 project [2017483NH8 to C.S.] (Italian Ministry of University and Research).</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btac678_Supplementary_Data</label>
      <media xlink:href="btac678_supplementary_data.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <sec sec-type="data-availability">
    <title>Data Availability</title>
    <p>The data underlying this article are available in the article, in its online supplementary material and at the E-SNPs&amp;GO web site: <ext-link xlink:href="https://esnpsandgo.biocomp.unibo.it" ext-link-type="uri">https://esnpsandgo.biocomp.unibo.it</ext-link>.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btac678-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Adzhubei</surname><given-names>I.A.</given-names></string-name></person-group><etal>et al</etal> (<year>2010</year>) <article-title>A method and server for predicting damaging missense mutations</article-title>. <source>Nat. Methods</source>, <volume>7</volume>, <fpage>248</fpage>–<lpage>249</lpage>.<pub-id pub-id-type="pmid">20354512</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alley</surname><given-names>E.C.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Unified rational protein engineering with sequence-based deep representation learning</article-title>. <source>Nat. Methods</source>, <volume>16</volume>, <fpage>1315</fpage>–<lpage>1322</lpage>.<pub-id pub-id-type="pmid">31636460</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Amberger</surname><given-names>J.S.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>OMIM.org: leveraging knowledge across phenotype–gene relationships</article-title>. <source>Nucleic Acids Res</source>., <volume>47</volume>, <fpage>D1038</fpage>–<lpage>D1043</lpage>.<pub-id pub-id-type="pmid">30445645</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Asgari</surname><given-names>E.</given-names></string-name>, <string-name><surname>Mofrad</surname><given-names>M.R.K.</given-names></string-name></person-group> (<year>2015</year>) <article-title>Continuous distributed representation of biological sequences for deep proteomics and genomics</article-title>. <source>PLoS One</source>, <volume>10</volume>, <fpage>e0141287</fpage>.<pub-id pub-id-type="pmid">26555596</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ashburner</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2000</year>) <article-title>Gene ontology: tool for the unification of biology</article-title>. <source>Nat. Genet</source>., <volume>25</volume>, <fpage>25</fpage>–<lpage>29</lpage>.<pub-id pub-id-type="pmid">10802651</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benevenuta</surname><given-names>S.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Calibrating variant-scoring methods for clinical decision making</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>5709</fpage>–<lpage>5711</lpage>.<pub-id pub-id-type="pmid">33492342</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bepler</surname><given-names>T.</given-names></string-name>, <string-name><surname>Berger</surname><given-names>B.</given-names></string-name></person-group> (<year>2021</year>) <article-title>Learning the protein language: evolution, structure, and function</article-title>. <source>Cell Syst</source>., <volume>12</volume>, <fpage>654</fpage>–<lpage>669.e3</lpage>.<pub-id pub-id-type="pmid">34139171</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calabrese</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2009</year>) <article-title>Functional annotations improve the predictive score of human disease-related mutations in proteins</article-title>. <source>Hum. Mutat</source>., <volume>30</volume>, <fpage>1237</fpage>–<lpage>1244</lpage>.<pub-id pub-id-type="pmid">19514061</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carter</surname><given-names>H.</given-names></string-name></person-group><etal>et al</etal> (<year>2013</year>) <article-title>Identifying Mendelian disease genes with the variant effect scoring tool</article-title>. <source>BMC Genomics</source>, <volume>14 (Suppl. 3)</volume>, <fpage>S3</fpage>.</mixed-citation>
    </ref>
    <ref id="btac678-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>OWL2Vec: embedding of OWL ontologies</article-title>. <source>Mach. Learn</source>., <volume>110</volume>, <fpage>1813</fpage>–<lpage>1845</lpage>.</mixed-citation>
    </ref>
    <ref id="btac678-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Choi</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2012</year>) <article-title>Predicting the functional effect of amino acid substitutions and indels</article-title>. <source>PLoS One</source>, <volume>7</volume>, <fpage>e46688</fpage>.<pub-id pub-id-type="pmid">23056405</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dallago</surname><given-names>C.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Learned embeddings from deep learning to visualize and predict protein sets</article-title>. <source>Curr. Protoc</source>., <volume>1</volume>, <fpage>e113</fpage>.<pub-id pub-id-type="pmid">33961736</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Edera</surname><given-names>A.A.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Anc2vec: embedding gene ontology terms by preserving ancestors relationships</article-title>. <source>Brief. Bioinformatics</source>, <volume>23</volume>, <fpage>bbac003</fpage>.<pub-id pub-id-type="pmid">35136916</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Elnaggar</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>ProtTrans: towards cracking the language of life’s code through Self-Supervised deep learning and high performance computing</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>., <volume>14</volume>, <fpage>1</fpage>.</mixed-citation>
    </ref>
    <ref id="btac678-B15">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Grover</surname><given-names>A.</given-names></string-name>, <string-name><surname>Leskovec</surname><given-names>J.</given-names></string-name></person-group> (<year>2016</year>) node2vec: scalable feature learning for networks. In: <italic toggle="yes">Proceedings of the 22<sup>nd</sup> ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2016, San Francisco, CA, USA</italic>, pp. 855–864.</mixed-citation>
    </ref>
    <ref id="btac678-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heinzinger</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Modeling aspects of the language of life through transfer-learning protein sequences</article-title>. <source>BMC Bioinformatics</source>, <volume>20</volume>, <fpage>723</fpage>.<pub-id pub-id-type="pmid">31847804</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jagadeesh</surname><given-names>K.A.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) <article-title>M-CAP eliminates a majority of variants of uncertain significance in clinical exomes at high sensitivity</article-title>. <source>Nat. Genet</source>., <volume>48</volume>, <fpage>1581</fpage>–<lpage>1586</lpage>.<pub-id pub-id-type="pmid">27776117</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kandathil</surname><given-names>S.M.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Ultrafast end-to-end protein structure prediction enables high-throughput exploration of uncharacterized proteins</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>119</volume>, <fpage>e2113348119</fpage>.<pub-id pub-id-type="pmid">35074909</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Landrum</surname><given-names>M.J.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>ClinVar: improving access to variant interpretations and supporting evidence</article-title>. <source>Nucleic Acids Res</source>., <volume>46</volume>, <fpage>D1062</fpage>–<lpage>D1067</lpage>.<pub-id pub-id-type="pmid">29165669</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lappalainen</surname><given-names>T.</given-names></string-name>, <string-name><surname>MacArthur</surname><given-names>D.G.</given-names></string-name></person-group> (<year>2021</year>) <article-title>From variant to function in human disease genetics</article-title>. <source>Science</source>, <volume>373</volume>, <fpage>1464</fpage>–<lpage>1468</lpage>.<pub-id pub-id-type="pmid">34554789</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>B.</given-names></string-name></person-group><etal>et al</etal> (<year>2009</year>) <article-title>Automated inference of molecular mechanisms of disease from amino acid substitutions</article-title>. <source>Bioinformatics</source>, <volume>25</volume>, <fpage>2744</fpage>–<lpage>2750</lpage>.<pub-id pub-id-type="pmid">19734154</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Littmann</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Embeddings from deep learning transfer GO annotations beyond homology</article-title>. <source>Sci. Rep</source>., <volume>11</volume>, <fpage>1160</fpage>.<pub-id pub-id-type="pmid">33441905</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mahbub</surname><given-names>S.</given-names></string-name>, <string-name><surname>Bayzid</surname><given-names>M.S.</given-names></string-name></person-group> (<year>2022</year>) <article-title>EGRET: edge aggregated graph attention networks and transfer learning improve protein–protein interaction site prediction</article-title>. <source>Brief. Bioinformatics</source>, <volume>23</volume>, <fpage>bbab578</fpage>.<pub-id pub-id-type="pmid">35106547</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marquet</surname><given-names>C.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Embeddings from protein language models predict conservation and variant effects</article-title>. <source>Hum. Genet</source>., <volume>141</volume>, <fpage>1629</fpage>–<lpage>1647</lpage>.<pub-id pub-id-type="pmid">34967936</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B26">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Meier</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) Language models enable zero-shot prediction of the effects of mutations on protein function. In: <person-group person-group-type="editor"><string-name><surname>Ranzato</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (eds) <italic toggle="yes">Advances in Neural Information Processing Systems. Proceedings of NeurIPS 2021</italic>, Vol. <volume>34</volume>. pp. <fpage>29287</fpage>–<lpage>29303</lpage>.</mixed-citation>
    </ref>
    <ref id="btac678-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nair</surname><given-names>P.S.</given-names></string-name>, <string-name><surname>Vihinen</surname><given-names>M.</given-names></string-name></person-group> (<year>2013</year>) <article-title>VariBench: a benchmark database for variations</article-title>. <source>Hum. Mutat</source>., <volume>34</volume>, <fpage>42</fpage>–<lpage>49</lpage>.<pub-id pub-id-type="pmid">22903802</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ng</surname><given-names>P.C.</given-names></string-name>, <string-name><surname>Henikoff</surname><given-names>S.</given-names></string-name></person-group> (<year>2001</year>) <article-title>Predicting deleterious amino acid substitutions</article-title>. <source>Genome Res</source>., <volume>11</volume>, <fpage>863</fpage>–<lpage>874</lpage>.<pub-id pub-id-type="pmid">11337480</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B29">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Niculescu-Mizil</surname><given-names>A.</given-names></string-name>, <string-name><surname>Caruana</surname><given-names>R.</given-names></string-name></person-group> (<year>2005</year>) Predicting good probabilities with supervised learning. In: <italic toggle="yes">Proceedings of the 22nd International Conference on Machine Learning</italic>, ICML ’05. <publisher-name>Association for Computing Machinery</publisher-name>, <publisher-loc>New York, NY, USA</publisher-loc>, pp. <fpage>625</fpage>–<lpage>632</lpage>.</mixed-citation>
    </ref>
    <ref id="btac678-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Niroula</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2015</year>) <article-title>PON-P2: prediction method for fast and reliable identification of harmful variants</article-title>. <source>PLoS One</source>, <volume>10</volume>, <fpage>e0117380</fpage>.<pub-id pub-id-type="pmid">25647319</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ofer</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>The language of proteins: NLP, machine learning &amp; protein sequences</article-title>. <source>Comput. Struct. Biotechnol. J</source>., <volume>19</volume>, <fpage>1750</fpage>–<lpage>1758</lpage>.<pub-id pub-id-type="pmid">33897979</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pedregosa</surname><given-names>F.</given-names></string-name></person-group><etal>et al</etal> (<year>2011</year>) <article-title>Scikit-learn: machine learning in python</article-title>. <source>J. Mach. Learn. Res</source>., <volume>12</volume>, <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="btac678-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pejaver</surname><given-names>V.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Inferring the molecular and phenotypic impact of amino acid variants with MutPred2</article-title>. <source>Nat. Commun</source>., <volume>11</volume>, <fpage>5918</fpage>.<pub-id pub-id-type="pmid">33219223</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B34">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Perozzi</surname><given-names>B.</given-names></string-name></person-group><etal>et al</etal> (<year>2014</year>) DeepWalk: online learning of social representations. In: <italic toggle="yes">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, New York, NY, USA</italic>, pp. <fpage>701</fpage>–<lpage>710</lpage>.</mixed-citation>
    </ref>
    <ref id="btac678-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raimondi</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>DEOGEN2: prediction and interactive visualization of single amino acid variant deleteriousness in human proteins</article-title>. <source>Nucleic Acids Res</source>., <volume>45</volume>, <fpage>W201</fpage>–<lpage>W206</lpage>.<pub-id pub-id-type="pmid">28498993</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rives</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>118</volume>, <fpage>e2016239118</fpage>.<pub-id pub-id-type="pmid">33876751</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schwarz</surname><given-names>J.M.</given-names></string-name></person-group><etal>et al</etal> (<year>2010</year>) <article-title>MutationTaster evaluates disease-causing potential of sequence alterations</article-title>. <source>Nat. Methods</source>, <volume>7</volume>, <fpage>575</fpage>–<lpage>576</lpage>.<pub-id pub-id-type="pmid">20676075</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shefchek</surname><given-names>K.A.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>The monarch initiative in 2019: an integrative data and analytic platform connecting phenotypes to genotypes across species</article-title>. <source>Nucleic Acids Res</source>., <volume>48</volume>, <fpage>D704</fpage>–<lpage>D715</lpage>.<pub-id pub-id-type="pmid">31701156</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Singh</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>SPOT-Contact-LM: improving single-sequence-based prediction of protein contact map using a transformer language model</article-title>. <source>Bioinformatics</source>, <volume>38</volume>, <fpage>1888</fpage>–<lpage>1894</lpage>.<pub-id pub-id-type="pmid">35104320</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stärk</surname><given-names>H.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Light attention predicts protein location from the language of life</article-title>. <source>Bioinform. Adv</source>., <volume>1</volume>, <fpage>vbab035</fpage>.</mixed-citation>
    </ref>
    <ref id="btac678-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steinegger</surname><given-names>M.</given-names></string-name>, <string-name><surname>Söding</surname><given-names>J.</given-names></string-name></person-group> (<year>2017</year>) <article-title>MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</article-title>. <source>Nat. Biotechnol</source>., <volume>35</volume>, <fpage>1026</fpage>–<lpage>1028</lpage>.<pub-id pub-id-type="pmid">29035372</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steinegger</surname><given-names>M.</given-names></string-name>, <string-name><surname>Söding</surname><given-names>J.</given-names></string-name></person-group> (<year>2018</year>) <article-title>Clustering huge protein sequence sets in linear time</article-title>. <source>Nat. Commun</source>., <volume>9</volume>, <fpage>2542</fpage>.<pub-id pub-id-type="pmid">29959318</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steinegger</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Protein-level assembly increases protein sequence recovery from metagenomic samples manyfold</article-title>. <source>Nat. Methods</source>, <volume>16</volume>, <fpage>603</fpage>–<lpage>606</lpage>.<pub-id pub-id-type="pmid">31235882</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Strodthoff</surname><given-names>N.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>UDSMProt: universal deep sequence models for protein classification</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>2401</fpage>–<lpage>2409</lpage>.<pub-id pub-id-type="pmid">31913448</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Suzek</surname><given-names>B.E.</given-names></string-name></person-group><etal>et al</etal>; <collab>the UniProt Consortium</collab>. (<year>2015</year>) <article-title>UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches</article-title>. <source>Bioinformatics</source>, <volume>31</volume>, <fpage>926</fpage>–<lpage>932</lpage>.<pub-id pub-id-type="pmid">25398609</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teufel</surname><given-names>E.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>SignalP 6.0 predicts all five types of signal peptides using protein language models</article-title>. <source>Nat. Biotechnol</source>., <volume>40</volume>, <fpage>1023</fpage>–<lpage>1025</lpage>.<pub-id pub-id-type="pmid">34980915</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B47">
      <mixed-citation publication-type="journal"><collab>The UniProt Consortium</collab> (<year>2021</year>) <article-title>UniProt: the universal protein knowledgebase in 2021</article-title>. <source>Nucleic Acids Res</source>., <volume>49</volume>, <fpage>D480</fpage>–<lpage>D489</lpage>.<pub-id pub-id-type="pmid">33237286</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B48">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Vaswani</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) Attention is all you need. In: <italic toggle="yes">Proceedings of the 31<sup>st</sup> Annual Conference on Neural Information Processing Systems, NIPS 2017, Long Beach, CA, USA</italic>, pp. <fpage>5999</fpage>–<lpage>6009</lpage></mixed-citation>
    </ref>
    <ref id="btac678-B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vihinen</surname><given-names>M.</given-names></string-name></person-group> (<year>2021</year>) <article-title>Functional effects of protein variants</article-title>. <source>Biochimie</source>, <volume>180</volume>, <fpage>104</fpage>–<lpage>120</lpage>.<pub-id pub-id-type="pmid">33164889</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Walsh</surname><given-names>I.</given-names></string-name></person-group><etal>et al</etal>; <collab>ELIXIR Machine Learning Focus Group</collab>. (<year>2021</year>) <article-title>DOME: recommendations for supervised machine learning validation in biology</article-title>. <source>Nat. Methods</source>, <volume>18</volume>, <fpage>1122</fpage>–<lpage>1127</lpage>.<pub-id pub-id-type="pmid">34316068</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>PON-All, amino acid substitution tolerance predictor for all organisms</article-title>. <source>Front. Mol. Biosci</source>., <volume>9</volume>, <fpage>867572</fpage>.<pub-id pub-id-type="pmid">35782867</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhong</surname><given-names>X.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>GO2Vec: transforming GO terms and proteins to vector representations via graph embeddings</article-title>. <source>BMC Genomics</source>, <volume>20</volume>, <fpage>918</fpage>.<pub-id pub-id-type="pmid">31874639</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9710551</article-id>
    <article-id pub-id-type="pmid">36227117</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btac678</article-id>
    <article-id pub-id-type="publisher-id">btac678</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Sequence Analysis</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>E-SNPs&amp;GO: embedding of protein sequence and function improves the annotation of human pathogenic variants</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Manfredi</surname>
          <given-names>Matteo</given-names>
        </name>
        <xref rid="btac678-FM1" ref-type="author-notes"/>
        <aff><institution>Biocomputing Group, Department of Pharmacy and Biotechnology, University of Bologna</institution>, Bologna 40126, <country country="IT">Italy</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-7359-0633</contrib-id>
        <name>
          <surname>Savojardo</surname>
          <given-names>Castrense</given-names>
        </name>
        <xref rid="btac678-FM1" ref-type="author-notes"/>
        <aff><institution>Biocomputing Group, Department of Pharmacy and Biotechnology, University of Bologna</institution>, Bologna 40126, <country country="IT">Italy</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0274-5669</contrib-id>
        <name>
          <surname>Martelli</surname>
          <given-names>Pier Luigi</given-names>
        </name>
        <xref rid="btac678-cor1" ref-type="corresp"/>
        <!--pierluigi.martelli@unibo.it-->
        <aff><institution>Biocomputing Group, Department of Pharmacy and Biotechnology, University of Bologna</institution>, Bologna 40126, <country country="IT">Italy</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-7462-7039</contrib-id>
        <name>
          <surname>Casadio</surname>
          <given-names>Rita</given-names>
        </name>
        <aff><institution>Biocomputing Group, Department of Pharmacy and Biotechnology, University of Bologna</institution>, Bologna 40126, <country country="IT">Italy</country></aff>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Boeva</surname>
          <given-names>Valentina</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btac678-cor1">To whom correspondence should be addressed. Email: <email>pierluigi.martelli@unibo.it</email></corresp>
      <fn id="btac678-FM1">
        <p>The authors wish it to be known that, in their opinion, Matteo Manfredi and Castrense Savojardo should be regarded as Joint First Authors.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="collection">
      <day>01</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-10-13">
      <day>13</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <volume>38</volume>
    <issue>23</issue>
    <fpage>5168</fpage>
    <lpage>5174</lpage>
    <history>
      <date date-type="received">
        <day>16</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>14</day>
        <month>9</month>
        <year>2022</year>
      </date>
      <date date-type="editorial-decision">
        <day>06</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>10</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="corrected-typeset">
        <day>26</day>
        <month>10</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btac678.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>The advent of massive DNA sequencing technologies is producing a huge number of human single-nucleotide polymorphisms occurring in protein-coding regions and possibly changing their sequences. Discriminating harmful protein variations from neutral ones is one of the crucial challenges in precision medicine. Computational tools based on artificial intelligence provide models for protein sequence encoding, bypassing database searches for evolutionary information. We leverage the new encoding schemes for an efficient annotation of protein variants.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>E-SNPs&amp;GO is a novel method that, given an input protein sequence and a single amino acid variation, can predict whether the variation is related to diseases or not. The proposed method adopts an input encoding completely based on protein language models and embedding techniques, specifically devised to encode protein sequences and GO functional annotations. We trained our model on a newly generated dataset of 101 146 human protein single amino acid variants in 13 661 proteins, derived from public resources. When tested on a blind set comprising 10 266 variants, our method well compares to recent approaches released in literature for the same task, reaching a Matthews Correlation Coefficient score of 0.72. We propose E-SNPs&amp;GO as a suitable, efficient and accurate large-scale annotator of protein variant datasets.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The method is available as a webserver at <ext-link xlink:href="https://esnpsandgo.biocomp.unibo.it" ext-link-type="uri">https://esnpsandgo.biocomp.unibo.it</ext-link>. Datasets and predictions are available at <ext-link xlink:href="https://esnpsandgo.biocomp.unibo.it/datasets" ext-link-type="uri">https://esnpsandgo.biocomp.unibo.it/datasets</ext-link>.</p>
      </sec>
      <sec id="s4a">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>PRIN 2017</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2017483NH8</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Italian Ministry of University and Research</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="7"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Single-nucleotide polymorphisms (SNPs) are major sources of human evolution. In many cases, these variations can be directly associated with the onset of genetic diseases. Specifically, SNPs occurring in protein-coding regions often lead to observable changes in the protein residue sequence. Single amino acid variations (SAVs) may have an impact at different levels, hampering protein structure, function, stability, localization and interaction with other proteins and/or nucleotides, hence setting the basis for the onset of pathologic conditions (<xref rid="btac678-B21" ref-type="bibr">Lappalainen and MacArthur, 2021</xref>; <xref rid="btac678-B50" ref-type="bibr">Vihinen, 2021</xref> and references therein).</p>
    <p>Public databases, such as HUMSAVAR (<xref rid="btac678-B47" ref-type="bibr">The UniProt Consortium, 2021</xref>) and ClinVar (<xref rid="btac678-B20" ref-type="bibr">Landrum <italic toggle="yes">et al.</italic>, 2018</xref>), store a compendium of known SAVs and provide, when available, information about the variant clinical significance. However, clear associations to diseases are still unknown for many SAVs, which substantially remain of Uncertain Significance (US). Therefore, SAV annotation is an issue, and effective computational tools are needed to provide large-scale annotation of uncharacterized human variation data.</p>
    <p>In the past years, several computational approaches have been implemented, with the aim of annotating whether a protein variation is or not disease associated (<xref rid="btac678-B1" ref-type="bibr">Adzhubei <italic toggle="yes">et al.</italic>, 2010</xref>; <xref rid="btac678-B8" ref-type="bibr">Calabrese <italic toggle="yes">et al.</italic>, 2009</xref>; <xref rid="btac678-B9" ref-type="bibr">Carter <italic toggle="yes">et al.</italic>, 2013</xref>; <xref rid="btac678-B11" ref-type="bibr">Choi <italic toggle="yes">et al.</italic>, 2012</xref>; <xref rid="btac678-B18" ref-type="bibr">Jagadeesh <italic toggle="yes">et al.</italic>, 2016</xref>; <xref rid="btac678-B22" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2009</xref>; <xref rid="btac678-B28" ref-type="bibr">Ng and Henikoff, 2001</xref>; <xref rid="btac678-B30" ref-type="bibr">Niroula <italic toggle="yes">et al.</italic>, 2015</xref>; <xref rid="btac678-B33" ref-type="bibr">Pejaver <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac678-B35" ref-type="bibr">Raimondi <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btac678-B37" ref-type="bibr">Schwarz <italic toggle="yes">et al.</italic>, 2010</xref>; <xref rid="btac678-B52" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2022</xref>). Methods like SIFT (<xref rid="btac678-B28" ref-type="bibr">Ng and Henikoff, 2001</xref>) or PROVEAN (<xref rid="btac678-B11" ref-type="bibr">Choi <italic toggle="yes">et al.</italic>, 2012</xref>) are based on the conservation analysis in multiple sequence alignments. More complex approaches stand on different types of machine-learning frameworks. These include neural networks (<xref rid="btac678-B33" ref-type="bibr">Pejaver <italic toggle="yes">et al.</italic>, 2020</xref>), random forests (<xref rid="btac678-B9" ref-type="bibr">Carter <italic toggle="yes">et al.</italic>, 2013</xref>; <xref rid="btac678-B22" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2009</xref>; <xref rid="btac678-B30" ref-type="bibr">Niroula <italic toggle="yes">et al.</italic>, 2015</xref>; <xref rid="btac678-B35" ref-type="bibr">Raimondi <italic toggle="yes">et al.</italic>, 2017</xref>), gradient tree boosting (<xref rid="btac678-B18" ref-type="bibr">Jagadeesh <italic toggle="yes">et al.</italic>, 2016</xref>; <xref rid="btac678-B52" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2022</xref>), support vector machines (SVMs) (<xref rid="btac678-B8" ref-type="bibr">Calabrese <italic toggle="yes">et al.</italic>, 2009</xref>) and naive Bayes classifiers (<xref rid="btac678-B1" ref-type="bibr">Adzhubei <italic toggle="yes">et al.</italic>, 2010</xref>; <xref rid="btac678-B37" ref-type="bibr">Schwarz <italic toggle="yes">et al.</italic>, 2010</xref>). Each method is trained/tested on different datasets of SAVs, either extracted directly from public resources like HUMSAVAR (<xref rid="btac678-B47" ref-type="bibr">The UniProt Consortium, 2021</xref>) and/or ClinVar (<xref rid="btac678-B20" ref-type="bibr">Landrum <italic toggle="yes">et al.</italic>, 2018</xref>), or taking advantage of pre-compiled datasets of variations, like VariBench (<xref rid="btac678-B27" ref-type="bibr">Nair and Vihinen, 2013</xref>). Different types of descriptors extract salient features of the protein sequence and/or the local sequence context surrounding the variant position, including physicochemical properties, sequence profiles, conservation scores, predicted structural motifs and functional annotations. SNPs&amp;GO (<xref rid="btac678-B8" ref-type="bibr">Calabrese <italic toggle="yes">et al.</italic>, 2009</xref>) firstly recognized the importance of functional annotations for the prediction of variant pathogenicity and introduced the LGO feature, a score of association between Gene Ontology (GO) (Ashburner <italic toggle="yes">et al.</italic>, 2000) annotations and the variant pathogenicity. The incorporation of the LGO feature significantly improved the prediction performance of SNPs&amp;GO (<xref rid="btac678-B8" ref-type="bibr">Calabrese <italic toggle="yes">et al.</italic>, 2009</xref>).</p>
    <p>Recent developments in the field of deep learning focus on the definition of new ways of representing protein sequences. Large-scale protein language models (PLMs) are inspired and derived from the natural language processing (NLP) field (<xref rid="btac678-B31" ref-type="bibr">Ofer <italic toggle="yes">et al</italic>, 2021</xref>). They learn numerical vector representations of protein sequences, containing important features that are reflected in the evolutionary conservation and in the sequence syntax (<xref rid="btac678-B7" ref-type="bibr">Bepler and Berger, 2021</xref>). These numerical vectors are then adopted to encode protein sequence and/or individual residues in place of canonical, hand-crafted features, such as physicochemical properties or evolutionary information. These distributed protein representations emerge from the application of learning models trained on large databases of sequence data (<xref rid="btac678-B7" ref-type="bibr">Bepler and Berger, 2021</xref>; <xref rid="btac678-B31" ref-type="bibr">Ofer <italic toggle="yes">et al.</italic>, 2021</xref>).</p>
    <p>Successful PLMs are routinely trained on databases composed of hundreds of millions of unique sequences with hundreds of billions of residues. Training is computationally demanding, routinely requiring weeks or months of computations on high-performance Tensor Processing Units (TPUs) and/or Graphical Processing Units (GPUs) (<xref rid="btac678-B14" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac678-B36" ref-type="bibr">Rives <italic toggle="yes">et al.</italic>, 2021</xref>). However, the advantage is that most of the computational cost is concentrated on the training phase, and once models are trained they can be adopted to embed new sequences with limited resources in terms of time, memory and computational power.</p>
    <p>Embeddings obtained with language models have been recently employed for many different applications with great success, including the prediction of protein function and localization (<xref rid="btac678-B23" ref-type="bibr">Littmann <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac678-B40" ref-type="bibr">Stärk <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac678-B46" ref-type="bibr">Teufel <italic toggle="yes">et al.</italic> 2022</xref>), of protein contact maps (<xref rid="btac678-B39" ref-type="bibr">Singh <italic toggle="yes">et al.</italic>, 2022</xref>) and binding sites (<xref rid="btac678-B24" ref-type="bibr">Mahbub and Bayzid, 2022</xref>).</p>
    <p>Several pre-trained language models currently exist in the literature (<xref rid="btac678-B2" ref-type="bibr">Alley <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac678-B4" ref-type="bibr">Asgari and Mofrad, 2015</xref>; <xref rid="btac678-B14" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac678-B16" ref-type="bibr">Heinzinger <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac678-B36" ref-type="bibr">Rives <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac678-B44" ref-type="bibr">Strodthoff <italic toggle="yes">et al.</italic>, 2020</xref>), mainly differing in their specific architectures [autoregressive, bidirectional, masked; see for review <xref rid="btac678-B7" ref-type="bibr">Bepler and Berger (2021)</xref>] and in the datasets adopted for training.</p>
    <p>Not limited to the encoding of protein sequence data, embedding techniques are also applied to model the relationships existing within more complex structures, such as graphs, networks, or biological ontologies (<xref rid="btac678-B13" ref-type="bibr">Edera <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btac678-B15" ref-type="bibr">Grover and Leskovec, 2016</xref>; <xref rid="btac678-B19" ref-type="bibr">Kandathil <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btac678-B34" ref-type="bibr">Perozzi <italic toggle="yes">et al.</italic>, 2014</xref>; <xref rid="btac678-B53" ref-type="bibr">Zhong <italic toggle="yes">et al.</italic>, 2019</xref>).</p>
    <p>In this article, we attempt to fully exploit the power of language models and embeddings for the prediction of variant pathogenicity from the human protein sequence. On the methodological side, two major contributions can be highlighted. Firstly, we adopt two different and complementary embedding procedures, ProtT5 (<xref rid="btac678-B14" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>) and ESM-1v (<xref rid="btac678-B26" ref-type="bibr">Meier <italic toggle="yes">et al.</italic>, 2021</xref>), to directly encode an input variation without introducing any hand-crafted feature as previously done. Secondly, leveraging the idea introduced in SNPs&amp;GO (<xref rid="btac678-B8" ref-type="bibr">Calabrese <italic toggle="yes">et al.</italic>, 2009</xref>), we explore a new way of encoding functional annotations by adopting a model called Anc2Vec (<xref rid="btac678-B13" ref-type="bibr">Edera <italic toggle="yes">et al.</italic>, 2022</xref>), specifically designed for the embedding of GO terms (<xref rid="btac678-B5" ref-type="bibr">Ashburner <italic toggle="yes">et al.</italic>, 2000</xref>).</p>
    <p>We trained an SVM using the above input encoding on a newly generated dataset of 101 146 human disease-related and benign variations obtained from the rational merging of data deposited in two databases, HUMSAVAR (<xref rid="btac678-B47" ref-type="bibr">The UniProt Consortium, 2021</xref>) and ClinVar (<xref rid="btac678-B20" ref-type="bibr">Landrum <italic toggle="yes">et al.</italic>, 2018</xref>). The method is tested on an independent, non-redundant blind set comprising 10 266 variations, adopting stringent homology reduction and evaluation procedures. Results obtained in a comparative benchmark and including one of the most recent and effective methods (<xref rid="btac678-B33" ref-type="bibr">Pejaver <italic toggle="yes">et al.</italic>, 2020</xref>), demonstrate that our model performs at the level or even better than the state-of-the-art (when available for comparison) reaching a Matthews Correlation Coefficient (MCC) of 0.72. Based on an input encoding derived solely from embedding models, our method is fast: this makes it suitable for large-scale annotation of human pathogenic variants.</p>
    <p>We release our tool as a webserver at <ext-link xlink:href="https://esnpsandgo.biocomp.unibo.it" ext-link-type="uri">https://esnpsandgo.biocomp.unibo.it</ext-link>.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Dataset</title>
      <p>We obtained the dataset of SAVs by merging information extracted from two resources: HUMSAVAR (accessed on August 4, 2021), listing all missense variants annotated in human UniProt/SwissProt (<xref rid="btac678-B47" ref-type="bibr">The UniProt Consortium, 2021</xref>) entries, and ClinVar (accessed on March 29, 2021), the NCBI resource of relationships among human variations and disease phenotypes (<xref rid="btac678-B20" ref-type="bibr">Landrum <italic toggle="yes">et al.</italic>, 2018</xref>).</p>
      <p>Both databases classify the effect of SAVs into different classes: Pathogenic or Likely Pathogenic (P/LP), Benign or Likely Benign (B/LB) and of US. We retained only P/LP SAVs clearly associated with the diseases catalogued in OMIM (<xref rid="btac678-B3" ref-type="bibr">Amberger <italic toggle="yes">et al.</italic>, 2019</xref>) or in MONDO (<xref rid="btac678-B38" ref-type="bibr">Shefchek <italic toggle="yes">et al.</italic>, 2020</xref>). We collected also all the B/LB variations and excluded SAVs labelled as US, somatic, or with contrasting annotations of the effect.</p>
      <p>Overall, the dataset consists of 13 661 protein sequences endowed with 111 412 SAVs, including 43 895 P/LP SAVs in 3603 proteins and 67 517 B/LB SAVs in 13 229 proteins (<xref rid="btac678-T1" ref-type="table">Table 1</xref>, last row).</p>
      <table-wrap position="float" id="btac678-T1">
        <label>Table 1.</label>
        <caption>
          <p>The dataset of SAVs adopted in this study</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Dataset</th>
              <th rowspan="1" colspan="1">No. of pathogenic SAVs</th>
              <th rowspan="1" colspan="1">No. of neutral SAVs</th>
              <th rowspan="1" colspan="1">No. of proteins</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Training set</td>
              <td align="char" char="." rowspan="1" colspan="1">39 812</td>
              <td align="char" char="." rowspan="1" colspan="1">61 334</td>
              <td align="char" char="." rowspan="1" colspan="1">12 347</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Blind test set</td>
              <td align="char" char="." rowspan="1" colspan="1">4083</td>
              <td align="char" char="." rowspan="1" colspan="1">6183</td>
              <td align="char" char="." rowspan="1" colspan="1">1314</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Total</td>
              <td align="char" char="." rowspan="1" colspan="1">43 895</td>
              <td align="char" char="." rowspan="1" colspan="1">67 517</td>
              <td align="char" char="." rowspan="1" colspan="1">13 661</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>For all proteins in the dataset, we extracted GO (Ashburner <italic toggle="yes">et al.</italic>, 2000) annotations from the corresponding entry in UniProt. Overall, our dataset is annotated with 17 076 GO terms, including 11 476 Biological Process (BP), 3955 Molecular Function (MF) and 1645 Cellular Component (CC). The complete dataset is available at <ext-link xlink:href="https://esnpsandgo.biocomp.unibo.it/datasets" ext-link-type="uri">https://esnpsandgo.biocomp.unibo.it/datasets</ext-link>.</p>
      <sec>
        <label>2.1.1</label>
        <title>Cross-validation procedure and generation of the blind test set</title>
        <p>To avoid biases between training and testing sets, we adopted a stringent clustering procedure to generate cross-validation sets. Firstly, we clustered protein sequences with the MMseqs2 program (<xref rid="btac678-B41" ref-type="bibr">Steinegger and Söding, 2017</xref>), by constraining a minimum sequence identity of 25% over a pairwise alignment coverage of at least 40%. We used a connected component clustering strategy so that if two proteins are clustered with a third one, they both end up in the same set. In this way, we limit sequence redundancy between training and testing sets, enabling a fair evaluation of the results. We selected 10% of the data to construct the blind test set for assessing the generalization performance of our approach and for benchmarking it with other popular methods available. The remaining 90% of the dataset was further split into 10 equally distributed subsets that were used in a 10-fold cross-validation procedure for optimizing the input encoding and for fixing the model hyperparameters. We also tried a 20–80% split (20% of the data for the blind test set and 80% for training with the 10-fold cross-validation procedure) and obtained a very similar performance. For this reason, we list results corresponding to the 10% blind test. When performing cross-validation, we took care of preserving the balancing of positive and negative examples in each subset (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>).</p>
        <p>It is worth noticing that the blind test can share similarity with proteins included in the training sets of the other benchmarked methods.</p>
      </sec>
    </sec>
    <sec>
      <title>2.2 General overview of the approach</title>
      <p><xref rid="btac678-F1" ref-type="fig">Figure 1</xref> depicts the architecture of E-SNPs&amp;GO, including three major blocks: an Input encoding, a Predictor and an Output. The input consists of a human protein sequence and a SAV occurring at a specific position along the sequence. In the input encoding phase, the sequence and its variant are embedded with two different procedures, ESM-1v (<xref rid="btac678-B26" ref-type="bibr">Meier <italic toggle="yes">et al.</italic>, 2021</xref>) and ProtT5 (<xref rid="btac678-B14" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>), generating for each sequence 1280 and 1024 features, respectively. In order to embed the functional protein annotation of the wild-type protein, we adopt Anc2Vec (<xref rid="btac678-B13" ref-type="bibr">Edera <italic toggle="yes">et al.</italic>, 2022</xref>), computing three sets of 200 features corresponding to the different subontologies.</p>
      <fig position="float" id="btac678-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>General overview of the architecture of E-SNPs&amp;GO. Inputs (wild-type sequence, variation and variation position) are in yellow. The architecture includes three major blocks: an Input encoding, a Predictor and an Output. During the Input encoding, three embedding models are adopted to generate vector representations. The wild-type sequence (green) and the variant sequence (red) are modelled with ESM-1v (<xref rid="btac678-B26" ref-type="bibr">Meier <italic toggle="yes">et al.</italic>, 2021</xref>) and ProtT5 (<xref rid="btac678-B14" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>). The GO functional annotations (blue MF, purple CC and pink BP) are modelled with Anc2Vec (<xref rid="btac678-B13" ref-type="bibr">Edera <italic toggle="yes">et al.</italic>, 2022</xref>). The vectors within the dashed box (marked with different colors), representing the variation position and the averaged (Avg) GO terms of the wild-type sequence, are then concatenated together to obtain a final representation consisting of 1280×2 + 1024×2 + 200×3 = 5208 features. This vector is fed to the Predictor, which includes a PCA to reduce the input dimensionality (from 5208 to 2400) and a SVM providing as a final output a binary classification into B/LB (negative class, Score &lt;0) or P/LP (positive class, Score ≥0). We apply an Isotonic Regression (Calibration) to obtain a calibrated probability (A color version of this figure appears in the online version of this article.)</p>
        </caption>
        <graphic xlink:href="btac678f1" position="float"/>
      </fig>
      <p>In the predictor, the vector representation generated in the input encoding is then processed using a principal component analysis (PCA), which reduces the dimensionality of the input from 5208 features to 2400. The output feeds a SVM classifier performing the final labelling as Pathogenic (P/LP) or Benign (B/LB). A given input variant is predicted as pathogenic when the SVM output score ≥0, benign otherwise. A final calibration step allows to convert scores into probabilities for a variant to be pathogenic. Details of the methods included in E-SNPs&amp;GO, are listed in the following sections.</p>
    </sec>
    <sec>
      <title>2.3 Input encoding: embeddings of protein sequence, its variant and GO terms</title>
      <sec>
        <label>2.3.1</label>
        <title>Transformers for embedding of protein sequences and their variants</title>
        <p>Several prominent language models and corresponding embedding generation schemes in NLP are available, and some of these have been adapted to protein sequences to perform specific prediction tasks (<xref rid="btac678-B7" ref-type="bibr">Bepler and Berger, 2021</xref>). Large-scale PLMs aim at learning a numerical vector representation that allows reconstructing the input sequence.</p>
        <p>Among PLMs, transformer-based models (<xref rid="btac678-B48" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic>, 2017</xref>) aim to solve the problem of efficiently capturing long-distance interactions in the sequence. Transformers are architectures that include a self-attention mechanism to extract the context information from the whole sequence (<xref rid="btac678-B48" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic>, 2017</xref>). In general, a transformer language model builds on top of an encoder–decoder architecture. However, the different transformer-based PLMs only utilize either the encoder or the decoder part. In this respect, transformer-based PLMs can be classified in three different categories: (i) encoder-only models use only the encoder part of the transformer accessing the whole input sequence and are trained to reconstruct a somewhat corrupted version of the input (e.g. masking random positions along the sequence); (ii) decoder-only models (also called autoregressive models) use only the decoder part accessing, at each position, all the residues placed before the current one in the sequence and are usually trained to predict the next residue in the sequence; (iii) sequence-to-sequence models use both the encoder and the decoder and are trained to reconstruct a masked input sequence (<xref rid="btac678-B48" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic>, 2017</xref>).</p>
        <p>The learned representation captures important features of the proteins, including physicochemical, structural, functional and evolutionary features (<xref rid="btac678-B7" ref-type="bibr">Bepler and Berger, 2021</xref>; <xref rid="btac678-B31" ref-type="bibr">Ofer <italic toggle="yes">et al.</italic>, 2021</xref>). By transfer learning, the embedded schemes are provided as input to Predictor block (<xref rid="btac678-F1" ref-type="fig">Fig. 1</xref>).</p>
        <p>In this article, we adopt two different protein embedding schemes, based on two different transformers models: ESM-1v (<xref rid="btac678-B26" ref-type="bibr">Meier <italic toggle="yes">et al.</italic>, 2021</xref>), an encoder-only model, and ProtT5 (<xref rid="btac678-B14" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>), a sequence-to-sequence model. The major difference stands in the volume of the sequence datasets used for generating the embedding schemes and in the adoption of different training procedures. ESM-1v was trained on a single run using a dataset of 98 million unique sequences extracted from UniRef90 (<xref rid="btac678-B45" ref-type="bibr">Suzek <italic toggle="yes">et al.</italic>, 2015</xref>). ESM-1v releases five models generated by training with five different random seeds (<xref rid="btac678-B26" ref-type="bibr">Meier <italic toggle="yes">et al.</italic>, 2021</xref>). Apparently, only a small difference in performance is obtained when the ensemble is compared to a single model (<xref rid="btac678-B26" ref-type="bibr">Meier <italic toggle="yes">et al.</italic>, 2021</xref>). Therefore, to reduce the computational cost, we adopted only one model (the first one). ProtT5 (version XL U50) was trained using a two-step procedure: in a first pass, training was performed using the large BFD database (<xref rid="btac678-B43" ref-type="bibr">Steinegger <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac678-B42" ref-type="bibr">Steinegger and Söding, 2018</xref>), comprising the whole UniProt as well as protein sequences translated from multiple metagenomic sequencing projects, and consisting of about 2.1 billion unique sequences. In the second pass, a fine-tuning of the model was obtained using a smaller database derived from UniRef50 (<xref rid="btac678-B45" ref-type="bibr">Suzek <italic toggle="yes">et al.</italic>, 2015</xref>) and including 45 million unique sequences.</p>
      </sec>
      <sec>
        <label>2.3.2</label>
        <title>Embedding of biological ontologies</title>
        <p>The concept of embedding can be generalized to any kind of data with different underlying structures, such as graphs or networks (<xref rid="btac678-B15" ref-type="bibr">Grover and Leskovec, 2016</xref>; <xref rid="btac678-B34" ref-type="bibr">Perozzi <italic toggle="yes">et al.</italic>, 2014</xref>). In particular, several embedding models have been defined to provide a numerical representation of nodes in ontologies (<xref rid="btac678-B10" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac678-B53" ref-type="bibr">Zhong <italic toggle="yes">et al.</italic>, 2019</xref>). Here, we adopt Anc2Vec (<xref rid="btac678-B13" ref-type="bibr">Edera <italic toggle="yes">et al.</italic>, 2022</xref>), a method that learns a vector representation for GO terms, by preserving ancestor relationships.</p>
        <p>Because the embedding is not context-dependent, we precompute the vector representation for each possible GO term.</p>
      </sec>
    </sec>
    <sec>
      <title>2.4 Predictor</title>
      <sec>
        <label>2.4.1</label>
        <title>Predictor input</title>
        <p>For encoding variations, we firstly perform a full-sequence generation of embeddings using both the ESM-1v (<xref rid="btac678-B26" ref-type="bibr">Meier <italic toggle="yes">et al.</italic>, 2021</xref>) and the ProtT5 XL U50 (<xref rid="btac678-B14" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>) models. Given a protein sequence with <italic toggle="yes">L</italic> residues, this provides protein encodings of dimensions <italic toggle="yes">L</italic>×1280 and <italic toggle="yes">L</italic>×1024, respectively. Sequence embeddings are carried out independently on both the wild-type and the variant sequence.</p>
        <p>For a variation at position <italic toggle="yes">i</italic> in a protein sequence, we compute a vector of 4608 features, including:
</p>
        <list list-type="bullet">
          <list-item>
            <p>1280 features corresponding to ESM-1v embedding in position <italic toggle="yes">i</italic> of the variated sequence.</p>
          </list-item>
          <list-item>
            <p>1280 features corresponding to ESM-1v embedding in position <italic toggle="yes">i</italic> of the wild-type sequence.</p>
          </list-item>
          <list-item>
            <p>1024 features corresponding to ProtT5 (version XL U50) embedding in position <italic toggle="yes">i</italic> of the variated sequence.</p>
          </list-item>
          <list-item>
            <p>1024 features corresponding to ProtT5 (version XL U50) embedding in position <italic toggle="yes">i</italic> of the wild-type sequence.</p>
          </list-item>
        </list>
        <p>The ESM-1v embedding model constrains the maximal protein length (<italic toggle="yes">L</italic>) to 1024 residues. For this reason, variations occurring on longer sequences were encoded using a 201 long sequence window centred on the variant position.</p>
        <p>After this step, we extract all the GO terms annotated in the UniProt entry of the wild-type protein carrying the variation. Potential term redundancy is removed by retaining only leaf terms. Terms from the three different GO sub-ontologies (MF, CC and BP) are processed independently. Each annotated GO term is then embedded as a vector of 200 features using the Anc2Vec model (<xref rid="btac678-B13" ref-type="bibr">Edera <italic toggle="yes">et al.</italic>, 2022</xref>). To obtain a single vector representation independent of the number of terms of a given protein, we average all the vector encodings (<xref rid="btac678-F1" ref-type="fig">Fig. 1</xref>). Three final average vectors, one for each GO sub-ontology, are concatenated obtaining a protein function encoding of 600 components.</p>
        <p>The final variation encoding comprises 5208 features, obtained by merging the local positional embedding (4608 features from ESM-1V + ProtT5 XL U50) described above and the Anc2Vec functional encoding (600 features). Eventually, we encode the different embeddings separately (see Section 3 and <xref rid="btac678-T2" ref-type="table">Table 2</xref>).</p>
        <table-wrap position="float" id="btac678-T2">
          <label>Table 2.</label>
          <caption>
            <p>Performance of different embedding schemes</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Input encoding</th>
                <th rowspan="1" colspan="1"><italic toggle="yes">Q</italic><sub>2</sub> (%)</th>
                <th rowspan="1" colspan="1">Precision (%)</th>
                <th rowspan="1" colspan="1">Recall (%)</th>
                <th rowspan="1" colspan="1"><italic toggle="yes">F</italic>1-score (%)</th>
                <th rowspan="1" colspan="1">ROC-AUC (%)</th>
                <th rowspan="1" colspan="1">MCC</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">ESM-1v</td>
                <td rowspan="1" colspan="1">82.4 (±1.5)</td>
                <td rowspan="1" colspan="1">80.4 (±2.6)</td>
                <td rowspan="1" colspan="1">77.0 (±2.8)</td>
                <td rowspan="1" colspan="1">78.6 (±1.9)</td>
                <td rowspan="1" colspan="1">81.6 (±1.5)</td>
                <td rowspan="1" colspan="1">0.64 (±0.03)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ESM-1v+GO</td>
                <td rowspan="1" colspan="1">83.3 (±1.4)</td>
                <td rowspan="1" colspan="1">81.7 (±2.5)</td>
                <td rowspan="1" colspan="1">78.1 (±2.7)</td>
                <td rowspan="1" colspan="1">79.8 (±1.8)</td>
                <td rowspan="1" colspan="1">82.6 (±1.4)</td>
                <td rowspan="1" colspan="1">0.66 (±0.03)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ProtT5</td>
                <td rowspan="1" colspan="1">83.0 (±1.3)</td>
                <td rowspan="1" colspan="1">79.8 (±1.9)</td>
                <td rowspan="1" colspan="1">80.0 (±2.8)</td>
                <td rowspan="1" colspan="1">79.9 (±1.7)</td>
                <td rowspan="1" colspan="1">82.6 (±1.4)</td>
                <td rowspan="1" colspan="1">0.65 (±0.03)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ProtT5+GO</td>
                <td rowspan="1" colspan="1">83.7 (±1.1)</td>
                <td rowspan="1" colspan="1">81.8 (±1.9)</td>
                <td rowspan="1" colspan="1">79.2 (±2.5)</td>
                <td rowspan="1" colspan="1">80.5 (±1.5)</td>
                <td rowspan="1" colspan="1">83.1 (±1.3)</td>
                <td rowspan="1" colspan="1">0.67 (±0.02)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ESM-1v+ProtT5</td>
                <td rowspan="1" colspan="1">83.6 (±1.4)</td>
                <td rowspan="1" colspan="1">81.8 (±2.3)</td>
                <td rowspan="1" colspan="1">78.6 (±2.9)</td>
                <td rowspan="1" colspan="1">80.1 (±1.8)</td>
                <td rowspan="1" colspan="1">82.9 (±1.5)</td>
                <td rowspan="1" colspan="1">0.66 (±0.03)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ESM-1v+ProtT5+GO(-PCA)</td>
                <td rowspan="1" colspan="1">83.1 (±0.8)</td>
                <td rowspan="1" colspan="1">81.0 (±1.4)</td>
                <td rowspan="1" colspan="1">78.0 (±1.5)</td>
                <td rowspan="1" colspan="1">79.4 (±1.1)</td>
                <td rowspan="1" colspan="1">82.8 (±0.8)</td>
                <td rowspan="1" colspan="1">0.66 (±0.02)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ESM-1v+ProtT5+GO(+PCA)</td>
                <td rowspan="1" colspan="1">85.1 (±0.9)</td>
                <td rowspan="1" colspan="1">82.4 (±1.7)</td>
                <td rowspan="1" colspan="1">79.1 (±1.7)</td>
                <td rowspan="1" colspan="1">80.7 (±1.1)</td>
                <td rowspan="1" colspan="1">84.1 (±0.9)</td>
                <td rowspan="1" colspan="1">0.69 (±0.02)</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn1">
              <p><italic toggle="yes">Note</italic>: We adopted a 10-fold cross-validation on a training set comprising 101 146 human variations (<xref rid="btac678-T1" ref-type="table">Table 1</xref>) for testing the effect of different input encodings on the performances of the method. Standard deviation (between brackets) is computed over the 10 cross-validation sets and scoring indexes (defined in Section 2.6) are average values.</p>
            </fn>
            <fn id="tblfn2">
              <p>ESM-1v (2×1280 = 2560 features).</p>
            </fn>
            <fn id="tblfn3">
              <p>ESM-1v + GO (2×1280 + 3×200 = 3160 features).</p>
            </fn>
            <fn id="tblfn4">
              <p>ProtT5 (2×1024 = 2048 features).</p>
            </fn>
            <fn id="tblfn5">
              <p>ProtT5 + GO (2×1024 + 3×200 = 2648 features).</p>
            </fn>
            <fn id="tblfn6">
              <p>ESM-1v + ProtT5 (2×1280 + 2×1024 = 4608 features).</p>
            </fn>
            <fn id="tblfn7">
              <p>ESM-1v + ProtT5 + GO (−PCA) (2×1280 + 2×1024 + 3×200 = 5208 features), no PCA used.</p>
            </fn>
            <fn id="tblfn8">
              <p>ESM-1v + ProtT5 + GO (+PCA) (2×1280 + 2×1024 + 3×200 = 5208 features), PCA used to reduce dimensionality.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec>
        <label>2.4.2</label>
        <title>Model selection and implementation</title>
        <p>The predictor includes two cascading components (<xref rid="btac678-F1" ref-type="fig">Fig. 1</xref>): a PCA for reducing the dimensionality of the input features and a binary SVM with a Radial Basis Function (RBF) kernel, which performs the variant classification into pathogenic or not. We optimized the hyperparameters of both methods (such as the number of components of PCA, the SVM cost parameter C and the gamma coefficient of the RBF kernel) with a grid search procedure. A complete list of hyperparameters tested and their optimal values are available in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>.</p>
        <p>It is worth clarifying that, during both cross-validation and blind testing, the execution of the PCA step is always computed on the training set and then applied for projecting vectors of the testing set in the reduced space.</p>
        <p>All methods are implemented in Python3 using the scikit-learn library (<xref rid="btac678-B32" ref-type="bibr">Pedregosa <italic toggle="yes">et al.</italic>, 2011</xref>). ESM-1v and ProtT5 embeddings are computed with the bio-embeddings package (<xref rid="btac678-B12" ref-type="bibr">Dallago <italic toggle="yes">et al.</italic>, 2021</xref>).</p>
        <p>The complete machine-learning workflow is compliant with the DOME recommendation checklist (<xref rid="btac678-B51" ref-type="bibr">Walsh <italic toggle="yes">et al.</italic>, 2021</xref>), as reported in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S3</xref>.</p>
      </sec>
    </sec>
    <sec>
      <title>2.5 Output</title>
      <p>The SVM adopted for classification computes a decision function that represents the distance of the point mapping the input from the discrimination boundary. We use this value to estimate the reliability of the prediction, in terms of the probability of the input variation to be pathogenic (<xref rid="btac678-F1" ref-type="fig">Fig. 1</xref>).</p>
      <p>In a perfectly calibrated method, when a set of predictions scored with probability <italic toggle="yes">P</italic> is tested on real data, we expect that the fraction of true positives is exactly <italic toggle="yes">P</italic>. In this work, we adopt a procedure previously described (Benevenuta <italic toggle="yes">et al.</italic>, 2021) to obtain a calibrated probability that we provide in output alongside the predicted class. In particular, we fit an Isotonic Regression (<xref rid="btac678-B29" ref-type="bibr">Niculescu-Mizil and Caruana, 2005</xref>) in cross-validation and we use it to obtain a probability score on the blind test. <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S1</xref> shows that E-SNPs&amp;GO output probabilities are very close to being perfectly calibrated, more than other popular methods.</p>
      <p>Keeping as a reference the probability of being P/PL, the probability score (<italic toggle="yes">P</italic><sub>P/PL</sub>) gives an integer Reliability Index from 0 (random prediction) to 10 (certain prediction) using the formula:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mi>RI</mml:mi><mml:mo>=</mml:mo><mml:mi>round</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>20</mml:mn><mml:mo>×</mml:mo><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mi>L</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula></p>
    </sec>
    <sec>
      <title>2.6 Scoring indexes</title>
      <p>We assess the performance with the following scores. P/LP variations are assumed to be the positive class, B/LB variations are the negative class. In what follows, TP, TN, FP and FN are true positive, true negative, false positive and false negative predictions, respectively.</p>
      <p>We compute the following scoring measures:
</p>
      <list list-type="bullet">
        <list-item>
          <p>Accuracy (<italic toggle="yes">Q</italic><sub>2</sub>):
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">TN</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FN</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        </list-item>
        <list-item>
          <p>Precision:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mi mathvariant="normal">Precision</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FP</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        </list-item>
        <list-item>
          <p>Recall:
<disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mi mathvariant="normal">Recall</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FN</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        </list-item>
        <list-item>
          <p><italic toggle="yes">F</italic>1-score, the harmonic mean of precision and recall:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">Precision</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">Recall</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Precision</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Recall</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        </list-item>
        <list-item>
          <p>Area under the receiver operating characteristic curve (ROC-AUC).</p>
        </list-item>
        <list-item>
          <p>MCC:
<disp-formula id="E6"><label>(6)</label><mml:math id="M6" display="block" overflow="scroll"><mml:mi mathvariant="normal">MCC</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">TN</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal">FP</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">FN</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mo>(</mml:mo><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FP</mml:mi><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="normal">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FN</mml:mi><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="normal">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FP</mml:mi><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="normal">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">FN</mml:mi><mml:mo>)</mml:mo></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        </list-item>
      </list>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Assessing the contribution of different input encodings</title>
      <p>To select the optimal input encoding, we performed different experiments to test various combinations of input features. To this aim, we trained in cross-validation several independent SVM+PCA models using different input features and using the MCC to score and select the optimal model.</p>
      <p>GO terms provide global protein information. Their embedding does not consider the specific variant position. If the prediction is run considering only averaged embedded GO terms vector (<xref rid="btac678-F1" ref-type="fig">Fig. 1</xref>), the predictor performance is very low (MCC = 0.27, data not shown). Different input encodings, corresponding to different predictors, perform differently (<xref rid="btac678-T2" ref-type="table">Table 2</xref>). The inclusion of GO embeddings in the final input is always beneficial, improving MCC by 2 or 3 percentage points in all cases (compare ESM-1v, ProtT5 and ESM-1v+ProtT5 with or without GO, respectively in <xref rid="btac678-T2" ref-type="table">Table 2</xref>). Considering the two protein sequence embeddings, ProtT5 outperforms ESM-1v both with and without the additional GO information. Most notably, the model trained on data from ProtT5 alone is the most balanced, reaching equal precision and recall. Finally, the concatenation of both sequence encodings and the GO embedding provides the best performance (MCC = 0.69), leading to an increase in precision without a corresponding decrease in recall.</p>
      <p>Based on these results, we select the model trained with ESM-1v+ProtT5+GO as the optimal one.</p>
    </sec>
    <sec>
      <title>3.2 Benchmark on the blind test set</title>
      <p>We test our method adopting both a 10-fold cross-validation procedure and an independent blind test set constructed to be non-redundant with respect to the training dataset (see Section 2.1). <xref rid="btac678-T3" ref-type="table">Table 3</xref> lists the results. E-SNPs&amp;GO obtains similar results in cross-validation and blind test, making it very robust to generalization. Concerning individual indexes, our method seems to be slightly more precise than sensitive (compare Precision and Recall).</p>
      <table-wrap position="float" id="btac678-T3">
        <label>Table 3.</label>
        <caption>
          <p>Benchmark of our and other top scoring methods available in literature</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Input encoding</th>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"><italic toggle="yes">Q</italic><sub>2</sub> (%)</th>
              <th rowspan="1" colspan="1">Precision (%)</th>
              <th rowspan="1" colspan="1">Recall (%)</th>
              <th rowspan="1" colspan="1"><italic toggle="yes">F</italic>1-score (%)</th>
              <th rowspan="1" colspan="1">ROC-AUC (%)</th>
              <th rowspan="1" colspan="1">MCC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">E-SNPs&amp;GO<xref rid="tblfn10" ref-type="table-fn"><sup>a</sup></xref></td>
              <td rowspan="1" colspan="1">Cross-validation</td>
              <td rowspan="1" colspan="1">85.1 (±0.9)</td>
              <td rowspan="1" colspan="1">82.4 (±1.7)</td>
              <td rowspan="1" colspan="1">79.1 (±1.7)</td>
              <td rowspan="1" colspan="1">80.7 (±1.1)</td>
              <td rowspan="1" colspan="1">84.1 (±0.9)</td>
              <td rowspan="1" colspan="1">0.69 (±0.018)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">E-SNPs&amp;GO<xref rid="tblfn10" ref-type="table-fn"><sup>a</sup></xref></td>
              <td rowspan="1" colspan="1">Blind test set</td>
              <td rowspan="1" colspan="1">86.8</td>
              <td rowspan="1" colspan="1">85.7</td>
              <td rowspan="1" colspan="1">80.1</td>
              <td rowspan="1" colspan="1">82.8</td>
              <td rowspan="1" colspan="1">85.6</td>
              <td rowspan="1" colspan="1">0.72</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SNPs&amp;GO<xref rid="tblfn10" ref-type="table-fn"><sup>a</sup></xref></td>
              <td rowspan="1" colspan="1">Blind test set</td>
              <td rowspan="1" colspan="1">79.8</td>
              <td rowspan="1" colspan="1">84.8</td>
              <td rowspan="1" colspan="1">63.2</td>
              <td rowspan="1" colspan="1">72.4</td>
              <td rowspan="1" colspan="1">77.5</td>
              <td rowspan="1" colspan="1">0.58</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MutPred2.0<xref rid="tblfn11" ref-type="table-fn"><sup>b</sup></xref></td>
              <td rowspan="1" colspan="1">Blind test set</td>
              <td rowspan="1" colspan="1">85.6</td>
              <td rowspan="1" colspan="1">78.6</td>
              <td rowspan="1" colspan="1">87.7</td>
              <td rowspan="1" colspan="1">82.9</td>
              <td rowspan="1" colspan="1">85.9</td>
              <td rowspan="1" colspan="1">0.71</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PROVEAN<xref rid="tblfn12" ref-type="table-fn"><sup>c</sup></xref></td>
              <td rowspan="1" colspan="1">Blind test set</td>
              <td rowspan="1" colspan="1">78.2</td>
              <td rowspan="1" colspan="1">68.7</td>
              <td rowspan="1" colspan="1">83.0</td>
              <td rowspan="1" colspan="1">75.2</td>
              <td rowspan="1" colspan="1">79.0</td>
              <td rowspan="1" colspan="1">0.57</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SIFT<xref rid="tblfn13" ref-type="table-fn"><sup>d</sup></xref></td>
              <td rowspan="1" colspan="1">Blind test set</td>
              <td rowspan="1" colspan="1">74.4</td>
              <td rowspan="1" colspan="1">62.7</td>
              <td rowspan="1" colspan="1">88.0</td>
              <td rowspan="1" colspan="1">73.2</td>
              <td rowspan="1" colspan="1">76.7</td>
              <td rowspan="1" colspan="1">0.53</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PolyPhen-2<xref rid="tblfn14" ref-type="table-fn"><sup>e</sup></xref></td>
              <td rowspan="1" colspan="1">Blind test set</td>
              <td rowspan="1" colspan="1">72.3</td>
              <td rowspan="1" colspan="1">60.6</td>
              <td rowspan="1" colspan="1">89.5</td>
              <td rowspan="1" colspan="1">72.2</td>
              <td rowspan="1" colspan="1">75.1</td>
              <td rowspan="1" colspan="1">0.50</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn9">
            <p><italic toggle="yes">Note</italic>: The benchmark is performed on a test set comprising 10 266 human variations (<xref rid="btac678-T1" ref-type="table">Table 1</xref>, 10% of the total number of SAVs) that is blind with respect to our training set. It could be redundant with respect to the training sets of other methods, leading to a possible overestimation of their performances. We also report our performances in cross-validation for comparison. We increased the size of the blind test set up to 20% of the number of SAVs and the E-SNPs&amp;GO MCC score values were negligibly affected (0.5%, data not shown).</p>
          </fn>
          <fn id="tblfn10">
            <label>a</label>
            <p>E-SNPs&amp;GO: this article; SNPs&amp;GO (<xref rid="btac678-B8" ref-type="bibr">Calabrese <italic toggle="yes">et al.</italic>, 2009</xref>).</p>
          </fn>
          <fn id="tblfn11">
            <label>b</label>
            <p>MutPred2.0 (<xref rid="btac678-B33" ref-type="bibr">Pejaver <italic toggle="yes">et al.</italic>, 2020</xref>).</p>
          </fn>
          <fn id="tblfn12">
            <label>c</label>
            <p>PROVEAN (<xref rid="btac678-B11" ref-type="bibr">Choi <italic toggle="yes">et al.</italic>, 2012</xref>).</p>
          </fn>
          <fn id="tblfn13">
            <label>d</label>
            <p>SIFT (<xref rid="btac678-B28" ref-type="bibr">Ng and Henikoff, 2001</xref>).</p>
          </fn>
          <fn id="tblfn14">
            <label>e</label>
            <p>PolyPhen-2 (<xref rid="btac678-B1" ref-type="bibr">Adzhubei <italic toggle="yes">et al.</italic>, 2010</xref>).</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p><xref rid="btac678-T3" ref-type="table">Table 3</xref> includes also a comparative benchmark of our method with other state-of-the-art tools, including our SNPs&amp;GO (<xref rid="btac678-B8" ref-type="bibr">Calabrese <italic toggle="yes">et al.</italic>, 2009</xref>), SIFT (<xref rid="btac678-B28" ref-type="bibr">Ng and Henikoff, 2001</xref>), PolyPhen-2 (<xref rid="btac678-B1" ref-type="bibr">Adzhubei <italic toggle="yes">et al.</italic>, 2010</xref>), PROVEAN (<xref rid="btac678-B11" ref-type="bibr">Choi <italic toggle="yes">et al.</italic>, 2012</xref>) and MutPred2 (<xref rid="btac678-B33" ref-type="bibr">Pejaver <italic toggle="yes">et al.</italic>, 2020</xref>), one of the most recent and best-performing approaches in the field. Methods are scored adopting our blind test set (Section 2.1), ensuring a fair evaluation of the performance of our method. However, this does not completely exclude the presence of biases in the evaluation of the other tools (with the exception of our SNPs&amp;GO), since variations included in our blind test may be present in the respective training sets, leading to potential overestimation of their performance.</p>
      <p>In <xref rid="btac678-T3" ref-type="table">Table 3</xref>, it appears that in this benchmark our method is performing at the state-of-the-art. Among tested approaches, PROVEAN, SIFT and PolyPhen-2, reporting MCCs of 0.57, 0.53 and 0.50, respectively, are scoring lower than our previous SNPs&amp;GO (that achieves an MCC of 0.58). Our E-SNPs&amp;GO and MutPred2, score with significantly higher MCC values of 0.72 and 0.71, respectively. Noticeably the embedding procedure seems to grasp all the properties extracted by an ensemble of different predictors of functional, structural and physicochemical properties, such as the one used by MutPred2 (including over 50 tools). Looking at individual scoring measures, MutPred2 appears more sensitive while our method reports a higher precision.</p>
      <p>A detailed ablation study performed to evaluate the effect of the GO terms on the prediction scores (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S4</xref>), indicates that the CC sub-ontology slightly outperforms the others.</p>
    </sec>
    <sec>
      <title>3.3 Prediction of variants of uncertain significance</title>
      <p>We tested E-SNPs&amp;GO on a dataset of 2588 proteins annotated with 9165 variants of uncertain significance (VUS) extracted from HUMSAVAR (accessed on May 12, 2022). Given that they are uncertain, we cannot assess our performances on this dataset. However, we can sample our predicted annotation in terms of probability and reliability [Equation (6)]. Setting as a reference the probability of being P/LP, <xref rid="btac678-F2" ref-type="fig">Figure 2</xref> shows the distribution of E-SNPs&amp;GO predictions over the whole VUS set as a function of probability and reliability index. A total of 4537 variations are P/LP (pathogenicity probability ≥0.5), while 4628 are B/LB (pathogenicity probability &lt;0.5). The reliability index increases as the probability goes towards 1 or 0 for P/LP and B/LB predictions, respectively [Equation (6)]. In the dataset, 3210 P/LP and 2908 B/LB predictions score with a reliability [RI, Equation (6)] ≥6, accounting for the 67% of VUS. The remaining 33% is predicted with RI lower than 6. For further validation, VUS predictions are available at <ext-link xlink:href="https://esnpsandgo.biocomp.unibo.it/datasets" ext-link-type="uri">https://esnpsandgo.biocomp.unibo.it/datasets</ext-link>.</p>
      <fig position="float" id="btac678-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>Distribution of predicted pathogenicity probabilities for the dataset of VUS. The value 0.5 discriminates between B/LB and P/LP prediction. Probability values close to either 0 or 1 correspond to prediction with a high reliability index [Equation (1)]</p>
        </caption>
        <graphic xlink:href="btac678f2" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.4 E-SNP&amp;GO web server</title>
      <p>E-SNPs&amp;GO web server is available at <ext-link xlink:href="https://esnpsandgo.biocomp.unibo.it" ext-link-type="uri">https://esnpsandgo.biocomp.unibo.it</ext-link>. The server allows users to submit up to 1000 variations per single job. Upon job completion, the results can be visualized on the web page and downloaded in either a tab-separated or a JSON file.</p>
      <p>We measured the average E-SNPs&amp;GO runtime by submitting 100 different jobs each including 1000 variations randomly selected from the blind test set. In order to estimate the real execution time for the end user, this experiment was performed in the machine hosting the web server, equipped with one AMD EPYC 7301 CPU with 12 cores, 48 GB of RAM and no GPU available. On average, we obtain a running time of 12.4 ± 4.4 s per variation, when submitting the maximum allowed number of variations per job (1000 variations). This highlights a significant improvement over time-consuming approaches using canonical features such as evolutionary information extracted from multiple sequence alignments.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusions</title>
    <p>We introduce E-SNPs&amp;GO, a method based on language models for annotating whether a single-nucleotide variation is or is not P/LP. We adopt two different protein embedding procedures based on transformers, ESM-1v (<xref rid="btac678-B26" ref-type="bibr">Meier <italic toggle="yes">et al.</italic>, 2021</xref>) and ProtT5 (<xref rid="btac678-B14" ref-type="bibr">Elnaggar <italic toggle="yes">et al.</italic>, 2021</xref>). Both embedding methods have been developed and tested on protein variant related problems, such as deep mutational scanning (<xref rid="btac678-B25" ref-type="bibr">Marquet <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac678-B26" ref-type="bibr">Meier <italic toggle="yes">et al.</italic>, 2021</xref>). Here, we address the problem of annotating pathogenic versus benign variations. To this aim, we also add an embedding scheme for functional annotations of wild-type proteins, Anc2Vec (<xref rid="btac678-B13" ref-type="bibr">Edera <italic toggle="yes">et al.</italic>, 2022</xref>), a method that learns a vector representation for GO terms by preserving ancestor relationships. When benchmarked towards state-of-the-art methods available, E-SNPs&amp;GO well compares to the recently developed MutPred2.0 (<xref rid="btac678-B33" ref-type="bibr">Pejaver <italic toggle="yes">et al.</italic>, 2020</xref>), which includes as input sequence features derived from some 50 predictors and outperforms previously published methods. Evidently, protein language models learn all the relevant information that can be eventually introduced as input by predictors addressing different tasks.</p>
    <p>We prove that embedding models overpass the problem of having as input thousands of different features in order to collect all the relevant features for a reliable annotation of the human pathogenic variations.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by PRIN 2017 project [2017483NH8 to C.S.] (Italian Ministry of University and Research).</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btac678_Supplementary_Data</label>
      <media xlink:href="btac678_supplementary_data.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <sec sec-type="data-availability">
    <title>Data Availability</title>
    <p>The data underlying this article are available in the article, in its online supplementary material and at the E-SNPs&amp;GO web site: <ext-link xlink:href="https://esnpsandgo.biocomp.unibo.it" ext-link-type="uri">https://esnpsandgo.biocomp.unibo.it</ext-link>.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btac678-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Adzhubei</surname><given-names>I.A.</given-names></string-name></person-group><etal>et al</etal> (<year>2010</year>) <article-title>A method and server for predicting damaging missense mutations</article-title>. <source>Nat. Methods</source>, <volume>7</volume>, <fpage>248</fpage>–<lpage>249</lpage>.<pub-id pub-id-type="pmid">20354512</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alley</surname><given-names>E.C.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Unified rational protein engineering with sequence-based deep representation learning</article-title>. <source>Nat. Methods</source>, <volume>16</volume>, <fpage>1315</fpage>–<lpage>1322</lpage>.<pub-id pub-id-type="pmid">31636460</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Amberger</surname><given-names>J.S.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>OMIM.org: leveraging knowledge across phenotype–gene relationships</article-title>. <source>Nucleic Acids Res</source>., <volume>47</volume>, <fpage>D1038</fpage>–<lpage>D1043</lpage>.<pub-id pub-id-type="pmid">30445645</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Asgari</surname><given-names>E.</given-names></string-name>, <string-name><surname>Mofrad</surname><given-names>M.R.K.</given-names></string-name></person-group> (<year>2015</year>) <article-title>Continuous distributed representation of biological sequences for deep proteomics and genomics</article-title>. <source>PLoS One</source>, <volume>10</volume>, <fpage>e0141287</fpage>.<pub-id pub-id-type="pmid">26555596</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ashburner</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2000</year>) <article-title>Gene ontology: tool for the unification of biology</article-title>. <source>Nat. Genet</source>., <volume>25</volume>, <fpage>25</fpage>–<lpage>29</lpage>.<pub-id pub-id-type="pmid">10802651</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benevenuta</surname><given-names>S.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Calibrating variant-scoring methods for clinical decision making</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>5709</fpage>–<lpage>5711</lpage>.<pub-id pub-id-type="pmid">33492342</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bepler</surname><given-names>T.</given-names></string-name>, <string-name><surname>Berger</surname><given-names>B.</given-names></string-name></person-group> (<year>2021</year>) <article-title>Learning the protein language: evolution, structure, and function</article-title>. <source>Cell Syst</source>., <volume>12</volume>, <fpage>654</fpage>–<lpage>669.e3</lpage>.<pub-id pub-id-type="pmid">34139171</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calabrese</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2009</year>) <article-title>Functional annotations improve the predictive score of human disease-related mutations in proteins</article-title>. <source>Hum. Mutat</source>., <volume>30</volume>, <fpage>1237</fpage>–<lpage>1244</lpage>.<pub-id pub-id-type="pmid">19514061</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carter</surname><given-names>H.</given-names></string-name></person-group><etal>et al</etal> (<year>2013</year>) <article-title>Identifying Mendelian disease genes with the variant effect scoring tool</article-title>. <source>BMC Genomics</source>, <volume>14 (Suppl. 3)</volume>, <fpage>S3</fpage>.</mixed-citation>
    </ref>
    <ref id="btac678-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>OWL2Vec: embedding of OWL ontologies</article-title>. <source>Mach. Learn</source>., <volume>110</volume>, <fpage>1813</fpage>–<lpage>1845</lpage>.</mixed-citation>
    </ref>
    <ref id="btac678-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Choi</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2012</year>) <article-title>Predicting the functional effect of amino acid substitutions and indels</article-title>. <source>PLoS One</source>, <volume>7</volume>, <fpage>e46688</fpage>.<pub-id pub-id-type="pmid">23056405</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dallago</surname><given-names>C.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Learned embeddings from deep learning to visualize and predict protein sets</article-title>. <source>Curr. Protoc</source>., <volume>1</volume>, <fpage>e113</fpage>.<pub-id pub-id-type="pmid">33961736</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Edera</surname><given-names>A.A.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Anc2vec: embedding gene ontology terms by preserving ancestors relationships</article-title>. <source>Brief. Bioinformatics</source>, <volume>23</volume>, <fpage>bbac003</fpage>.<pub-id pub-id-type="pmid">35136916</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Elnaggar</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>ProtTrans: towards cracking the language of life’s code through Self-Supervised deep learning and high performance computing</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>., <volume>14</volume>, <fpage>1</fpage>.</mixed-citation>
    </ref>
    <ref id="btac678-B15">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Grover</surname><given-names>A.</given-names></string-name>, <string-name><surname>Leskovec</surname><given-names>J.</given-names></string-name></person-group> (<year>2016</year>) node2vec: scalable feature learning for networks. In: <italic toggle="yes">Proceedings of the 22<sup>nd</sup> ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2016, San Francisco, CA, USA</italic>, pp. 855–864.</mixed-citation>
    </ref>
    <ref id="btac678-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heinzinger</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Modeling aspects of the language of life through transfer-learning protein sequences</article-title>. <source>BMC Bioinformatics</source>, <volume>20</volume>, <fpage>723</fpage>.<pub-id pub-id-type="pmid">31847804</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jagadeesh</surname><given-names>K.A.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) <article-title>M-CAP eliminates a majority of variants of uncertain significance in clinical exomes at high sensitivity</article-title>. <source>Nat. Genet</source>., <volume>48</volume>, <fpage>1581</fpage>–<lpage>1586</lpage>.<pub-id pub-id-type="pmid">27776117</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kandathil</surname><given-names>S.M.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Ultrafast end-to-end protein structure prediction enables high-throughput exploration of uncharacterized proteins</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>119</volume>, <fpage>e2113348119</fpage>.<pub-id pub-id-type="pmid">35074909</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Landrum</surname><given-names>M.J.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>ClinVar: improving access to variant interpretations and supporting evidence</article-title>. <source>Nucleic Acids Res</source>., <volume>46</volume>, <fpage>D1062</fpage>–<lpage>D1067</lpage>.<pub-id pub-id-type="pmid">29165669</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lappalainen</surname><given-names>T.</given-names></string-name>, <string-name><surname>MacArthur</surname><given-names>D.G.</given-names></string-name></person-group> (<year>2021</year>) <article-title>From variant to function in human disease genetics</article-title>. <source>Science</source>, <volume>373</volume>, <fpage>1464</fpage>–<lpage>1468</lpage>.<pub-id pub-id-type="pmid">34554789</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>B.</given-names></string-name></person-group><etal>et al</etal> (<year>2009</year>) <article-title>Automated inference of molecular mechanisms of disease from amino acid substitutions</article-title>. <source>Bioinformatics</source>, <volume>25</volume>, <fpage>2744</fpage>–<lpage>2750</lpage>.<pub-id pub-id-type="pmid">19734154</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Littmann</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Embeddings from deep learning transfer GO annotations beyond homology</article-title>. <source>Sci. Rep</source>., <volume>11</volume>, <fpage>1160</fpage>.<pub-id pub-id-type="pmid">33441905</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mahbub</surname><given-names>S.</given-names></string-name>, <string-name><surname>Bayzid</surname><given-names>M.S.</given-names></string-name></person-group> (<year>2022</year>) <article-title>EGRET: edge aggregated graph attention networks and transfer learning improve protein–protein interaction site prediction</article-title>. <source>Brief. Bioinformatics</source>, <volume>23</volume>, <fpage>bbab578</fpage>.<pub-id pub-id-type="pmid">35106547</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marquet</surname><given-names>C.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Embeddings from protein language models predict conservation and variant effects</article-title>. <source>Hum. Genet</source>., <volume>141</volume>, <fpage>1629</fpage>–<lpage>1647</lpage>.<pub-id pub-id-type="pmid">34967936</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B26">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Meier</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) Language models enable zero-shot prediction of the effects of mutations on protein function. In: <person-group person-group-type="editor"><string-name><surname>Ranzato</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (eds) <italic toggle="yes">Advances in Neural Information Processing Systems. Proceedings of NeurIPS 2021</italic>, Vol. <volume>34</volume>. pp. <fpage>29287</fpage>–<lpage>29303</lpage>.</mixed-citation>
    </ref>
    <ref id="btac678-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nair</surname><given-names>P.S.</given-names></string-name>, <string-name><surname>Vihinen</surname><given-names>M.</given-names></string-name></person-group> (<year>2013</year>) <article-title>VariBench: a benchmark database for variations</article-title>. <source>Hum. Mutat</source>., <volume>34</volume>, <fpage>42</fpage>–<lpage>49</lpage>.<pub-id pub-id-type="pmid">22903802</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ng</surname><given-names>P.C.</given-names></string-name>, <string-name><surname>Henikoff</surname><given-names>S.</given-names></string-name></person-group> (<year>2001</year>) <article-title>Predicting deleterious amino acid substitutions</article-title>. <source>Genome Res</source>., <volume>11</volume>, <fpage>863</fpage>–<lpage>874</lpage>.<pub-id pub-id-type="pmid">11337480</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B29">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Niculescu-Mizil</surname><given-names>A.</given-names></string-name>, <string-name><surname>Caruana</surname><given-names>R.</given-names></string-name></person-group> (<year>2005</year>) Predicting good probabilities with supervised learning. In: <italic toggle="yes">Proceedings of the 22nd International Conference on Machine Learning</italic>, ICML ’05. <publisher-name>Association for Computing Machinery</publisher-name>, <publisher-loc>New York, NY, USA</publisher-loc>, pp. <fpage>625</fpage>–<lpage>632</lpage>.</mixed-citation>
    </ref>
    <ref id="btac678-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Niroula</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2015</year>) <article-title>PON-P2: prediction method for fast and reliable identification of harmful variants</article-title>. <source>PLoS One</source>, <volume>10</volume>, <fpage>e0117380</fpage>.<pub-id pub-id-type="pmid">25647319</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ofer</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>The language of proteins: NLP, machine learning &amp; protein sequences</article-title>. <source>Comput. Struct. Biotechnol. J</source>., <volume>19</volume>, <fpage>1750</fpage>–<lpage>1758</lpage>.<pub-id pub-id-type="pmid">33897979</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pedregosa</surname><given-names>F.</given-names></string-name></person-group><etal>et al</etal> (<year>2011</year>) <article-title>Scikit-learn: machine learning in python</article-title>. <source>J. Mach. Learn. Res</source>., <volume>12</volume>, <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="btac678-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pejaver</surname><given-names>V.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Inferring the molecular and phenotypic impact of amino acid variants with MutPred2</article-title>. <source>Nat. Commun</source>., <volume>11</volume>, <fpage>5918</fpage>.<pub-id pub-id-type="pmid">33219223</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B34">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Perozzi</surname><given-names>B.</given-names></string-name></person-group><etal>et al</etal> (<year>2014</year>) DeepWalk: online learning of social representations. In: <italic toggle="yes">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, New York, NY, USA</italic>, pp. <fpage>701</fpage>–<lpage>710</lpage>.</mixed-citation>
    </ref>
    <ref id="btac678-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raimondi</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>DEOGEN2: prediction and interactive visualization of single amino acid variant deleteriousness in human proteins</article-title>. <source>Nucleic Acids Res</source>., <volume>45</volume>, <fpage>W201</fpage>–<lpage>W206</lpage>.<pub-id pub-id-type="pmid">28498993</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rives</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>118</volume>, <fpage>e2016239118</fpage>.<pub-id pub-id-type="pmid">33876751</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schwarz</surname><given-names>J.M.</given-names></string-name></person-group><etal>et al</etal> (<year>2010</year>) <article-title>MutationTaster evaluates disease-causing potential of sequence alterations</article-title>. <source>Nat. Methods</source>, <volume>7</volume>, <fpage>575</fpage>–<lpage>576</lpage>.<pub-id pub-id-type="pmid">20676075</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shefchek</surname><given-names>K.A.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>The monarch initiative in 2019: an integrative data and analytic platform connecting phenotypes to genotypes across species</article-title>. <source>Nucleic Acids Res</source>., <volume>48</volume>, <fpage>D704</fpage>–<lpage>D715</lpage>.<pub-id pub-id-type="pmid">31701156</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Singh</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>SPOT-Contact-LM: improving single-sequence-based prediction of protein contact map using a transformer language model</article-title>. <source>Bioinformatics</source>, <volume>38</volume>, <fpage>1888</fpage>–<lpage>1894</lpage>.<pub-id pub-id-type="pmid">35104320</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stärk</surname><given-names>H.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Light attention predicts protein location from the language of life</article-title>. <source>Bioinform. Adv</source>., <volume>1</volume>, <fpage>vbab035</fpage>.</mixed-citation>
    </ref>
    <ref id="btac678-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steinegger</surname><given-names>M.</given-names></string-name>, <string-name><surname>Söding</surname><given-names>J.</given-names></string-name></person-group> (<year>2017</year>) <article-title>MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</article-title>. <source>Nat. Biotechnol</source>., <volume>35</volume>, <fpage>1026</fpage>–<lpage>1028</lpage>.<pub-id pub-id-type="pmid">29035372</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steinegger</surname><given-names>M.</given-names></string-name>, <string-name><surname>Söding</surname><given-names>J.</given-names></string-name></person-group> (<year>2018</year>) <article-title>Clustering huge protein sequence sets in linear time</article-title>. <source>Nat. Commun</source>., <volume>9</volume>, <fpage>2542</fpage>.<pub-id pub-id-type="pmid">29959318</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steinegger</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Protein-level assembly increases protein sequence recovery from metagenomic samples manyfold</article-title>. <source>Nat. Methods</source>, <volume>16</volume>, <fpage>603</fpage>–<lpage>606</lpage>.<pub-id pub-id-type="pmid">31235882</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Strodthoff</surname><given-names>N.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>UDSMProt: universal deep sequence models for protein classification</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>2401</fpage>–<lpage>2409</lpage>.<pub-id pub-id-type="pmid">31913448</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Suzek</surname><given-names>B.E.</given-names></string-name></person-group><etal>et al</etal>; <collab>the UniProt Consortium</collab>. (<year>2015</year>) <article-title>UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches</article-title>. <source>Bioinformatics</source>, <volume>31</volume>, <fpage>926</fpage>–<lpage>932</lpage>.<pub-id pub-id-type="pmid">25398609</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teufel</surname><given-names>E.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>SignalP 6.0 predicts all five types of signal peptides using protein language models</article-title>. <source>Nat. Biotechnol</source>., <volume>40</volume>, <fpage>1023</fpage>–<lpage>1025</lpage>.<pub-id pub-id-type="pmid">34980915</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B47">
      <mixed-citation publication-type="journal"><collab>The UniProt Consortium</collab> (<year>2021</year>) <article-title>UniProt: the universal protein knowledgebase in 2021</article-title>. <source>Nucleic Acids Res</source>., <volume>49</volume>, <fpage>D480</fpage>–<lpage>D489</lpage>.<pub-id pub-id-type="pmid">33237286</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B48">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Vaswani</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) Attention is all you need. In: <italic toggle="yes">Proceedings of the 31<sup>st</sup> Annual Conference on Neural Information Processing Systems, NIPS 2017, Long Beach, CA, USA</italic>, pp. <fpage>5999</fpage>–<lpage>6009</lpage></mixed-citation>
    </ref>
    <ref id="btac678-B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vihinen</surname><given-names>M.</given-names></string-name></person-group> (<year>2021</year>) <article-title>Functional effects of protein variants</article-title>. <source>Biochimie</source>, <volume>180</volume>, <fpage>104</fpage>–<lpage>120</lpage>.<pub-id pub-id-type="pmid">33164889</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Walsh</surname><given-names>I.</given-names></string-name></person-group><etal>et al</etal>; <collab>ELIXIR Machine Learning Focus Group</collab>. (<year>2021</year>) <article-title>DOME: recommendations for supervised machine learning validation in biology</article-title>. <source>Nat. Methods</source>, <volume>18</volume>, <fpage>1122</fpage>–<lpage>1127</lpage>.<pub-id pub-id-type="pmid">34316068</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>PON-All, amino acid substitution tolerance predictor for all organisms</article-title>. <source>Front. Mol. Biosci</source>., <volume>9</volume>, <fpage>867572</fpage>.<pub-id pub-id-type="pmid">35782867</pub-id></mixed-citation>
    </ref>
    <ref id="btac678-B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhong</surname><given-names>X.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>GO2Vec: transforming GO terms and proteins to vector representations via graph embeddings</article-title>. <source>BMC Genomics</source>, <volume>20</volume>, <fpage>918</fpage>.<pub-id pub-id-type="pmid">31874639</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
