<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr Pac Symp Biocomput?>
<?submitter-system nihms?>
<?submitter-canonical-name World Scientific Publishing?>
<?submitter-canonical-id WORLDSCIENCE?>
<?submitter-userid 8068881?>
<?submitter-authority myNCBI?>
<?submitter-login worldscience?>
<?submitter-name World Scientific Publishing?>
<?domain nihpa?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">9711271</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">20660</journal-id>
    <journal-id journal-id-type="nlm-ta">Pac Symp Biocomput</journal-id>
    <journal-id journal-id-type="iso-abbrev">Pac Symp Biocomput</journal-id>
    <journal-title-group>
      <journal-title>Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">2335-6928</issn>
    <issn pub-type="epub">2335-6936</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6980320</article-id>
    <article-id pub-id-type="pmid">31797618</article-id>
    <article-id pub-id-type="manuscript">nihpa1061174</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Tree-Weighting for Multi-Study Ensemble Learners</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Ramchandran</surname>
          <given-names>Maya</given-names>
        </name>
        <xref ref-type="aff" rid="A1">1</xref>
        <xref rid="CR1" ref-type="corresp">†</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Patil</surname>
          <given-names>Prasad</given-names>
        </name>
        <xref ref-type="aff" rid="A1">1</xref>
        <xref ref-type="aff" rid="A2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Parmigiani</surname>
          <given-names>Giovanni</given-names>
        </name>
        <xref ref-type="aff" rid="A1">1</xref>
        <xref ref-type="aff" rid="A2">2</xref>
      </contrib>
    </contrib-group>
    <aff id="A1"><label>1</label>Department of Biostatistics, Harvard T.H. Chan School of Public Health</aff>
    <aff id="A2"><label>2</label>Department of Data Sciences, Dana-Farber Cancer Institute, Boston, MA 02115, USA</aff>
    <author-notes>
      <corresp id="CR1">
        <label>†</label>
        <email>maya_ramchandran@g.harvard.edu</email>
      </corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>7</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>24</day>
      <month>1</month>
      <year>2020</year>
    </pub-date>
    <volume>25</volume>
    <fpage>451</fpage>
    <lpage>462</lpage>
    <permissions>
      <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>Open Access chapter published by World Scientific Publishing Company and distributed under the terms of the Creative Commons Attribution Non-Commercial (CC BY-NC) 4.0 License.</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P1">Multi-study learning uses multiple training studies, separately trains classifiers on each, and forms an ensemble with weights rewarding members with better cross-study prediction ability. This article considers novel weighting approaches for constructing tree-based ensemble learners in this setting. Using Random Forests as a single-study learner, we compare weighting each forest to form the ensemble, to extracting the individual trees trained by each Random Forest and weighting them directly. We find that incorporating multiple layers of ensembling in the training process by weighting trees increases the robustness of the resulting predictor. Furthermore, we explore how ensembling weights correspond to tree structure, to shed light on the features that determine whether weighting trees directly is advantageous. Finally, we apply our approach to genomic datasets and show that weighting trees improves upon the basic multi-study learning paradigm.</p>
    </abstract>
    <kwd-group>
      <kwd>Ensemble learning</kwd>
      <kwd>Random Forests</kwd>
      <kwd>Replicability</kwd>
      <kwd>Multi-study learning</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <title>Introduction</title>
    <p id="P2">With increasing availability of multiple datasets measuring the same outcome and many of the same features, it is important to consider the totality of this information to train replicable predictors. Training and validating on a single study can often produce inflated reports of accuracy, especially when compared with training and validating on different studies.<sup><xref rid="R1" ref-type="bibr">1</xref></sup> This motivates the use of ensemble learning methods to combine predictions based on multiple studies. A cross-study learner (CSL)<sup><xref rid="R2" ref-type="bibr">2</xref></sup> is an ensemble of predictors trained separately on different studies. Single Study Learners (SSL’s) refer to any algorithm that produces a prediction model using a single study. The CSL combines SSL’s using a specified weighting strategy, creating a single predictor that can be applied to external studies. For <italic>K</italic> training datasets, let <inline-formula><mml:math display="inline" id="M1" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> be the prediction from SSL<italic>k</italic> at a new point <italic>x</italic>, and by <italic>w</italic><sub><italic>k</italic></sub>, for <italic>k</italic> = 1<italic>, ..., K</italic>, the weight given to SSL<sub><italic>k</italic></sub> in the ensemble. The prediction made by the CSL is <inline-formula><mml:math display="inline" id="M2" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Ensembles may be created using multiple SSL types (e.g. a neural net and a support vector machine), or a single type, which will be the setting explored in this paper. Depending on the weighting scheme utilized, CSLs can enhance replicability and robustness to different feature-response relationships across studies, without explicitly modeling these differences.</p>
    <p id="P3">Patil and Parmigiani<sup><xref rid="R2" ref-type="bibr">2</xref></sup> have previously explored using multiple training studies to devise ensembling strategies. As a benchmark, they considered a single learner trained by merging all the studies (merging). They found that in the presence of cross-study heterogeneity, merging and meta-analysis performed worse than CSLs. They additionally observed that when training with Random Forests<sup><xref rid="R3" ref-type="bibr">3</xref></sup> (RF) as the SSL, ensembling always produced a better predictor than merging, even in simulations with no between-study heterogeneity in the feature-outcome relationship. It would appear in this case that merging all studies, thereby including more data points with the same feature-outcome relationship in training, would be most likely to result in a prediction rule that closely reflects the data generating mechanism. Patil and Parmigiani found this to be the case for all SSLs they considered except RF. For RF, ensembling SSLs proved better than merging, even though each dataset provides fewer samples for a single forest to learn the same prediction rule. This unexpected finding motivates our present exploration into the properties of Random Forest and optimal tree-weighting strategies.</p>
    <p id="P4">RF<sup><xref rid="R3" ref-type="bibr">3</xref></sup> uses bagging<sup><xref rid="R4" ref-type="bibr">4</xref></sup> to train an ensemble of decision trees. In the original algorithm, every tree within the Forest is given an equal weight in determining a prediction. We consider RF as our SSL to explore ensembling a group of SSL’s that are themselves ensemble learners. We are particularly interested in the effect of extracting the trees trained by each SSL forest and weighting them directly, instead of simply assigning weights to each forest and giving all internal trees equal weighting. We consider weighting approaches that reward cross-study replicability within the training set. Rewarding cross-study replicability on the tree level should increase the robustness of the overall ensemble to differing population parameters, such as between-study heterogeneity in the relationships between the features and the outcome.</p>
    <p id="P5">Ours is the first tree-weighting scheme designed for the multi-study setting. Several approaches exist for weighting trees within random forests, include using a weighted random sampling scheme to train the trees,<sup><xref rid="R5" ref-type="bibr">5</xref></sup> weighting the trees themselves<sup><xref rid="R6" ref-type="bibr">6</xref>,<xref rid="R7" ref-type="bibr">7</xref></sup> or some combination of the two<sup><xref rid="R8" ref-type="bibr">8</xref></sup>.<sup><xref rid="R9" ref-type="bibr">9</xref></sup> Little has been developed on regression tree-weighting strategies, which is the primary focus of this paper. Winham et. al.<sup><xref rid="R6" ref-type="bibr">6</xref>,</sup> weighted Random Forests (wRF) uses tree-level weights to upweight more accurate trees as evaluated on a held-out test set. The authors showed that while wRF can outperform RF in high-dimensional data, this improvement only holds for larger effect sizes than typically seen in real data. We propose a tree-weighting method that improves on the existing paradigms given realistic effect sizes for multiple studies. In summary, this paper examines whether directly weighting trees improves on weighting forests, using simulations based on data including gene expression measurements and survival outcomes in ovarian cancer patients. We additionally apply our tree-based ensembling methods to other genomic multi-study and large-scale datasets</p>
  </sec>
  <sec id="S2">
    <title>Methods</title>
    <sec id="S3">
      <title>Ovarian Cancer Datasets.</title>
      <p id="P6">CuratedOvarianData<sup><xref rid="R10" ref-type="bibr">10</xref></sup> from Bioconductor in R provides data for gene expression meta-analysis of patients with ovarian cancer. In this study, we use all 15 studies in CuratedOvarianData that provide survival information without any missing data in the features. Sample sizes range from 42 to 510 subjects. We focus on 2,909 gene features observed in all studies. The gene expression features in each dataset are normalized.<sup><xref rid="R10" ref-type="bibr">10</xref></sup></p>
    </sec>
    <sec id="S4">
      <title>Simulations.</title>
      <p id="P7">We used feature vectors resampled from CuratedOvarianData, to ensure realistic distributions and covariances. For N = 100 iterations for each set of generative parameters, we randomly separated the 15 datasets from CuratedOvarianData into <italic>K</italic> = 10 training and <italic>V</italic> = 5 validation sets. Per iteration, we reduced each dataset to the same randomly sampled 100 genes out of the 2909 available. We chose 10 of these 100 genes randomly to create a linear data-generating model and drew their coefficients uniformly from [<italic>−</italic>5<italic>, −</italic>0.5] ∪ [0.5, 5], so each has a non-zero contribution to the outcome. We added user-specified between-study heterogeneity in the feature-outcome relationship through coefficient perturbation. We chose perturbation windows to explore both small- and large-scale between-study heterogeneity, in line with the differences in feature-outcome relationships we observe in the actual data. If <bold>c</bold> is the vector of coefficients and <italic>l</italic> the desired level of heterogeneity, we drew uniformly from [<bold>c</bold> − <italic>l,</italic>
<bold>c</bold> + <italic>l</italic>]. In every simulation, we assigned 5 of the training datasets low levels of coefficient perturbation, the other 5 higher levels, and the validation sets an intermediate level, to provide more pronounced variation in training than in testing. At baseline, we set <italic>l</italic>’s as .25, 1, and .4 respectively, to introduce relatively small perturbations throughout, with the validation level representing a middle ground between the two levels used in training. We varied the heterogeneity levels throughout the analysis, to evaluate the performance of the ensembles; in all figures, the heterogeneity level corresponds to the highest level in the training sets. The low level for the other 5 training sets was held constant at .25, and the intermediate level given to the validation sets was half of the highest level. We used a location shift instead of a scale change so that the features could be affected differently by between-dataset variation. We then generated the outcome vector <italic>Y</italic><sub><italic>k</italic></sub> for each study <italic>k</italic> = 1<italic>, ...</italic>15 conditional on the chosen subset of the observed predictors and the simulated coefficients. We also performed simulations with interactions between some of the features. We considered two scenarios: (1) Two datasets in the training set and two in the validation set contain interaction terms, and (2) Six datasets in the training set and two in the validation set have interaction terms. For all scenarios involving interaction terms, we chose moderately low baseline heterogeneity parameters. Further modifications used in baseline simulations are described in the Results section.</p>
    </sec>
    <sec id="S5">
      <title>Stacked regression weights.</title>
      <p id="P8">Our goal is to develop ensembles of RF SSL’s that improve predictions. To construct the weights for each SSL in the ensemble, we used the multi-study version of the stacked regression method,<sup><xref rid="R2" ref-type="bibr">2</xref></sup> which rewards cross-study generalizability of SSL’s in determining the ensembling weights. Stacked regression<sup><xref rid="R11" ref-type="bibr">11</xref></sup> forms linear combinations of multiple predictors that improves on the performance of any single one. The weights given to each predictor are determined by cross-validation and least squares regression. Breiman<sup><xref rid="R11" ref-type="bibr">11</xref></sup> explored several modifications to least squares and found that imposing a non-negativity constraint to the coefficients gave the best results. We considered variations here, including Lasso and adding or eliminating an intercept, and found overall that using stacked regression with a Ridge constraint and intercept term produced the most robust ensemble in our setting. Ridge regression shrinks coefficients rather than directly zeroing some out; this appears to allow for greater generalizability of the resulting predictor. Setting the contributions of SSL’s directly to zero within training due to poor cross-validation performance may promote overfitting to the training datasets, as such SSL’s could provide value when faced with an observation arising from a new dataset with a potentially different data generating model.</p>
      <p id="P9">We first trained an SSL on each training set. We then stacked the predictions into matrix <inline-formula><mml:math display="inline" id="M3" overflow="scroll"><mml:mrow><mml:mtext mathvariant="bold">T</mml:mtext><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>K</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>′</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math display="inline" id="M4" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mn>...</mml:mn><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>K</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula> for <italic>k</italic> = 1<italic>,..., K</italic>; <inline-formula><mml:math display="inline" id="M5" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>is the vector of predictions of SSL<italic><sub>k</sub></italic> on dataset <italic>i</italic>, and <inline-formula><mml:math display="inline" id="M6" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the stacked vector of all predictions made by SSL<sub><italic>k</italic></sub> on every training dataset. T is <italic>N × K</italic>, with <italic>N</italic> the total number of observations in all datasets. We stacked the true outcomes across all training studies into the <italic>N ×</italic> 1 vector <italic>Y</italic> . We then regressed <italic>Y</italic> against <italic>T</italic> , with a ridge penalization and non-negativity constraint. The SSL weights <bold>w</bold><sub><italic>stack</italic></sub> are determined by solving <inline-formula><mml:math display="inline" id="M7" overflow="scroll"><mml:mrow><mml:mtext>mi</mml:mtext><mml:msub><mml:mtext>n</mml:mtext><mml:mrow><mml:mtext>W</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow/><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">T</mml:mtext><mml:mo>×</mml:mo><mml:msub><mml:mtext mathvariant="bold">w</mml:mtext><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> such that <inline-formula><mml:math display="inline" id="M8" overflow="scroll"><mml:mrow><mml:msub><mml:mtext mathvariant="bold">w</mml:mtext><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mspace width="0.25em"/><mml:mtext>and</mml:mtext><mml:mspace width="0.35em"/><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold">w</mml:mtext><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>≤</mml:mo><mml:mi>λ</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where λ is optimized using the cross-validation procedure in the glmnet package in R.<sup><xref rid="R12" ref-type="bibr">12</xref></sup></p>
      <p id="P10">Throughout, <italic>Weighting Trees</italic> will indicate individually weighting each tree using stacked regression with Ridge regularization and a non-negativity constraint, while <italic>Weighting Forests</italic> will indicate using the same method on the predictions made by whole forests. The term <italic>Unweighted</italic> will refer to the ensemble created by giving every single-study forest equal weighting. The term <italic>Merged</italic> will refer to the predictor trained using a single RF on the dataset formed by merging the 15 studies. The difference between the Weighting Trees and Weighting Forests approaches is in what constitutes an SSL: individually weighting trees corresponds to first training a Random Forest on each study, then extracting the trees and treating each of the extracted trees as an SSL. If <italic>m</italic> is the number of trees per forest, the length of <bold>w</bold><sub><italic>stack</italic></sub> is <italic>K × m</italic> for the Weighting Trees approach and <italic>K</italic> for the Weighting Forests approach. In some analyses we compare tree-level weights for Weighting Trees to the tree-level weights implied by Weighting Forests, which we define by dividing each forest-level weight from <bold>w</bold><italic>stack</italic> by <italic>m</italic>, as each tree within the <italic>k</italic><sup><italic>th</italic></sup> forest is implicitly assigned weight <italic>w</italic><sub><italic>stack,k</italic></sub>
<italic>/m</italic>.</p>
      <sec id="S6">
        <title>Number of trees per forest.</title>
        <p id="P11">The RF implementation in the randomForest package in R uses 500 trees per forest as a default. In general, the performance of RF increases as a function of the number of trees before reaching a plateau; for this reason, the number of trees/forest is not typically considered to be a tuning parameter optimized through cross-validation. We found in our preliminary simulations that the order relationship between approaches was conserved as the number of trees per ensemble increased; this is illustrated in <xref rid="SD1" ref-type="supplementary-material">Figure S2</xref> from the supplement. We also saw larger gains in predictive performance for the ensembling methods we evaluate when using greater numbers of trees per forest. For speed of computation, we used 10 trees per forest in all ensembles of our main analyses, so we anticipate that the improvements we report are conservative representations of what is possible with larger forests. We trained the Merged learner with the same number of trees contained in the total weighted ensembles, to keep the total number of trees for each method equal. Typically, this means that there are 100 trees per ensemble, unless otherwise specified.</p>
      </sec>
    </sec>
  </sec>
  <sec id="S7">
    <title>Simulation Results</title>
    <sec id="S8">
      <title>Goals.</title>
      <p id="P12">Our simulations evaluate the overall behavior of different tree-weighting approaches in realistic scenarios. The presence of interactions between variables associated with the outcome in the true model is prevalent in real datasets, including those arising in genomics. Identifying such interactions is typically difficult, and the information that could be leveraged by recognizing the utility of these terms is often lost. We were therefore interested in how the presence of interactions and the magnitude of their coefficients would affect the structure of trees trained by RF and the performance of the ensembles. We also study differences in the relationship between the covariates and the outcome across studies, and evaluate how these affect cross-study generalizability of trees within the ensemble.</p>
    </sec>
    <sec id="S9">
      <title>Baseline analyses.</title>
      <p id="P13">Throughout this section we report averages over 100 simulations per scenario, with 95% confidence bands computed as mean <italic>±</italic>1.96<italic>×</italic> standard error. We begin in <xref rid="F1" ref-type="fig">Figure 1A</xref> by detailing the relationship between the size of interaction terms and the variation in the outcome, to get a sense of the importance of interactions, at different levels of heterogeneity. We observe a roughly linear relationship between interaction strength and percent variation. The level of between-study heterogeneity does not affect this conclusion.</p>
      <p id="P14">We next examine the relationship between the level of between-study heterogeneity and the performance of the ensembling approaches. To interpret panels B and C of <xref rid="F1" ref-type="fig">Figure 1</xref>, we must first distinguish between “feature distribution heterogeneity” and “feature effect heterogeneity” (see also<sup><xref rid="R13" ref-type="bibr">13</xref></sup>). The former refers to changes in the distribution of features between datasets. The latter refers to changes in the relationship between the features and the outcome: in our simulations, it is captured by changes in the coefficients of the generating linear model, and is used throughout as the horizontal scale in the figures. At baseline, we explored removing feature distribution heterogeneity and evaluating ensembles at differing levels of feature effect heterogeneity. Both types of heterogeneity may influence the utility of ensembling, so these simulations helped us more sharply analyze feature effect heterogeneity alone.</p>
      <p id="P15">In the simulation used to generate <xref rid="F1" ref-type="fig">Figure 1B</xref>, we set <italic>K</italic> = 10. Each of the <italic>K</italic> studies has the same sample size and the same feature set obtained by repeating a single randomly chosen ovarian cancer dataset <italic>K</italic> times, thus removing feature distribution heterogeneity. Each iteration has a different level of feature effect heterogeneity. In each we train the Merged learner and the SSL’s and evaluate them on 5 independent datasets not used in training. The Weighting Trees and Weighting Forests approaches perform better than the Merged learner across all levels of heterogeneity considered, while the Unweighted method generally only slightly improves on the Merged. Training an SSL on each set of observations and reweighting upwards (upweighting) learners with better cross-study validation performance (which at level 0 is akin to cross-validation) produces markedly better results. The difference is most pronounced at levels of feature effect heterogeneity around 2, where it exceeds 10%, and is substantial at level 0. As the level of heterogeneity rises, the RMSE’s increase, and the difference in performance among approaches decreases. We also evaluated the performance of a single learner trained on one copy of the dataset chosen to provide the features in the training set, and found that in the absence of feature distribution heterogeneity, the ensembles using the stacking weights trained on the repeated training set outperform this single learner.</p>
      <p id="P16">The dataset used to train the Merged includes each sample in the training dataset 10 times, a somewhat artificial setting. Alternatively, for <xref rid="F1" ref-type="fig">Figure 1C</xref>, in each iteration, we split the TCGA study within curatedOvarianData (comprising 510 subjects) into 5 equally sized pseudo-datasets. We used 4 to train the merged learner and SSL’s, and the 5th to test the resulting ensembles. This simulation helped us investigate whether differences in the the distribution between smaller sub-datasets and the whole dataset would be sufficient to result in a better performance of ensembling approaches compared to merging. Both the Weighting Trees and Weighting Forests approaches outperform the Merged at all heterogeneity levels, while the Unweighted ensemble results in a decrease in performance.</p>
    </sec>
    <sec id="S10">
      <title>Overall performance of ensembling approaches.</title>
      <p id="P17">Our next simulations include varying levels of both feature distribution and feature effect heterogeneity, and additionally explore including interaction terms in the outcome-generating mechanism, representing the complexity we expect to see in real data. <xref rid="F2" ref-type="fig">Figure 2</xref> displays average RMSE’s over 100 iterations for the various ensembling approaches for three different scenarios. Out of 100 available features, 10 affect the continuous outcome. Panel A corresponds to interaction scenario (1) as described earlier, while panel B corresponds to scenario (2). Panel C includes no interactions and considers the effect of increasing the level of feature effect heterogeneity.</p>
      <p id="P18">Overall, both Weighting Trees and Weighting Forests perform significantly better than either of the simpler approaches considered as benchmarks. Interestingly, ensembling forests is only better than merging when we suitably weight each forest or tree, since the Unweighted approach consistently performs worse than the Merged. The Merged and Unweighted each give the same overall weight to each tree (1/100), but they differ in the number of candidate data points available to form a bootstrap sample to train each tree. We can extrapolate that training each tree on the merged dataset produces a predictor with better generalizability than restricting each tree to a single dataset and using simple averaging to combine the resulting forests. This motivates the utility of using weighting approaches that reward cross-study replicability when constructing an ensemble of ensembles. Otherwise, it may be both more efficient and accurate to rely on the ensembling produced within a single RF.</p>
      <p id="P19">Furthermore, the Weighting Trees strategy consistently performs better than Weighting Forests as well as the other baseline approaches. This follows the intuition that we can create a more robust ensemble by rewarding cross-study replicability at the individual tree level, by considering the strengths and weaknesses of individual trees rather than forests in terms of out-of-sample cross-study validation performance within the studies in the training collection. A comparison of panels A and B from <xref rid="F2" ref-type="fig">Figure 2</xref> indicates that scenarios with more training datasets containing interactions do not generally correspond to improved performance of any of the ensembling constructions considered; additionally, there is more variability around the average trend line. This suggests there is a balance in creating an optimally heterogeneous set of training studies for ensembling, and that the presence of heterogeneity could be more important than the sample size.</p>
      <p id="P20"><xref rid="F2" ref-type="fig">Figure 2C</xref> demonstrates that as the level of feature effect heterogeneity increases, the differences in performance of the ensembling approaches we considered decreases until they become virtually indistinguishable. At the lowest level of heterogeneity, we observe a significant separation between Weighting Trees, Weighting Forests and the simpler approaches. This suggests that in our data, there is intrinsic cross-study heterogeneity in the distribution of the features, that motivates utilizing replicability weights over merging or simple averaging, supporting the conclusions in the baseline simulations.</p>
    </sec>
    <sec id="S11">
      <title>Tree structure and variable importance measures.</title>
      <p id="P21">One of our primary interests in this study was to explore how the trees trained by Random Forest capture relationships between the variables and the outcome that are stable across studies, to elucidate which features are important to weighting schemes that reward replicability. To this end, we examined average trends across several different components of the internal tree structures. <xref rid="T1" ref-type="table">Table 1</xref> displays results about how tree structure and variable importance measures are related to tree weights. The ‘true’ variables are the variables chosen to linearly combine to generate the outcome in each simulation. All averages are taken over 100 simulations; 2 datasets in both the training and testing group contain interaction terms. For all ensembles, 9 variables per tree are available to create splits at each node; in the data generating mechanism, 3 variables out of the 10 associated with the outcome are involved in interaction terms.</p>
      <p id="P22">For both tree-based and forest-based ensembles, trees with larger weights also have a higher percentage of true variables, number of variables involved in interactions, and true variables with higher variable importance. Interestingly, the frequency of true variables and true interaction variables in trees in the Merged learner is higher than in the top decile for either Weighting Forests or Weighting Trees. Yet, the Merged has worse overall performance than both ensembling approaches. We speculate that these metrics of tree-level variable importance are imperfect markers of cross-study generalizability, as they do not address the stability of a variable’s effect which is required for replicability. The true variables in each iteration of the simulation are shared across all studies, so the frequency at which such variables appear within trees is not intrinsically a study-specific feature. Similarly, the variables chosen to be involved in interaction terms are shared across all studies chosen to contain interactions, trees trained on such studies share similarities across studies. However, the coefficients do vary, and cross-study learners attempt to build predictors that are more robust to this variation.</p>
      <p id="P23">The top decile of tree-based ensembles and forest-based ensembles contain on average nearly half of the 3 variables involved in interactions, with the tree-based average slightly higher than that for the forest-based ensemble. This is significantly higher than either the total average or the average within the lowest decile for either method, indicating that when the true relationship between the features involves interactions between variables, trees containing such variables have markedly increased cross-study generalizability and are therefore given higher weights within the ensemble.</p>
    </sec>
    <sec id="S12">
      <title>Distribution of tree-level weights.</title>
      <p id="P24">In general, the distributions of the weights given to individual trees have a similar location, but are more dispersed than those given to forests, and have a more pronounced upper tail. From <xref rid="T1" ref-type="table">Table 1</xref>, we see that for the Weighting Trees approach, the lowest decile of trees have weights that are 10-fold lower compared to the merged, while the top decile is approximately 7 times higher. The Weighting Forests method has a smaller inter-decile range. Forest-based ensembles do not allow for control over tree-level weights, resulting in a narrower overall distribution. The benefits afforded by a few trees are potentially averaged out in the performance of the corresponding forest and potentially not rewarded optimally. <xref rid="F3" ref-type="fig">Figure 3</xref> illustrates this. We implement stacking without a normalization of weights. Both tree- and forest-weighting approaches produce distributions centered above 0.01, the weight that would be given to each tree if all were equally weighted, as in the Merged and Unweighted. The distribution of the difference between the weight given to the same tree by the Weighting Forests and Weighting Trees approaches is centered around zero, but displays a pronounced lower tail, suggesting that Weighting Trees may identify a relatively small subset of trees for up-weighting, while reducing the weight of most others, compared to Weighting Forests. Consistently, as displayed in <xref rid="T1" ref-type="table">Table 1</xref>, the top decile of the Weighting Forests distribution is on average smaller than that for Weighting Trees while the overall means are roughly equal. Essentially, trees deemed by the Weighted Trees approach to be among the least beneficial in the ensemble may still receive moderate or high weight if the whole forest is weighted together. This may lead to the decrease in performance we see throughout.</p>
      <p id="P25">Several factors (both measured and unmeasured) contribute to improving cross-study generalizability of individual trees in addition to the presence of variables involved in interaction terms. It appears that the interplay between all such components contributes to determining the weight of each tree, and that isolating the presence or absence of any single one will not explain significant variation in weights.</p>
    </sec>
  </sec>
  <sec id="S13">
    <title>Data Application</title>
    <p id="P26">To explore the performance of these classifiers on real data with natural feature-outcome relationships, we considered relatively high-dimensional multi-study datasets similar to those used in our simulations, in which there may plausibly be inter-study heterogeneity and interactions between features. Gene expression and clinical data were natural candidates.</p>
    <p id="P27">We considered datasets in CuratedBreastData from Bioconductor in R, which provides gene expression and clinical data for 34 studies following patients with breast cancer.<sup><xref rid="R14" ref-type="bibr">14</xref></sup> Clinical information is binary; five studies measured Overall Survival (OS), our clinical outcome of interest. OS is defined as 1 if the patient survives to the end of the study period, and 0 otherwise. Study periods may vary, resulting in a further source of heterogeneity, affecting feature effects. The sample sizes range from 14 to 118, with a total of 336 subjects across studies. The five datasets have 76 gene features in common and 47 clinical features with fewer than 10 missing datapoints across studies. These 123 variables were used as features in the analysis. We tested the performance of the ensembling approaches when predicting OS while training on four studies and validating on the fifth, using log loss as our metric of success. Since randomForest trains a different set of trees at each iteration, we replicated the method 100 times to obtain average log loss values and standard errors.</p>
    <p id="P28">The results are pictured in <xref rid="F4" ref-type="fig">Figure 4A.1</xref>-<xref rid="F4" ref-type="fig">A.2</xref>. The Weighting Trees approach is the overall best for predicting OS. The Merged learner performs several orders of magnitude worse than the rest, in both relative and absolute terms. Conversely, the Unweighted, Weighting Trees, and Weighting Forests methods have low absolute prediction Log Loss. This agrees with the results from simulations using binary outcomes shown in <xref rid="SD1" ref-type="supplementary-material">Figure S1</xref> from the supplement.</p>
    <p id="P29">We also evaluated the performance of the ensembles on a continuous outcome. As there was no continuous clinical variable, we considered the task of predicting gene expression levels. Clinical decisions are often informed by expression levels of certain genes, thus predicting these levels when they may be missing can be useful in determining treatment. We used six studies in this analysis. The sample sizes range from 21 to 195 subjects, with 511 total across studies. The six datasets have 1,312 gene features in common. We predicted the expression level of one gene given the rest of the expression data, for the 500 most variable genes included in the features. Our performance metric was the percent change in average RMSE from the merged learner. The results, averaged across all 500 genes tested with associated 95% confidence intervals, are pictured in <xref rid="F4" ref-type="fig">Figure 4B</xref>. The general ordering the performance of the ensembling approaches follow that of the discrete outcome case, with the Weighting Trees approach outperforming the rest. It is slightly superior to Weighting Forests, and both surpass the Unweighted. All three ensembles significantly improve upon the Merged learner; this slightly differs from the results seen in the simulations, in which the Merged approach typically outperforms the Unweighted. This difference is likely due to different composition of the gene expression data in the breast cancer studies compared to that in the simulations, and provides more motivation for using ensembling over merging when dealing with multiple datasets.</p>
  </sec>
  <sec id="S14">
    <title>Discussion</title>
    <p id="P30">In this paper we proposed and investigated a methodology for building ensembles of trees trained on multiple studies, to achieve robustness to cross-study heterogeneity. Patil and Parmigiani<sup><xref rid="R2" ref-type="bibr">2</xref></sup> introduced a general multi-study learning architecture that uses stacking to combine predictions built on separate studies. In this setting, using Random Forests as a single study learner, we compared weighting each forest to form the ensemble, as they did, to extracting the individual trees trained by each Random Forest and weighting them directly. Our methodology thus extends their methods.</p>
    <p id="P31">Our results broadly indicate that Weighting Trees is often more effective than Weighting Forests, sometimes by a considerable margin. In turn, this suggests that more efficient families of cross-study learners can be constructed by “unpacking” learners that are themselves based on ensembles, and weighting the individual components directly. Our results may depend on the specific implementation of the stacking algorithm used to re-weight the trees. We used ridge regression stacking, but other approaches to regularization would be worth exploring.</p>
    <p id="P32">Our strategy can be applied whether or not the training sample is naturally divisible into different studies or sub-populations. We investigated this case as a limiting case when heterogeneity is close to 0. The findings from our baseline analysis (see <xref rid="F1" ref-type="fig">Figure 1</xref>) suggest that for larger datasets, it may be advantageous to train SSL’s on subsections and ensemble using stacking weights that reward prediction across different sections of the data, rather than simply training a learner on the whole dataset. Focusing SSL’s on fewer observations may allow the learners to capture more of the specific features of the dataset, which can then be combined in a way that promotes cross-study generalizability. Simply training one learner on the entire dataset may ignore potential heterogeneity in feature distributions between subsets of observations. Further exploration of the single study setting can be found in the supplement.</p>
  </sec>
  <sec sec-type="supplementary-material" id="SM1">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="SD1">
      <label>1</label>
      <media xlink:href="NIHMS1061174-supplement-1.pdf" orientation="portrait" xlink:type="simple" id="d36e852" position="anchor"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="S15">
    <title>Acknowledgements</title>
    <p id="P33">We thank Matt Ploenzke and Lorenzo Trippa for useful suggestions. Maya Ramchandran and Prasad Patil were supported by NIH-NCI Training Grant T32CA009337. Giovanni Parmigiani was supported by grants NSF-DMS 1810829 and NIH-NCI 4P30CA006516-51.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><name><surname>Bernau</surname><given-names>C</given-names></name>, <name><surname>Riester</surname><given-names>M</given-names></name>, <name><surname>Boulesteix</surname><given-names>A-L</given-names></name>, <name><surname>Parmigiani</surname><given-names>G</given-names></name>, <name><surname>Huttenhower</surname><given-names>C</given-names></name>, <name><surname>Waldron</surname><given-names>L</given-names></name> and <name><surname>Trippa</surname><given-names>L</given-names></name>, <article-title>Cross-study validation for the assessment of prediction algorithms</article-title>, <source>Bioinformatics</source>
<volume>30</volume>, <comment>i105</comment> (<year>2014</year>).</mixed-citation>
    </ref>
    <ref id="R2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><name><surname>Patil</surname><given-names>P</given-names></name> and <name><surname>Parmigiani</surname><given-names>G</given-names></name>, <article-title>Training replicable predictors in multiple studies</article-title>, <source>Proceedings of the National Academy of Sciences</source>
<volume>115</volume>, <fpage>2578</fpage> (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="R3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><name><surname>Breiman</surname><given-names>L</given-names></name>, <article-title>Random forests</article-title>, <source>Machine Learning</source><volume>45</volume>, <fpage>5</fpage> (<month>10</month>
<year>2001</year>).</mixed-citation>
    </ref>
    <ref id="R4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><name><surname>Breiman</surname><given-names>L</given-names></name>, <article-title>Bagging predictors</article-title>, <source>Machine Learning</source><volume>24</volume>, <fpage>123</fpage> (<month>8</month>
<year>1996</year>).</mixed-citation>
    </ref>
    <ref id="R5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><name><surname>Maudes</surname><given-names>J</given-names></name>, <name><surname>Rodŕíguez</surname><given-names>JJ</given-names></name>, <name><surname>Garćía-Osorio</surname><given-names>C</given-names></name> and <name><surname>Garćía-Pedrajas</surname><given-names>N</given-names></name>, <article-title>Random feature weights for decision tree ensemble construction</article-title>, <source>Inf. Fusion</source>
<volume>13</volume>, <fpage>20</fpage> (<month>1</month>
<year>2012</year>).</mixed-citation>
    </ref>
    <ref id="R6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><name><surname>Winham</surname><given-names>SJ</given-names></name>, <name><surname>Freimuth</surname><given-names>RR</given-names></name> and <name><surname>Biernacka</surname><given-names>JM</given-names></name>, <article-title>A weighted random forests approach to improve predictive performance</article-title>, <source>Statistical Analysis and Data Mining: The ASA Data Science Journal</source>
<volume>6</volume> (<year>2013</year>).</mixed-citation>
    </ref>
    <ref id="R7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><name><surname>Rahman</surname><given-names>R</given-names></name>, <name><surname>Haider</surname><given-names>S</given-names></name>, <name><surname>Ghosh</surname><given-names>S</given-names></name> and <name><surname>Pal</surname><given-names>R</given-names></name>, <article-title>Design of probabilistic random forests with applications to anticancer drug sensitivity prediction</article-title>, in <source>Cancer informatics</source>, <year>2015</year>.</mixed-citation>
    </ref>
    <ref id="R8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><name><surname>Kim</surname><given-names>H</given-names></name>, <name><surname>Kim</surname><given-names>H</given-names></name>, <name><surname>Moon</surname><given-names>H</given-names></name> and <name><surname>Ahn</surname><given-names>H</given-names></name>, <article-title>A weight-adjusted voting algorithm for ensembles of classifiers</article-title>, <source>Journal of the Korean Statistical Society</source>
<volume>40</volume>, <fpage>437</fpage> (<comment>12</comment>
<year>2011</year>).</mixed-citation>
    </ref>
    <ref id="R9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><name><surname>Basu</surname><given-names>S</given-names></name>, <name><surname>Kumbier</surname><given-names>K</given-names></name>, <name><surname>Brown</surname><given-names>JB</given-names></name> and <name><surname>Yu</surname><given-names>B</given-names></name>, <article-title>Iterative random forests to discover predictive and stable high-order interactions</article-title>, <source>Proceedings of the National Academy of Sciences</source>
<volume>115</volume> (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="R10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><name><surname>Ganzfried</surname><given-names>BF</given-names></name>, <name><surname>Riester</surname><given-names>M</given-names></name>, <name><surname>Haibe-Kains</surname><given-names>B</given-names></name>, <name><surname>Risch</surname><given-names>T</given-names></name>, <name><surname>Tyekucheva</surname><given-names>S</given-names></name>, <name><surname>Jazic</surname><given-names>I</given-names></name>, <name><surname>Wang</surname><given-names>XV</given-names></name>, <name><surname>Ahmadifar</surname><given-names>M</given-names></name>, <name><surname>Birrer</surname><given-names>MJ</given-names></name>, <name><surname>Parmigiani</surname><given-names>G</given-names></name>, <name><surname>Huttenhower</surname><given-names>C</given-names></name> and <name><surname>Waldron</surname><given-names>L</given-names></name>, <article-title>curatedOvarianData: clinically annotated data for the ovarian cancer transcriptome</article-title>., <source>Database (Oxford) 2013, p. bat013</source> (<year>2013</year>), PMCID: .<?supplied-pmid PMC3625954?></mixed-citation>
    </ref>
    <ref id="R11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><name><surname>Breiman</surname><given-names>L</given-names></name>, <article-title>Stacked regressions</article-title>, <source>Machine Learning</source><volume>24</volume>, <fpage>49</fpage> (<month>7</month>
<year>1996</year>).</mixed-citation>
    </ref>
    <ref id="R12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><name><surname>Friedman</surname><given-names>J</given-names></name>, <name><surname>Hastie</surname><given-names>T</given-names></name> and <name><surname>Tibshirani</surname><given-names>R</given-names></name>, <article-title>Regularization paths for generalized linear models via coordinate descent</article-title>, <source>Journal of Statistical Software</source>
<volume>33</volume>, <fpage>1</fpage> (<year>2010</year>).<pub-id pub-id-type="pmid">20808728</pub-id></mixed-citation>
    </ref>
    <ref id="R13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Bernau</surname><given-names>C</given-names></name>, <name><surname>Parmigiani</surname><given-names>G</given-names></name> and <name><surname>Waldron</surname><given-names>L</given-names></name>, <article-title>The impact of different sources of heterogeneity on loss of accuracy from genomic prediction models</article-title>, <source>Biostatistics (Oxford, England)</source>
<volume>6</volume>, p. <fpage>701</fpage> (<month>9</month>
<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="R14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><name><surname>Planey</surname><given-names>CR</given-names></name> and <name><surname>Gevaert</surname><given-names>O</given-names></name>, <article-title>Coincide: A framework for discovery of patient subtypes across multiple datasets</article-title>, <source>Genome Medicine</source>
<volume>8</volume> (<month>3</month>
<year>2016</year>).</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="F1" orientation="portrait" position="float">
    <label>Fig. 1:</label>
    <caption>
      <p id="P34">Baseline analyses. <bold>(A)</bold> Percent variation in outcome explained by interactions, as a function of magnitude of interaction coefficients in the generating model. <bold>(B)</bold>-<bold>(C)</bold> Percent change in average RMSE of each ensembling approach compared to the Merged learner, as a function of between- study heterogeneity level. In <bold>(B)</bold> all training sets have identical feature distributions while in <bold>(C)</bold> the TCGA study is randomly split into 5 sub-datasets at every iteration for training and testing. Weighting Trees and Weighting Forests significantly improve upon Merged, with the difference in performance decreasing as heterogeneity increases. Smoothing is applied to reduce simulation noise.</p>
    </caption>
    <graphic xlink:href="nihms-1061174-f0001"/>
  </fig>
  <fig id="F2" orientation="portrait" position="float">
    <label>Fig. 2:</label>
    <caption>
      <p id="P35">Average RMSE’s of ensembling approaches (color labeled) across different data-generating scenarios, as a function of increasing interaction strength or heterogeneity. <bold>(A)</bold> 2 datasets with interaction terms between features in the outcome-generating generating mechanism are included in the training set, and 2 are included in the testing set. <bold>(B)</bold> 6 datasets with interactions are included in the training set, 2 in the testing set. <bold>(C)</bold> No datasets with interaction terms are included in either training or testing, and performance is evaluated for increasing feature effect heterogeneity.</p>
    </caption>
    <graphic xlink:href="nihms-1061174-f0002"/>
  </fig>
  <fig id="F3" orientation="portrait" position="float">
    <label>Fig. 3:</label>
    <caption>
      <p id="P36">Distribution of tree-level weights using the Weighting Trees or Weighting Forests methods, as well as their difference. 2 training sets contain interaction terms. Tree-level weights for Weighting Forests are obtained by dividing the forest-level weight returned by the stacking algorithm by the number of trees per forest. Correspondingly, each point in Weighting Forests represents the value of the weight given to 10 trees. The dashed red line at <italic>y</italic> = .01 represents the weight given to every tree within the Merged and Unweighted.</p>
    </caption>
    <graphic xlink:href="nihms-1061174-f0003"/>
  </fig>
  <fig id="F4" orientation="portrait" position="float">
    <label>Fig. 4:</label>
    <caption>
      <p id="P37">Performance of ensembling approaches on the breast cancer datasets in the multi-study setting, with associated 95% confidence intervals. <bold>(A.1)</bold> Average percent change in prediction Log Loss from the Merged for each of the ensembling approaches on the binary outcome variable, Overall Survival (OS). Confidence intervals were obtained by training each of the ensembling approaches 100 times, with differences in performance across iterations induced by the randomization within the Random Forest algorithm. <bold>(A.2)</bold> A view of panel A.1, without the Merged learner to improve scaling, so differences between the ensembles can be clearly visualized. <bold>(B)</bold> Average percent change in RMSE from the Merged when predicting expression levels for each of the top 500 variable genes given the rest of the gene expression data. The standard errors were therefore computed over 500 samples, as opposed to the 100 in panel A.2.</p>
    </caption>
    <graphic xlink:href="nihms-1061174-f0004"/>
  </fig>
  <table-wrap id="T1" position="float" orientation="portrait">
    <label>Table 1:</label>
    <caption>
      <p id="P38">Tree structure summaries and variable importance measures for each ensembling approach, by tree weight. The left three columns are average tree-level measures. The rightmost column measures the proportion of the total sum of variable importance scores attributed to true variables. Variable importance is computed as follows: for each tree, the prediction error on the out-of-bag portion of the data is recorded (using MSE for regression). Then the same is done after permuting each predictor variable. The difference between the two is then averaged over all trees, and normalized by the standard deviation of the differences.</p>
    </caption>
    <table frame="hsides" rules="none">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">Tree weight<break/> percentile</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Average<break/> tree weight</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Freq. of true<break/> variables/tree</th>
          <th align="left" valign="top" rowspan="1" colspan="1"># of variables<break/> in interactions</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Proportion of total<break/> varImp by true vars.</th>
        </tr>
        <tr>
          <th colspan="5" align="left" valign="top" rowspan="1">
            <hr/>
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td colspan="5" align="center" valign="top" rowspan="1">Weighting Trees</td>
        </tr>
        <tr>
          <td colspan="5" align="center" valign="top" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td align="right" valign="top" rowspan="1" colspan="1">90–100%</td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.068</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.402</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>1.459</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.699</bold>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1"/>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.0675, 0.0685)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.400, 0.404)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(1.443, 1.475)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.671, 0.727)</td>
        </tr>
        <tr>
          <td align="right" valign="top" rowspan="1" colspan="1">0–10%</td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.001</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.143</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.48</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.502</bold>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1"/>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.001, 0.001)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.141, 0.145)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.468, 0.492)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.477, 0.527)</td>
        </tr>
        <tr>
          <td align="right" valign="top" rowspan="1" colspan="1">0–100%</td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.026</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.265</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.948</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.655</bold>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1"/>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.0259, 0.0261)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.264, 0.266)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.937, 0.959)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.627, 0.683)</td>
        </tr>
        <tr>
          <td colspan="5" align="left" valign="top" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td colspan="5" align="center" valign="top" rowspan="1">Weighting Forests</td>
        </tr>
        <tr>
          <td colspan="5" align="left" valign="top" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td align="right" valign="top" rowspan="1" colspan="1">90–100%</td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.0463</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.368</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>1.348</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.704</bold>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1"/>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.0458, 0.0468)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.364, 0.372)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(1.326, 1.37)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.675, 0.733)</td>
        </tr>
        <tr>
          <td align="right" valign="top" rowspan="1" colspan="1">0–10%</td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.0091</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.171</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.571</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.512</bold>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1"/>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.0089, 0.0093)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.169, 0.173)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.558, 0.584)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.486, 0.538)</td>
        </tr>
        <tr>
          <td align="right" valign="top" rowspan="1" colspan="1">0–100%</td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.0256</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.265</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.948</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.655</bold>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1"/>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.0254, 0.0258)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.264, 0.266)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.937, 0.959)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.627, 0.683)</td>
        </tr>
        <tr>
          <td colspan="5" align="left" valign="top" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td colspan="5" align="center" valign="top" rowspan="1">Merged</td>
        </tr>
        <tr>
          <td colspan="5" align="left" valign="top" rowspan="1">
            <hr/>
          </td>
        </tr>
        <tr>
          <td align="right" valign="top" rowspan="1" colspan="1">0–100%</td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.01</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.504</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>1.889</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>0.624</bold>
          </td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1"/>
          <td align="left" valign="top" rowspan="1" colspan="1"/>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.502, 0.506)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(1.869, 1.909)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(0.600, 0.648)</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
