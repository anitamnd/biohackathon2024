<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_COMM106236 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEgr6 jpg ?>
<?FILEgr7 jpg ?>
<?FILEfx1 jpg ?>
<?FILEfx2 jpg ?>
<?FILEfx3 jpg ?>
<?FILEfx4 jpg ?>
<?FILEfx5 jpg ?>
<?FILEfx6 jpg ?>
<?FILEfx7 jpg ?>
<?FILEfx8 jpg ?>
<?FILEsi1 svg ?>
<?FILEsi2 svg ?>
<?FILEsi3 svg ?>
<?FILEsi4 svg ?>
<?FILEsi5 svg ?>
<?FILEsi6 svg ?>
<?FILEsi7 svg ?>
<?FILEsi8 svg ?>
<?FILEsi9 svg ?>
<?FILEsi10 svg ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Comput Methods Programs Biomed</journal-id>
    <journal-id journal-id-type="iso-abbrev">Comput Methods Programs Biomed</journal-id>
    <journal-title-group>
      <journal-title>Computer Methods and Programs in Biomedicine</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0169-2607</issn>
    <issn pub-type="epub">1872-7565</issn>
    <publisher>
      <publisher-name>Elsevier Scientific Publishers</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8542803</article-id>
    <article-id pub-id-type="pii">S0169-2607(21)00310-2</article-id>
    <article-id pub-id-type="doi">10.1016/j.cmpb.2021.106236</article-id>
    <article-id pub-id-type="publisher-id">106236</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au0001">
        <name>
          <surname>Pérez-García</surname>
          <given-names>Fernando</given-names>
        </name>
        <email>fernando.perezgarcia.17@ucl.ac.uk</email>
        <xref rid="aff0001" ref-type="aff">a</xref>
        <xref rid="aff0002" ref-type="aff">b</xref>
        <xref rid="aff0003" ref-type="aff">c</xref>
        <xref rid="cor0001" ref-type="corresp">⁎</xref>
      </contrib>
      <contrib contrib-type="author" id="au0002">
        <name>
          <surname>Sparks</surname>
          <given-names>Rachel</given-names>
        </name>
        <xref rid="aff0003" ref-type="aff">c</xref>
      </contrib>
      <contrib contrib-type="author" id="au0003">
        <name>
          <surname>Ourselin</surname>
          <given-names>Sébastien</given-names>
        </name>
        <xref rid="aff0003" ref-type="aff">c</xref>
      </contrib>
      <aff id="aff0001"><label>a</label>Department of Medical Physics and Biomedical Engineering, University College London, UK</aff>
      <aff id="aff0002"><label>b</label>Wellcome / EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, UK</aff>
      <aff id="aff0003"><label>c</label>School of Biomedical Engineering &amp; Imaging Sciences (BMEIS), King’s College London, UK</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor0001"><label>⁎</label>Corresponding author. <email>fernando.perezgarcia.17@ucl.ac.uk</email></corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>1</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="ppub">.-->
    <pub-date pub-type="ppub">
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <volume>208</volume>
    <elocation-id>106236</elocation-id>
    <history>
      <date date-type="received">
        <day>2</day>
        <month>12</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>9</day>
        <month>6</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 The Author(s)</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p>
      </license>
    </permissions>
    <abstract abstract-type="author-highlights" id="absh001">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="lst0001">
          <list-item id="lstitem0001">
            <label>•</label>
            <p id="p0001">Open-source Python library for preprocessing, augmentation and sampling of medical images for deep learning.</p>
          </list-item>
          <list-item id="lstitem0002">
            <label>•</label>
            <p id="p0002">Support for 2D, 3D and 4D images such as X-ray, histopathology, CT, ultrasound and diffusion MRI.</p>
          </list-item>
          <list-item id="lstitem0003">
            <label>•</label>
            <p id="p0003">Modular design inspired by the deep learning framework PyTorch.</p>
          </list-item>
          <list-item id="lstitem0004">
            <label>•</label>
            <p id="p0004">Focus on reproducibility and traceability to encourage open-science practices.</p>
          </list-item>
          <list-item id="lstitem0005">
            <label>•</label>
            <p id="p0005">Compatible with related frameworks for medical image processing with deep learning.</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <abstract id="abs0001">
      <sec>
        <title>Background and objective</title>
        <p>Processing of medical images such as MRI or CT presents different challenges compared to RGB images typically used in computer vision. These include a lack of labels for large datasets, high computational costs, and the need of metadata to describe the physical properties of voxels. Data augmentation is used to artificially increase the size of the training datasets. Training with image subvolumes or patches decreases the need for computational power. Spatial metadata needs to be carefully taken into account in order to ensure a correct alignment and orientation of volumes.</p>
      </sec>
      <sec>
        <title>Methods</title>
        <p>We present TorchIO, an open-source Python library to enable efficient loading, preprocessing, augmentation and patch-based sampling of medical images for deep learning. TorchIO follows the style of PyTorch and integrates standard medical image processing libraries to efficiently process images during training of neural networks. TorchIO transforms can be easily composed, reproduced, traced and extended. Most transforms can be inverted, making the library suitable for test-time augmentation and estimation of aleatoric uncertainty in the context of segmentation. We provide multiple generic preprocessing and augmentation operations as well as simulation of MRI-specific artifacts.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>Source code, comprehensive tutorials and extensive documentation for TorchIO can be found at <ext-link ext-link-type="uri" xlink:href="http://torchio.rtfd.io/" id="PC_linkZ20kUlM4vg">http://torchio.rtfd.io/</ext-link>. The package can be installed from the Python Package Index (PyPI) running pip install torchio. It includes a command-line interface which allows users to apply transforms to image files without using Python. Additionally, we provide a graphical user interface within a TorchIO extension in 3D Slicer to visualize the effects of transforms.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p>TorchIO was developed to help researchers standardize medical image processing pipelines and allow them to focus on the deep learning experiments. It encourages good open-science practices, as it supports experiment reproducibility and is version-controlled so that the software can be cited precisely. Due to its modularity, the library is compatible with other frameworks for deep learning with medical images.</p>
      </sec>
    </abstract>
    <kwd-group id="keys0001">
      <title>Keywords</title>
      <kwd>Medical image computing</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Data augmentation</kwd>
      <kwd>Preprocessing</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec0001">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0006">Recently, deep learning has become a ubiquitous research approach for solving image understanding and analysis problems. Convolutional neural networks (CNNs) have become the state of the art for many medical imaging tasks including segmentation <xref rid="bib0001" ref-type="bibr">[1]</xref>, classification <xref rid="bib0002" ref-type="bibr">[2]</xref>, reconstruction <xref rid="bib0003" ref-type="bibr">[3]</xref> and registration <xref rid="bib0004" ref-type="bibr">[4]</xref>. Many of the network architectures and techniques have been adopted from computer vision.</p>
    <p id="p0008">Compared to 2D red-green-blue (RGB) images typically used in computer vision, processing of medical images such as MRI, ultrasound (US) or CT presents different challenges. These include a lack of labels for large datasets, high computational costs (as the data is typically volumetric), and the use of metadata to describe the physical size and position of voxels.</p>
    <p id="p0009">Open-source frameworks for training CNNs with medical images have been built on top of TensorFlow <xref rid="bib0005" ref-type="bibr">[5]</xref>, <xref rid="bib0006" ref-type="bibr">[6]</xref>, <xref rid="bib0007" ref-type="bibr">[7]</xref>. Recently, the popularity of PyTorch <xref rid="bib0008" ref-type="bibr">[8]</xref> has increased among researchers due to its improved usability compared to TensorFlow <xref rid="bib0009" ref-type="bibr">[9]</xref>, driving the need for open-source tools compatible with PyTorch. To reduce duplication of effort among research groups, improve experimental reproducibility and encourage open-science practices, we have developed TorchIO: an open-source Python library for efficient loading, preprocessing, augmentation, and patch-based sampling of medical images designed to be integrated into deep learning workflows.</p>
    <p id="p0010">TorchIO is a compact and modular library that can be seamlessly used alongside higher-level deep learning frameworks for medical imaging, such as the Medical Open Network for AI (MONAI). It removes the need for researchers to code their own preprocessing pipelines from scratch, which might be error-prone due to the complexity of medical image representations. Instead, it allows researchers to focus on their experiments, supporting experiment reproducibility and traceability of their work, and standardization of the methods used to process medical images for deep learning.</p>
    <sec id="sec0002">
      <label>1.1</label>
      <title>Motivation</title>
      <p id="p0011">The nature of medical images makes it difficult to rely on a typical computer-vision pipeline for neural network training. In <xref rid="sec0003" ref-type="sec">Section 1.1.1</xref>, we describe challenges related to medical images that need to be overcome when designing deep learning workflows. In <xref rid="sec0004" ref-type="sec">Section 1.1.2</xref>, we justify the choice of PyTorch as the main deep learning framework dependency of TorchIO.</p>
      <sec id="sec0003">
        <label>1.1.1</label>
        <title>Challenges in medical image processing for deep learning</title>
        <p id="p0012">In practice, multiple challenges must be addressed when developing deep learning algorithms for medical images: 1) handling metadata related to physical position and size, 2) lack of large labeled datasets, 3) high computational costs due to data multidimensionality and 4) lack of consensus for best normalization practices. These challenges are very common in medical imaging and require certain features that may not be implemented in more general-purpose image processing frameworks such as Albumentations <xref rid="bib0010" ref-type="bibr">[10]</xref> or TorchVision <xref rid="bib0008" ref-type="bibr">[8]</xref>.</p>
        <sec id="sec0003b">
          <title>Metadata</title>
          <p id="p0014">In computer vision, picture elements, or <italic>pixels</italic>, which are assumed to be square, have a spatial relationship that comprises proximity and depth according to both the arrangement of objects in the scene and camera placement. In comparison, medical images are reconstructed such that the location of volume elements, or cuboid-shaped <italic>voxels</italic>, encodes a meaningful 3D spatial relationship. In simple terms, for 2D natural images, pixel vicinity does not necessarily indicate spatial correspondence, while for medical images spatial correspondence between nearby voxels can often be assumed.</p>
          <p id="p0015">Metadata, which encodes the physical size, spacing, and orientation of voxels, determines spatial relationships between voxels <xref rid="bib0011" ref-type="bibr">[11]</xref>. This information can provide meaningful context when performing medical image processing, and is often implicitly or explicitly used in medical imaging software. Furthermore, metadata is often used to determine correspondence between images as well as voxels within an image. For example, registration algorithms for medical images typically work with physical coordinates rather than voxel indices.</p>
          <p id="p0016"><xref rid="fig0001" ref-type="fig">Fig. 1</xref> shows the superposition of an MRI and a corresponding brain parcellation <xref rid="bib0012" ref-type="bibr">[12]</xref> with the same size (<inline-formula><mml:math id="M1" altimg="si1.svg"><mml:mrow><mml:mn>181</mml:mn><mml:mo linebreak="goodbreak">×</mml:mo><mml:mn>181</mml:mn></mml:mrow></mml:math></inline-formula>) but different origin, spacing and orientation. A native user would assume that, given that the superimposition looks correct and both images have the same size, they are ready for training. However, the visualization is correct only because 3D Slicer <xref rid="bib0013" ref-type="bibr">[13]</xref>, the software used for visualization, is aware of the spatial metadata of the images. As CNNs generally do not take spatial metadata into account, training using these images without preprocessing would lead to poor results.<fig id="fig0001"><label>Fig. 1</label><caption><p>Demonstration of the importance of spatial metadata in medical image processing. The size of both the MRI and the segmentation is <inline-formula><mml:math id="M2" altimg="si1.svg"><mml:mrow><mml:mn>181</mml:mn><mml:mo linebreak="goodbreak">×</mml:mo><mml:mn>181</mml:mn></mml:mrow></mml:math></inline-formula>. When spatial metadata is taken into account (a), images are correctly superimposed (only the borders of each region are shown for clarity purposes). Images are incorrectly superimposed if (b) origin, (c) orientation or (d) spacing are ignored.</p></caption><alt-text id="at0001">Fig. 1</alt-text><graphic xlink:href="gr1"/></fig></p>
          <p id="p0017">Medical images are typically stored in specialized formats such as Data Imaging and Communications in Medicine (DICOM) or Neuroimaging Informatics Technology Initiative (NIfTI) <xref rid="bib0011" ref-type="bibr">[11]</xref>, and commonly read and processed by medical imaging frameworks such as SimpleITK <xref rid="bib0014" ref-type="bibr">[14]</xref> or NiBabel <xref rid="bib0015" ref-type="bibr">[15]</xref>.</p>
          <p id="p0018"><italic>Limited training data.</italic> Deep learning methods typically require large amounts of annotated data, which are often scarce in clinical scenarios due to concerns over patient privacy, the financial and time burden associated with collecting data as part of a clinical trial, and the need for annotations from highly-trained and experienced raters. Data augmentation techniques can be used to increase the size of the training dataset artificially by applying different transformations to each training instance while preserving the relationship to annotations.</p>
          <p id="p0019">Data augmentation performed in computer vision typically aims to simulate variations in camera properties, field of view (FOV), or perspective. Traditional data augmentation operations applied in computer vision include geometrical transforms such as random rotation or zoom, color-space transforms such as random channel swapping or kernel filtering such as random Gaussian blurring. Data augmentation is usually performed on the fly, i.e., every time an image is loaded from disk during training.</p>
          <p id="p0020">Several computer vision libraries supporting data augmentation have appeared recently, such as Albumentations <xref rid="bib0010" ref-type="bibr">[10]</xref>, or imgaug <xref rid="bib0016" ref-type="bibr">[16]</xref>. PyTorch also includes some computer vision transforms, mostly implemented as Pillow wrappers <xref rid="bib0017" ref-type="bibr">[17]</xref>. However, none of these libraries support reading or transformations for 3D images. Furthermore, medical images are almost always grayscale, therefore color-space transforms are not applicable. Additionally, cropping and scaling are more challenging to apply to medical images without affecting the spatial relationships of the data. Metadata should usually be considered when applying these transformations to medical images.</p>
          <p id="p0021">In medical imaging, the purpose of data augmentation is designed to simulate anatomical variations and scanner artifacts. Anatomical variation and sample position can be simulated using spatial transforms such as elastic deformation, lateral flipping, or affine transformations. Some artifacts are unique to specific medical image modalities. For example, ghosting artifacts will be present in MRI if the patient moves during acquisition, and metallic implants often produce streak artifacts in CT. Simulation of these artifacts can be useful when performing augmentation on medical images.</p>
          <p id="p0022"><italic>Computational costs.</italic> The number of pixels in 2D images used in deep learning is rarely larger than one million. For example, the input size of several popular image classification models is <inline-formula><mml:math id="M3" altimg="si2.svg"><mml:mrow><mml:mn>224</mml:mn><mml:mo linebreak="goodbreak">×</mml:mo><mml:mn>224</mml:mn><mml:mo linebreak="goodbreak">×</mml:mo><mml:mn>3</mml:mn><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>150</mml:mn><mml:mspace width="0.28em"/><mml:mn>528</mml:mn></mml:mrow></mml:math></inline-formula> pixels (588 KiB if 32 bits per pixel are used). In contrast, 3D medical images often contain hundreds of millions of voxels, and downsampling might not be acceptable when small details should be preserved. For example, the size of a high-resolution lung CT-scan used for quantifying chronic obstructive pulmonary disease (COPD) damage in a research setting, with spacing <inline-formula><mml:math id="M4" altimg="si3.svg"><mml:mrow><mml:mn>0.66</mml:mn><mml:mo linebreak="goodbreak">×</mml:mo><mml:mn>0.66</mml:mn><mml:mo linebreak="goodbreak">×</mml:mo><mml:mn>0.30</mml:mn></mml:mrow></mml:math></inline-formula> mm, is <inline-formula><mml:math id="M5" altimg="si4.svg"><mml:mrow><mml:mn>512</mml:mn><mml:mo linebreak="goodbreak">×</mml:mo><mml:mn>512</mml:mn><mml:mo linebreak="goodbreak">×</mml:mo><mml:mn>1069</mml:mn><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>280</mml:mn><mml:mspace width="0.28em"/><mml:mn>231</mml:mn><mml:mspace width="0.28em"/><mml:mn>936</mml:mn></mml:mrow></mml:math></inline-formula> voxels (1.04 GiB if 32 bits per voxel are used).</p>
          <p id="p0023">In computer vision applications, images used for training are grouped in batches whose size is often in the order of hundreds <xref rid="bib0018" ref-type="bibr">[18]</xref> or even thousands <xref rid="bib0019" ref-type="bibr">[19]</xref> of training instances, depending on the available graphics processing unit (GPU) memory. In medical image applications, batches rarely contain more than one <xref rid="bib0001" ref-type="bibr">[1]</xref> or two <xref rid="bib0020" ref-type="bibr">[20]</xref> training instances due to their larger memory footprint compared to natural images. This reduces the utility of techniques such as batch normalization, which rely on batches being large enough to estimate dataset variance appropriately <xref rid="bib0021" ref-type="bibr">[21]</xref>. Moreover, large image size and small batches result in longer training time, hindering the experimental cycle that is necessary for hyperparameter optimization. In cases where GPU memory is limited and the network architecture is large, it is possible that not even the entirety of a single volume can be processed during a training iteration. To overcome this challenge, it is common in medical imaging to train using subsets of the image, or image <italic>patches</italic>, randomly extracted from the volumes.</p>
          <p id="p0024">Networks can be trained with 2D slices extracted from 3D volumes, aggregating the inference results to generate a 3D volume <xref rid="bib0022" ref-type="bibr">[22]</xref>. This can be seen as a specific case of patch-based training, where the size of the patches along a dimension is one. Other methods extract volumetric patches for training, that are often cubes, if the voxel spacing is isotropic <xref rid="bib0023" ref-type="bibr">[23]</xref>, or cuboids adapted to the anisotropic spacing of the training images <xref rid="bib0024" ref-type="bibr">[24]</xref>.</p>
          <p id="p0025"><italic>Transfer learning and normalization.</italic> One can pre-train a network on a large dataset of natural images such as ImageNet <xref rid="bib0025" ref-type="bibr">[25]</xref>, which contains more than 14 million labeled images, and fine-tune on a custom, much smaller target dataset. This is a typical use of transfer learning in computer vision <xref rid="bib0026" ref-type="bibr">[26]</xref>. The literature has reported mixed results using transfer learning to apply models pretrained on natural images to medical images <xref rid="bib0027" ref-type="bibr">[27]</xref>, <xref rid="bib0028" ref-type="bibr">[28]</xref>.</p>
          <p id="p0027">In computer vision, best practice is to normalize each training instance before training, using statistics computed from the whole training dataset <xref rid="bib0018" ref-type="bibr">[18]</xref>. Preprocessing of medical images is often performed on a per-image basis, and best practice is to take into account the bimodal nature of medical images (i.e., that an image has a background and a foreground).</p>
          <p id="p0028">Medical image voxel intensity values can be encoded with different data types and intensity ranges, and the meaning of a specific value can vary between different modalities, sequence acquisitions, or scanners. Therefore, intensity normalization methods for medical images often involve more complex parameterization of intensities than those used for natural images <xref rid="bib0029" ref-type="bibr">[29]</xref>.</p>
        </sec>
        <sec id="sec0004">
          <label>1.1.2</label>
          <title>Deep learning frameworks</title>
          <p id="p0029">There are currently two major generic deep learning frameworks: TensorFlow <xref rid="bib0005" ref-type="bibr">[5]</xref> and PyTorch <xref rid="bib0008" ref-type="bibr">[8]</xref>, primarily maintained by Google and Facebook, respectively. Although TensorFlow has traditionally been the primary choice for both research and industry, PyTorch has recently seen a substantial increase in popularity, especially among the research community <xref rid="bib0009" ref-type="bibr">[9]</xref>.</p>
          <p id="p0030">PyTorch is often preferred by the research community as it is <italic>pythonic</italic>, i.e., its design, usage, and application programming interfaceAPI follow the conventions of plain Python. Moreover, the API for tensor operations follows a similar paradigm to the one for NumPy multidimensional arrays, which is the primary array programming library for the Python language <xref rid="bib0030" ref-type="bibr">[30]</xref>. In contrast, for TensorFlow, researchers need to become familiar with new design elements such as sessions, placeholders, feed dictionaries, gradient tapes and static graphs. In PyTorch, objects are standard Python classes and variables, and a dynamic graph makes debugging intuitive and familiar to anyone already using Python. These differences have decreased with the recent release of TensorFlow 2, whose eager mode makes usage reminiscent of Python.</p>
          <p id="p0031">TorchIO was designed to be in the style of PyTorch and uses several of its tools to reduce the barrier to learning how to use TorchIO for those researchers already familiar with PyTorch.</p>
        </sec>
      </sec>
      <sec id="sec0005">
        <label>1.2</label>
        <title>Related work</title>
        <p id="p0032">NiftyNet <xref rid="bib0007" ref-type="bibr">[7]</xref> and the Deep Learning Toolkit (DLTK) <xref rid="bib0006" ref-type="bibr">[6]</xref> are deep learning frameworks designed explicitly for medical image processing using the TensorFlow 1 platform. Both of them are no longer being actively maintained. They provide implementations of some popular network architectures such as U-Net <xref rid="bib0001" ref-type="bibr">[1]</xref>, and can be used to train 3D CNNs for different tasks. For example, NiftyNet was used to train a 3D residual network for brain parcellation <xref rid="bib0023" ref-type="bibr">[23]</xref>, and DLTK was used to perform multi-organ segmentation on CT and MRI <xref rid="bib0031" ref-type="bibr">[31]</xref>.</p>
        <p id="p0033">The medicaltorch library <xref rid="bib0032" ref-type="bibr">[32]</xref> closely follows the PyTorch design, and provides some functionalities for preprocessing, augmentation and training of medical images. However, it does not leverage the power of specialized medical image processing libraries, such as SimpleITK <xref rid="bib0014" ref-type="bibr">[14]</xref>, to process volumetric images.</p>
        <p id="p0034">Similar to DLTK, this library has not seen much activity since 2018.</p>
        <p id="p0035">The batchgenerators library <xref rid="bib0033" ref-type="bibr">[33]</xref>, used within the popular medical segmentation framework nn-UNet <xref rid="bib0034" ref-type="bibr">[34]</xref>, includes custom dataset and data loader classes for multithreaded loading of 3D medical images, implemented before data loaders were available in PyTorch. In the usage examples from GitHub, preprocessing is applied to the whole dataset before training. Then, spatial data augmentation is performed at the volume level, from which one patch is extracted and intensity augmentation is performed at the patch level. In this approach, only one patch is extracted per volume, diminishing the efficiency of training pipelines. Transforms in batchgenerators are mostly implemented using NumPy <xref rid="bib0030" ref-type="bibr">[30]</xref> and SciPy <xref rid="bib0035" ref-type="bibr">[35]</xref>.</p>
        <p id="p0036">More recently, a few PyTorch-based libraries for deep learning and medical images have appeared. There are two other libraries, developed in parallel to TorchIO, focused on data preprocessing and augmentation. Rising<xref rid="fn0001" ref-type="fn">1</xref> is a library for data augmentation entirely written in PyTorch, which allows for gradients to be propagated through the transformations and perform all computations on the GPU. However, this means specialized medical imaging libraries such as SimpleITK cannot be used. pymia <xref rid="bib0036" ref-type="bibr">[36]</xref> provides features for data handling (loading, preprocessing, sampling) and evaluation. It is compatible with TorchIO transforms, which are typically leveraged for data augmentation, as their data handling is more focused on preprocessing. pymia can be easily integrated into either PyTorch or TensorFlow pipelines. It was recently used to assess the suitability of evaluation metrics for medical image segmentation <xref rid="bib0037" ref-type="bibr">[37]</xref>.</p>
        <p id="p0037">MONAI <xref rid="bib0038" ref-type="bibr">[38]</xref> and Eisen <xref rid="bib0039" ref-type="bibr">[39]</xref> are PyTorch-based frameworks for deep learning workflows with medical images. Similar to NiftyNet and DLTK, they include implementation of network architectures, transforms, and higher-level features to perform training and inference. For example, MONAI was recently used for brain segmentation on fetal MRI <xref rid="bib0040" ref-type="bibr">[40]</xref>. As these packages are solving a large problem, i.e., that of workflow in deep learning for medical images, they do not contain all of the data augmentation transforms present in TorchIO. However, it is important to note that an end user does not need to select only one open-source package, as TorchIO transforms are compatible with both Eisen and MONAI.</p>
        <p id="p0038">TorchIO is a library that specializes in preprocessing and augmentation using PyTorch, focusing on ease of use for researchers. This is achieved by providing a PyTorch-like API, comprehensive documentation with many usage examples, and tutorials showcasing different features, and by actively addressing feature requests and bug reports from the many users that have already adopted TorchIO. This is in contrast with other modern libraries released after TorchIO such as MONAI, which aims to deliver a larger umbrella of functionalities including federated learning or active learning, but may have slower development and deployment.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec0006">
    <label>2</label>
    <title>Methods</title>
    <p id="p0039">We developed TorchIO, a Python library that focuses on data loading and augmentation of medical images in the context of deep learning.</p>
    <p id="p0040">TorchIO is a unified library to load and augment data that makes explicit use of medical image properties, and is flexible enough to be used for different loading workflows. It can accelerate research by avoiding the need to code a processing pipeline for medical images from scratch.</p>
    <p id="p0041">In contrast with Eisen or MONAI, we do not implement network architectures, loss functions or training workflows. This is to limit the scope of the library and to enforce modularity between training of neural networks and preprocessing and data augmentation.</p>
    <p id="p0042">Following the PyTorch philosophy <xref rid="bib0008" ref-type="bibr">[8]</xref>, we designed TorchIO with an emphasis on simplicity and usability while reusing PyTorch classes and infrastructure where possible. Note that, although we designed TorchIO following PyTorch style, the library could also be used with other deep learning platforms such as TensorFlow or Keras <xref rid="bib0041" ref-type="bibr">[41]</xref>.</p>
    <p id="p0043">TorchIO makes use of open-source medical imaging software platforms. Packages were selected to reduce the number of required external dependencies and the need to re-implement basic medical imaging processing operations (image loading, resampling, etc.).</p>
    <p id="p0044">TorchIO features are divided into two categories: data structures and input/output (torchio.data), and transforms for preprocessing and augmentation (torchio.transforms). <xref rid="fig0002" ref-type="fig">Fig. 2</xref> represents a diagram of the codebase and the different interfaces to the library.<fig id="fig0002"><label>Fig. 2</label><caption><p>General diagram of TorchIO, its dependencies and its interfaces. Boxes with a red border (<inline-graphic xlink:href="fx1.gif"><alt-text id="at0002">Image 1</alt-text></inline-graphic>) represent elements implemented in TorchIO. Logos indicate lower-level Python libraries used by TorchIO. <inline-graphic xlink:href="fx2.gif"><alt-text id="at0003">Image 2</alt-text></inline-graphic>: NiBabel <xref rid="bib0015" ref-type="bibr">[15]</xref>; <inline-graphic xlink:href="fx3.gif"><alt-text id="at0004">Image 3</alt-text></inline-graphic>: SimpleITK <xref rid="bib0014" ref-type="bibr">[14]</xref>; <inline-graphic xlink:href="fx4.gif"><alt-text id="at0005">Image 4</alt-text></inline-graphic>: NumPy <xref rid="bib0030" ref-type="bibr">[30]</xref>; <inline-graphic xlink:href="fx5.gif"><alt-text id="at0006">Image 5</alt-text></inline-graphic>: PyTorch <xref rid="bib0008" ref-type="bibr">[8]</xref>.</p></caption><alt-text id="at0007">Fig. 2</alt-text><graphic xlink:href="gr2"/></fig></p>
    <sec id="sec0007">
      <label>2.1</label>
      <title>Data</title>
      <sec id="sec0008">
        <label>2.1.1</label>
        <title>Input/Output</title>
        <p id="p0045">TorchIO uses the medical imaging libraries NiBabel and SimpleITK to read and write images. Dependency on both is necessary to ensure broad support of image formats. For instance, NiBabel does not support reading Portable Network Graphics (PNG) files, while SimpleITK does not support some neuroimaging-specific formats.</p>
        <p id="p0046">TorchIO supports up to 4D images, i.e., 2D or 3D single-channel or multi-channel data such as X-rays, RGB histological slides, microscopy stacks, multispectral images, CT-scans, functional MRI (fMRI) and diffusion MRI (dMRI).</p>
      </sec>
      <sec id="sec0009">
        <label>2.1.2</label>
        <title>Data structures</title>
        <p id="p0047"><italic>Image.</italic> The Image class, representing one medical image, stores a 4D tensor, whose voxels encode, e.g., signal intensity or segmentation labels, and the corresponding affine transform, typically a rigid (Euclidean) transform, to convert voxel indices to world coordinates in millimeters. Arbitrary fields such as acquisition parameters may also be stored.</p>
        <p id="p0049">Subclasses are used to indicate specific types of images, such as ScalarImage and LabelMap, which are used to store, e.g., CT scans and segmentations, respectively.</p>
        <p id="p0050">An instance of Image can be created using a filepath, a PyTorch tensor, or a NumPy array. This class uses lazy loading, i.e., the data is not loaded from disk at instantiation time. Instead, the data is only loaded when needed for an operation (e.g., if a transform is applied to the image).</p>
        <p id="p0051"><xref rid="fig0003" ref-type="fig">Fig. 3</xref> shows two instances of Image. The instance of ScalarImage contains a 4D tensor representing a dMRI, which contains four 3D volumes (one per gradient direction), and the associated affine matrix. Additionally, it stores the strength and direction for each of the four gradients. The instance of LabelMap contains a brain parcellation of the same subject, the associated affine matrix, and the name and color of each brain structure.<fig id="fig0003"><label>Fig. 3</label><caption><p>Usage example of ScalarImage, LabelMap, Subject and SubjectsDataset. The images store a 4D dMRI and a brain parcellation, and other related metadata.</p></caption><alt-text id="at0008">Fig. 3</alt-text><graphic xlink:href="gr3"/></fig></p>
        <p id="p0052"><italic>Subject.</italic> The Subject class stores instances of Image associated to a subject, e.g., a human or a mouse. As in the Image class, Subject can store arbitrary fields such as age, diagnosis or ethnicity.</p>
        <p id="p0054"><italic>Subjects dataset</italic>. The SubjectsDataset inherits from the PyTorch Dataset. It contains the list of subjects and optionally a transform to be applied to each subject after loading. When SubjectsDataset is queried for a specific subject, the corresponding set of images are loaded, a transform is applied to the images and the instance of Subject is returned.</p>
        <p id="p0056">For parallel loading, a PyTorch DataLoader may be used. This loader spawns multiple processes, each of which contains a shallow copy of the SubjectsDataset. Each copy is queried for a different subject, therefore loading and transforming is applied to different subjects in parallel on the central processing unit (CPU) (<xref rid="fig0004" ref-type="fig">Fig. 4</xref>a).<fig id="fig0004"><label>Fig. 4</label><caption><p>Diagram of data pipelines for training with whole volumes (top) and patches (bottom). Boxes with a red border represent PyTorch classes (<inline-graphic xlink:href="fx1.gif"><alt-text id="at0009">Image 1</alt-text></inline-graphic>) or TorchIO classes that inherit from PyTorch classes (<inline-graphic xlink:href="fx6.gif"><alt-text id="at0010">Image 6</alt-text></inline-graphic>).</p></caption><alt-text id="at0011">Fig. 4</alt-text><graphic xlink:href="gr4"/></fig></p>
        <p id="p0057">An example of subclassing SubjectsDataset is torchio.datasets.IXI, which may be used to download the Information eXtraction from Images (IXI) dataset.<xref rid="fn0002" ref-type="fn">2</xref></p>
      </sec>
      <sec id="sec0010">
        <label>2.1.3</label>
        <title>Patch-based training</title>
        <p id="p0058">Memory limitations often require training and inference steps to be performed using image subvolumes or <italic>patches</italic> instead of the whole volumes, as explained in <xref rid="sec0003" ref-type="sec">Section 1.1.1.3</xref>. In this section, we describe how TorchIO implements patch-based training via image sampling and queueing.</p>
        <p id="p0059"><italic>Samplers.</italic> A sampler takes as input an instance of Subject and returns a version of it whose images have a reduced FOV, i.e., the new images are subvolumes, also called windows or <italic>patches</italic>. For this, a PatchSampler may be used.</p>
        <p id="p0061">Different criteria may be used to select the center voxel of each output patch. A UniformSampler selects a voxel as the center at random with all voxels having an equal probability of being selected. A WeightedSampler selects the patch center according to a probability distribution image defined over all voxels, which is passed as input to the sampler.</p>
        <p id="p0062">At testing time, images are sampled such that a dense inference can be performed on the input volume. A GridSampler can be used to sample patches such that the center voxel is selected using a set stride. In this way, sampling over the entire volume is ensured. The potentially-overlapping inferred patches can be passed to a GridAggregator that builds the resulting volume patch by patch (or batch by batch).</p>
        <p id="p0063"><italic>Queue.</italic> A training iteration (i.e., forward and backward pass) performed on a GPU is usually faster than loading, preprocessing, augmenting, and cropping a volume on a CPU. Most preprocessing operations could be performed using a GPU, but these devices are typically reserved for training the CNN so that the batch size and input tensor can be as large as possible. Therefore, it is beneficial to prepare (i.e., load, preprocess and augment) the volumes using multiprocessing CPU techniques in parallel with the forward-backward passes of a training iteration.</p>
        <p id="p0065">Once a volume is appropriately prepared, it is computationally beneficial to sample multiple patches from a volume rather than having to prepare the same volume each time a patch needs to be extracted. The sampled patches are then stored in a buffer or <italic>queue</italic> until the next training iteration, at which point they are loaded onto the GPU to perform an optimization iteration. For this, TorchIO provides the Queue class, which inherits from the PyTorch Dataset (<xref rid="fig0004" ref-type="fig">Fig. 4</xref>b). In this queueing system, samplers behave as generators that yield patches from volumes contained in the SubjectsDataset.</p>
        <p id="p0066">The end of a training epoch is defined as the moment after which patches from all subjects have been used for training. At the beginning of each training epoch, the subjects list in the SubjectsDataset is shuffled, as is typically done in machine learning pipelines to increase variance of training instances during model optimization. A PyTorch loader begins by shallow-copying the dataset to each subprocess. Each worker subprocess loads and applies image transforms to the volumes in parallel. A patches list is filled with patches extracted by the sampler, and the queue is shuffled once it has reached a specified maximum length so that batches are composed of patches from different subjects. The internal data loader continues querying the SubjectsDataset using multiprocessing. The patches list, when emptied, is refilled with new patches. A second data loader, external to the queue, may be used to collate batches of patches stored in the queue, which are passed to the neural network.</p>
      </sec>
    </sec>
    <sec id="sec0011">
      <label>2.2</label>
      <title>Transforms</title>
      <p id="p0067">The transforms API was designed to be similar to the PyTorch</p>
      <p id="p0068">torchvision.transforms module. TorchIO includes augmentations such as random affine transformation (<xref rid="fig0005" ref-type="fig">Fig. 5</xref>e) or random blur (<xref rid="fig0005" ref-type="fig">Fig. 5</xref>b), but they are implemented using medical imaging libraries <xref rid="bib0014" ref-type="bibr">[14]</xref>, <xref rid="bib0015" ref-type="bibr">[15]</xref> to take into account specific properties of medical images, namely their size, resolution, location, and orientation (see <xref rid="sec0003" ref-type="sec">Section 1.1.1.1</xref>). <xref rid="tbl0001" ref-type="table">Table 1</xref> shows transforms implemented in TorchIO v0.18.0 and their main corresponding library dependencies.<fig id="fig0005"><label>Fig. 5</label><caption><p>A selection of data augmentation techniques available in TorchIO v0.18.0. Each example is presented as a pair of images composed of the transformed image and a corresponding transformed label map. Note that all screenshots are from a 2D coronal slice of the transformed 3D images. The MRI corresponds to the Montreal Neurological Institute (MNI) Colin 27 average brain <xref rid="bib0049" ref-type="bibr">[49]</xref>, which can be downloaded using torchio.datasets.Colin27. Label maps were generated using an automated brain parcellation algorithm <xref rid="bib0012" ref-type="bibr">[12]</xref>.</p></caption><alt-text id="at0012">Fig. 5</alt-text><graphic xlink:href="gr5"/></fig><table-wrap position="float" id="tbl0001"><label>Table 1</label><caption><p>Transforms included in TorchIO v0.18.0. Logos indicate the main library used to process the images. <inline-graphic xlink:href="fx2.gif"><alt-text id="at0015">Image 2</alt-text></inline-graphic>: NiBabel <xref rid="bib0015" ref-type="bibr">[15]</xref>; <inline-graphic xlink:href="fx3.gif"><alt-text id="at0016">Image 3</alt-text></inline-graphic>: SimpleITK <xref rid="bib0014" ref-type="bibr">[14]</xref>; <inline-graphic xlink:href="fx4.gif"><alt-text id="at0017">Image 4</alt-text></inline-graphic>: NumPy <xref rid="bib0030" ref-type="bibr">[30]</xref>; <inline-graphic xlink:href="fx5.gif"><alt-text id="at0018">Image 5</alt-text></inline-graphic>: PyTorch <xref rid="bib0008" ref-type="bibr">[8]</xref>, <xref rid="bib0043" ref-type="bibr">[43]</xref>, <xref rid="bib0045" ref-type="bibr">[45]</xref>.</p></caption><alt-text id="at0019">Table 1</alt-text><table frame="hsides" rules="groups"><tbody><tr><td valign="top"><inline-graphic xlink:href="fx7.gif"><alt-text id="at0022">Image 7</alt-text></inline-graphic></td></tr></tbody></table></table-wrap></p>
      <p id="p0069">Transforms are designed to be flexible regarding input and output types. Following a duck typing approach, they can take as input PyTorch tensors, SimpleITK images, NumPy arrays, Pillow images, Python dictionaries, and instances of Subject and Image, and will return an output of the same type.</p>
      <p id="p0070">TorchIO transforms can be classified into either spatial and intensity transforms, or preprocessing and augmentation transforms (<xref rid="tbl0001" ref-type="table">Table 1</xref>). All are subclasses of the Transform base class. Spatial transforms and intensity transforms are related to the SpatialTransform and IntensityTransform classes, respectively. Transforms whose parameters are randomly chosen are subclasses of RandomTransform.</p>
      <p id="p0071">Instances of SpatialTransform typically modify the image bounds or spacing, and often need to resample the image using interpolation. They are applied to all image types. Instances of IntensityTransform do not modify the position of voxels, only their values, and they are only applied to instances of ScalarImage. For example, if a RandomNoise transform (which is a subclass of IntensityTransform) receives as input a Subject with a ScalarImage representing a CT scan and a LabelMap representing a segmentation, it will add noise to only the CT scan. On the other hand, if a RandomAffine transform (which is a subclass of SpatialTransform) receives the same input, the same affine transformation will be applied to both images, with nearest-neighbor interpolation always used to interpolate LabelMap objects.</p>
      <sec id="sec0012">
        <label>2.2.1</label>
        <title>Preprocessing</title>
        <p id="p0072">Preprocessing transforms are necessary to ensure spatial and intensity uniformity of training instances.</p>
        <p id="p0073">Spatial preprocessing is important as CNNs do not generally take into account metadata related to medical images (see <xref rid="sec0003" ref-type="sec">Section 1.1.1.1</xref>), therefore it is necessary to ensure that voxels across images have similar spatial location and relationships before training. Spatial preprocessing transforms typically used in medical imaging include resampling (e.g., to make voxel spacing isotropic for all training samples) and reorientation (e.g., to orient all training samples in the same way). For example, the Resample transform can be used to fix the issue presented in <xref rid="fig0001" ref-type="fig">Fig. 1</xref>.</p>
        <p id="p0074">Intensity normalization is generally beneficial for optimization of neural networks. TorchIO provides intensity normalization techniques including min-max scaling or standardization,<xref rid="fn0003" ref-type="fn">3</xref> which are computed using pure PyTorch. A binary image, such as a mask representing the foreground or structures of interest, can be used to define the set of voxels to be taken into account when computing statistics for intensity normalization. We also provide a method for MRI histogram standardization <xref rid="bib0048" ref-type="bibr">[48]</xref>, computed using NumPy, which may be used to overcome the differences in intensity distributions between images acquired using different scanners or sequences.</p>
      </sec>
      <sec id="sec0013">
        <label>2.2.2</label>
        <title>Augmentation</title>
        <p id="p0076">TorchIO includes spatial augmentation transforms such as random flipping using PyTorch and random affine and elastic deformation transforms using SimpleITK. Intensity augmentation transforms include random Gaussian blur using a SimpleITK filter (<xref rid="fig0005" ref-type="fig">Fig. 5</xref>b) and addition of random Gaussian noise using pure PyTorch (<xref rid="fig0005" ref-type="fig">Fig. 5</xref>d). All augmentation transforms are subclasses of RandomTransform.</p>
        <p id="p0077">Although current domain-specific data augmentation transforms available in TorchIO are mostly related to MRI, we encourage users to contribute physics-based data augmentation techniques for US or CT <xref rid="bib0050" ref-type="bibr">[50]</xref>.</p>
        <p id="p0078">We provide several MRI-specific augmentation transforms related to <inline-formula><mml:math id="M6" altimg="si5.svg"><mml:mi>k</mml:mi></mml:math></inline-formula>-space, which are described below. An MR image is usually reconstructed as the magnitude of the inverse Fourier transform of the <inline-formula><mml:math id="M7" altimg="si5.svg"><mml:mi>k</mml:mi></mml:math></inline-formula>-space signal, which is populated with the signals generated by the sample as a response to a radio-frequency electromagnetic pulse. These signals are modulated using coils that create gradients of the magnetic field inside the scanner. Artifacts are created by using <inline-formula><mml:math id="M8" altimg="si5.svg"><mml:mi>k</mml:mi></mml:math></inline-formula>-space transforms to perturb the Fourier space and generate corresponding intensity artifacts in image space. The forward and inverse Fourier transforms are computed using the Fast Fourier Transform (FFT) algorithm implemented in NumPy.</p>
        <p id="p0079"><italic>Random</italic><inline-formula><mml:math id="M9" altimg="si5.svg"><mml:mi>k</mml:mi></mml:math></inline-formula><italic>-space spike artifact.</italic> Gradients applied at a very high duty cycle may produce bad data points, or noise spikes, in <inline-formula><mml:math id="M10" altimg="si5.svg"><mml:mi>k</mml:mi></mml:math></inline-formula>-space <xref rid="bib0051" ref-type="bibr">[51]</xref>. These points in <inline-formula><mml:math id="M11" altimg="si5.svg"><mml:mi>k</mml:mi></mml:math></inline-formula>-space generate a spike artifact, also known as Herringbone, crisscross or corduroy artifact, which manifests as uniformly-separated stripes in image space, as shown in <xref rid="fig0005" ref-type="fig">Fig. 5</xref>i. This type of data augmentation has recently been used to estimate uncertainty through a heteroscedastic noise model <xref rid="bib0044" ref-type="bibr">[44]</xref>.</p>
        <p id="p0081"><italic>Random</italic><inline-formula><mml:math id="M12" altimg="si5.svg"><mml:mi>k</mml:mi></mml:math></inline-formula><italic>-space motion artifact</italic>. The <inline-formula><mml:math id="M13" altimg="si5.svg"><mml:mi>k</mml:mi></mml:math></inline-formula>-space is often populated line by line, and the sample in the scanner is assumed to remain static. If a patient moves during the MRI acquisition, motion artifacts will appear in the reconstructed image. We implemented a method to simulate random motion artifacts (<xref rid="fig0005" ref-type="fig">Fig. 5</xref>h) that has been used successfully for data augmentation to model uncertainty and improve segmentation <xref rid="bib0042" ref-type="bibr">[42]</xref>.</p>
        <p id="p0083"><italic>Random</italic><inline-formula><mml:math id="M14" altimg="si5.svg"><mml:mi>k</mml:mi></mml:math></inline-formula><italic>-space ghosting artifact</italic>. Organs motion such as respiration or cardiac pulsation may generate ghosting artifacts along the phase-encoding direction <xref rid="bib0051" ref-type="bibr">[51]</xref> (see <xref rid="fig0005" ref-type="fig">Fig. 5</xref>j). We simulate this phenomenon by removing every <inline-formula><mml:math id="M15" altimg="si6.svg"><mml:mi>n</mml:mi></mml:math></inline-formula>th plane of the <inline-formula><mml:math id="M16" altimg="si5.svg"><mml:mi>k</mml:mi></mml:math></inline-formula>-space along one direction to generate <inline-formula><mml:math id="M17" altimg="si6.svg"><mml:mi>n</mml:mi></mml:math></inline-formula> ghosts along that dimension, while keeping the center of <inline-formula><mml:math id="M18" altimg="si5.svg"><mml:mi>k</mml:mi></mml:math></inline-formula>-space intact.</p>
        <p id="p0085"><italic>Random bias field artifact.</italic> Inhomogeneity of the static magnetic field in the MRI scanner produces intensity artifacts of very low spatial frequency along the entirety of the image. These artifacts can be simulated using polynomial basis functions <xref rid="bib0052" ref-type="bibr">[52]</xref>, as shown in <xref rid="fig0005" ref-type="fig">Fig. 5</xref>g.</p>
      </sec>
      <sec id="sec0014">
        <label>2.2.3</label>
        <title>Composability</title>
        <p id="p0087">All transforms can be composed in a linear fashion, as in the PyTorch torchvision library, or building a directed acyclic graphDAG using the OneOf transform (as in <xref rid="bib0010" ref-type="bibr">[10]</xref>). For example, a user might want to apply a random spatial augmentation transform to <inline-formula><mml:math id="M19" altimg="si7.svg"><mml:mrow><mml:mn>50</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> of the samples using either an affine or an elastic transform, but they want the affine transform to be applied to <inline-formula><mml:math id="M20" altimg="si8.svg"><mml:mrow><mml:mn>80</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> of the augmented images, as the execution time is faster. Then, they might want to rescale the volume intensity for all images to be between 0 and 1. <xref rid="fig0006" ref-type="fig">Fig. 6</xref> shows a graph representing the transform composition. This transform composition can be implemented with just three statements:<fig id="fig0006"><label>Fig. 6</label><caption><p>Graph representation of the composed transform described in <xref rid="sec0014" ref-type="sec">Section 2.2.3</xref>.</p></caption><alt-text id="at0013">Fig. 6</alt-text><graphic xlink:href="gr6"/></fig></p>
        <p id="p0088">
          <inline-graphic xlink:href="fx8.gif">
            <alt-text id="at0041">Image 8</alt-text>
          </inline-graphic>
        </p>
        <p id="p0089">Compose and OneOf are implemented as TorchIO transforms.</p>
      </sec>
      <sec id="sec0015">
        <label>2.2.4</label>
        <title>Extensibility</title>
        <p id="p0090">The Lambda transform can be passed an arbitrary callable object, which allows the user to augment the library with custom transforms without having a deep understanding of the underlying code.</p>
        <p id="p0091">Additionally, more complex transforms can be developed. For example, we implemented a TorchIO transform to simulate brain resection cavities from preoperative MR images within a self-supervised learning pipeline <xref rid="bib0053" ref-type="bibr">[53]</xref>. The RandomLabelsToImage transform may be used to simulate an image from a tissue segmentation. It can be composed with RandomAnisotropy to train neural networks agnostic to image contrast and resolution <xref rid="bib0046" ref-type="bibr">[46]</xref>, <xref rid="bib0047" ref-type="bibr">[47]</xref>, <xref rid="bib0054" ref-type="bibr">[54]</xref>.</p>
      </sec>
      <sec id="sec0016">
        <label>2.2.5</label>
        <title>Reproducibility and traceability</title>
        <p id="p0092">To promote open science principles, we designed TorchIO to support experiment reproducibility and traceability.</p>
        <p id="p0093">All transforms support receiving Python primitives as arguments, which makes TorchIO suitable to be used with a configuration file associated to a specific experiment.</p>
        <p id="p0094">A history of all applied transforms and their computed random parameters is saved in the transform output so that the path in the DAG and the parameters used can be traced and reproduced. Furthermore, the Subject class includes a method to compose the transforms history into a single transform that may be used to reproduce the exact result (<xref rid="sec0014" ref-type="sec">Section 2.2.3</xref>).</p>
      </sec>
      <sec id="sec0017">
        <label>2.2.6</label>
        <title>Invertibility</title>
        <p id="p0095">Inverting transforms is especially useful in scenarios where one needs to apply some transformation, infer a segmentation on the transformed data and then apply the inverse transformation to bring the inference into the original image space. The Subject class includes a method to invert the transformations applied. It does this by first inverting all transforms that are invertible, discarding the ones that are not. Then, it composes the invertible transforms into a single transform.</p>
        <p id="p0096">Transforms invertibility is most commonly applied to test-time augmentation <xref rid="bib0055" ref-type="bibr">[55]</xref> or estimation of aleatoric uncertainty <xref rid="bib0056" ref-type="bibr">[56]</xref> in the context of image segmentation.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec0018">
    <label>3</label>
    <title>Results</title>
    <sec id="sec0019">
      <label>3.1</label>
      <title>Code availability</title>
      <p id="p0097">All the code for TorchIO is available on GitHub<xref rid="fn0004" ref-type="fn">4</xref>. We follow the semantic versioning system <xref rid="bib0057" ref-type="bibr">[57]</xref> to tag and release our library. Releases are published on the Zenodo data repository<xref rid="fn0005" ref-type="fn">5</xref> to allow users to cite the specific version of the package they used in their experiments. The version described in this paper is v0.18.0 <xref rid="bib0058" ref-type="bibr">[58]</xref>. Detailed API documentation is hosted on Read the Docs and comprehensive Jupyter notebook tutorials are hosted on Google Colaboratory, where users can run examples online. The library can be installed with a single line of code on Windows, macOS or Linux using the Pip Installs Packages (PIP) package manager: pip install torchio.</p>
      <p id="p0098">TorchIO has a strong community of users, with more than 900 stars on GitHub and more than 7000 Python Package Index (PyPI) downloads per month<xref rid="fn0006" ref-type="fn">6</xref> as of July 2021.</p>
      <sec id="sec0020">
        <label>3.1.1</label>
        <title>Additional interfaces</title>
        <p id="p0099">The provided command-line interface (CLI) tool torchio-transform allows users to apply a transform to an image file without using Python. This tool can be used to visualize only the preprocessing and data augmentation pipelines and aid in experimental design for a given application. It can also be used in shell scripts to preprocess and augment datasets in cases where large storage is available and on-the-fly loading needs to be faster.</p>
        <p id="p0100">Additionally, we provide a graphical user interface (GUI) implemented as a Python scripted module within the <italic>TorchIO</italic> extension available in 3D Slicer <xref rid="bib0013" ref-type="bibr">[13]</xref>. It can be used to visualize the effect of the transforms parameters without any coding (<xref rid="fig0007" ref-type="fig">Fig. 7</xref>). As with the CLI tool, users can experimentally assess preprocessing and data augmentation before network training to ensure the preprocessing pipeline is suitable for a given application.<fig id="fig0007"><label>Fig. 7</label><caption><p>GUI for TorchIO, implemented as a 3D Slicer extension. In this example, the applied transforms are RandomBiasField, RandomGhosting, RandomMotion, RandomAffine and RandomElasticDeformation.</p></caption><alt-text id="at0014">Fig. 7</alt-text><graphic xlink:href="gr7"/></fig></p>
      </sec>
    </sec>
    <sec id="sec0021">
      <label>3.2</label>
      <title>Usage examples</title>
      <p id="p0101">In this section, we briefly describe the implementations of two medical image computing papers from the literature, pointing out the TorchIO features that could be used to replicate their experiments.</p>
      <sec id="sec0022">
        <label>3.2.1</label>
        <title>Super-resolution and synthesis of MRI</title>
        <p id="p0102">In <xref rid="bib0054" ref-type="bibr">[54]</xref>, a method is proposed to simulate high-resolution <inline-formula><mml:math id="M21" altimg="si9.svg"><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>-weighted MRIs from images of different modalities and resolutions.</p>
        <p id="p0103">First, brain regions are segmented on publicly available datasets of brain MRI. During training, an MRI (ScalarImage) and the corresponding segmentation (LabelMap) corresponding to a specific subject (Subject) are sampled from the training dataset (SubjectsDataset). Next, the same spatial augmentation transform is applied to both images by composing an affine transform (RandomAffine) and a nonlinear diffeomorphic transform (RandomElasticDeformation). Then, a Gaussian mixture modelGMM conditioned on the labels is sampled at each voxel location to simulate an MRI of arbitrary contrast (RandomLabelsToImage) <xref rid="bib0046" ref-type="bibr">[46]</xref>. Finally, multiple degrading phenomena are simulated on the synthetic image: variability in the coordinate frames (RandomAffine), bias field inhomogeneities (RandomBiasField), partial-volume effects due to a large slice thickness during acquisition <xref rid="bib0047" ref-type="bibr">[47]</xref> (RandomAnisotropy), registration errors (RandomAffine), and resampling artifacts (Resample).</p>
      </sec>
      <sec id="sec0023">
        <label>3.2.2</label>
        <title>Adaptive sampling for segmentation of CT scans</title>
        <p id="p0104">In <xref rid="bib0059" ref-type="bibr">[59]</xref>, CT scans that are too large to fit on a GPU are segmented using patch-based training with weighted sampling of patches. Discrepancies between labels and predictions are used to create error maps and patches are preferentially sampled from voxels with larger error.</p>
        <p id="p0105">During training, a CT scan (ScalarImage) and its corresponding segmentation (LabelMap) from a subject (Subject) are loaded and the same augmentation is performed to both by applying random rotations and scaling (RandomAffine). Then, voxel intensities are clipped to <inline-formula><mml:math id="M22" altimg="si10.svg"><mml:mrow><mml:mo>[</mml:mo><mml:mo>−</mml:mo><mml:mn>1000</mml:mn><mml:mo>,</mml:mo><mml:mn>1000</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> (RescaleIntensity) and divided by a constant factor representing the standard deviation of the dataset (can be implemented with Lambda). As the CT scans are too large to fit in the GPU, patch-based training is used (Queue). To obtain high-resolution predictions and a large receptive field simultaneously, two patches of similar size but different FOV are generated from each sampled patch: a context patch generated by downsampling the original patch (Resample) and a full-resolution patch with a smaller FOV (CropOrPad). At the end of each epoch, error maps for each subject (Subject) are computed as the difference between the labels and predictions. The error maps are used in the following epoch to sample patches with large errors more often (WeightedSampler). At inference time, a sliding window (GridSampler) is used to predict the segmentation patch by patch, and patches are aggregated to build the prediction for the whole input volume (GridAggregator).</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec0024">
    <label>4</label>
    <title>Discussion</title>
    <p id="p0106">We have presented TorchIO, a new library to efficiently load, preprocess, augment and sample medical imaging data during the training of CNNs. It is designed in the style of the deep learning framework PyTorch to provide medical imaging specific preprocessing and data augmentation algorithms.</p>
    <p id="p0107">The main motivation for developing TorchIO as an open-source toolkit is to help researchers standardize medical image processing pipelines and allow them to focus on the deep learning experiments. It also encourages good open-science practices, as it supports experiment reproducibility and is version-controlled so that the software can be cited precisely.</p>
    <p id="p0108">The library is compatible with other higher-level deep learning frameworks for medical imaging such as MONAI. For example, users can benefit from TorchIO’s MRI transforms and patch-based sampling while using MONAI’s networks, losses, training pipelines and evaluation metrics.</p>
    <p id="p0109">The main limitation of TorchIO is that most transforms are not differentiable. The reason is that PyTorch tensors stored in TorchIO data structures must be converted to SimpleITK images or NumPy arrays within most transforms, making them not compatible with PyTorch’s automatic differentiation engine. However, compatibility between PyTorch and ITK has recently been improved, partly thanks to the appearance of the MONAI project <xref rid="bib0060" ref-type="bibr">[60]</xref>. Therefore, TorchIO might provide differentiable transforms in the future, which could be used to implement, e.g., spatial transformer networks for image registration <xref rid="bib0061" ref-type="bibr">[61]</xref>. Another limitation is that many more transforms that are MRI-specific exist than for other imaging modalities such as CT or US. This is in part due to more users working on MRI applications and requesting MRI-specific transforms. However, we welcome contributions for other modalities as well.</p>
    <p id="p0110">In the future, we will work on extending the preprocessing and augmentation transforms to different medical imaging modalities such as CT or US, and improving compatibility with related works. The source code, as well as examples and documentation, are made publicly available online, on GitHub. We welcome feedback, feature requests, and contributions to the library, either by creating issues on the GitHub repository or by emailing the authors.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="p0111">The authors declare no conflicts of interest.</p>
  </sec>
</body>
<back>
  <ref-list id="bib001">
    <title>References</title>
    <ref id="bib0001">
      <label>1</label>
      <element-citation publication-type="book" id="sbref0001">
        <person-group person-group-type="author">
          <name>
            <surname>Çiçek</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Abdulkadir</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Lienkamp</surname>
            <given-names>S.S.</given-names>
          </name>
          <name>
            <surname>Brox</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Ronneberger</surname>
            <given-names>O.</given-names>
          </name>
        </person-group>
        <part-title>3D U-Net: learning dense volumetric segmentation from sparse annotation</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Ourselin</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Joskowicz</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Sabuncu</surname>
            <given-names>M.R.</given-names>
          </name>
          <name>
            <surname>Unal</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Wells</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <source>Medical Image Computing and Computer-Assisted Intervention MICCAI 2016</source>
        <series>Lecture Notes in Computer Science</series>
        <year>2016</year>
        <publisher-name>Springer International Publishing</publisher-name>
        <publisher-loc>Cham</publisher-loc>
        <fpage>424</fpage>
        <lpage>432</lpage>
      </element-citation>
    </ref>
    <ref id="bib0002">
      <label>2</label>
      <element-citation publication-type="journal" id="sbref0002">
        <person-group person-group-type="author">
          <name>
            <surname>Lu</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Popuri</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>G.W.</given-names>
          </name>
          <name>
            <surname>Balachandar</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Beg</surname>
            <given-names>M.F.</given-names>
          </name>
          <name>
            <surname>Alzheimers Disease Neuroimaging Initiative</surname>
          </name>
        </person-group>
        <article-title>Multimodal and multiscale deep neural networks for the early diagnosis of Alzheimer’s disease using structural MR and FDG-PET images</article-title>
        <source>Sci. Rep.</source>
        <volume>8</volume>
        <issue>1</issue>
        <year>2018</year>
        <fpage>5697</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-018-22871-z</pub-id>
        <pub-id pub-id-type="pmid">29632364</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0003">
      <label>3</label>
      <element-citation publication-type="journal" id="sbref0003">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Taviani</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Malkiel</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>J.Y.</given-names>
          </name>
          <name>
            <surname>Tamir</surname>
            <given-names>J.I.</given-names>
          </name>
          <name>
            <surname>Shaikh</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>S.T.</given-names>
          </name>
          <name>
            <surname>Hardy</surname>
            <given-names>C.J.</given-names>
          </name>
          <name>
            <surname>Pauly</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Vasanawala</surname>
            <given-names>S.S.</given-names>
          </name>
        </person-group>
        <article-title>Variable-density single-shot fast spin-echo MRI with deep learning reconstruction by using variational networks</article-title>
        <source>Radiology</source>
        <volume>289</volume>
        <issue>2</issue>
        <year>2018</year>
        <fpage>366</fpage>
        <lpage>373</lpage>
        <pub-id pub-id-type="doi">10.1148/radiol.2018180445</pub-id>
        <pub-id pub-id-type="pmid">30040039</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0004">
      <label>4</label>
      <element-citation publication-type="journal" id="sbref0004">
        <person-group person-group-type="author">
          <name>
            <surname>Shan</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>E.I.-C.</given-names>
          </name>
          <name>
            <surname>Fan</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Unsupervised end-to-end learning for deformable medical image registration</article-title>
        <source>arXiv:1711.08608 [cs]</source>
        <year>2018</year>
      </element-citation>
      <note>
        <p>ArXiv: 1711.08608.</p>
      </note>
    </ref>
    <ref id="bib0005">
      <label>5</label>
      <element-citation publication-type="book" id="sbref0005">
        <person-group person-group-type="author">
          <name>
            <surname>Abadi</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Barham</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Davis</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Dean</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Devin</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ghemawat</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Irving</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Isard</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kudlur</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Levenberg</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Monga</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Moore</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Murray</surname>
            <given-names>D.G.</given-names>
          </name>
          <name>
            <surname>Steiner</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Tucker</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Vasudevan</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Warden</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Wicke</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <part-title>TensorFlow: a system for large-scale machine learning</part-title>
        <source>Proceedings of the 12th USENIX conference on Operating Systems Design and Implementation</source>
        <series>OSDI’16</series>
        <year>2016</year>
        <publisher-name>USENIX Association</publisher-name>
        <publisher-loc>USA</publisher-loc>
        <fpage>265</fpage>
        <lpage>283</lpage>
      </element-citation>
    </ref>
    <ref id="bib0006">
      <label>6</label>
      <element-citation publication-type="journal" id="sbref0006">
        <person-group person-group-type="author">
          <name>
            <surname>Pawlowski</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Ktena</surname>
            <given-names>S.I.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>M.C.H.</given-names>
          </name>
          <name>
            <surname>Kainz</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Rueckert</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Glocker</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Rajchl</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>DLTK: state of the art reference implementations for deep learning on medical images</article-title>
        <source>arXiv:1711.06853 [cs]</source>
        <year>2017</year>
      </element-citation>
      <note>
        <p>ArXiv: 1711.06853.</p>
      </note>
    </ref>
    <ref id="bib0007">
      <label>7</label>
      <element-citation publication-type="journal" id="sbref0007">
        <person-group person-group-type="author">
          <name>
            <surname>Gibson</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Sudre</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Fidon</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Shakir</surname>
            <given-names>D.I.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Eaton-Rosen</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Gray</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Doel</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Whyntie</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Nachev</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Modat</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Barratt</surname>
            <given-names>D.C.</given-names>
          </name>
          <name>
            <surname>Ourselin</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Cardoso</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Vercauteren</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>NiftyNet: a deep-learning platform for medical imaging</article-title>
        <source>Comput. Methods Programs Biomed.</source>
        <volume>158</volume>
        <year>2018</year>
        <fpage>113</fpage>
        <lpage>122</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cmpb.2018.01.025</pub-id>
        <pub-id pub-id-type="pmid">29544777</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0008">
      <label>8</label>
      <element-citation publication-type="book" id="sbref0008">
        <person-group person-group-type="author">
          <name>
            <surname>Paszke</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gross</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Massa</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Lerer</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bradbury</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chanan</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Killeen</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Gimelshein</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Antiga</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Desmaison</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Kopf</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>DeVito</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Raison</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Tejani</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Chilamkurthy</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Steiner</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Fang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Bai</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chintala</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <part-title>PyTorch: an imperative style, high-performance deep learning library</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Wallach</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Larochelle</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Beygelzimer</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Alch-Buc</surname>
            <given-names>F.d.</given-names>
          </name>
          <name>
            <surname>Fox</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Garnett</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <source>Advances in Neural Information Processing Systems 32</source>
        <year>2019</year>
        <publisher-name>Curran Associates, Inc.</publisher-name>
        <fpage>8026</fpage>
        <lpage>8037</lpage>
      </element-citation>
    </ref>
    <ref id="bib0009">
      <label>9</label>
      <mixed-citation publication-type="other" id="sbref0009">H. He, The State of Machine Learning Frameworks in 2019, 2019, <ext-link ext-link-type="uri" xlink:href="http://bit.ly/3cjpliJ" id="interref1001ab">http://bit.ly/3cjpliJ</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0010">
      <label>10</label>
      <element-citation publication-type="journal" id="sbref0010">
        <person-group person-group-type="author">
          <name>
            <surname>Buslaev</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Iglovikov</surname>
            <given-names>V.I.</given-names>
          </name>
          <name>
            <surname>Khvedchenya</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Parinov</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Druzhinin</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kalinin</surname>
            <given-names>A.A.</given-names>
          </name>
        </person-group>
        <article-title>Albumentations: Fast and Flexible Image Augmentations</article-title>
        <source>Information</source>
        <volume>11</volume>
        <issue>2</issue>
        <year>2020</year>
        <fpage>125</fpage>
        <pub-id pub-id-type="doi">10.3390/info11020125</pub-id>
      </element-citation>
      <note>
        <p>Number: 2 Publisher: Multidisciplinary Digital Publishing Institute</p>
      </note>
    </ref>
    <ref id="bib0011">
      <label>11</label>
      <element-citation publication-type="journal" id="sbref0011">
        <person-group person-group-type="author">
          <name>
            <surname>Larobina</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Murino</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Medical image file formats</article-title>
        <source>J. Digit. Imaging</source>
        <volume>27</volume>
        <issue>2</issue>
        <year>2014</year>
        <fpage>200</fpage>
        <lpage>206</lpage>
        <pub-id pub-id-type="doi">10.1007/s10278-013-9657-9</pub-id>
        <pub-id pub-id-type="pmid">24338090</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0012">
      <label>12</label>
      <element-citation publication-type="journal" id="sbref0012">
        <person-group person-group-type="author">
          <name>
            <surname>Cardoso</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Modat</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Wolz</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Melbourne</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Cash</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Rueckert</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Ourselin</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Geodesic information flows: spatially-variant graphs and their application to segmentation and fusion</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <volume>34</volume>
        <issue>9</issue>
        <year>2015</year>
        <fpage>1976</fpage>
        <lpage>1988</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2015.2418298</pub-id>
        <pub-id pub-id-type="pmid">25879909</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0013">
      <label>13</label>
      <element-citation publication-type="journal" id="sbref0013">
        <person-group person-group-type="author">
          <name>
            <surname>Fedorov</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Beichel</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Kalpathy-Cramer</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Finet</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Fillion-Robin</surname>
            <given-names>J.-C.</given-names>
          </name>
          <name>
            <surname>Pujol</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Bauer</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Jennings</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Fennessy</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Sonka</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Buatti</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Aylward</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>J.V.</given-names>
          </name>
          <name>
            <surname>Pieper</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Kikinis</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>3D slicer as an image computing platform for the quantitative imaging network</article-title>
        <source>Magn. Reson. Imaging</source>
        <volume>30</volume>
        <issue>9</issue>
        <year>2012</year>
        <fpage>1323</fpage>
        <lpage>1341</lpage>
        <pub-id pub-id-type="doi">10.1016/j.mri.2012.05.001</pub-id>
        <pub-id pub-id-type="pmid">22770690</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0014">
      <label>14</label>
      <element-citation publication-type="journal" id="sbref0014">
        <person-group person-group-type="author">
          <name>
            <surname>Lowekamp</surname>
            <given-names>B.C.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>D.T.</given-names>
          </name>
          <name>
            <surname>Ibez</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Blezek</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>The design of SimpleITK</article-title>
        <source>Front. Neuroinformatics</source>
        <volume>7</volume>
        <year>2013</year>
        <fpage>45</fpage>
        <pub-id pub-id-type="doi">10.3389/fninf.2013.00045</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0015">
      <label>15</label>
      <mixed-citation publication-type="other" id="tboref0001">M. Brett, C.J. Markiewicz, M. Hanke, M.-A. Ct, B. Cipollini, P. McCarthy, C.P. Cheng, Y.O. Halchenko, M. Cottaar, S. Ghosh, E. Larson, D. Wassermann, S. Gerhard, G.R. Lee, H.-T. Wang, E. Kastman, A. Rokem, C. Madison, F.C. Morency, B. Moloney, M. Goncalves, C. Riddell, C. Burns, J. Millman, A. Gramfort, J. Leppkangas, R. Markello, J.J. van den Bosch, R.D. Vincent, H. Braun, K. Subramaniam, D. Jarecka, K.J. Gorgolewski, P.R. Raamana, B.N. Nichols, E.M. Baker, S. Hayashi, B. Pinsard, C. Haselgrove, M. Hymers, O. Esteban, S. Koudoro, N.N. Oosterhof, B. Amirbekian, I. Nimmo-Smith, L. Nguyen, S. Reddigari, S. St-Jean, E. Panfilov, E. Garyfallidis, G. Varoquaux, J. Kaczmarzyk, J.H. Legarreta, K.S. Hahn, O.P. Hinds, B. Fauber, J.-B. Poline, J. Stutters, K. Jordan, M. Cieslak, M.E. Moreno, V. Haenel, Y. Schwartz, B.C. Darwin, B. Thirion, D. Papadopoulos Orfanos, F. Pérez-García, I. Solovey, I. Gonzalez, J. Palasubramaniam, J. Lecher, K. Leinweber, K. Raktivan, P. Fischer, P. Gervais, S. Gadde, T. Ballinger, T. Roos, V.R. Reddam, freec84, nipy/nibabel: 3.0.1, 2020, <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/3628482.XlyGkJP7S8o" id="s6">https://zenodo.org/record/3628482.XlyGkJP7S8o</ext-link>. doi:<pub-id pub-id-type="doi">10.5281/zenodo.3628482</pub-id></mixed-citation>
    </ref>
    <ref id="bib0016">
      <label>16</label>
      <mixed-citation publication-type="other" id="sbref0016">A.B. Jung, K. Wada, J. Crall, S. Tanaka, J. Graving, C. Reinders, S. Yadav, J. Banerjee, G. Vecsei, A. Kraft, Z. Rui, J. Borovec, C. Vallentin, S. Zhydenko, K. Pfeiffer, B. Cook, I. Fernndez, F.-M. De Rainville, C.-H. Weng, A. Ayala-Acevedo, R. Meudec, M. Laporte, others, imgaug, 2020, <ext-link ext-link-type="uri" xlink:href="https://github.com/aleju/imgaug" id="interref0001abd">https://github.com/aleju/imgaug</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0017">
      <label>17</label>
      <mixed-citation publication-type="other" id="sbref0017">wiredfool, A. Clark, Hugo, A. Murray, A. Karpinsky, C. Gohlke, B. Crowell, D. Schmidt, A. Houghton, S. Johnson, S. Mani, J. Ware, D. Caro, S. Kossouho, E.W. Brown, A. Lee, M. Korobov, M. Grny, E.S. Santana, N. Pieuchot, O. Tonnhofer, M. Brown, B. Pierre, J.C. Abela, L.J. Solberg, F. Reyes, A. Buzanov, Y. Yu, eliempje, F. Tolf, Pillow: 3.1.0, 2016, <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/44297.Xlx04pP7S8o" id="interref0001abe">https://zenodo.org/record/44297.Xlx04pP7S8o</ext-link>. doi:<pub-id pub-id-type="doi">10.5281/zenodo.44297</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib0018">
      <label>18</label>
      <element-citation publication-type="book" id="sbref0018">
        <person-group person-group-type="author">
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G.E.</given-names>
          </name>
        </person-group>
        <part-title>ImageNet classification with deep convolutional neural networks</part-title>
        <source>Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1</source>
        <series>NIPS’12</series>
        <year>2012</year>
        <publisher-name>Curran Associates Inc.</publisher-name>
        <publisher-loc>USA</publisher-loc>
        <fpage>1097</fpage>
        <lpage>1105</lpage>
      </element-citation>
    </ref>
    <ref id="bib0019">
      <label>19</label>
      <element-citation publication-type="book" id="sbref0019">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Kornblith</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Norouzi</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <part-title>A simple framework for contrastive learning of visual representations</part-title>
        <source>International Conference on Machine Learning</source>
        <year>2020</year>
        <publisher-name>PMLR</publisher-name>
        <fpage>1597</fpage>
        <lpage>1607</lpage>
      </element-citation>
      <note>
        <p>ISSN: 2640-3498</p>
      </note>
    </ref>
    <ref id="bib0020">
      <label>20</label>
      <element-citation publication-type="book" id="sbref0020">
        <person-group person-group-type="author">
          <name>
            <surname>Milletari</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Navab</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Ahmadi</surname>
            <given-names>S.-A.</given-names>
          </name>
        </person-group>
        <part-title>V-Net: fully convolutional neural networks for volumetric medical image segmentation</part-title>
        <source>2016 Fourth International Conference on 3D Vision (3DV)</source>
        <year>2016</year>
        <fpage>565</fpage>
        <lpage>571</lpage>
        <pub-id pub-id-type="doi">10.1109/3DV.2016.79</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0021">
      <label>21</label>
      <element-citation publication-type="book" id="sbref0021">
        <person-group person-group-type="author">
          <name>
            <surname>Ioffe</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Szegedy</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <part-title>Batch normalization: accelerating deep network training by reducing internal covariate shift</part-title>
        <source>International Conference on Machine Learning</source>
        <year>2015</year>
        <publisher-name>PMLR</publisher-name>
        <fpage>448</fpage>
        <lpage>456</lpage>
      </element-citation>
      <note>
        <p>ISSN: 1938-7228</p>
      </note>
    </ref>
    <ref id="bib0022">
      <label>22</label>
      <element-citation publication-type="journal" id="sbref0022">
        <person-group person-group-type="author">
          <name>
            <surname>Lucena</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Souza</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Rittner</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Frayne</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Lotufo</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Convolutional neural networks for skull-stripping in brain MR imaging using silver standard masks</article-title>
        <source>Artif. Intell. Med.</source>
        <volume>98</volume>
        <year>2019</year>
        <fpage>48</fpage>
        <lpage>58</lpage>
        <pub-id pub-id-type="doi">10.1016/j.artmed.2019.06.008</pub-id>
        <pub-id pub-id-type="pmid">31521252</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0023">
      <label>23</label>
      <element-citation publication-type="book" id="sbref0023">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Fidon</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Ourselin</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Cardoso</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Vercauteren</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <part-title>On the compactness, efficiency, and representation of 3d convolutional networks: brain parcellation as a pretext task</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Niethammer</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Styner</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Aylward</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Oguz</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Yap</surname>
            <given-names>P.-T.</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <source>Information Processing in Medical Imaging</source>
        <series>Lecture Notes in Computer Science</series>
        <year>2017</year>
        <publisher-name>Springer International Publishing</publisher-name>
        <publisher-loc>Cham</publisher-loc>
        <fpage>348</fpage>
        <lpage>360</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-319-59050-9_28</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0024">
      <label>24</label>
      <element-citation publication-type="journal" id="sbref0024">
        <person-group person-group-type="author">
          <name>
            <surname>Nikolov</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Blackwell</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Mendes</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>De Fauw</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Meyer</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Hughes</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Askham</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Romera-Paredes</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Karthikesalingam</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Chu</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Carnell</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Boon</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>D’Souza</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Moinuddin</surname>
            <given-names>S.A.</given-names>
          </name>
          <name>
            <surname>Sullivan</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Consortium</surname>
            <given-names>D.R.</given-names>
          </name>
          <name>
            <surname>Montgomery</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Rees</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Suleyman</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Back</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Ledsam</surname>
            <given-names>J.R.</given-names>
          </name>
          <name>
            <surname>Ronneberger</surname>
            <given-names>O.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy</article-title>
        <source>arXiv:1809.04430 [physics, stat]</source>
        <year>2018</year>
      </element-citation>
      <note>
        <p>ArXiv: 1809.04430</p>
      </note>
    </ref>
    <ref id="bib0025">
      <label>25</label>
      <element-citation publication-type="book" id="sbref0025">
        <person-group person-group-type="author">
          <name>
            <surname>Deng</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Dong</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Socher</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>L.-J.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Fei-Fei</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <part-title>ImageNet: a large-scale hierarchical image database</part-title>
        <source>2009 IEEE Conference on Computer Vision and Pattern Recognition</source>
        <year>2009</year>
        <fpage>248</fpage>
        <lpage>255</lpage>
        <pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id>
      </element-citation>
      <note>
        <p>ISSN: 1063-6919</p>
      </note>
    </ref>
    <ref id="bib0026">
      <label>26</label>
      <element-citation publication-type="journal" id="sbref0026">
        <person-group person-group-type="author">
          <name>
            <surname>Weiss</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Khoshgoftaar</surname>
            <given-names>T.M.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>A survey of transfer learning</article-title>
        <source>J. Big Data</source>
        <volume>3</volume>
        <issue>1</issue>
        <year>2016</year>
        <fpage>9</fpage>
        <pub-id pub-id-type="doi">10.1186/s40537-016-0043-6</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0027">
      <label>27</label>
      <element-citation publication-type="journal" id="sbref0027">
        <person-group person-group-type="author">
          <name>
            <surname>Cheplygina</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>Cats or CAT scans: transfer learning from natural or medical image source data sets?</article-title>
        <source>Curr. Opin. Biomed. Eng.</source>
        <volume>9</volume>
        <year>2019</year>
        <fpage>21</fpage>
        <lpage>27</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cobme.2018.12.005</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0028">
      <label>28</label>
      <element-citation publication-type="book" id="sbref0028">
        <person-group person-group-type="author">
          <name>
            <surname>Raghu</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Kleinberg</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <part-title>Transfusion: understanding transfer learning for medical imaging</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Wallach</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Larochelle</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Beygelzimer</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Alch-Buc</surname>
            <given-names>F.d.</given-names>
          </name>
          <name>
            <surname>Fox</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Garnett</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <source>Advances in Neural Information Processing Systems</source>
        <volume>volume 32</volume>
        <year>2019</year>
        <publisher-name>Curran Associates, Inc.</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib0029">
      <label>29</label>
      <element-citation publication-type="journal" id="sbref0029">
        <person-group person-group-type="author">
          <name>
            <surname>Nyl</surname>
            <given-names>L.G.</given-names>
          </name>
          <name>
            <surname>Udupa</surname>
            <given-names>J.K.</given-names>
          </name>
        </person-group>
        <article-title>On standardizing the MR image intensity scale</article-title>
        <source>Magn. Reson. Med.</source>
        <volume>42</volume>
        <issue>6</issue>
        <year>1999</year>
        <fpage>1072</fpage>
        <lpage>1081</lpage>
        <pub-id pub-id-type="doi">10.1002/(sici)1522-2594(199912)42:6&lt;1072::aid-mrm11&gt;3.0.co;2-m</pub-id>
        <pub-id pub-id-type="pmid">10571928</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0030">
      <label>30</label>
      <element-citation publication-type="journal" id="sbref0030">
        <person-group person-group-type="author">
          <name>
            <surname>van der Walt</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Colbert</surname>
            <given-names>S.C.</given-names>
          </name>
          <name>
            <surname>Varoquaux</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>The NumPy array: a structure for efficient numerical computation</article-title>
        <source>Comput. Sci. Eng.</source>
        <volume>13</volume>
        <issue>2</issue>
        <year>2011</year>
        <fpage>22</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="doi">10.1109/MCSE.2011.37</pub-id>
      </element-citation>
      <note>
        <p>Conference Name: Computing in Science Engineering.</p>
      </note>
    </ref>
    <ref id="bib0031">
      <label>31</label>
      <element-citation publication-type="book" id="sbref0031">
        <person-group person-group-type="author">
          <name>
            <surname>Valindria</surname>
            <given-names>V.V.</given-names>
          </name>
          <name>
            <surname>Pawlowski</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Rajchl</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Lavdas</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Aboagye</surname>
            <given-names>E.O.</given-names>
          </name>
          <name>
            <surname>Rockall</surname>
            <given-names>A.G.</given-names>
          </name>
          <name>
            <surname>Rueckert</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Glocker</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <part-title>Multi-modal learning from unpaired images: application to multi-organ segmentation in CT and MRI</part-title>
        <source>2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</source>
        <year>2018</year>
        <fpage>547</fpage>
        <lpage>556</lpage>
        <pub-id pub-id-type="doi">10.1109/WACV.2018.00066</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0032">
      <label>32</label>
      <mixed-citation publication-type="other" id="sbref0032">C.S. Perone, cclauss, E. Saravia, P.L. Ballester, MohitTare, perone/medicaltorch: Release v0.2, 2018, <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/1495335.XlqwUZP7S8o" id="interref0001abh">https://zenodo.org/record/1495335.XlqwUZP7S8o</ext-link>. doi:<pub-id pub-id-type="doi">10.5281/zenodo.1495335</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib0033">
      <label>33</label>
      <mixed-citation publication-type="other" id="sbref0033">F. Isensee, P. Jger, J. Wasserthal, D. Zimmerer, J. Petersen, S. Kohl, J. Schock, A. Klein, T. Ro, S. Wirkert, P. Neher, S. Dinkelacker, G. Köhler, K. Maier-Hein, batchgenerators - a python framework for data augmentation, 2020, <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/3632567.Xlqnb5P7S8o" id="interref0001abi">https://zenodo.org/record/3632567.Xlqnb5P7S8o</ext-link>. doi:<pub-id pub-id-type="doi">10.5281/zenodo.3632567</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib0034">
      <label>34</label>
      <element-citation publication-type="journal" id="sbref0034">
        <person-group person-group-type="author">
          <name>
            <surname>Isensee</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Jaeger</surname>
            <given-names>P.F.</given-names>
          </name>
          <name>
            <surname>Kohl</surname>
            <given-names>S.A.A.</given-names>
          </name>
          <name>
            <surname>Petersen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Maier-Hein</surname>
            <given-names>K.H.</given-names>
          </name>
        </person-group>
        <article-title>nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</article-title>
        <source>Nat. Methods</source>
        <volume>18</volume>
        <issue>2</issue>
        <year>2021</year>
        <fpage>203</fpage>
        <lpage>211</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-020-01008-z</pub-id>
        <pub-id pub-id-type="pmid">33288961</pub-id>
      </element-citation>
      <note>
        <p>Number: 2 Publisher: Nature Publishing Group</p>
      </note>
    </ref>
    <ref id="bib0035">
      <label>35</label>
      <element-citation publication-type="journal" id="sbref0035">
        <person-group person-group-type="author">
          <name>
            <surname>Virtanen</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Gommers</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Oliphant</surname>
            <given-names>T.E.</given-names>
          </name>
          <name>
            <surname>Haberland</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Reddy</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Cournapeau</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Burovski</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Peterson</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Weckesser</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Bright</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>van der Walt</surname>
            <given-names>S.J.</given-names>
          </name>
          <name>
            <surname>Brett</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Wilson</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Millman</surname>
            <given-names>K.J.</given-names>
          </name>
          <name>
            <surname>Mayorov</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Nelson</surname>
            <given-names>A.R.J.</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Kern</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Larson</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Carey</surname>
            <given-names>C.J.</given-names>
          </name>
          <name>
            <surname>Polat</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Moore</surname>
            <given-names>E.W.</given-names>
          </name>
          <name>
            <surname>VanderPlas</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Laxalde</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Perktold</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Cimrman</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Henriksen</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Quintero</surname>
            <given-names>E.A.</given-names>
          </name>
          <name>
            <surname>Harris</surname>
            <given-names>C.R.</given-names>
          </name>
          <name>
            <surname>Archibald</surname>
            <given-names>A.M.</given-names>
          </name>
          <name>
            <surname>Ribeiro</surname>
            <given-names>A.H.</given-names>
          </name>
          <name>
            <surname>Pedregosa</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>van Mulbregt</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title>
        <source>Nat. Methods</source>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>
        <pub-id pub-id-type="pmid">31907477</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0036">
      <label>36</label>
      <element-citation publication-type="journal" id="sbref0036">
        <person-group person-group-type="author">
          <name>
            <surname>Jungo</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Scheidegger</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Reyes</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Balsiger</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>pymia: a Python package for data handling and evaluation in deep learning-based medical image analysis</article-title>
        <source>Comput. Methods Programs Biomed.</source>
        <volume>198</volume>
        <year>2021</year>
        <fpage>105796</fpage>
        <pub-id pub-id-type="doi">10.1016/j.cmpb.2020.105796</pub-id>
        <pub-id pub-id-type="pmid">33137700</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0037">
      <label>37</label>
      <element-citation publication-type="journal" id="sbref0037">
        <person-group person-group-type="author">
          <name>
            <surname>Kofler</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Ezhov</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Isensee</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Balsiger</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Berger</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Koerner</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Paetzold</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Shit</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>McKinley</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Bakas</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zimmer</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Ankerst</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Kirschke</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wiestler</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Menze</surname>
            <given-names>B.H.</given-names>
          </name>
        </person-group>
        <article-title>Are we using appropriate segmentation metrics? Identifying correlates of human expert perception for CNN training beyond rolling the DICE coefficient</article-title>
        <source>arXiv:2103.06205 [cs, eess]</source>
        <year>2021</year>
      </element-citation>
      <note>
        <p>ArXiv: 2103.06205</p>
      </note>
    </ref>
    <ref id="bib0038">
      <label>38</label>
      <mixed-citation publication-type="other" id="sbref0038">N. Ma, W. Li, R. Brown, Y. Wang, B. Gorman, Behrooz, H. Johnson, I. Yang, E. Kerfoot, Y. Li, M. Adil, Y.-T. Hsieh, charliebudd, A. Aggarwal, C. Trentz, adam aji, B. Murray, G. Daroach, P.-D. Tudosiu, myron, M. Graham, Balamurali, C. Baker, J. Sellner, L. Fidon, A. Powers, G. Leroy, Alxaline, D. Schulz, Project-MONAI/MONAI: 0.5.0, 2021, <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/4679866.YImZHZNKgWo" id="interref0001abl">https://zenodo.org/record/4679866.YImZHZNKgWo</ext-link>. doi:<pub-id pub-id-type="doi">10.5281/zenodo.4679866</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib0039">
      <label>39</label>
      <element-citation publication-type="journal" id="sbref0039">
        <person-group person-group-type="author">
          <name>
            <surname>Mancolo</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Eisen: a python package for solid deep learning</article-title>
        <source>arXiv:2004.02747 [cs, eess]</source>
        <year>2020</year>
      </element-citation>
      <note>
        <p>ArXiv: 2004.02747</p>
      </note>
    </ref>
    <ref id="bib0040">
      <label>40</label>
      <element-citation publication-type="journal" id="sbref0040">
        <person-group person-group-type="author">
          <name>
            <surname>Ranzini</surname>
            <given-names>M.B.M.</given-names>
          </name>
          <name>
            <surname>Fidon</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Ourselin</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Modat</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Vercauteren</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>MONAIfbs: MONAI-based fetal brain MRI deep learning segmentation</article-title>
        <source>arXiv:2103.13314 [cs, eess]</source>
        <year>2021</year>
      </element-citation>
      <note>
        <p>ArXiv: 2103.13314</p>
      </note>
    </ref>
    <ref id="bib0041">
      <label>41</label>
      <element-citation publication-type="book" id="sbref0041">
        <person-group person-group-type="author">
          <name>
            <surname>Chollet</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>others</surname>
          </name>
        </person-group>
        <part-title>Keras</part-title>
        <year>2015</year>
      </element-citation>
    </ref>
    <ref id="bib0042">
      <label>42</label>
      <element-citation publication-type="book" id="sbref0042">
        <person-group person-group-type="author">
          <name>
            <surname>Shaw</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Sudre</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Ourselin</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Cardoso</surname>
            <given-names>M.J.</given-names>
          </name>
        </person-group>
        <part-title>MRI k-space motion artefact augmentation: model robustness and task-specific uncertainty</part-title>
        <source>International Conference on Medical Imaging with Deep Learning</source>
        <year>2019</year>
        <fpage>427</fpage>
        <lpage>436</lpage>
        <comment>http://proceedings.mlr.press/v102/shaw19a.html.</comment>
      </element-citation>
    </ref>
    <ref id="bib0043">
      <label>43</label>
      <element-citation publication-type="journal" id="sbref0043">
        <person-group person-group-type="author">
          <name>
            <surname>Sudre</surname>
            <given-names>C.H.</given-names>
          </name>
          <name>
            <surname>Cardoso</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Ourselin</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Longitudinal segmentation of age-related white matter hyperintensities</article-title>
        <source>Med. Image Anal.</source>
        <volume>38</volume>
        <year>2017</year>
        <fpage>50</fpage>
        <lpage>64</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2017.02.007</pub-id>
        <pub-id pub-id-type="pmid">28282640</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0044">
      <label>44</label>
      <element-citation publication-type="book" id="sbref0044">
        <person-group person-group-type="author">
          <name>
            <surname>Shaw</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Sudre</surname>
            <given-names>C.H.</given-names>
          </name>
          <name>
            <surname>Ourselin</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Cardoso</surname>
            <given-names>M.J.</given-names>
          </name>
        </person-group>
        <part-title>A heteroscedastic uncertainty model for decoupling sources of MRI image quality</part-title>
        <source>Medical Imaging with Deep Learning</source>
        <year>2020</year>
        <publisher-name>PMLR</publisher-name>
        <fpage>733</fpage>
        <lpage>742</lpage>
        <comment>http://proceedings.mlr.press/v121/shaw20a.html.</comment>
      </element-citation>
      <note>
        <p>ISSN: 2640-3498</p>
      </note>
    </ref>
    <ref id="bib0045">
      <label>45</label>
      <element-citation publication-type="journal" id="sbref0045">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Bentley</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Mori</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Misawa</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Fujiwara</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Rueckert</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Self-supervised learning for medical image analysis using image context restoration</article-title>
        <source>Med. Image Anal.</source>
        <volume>58</volume>
        <year>2019</year>
        <fpage>101539</fpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2019.101539</pub-id>
        <pub-id pub-id-type="pmid">31374449</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0046">
      <label>46</label>
      <element-citation publication-type="book" id="sbref0046">
        <person-group person-group-type="author">
          <name>
            <surname>Billot</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Greve</surname>
            <given-names>D.N.</given-names>
          </name>
          <name>
            <surname>Leemput</surname>
            <given-names>K.V.</given-names>
          </name>
          <name>
            <surname>Fischl</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Iglesias</surname>
            <given-names>J.E.</given-names>
          </name>
          <name>
            <surname>Dalca</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>A learning strategy for contrast-agnostic MRI segmentation</part-title>
        <source>Medical Imaging with Deep Learning</source>
        <year>2020</year>
        <publisher-name>PMLR</publisher-name>
        <fpage>75</fpage>
        <lpage>93</lpage>
      </element-citation>
      <note>
        <p>ISSN: 2640-3498</p>
      </note>
    </ref>
    <ref id="bib0047">
      <label>47</label>
      <element-citation publication-type="book" id="sbref0047">
        <person-group person-group-type="author">
          <name>
            <surname>Billot</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Robinson</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Dalca</surname>
            <given-names>A.V.</given-names>
          </name>
          <name>
            <surname>Iglesias</surname>
            <given-names>J.E.</given-names>
          </name>
        </person-group>
        <part-title>Partial volume segmentation of brain MRI scans of any resolution and contrast</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Martel</surname>
            <given-names>A.L.</given-names>
          </name>
          <name>
            <surname>Abolmaesumi</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Stoyanov</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Mateus</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Zuluaga</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>S.K.</given-names>
          </name>
          <name>
            <surname>Racoceanu</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Joskowicz</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <source>Medical Image Computing and Computer Assisted Intervention MICCAI 2020</source>
        <series>Lecture Notes in Computer Science</series>
        <year>2020</year>
        <publisher-name>Springer International Publishing</publisher-name>
        <publisher-loc>Cham</publisher-loc>
        <fpage>177</fpage>
        <lpage>187</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-030-59728-3_18</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0048">
      <label>48</label>
      <element-citation publication-type="journal" id="sbref0048">
        <person-group person-group-type="author">
          <name>
            <surname>Nyl</surname>
            <given-names>L.G.</given-names>
          </name>
          <name>
            <surname>Udupa</surname>
            <given-names>J.K.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>New variants of a method of MRI scale standardization</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <volume>19</volume>
        <issue>2</issue>
        <year>2000</year>
        <fpage>143</fpage>
        <lpage>150</lpage>
        <pub-id pub-id-type="doi">10.1109/42.836373</pub-id>
        <pub-id pub-id-type="pmid">10784285</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0049">
      <label>49</label>
      <element-citation publication-type="journal" id="sbref0049">
        <person-group person-group-type="author">
          <name>
            <surname>Holmes</surname>
            <given-names>C.J.</given-names>
          </name>
          <name>
            <surname>Hoge</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Collins</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Woods</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Toga</surname>
            <given-names>A.W.</given-names>
          </name>
          <name>
            <surname>Evans</surname>
            <given-names>A.C.</given-names>
          </name>
        </person-group>
        <article-title>Enhancement of MR images using registration for signal averaging</article-title>
        <source>J. Comput. Assist. Tomogr.</source>
        <volume>22</volume>
        <issue>2</issue>
        <year>1998</year>
        <fpage>324</fpage>
        <lpage>333</lpage>
        <pub-id pub-id-type="doi">10.1097/00004728-199803000-00032</pub-id>
        <pub-id pub-id-type="pmid">9530404</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0050">
      <label>50</label>
      <element-citation publication-type="journal" id="sbref0050">
        <person-group person-group-type="author">
          <name>
            <surname>Omigbodun</surname>
            <given-names>A.O.</given-names>
          </name>
          <name>
            <surname>Noo</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>McNitt-yy</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Hsu</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Hsieh</surname>
            <given-names>S.S.</given-names>
          </name>
        </person-group>
        <article-title>The effects of physics-based data augmentation on the generalizability of deep neural networks: demonstration on nodule false-positive reduction</article-title>
        <source>Med. Phys.</source>
        <volume>46</volume>
        <issue>10</issue>
        <year>2019</year>
        <fpage>4563</fpage>
        <lpage>4574</lpage>
        <pub-id pub-id-type="doi">10.1002/mp.13755</pub-id>
        <pub-id pub-id-type="pmid">31396974</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0051">
      <label>51</label>
      <element-citation publication-type="journal" id="sbref0051">
        <person-group person-group-type="author">
          <name>
            <surname>Zhuo</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Gullapalli</surname>
            <given-names>R.P.</given-names>
          </name>
        </person-group>
        <article-title>MR artifacts, safety, and quality control</article-title>
        <source>RadioGraphics</source>
        <volume>26</volume>
        <issue>1</issue>
        <year>2006</year>
        <fpage>275</fpage>
        <lpage>297</lpage>
        <pub-id pub-id-type="doi">10.1148/rg.261055134</pub-id>
        <pub-id pub-id-type="pmid">16418258</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0052">
      <label>52</label>
      <element-citation publication-type="journal" id="sbref0052">
        <person-group person-group-type="author">
          <name>
            <surname>Van Leemput</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Maes</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Vandermeulen</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Suetens</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Automated model-based tissue classification of MR images of the brain</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <volume>18</volume>
        <issue>10</issue>
        <year>1999</year>
        <fpage>897</fpage>
        <lpage>908</lpage>
        <pub-id pub-id-type="doi">10.1109/42.811270</pub-id>
        <pub-id pub-id-type="pmid">10628949</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0053">
      <label>53</label>
      <element-citation publication-type="book" id="sbref0053">
        <person-group person-group-type="author">
          <name>
            <surname>Pérez-García</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Rodionov</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Alim-Marvasti</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Sparks</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Duncan</surname>
            <given-names>J.S.</given-names>
          </name>
          <name>
            <surname>Ourselin</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <part-title>Simulation of brain resection for cavity segmentation using self-supervised and semi-supervised learning</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Martel</surname>
            <given-names>A.L.</given-names>
          </name>
          <name>
            <surname>Abolmaesumi</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Stoyanov</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Mateus</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Zuluaga</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>S.K.</given-names>
          </name>
          <name>
            <surname>Racoceanu</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Joskowicz</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <source>Medical Image Computing and Computer Assisted Intervention MICCAI 2020</source>
        <series>Lecture Notes in Computer Science</series>
        <year>2020</year>
        <publisher-name>Springer International Publishing</publisher-name>
        <publisher-loc>Cham</publisher-loc>
        <fpage>115</fpage>
        <lpage>125</lpage>
      </element-citation>
    </ref>
    <ref id="bib0054">
      <label>54</label>
      <element-citation publication-type="journal" id="sbref0054">
        <person-group person-group-type="author">
          <name>
            <surname>Iglesias</surname>
            <given-names>J.E.</given-names>
          </name>
          <name>
            <surname>Billot</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Balbastre</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Tabari</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Conklin</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Alexander</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Golland</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Edlow</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Fischl</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Joint super-resolution and synthesis of 1 mm isotropic MP-RAGE volumes from clinical MRI exams with scans of different orientation, resolution and contrast</article-title>
        <source>arXiv preprint arXiv:2012.13340</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="bib0055">
      <label>55</label>
      <element-citation publication-type="journal" id="sbref0055">
        <person-group person-group-type="author">
          <name>
            <surname>Moshkov</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Mathe</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Kertesz-Farkas</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Hollandi</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Horvath</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Test-time augmentation for deep learning-based cell segmentation on microscopy images</article-title>
        <source>Sci. Rep.</source>
        <volume>10</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>5068</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-020-61808-3</pub-id>
        <pub-id pub-id-type="pmid">32193485</pub-id>
      </element-citation>
      <note>
        <p>Number: 1 Publisher: Nature Publishing Group</p>
      </note>
    </ref>
    <ref id="bib0056">
      <label>56</label>
      <element-citation publication-type="journal" id="sbref0056">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Aertsen</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Deprest</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ourselin</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Vercauteren</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks</article-title>
        <source>Neurocomputing</source>
        <volume>338</volume>
        <year>2019</year>
        <fpage>34</fpage>
        <lpage>45</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neucom.2019.01.103</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0057">
      <label>57</label>
      <mixed-citation publication-type="other" id="sbref0057">T. Preston-Werner, Semantic Versioning 2.0.0, 2020, Library Catalog: semver.org, <ext-link ext-link-type="uri" xlink:href="https://semver.org/" id="w11114">https://semver.org/</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0058">
      <label>58</label>
      <mixed-citation publication-type="other" id="sbref1057">F. Pérez-García, fepegar/torchio: TorchIO: a Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning (Nov. 2020). doi:<pub-id pub-id-type="doi">10.5281/zenodo.4296288</pub-id></mixed-citation>
    </ref>
    <ref id="bib0059">
      <label>59</label>
      <element-citation publication-type="book" id="sbref0059">
        <person-group person-group-type="author">
          <name>
            <surname>Berger</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Eoin</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Cardoso</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Ourselin</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <part-title>An adaptive sampling scheme to efficiently train fully convolutional networks for semantic segmentation</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Nixon</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Mahmoodi</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zwiggelaar</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <source>Medical Image Understanding and Analysis</source>
        <series>Communications in Computer and Information Science</series>
        <year>2018</year>
        <publisher-name>Springer International Publishing</publisher-name>
        <publisher-loc>Cham</publisher-loc>
        <fpage>277</fpage>
        <lpage>286</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-319-95921-4_26</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0060">
      <label>60</label>
      <mixed-citation publication-type="other" id="sbref0060">M. McCormick, D. Zukić, S.A. on, ITK 5.2 Release Candidate 3 available for testing, 2021, <ext-link ext-link-type="uri" xlink:href="https://blog.kitware.com/itk-5-2-release-candidate-3-available-for-testing/" id="intrrf0006">https://blog.kitware.com/itk-5-2-release-candidate-3-available-for-testing/</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0061">
      <label>61</label>
      <element-citation publication-type="book" id="sbref0061">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>M.C.H.</given-names>
          </name>
          <name>
            <surname>Oktay</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Schuh</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Schaap</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Glocker</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <part-title>Image-and-spatial transformer networks for structure-guided image registration</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Shen</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Peters</surname>
            <given-names>T.M.</given-names>
          </name>
          <name>
            <surname>Staib</surname>
            <given-names>L.H.</given-names>
          </name>
          <name>
            <surname>Essert</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Yap</surname>
            <given-names>P.-T.</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <source>Medical Image Computing and Computer Assisted Intervention MICCAI 2019</source>
        <series>Lecture Notes in Computer Science</series>
        <year>2019</year>
        <publisher-name>Springer International Publishing</publisher-name>
        <publisher-loc>Cham</publisher-loc>
        <fpage>337</fpage>
        <lpage>345</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-030-32245-8_38</pub-id>
      </element-citation>
    </ref>
  </ref-list>
  <ack id="ack0001">
    <title>Acknowledgments</title>
    <p id="p0112">The authors would like to acknowledge all of the contributors to the TorchIO library. We thank the NiftyNet team for their support, and Alejandro Granados, Romain Valabregue, Fabien Girka, Ghiles Reguig, David Völgyes and Reuben Dorent for their valuable insight and contributions.</p>
    <p id="p0113">This work is supported by the Engineering and Physical Sciences Research Council (EPSRC) [EP/R512400/1]. This work is additionally supported by the EPSRC-funded UCL Centre for Doctoral Training in Intelligent, Integrated Imaging in Healthcare (i4health) [EP/S021930/1] and the Wellcome / EPSRC Centre for Interventional and Surgical Sciences (WEISS, UCL) [203145Z/16/Z]. This publication represents, in part, independent research commissioned by the Wellcome Innovator Award [218380/Z/19/Z/]. The views expressed in this publication are those of the authors and not necessarily those of the Wellcome Trust.</p>
  </ack>
  <fn-group>
    <fn id="fn0001">
      <label>1</label>
      <p id="notep0001"><ext-link ext-link-type="uri" xlink:href="https://github.com/PhoenixDL/rising" id="intrrf0002">https://github.com/PhoenixDL/rising</ext-link>.</p>
    </fn>
    <fn id="fn0002">
      <label>2</label>
      <p id="notep0002">https://brain-development.org/ixi-dataset/.</p>
    </fn>
    <fn id="fn0003">
      <label>3</label>
      <p id="notep0003">In this context, standardization refers to correcting voxel intensity values to have zero mean and unit variance.</p>
    </fn>
    <fn id="fn0004">
      <label>4</label>
      <p id="notep0004"><ext-link ext-link-type="uri" xlink:href="https://github.com/fepegar/torchio" id="intrrf0003">https://github.com/fepegar/torchio</ext-link>.</p>
    </fn>
    <fn id="fn0005">
      <label>5</label>
      <p id="notep0005"><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/" id="intrrf0004">https://zenodo.org/</ext-link>.</p>
    </fn>
    <fn id="fn0006">
      <label>6</label>
      <p id="notep0006"><ext-link ext-link-type="uri" xlink:href="https://pypistats.org/packages/torchio" id="intrrf0005">https://pypistats.org/packages/torchio</ext-link>.</p>
    </fn>
  </fn-group>
</back>
