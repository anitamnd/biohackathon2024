<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-title>BMC Bioinformatics</journal-title>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">2646186</article-id>
    <article-id pub-id-type="publisher-id">1471-2105-9-439</article-id>
    <article-id pub-id-type="pmid">18925941</article-id>
    <article-id pub-id-type="doi">10.1186/1471-2105-9-439</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CMA – a comprehensive Bioconductor package for supervised classification with high dimensional data</article-title>
    </title-group>
    <contrib-group>
      <contrib id="A1" contrib-type="author">
        <name>
          <surname>Slawski</surname>
          <given-names>M</given-names>
        </name>
        <xref ref-type="aff" rid="I1">1</xref>
        <email>martin.slawski@campus.lmu.de</email>
      </contrib>
      <contrib id="A2" contrib-type="author">
        <name>
          <surname>Daumer</surname>
          <given-names>M</given-names>
        </name>
        <xref ref-type="aff" rid="I1">1</xref>
        <email>daumer@slcmsr.org</email>
      </contrib>
      <contrib id="A3" corresp="yes" contrib-type="author">
        <name>
          <surname>Boulesteix</surname>
          <given-names>A-L</given-names>
        </name>
        <xref ref-type="aff" rid="I1">1</xref>
        <xref ref-type="aff" rid="I2">2</xref>
        <email>boulesteix@stat.uni-muenchen.de</email>
      </contrib>
    </contrib-group>
    <aff id="I1"><label>1</label>Sylvia Lawry Centre for Multiple Sclerosis Research, Hohenlindenerstr. 1, D-81677 Munich, Germany</aff>
    <aff id="I2"><label>2</label>Department of Statistics, University of Munich, Ludwigstr. 33, D-80539 Munich, Germany</aff>
    <pub-date pub-type="collection">
      <year>2008</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>16</day>
      <month>10</month>
      <year>2008</year>
    </pub-date>
    <volume>9</volume>
    <fpage>439</fpage>
    <lpage>439</lpage>
    <ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/9/439"/>
    <history>
      <date date-type="received">
        <day>3</day>
        <month>6</month>
        <year>2008</year>
      </date>
      <date date-type="accepted">
        <day>16</day>
        <month>10</month>
        <year>2008</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2008 Slawski et al; licensee BioMed Central Ltd.</copyright-statement>
      <copyright-year>2008</copyright-year>
      <copyright-holder>Slawski et al; licensee BioMed Central Ltd.</copyright-holder>
      <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0">
        <p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p>
        <!--<rdf xmlns="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:dcterms="http://purl.org/dc/terms"><Work xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" rdf:about=""><license rdf:resource="http://creativecommons.org/licenses/by/2.0"/><dc:type rdf:resource="http://purl.org/dc/dcmitype/Text"/><dc:author>
               Slawski
               M
               
               martin.slawski@campus.lmu.de
            </dc:author><dc:title>
            CMA &#x02013; a comprehensive Bioconductor package for supervised classification with high dimensional data
         </dc:title><dc:date>2008</dc:date><dcterms:bibliographicCitation>BMC Bioinformatics 9(1): 439-. (2008)</dcterms:bibliographicCitation><dc:identifier type="sici">1471-2105(2008)9:1&#x0003c;439&#x0003e;</dc:identifier><dcterms:isPartOf>urn:ISSN:1471-2105</dcterms:isPartOf><License rdf:about="http://creativecommons.org/licenses/by/2.0"><permits rdf:resource="http://web.resource.org/cc/Reproduction" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/Distribution" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Notice" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Attribution" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/DerivativeWorks" xmlns=""/></License></Work></rdf>-->
      </license>
    </permissions>
    <abstract>
      <sec>
        <title>Background</title>
        <p>For the last eight years, microarray-based classification has been a major topic in statistics, bioinformatics and biomedicine research. Traditional methods often yield unsatisfactory results or may even be inapplicable in the so-called "<italic>p </italic>≫ <italic>n</italic>" setting where the number of predictors <italic>p </italic>by far exceeds the number of observations <italic>n</italic>, hence the term "ill-posed-problem". Careful model selection and evaluation satisfying accepted good-practice standards is a very complex task for statisticians without experience in this area or for scientists with limited statistical background. The multiplicity of available methods for class prediction based on high-dimensional data is an additional practical challenge for inexperienced researchers.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>In this article, we introduce a new Bioconductor package called CMA (standing for "<bold>C</bold>lassification for <bold>M</bold>icro<bold>A</bold>rrays") for automatically performing variable selection, parameter tuning, classifier construction, and unbiased evaluation of the constructed classifiers using a large number of usual methods. Without much time and effort, users are provided with an overview of the unbiased accuracy of most top-performing classifiers. Furthermore, the standardized evaluation framework underlying CMA can also be beneficial in statistical research for comparison purposes, for instance if a new classifier has to be compared to existing approaches.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p>CMA is a user-friendly comprehensive package for classifier construction and evaluation implementing most usual approaches. It is freely available from the Bioconductor website at <ext-link ext-link-type="uri" xlink:href="http://bioconductor.org/packages/2.3/bioc/html/CMA.html"/>.</p>
      </sec>
    </abstract>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Background</title>
    <p>Conventional class prediction methods often yield poor results or may even be inapplicable in the context of high-dimensional data with more predictors than observations like microarray data. Microarray studies have thus stimulated the development of new approaches and motivated the adaptation of known traditional methods to the high-dimensional setting. Most of them are implemented in the R language [<xref ref-type="bibr" rid="B1">1</xref>] and freely available at <ext-link ext-link-type="uri" xlink:href="http://cran.r-project.org"/> or from the bioinformatics platform <ext-link ext-link-type="uri" xlink:href="http://www.bioconductor.org"/>. Meanwhile, the latter has established itself as a standard tool for analyzing various types of high-throughput genomic data including microarray data [<xref ref-type="bibr" rid="B2">2</xref>]. Throughout this article, the focus is on microarray data, but the presented package can be applied to any supervised classification problem involving a large number of continuous predictors such as, e.g. proteomic, metabolomic, or signal data. Model selection and evaluation of prediction rules turn out to be highly difficult in the <italic>p </italic>≫ <italic>n </italic>setting for several reasons: i) the hazard of overfitting, which is common to all prediction problems, is considerably increased by high dimensionality, ii) the usual evaluation scheme based on the splitting into learning and test data sets often applies only partially in the case of small samples, iii) modern classification techniques rely on the proper choice of hyperparameters whose optimization is highly computer-intensive, especially with high-dimensional data.</p>
    <p>The multiplicity of available methods for class prediction based on high-dimensional data is an additional practical challenge for inexperienced researchers. Whereas logistic regression is well-established as the standard method to be used when analyzing classical data sets with much more observations than variables (<italic>n </italic>&gt; <italic>p</italic>), there is no unique reference standard method for the <italic>n </italic>≪ <italic>p </italic>case. Moreover, the programs implementing well-known popular methods such as penalized logistic regression, nearest shrunken centroids [<xref ref-type="bibr" rid="B3">3</xref>], random forests [<xref ref-type="bibr" rid="B4">4</xref>], or partial least squares [<xref ref-type="bibr" rid="B5">5</xref>] are characterized by a high heterogeneity as far as input format, output format, and tuning procedures are concerned. Inexperienced users have thus to spend much effort understanding each of the programs and modifying the data formats, while potentially introducing severe errors which may considerably affect the final results. Furthermore, the users may overlook important tuning parameters or detail settings that sometimes noticeably contribute to the success of the classifier. Note that circumventing the problem of the multiplicity of methods by always using a single "favorite method" (usually the method in the user's expertise area or a method which has been identified as top-performing method in a seminal comparison study) potentially leads to poor results, especially when the considered method involves strong assumptions on the data structure.</p>
    <p>From the difficulties outlined above, we conclude that careful model selection and evaluation satisfying accepted good-practice standards [<xref ref-type="bibr" rid="B6">6</xref>] is a very complex task for inexperienced users with limited statistical background. In this article, we introduce a new Bioconductor package called CMA (standing for "<bold>C</bold>lassification for <bold>M</bold>icro<bold>A</bold>rrays") for automatically performing variable selection, parameter tuning, classifier construction, and unbiased evaluation of the constructed classifiers. The primary goal of CMA is to enable statisticians with limited experience on high-dimensional class prediction or biologists and bioinformaticians with statistical background to achieve such a demanding task on their own. Without much time and effort, users are provided with an overview of the unbiased accuracy of most top-performing classifiers. Furthermore, the standardized evaluation framework underlying CMA involving variable selection and hyperparameter tuning can also be beneficial for comparison purposes, for instance if a new classifier has to be compared to existing approaches.</p>
    <p>In a nutshell, CMA offers an interface to a total of more than twenty different classifiers, seven univariate and multivariate variable selection methods, different evaluation schemes (such as, e.g. cross-validation or bootstrap), and different measures of classification accuracy. A particular attention is devoted to preliminary variable selection and hyperparameter tuning, issues that are often neglected in current literature and software. More specifically, variable selection is always performed using the training data only, i.e. for each iteration successively in the case of cross-validation, following well-established good-practice guidelines [<xref ref-type="bibr" rid="B6">6</xref>-<xref ref-type="bibr" rid="B9">9</xref>]. Hyperparameter tuning is performed through an inner cross-validation loop, as usually recommended [<xref ref-type="bibr" rid="B10">10</xref>]. This feature is intended to prevent users from trying several hyperparameter values on their own and selecting the best results a posteriori, a strategy which would obviously lead to severe bias [<xref ref-type="bibr" rid="B11">11</xref>].</p>
    <p>The CMA package is freely available from the Bioconductor website at <ext-link ext-link-type="uri" xlink:href="http://bioconductor.org/packages/2.3/bioc/html/CMA.html"/></p>
    <sec>
      <title>Overview of existing packages</title>
      <p>The idea of an R interface for the integration of microarray-based classification methods is not new. The CMA package shows similarities to the Bioconductor package 'MLInterfaces' standing for "An interface to various machine learning methods" [<xref ref-type="bibr" rid="B12">12</xref>], see also the Bioconductor textbook [<xref ref-type="bibr" rid="B13">13</xref>] for a presentation of an older version. The MLInterfaces package includes numerous facilities such as the unified MLearn interface, the flexible learnerSchema design enabling the introduction of new procedures on the y, and the xvalSpec interface that allows arbitrary types of resampling and cross-validation to be employed. MLearn also returns the native R object from the learner for further interrogation. The package architecture of MLInterfaces is similar the CMA structure in the sense that wrapper functions are used to call classification methods from other packages.</p>
      <p>However, CMA includes additional predefined features as far as variable selection, hyperparameter tuning, classifier evaluation and comparison are concerned. While the method xval is flexible for experienced users, it provides only cross-validation (including leave-one-out) as predefined option. As the CMA package also addresses inexperienced users, it includes the most common validation schemes in a standardized manner. In the current version of MLInterfaces, variable selection can also be carried out separately for each different learning set, but it does not seem to be a standard procedure. In the examples presented in the Bioconductor textbook [<xref ref-type="bibr" rid="B13">13</xref>], variable selection is only performed once using the complete sample. In contrast, CMA performs variable selection separately for each learning set by default. Further, CMA includes additional features for hyperparameter tuning, thus allowing an objective comparison of different class prediction methods. If tuning is ignored, simpler methods without (or with few) tuning parameters tend to perform seemingly better than more complex algorithms. CMA also implements additional measures of prediction accuracy and user-friendly visualization tools.</p>
      <p>The package 'MCRestimate' [<xref ref-type="bibr" rid="B14">14</xref>,<xref ref-type="bibr" rid="B15">15</xref>] emphasizes very similar aspects as CMA, focussing on the estimation of misclassification rates and cross-validation for model selection and evaluation. It is (to our knowledge) the only Bioconductor package beside ours supporting hyperparameter tuning and the workflow is fully compatible with good practice standards. The advances of CMA compared to MCRestimate are summarized below. CMA includes much more classifiers (21 in the current version), which allows a comfortable extensive comparison without much effort. In particular, it provides an interface to recent machine learning methods, including two highly competitive boosting methods (tree-based and componentwise boosting). CMA also allows to pass arguments to the classifier, which may be useful in some cases, for instance to reduce the number of trees in a random forest for computational reasons. Furthermore, all the methods included in CMA support multi-class response variables, even the methods based on logistic regression (which can only be applied to binary response variables in MCRestimate). A very wide range of variable selection methods are available from CMA, e.g. fast implementations of important univariate test statistics including typical multi-class approach (Kruskal-Wallis/F-test). Moreover, CMA offers the possibility of constructing classifiers in a hybrid way: variable selection can be performed via the lasso and subsequently plugged into another algorithm. In addition to cross-validation, evaluation can be performed based on several most often used schemes such as bootstrap (and the associated '0.632' or '0.632+' estimators) or repeated subsampling. The definition of the learning sets can also be customized, which may be an advantage when, e.g. one wants to evaluate a classifier based on a single split learning/test data, as usual in the context of validation. CMA also includes additional accuracy measures which are commonly used in medical research. Convivial visualization tools are provided at the intention of either statisticians or practitioners. When several classifiers are run, the compare function produces ready-to-use tables listing different performance measures for several classifiers.</p>
      <p>From the technical point of view, an additional advance is that CMA's implementation is fully organized in S4 classes, which bears advantages for both experienced users (who may easily incorporate their own functions) and inexperienced users (who have access to convenient visualization tools without entering much code). As a consequence, CMA has a clear inherent modular structure. The use of S4 classes is highly beneficial when adding new features, because it requires at most changes in one 'building block'. Furthermore, S4 classes offer the advantage of specifying the input data in manifold ways, depending on the user's needs. For example, the CMA users can enter their gene expression data as matrices, data frames combined with formulae, or ExpressionSets.</p>
    </sec>
    <sec>
      <title>Overview of class prediction with high-dimensional data and notations</title>
      <sec>
        <title>Settings and Notation</title>
        <p>The classification problem can be briefly outlined as follows. We have a predictor space <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M1" name="1471-2105-9-439-i1" overflow="scroll"><mml:semantics><mml:mi mathvariant="script">X</mml:mi></mml:semantics></mml:math></inline-formula>, here <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M2" name="1471-2105-9-439-i1" overflow="scroll"><mml:semantics><mml:mi mathvariant="script">X</mml:mi></mml:semantics></mml:math></inline-formula> ⊆ ℝ<sup><italic>p </italic></sup>(for instance, the predictors may be gene expression levels, but the scope of CMA is not limited to this case). The finite set of class labels is denoted as <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M3" name="1471-2105-9-439-i2" overflow="scroll"><mml:semantics><mml:mi mathvariant="script">Y</mml:mi></mml:semantics></mml:math></inline-formula> = {0, ..., <italic>K </italic>- 1}, with <italic>K </italic>standing for the total number of classes, and <italic>P</italic>(<bold>x</bold>, <italic>y</italic>) denotes the joint probability distribution on <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M4" name="1471-2105-9-439-i3" overflow="scroll"><mml:semantics><mml:mrow><mml:mi mathvariant="script">X</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="script">Y</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula>. We are given a finite sample <italic>S </italic>= {(<bold>x</bold><sub>1</sub>, <italic>y</italic><sub>1</sub>),...,(<bold>x</bold><sub><italic>n</italic></sub>, <italic>y</italic><sub><italic>n</italic></sub>)} of <italic>n </italic>predictor-class pairs. The considered task is to construct a decision function</p>
        <p>
          <disp-formula>
            <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M5" name="1471-2105-9-439-i4" overflow="scroll">
              <mml:semantics>
                <mml:mrow>
                  <mml:mtable columnalign="left">
                    <mml:mtr columnalign="left">
                      <mml:mtd columnalign="left">
                        <mml:mrow>
                          <mml:mover accent="true">
                            <mml:mi>f</mml:mi>
                            <mml:mo>^</mml:mo>
                          </mml:mover>
                          <mml:mo>:</mml:mo>
                        </mml:mrow>
                      </mml:mtd>
                      <mml:mtd columnalign="left">
                        <mml:mrow>
                          <mml:mi mathvariant="script">X</mml:mi>
                          <mml:mo>→</mml:mo>
                          <mml:mi mathvariant="script">Y</mml:mi>
                        </mml:mrow>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr columnalign="left">
                      <mml:mtd columnalign="left">
                        <mml:mrow/>
                      </mml:mtd>
                      <mml:mtd columnalign="left">
                        <mml:mrow>
                          <mml:mstyle mathvariant="bold" mathsize="normal">
                            <mml:mi>x</mml:mi>
                          </mml:mstyle>
                          <mml:mo>↦</mml:mo>
                          <mml:mover accent="true">
                            <mml:mi>f</mml:mi>
                            <mml:mo>^</mml:mo>
                          </mml:mover>
                          <mml:mo stretchy="false">(</mml:mo>
                          <mml:mstyle mathvariant="bold" mathsize="normal">
                            <mml:mi>x</mml:mi>
                          </mml:mstyle>
                          <mml:mo stretchy="false">)</mml:mo>
                        </mml:mrow>
                      </mml:mtd>
                    </mml:mtr>
                  </mml:mtable>
                </mml:mrow>
              </mml:semantics>
            </mml:math>
          </disp-formula>
        </p>
        <p>such that the <italic>generalization error</italic></p>
        <p>
          <disp-formula id="bmcM1">
            <label>(1)</label>
            <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M6" name="1471-2105-9-439-i5" overflow="scroll">
              <mml:semantics>
                <mml:mrow>
                  <mml:mi>R</mml:mi>
                  <mml:mo stretchy="false">[</mml:mo>
                  <mml:mi>f</mml:mi>
                  <mml:mo stretchy="false">]</mml:mo>
                  <mml:mo>=</mml:mo>
                  <mml:msub>
                    <mml:mstyle mathvariant="bold" mathsize="normal">
                      <mml:mi>E</mml:mi>
                    </mml:mstyle>
                    <mml:mi>P</mml:mi>
                  </mml:msub>
                  <mml:mo stretchy="false">[</mml:mo>
                  <mml:mi>L</mml:mi>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mover accent="true">
                    <mml:mi>f</mml:mi>
                    <mml:mo>^</mml:mo>
                  </mml:mover>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mstyle mathvariant="bold" mathsize="normal">
                    <mml:mi>x</mml:mi>
                  </mml:mstyle>
                  <mml:mo stretchy="false">)</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mi>y</mml:mi>
                  <mml:mo stretchy="false">)</mml:mo>
                  <mml:mo stretchy="false">]</mml:mo>
                  <mml:mo>=</mml:mo>
                  <mml:mstyle displaystyle="true">
                    <mml:mrow>
                      <mml:msub>
                        <mml:mo>∫</mml:mo>
                        <mml:mrow>
                          <mml:mi mathvariant="script">X</mml:mi>
                          <mml:mo>×</mml:mo>
                          <mml:mi mathvariant="script">Y</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mrow>
                        <mml:mi>L</mml:mi>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mi>y</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mover accent="true">
                          <mml:mi>f</mml:mi>
                          <mml:mo>^</mml:mo>
                        </mml:mover>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mstyle mathvariant="bold" mathsize="normal">
                          <mml:mi>x</mml:mi>
                        </mml:mstyle>
                        <mml:mo stretchy="false">)</mml:mo>
                        <mml:mo stretchy="false">)</mml:mo>
                        <mml:mtext> </mml:mtext>
                        <mml:mi>d</mml:mi>
                        <mml:mi>P</mml:mi>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mstyle mathvariant="bold" mathsize="normal">
                          <mml:mi>x</mml:mi>
                        </mml:mstyle>
                        <mml:mo>,</mml:mo>
                        <mml:mi>y</mml:mi>
                        <mml:mo stretchy="false">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:mstyle>
                </mml:mrow>
              </mml:semantics>
            </mml:math>
          </disp-formula>
        </p>
        <p>is minimized, where <italic>L</italic>(·,·) is a suitable loss function, usually taken to be the indicator loss (<italic>L</italic>(<italic>u</italic>, <italic>v</italic>) = 1 if <italic>u </italic>≠ <italic>v</italic>, <italic>L</italic>(<italic>u</italic>, <italic>v</italic>) = 0 otherwise). Other loss functions and performances measures are discussed extensively in section 3.1.5. The symbol ^ indicates that the function is estimated from the given sample <italic>S</italic>.</p>
      </sec>
      <sec>
        <title>Estimation of the generalization error</title>
        <p>As we are only equipped with a finite sample <italic>S </italic>and the underlying distribution is unknown, approximations to Eq. (1) have to be found. The empirical counterpart to <italic>R </italic>[<italic>f</italic>]</p>
        <p>
          <disp-formula id="bmcM2">
            <label>(2)</label>
            <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M7" name="1471-2105-9-439-i6" overflow="scroll">
              <mml:semantics>
                <mml:mrow>
                  <mml:msub>
                    <mml:mi>R</mml:mi>
                    <mml:mrow>
                      <mml:mtext>emp</mml:mtext>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo stretchy="false">[</mml:mo>
                  <mml:mi>f</mml:mi>
                  <mml:mo stretchy="false">]</mml:mo>
                  <mml:mo>=</mml:mo>
                  <mml:msup>
                    <mml:mi>n</mml:mi>
                    <mml:mrow>
                      <mml:mo>−</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:msup>
                  <mml:mstyle displaystyle="true">
                    <mml:munderover>
                      <mml:mo>∑</mml:mo>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>n</mml:mi>
                    </mml:munderover>
                    <mml:mrow>
                      <mml:mi>L</mml:mi>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:msub>
                        <mml:mi>y</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                      <mml:mo>,</mml:mo>
                      <mml:mover accent="true">
                        <mml:mi>f</mml:mi>
                        <mml:mo>^</mml:mo>
                      </mml:mover>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:msub>
                        <mml:mstyle mathvariant="bold" mathsize="normal">
                          <mml:mi>x</mml:mi>
                        </mml:mstyle>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                      <mml:mo stretchy="false">)</mml:mo>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mstyle>
                </mml:mrow>
              </mml:semantics>
            </mml:math>
          </disp-formula>
        </p>
        <p>has a (usually large) negative bias, i.e. prediction error is underestimated. Moreover, choosing the best classifier based on Eq. (2) potentially leads to the selection of a classifier overfitting the sample <italic>S </italic>which may show poor performance on independent data. More details can be found in recent overview articles [<xref ref-type="bibr" rid="B16">16</xref>-<xref ref-type="bibr" rid="B18">18</xref>]. A better strategy consists of splitting <italic>S </italic>into distinct subsets ℒ (learning sample) and <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M8" name="1471-2105-9-439-i8" overflow="scroll"><mml:semantics><mml:mi mathvariant="script">T</mml:mi></mml:semantics></mml:math></inline-formula> (test sample) with the intention to separate model selection and model evaluation. The classifier <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M9" name="1471-2105-9-439-i9" overflow="scroll"><mml:semantics><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:semantics></mml:math></inline-formula>(·) is constructed using ℒ only and evaluated using <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M10" name="1471-2105-9-439-i8" overflow="scroll"><mml:semantics><mml:mi mathvariant="script">T</mml:mi></mml:semantics></mml:math></inline-formula> only, as depicted in Figure <xref ref-type="fig" rid="F1">1</xref> (top).</p>
        <fig position="float" id="F1">
          <label>Figure 1</label>
          <caption>
            <p><bold>Evaluation schemes</bold>. The top panel illustrates the splitting into learning and test data sets. The whole sample <italic>S </italic>is split into a learning set ℒ and a test set <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M11" name="1471-2105-9-439-i8" overflow="scroll"><mml:semantics><mml:mi mathvariant="script">T</mml:mi></mml:semantics></mml:math></inline-formula>. The classifier <italic>f</italic>(·) is constructed using the learning set ℒ and subsequently applied to the test set <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M12" name="1471-2105-9-439-i8" overflow="scroll"><mml:semantics><mml:mi mathvariant="script">T</mml:mi></mml:semantics></mml:math></inline-formula>. The bottom panel displays schematically <italic>k</italic>-fold cross-validation (left), Monte-Carlo cross-validation with <italic>n </italic>= 5 and ntrain = 3 (middle), and bootstrap sampling (with replacement) with <italic>n </italic>= 5 and ntrain = 3 (right).</p>
          </caption>
          <graphic xlink:href="1471-2105-9-439-1"/>
        </fig>
        <p>In microarray data, the sample size <italic>n </italic>is usually very small, leading to serious problems for both the construction of the classifier and the estimation of its prediction accuracy. Increasing the size of the learning set (<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M13" name="1471-2105-9-439-i10" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi mathvariant="script">L</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> → <italic>n</italic>) typically improves the constructed prediction rule <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M14" name="1471-2105-9-439-i9" overflow="scroll"><mml:semantics><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:semantics></mml:math></inline-formula>(·), but decreases the reliability of its evaluation. Conversely, increasing the size of the test set (<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M15" name="1471-2105-9-439-i43" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi mathvariant="script">T</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> → <italic>n</italic>) improves the accuracy estimation, but leads to poor classifiers, since these are based on fewer observations. While a compromise can be found if the sample size is large enough, alternative designs are needed for the case of small sizes. The CMA package implements several approaches which are all based on the following scheme.</p>
        <p>1. Generate <italic>B </italic>learning sets <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M16" name="1471-2105-9-439-i11" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> (<italic>b </italic>= 1, ..., <italic>B</italic>) from <italic>S </italic>and define the corresponding test set as <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M17" name="1471-2105-9-439-i12" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mo>\</mml:mo><mml:msub><mml:mi>ℒ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula></p>
        <p>2. Obtain <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M18" name="1471-2105-9-439-i13" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> (·) from <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M19" name="1471-2105-9-439-i11" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, for <italic>b </italic>= 1, ..., <italic>B</italic>.</p>
        <p>3. The quantity</p>
        <p>
          <disp-formula id="bmcM3">
            <label>(3)</label>
            <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M20" name="1471-2105-9-439-i14" overflow="scroll">
              <mml:semantics>
                <mml:mrow>
                  <mml:mover accent="true">
                    <mml:mi>ϵ</mml:mi>
                    <mml:mo>^</mml:mo>
                  </mml:mover>
                  <mml:mo>=</mml:mo>
                  <mml:mfrac>
                    <mml:mn>1</mml:mn>
                    <mml:mi>B</mml:mi>
                  </mml:mfrac>
                  <mml:mstyle displaystyle="true">
                    <mml:munderover>
                      <mml:mo>∑</mml:mo>
                      <mml:mrow>
                        <mml:mi>b</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>B</mml:mi>
                    </mml:munderover>
                    <mml:mrow>
                      <mml:mfrac>
                        <mml:mn>1</mml:mn>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:mo>|</mml:mo>
                            <mml:mrow>
                              <mml:msub>
                                <mml:mi mathvariant="script">T</mml:mi>
                                <mml:mi>b</mml:mi>
                              </mml:msub>
                            </mml:mrow>
                            <mml:mo>|</mml:mo>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mrow>
                  </mml:mstyle>
                  <mml:mstyle displaystyle="true">
                    <mml:munder>
                      <mml:mo>∑</mml:mo>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mo>∈</mml:mo>
                        <mml:msub>
                          <mml:mi mathvariant="script">T</mml:mi>
                          <mml:mi>b</mml:mi>
                        </mml:msub>
                      </mml:mrow>
                    </mml:munder>
                    <mml:mrow>
                      <mml:mi>L</mml:mi>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:msub>
                        <mml:mi>y</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                      <mml:mo>,</mml:mo>
                      <mml:mover accent="true">
                        <mml:mrow>
                          <mml:msub>
                            <mml:mi>f</mml:mi>
                            <mml:mi>b</mml:mi>
                          </mml:msub>
                        </mml:mrow>
                        <mml:mo stretchy="true">^</mml:mo>
                      </mml:mover>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:msub>
                        <mml:mstyle mathvariant="bold" mathsize="normal">
                          <mml:mi>x</mml:mi>
                        </mml:mstyle>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                      <mml:mo stretchy="false">)</mml:mo>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mstyle>
                </mml:mrow>
              </mml:semantics>
            </mml:math>
          </disp-formula>
        </p>
        <p>is then used as an estimator of the error rate, where |·| stands for the cardinality of the considered set.</p>
        <p>The underlying idea is to reduce the variance of the error estimator by averaging, in the spirit of the bagging principle introduced by Breiman [<xref ref-type="bibr" rid="B19">19</xref>]. The function GenerateLearningsets from the package CMA implements several methods for generating <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M21" name="1471-2105-9-439-i11" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> and <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M22" name="1471-2105-9-439-i15" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> in step 1, which are described below.</p>
        <p>LOOCV <bold>Leaving-one-out cross-validation</bold></p>
        <p>For the <italic>b</italic>-th iteration, <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M23" name="1471-2105-9-439-i15" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> consists of the <italic>b</italic>-th observation only. This is repeated for each observation in <italic>S</italic>, so that <italic>B </italic>= <italic>n</italic>.</p>
        <p>CV <italic>k</italic>-<bold>fold cross-validation </bold>(method = "CV", fold, niter)</p>
        <p><italic>S </italic>is split into fold non-overlapping subsets of approximately equal size. For each iteration <italic>b</italic>, the <italic>b</italic>-th subset is used as <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M24" name="1471-2105-9-439-i15" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> and the union of the remaining subsets as <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M25" name="1471-2105-9-439-i11" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, such that <italic>B </italic>= fold. Setting fold = n is equivalent to method = "LOOCV". For fold &lt;<italic>n</italic>, the splitting is not uniquely determined. It is thus recommended to repeat the whole procedure niter times [<xref ref-type="bibr" rid="B16">16</xref>] (for instance niter = 5 or niter = 10) to partly average out random variations.</p>
        <p>MCCV <bold>Monte-Carlo-cross-validation </bold>(method = "MCCV", fold, ntrain, niter)</p>
        <p>Each of the <italic>B </italic>= niter learning sets of cardinality ntrain is drawn randomly from <italic>S </italic>without replacement. The argument ntrain specifies the number of observations to be included in each learning set.</p>
        <p>boot <bold>Bootstrap </bold>(method = "bootstrap", ntrain, niter)</p>
        <p><italic>B </italic>= niter bootstrap samples (drawn with replacement) [<xref ref-type="bibr" rid="B20">20</xref>] of cardinality ntrain are used as learning sets. In practice, ntrain is usually set to the total sample size <italic>n</italic>.</p>
        <p>A schematic representation of CV, MCCV and bootstrap sampling is provided in Figure <xref ref-type="fig" rid="F1">1</xref> (bottom). "Stratified sampling" is possible by setting strat = TRUE. This implies that, in each learning set <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M26" name="1471-2105-9-439-i11" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, the proportion of the classes {0, ..., <italic>K </italic>- 1} is approximately the same as in <italic>S</italic>. This option is very useful (and sometimes even necessary) in order to guarantee that each class is sufficiently represented in each <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M27" name="1471-2105-9-439-i11" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>, in particular if there are classes of small size. For more details on the evaluation of classifiers, readers may refer to recent overview articles discussing the respective drawbacks and advantages of these methods in [<xref ref-type="bibr" rid="B16">16</xref>,<xref ref-type="bibr" rid="B17">17</xref>].</p>
        <p>Note that, if one employs a method to impute missing values making use of class label information, imputation should be performed for each learning set separately. This procedure is not supported by CMA. Instead, we recommend to impute missing values before beginning the analyses with CMA using a package like 'impute' [<xref ref-type="bibr" rid="B21">21</xref>] that does not involve class label information.</p>
        <p>In CMA, cross-validation is also used for hyperparameter tuning. The optimal value(s) of the method parameter(s) is(are) determined within an inner cross-validation, as commonly recommended [<xref ref-type="bibr" rid="B10">10</xref>,<xref ref-type="bibr" rid="B11">11</xref>]. If cross-validation is used for both tuning parameters and evaluating a classifiers, the whole procedure is denoted as nested cross-validation. See Figure <xref ref-type="fig" rid="F2">2</xref> for a schematic representation and Section 3.1.4 for more details on hyperparameter tuning.</p>
        <fig position="float" id="F2">
          <label>Figure 2</label>
          <caption>
            <p><bold>Hyperparameter tuning</bold>. Schematic display of nested cross-validation. In the procedure displayed above, <italic>k</italic>-fold cross-validation is used for evaluation purposes, whereas tuning is performed within each iteration using inner (<italic>l</italic>-fold) cross-validation.</p>
          </caption>
          <graphic xlink:href="1471-2105-9-439-2"/>
        </fig>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>2 Implementation</title>
    <p>The Bioconductor package CMA is user-friendly in the sense that (i) the methods automatically adapt to the data format provided by the user; (ii) convenient functions take over frequent tasks such as automatic visualization of results; (iii) reasonable default settings for hyperparameter tuning and other parameters requiring expert knowledge of particular classifiers are provided; (iv) it works with uniform data structures. To do so, CMA exploits the rich possibilities of object-oriented programming as realized by S4 classes of the methods package [<xref ref-type="bibr" rid="B22">22</xref>] which make it easy to incorporate new features into an existing framework. For instance, with some basic knowledge of the S4 class system (which is standard for bioconductor packages), users can easily embed new classification methods in addition to the 21 currently available in CMA. Moreover, the process of classifier building described in more detail in section 3.1.2 can either be partitioned into several transparent small steps (variable selection, hyperparameter tuning, etc) or executed by only one compact function call. The last feature is beneficial for users who are not very familiar with R commands.</p>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 CMA features</title>
      <sec>
        <title>3.1.1 Overview</title>
        <p>The package offers a uniform, user-friendly interface to a total of more than twenty classification methods (see Table <xref ref-type="table" rid="T1">1</xref>) comprising classical approaches as well as more sophisticated methods. User-friendliness means that the input formats are the same for all implemented methods, that the user may choose between three different input formats and that the output is self-explicable and informative. The implementation is fully organized in S4 classes, thus making the extension of CMA very easy. In particular, own classification methods can easily be integrated if they return a proper object of class cloutput. In addition to the packages listed in Table <xref ref-type="table" rid="T1">1</xref>, CMA only requires the package 'limma' for full functionality. For all other features, no code of foreign packages is used. Like most R packages, CMA is more flexible than, e.g., web-based tools. Experienced users can easily modify the programs or add new methods.</p>
        <table-wrap position="float" id="T1">
          <label>Table 1</label>
          <caption>
            <p>Overview of the classification methods in CMA.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <td align="left">
                  <bold>Method name</bold>
                </td>
                <td align="left">
                  <bold>CMA function name</bold>
                </td>
                <td align="left">
                  <bold>Package</bold>
                </td>
                <td align="left">
                  <bold>Reference</bold>
                </td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left">Componentwise boosting</td>
                <td align="left">compBoostCMA</td>
                <td align="left">CMA</td>
                <td align="left">[<xref ref-type="bibr" rid="B39">39</xref>]</td>
              </tr>
              <tr>
                <td align="left">Diagonal discriminant analysis</td>
                <td align="left">dldaCMA</td>
                <td align="left">CMA</td>
                <td align="left">[<xref ref-type="bibr" rid="B56">56</xref>]</td>
              </tr>
              <tr>
                <td align="left">Elastic net</td>
                <td align="left">ElasticNetCMA</td>
                <td align="left">'glmpath'</td>
                <td align="left">[<xref ref-type="bibr" rid="B29">29</xref>]</td>
              </tr>
              <tr>
                <td align="left">Fisher's discriminant analysis</td>
                <td align="left">fdaCMA</td>
                <td align="left">CMA</td>
                <td align="left">[<xref ref-type="bibr" rid="B24">24</xref>]</td>
              </tr>
              <tr>
                <td align="left">Flexible discriminant analysis</td>
                <td align="left">flexdaCMA</td>
                <td align="left">'mgcv'</td>
                <td align="left">[<xref ref-type="bibr" rid="B24">24</xref>]</td>
              </tr>
              <tr>
                <td align="left">Tree-based boosting</td>
                <td align="left">gbmCMA</td>
                <td align="left">'gbm'</td>
                <td align="left">[<xref ref-type="bibr" rid="B33">33</xref>]</td>
              </tr>
              <tr>
                <td align="left"><italic>k</italic>-nearest neighbors</td>
                <td align="left">knnCMA</td>
                <td align="left">'class'</td>
                <td align="left">[<xref ref-type="bibr" rid="B24">24</xref>]</td>
              </tr>
              <tr>
                <td align="left">Linear discriminant analysis *</td>
                <td align="left">ldaCMA</td>
                <td align="left">'MASS'</td>
                <td align="left">[<xref ref-type="bibr" rid="B56">56</xref>]</td>
              </tr>
              <tr>
                <td align="left">Lasso</td>
                <td align="left">LassoCMA</td>
                <td align="left">'glmpath'</td>
                <td align="left">[<xref ref-type="bibr" rid="B57">57</xref>]</td>
              </tr>
              <tr>
                <td align="left">Feed-forward neural networks</td>
                <td align="left">nnetCMA</td>
                <td align="left">'nnet'</td>
                <td align="left">[<xref ref-type="bibr" rid="B24">24</xref>]</td>
              </tr>
              <tr>
                <td align="left">Probalistic nearest neighbors</td>
                <td align="left">pknnCMA</td>
                <td align="left">CMA</td>
                <td align="left">-</td>
              </tr>
              <tr>
                <td align="left">Penalized logistic regression</td>
                <td align="left">plrCMA</td>
                <td align="left">CMA</td>
                <td align="left">[<xref ref-type="bibr" rid="B58">58</xref>]</td>
              </tr>
              <tr>
                <td align="left">Partial Least Squares ⋆ + *</td>
                <td align="left">pls_ldaCMA</td>
                <td align="left">'plsgenomics'</td>
                <td align="left">[<xref ref-type="bibr" rid="B5">5</xref>]</td>
              </tr>
              <tr>
                <td align="left">⋆ + logistic regression</td>
                <td align="left">pls_lrCMA</td>
                <td align="left">'plsgenomics'</td>
                <td align="left">[<xref ref-type="bibr" rid="B5">5</xref>]</td>
              </tr>
              <tr>
                <td align="left">⋆ + random forest</td>
                <td align="left">pls_rfCMA</td>
                <td align="left">'plsgenomics'</td>
                <td align="left">[<xref ref-type="bibr" rid="B5">5</xref>]</td>
              </tr>
              <tr>
                <td align="left">Probabilistic neural networks</td>
                <td align="left">pnnCMA</td>
                <td align="left">CMA</td>
                <td align="left">[<xref ref-type="bibr" rid="B59">59</xref>]</td>
              </tr>
              <tr>
                <td align="left">Quadratic discriminant analysis </td>
                <td align="left">qdaCMA</td>
                <td align="left">'MASS'</td>
                <td align="left">[<xref ref-type="bibr" rid="B56">56</xref>]</td>
              </tr>
              <tr>
                <td align="left">Random forest</td>
                <td align="left">rfCMA</td>
                <td align="left">'randomForest'</td>
                <td align="left">[<xref ref-type="bibr" rid="B4">4</xref>]</td>
              </tr>
              <tr>
                <td align="left">PAM</td>
                <td align="left">scdaCMA</td>
                <td align="left">CMA</td>
                <td align="left">[<xref ref-type="bibr" rid="B44">44</xref>]</td>
              </tr>
              <tr>
                <td align="left">Shrinkage discriminant analysis</td>
                <td align="left">shrinkldaCMA</td>
                <td align="left">CMA</td>
                <td align="left">-</td>
              </tr>
              <tr>
                <td align="left">Support vector machines</td>
                <td align="left">svmCMA</td>
                <td align="left">'e1071'</td>
                <td align="left">[<xref ref-type="bibr" rid="B60">60</xref>]</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <p>The first column gives the method name, whereas the name of the classifier in the CMA package is given in the second column. For each classifier, CMA uses either own code or code borrowed from another package, as specified in the third column.</p>
          </table-wrap-foot>
        </table-wrap>
        <p>Moreover, CMA automatically performs all important steps towards the construction and evaluation of classifiers. It can generate learning samples as explained in section 1, including the generation of stratified samples. Different schemes for generating learning sets and test sets are displayed schematically in Figure <xref ref-type="fig" rid="F1">1</xref> (bottom). The method GeneSelection provides optional variable selection preceding classification for each iteration <italic>b </italic>= 1, ..., <italic>B </italic>separately, based on various ranking procedures, whereas the method tune carries out hyperparameter tuning for a <italic>fixed </italic>(sub-)set of variables. It can be performed in a fully automatic manner using pre-defined grids. Alternatively, it can be completely customized by the experienced user. Performance can be assessed using the method evaluation for several performance measures commonly used in practice. Comparison of the performance of several classifiers can be quickly tabulated and visualized using the method comparison. Moreover, estimations of conditional class probabilities for predicted observations are provided by most of the classifiers, with only a few exceptions. This is more informative than only returning class labels and allows a more precise comparison of different classifiers. Last but not least, most results can conveniently be summarized and visualized using pre-defined convenience methods as demonstrated in section 3.2. For example, plot, cloutput-method produces probability plots, also known as "voting plot", plot, genesel-method visualizes variable importance as derived from one of the ranking procedures via a barplot, roc, cloutput-method draws empirical ROC curves, toplist, genesel-method lists the most relevant variables, and summary, evaloutput-method makes a summary out of iteration- or observationwise performance measures.</p>
      </sec>
      <sec>
        <title>3.1.2 Classification methods</title>
        <p>This subsection gives a brief summarizing overview of the classifiers implemented in CMA. We have tried to compose a balanced mixture of methods from several fields although we do not claim our selection to be representative, taking into account the large amount of literature on this subject. For more detailed information on the single methods, readers are referred to the references given in Table <xref ref-type="table" rid="T1">1</xref> and the references therein. All classifiers can be constructed using the CMA method classification, where the argument classifier specifies the classification method to be used.</p>
        <sec>
          <title>Discriminant Analysis</title>
          <p>Discriminant analysis is the (Bayes-)optimal classifier if the conditional distributions of the predictors given the classes are Gaussian. Diagonal, linear and quadratic discriminant analysis differ only by their assumptions for the (conditional) covariance matrices <bold>Σ</bold><sub><italic>k </italic></sub>= Cov(<bold>x</bold>|<italic>y </italic>= <italic>k</italic>), <italic>k </italic>= 0, ..., <italic>K </italic>- 1.</p>
          <p>(a) Diagonal linear discriminant analysis (classifier = "dldaCMA") assumes that the within-class covariance matrices <bold>Σ</bold><sub><italic>k </italic></sub>are diagonal and equal for all classes, i.e. <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M28" name="1471-2105-9-439-i16" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Σ</mml:mi><mml:mo>=</mml:mo><mml:mtext>diag</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>, <italic>k </italic>= 1, ..., <italic>K </italic>- 1, thus requiring the estimation of only <italic>p </italic>covariance parameters.</p>
          <p>(b) Linear discriminant analysis (classifier = "ldaCMA") assumes <bold>Σ</bold><sub><italic>k </italic></sub>= <bold>Σ</bold>, <italic>k </italic>= 1, ..., <italic>K </italic>- 1 without further restrictions for <bold>Σ </bold>so that <italic>p</italic>(<italic>p </italic>+ 1)/2 parameters have to be estimated.</p>
          <p>(c) Quadratic discriminant analysis (classifier = "qdaCMA") does not impose any particular restriction on <bold>Σ</bold><sub><italic>k</italic></sub>, <italic>k </italic>= 1, ..., <italic>K </italic>- 1.</p>
          <p>While (a) turns out to be still practicable for microarray data, linear and quadratic discriminant analysis are not competitive in this setting, at least not without dimension reduction or excessive variable selection (see below).</p>
          <p>The so-called PAM method (standing for "Prediction Analysis for Microarrays"), which is also commonly denoted as "shrunken centroids discriminant analysis" can be viewed as a modification of diagonal discriminant analysis (also referred to as "naive Bayes" classifier) using univariate soft thresholding [<xref ref-type="bibr" rid="B23">23</xref>] to perform variable selection and yield stabilized estimates of the variance parameters (classifier = "scdaCMA").</p>
          <p>Fisher's discriminant analysis (FDA) (classifier = "fdaCMA") has a different motivation, but can be shown to be equivalent to linear discriminant analysis under certain assumptions. It looks for projections <bold>a</bold><sup><bold>T</bold></sup><bold>x </bold>such that the ratio of between-class and within-class variance is maximized, leading to a linear decision function in a lower dimensional space. Flexible discriminant analysis (classifier = "flexdaCMA") can be interpreted as FDA in a higher-dimensional space generated by basis functions, also allowing nonlinear decision functions [<xref ref-type="bibr" rid="B24">24</xref>]. In CMA, the basis functions are given by penalized splines as implemented in the R package 'mgcv' [<xref ref-type="bibr" rid="B25">25</xref>].</p>
          <p>Shrinkage discriminant analysis [<xref ref-type="bibr" rid="B26">26</xref>,<xref ref-type="bibr" rid="B27">27</xref>] (classifier = "shrinkldaCMA") tries to stabilize covariance estimation by shrinking the unrestricted covariance matrix from linear discriminant analysis to a more simply structured target covariance matrix, e.g. a diagonal matrix.</p>
        </sec>
        <sec>
          <title>Partial Least Squares</title>
          <p>Partial Least Squares is a dimension reduction method that looks for directions <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M29" name="1471-2105-9-439-i17" overflow="scroll"><mml:semantics><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>r</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> maximizing <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M30" name="1471-2105-9-439-i18" overflow="scroll"><mml:semantics><mml:mrow><mml:mo>|</mml:mo><mml:mtext>Cov</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>r</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>T</mml:mi></mml:mstyle></mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> (<italic>r </italic>= 1, ..., <italic>R</italic>) subject to the constraints <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M31" name="1471-2105-9-439-i19" overflow="scroll"><mml:semantics><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>r</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>T</mml:mi></mml:mstyle></mml:msubsup><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> and <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M32" name="1471-2105-9-439-i20" overflow="scroll"><mml:semantics><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>r</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>T</mml:mi></mml:mstyle></mml:msubsup><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> for <italic>r </italic>≠ <italic>s</italic>, where R ≪ <italic>p</italic>. Instead of working with the original predictors, one then plugs the projections living in a lower dimensional space into other classification methods, for example linear discriminant analysis (classifier = "pls_ldaCMA"), logistic regression (classifier = "pls_lrCMA") or random forest (classifier = "pls_rfCMA"). See Boulesteix and Strimmer [<xref ref-type="bibr" rid="B9">9</xref>] for an overview of partial least squares applications to genomic data analysis.</p>
        </sec>
        <sec>
          <title>Regularization and shrinkage methods</title>
          <p>In both penalized logistic regression and support vector machines, <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M33" name="1471-2105-9-439-i9" overflow="scroll"><mml:semantics><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:semantics></mml:math></inline-formula>(·) is constructed such that it minimizes an expression of the form</p>
          <p>
            <disp-formula id="bmcM4">
              <label>(4)</label>
              <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M34" name="1471-2105-9-439-i21" overflow="scroll">
                <mml:semantics>
                  <mml:mrow>
                    <mml:mstyle displaystyle="true">
                      <mml:munderover>
                        <mml:mo>∑</mml:mo>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mi>n</mml:mi>
                      </mml:munderover>
                      <mml:mrow>
                        <mml:mi>L</mml:mi>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:msub>
                          <mml:mi>y</mml:mi>
                          <mml:mi>i</mml:mi>
                        </mml:msub>
                        <mml:mo>,</mml:mo>
                        <mml:mi>f</mml:mi>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:msub>
                          <mml:mstyle mathvariant="bold" mathsize="normal">
                            <mml:mi>x</mml:mi>
                          </mml:mstyle>
                          <mml:mi>i</mml:mi>
                        </mml:msub>
                        <mml:mo stretchy="false">)</mml:mo>
                        <mml:mo stretchy="false">)</mml:mo>
                        <mml:mo>+</mml:mo>
                        <mml:mi>λ</mml:mi>
                        <mml:mi>J</mml:mi>
                        <mml:mo stretchy="false">[</mml:mo>
                        <mml:mi>f</mml:mi>
                        <mml:mo stretchy="false">]</mml:mo>
                      </mml:mrow>
                    </mml:mstyle>
                    <mml:mo>,</mml:mo>
                  </mml:mrow>
                </mml:semantics>
              </mml:math>
            </disp-formula>
          </p>
          <p>where <italic>L</italic>(·,·) is a loss function as outlined above and <italic>J </italic>[<italic>f</italic>] is a regularizer preventing overfitting. The trade-off between the two terms is known as bias-variance trade-off and governed via the tuning parameter <italic>λ</italic>. For ℓ<sup>2 </sup>penalized logistic regression (classifier = "plrCMA"), <italic>f</italic>(<bold>x</bold>) = <bold>x</bold><sup><bold>T</bold></sup><bold><italic>β </italic></bold>is linear and depends only on the vector <italic>β </italic>of regression coefficients, <italic>J </italic>[<italic>f</italic>] is the ℓ<sup>2 </sup>norm <italic>J </italic>[<italic>f</italic>] = <bold><italic>β</italic></bold><sup><bold>T</bold></sup><bold><italic>β </italic></bold>and <italic>L</italic>(·,·) is the negative log-likelihood of a binomial distribution. Setting <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M35" name="1471-2105-9-439-i22" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>J</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:mi>β</mml:mi><mml:mo>|</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:semantics></mml:math></inline-formula> yields the Lasso [<xref ref-type="bibr" rid="B28">28</xref>] (classifier = "LassoCMA"), while combining both regularizers yields the elastic net [<xref ref-type="bibr" rid="B29">29</xref>] (classifier = "ElasticNetCMA"). CMA also implements a multi-class version of ℓ<sup>2 </sup>penalized logistic regression, replacing the binomial negative likelihood by its multinomial counterpart.</p>
          <p>For Support Vector Machines (classifier = "svmCMA"), we have</p>
          <p>
            <disp-formula>
              <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M36" name="1471-2105-9-439-i23" overflow="scroll">
                <mml:semantics>
                  <mml:mrow>
                    <mml:mi>f</mml:mi>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mstyle mathvariant="bold" mathsize="normal">
                      <mml:mi>x</mml:mi>
                    </mml:mstyle>
                    <mml:mo stretchy="false">)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mstyle displaystyle="true">
                      <mml:munder>
                        <mml:mo>∑</mml:mo>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>∈</mml:mo>
                          <mml:mi mathvariant="script">V</mml:mi>
                        </mml:mrow>
                      </mml:munder>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mi>α</mml:mi>
                          <mml:mi>i</mml:mi>
                        </mml:msub>
                        <mml:mi>k</mml:mi>
                        <mml:mo stretchy="false">(</mml:mo>
                        <mml:mstyle mathvariant="bold" mathsize="normal">
                          <mml:mi>x</mml:mi>
                        </mml:mstyle>
                        <mml:mo>,</mml:mo>
                        <mml:msub>
                          <mml:mstyle mathvariant="bold" mathsize="normal">
                            <mml:mi>x</mml:mi>
                          </mml:mstyle>
                          <mml:mi>i</mml:mi>
                        </mml:msub>
                        <mml:mo stretchy="false">)</mml:mo>
                      </mml:mrow>
                    </mml:mstyle>
                    <mml:mo>,</mml:mo>
                  </mml:mrow>
                </mml:semantics>
              </mml:math>
            </disp-formula>
          </p>
          <p>where <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M37" name="1471-2105-9-439-i24" overflow="scroll"><mml:semantics><mml:mi mathvariant="script">V</mml:mi></mml:semantics></mml:math></inline-formula> ⊂ {1, ..., <italic>n</italic>} is the set of the so-called "support vectors", <italic>α</italic><sub><italic>i </italic></sub>are coefficients and <italic>k</italic>(·,·) is a positive definite kernel. Frequently used kernels are the linear kernel ⟨·,·⟩, the polynomial kernel ⟨·,·⟩<sup><italic>d </italic></sup>or the Gaussian kernel <italic>k</italic>(<bold>x</bold><sub><italic>i</italic></sub>, <bold>x</bold><sub><italic>j</italic></sub>) = exp((<bold>x</bold><sub><italic>i </italic></sub>- <bold>x</bold><sub><italic>j</italic></sub>)<sup><italic>T</italic></sup>(<bold>x</bold><sub><italic>i </italic></sub>- <bold>x</bold><sub><italic>j</italic></sub>)/<italic>σ</italic><sup>2</sup>). The function <italic>J </italic>[<italic>f </italic>] is given as <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M38" name="1471-2105-9-439-i25" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>J</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">V</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>α</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:semantics></mml:math></inline-formula>  and L(.,.) is the so-called hinge loss [<xref ref-type="bibr" rid="B30">30</xref>].</p>
        </sec>
        <sec>
          <title>Random Forests</title>
          <p>The random forest method [<xref ref-type="bibr" rid="B4">4</xref>] aggregates an ensemble of binary decision-tree classifiers [<xref ref-type="bibr" rid="B31">31</xref>] constructed based on bootstrap samples drawn from the learning set (classifier = "rfCMA"). The "<bold>b</bold>ootstrap <bold>agg</bold>regat<bold>ing</bold>" strategy (abbreviated as "bagging") turns out to be particularly successful in combination with unstable classifiers such as decision trees. In order to make the obtained trees even more different and thus increase their stability and to reduce the computation time, random forests have an additional feature. At each split, a subset of candidate predictors is selected out of the available predictors. The random forest method also performs implicit variable selection and can be used to assess variable importance (see section 3.1.3).</p>
        </sec>
        <sec>
          <title>Boosting</title>
          <p>Similarly to random forests, boosting is based on a weighted ensemble of "weak learners" for classification, i.e. <italic>f</italic>(·) = ∑<italic>α</italic><sub><italic>m</italic></sub><italic>f</italic><sub>weak</sub>(·), where <italic>α</italic><sub><italic>m </italic></sub>&gt; 0 (<italic>m </italic>= 1, ..., <italic>M</italic>) are adequately chosen coefficients. The term weak learner which stems from the machine learning community [<xref ref-type="bibr" rid="B32">32</xref>], denotes a method with poor performance (but still significantly better performance than random guessing) and low complexity. Famous examples for weak learners are binary decision trees with few (one or two) splits or linear functions in one predictor which is termed componentwise boosting. Friedman [<xref ref-type="bibr" rid="B33">33</xref>] reformulates boosting as a functional gradient descent combined with appropriate loss functions. The CMA package implements decision tree-based (classifier = "gbmCMA") and componentwise (classifier = "compBoostCMA") boosting with exponential, binomial and squared loss in the two-class case, and multinomial loss in the multi-class case.</p>
        </sec>
        <sec>
          <title>Feed-Forward Neural Networks</title>
          <p>CMA implements one-hidden-layer feed-forward neural networks (classifier = "nnetCMA"). Starting with a vector of covariates <bold>x</bold>, one forms projections <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M39" name="1471-2105-9-439-i26" overflow="scroll"><mml:semantics><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>a</mml:mi></mml:mstyle><mml:mi>r</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>T</mml:mi></mml:mstyle></mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle></mml:mrow></mml:semantics></mml:math></inline-formula>, <italic>r </italic>= 1, ..., <italic>R</italic>, that are then transformed using an activation function <italic>h</italic>(·), usually sigmoidal, in order to obtain a hidden layer consisting of units <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M40" name="1471-2105-9-439-i27" overflow="scroll"><mml:semantics><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>a</mml:mi></mml:mstyle><mml:mi>r</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>T</mml:mi></mml:mstyle></mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> that are subsequently used for prediction. Training of neural networks tends to be rather complicated and unstable. For large <italic>p</italic>, CMA works in the space of "eigengenes", following the suggestion of [<xref ref-type="bibr" rid="B34">34</xref>] by applying the singular value decomposition [<xref ref-type="bibr" rid="B35">35</xref>] to the predictor matrix.</p>
        </sec>
        <sec>
          <title>Probabilistic Neural Networks</title>
          <p>Although termed "Neural Networks", probabilistic neural networks (classifier = "pnnCMA") are actually a Parzen-Windows type classifier [<xref ref-type="bibr" rid="B36">36</xref>] related to the nearest neighbors approach. For <bold>x </bold>∈ <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M41" name="1471-2105-9-439-i8" overflow="scroll"><mml:semantics><mml:mi mathvariant="script">T</mml:mi></mml:semantics></mml:math></inline-formula> from the test set and each class <italic>k </italic>= 0, ..., <italic>K </italic>- 1, one computes</p>
          <p>
            <disp-formula>
              <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M42" name="1471-2105-9-439-i28" overflow="scroll">
                <mml:semantics>
                  <mml:mrow>
                    <mml:mtable>
                      <mml:mtr>
                        <mml:mtd>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mi>w</mml:mi>
                              <mml:mi>k</mml:mi>
                            </mml:msub>
                            <mml:mo>=</mml:mo>
                            <mml:msubsup>
                              <mml:mi>n</mml:mi>
                              <mml:mi>k</mml:mi>
                              <mml:mrow>
                                <mml:mo>−</mml:mo>
                                <mml:mn>1</mml:mn>
                              </mml:mrow>
                            </mml:msubsup>
                            <mml:mstyle displaystyle="true">
                              <mml:munder>
                                <mml:mo>∑</mml:mo>
                                <mml:mrow>
                                  <mml:msub>
                                    <mml:mstyle mathvariant="bold" mathsize="normal">
                                      <mml:mi>x</mml:mi>
                                    </mml:mstyle>
                                    <mml:mi>i</mml:mi>
                                  </mml:msub>
                                  <mml:mo>∈</mml:mo>
                                  <mml:mi>ℒ</mml:mi>
                                </mml:mrow>
                              </mml:munder>
                              <mml:mrow>
                                <mml:mi>I</mml:mi>
                                <mml:mo stretchy="false">(</mml:mo>
                                <mml:msub>
                                  <mml:mi>y</mml:mi>
                                  <mml:mi>i</mml:mi>
                                </mml:msub>
                                <mml:mo>=</mml:mo>
                                <mml:mi>k</mml:mi>
                                <mml:mo stretchy="false">)</mml:mo>
                                <mml:mo>⋅</mml:mo>
                                <mml:mi>exp</mml:mi>
                                <mml:mo>⁡</mml:mo>
                                <mml:mo stretchy="false">(</mml:mo>
                                <mml:msup>
                                  <mml:mrow>
                                    <mml:mo stretchy="false">(</mml:mo>
                                    <mml:msub>
                                      <mml:mstyle mathvariant="bold" mathsize="normal">
                                        <mml:mi>x</mml:mi>
                                      </mml:mstyle>
                                      <mml:mi>i</mml:mi>
                                    </mml:msub>
                                    <mml:mo>−</mml:mo>
                                    <mml:mstyle mathvariant="bold" mathsize="normal">
                                      <mml:mi>x</mml:mi>
                                    </mml:mstyle>
                                    <mml:mo stretchy="false">)</mml:mo>
                                  </mml:mrow>
                                  <mml:mstyle mathvariant="bold" mathsize="normal">
                                    <mml:mi>T</mml:mi>
                                  </mml:mstyle>
                                </mml:msup>
                                <mml:mo stretchy="false">(</mml:mo>
                                <mml:msub>
                                  <mml:mstyle mathvariant="bold" mathsize="normal">
                                    <mml:mi>x</mml:mi>
                                  </mml:mstyle>
                                  <mml:mi>i</mml:mi>
                                </mml:msub>
                                <mml:mo>−</mml:mo>
                                <mml:mstyle mathvariant="bold" mathsize="normal">
                                  <mml:mi>x</mml:mi>
                                </mml:mstyle>
                                <mml:mo stretchy="false">)</mml:mo>
                                <mml:mo>/</mml:mo>
                                <mml:msup>
                                  <mml:mi>σ</mml:mi>
                                  <mml:mn>2</mml:mn>
                                </mml:msup>
                                <mml:mo stretchy="false">)</mml:mo>
                                <mml:mo>,</mml:mo>
                              </mml:mrow>
                            </mml:mstyle>
                          </mml:mrow>
                        </mml:mtd>
                        <mml:mtd>
                          <mml:mrow>
                            <mml:mi>k</mml:mi>
                            <mml:mo>=</mml:mo>
                            <mml:mn>0</mml:mn>
                            <mml:mo>,</mml:mo>
                            <mml:mn>...</mml:mn>
                            <mml:mo>,</mml:mo>
                            <mml:mi>K</mml:mi>
                            <mml:mo>−</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:mtd>
                      </mml:mtr>
                    </mml:mtable>
                  </mml:mrow>
                </mml:semantics>
              </mml:math>
            </disp-formula>
          </p>
          <p>where <italic>n</italic><sub><italic>k </italic></sub>denotes the number of observations from class <italic>k </italic>in the learning set and <italic>σ</italic><sup>2 </sup>&gt; 0 is a parameter. The quotient <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M43" name="1471-2105-9-439-i29" overflow="scroll"><mml:semantics><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> is then considered as an estimate of the class probability, for <italic>k </italic>= 0, ..., <italic>K </italic>- 1.</p>
        </sec>
        <sec>
          <title>Nearest Neighbors and Probabilistic Nearest Neighbors</title>
          <p>CMA implements one of the variants of the ordinary nearest neighbors approach using the euclidean distance as distance measure (classifier = "knnCMA") and another variant called "probabilistic" that additionally provides estimates for class probabilities by using distances as weights, however without a genuine underlying probability model (classifier = "pknnCMA"). Given a learning set ℒ and a test set <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M44" name="1471-2105-9-439-i8" overflow="scroll"><mml:semantics><mml:mi mathvariant="script">T</mml:mi></mml:semantics></mml:math></inline-formula>, respectively, the probabilistic nearest neighbors method determines for each element in ℒ the <italic>k </italic>&gt; 1 nearest neighbors <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M45" name="1471-2105-9-439-i30" overflow="scroll"><mml:semantics><mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mo>⊂</mml:mo><mml:mi>ℒ</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula> and then estimates class probabilities as</p>
          <p>
            <disp-formula>
              <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M46" name="1471-2105-9-439-i31" overflow="scroll">
                <mml:semantics>
                  <mml:mrow>
                    <mml:mtable>
                      <mml:mtr>
                        <mml:mtd>
                          <mml:mrow>
                            <mml:mi>P</mml:mi>
                            <mml:mo stretchy="false">(</mml:mo>
                            <mml:mi>y</mml:mi>
                            <mml:mo>=</mml:mo>
                            <mml:mi>k</mml:mi>
                            <mml:mo>|</mml:mo>
                            <mml:mstyle mathvariant="bold" mathsize="normal">
                              <mml:mi>x</mml:mi>
                            </mml:mstyle>
                            <mml:mo stretchy="false">)</mml:mo>
                            <mml:mo>=</mml:mo>
                            <mml:mfrac>
                              <mml:mrow>
                                <mml:mi>exp</mml:mi>
                                <mml:mo>⁡</mml:mo>
                                <mml:mo stretchy="false">(</mml:mo>
                                <mml:mi>β</mml:mi>
                                <mml:mstyle displaystyle="true">
                                  <mml:msub>
                                    <mml:mo>∑</mml:mo>
                                    <mml:mrow>
                                      <mml:msub>
                                        <mml:mstyle mathvariant="bold" mathsize="normal">
                                          <mml:mi>x</mml:mi>
                                        </mml:mstyle>
                                        <mml:mi>i</mml:mi>
                                      </mml:msub>
                                      <mml:mo>∈</mml:mo>
                                      <mml:mi mathvariant="script">N</mml:mi>
                                    </mml:mrow>
                                  </mml:msub>
                                  <mml:mrow>
                                    <mml:mo>−</mml:mo>
                                    <mml:mi>d</mml:mi>
                                    <mml:mo stretchy="false">(</mml:mo>
                                    <mml:mstyle mathvariant="bold" mathsize="normal">
                                      <mml:mi>x</mml:mi>
                                    </mml:mstyle>
                                    <mml:mo>,</mml:mo>
                                    <mml:msub>
                                      <mml:mstyle mathvariant="bold" mathsize="normal">
                                        <mml:mi>x</mml:mi>
                                      </mml:mstyle>
                                      <mml:mi>i</mml:mi>
                                    </mml:msub>
                                    <mml:mo stretchy="false">)</mml:mo>
                                    <mml:mi>I</mml:mi>
                                    <mml:mo stretchy="false">(</mml:mo>
                                    <mml:msub>
                                      <mml:mi>y</mml:mi>
                                      <mml:mi>i</mml:mi>
                                    </mml:msub>
                                    <mml:mo>=</mml:mo>
                                    <mml:mi>k</mml:mi>
                                    <mml:mo stretchy="false">)</mml:mo>
                                    <mml:mo stretchy="false">)</mml:mo>
                                  </mml:mrow>
                                </mml:mstyle>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mi>exp</mml:mi>
                                <mml:mo>⁡</mml:mo>
                                <mml:mo stretchy="false">(</mml:mo>
                                <mml:mn>1</mml:mn>
                                <mml:mo>+</mml:mo>
                                <mml:mi>β</mml:mi>
                                <mml:mstyle displaystyle="true">
                                  <mml:msub>
                                    <mml:mo>∑</mml:mo>
                                    <mml:mrow>
                                      <mml:msub>
                                        <mml:mi>x</mml:mi>
                                        <mml:mi>i</mml:mi>
                                      </mml:msub>
                                      <mml:mo>∈</mml:mo>
                                      <mml:mi mathvariant="script">N</mml:mi>
                                    </mml:mrow>
                                  </mml:msub>
                                  <mml:mrow>
                                    <mml:mo>−</mml:mo>
                                    <mml:mi>d</mml:mi>
                                    <mml:mo stretchy="false">(</mml:mo>
                                    <mml:mstyle mathvariant="bold" mathsize="normal">
                                      <mml:mi>x</mml:mi>
                                    </mml:mstyle>
                                    <mml:mo>,</mml:mo>
                                    <mml:msub>
                                      <mml:mstyle mathvariant="bold" mathsize="normal">
                                        <mml:mi>x</mml:mi>
                                      </mml:mstyle>
                                      <mml:mi>i</mml:mi>
                                    </mml:msub>
                                    <mml:mo stretchy="false">)</mml:mo>
                                    <mml:mi>I</mml:mi>
                                    <mml:mo stretchy="false">(</mml:mo>
                                    <mml:msub>
                                      <mml:mi>y</mml:mi>
                                      <mml:mi>i</mml:mi>
                                    </mml:msub>
                                    <mml:mo>=</mml:mo>
                                    <mml:mi>k</mml:mi>
                                    <mml:mo stretchy="false">)</mml:mo>
                                    <mml:mo stretchy="false">)</mml:mo>
                                  </mml:mrow>
                                </mml:mstyle>
                              </mml:mrow>
                            </mml:mfrac>
                            <mml:mo>,</mml:mo>
                          </mml:mrow>
                        </mml:mtd>
                        <mml:mtd>
                          <mml:mrow>
                            <mml:mi>k</mml:mi>
                            <mml:mo>=</mml:mo>
                            <mml:mn>0</mml:mn>
                            <mml:mo>,</mml:mo>
                            <mml:mn>...</mml:mn>
                            <mml:mo>,</mml:mo>
                            <mml:mi>K</mml:mi>
                            <mml:mo>−</mml:mo>
                            <mml:mn>1</mml:mn>
                            <mml:mo>,</mml:mo>
                          </mml:mrow>
                        </mml:mtd>
                        <mml:mtd>
                          <mml:mrow>
                            <mml:mstyle mathvariant="bold" mathsize="normal">
                              <mml:mi>x</mml:mi>
                            </mml:mstyle>
                            <mml:mo>∈</mml:mo>
                            <mml:mi mathvariant="script">T</mml:mi>
                          </mml:mrow>
                        </mml:mtd>
                      </mml:mtr>
                    </mml:mtable>
                  </mml:mrow>
                </mml:semantics>
              </mml:math>
            </disp-formula>
          </p>
          <p>where <italic>β </italic>&gt; 0 is a method parameter and <italic>d</italic>(·,·) a distance measure.</p>
          <p>Note that users can easily incorporate their own classifiers into the CMA framework. To do this, they have to define a classifier with the same structure as those already implemented in CMA. For illustrative purposes, we consider a simple classifier assigning observations to two classes. The code defining this classifier is given below. Once the classifier is defined, it can be used in the method classification in place of the CMA classifiers enumerated above.</p>
          <p>myclassifier &lt;- function(X, y, learnind, hyperpar = 1) {</p>
          <p>   Xlearn &lt;- X [learnind, ,drop = F]; yearn &lt;- y [learnind]</p>
          <p>   Xtest &lt;- X [-learnind, ,drop = F]; ytest &lt;- y [-learnind]</p>
          <p>   w &lt;- hyperpar * t(ylearn %*% Xlearn)</p>
          <p>   pred &lt;- (sign(drop(Xtest %*% w))+1)/2</p>
          <p>   new("cloutput", learnind = learnind, y = ylearn,</p>
          <p>      yhat = pred, prob = NA,</p>
          <p>      method="myclassifier", mode = "binary")</p>
          <p>}</p>
        </sec>
      </sec>
      <sec>
        <title>3.1.3 Variable selection methods</title>
        <p>This section addresses the variable ranking- and selection procedures available in CMA. We distinguish three types of methods: pure filter methods (f) based on parametric or nonparametric statistical tests not directly related to the prediction task, methods which rank variables according to their discriminatory power (r), and classification methods selecting sparse sets of variables that can be used for other classification methods in a hybrid way (s). The multi-class case is fully supported by all the methods. Methods that are defined for binary responses only are applied within a "one-vs-all" or "pairwise" scheme. The former means that for each class <italic>k </italic>= 0, ..., <italic>K </italic>- 1, one recodes the class label <italic>y </italic>into <italic>K </italic>pseudo class labels <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M47" name="1471-2105-9-439-i32" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> = <italic>I</italic>(<italic>y</italic> = <italic>k</italic>) for <italic>k </italic>= 0,..., <italic>K </italic>- 1, while the latter considers all <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M48" name="1471-2105-9-439-i33" overflow="scroll"><mml:semantics><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>K</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>2</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula> possible pairs of classes successively. The variable selection procedure is run <italic>K </italic>times or <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M49" name="1471-2105-9-439-i33" overflow="scroll"><mml:semantics><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>K</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>2</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula> times, respectively, and the same number of genes are selected for each run. The final subset of selected genes consists of the union of the subsets obtained in the different runs.</p>
        <p>In the CMA package, variable selection can be performed (for each learning set separately) using the method geneselection, with the argument method specifying the procedure and the argument scheme indicating which scheme (one-vs-all or pairwise) should be used in the <italic>K </italic>&gt; 2 case. The implemented methods are:</p>
        <p>(f) ordinary two-sample t.test (method = "t.test")</p>
        <p>(f) Welch modification of the t.test (method = "welch.test")</p>
        <p>(f) Wilcoxon rank sum test (method = "wilcox.test")</p>
        <p>(f) F test (method = "f.test")</p>
        <p>(f) Kruskal-Wallis test (method = "kruskal.test")</p>
        <p>(f) "moderated" t and F test, respectively, using the package 'limma' [<xref ref-type="bibr" rid="B37">37</xref>] (method = "limma")</p>
        <p>(r) one-step Recursive Feature Elimination (RFE) in combination with the linear SVM [<xref ref-type="bibr" rid="B38">38</xref>] (method = "rfe")</p>
        <p>(r) random forest variable importance measure [<xref ref-type="bibr" rid="B4">4</xref>] (method = "rf")</p>
        <p>(s) Lasso [<xref ref-type="bibr" rid="B28">28</xref>] (method = "lasso")</p>
        <p>(s) elastic net [<xref ref-type="bibr" rid="B29">29</xref>] (method = "elasticnet")</p>
        <p>(s) componentwise boosting (method = "boosting") [<xref ref-type="bibr" rid="B39">39</xref>]</p>
        <p>(f) ad-hoc "Golub" criterion [<xref ref-type="bibr" rid="B40">40</xref>]</p>
        <p>Each method can be interpreted as a function <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M50" name="1471-2105-9-439-i34" overflow="scroll"><mml:semantics><mml:mi mathvariant="script">I</mml:mi></mml:semantics></mml:math></inline-formula>(·) on the set of predictor indices: <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M51" name="1471-2105-9-439-i34" overflow="scroll"><mml:semantics><mml:mi mathvariant="script">I</mml:mi></mml:semantics></mml:math></inline-formula>: {1,..., <italic>p</italic>} → ℝ<sup>+ </sup>where <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M52" name="1471-2105-9-439-i34" overflow="scroll"><mml:semantics><mml:mi mathvariant="script">I</mml:mi></mml:semantics></mml:math></inline-formula>(·) increases with discriminating power. <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M53" name="1471-2105-9-439-i34" overflow="scroll"><mml:semantics><mml:mi mathvariant="script">I</mml:mi></mml:semantics></mml:math></inline-formula>(·) is the absolute value of the test statistic for the (f) methods and the absolute value of the corresponding regression coefficient for the (s)-methods, while the (r)-methods are already variable importance measures per definition. Predictor <italic>j </italic>is said to be more important than predictor <italic>l </italic>if <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M54" name="1471-2105-9-439-i34" overflow="scroll"><mml:semantics><mml:mi mathvariant="script">I</mml:mi></mml:semantics></mml:math></inline-formula>(<italic>l</italic>) &lt;<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M55" name="1471-2105-9-439-i34" overflow="scroll"><mml:semantics><mml:mi mathvariant="script">I</mml:mi></mml:semantics></mml:math></inline-formula>(<italic>j</italic>). It should be noted that the variable ordering is not necessarily determined uniquely, especially for the (s)-methods where variable importances are non-zero for few predictors only and for the (f) methods based on ranks. After variable ranking, variable selection is then completed by choosing a suitable number of variables (as defined by the user) that should be used by the classifier. For the multi-class case with one-vs-all or pairwise schemes, one obtains <italic>K </italic>and <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M56" name="1471-2105-9-439-i33" overflow="scroll"><mml:semantics><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>K</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>2</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula> separate rankings, respectively, and the union of them forms the set of predictor variables. We again emphasize that the variable importance assignment is based on learning data only, which means that the procedure is repeated for each learning/test set splitting successively.</p>
        <p>Note that the method GeneSelection may be used to order any kind of variables, not only genes. For example, if one uses CMA to classify genes based on samples instead of classifying samples based on genes, the function GeneSelection is used to select samples instead of selecting genes in spite of its name.</p>
      </sec>
      <sec>
        <title>3.1.4 Hyperparameter tuning</title>
        <p>The function tune of the CMA package implements inner cross-validation for hyperparameter tuning, as represented schematically in Figure <xref ref-type="fig" rid="F2">2</xref>. The following procedure is repeated for each learning set <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M57" name="1471-2105-9-439-i11" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> defined by the argument learningsets. The learning set is partitioned into <italic>l </italic>subsets of approximately equal size. For different values of the hyperparameters, the error rate is estimated based on <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M58" name="1471-2105-9-439-i11" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> within a <italic>l</italic>-fold cross-validation scheme. The hyperparameter values yielding the smallest cross-validated error rate are then selected and used for the construction of the classifier based on <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M59" name="1471-2105-9-439-i11" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>. Hence, the test data set <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M60" name="1471-2105-9-439-i15" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> is not used for hyperparameter tuning. This procedure is often denoted as inner or internal cross-validation, yielding a so-called nested cross-validation procedure if cross-validation is also used to evaluate the error rate of the classification method of interest.</p>
        <p>Examples of tuning parameters for the classifiers included in CMA are given in Table <xref ref-type="table" rid="T2">2</xref>. Note that one could also consider the number of genes as an hyperparameter to be tuned in nested cross-validation, although this is still not standard practice. We plan to do this extension in future work.</p>
        <table-wrap position="float" id="T2">
          <label>Table 2</label>
          <caption>
            <p>Overview of hyperparameter tuning in CMA.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <td align="left">
                  <bold>Method</bold>
                </td>
                <td align="left">
                  <bold>Name in CMA</bold>
                </td>
                <td align="left">
                  <bold>Range</bold>
                </td>
                <td align="left">
                  <bold>Signification</bold>
                </td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left">gbmCMA</td>
                <td align="left">n.trees</td>
                <td align="left">1, 2,...</td>
                <td align="left">number of base learners (decision trees)</td>
              </tr>
              <tr>
                <td align="left">LassoCMA</td>
                <td align="left">norm.fraction</td>
                <td align="left">[0;1]</td>
                <td align="left">relative bound imposed on the ℓ<sup>1 </sup>norm on the weight vector</td>
              </tr>
              <tr>
                <td align="left">knnCMA</td>
                <td align="left">k</td>
                <td align="left">1, 2,...,|ℒ|</td>
                <td align="left">number of nearest neighbours</td>
              </tr>
              <tr>
                <td align="left">nnetCMA</td>
                <td align="left">size</td>
                <td align="left">1, 2, ...</td>
                <td align="left">number of units in the hidden layer</td>
              </tr>
              <tr>
                <td align="left">scdaCMA</td>
                <td align="left">delta</td>
                <td align="left">ℝ<sup>+</sup></td>
                <td align="left">shrinkage towards zero applied to the centroids</td>
              </tr>
              <tr>
                <td align="left">svmCMA</td>
                <td align="left">cost</td>
                <td align="left">ℝ<sup>+</sup></td>
                <td align="left">cost: controls the violations of the margin of the hyperplane</td>
              </tr>
              <tr>
                <td/>
                <td align="left">gamma</td>
                <td align="left">ℝ<sup>+</sup></td>
                <td align="left">controls the width of the Gaussian kernel (if used)</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <p>The first column gives the method name, whereas the name of the hyperparameter in the CMA package is given in the second column. The third column gives the range of the parameter and the fourth column its signification.</p>
          </table-wrap-foot>
        </table-wrap>
        <p>It is crucial to perform hyperparameter tuning properly, for instance within an inner cross-validation as implemented in CMA. On the one hand, by using a complicated classifier involving many parameters without tuning them, one implicitly favors simpler classifiers which do not involve any hyperparameters. On the other hand, it would be completely incorrect to tune the parameters a posteriori, i.e. to try several values of the tuning parameters successively and to show only the best results [<xref ref-type="bibr" rid="B11">11</xref>]. Such a design would artificially favor complicated classifiers with many tuning parameters, and one would expect the obtained optimal classifier to generalize poorly on an independent validation data set. This problem is connected to Occam's Razor.</p>
        <p>For example, let us consider diagonal discriminant analysis (DLDA), which is also known as Naive Bayes classifier due to its simplicity, and on the other hand a SVM with a Gaussian kernel, which depends on at least two hyperparameters: the first one is the width of the Gaussian kernel and the second one is the cost parameter that governs the amount of regularization. While there are rules of thumb for choosing the width of the Gaussian kernel, this does not apply to the cost. Setting the cost to an arbitrary value can cause both over- and underfitting. In contrast, DLDA does not overfit due to its simplicity, but may tend to underfit.</p>
        <p>Note that the hyperparameter tuning procedure may yield sub-optimal values for the hyperparameters if used in combination with the gene selection procedure. That is because, in the present version of the CMA package, the set of genes remains fixed for all iterations of the inner cross-validation. This may affect the performance of the tuning procedure in the following way. Since in the inner cross-validation procedure variable selection is based on both the training sets and the test sets, the selected gene subset fits the test sets artificially well, which may affect the selection of the optimal hyperparameter values. For example, hyperparameter values corresponding to too complex models (for instance a too low penalty in penalized logistic regression) might yield small cross-validation error rates and get selected by the inner cross-validation procedure. Thus, using the tuning procedure in combination with gene selection tends to yield sub-optimal hyperparameter values. This may result in overestimated error rates in outer cross-validation, but not in "false positive research findings" [<xref ref-type="bibr" rid="B41">41</xref>], in the sense that in this case CMA will rather underestimate the association between predictors and response than find an association when there is none.</p>
        <p>That is why we recommend to use the tuning procedure of CMA only with methods that do not require any preliminary variable selection. Note that most of the classification methods needing tuning do not require any preliminary variable selection (like, e.g., support vector machines, shrunken centroids discriminant analysis, penalized logistic regression). Hence, this recommendation is not very restrictive in practice. However, we plan to modify the tuning procedure of CMA in a future version in order to allow the combination of tuning and gene selection. This can be done by re-performing gene selection in each inner cross-validation iteration successively, as already correctly implemented in the existing package 'MCRestimate' [<xref ref-type="bibr" rid="B14">14</xref>,<xref ref-type="bibr" rid="B15">15</xref>].</p>
      </sec>
      <sec>
        <title>3.1.5 Performance measures</title>
        <p>Once the classification step has been performed for all <italic>B </italic>iterations using the method classification, the method evaluation offers a variety of possibilities for evaluating the results. As accuracy measures, the user may choose among the following criteria.</p>
        <sec>
          <title>• Misclassification rate</title>
          <p>This is the simplest and most commonly used performance measure, corresponding to the indicator loss function in Eq. (1). From <italic>B </italic>iterations, one obtains a total of <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M61" name="1471-2105-9-439-i35" overflow="scroll"><mml:semantics><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi>b</mml:mi></mml:msub><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="script">T</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:semantics></mml:math></inline-formula> predictions. It implies that, with most procedures, the class label of each predictor-class pair in the sample <italic>S </italic>is predicted several times. The method evaluation can be applied in two directions: one can compute the misclassification rate either iterationwise, i.e. for each iteration separately (scheme="iterationwise"), yielding <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M62" name="1471-2105-9-439-i36" overflow="scroll"><mml:semantics><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>ϵ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mtext>iter</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>ϵ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula> or observationwise, i.e. for each observation separately (scheme = "observationwise"), yielding <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M63" name="1471-2105-9-439-i37" overflow="scroll"><mml:semantics><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>ϵ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mtext>obs</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>ϵ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:semantics></mml:math></inline-formula>.  The latter can be aggregated by classes which is useful in the frequent case where some classes can be discriminated better than the other. Furthermore, observationwise evaluation can help identifying outliers which are often characterized by high misclassification error rates. Although <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M64" name="1471-2105-9-439-i38" overflow="scroll"><mml:semantics><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>ϵ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mtext>iter</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> or <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M65" name="1471-2105-9-439-i39" overflow="scroll"><mml:semantics><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>ϵ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mtext>obs</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:semantics></mml:math></inline-formula> can be further averaged, the whole vectors are preferred to their less informative average, in order to reflect uncertainty more appropriately. A second advantage is that graphical summaries in the form of boxplots can be obtained.</p>
        </sec>
        <sec>
          <title>• Cost-based evaluation</title>
          <p>Cost-based evaluation is a generalization of the misclassification error rate. The loss function is defined on the discrete set {0, ..., <italic>K </italic>- 1} × {0, ..., <italic>K </italic>- 1}, associating a specific cost to each possible combination of predicted and true classes. It can be represented as a matrix <bold><italic>L </italic></bold>= (<italic>l</italic><sub><italic>rs</italic></sub>), <italic>r</italic>, <italic>s </italic>= 0,...,(<italic>K </italic>- 1) where <italic>l</italic><sub><italic>rs </italic></sub>is the cost or loss caused by assigning an observation of class <italic>r </italic>to class <italic>s</italic>. A usual convention is <italic>l</italic><sub><italic>rr </italic></sub>= 0 and <italic>l</italic><sub><italic>rs </italic></sub>&gt; 0 for <italic>r </italic>≠ <italic>s</italic>. As for the misclassification rate, both iteration- and observationwise evaluation are possible.</p>
        </sec>
        <sec>
          <title>• Sensitivity, specificity and area under the curve (AUC)</title>
          <p>These three performance measures are standard measures in medical diagnosis, see [<xref ref-type="bibr" rid="B18">18</xref>] for an overview. They are computed for binary classification only.</p>
        </sec>
        <sec>
          <title>• Brier Score and average probability of correct classification</title>
          <p>In classification settings, the Brier Score is defined as</p>
          <p>
            <disp-formula>
              <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M66" name="1471-2105-9-439-i40" overflow="scroll">
                <mml:semantics>
                  <mml:mrow>
                    <mml:msup>
                      <mml:mi>n</mml:mi>
                      <mml:mrow>
                        <mml:mo>−</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mstyle displaystyle="true">
                      <mml:munderover>
                        <mml:mo>∑</mml:mo>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mi>n</mml:mi>
                      </mml:munderover>
                      <mml:mrow>
                        <mml:mstyle displaystyle="true">
                          <mml:munderover>
                            <mml:mo>∑</mml:mo>
                            <mml:mrow>
                              <mml:mi>k</mml:mi>
                              <mml:mo>=</mml:mo>
                              <mml:mn>0</mml:mn>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>K</mml:mi>
                              <mml:mo>−</mml:mo>
                              <mml:mn>1</mml:mn>
                            </mml:mrow>
                          </mml:munderover>
                          <mml:mrow>
                            <mml:msup>
                              <mml:mrow>
                                <mml:mo stretchy="false">(</mml:mo>
                                <mml:mi>I</mml:mi>
                                <mml:mo stretchy="false">(</mml:mo>
                                <mml:msub>
                                  <mml:mi>y</mml:mi>
                                  <mml:mi>i</mml:mi>
                                </mml:msub>
                                <mml:mo>=</mml:mo>
                                <mml:mi>k</mml:mi>
                                <mml:mo stretchy="false">)</mml:mo>
                                <mml:mo>−</mml:mo>
                                <mml:mover accent="true">
                                  <mml:mi>P</mml:mi>
                                  <mml:mo>^</mml:mo>
                                </mml:mover>
                                <mml:mo stretchy="false">(</mml:mo>
                                <mml:msub>
                                  <mml:mi>y</mml:mi>
                                  <mml:mi>i</mml:mi>
                                </mml:msub>
                                <mml:mo>=</mml:mo>
                                <mml:mi>k</mml:mi>
                                <mml:mo>|</mml:mo>
                                <mml:msub>
                                  <mml:mi>x</mml:mi>
                                  <mml:mi>i</mml:mi>
                                </mml:msub>
                                <mml:mo stretchy="false">)</mml:mo>
                                <mml:mo stretchy="false">)</mml:mo>
                              </mml:mrow>
                              <mml:mn>2</mml:mn>
                            </mml:msup>
                          </mml:mrow>
                        </mml:mstyle>
                      </mml:mrow>
                    </mml:mstyle>
                    <mml:mo>,</mml:mo>
                  </mml:mrow>
                </mml:semantics>
              </mml:math>
            </disp-formula>
          </p>
          <p>where <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M67" name="1471-2105-9-439-i41" overflow="scroll"><mml:semantics><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:semantics></mml:math></inline-formula>(<italic>y </italic>= <italic>k</italic>|<bold><italic>x</italic></bold>) stands for the estimated probability for class <italic>k</italic>, conditional on <bold><italic>x</italic></bold>. Zero is the optimal value of the Brier Score.</p>
          <p>A similar measure is the average probability of correct classification which is defined as</p>
          <p>
            <disp-formula>
              <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M68" name="1471-2105-9-439-i42" overflow="scroll">
                <mml:semantics>
                  <mml:mrow>
                    <mml:msup>
                      <mml:mi>n</mml:mi>
                      <mml:mrow>
                        <mml:mo>−</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mstyle displaystyle="true">
                      <mml:munderover>
                        <mml:mo>∑</mml:mo>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mi>n</mml:mi>
                      </mml:munderover>
                      <mml:mrow>
                        <mml:mstyle displaystyle="true">
                          <mml:munderover>
                            <mml:mo>∑</mml:mo>
                            <mml:mrow>
                              <mml:mi>k</mml:mi>
                              <mml:mo>=</mml:mo>
                              <mml:mn>0</mml:mn>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>K</mml:mi>
                              <mml:mo>−</mml:mo>
                              <mml:mn>1</mml:mn>
                            </mml:mrow>
                          </mml:munderover>
                          <mml:mrow>
                            <mml:mo stretchy="false">(</mml:mo>
                            <mml:mi>I</mml:mi>
                            <mml:mo stretchy="false">(</mml:mo>
                            <mml:msub>
                              <mml:mi>y</mml:mi>
                              <mml:mi>i</mml:mi>
                            </mml:msub>
                            <mml:mo>=</mml:mo>
                            <mml:mi>k</mml:mi>
                            <mml:mo stretchy="false">)</mml:mo>
                            <mml:mover accent="true">
                              <mml:mi>P</mml:mi>
                              <mml:mo>^</mml:mo>
                            </mml:mover>
                            <mml:mo stretchy="false">(</mml:mo>
                            <mml:msub>
                              <mml:mi>y</mml:mi>
                              <mml:mi>i</mml:mi>
                            </mml:msub>
                            <mml:mo>=</mml:mo>
                            <mml:mi>k</mml:mi>
                            <mml:mo>|</mml:mo>
                            <mml:mi>x</mml:mi>
                            <mml:mo stretchy="false">)</mml:mo>
                          </mml:mrow>
                        </mml:mstyle>
                      </mml:mrow>
                    </mml:mstyle>
                    <mml:mo>,</mml:mo>
                  </mml:mrow>
                </mml:semantics>
              </mml:math>
            </disp-formula>
          </p>
          <p>and equals 1 in the optimal case. Both measures have the advantage that they are based on the continuous scale of probabilities, thus yielding more precise results. As a drawback, however, they cannot be applied to all classifiers but only to those associated with a probabilistic background (e.g. penalized regression). For other methods, they can either not be computed at all (e.g. nearest neighbors) or their application is questionable (e.g. support vector machines).</p>
        </sec>
        <sec>
          <title>• 0.632 and 0.632+ estimators</title>
          <p>The ordinary misclassification error rate estimates resulting from working with learning sets of size &lt;<italic>n </italic>tend to overestimate the true prediction error. A simple correction proposed for learning sets generated from bootstrapping (argument method="bootstrap" in the function GenerateLearningsets) uses a convex combination of the re-substitution error -which has a bias in the other direction (weight: 0.368) and the bootstrap error estimation (weight: 0.632). A further refinement of this idea is the 0.632+ estimator [<xref ref-type="bibr" rid="B42">42</xref>] which is approximately unbiased and seems to be particularly appropriate in the case of overfitting classifiers.</p>
          <p>The method compare can be used as a shortcut if several measures have to be computed for several classifiers. The function obsinfo can be used for outlier detection: given a vector of observationwise performance measures, it filters out observations for which the classifier fits poorly on average (i.e. high misclassification rate or low Brier Score, for example).</p>
        </sec>
      </sec>
    </sec>
    <sec>
      <title>3.2 A real-life data example</title>
      <sec>
        <title>3.2.1 Application to the SRBCT data</title>
        <p>This section gives a demonstration of the CMA package through an application to real life microarray data. It illustrates the typical workflow comprising learning set generation, variable selection, hyperparameter tuning, classifier training, and evaluation. The small blue round cell tumor data set was first analyzed by Khan et al [<xref ref-type="bibr" rid="B43">43</xref>] and is available from the R package 'pamr' [<xref ref-type="bibr" rid="B44">44</xref>]. It comprises <italic>n </italic>= 65 samples from four tumor classes and expression levels from <italic>p </italic>= 2308 genes. In general, good classification results can be obtained with this data set, even with relatively simple methods [<xref ref-type="bibr" rid="B45">45</xref>]. The main difficulty arises from the two classes with small size (8 and 12 observations, respectively).</p>
        <p>CMA implements a large number of classifiers and variable selection methods. In this demonstrating example, we compare the performance of seven of them, which are representative of the CMA functionalities: i) diagonal linear discriminant without variable selection and tuning, ii) linear discriminant analysis with variable selection, iii) quadratic discriminant analysis with variable selection, iv) Partial Least Squares followed by linear discriminant analysis with tuning of the number of components, v) shrunken centroids discriminant analysis with tuning of the shrinkage parameter, vi) support vector machines with radial kernel without tuning (i.e. with the default parameter values of the package 'e1071'), and vii) support vector machines with radial kernel and with tuning of the cost parameter and the width of the Gaussian kernel.</p>
        <p>We choose to work with stratified five-fold cross-validation, repeated ten times in order to achieve more stable results [<xref ref-type="bibr" rid="B16">16</xref>]. For linear discriminant analysis we decide to work with ten and for quadratic discriminant analysis with only two variables. These numbers are chosen arbitrarily without any deeper motivation, which we consider legitimate for the purpose of illustration. In practice, this choice should be given more attention. We start by preparing the data and generating learning sets:</p>
        <p>&gt; <italic>data(khan)</italic></p>
        <p>&gt; <italic>khanY &lt;- khan [, 1]</italic></p>
        <p>&gt; <italic>khanX &lt;- as.matrix(khan [, -1])</italic></p>
        <p>&gt; <italic>set.seed(27611)</italic></p>
        <p>&gt; <italic>fiveCV10iter &lt;- GenerateLearningsets(y = khanY, method = "CV", fold = 5, niter = 10, strat = TRUE)</italic></p>
        <p>khanY is an <italic>n</italic>-vector of class labels coded as 1,2,3,4, for the number of transcripts. fiveCV10iter is an object of class learningsets that stores for each of the niter = 10 iterations which observations belong to the learning sets as generated by the chosen method specified through the arguments method, fold and strat. For reproducibility purposes, it is crucial to set the random seed.</p>
        <p>As a preliminary step to classification, we then perform variable selection for those methods requiring it (linear and quadratic discriminant analysis). For illustrative purposes, we first try several variable selection methods and display a part of their results for comparison.</p>
        <p>&gt; <italic>genesel_f &lt;- GeneSelection(X = khanX, y = khanY, learningsets = fiveCV10iter, method = "f.test")</italic></p>
        <p>&gt; genesel_kru &lt;- GeneSelection(X = khanX, y = khanY, learningsets = fiveCV10iter, method = "kruskal.test")</p>
        <p>&gt; genesel_lim &lt;- GeneSelection(X = khanX, y = khanY, learningsets = fiveCV10iter, method = "limma")</p>
        <p>&gt; genesel_rf &lt;- GeneSelection(X = khanX, y = khanY, learningsets = fiveCV10iter, method = "rf", seed = 100)</p>
        <p>For comparing these four variable selection methods, one can now use the toplist method on the objects genesel_f, genesel_kru, genesel_wil, genesel_rf to show, e.g. their top-10 genes. Genes are referred to by their column index in X. The commands given below display the 10 top-ranking genes found based on the first learning set using each of the four methods.</p>
        <p>&gt; tab &lt;- cbind(f.test = toplist(genesel_f, s = F) [, 1], kru.test = toplist(genesel_kru, s = F) [, 1], lim.test = toplist(genesel_lim, s = F) [, 1], rf.imp = toplist(genesel_rf, s = F) [, 1])</p>
        <p>&gt; rownames(tab) &lt;- paste("top", 1:10, sep = ".")</p>
        <p>&gt; print(tab)</p>
        <table-wrap position="float" id="T3">
          <label>Table 3</label>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <td/>
                <td align="right">f.test</td>
                <td align="right">kru.test</td>
                <td align="right">lim.test</td>
                <td align="right">rf.imp</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left">top.1</td>
                <td align="right">1954</td>
                <td align="right">1194</td>
                <td align="right">22</td>
                <td align="right">545</td>
              </tr>
              <tr>
                <td align="left">top.2</td>
                <td align="right">1389</td>
                <td align="right">545</td>
                <td align="right">26</td>
                <td align="right">1954</td>
              </tr>
              <tr>
                <td align="left">top.3</td>
                <td align="right">1003</td>
                <td align="right">1389</td>
                <td align="right">723</td>
                <td align="right">2050</td>
              </tr>
              <tr>
                <td align="left">top.4</td>
                <td align="right">129</td>
                <td align="right">2050</td>
                <td align="right">1897</td>
                <td align="right">1003</td>
              </tr>
              <tr>
                <td align="left">top.5</td>
                <td align="right">1955</td>
                <td align="right">1954</td>
                <td align="right">148</td>
                <td align="right">246</td>
              </tr>
              <tr>
                <td align="left">top.6</td>
                <td align="right">246</td>
                <td align="right">246</td>
                <td align="right">428</td>
                <td align="right">1389</td>
              </tr>
              <tr>
                <td align="left">top.7</td>
                <td align="right">1194</td>
                <td align="right">1003</td>
                <td align="right">1065</td>
                <td align="right">187</td>
              </tr>
              <tr>
                <td align="left">top.8</td>
                <td align="right">2050</td>
                <td align="right">554</td>
                <td align="right">11</td>
                <td align="right">554</td>
              </tr>
              <tr>
                <td align="left">top.9</td>
                <td align="right">2046</td>
                <td align="right">1708</td>
                <td align="right">735</td>
                <td align="right">2046</td>
              </tr>
              <tr>
                <td align="left">top.10</td>
                <td align="right">545</td>
                <td align="right">1158</td>
                <td align="right">62</td>
                <td align="right">1896</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>The object tab indicates how many methods selected each gene in the top-10 list. We observe a moderate overlap of the four lists, with some genes appearing in three out of four lists:</p>
        <p>&gt; table(drop(tab))</p>
        <table-wrap position="float" id="T4">
          <label>Table 4</label>
          <table frame="hsides" rules="groups">
            <tbody>
              <tr>
                <td align="right">11</td>
                <td align="right">22</td>
                <td align="right">26</td>
                <td align="right">62</td>
                <td align="right">129</td>
                <td align="right">148</td>
                <td align="right">187</td>
                <td align="right">246</td>
                <td align="right">428</td>
                <td align="right">545</td>
                <td align="right">554</td>
                <td align="right">723</td>
                <td align="right">735</td>
                <td align="right">1003</td>
                <td align="right">1065</td>
                <td align="right">1158</td>
              </tr>
              <tr>
                <td align="right">1</td>
                <td align="right">1</td>
                <td align="right">1</td>
                <td align="right">1</td>
                <td align="right">1</td>
                <td align="right">1</td>
                <td align="right">1</td>
                <td align="right">3</td>
                <td align="right">1</td>
                <td align="right">3</td>
                <td align="right">2</td>
                <td align="right">1</td>
                <td align="right">1</td>
                <td align="right">3</td>
                <td align="right">1</td>
                <td align="right">1</td>
              </tr>
              <tr>
                <td align="right">1194</td>
                <td align="right">1389</td>
                <td align="right">1708</td>
                <td align="right">1896</td>
                <td align="right">1897</td>
                <td align="right">1954</td>
                <td align="right">1955</td>
                <td align="right">2046</td>
                <td align="right">2050</td>
                <td/>
                <td/>
                <td/>
                <td/>
                <td/>
                <td/>
                <td/>
              </tr>
              <tr>
                <td align="right">2</td>
                <td align="right">3</td>
                <td align="right">1</td>
                <td align="right">1</td>
                <td align="right">1</td>
                <td align="right">3</td>
                <td align="right">1</td>
                <td align="right">2</td>
                <td align="right">3</td>
                <td/>
                <td/>
                <td/>
                <td/>
                <td/>
                <td/>
                <td/>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>We now turn to hyperparameter tuning, which is performed via nested cross-validation. For Partial Least Squares, we optimize the number of latent components <italic>R </italic>over the grid {1,..., 5}. For the nearest shrunken centroids approach, the shrinkage intensity Δ is optimized over the grid {0.1, 0.25, 0.5, 1, 2, 5} (default). For the SVM with Gaussian kernel, one has to tune two hyperparameters: the cost for violating the margin in the primal formulation of the SVM and the width of the Gaussian kernel (see section tuning) which are optimized over {0.1, 1, 5, 10, 50, 100, 500} × {1/(4<italic>p</italic>), 1/(2<italic>p</italic>), 1/<italic>p</italic>, 2/<italic>p</italic>, 4/<italic>p</italic>} (also default).</p>
        <p>&gt; tune_pls &lt;- tune(X = khanX, y = khanY, learningsets = fiveCV10iter, classifier = pls_ldaCMA, grids = list(comp = 1:5))</p>
        <p>&gt; tune_scda &lt;- tune(X = khanX, y = khanY, learningsets = fiveCV10iter, classifier = scdaCMA, grids = list( ))</p>
        <p>&gt; tune_svm &lt;- tune(X = khanX, y = khanY, learningsets = fiveCV10iter, classifier = svmCMA, grids = list( ), kernel = "radial")</p>
        <p>In the second and third function calls to tune, the argument grids( ) is an empty list, which means that the default settings are used. The objects created in the steps described above are now passed to the function classification. The object genesel_f is passed to the classifiers ldaCMA and qdaCMA, since the F-test is the standard approach for variable selection in the multi-class setting. The argument nbgene indicates that only the "best" nbgene genes are used, where "best" is understood in terms of the F ratio.</p>
        <p>&gt; class_dlda &lt;- classification(X = khanX, y = khanY, learningsets = fiveCV10iter, classifier = dldaCMA)</p>
        <p>&gt; class_lda &lt;- classification(X = khanX, y = khanY, learningsets = fiveCV10iter, classifier = ldaCMA, genesel = genesel_f, nbgene = 10)</p>
        <p>&gt; class_qda &lt;- classification(X = khanX, y = khanY, learningsets = fiveCV10iter, classifier = qdaCMA, genesel = genesel_f, nbgene = 2)</p>
        <p>&gt; class_plsda &lt;- classification(X = khanX, y = khanY, learningsets = fiveCV10iter, classifier = pls_ldaCMA, tuneres = tune_pls)</p>
        <p>&gt; class_scda &lt;- classification(X = khanX, y = khanY, learningsets = fiveCV10iter, classifier = scdaCMA, tuneres = tune_scda)</p>
        <p>&gt; class_svm &lt;- classification(X = khanX, y = khanY, learningsets = fiveCV10iter, classifier = svmCMA, kernel = "radial")</p>
        <p>&gt; class_svm_t &lt;- classification(X = khanX, y = khanY, learningsets = fiveCV10iter, classifier = svmCMA, tuneres = tune_svm, kernel = "radial")</p>
        <p>The classification results can now be visualized using the function comparison, which takes a list of classifier outputs as input. For instance, the results may be tabulated and visualized in the form of boxplots, as displayed in Figure <xref ref-type="fig" rid="F3">3</xref>:</p>
        <fig position="float" id="F3">
          <label>Figure 3</label>
          <caption>
            <p><bold>Classification accuracy with Khan's SRBCT data</bold>. Boxplots representing the misclassification rate (top), the Brier score (middle), and the average probability of correct classification (bottom) for Khan's SRBCT data, using seven classifiers: diagonal linear discriminant analysis, linear discriminant analysis, quadratic discriminant analysis, shrunken centroids discriminant analysis (PAM), PLS followed by linear discriminant analysis, SVM without tuning, and SVM with tuning.</p>
          </caption>
          <graphic xlink:href="1471-2105-9-439-3"/>
        </fig>
        <p>&gt; classifierlist &lt;- list(class_dlda, class_lda, class_qda, class_scda, class_plsda, class_svm, class_svm_t)</p>
        <p>&gt; par(mfrow = c(3, 1))</p>
        <p>&gt; comparison &lt;- compare(classifierlist, plot = TRUE, measure = c("misclassification", "brier score", "average probability"))</p>
        <p>&gt; print(comparison)</p>
        <table-wrap position="float" id="T5">
          <label>Table 5</label>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <td/>
                <td align="right">misclassification</td>
                <td align="right">brier.score</td>
                <td align="right">average.probability</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left">DLDA</td>
                <td align="right">0.06807692</td>
                <td align="right">0.13420913</td>
                <td align="right">0.9310332</td>
              </tr>
              <tr>
                <td align="left">LDA</td>
                <td align="right">0.04269231</td>
                <td align="right">0.07254283</td>
                <td align="right">0.9556106</td>
              </tr>
              <tr>
                <td align="left">QDA</td>
                <td align="right">0.24000000</td>
                <td align="right">0.34247861</td>
                <td align="right">0.7362778</td>
              </tr>
              <tr>
                <td align="left">scDA</td>
                <td align="right">0.01910256</td>
                <td align="right">0.03264544</td>
                <td align="right">0.9754012</td>
              </tr>
              <tr>
                <td align="left">pls_lda</td>
                <td align="right">0.01743590</td>
                <td align="right">0.02608426</td>
                <td align="right">0.9819003</td>
              </tr>
              <tr>
                <td align="left">svm</td>
                <td align="right">0.06076923</td>
                <td align="right">0.12077855</td>
                <td align="right">0.7872984</td>
              </tr>
              <tr>
                <td align="left">svm2</td>
                <td align="right">0.04461538</td>
                <td align="right">0.10296755</td>
                <td align="right">0.8014135</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>The tuned SVM labeled svm2 performs slightly better than it is untuned version, though the difference is not substantial. Compared with the performance of the other classifiers applied to this dataset, it seems that the additional complexity of the SVM does not pay out in a setting where even simple methods perform well.</p>
      </sec>
      <sec>
        <title>3.2.2 Running times</title>
        <p>Table <xref ref-type="table" rid="T6">6</xref> shows the running times corresponding to the classifiers and variable selection methods outlined above, where * indicates that other programming languages than R are called, e.g. C/C++, Fortran. All computations were executed with a Pentium IV workstation, 2.8 Ghz, 1 GB main memory. The operating system was Windows XP.</p>
        <table-wrap position="float" id="T6">
          <label>Table 6</label>
          <caption>
            <p>Running times.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <td align="left" colspan="3">
                  <bold>Variable selection methods</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="2">Method</td>
                <td align="left">Running time per learningset</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" colspan="2">Multiclass F-Test</td>
                <td align="left">3.1 s</td>
              </tr>
              <tr>
                <td align="left" colspan="2">Krusal-Wallis test</td>
                <td align="left">3.5 s</td>
              </tr>
              <tr>
                <td align="left" colspan="2">Limma*</td>
                <td align="left">0.16s</td>
              </tr>
              <tr>
                <td align="left" colspan="2">Random Forest<sup>†,*</sup></td>
                <td align="left">4.1 s</td>
              </tr>
              <tr>
                <td colspan="3">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="3">
                  <bold>Classification methods</bold>
                </td>
              </tr>
              <tr>
                <td align="left">Method</td>
                <td align="left"># variables</td>
                <td align="left">Running time per <bold>50 </bold>learningsets</td>
              </tr>
              <tr>
                <td colspan="3">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td align="left">DLDA</td>
                <td align="left">all (2308)</td>
                <td align="left">2.7 s</td>
              </tr>
              <tr>
                <td align="left">LDA</td>
                <td align="left">10</td>
                <td align="left">1.4 s</td>
              </tr>
              <tr>
                <td align="left">QDA</td>
                <td align="left">2</td>
                <td align="left">1.0 s</td>
              </tr>
              <tr>
                <td align="left">Partial Least Squares</td>
                <td align="left">all (2308)</td>
                <td align="left">4.2 s</td>
              </tr>
              <tr>
                <td align="left">Shrunken Centroids</td>
                <td align="left">all (2308)</td>
                <td align="left">2.8 s</td>
              </tr>
              <tr>
                <td align="left">SVM*</td>
                <td align="left">all (2308)</td>
                <td align="left">88s</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <p>Running times of the different variable selection and classification methods used in the real life example. †: 500 bootstrap trees per run.</p>
          </table-wrap-foot>
        </table-wrap>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>CMA is a new user-friendly Bioconductor package for constructing and evaluating classifiers based on a high number of predictors in a unified framework. It was originally motivated by microarray-based classification, but can also be used for prediction based on other types of high-dimensional data such as, e.g. proteomic, metabolomic data, or signal data. CMA combines user-friendliness (simple and intuitive syntax, visualization tools) and methodological strength (especially in respect to variable selection and tuning procedures). We plan to further develop CMA and include additional features. Some potential extensions are outlined below.</p>
    <p>In the context of clinical bioinformatics, researchers often focus their attention on the additional predictive value of high-dimensional molecular data given that good clinical predictors are already available. In this context, combined classifiers using both clinical and high-dimensional molecular data have been recently developed [<xref ref-type="bibr" rid="B18">18</xref>,<xref ref-type="bibr" rid="B46">46</xref>]. Such methods could be integrated into the CMA framework by defining an additional argument corresponding to (mandatory) clinical variables.</p>
    <p>Another potential extension is the development of procedures for measuring the stability of classifiers, following the scheme of our Bioconductor package 'GeneSelector' [<xref ref-type="bibr" rid="B47">47</xref>] which implements resampling methods in the context of univariate ranking for the detection of differential expression. In our opinion, it is important to check the stability of predictive rules with respect to perturbations of the original data. This last aspect refers to the issue of 'noise discovery' and 'random findings' from microarray data [<xref ref-type="bibr" rid="B41">41</xref>,<xref ref-type="bibr" rid="B48">48</xref>]. In future research, one could also work on the inclusion of additional information about predictor variables in the form of gene ontologies or pathway maps as available from KEGG [<xref ref-type="bibr" rid="B49">49</xref>] or cMAP <ext-link ext-link-type="uri" xlink:href="http://pid.nci.nih.gov/"/> with the intention to stabilize variable selection and to simultaneously select groups of predictors, in the vein of the so-called "gene set enrichment analysis" [<xref ref-type="bibr" rid="B50">50</xref>].</p>
    <p>As kindly suggested by a reviewer, it could also be interesting to combine several classifiers into an ensemble. Such aggregated classifiers may be more robust and thus perform better than each single classifier. As a standardized interface to a large number of classifiers, CMA offers the opportunity to combine their results with very little effort.</p>
    <p>Lastly, CMA currently deals only with classification. The framework could be extended to other forms of high-dimensional regression, for instance high-dimensional survival analysis [<xref ref-type="bibr" rid="B51">51</xref>-<xref ref-type="bibr" rid="B54">54</xref>].</p>
    <p>In conclusion, we would like to outline in which situations CMA may help and warn against potential wrong use. CMA provides a unified interface to a large number of classifiers and allows a fair evaluation and comparison of the considered methods. Hence, CMA is a step towards reproducibility and standardization of research in the field of microarray-based outcome prediction. In particular, CMA users do not favor a given method or overestimate prediction accuracy due to wrong variable selection/tuning schemes. However, they should be cautious while interpreting and presenting their results. Trying all available classifiers successively and reporting only the best results would be a wrong approach [<xref ref-type="bibr" rid="B6">6</xref>] potentially leading to severe "optimistic bias". In this spirit, Ioannidis [<xref ref-type="bibr" rid="B41">41</xref>] points out that many results obtained with microarray data are nothing but "noise discovery" and Daumer et al [<xref ref-type="bibr" rid="B55">55</xref>] recommend to try to validate findings in an independent data set, whenever possible and feasible. In summary, instead of fishing for low prediction errors using all available methods, one should rather report all the obtained results or validate the best classifier using independent fresh validation data. Note that both procedures can be performed using CMA.</p>
  </sec>
  <sec>
    <title>5 Availability and requirements</title>
    <p>• Project name: CMA</p>
    <p>• Project homepage: <ext-link ext-link-type="uri" xlink:href="http://bioconductor.org/packages/2.3/bioc/html/CMA.html"/></p>
    <p>• Operating system: Windows, Linux, Mac</p>
    <p>• Programming language: R</p>
    <p>• Other requirements: Installation of the R software for statistical computing, release 2.7.0 or higher. For full functionality, the add-on packages 'MASS', 'class', 'nnet', 'glmpath', 'e1071', 'randomForest', 'plsgenomics', 'gbm', 'mgcv', 'corpcor', 'limma' are also required.</p>
    <p>• License: None for usage</p>
    <p>• Any restrictions to use by non-academics: None</p>
  </sec>
  <sec>
    <title>6 Authors' contributions</title>
    <p>MS implemented the CMA package and wrote the manuscript. ALB had the initial idea and supervised the project. ALB and MD contributed to the concept and to the manuscript.</p>
  </sec>
</body>
<back>
  <ack>
    <sec>
      <title>7 Acknowledgements</title>
      <p>We thank the four referees for their very constructive comments which helped us to improve this manuscript. This work was partially supported by the Porticus Foundation in the context of the International School for Technical Medicine and Clinical Bioinformatics.</p>
    </sec>
  </ack>
  <ref-list>
    <ref id="B1">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ihaka</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Gentleman</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>R: A language for data analysis and graphics</article-title>
        <source>Journal of Computational and Graphical Statistics</source>
        <year>1996</year>
        <volume>5</volume>
        <fpage>299</fpage>
        <lpage>314</lpage>
        <pub-id pub-id-type="doi">10.2307/1390807</pub-id>
      </citation>
    </ref>
    <ref id="B2">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gentleman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Carey</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bates</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Bolstad</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Dettling</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Dudoit</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ellis</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Gautier</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Ge</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Gentry</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hornik</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Hothorn</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Huber</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Iacus</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Irizarry</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Leisch</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Maechler</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rossini</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Sawitzki</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Smyth</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Tierney</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>JYH</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Bioconductor: Open software development for computational biology and bioinformatics</article-title>
        <source>Genome Biology</source>
        <year>2004</year>
        <volume>5</volume>
        <fpage>R80</fpage>
        <pub-id pub-id-type="pmid">15461798</pub-id>
        <pub-id pub-id-type="doi">10.1186/gb-2004-5-10-r80</pub-id>
      </citation>
    </ref>
    <ref id="B3">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Narasimhan</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Chu</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Diagnosis of multiple cancer types by shrunken centroids of gene expression</article-title>
        <source>Proceedings of the National Academy of Sciences</source>
        <year>2002</year>
        <volume>99</volume>
        <fpage>6567</fpage>
        <lpage>6572</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.082099299</pub-id>
      </citation>
    </ref>
    <ref id="B4">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Random Forests</article-title>
        <source>Machine Learning</source>
        <year>2001</year>
        <volume>45</volume>
        <fpage>5</fpage>
        <lpage>32</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id>
      </citation>
    </ref>
    <ref id="B5">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Boulesteix</surname>
            <given-names>AL</given-names>
          </name>
          <name>
            <surname>Strimmer</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Partial Least Squares: A versatile tool for the analysis of high-dimensional genomic data</article-title>
        <source>Briefings in Bioinformatics</source>
        <year>2007</year>
        <volume>8</volume>
        <fpage>32</fpage>
        <lpage>44</lpage>
        <pub-id pub-id-type="pmid">16772269</pub-id>
        <pub-id pub-id-type="doi">10.1093/bib/bbl016</pub-id>
      </citation>
    </ref>
    <ref id="B6">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dupuy</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Simon</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Critical Review of Published Microarray Studies for Cancer Outcome and Guidelines on Statistical Analysis and Reporting</article-title>
        <source>Journal of the National Cancer Institute</source>
        <year>2007</year>
        <volume>99</volume>
        <fpage>147</fpage>
        <lpage>157</lpage>
        <pub-id pub-id-type="pmid">17227998</pub-id>
        <pub-id pub-id-type="doi">10.1093/jnci/djk018</pub-id>
      </citation>
    </ref>
    <ref id="B7">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ambroise</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>McLachlan</surname>
            <given-names>GJ</given-names>
          </name>
        </person-group>
        <article-title>Selection bias in gene extraction in tumour classification on basis of microarray gene expression data</article-title>
        <source>Proceedings of the National Academy of Science</source>
        <year>2002</year>
        <volume>99</volume>
        <fpage>6562</fpage>
        <lpage>6566</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.102102699</pub-id>
      </citation>
    </ref>
    <ref id="B8">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Berrar</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Bradbury</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Dubitzky</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Avoiding model selection bias in small-sample genomic datasets</article-title>
        <source>Bioinformatics</source>
        <year>2006</year>
        <volume>22</volume>
        <fpage>1245</fpage>
        <lpage>1250</lpage>
        <pub-id pub-id-type="pmid">16500931</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl066</pub-id>
      </citation>
    </ref>
    <ref id="B9">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Boulesteix</surname>
            <given-names>AL</given-names>
          </name>
        </person-group>
        <article-title>WilcoxCV: An R package for fast variable selection in cross-validation</article-title>
        <source>Bioinformatics</source>
        <year>2007</year>
        <volume>23</volume>
        <fpage>1702</fpage>
        <lpage>1704</lpage>
        <pub-id pub-id-type="pmid">17495999</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btm162</pub-id>
      </citation>
    </ref>
    <ref id="B10">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Statnikov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Aliferis</surname>
            <given-names>CF</given-names>
          </name>
          <name>
            <surname>Tsamardinos</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Hardin</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Levy</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>A comprehensive evaluation of multicategory classification methods for microarray gene expression cancer diagnosis</article-title>
        <source>Bioinformatics</source>
        <year>2005</year>
        <volume>21</volume>
        <fpage>631</fpage>
        <lpage>643</lpage>
        <pub-id pub-id-type="pmid">15374862</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bti033</pub-id>
      </citation>
    </ref>
    <ref id="B11">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Varma</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Simon</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Bias in error estimation when using cross-validation for model selection</article-title>
        <source>BMC Bioinformatics</source>
        <year>2006</year>
        <volume>7</volume>
        <fpage>91</fpage>
        <pub-id pub-id-type="pmid">16504092</pub-id>
        <pub-id pub-id-type="doi">10.1186/1471-2105-7-91</pub-id>
      </citation>
    </ref>
    <ref id="B12">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Mar</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Gentleman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Carey</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <source>MLInterfaces: Uniform interfaces to R machine learning procedures for data in Bioconductor containers</source>
        <year>2007</year>
      </citation>
    </ref>
    <ref id="B13">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Gentleman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Carey</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Huber</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Irizarry</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Dudoit</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <source>Bioinformatics and Computational Biology Solutions Using R and Bioconductor</source>
        <year>2005</year>
        <publisher-name>New York: Springer</publisher-name>
      </citation>
    </ref>
    <ref id="B14">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Ruschhaupt</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mansmann</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Warnat</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Huber</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Benner</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <source>MCRestimate: Misclassification error estimation with cross-validation</source>
        <year>2007</year>
      </citation>
    </ref>
    <ref id="B15">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ruschhaupt</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Huber</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Poustka</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mansmann</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <article-title>A compendium to ensure computational reproducibility in high-dimensional classification tasks</article-title>
        <source>Statistical Applications in Genetics and Molecular Biology</source>
        <year>2004</year>
        <volume>3</volume>
        <fpage>37</fpage>
        <pub-id pub-id-type="doi">10.2202/1544-6115.1078</pub-id>
      </citation>
    </ref>
    <ref id="B16">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Braga-Neto</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Dougherty</surname>
            <given-names>ER</given-names>
          </name>
        </person-group>
        <article-title>Is cross-validation valid for small-sample microarray classification?</article-title>
        <source>Bioinformatics</source>
        <year>2004</year>
        <volume>20</volume>
        <fpage>374</fpage>
        <lpage>380</lpage>
        <pub-id pub-id-type="pmid">14960464</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btg419</pub-id>
      </citation>
    </ref>
    <ref id="B17">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Molinaro</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Simon</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Pfeiffer</surname>
            <given-names>RM</given-names>
          </name>
        </person-group>
        <article-title>Prediction error estimation: a comparison of resampling methods</article-title>
        <source>Bioinformatics</source>
        <year>2005</year>
        <volume>21</volume>
        <fpage>3301</fpage>
        <lpage>3307</lpage>
        <pub-id pub-id-type="pmid">15905277</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bti499</pub-id>
      </citation>
    </ref>
    <ref id="B18">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Boulesteix</surname>
            <given-names>AL</given-names>
          </name>
          <name>
            <surname>Porzelius</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Daumer</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Microarray-based classification and clinical predictors: On combined classifiers and additional predictive value</article-title>
        <source>Bioinformatics</source>
        <year>2008</year>
        <volume>24</volume>
        <fpage>1698</fpage>
        <lpage>1706</lpage>
        <pub-id pub-id-type="pmid">18544547</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btn262</pub-id>
      </citation>
    </ref>
    <ref id="B19">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Bagging predictors</article-title>
        <source>Machine Learning</source>
        <year>1996</year>
        <volume>24</volume>
        <fpage>123</fpage>
        <lpage>140</lpage>
      </citation>
    </ref>
    <ref id="B20">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Efron</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <source>An introduction to the bootstrap</source>
        <year>1993</year>
        <publisher-name>Chapman and Hall</publisher-name>
      </citation>
    </ref>
    <ref id="B21">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Narasimhan</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Chu</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <source>Imputation for microarray data (currently KNN only)</source>
        <year>2008</year>
      </citation>
    </ref>
    <ref id="B22">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Chambers</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>Programming with Data</source>
        <year>1998</year>
        <publisher-name>Springer, N.Y</publisher-name>
      </citation>
    </ref>
    <ref id="B23">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Donoho</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Johnstone</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Ideal spatial adaption by wavelet shrinkage</article-title>
        <source>Biometrika</source>
        <year>1994</year>
        <volume>81</volume>
        <fpage>425</fpage>
        <lpage>455</lpage>
        <pub-id pub-id-type="doi">10.1093/biomet/81.3.425</pub-id>
      </citation>
    </ref>
    <ref id="B24">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Ripley</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <source>Pattern Recognition and Neural Networks</source>
        <year>1996</year>
        <publisher-name>Cambridge University Press</publisher-name>
      </citation>
    </ref>
    <ref id="B25">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Wood</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <source>Generalized Additive Models: An Introduction with R</source>
        <year>2006</year>
        <publisher-name>Chapman and Hall/CRC</publisher-name>
      </citation>
    </ref>
    <ref id="B26">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Friedman</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Regularized discriminant analysis</article-title>
        <source>Journal of the American Statistical Association</source>
        <year>1989</year>
        <volume>84</volume>
        <fpage>165</fpage>
        <lpage>175</lpage>
        <pub-id pub-id-type="doi">10.2307/2289860</pub-id>
      </citation>
    </ref>
    <ref id="B27">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Regularized Discriminant Analysis and its Application in Microarrays</article-title>
        <source>Biostatistics</source>
        <year>2007</year>
        <volume>8</volume>
        <fpage>86</fpage>
        <lpage>100</lpage>
        <pub-id pub-id-type="pmid">16603682</pub-id>
        <pub-id pub-id-type="doi">10.1093/biostatistics/kxj035</pub-id>
      </citation>
    </ref>
    <ref id="B28">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Regression shrinkage and selection via the LASSO</article-title>
        <source>Journal of the Royal Statistical Society B</source>
        <year>1996</year>
        <volume>58</volume>
        <fpage>267</fpage>
        <lpage>288</lpage>
      </citation>
    </ref>
    <ref id="B29">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zou</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Regularization and variable selection via the elastic net</article-title>
        <source>Journal of the Royal Statistical Society B</source>
        <year>2005</year>
        <volume>67</volume>
        <fpage>301</fpage>
        <lpage>320</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1467-9868.2005.00503.x</pub-id>
      </citation>
    </ref>
    <ref id="B30">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Friedman</surname>
            <given-names>JH</given-names>
          </name>
        </person-group>
        <source>The elements of statistical learning</source>
        <year>2001</year>
        <publisher-name>New York: Springer-Verlag</publisher-name>
      </citation>
    </ref>
    <ref id="B31">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Friedman</surname>
            <given-names>JH</given-names>
          </name>
          <name>
            <surname>Olshen</surname>
            <given-names>RA</given-names>
          </name>
          <name>
            <surname>Stone</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <source>Classification and Regression Trees</source>
        <year>1984</year>
        <publisher-name>Monterey, CA: Wadsworth</publisher-name>
      </citation>
    </ref>
    <ref id="B32">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Freund</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Schapire</surname>
            <given-names>RE</given-names>
          </name>
        </person-group>
        <article-title>A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting</article-title>
        <source>Journal of Computer and System Sciences</source>
        <year>1997</year>
        <volume>55</volume>
        <fpage>119</fpage>
        <lpage>139</lpage>
        <pub-id pub-id-type="doi">10.1006/jcss.1997.1504</pub-id>
      </citation>
    </ref>
    <ref id="B33">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Friedman</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Greedy Function Approximation: A Gradient Boosting Machine</article-title>
        <source>Annals of Statistics</source>
        <year>2001</year>
        <volume>29</volume>
        <fpage>1189</fpage>
        <lpage>1232</lpage>
        <pub-id pub-id-type="doi">10.1214/aos/1013203451</pub-id>
      </citation>
    </ref>
    <ref id="B34">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Efficient quadratic regularization for expression arrays</article-title>
        <source>Biostatistics</source>
        <year>2004</year>
        <volume>5</volume>
        <fpage>329</fpage>
        <lpage>340</lpage>
        <pub-id pub-id-type="pmid">15208198</pub-id>
        <pub-id pub-id-type="doi">10.1093/biostatistics/kxh010</pub-id>
      </citation>
    </ref>
    <ref id="B35">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Golub</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Loan</surname>
            <given-names>CV</given-names>
          </name>
        </person-group>
        <source>Matrix Computations</source>
        <year>1983</year>
        <publisher-name>Johns Hopkins University Press</publisher-name>
      </citation>
    </ref>
    <ref id="B36">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Parzen</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>On estimation of a probability density function and mode</article-title>
        <source>Annals of Mathematical Statistics</source>
        <year>1962</year>
        <volume>33</volume>
        <fpage>1065</fpage>
        <lpage>1076</lpage>
        <pub-id pub-id-type="doi">10.1214/aoms/1177704472</pub-id>
      </citation>
    </ref>
    <ref id="B37">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Smyth</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Linear models and empirical Bayes methods for assessing differential expression in microarray experiments</article-title>
        <source>Statistical Applications in Genetics and Molecular Biology</source>
        <year>2004</year>
        <volume>3</volume>
        <fpage>3</fpage>
        <pub-id pub-id-type="doi">10.2202/1544-6115.1027</pub-id>
      </citation>
    </ref>
    <ref id="B38">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guyon</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Weston</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Barnhill</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Vapnik</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Gene Selection for Cancer Classification using support vector machines</article-title>
        <source>Journal of Machine Learning Research</source>
        <year>2002</year>
        <volume>46</volume>
        <fpage>389</fpage>
        <lpage>422</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1012487302797</pub-id>
      </citation>
    </ref>
    <ref id="B39">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bühlmann</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Boosting with the L2 loss: Regression and Classification</article-title>
        <source>Journal of the American Statistical Association</source>
        <year>2003</year>
        <volume>98</volume>
        <fpage>324</fpage>
        <lpage>339</lpage>
        <pub-id pub-id-type="doi">10.1198/016214503000125</pub-id>
      </citation>
    </ref>
    <ref id="B40">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Golub</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Slonim</surname>
            <given-names>DK</given-names>
          </name>
          <name>
            <surname>Tamayo</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Huard</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Gaasenbeek</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mesirov</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Coller</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Loh</surname>
            <given-names>ML</given-names>
          </name>
          <name>
            <surname>Downing</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Caligiuri</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Bloomfield</surname>
            <given-names>CD</given-names>
          </name>
          <name>
            <surname>Lander</surname>
            <given-names>ES</given-names>
          </name>
        </person-group>
        <article-title>Molecular classification of cancer: class discovery and class prediction by gene expression monitoring</article-title>
        <source>Science</source>
        <year>1999</year>
        <volume>286</volume>
        <fpage>531</fpage>
        <lpage>537</lpage>
        <pub-id pub-id-type="pmid">10521349</pub-id>
        <pub-id pub-id-type="doi">10.1126/science.286.5439.531</pub-id>
      </citation>
    </ref>
    <ref id="B41">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ioannidis</surname>
            <given-names>JP</given-names>
          </name>
        </person-group>
        <article-title>Microarrays and molecular research: noise discovery</article-title>
        <source>The Lancet</source>
        <year>2005</year>
        <volume>365</volume>
        <fpage>488</fpage>
        <lpage>492</lpage>
        <pub-id pub-id-type="doi">10.1016/S0140-6736(05)17866-0</pub-id>
      </citation>
    </ref>
    <ref id="B42">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Efron</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Improvements on cross-validation: The .632+ bootstrap method</article-title>
        <source>Journal of the American Statistical Association</source>
        <year>1997</year>
        <volume>92</volume>
        <fpage>548</fpage>
        <lpage>560</lpage>
        <pub-id pub-id-type="doi">10.2307/2965703</pub-id>
      </citation>
    </ref>
    <ref id="B43">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Khan</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ringner</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Saal</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Ladanyi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Westermann</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Berthold</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Schwab</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Antonescu</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Peterson</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Meltzer</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks</article-title>
        <source>Nature Medicine</source>
        <year>2001</year>
        <volume>7</volume>
        <fpage>673</fpage>
        <lpage>679</lpage>
        <pub-id pub-id-type="pmid">11385503</pub-id>
        <pub-id pub-id-type="doi">10.1038/89044</pub-id>
      </citation>
    </ref>
    <ref id="B44">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Narasimhan</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Chu</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Class prediction by nearest shrunken centroids, with applications to DNA microarrays</article-title>
        <source>Statistical Science</source>
        <year>2002</year>
        <volume>18</volume>
        <fpage>104</fpage>
        <lpage>117</lpage>
        <pub-id pub-id-type="doi">10.1214/ss/1056397488</pub-id>
      </citation>
    </ref>
    <ref id="B45">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Boulesteix</surname>
            <given-names>AL</given-names>
          </name>
        </person-group>
        <article-title>PLS dimension reduction for classification with microarray data</article-title>
        <source>Statistical Applications in Genetics and Molecular Biology</source>
        <year>2004</year>
        <volume>3</volume>
        <fpage>33</fpage>
        <pub-id pub-id-type="doi">10.2202/1544-6115.1075</pub-id>
      </citation>
    </ref>
    <ref id="B46">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Binder</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Schumacher</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Allowing for mandatory covariates in boosting estimation of sparse high-dimensional survival models</article-title>
        <source>BMC Bioinformatics</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>14</fpage>
        <pub-id pub-id-type="pmid">18186927</pub-id>
        <pub-id pub-id-type="doi">10.1186/1471-2105-9-14</pub-id>
      </citation>
    </ref>
    <ref id="B47">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Slawski</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Boulesteix</surname>
            <given-names>AL</given-names>
          </name>
        </person-group>
        <article-title>GeneSelector</article-title>
        <source>Bioconductor</source>
        <year>2008</year>
        <ext-link ext-link-type="uri" xlink:href="http://www.bioconductor.org/packages/devel/bioc/html/GeneSelector.html"/>
      </citation>
    </ref>
    <ref id="B48">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Davis</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Gerick</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Hintermair</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Friedel</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Fundel</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kueffner</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Zimmer</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Reliable gene signatures for microarray classification: assessment of stability and performance</article-title>
        <source>Bioinformatics</source>
        <year>2006</year>
        <volume>22</volume>
        <fpage>2356</fpage>
        <lpage>2363</lpage>
        <pub-id pub-id-type="pmid">16882647</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl400</pub-id>
      </citation>
    </ref>
    <ref id="B49">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kanehisa</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Goto</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>KEGG: Kyoto encyclopedia of genes and genomes</article-title>
        <source>Nucleic Acids Research</source>
        <year>2000</year>
        <volume>28</volume>
        <fpage>27</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="pmid">10592173</pub-id>
        <pub-id pub-id-type="doi">10.1093/nar/28.1.27</pub-id>
      </citation>
    </ref>
    <ref id="B50">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Subramanian</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tamayo</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Mootha</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Mukherjee</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ebert</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Gilette</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Paulovich</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pomeroy</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Golub</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Lander</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Mesirov</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Gene set enrichment analysis: A knowledge-based approach for interpreting genome-wide expression profiles</article-title>
        <source>Proceedings of the National Academy of Science</source>
        <year>2005</year>
        <volume>102</volume>
        <fpage>15545</fpage>
        <lpage>15550</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.0506580102</pub-id>
      </citation>
    </ref>
    <ref id="B51">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bovelstad</surname>
            <given-names>HM</given-names>
          </name>
          <name>
            <surname>Nygard</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Storvold</surname>
            <given-names>HL</given-names>
          </name>
          <name>
            <surname>Aldrin</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Borgan</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Frigessi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lingjaerde</surname>
            <given-names>OC</given-names>
          </name>
        </person-group>
        <article-title>Predicting survival from microarray data a comparative study</article-title>
        <source>Bioinformatics</source>
        <year>2007</year>
        <volume>23</volume>
        <fpage>2080</fpage>
        <lpage>2087</lpage>
        <pub-id pub-id-type="pmid">17553857</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btm305</pub-id>
      </citation>
    </ref>
    <ref id="B52">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schumacher</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Binder</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Gerds</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Assessment of survival prediction models based on microarray data</article-title>
        <source>Bioinformatics</source>
        <year>2007</year>
        <volume>23</volume>
        <fpage>1768</fpage>
        <lpage>1774</lpage>
        <pub-id pub-id-type="pmid">17485430</pub-id>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btm232</pub-id>
      </citation>
    </ref>
    <ref id="B53">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Diaz-Uriarte</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>SignS: a parallelized, open-source, freely available, web-based tool for gene selection and molecular signatures for survival and censored data</article-title>
        <source>BMC Bioinformatics</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>30</fpage>
        <pub-id pub-id-type="pmid">18208605</pub-id>
        <pub-id pub-id-type="doi">10.1186/1471-2105-9-30</pub-id>
      </citation>
    </ref>
    <ref id="B54">
      <citation citation-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>van Wieringen</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Kun</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Hampel</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Boulesteix</surname>
            <given-names>AL</given-names>
          </name>
        </person-group>
        <article-title>Survival prediction using gene expression data: a review and comparison</article-title>
        <source>Computational Statistics Data Analysis</source>
        <year>2008</year>
        <comment/>
      </citation>
    </ref>
    <ref id="B55">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Daumer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Held</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Ickstadt</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Heinz</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Schach</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ebers</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Reducing the probability of false positive research findings by pre-publication validation: Experience with a large multiple sclerosis database</article-title>
        <source>BMC Medical Research Methodology</source>
        <year>2008</year>
        <volume>8</volume>
        <fpage>18</fpage>
        <pub-id pub-id-type="pmid">18402689</pub-id>
        <pub-id pub-id-type="doi">10.1186/1471-2288-8-18</pub-id>
      </citation>
    </ref>
    <ref id="B56">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>McLachlan</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <source>Discriminant Analysis and Statistical Pattern Recognition</source>
        <year>1992</year>
        <publisher-name>Wiley, New York</publisher-name>
      </citation>
    </ref>
    <ref id="B57">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Young-Park</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>L1-regularization path algorithm for generalized linear models</article-title>
        <source>Journal of the Royal Statistical Society B</source>
        <year>2007</year>
        <volume>69</volume>
        <fpage>659</fpage>
        <lpage>677</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1467-9868.2007.00607.x</pub-id>
      </citation>
    </ref>
    <ref id="B58">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Classification of gene expression microarrays by penalized logistic regression</article-title>
        <source>Biostatistics</source>
        <year>2004</year>
        <volume>5</volume>
        <fpage>427</fpage>
        <lpage>443</lpage>
        <pub-id pub-id-type="pmid">15208204</pub-id>
        <pub-id pub-id-type="doi">10.1093/biostatistics/kxg046</pub-id>
      </citation>
    </ref>
    <ref id="B59">
      <citation citation-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Specht</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Probabilistic Neural Networks</article-title>
        <source>Neural Networks</source>
        <year>1990</year>
        <volume>3</volume>
        <fpage>109</fpage>
        <lpage>118</lpage>
        <pub-id pub-id-type="doi">10.1016/0893-6080(90)90049-Q</pub-id>
      </citation>
    </ref>
    <ref id="B60">
      <citation citation-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Scholkopf</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Smola</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <source>Learning with Kernels</source>
        <year>2002</year>
        <publisher-name>Cambridge, MA, USA: MIT Press</publisher-name>
      </citation>
    </ref>
  </ref-list>
</back>
