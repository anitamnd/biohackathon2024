<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8665768</article-id>
    <article-id pub-id-type="pmid">34270690</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btab501</article-id>
    <article-id pub-id-type="publisher-id">btab501</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Data and Text Mining</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Stable Iterative Variable Selection</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-7650-1862</contrib-id>
        <name>
          <surname>Mahmoudian</surname>
          <given-names>Mehrad</given-names>
        </name>
        <aff><institution>Turku Bioscience Centre, University of Turku and Åbo Akademi University</institution>, Turku, <country country="FI">Finland</country></aff>
        <aff><institution>Department of Future Technologies, University of Turku</institution>, Turku, <country country="FI">Finland</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Venäläinen</surname>
          <given-names>Mikko S</given-names>
        </name>
        <aff><institution>Turku Bioscience Centre, University of Turku and Åbo Akademi University</institution>, Turku, <country country="FI">Finland</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Klén</surname>
          <given-names>Riku</given-names>
        </name>
        <aff><institution>Turku Bioscience Centre, University of Turku and Åbo Akademi University</institution>, Turku, <country country="FI">Finland</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Elo</surname>
          <given-names>Laura L</given-names>
        </name>
        <xref rid="btab501-cor1" ref-type="corresp"/>
        <aff><institution>Turku Bioscience Centre, University of Turku and Åbo Akademi University</institution>, Turku, <country country="FI">Finland</country></aff>
        <aff><institution>Institute of Biomedicine, University of Turku</institution>, Turku, <country country="FI">Finland</country></aff>
        <!--laura.elo@utu.fi-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Wren</surname>
          <given-names>Jonathan</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btab501-cor1">To whom correspondence should be addressed. <email>laura.elo@utu.fi</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>15</day>
      <month>12</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2021-07-16">
      <day>16</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>16</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <volume>37</volume>
    <issue>24</issue>
    <fpage>4810</fpage>
    <lpage>4817</lpage>
    <history>
      <date date-type="received">
        <day>30</day>
        <month>12</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>20</day>
        <month>5</month>
        <year>2021</year>
      </date>
      <date date-type="editorial-decision">
        <day>17</day>
        <month>6</month>
        <year>2021</year>
      </date>
      <date date-type="corrected-typeset">
        <day>03</day>
        <month>8</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btab501.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>The emergence of datasets with tens of thousands of features, such as high-throughput omics biomedical data, highlights the importance of reducing the feature space into a distilled subset that can truly capture the signal for research and industry by aiding in finding more effective biomarkers for the question in hand. A good feature set also facilitates building robust predictive models with improved interpretability and convergence of the applied method due to the smaller feature space.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>Here, we present a robust feature selection method named Stable Iterative Variable Selection (SIVS) and assess its performance over both omics and clinical data types. As a performance assessment metric, we compared the number and goodness of the selected feature using SIVS to those selected by Least Absolute Shrinkage and Selection Operator regression. The results suggested that the feature space selected by SIVS was, on average, 41% smaller, without having a negative effect on the model performance. A similar result was observed for comparison with Boruta and caret RFE.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The method is implemented as an R package under GNU General Public License v3.0 and is accessible via Comprehensive R Archive Network (CRAN) via <ext-link xlink:href="https://cran.r-project.org/package=sivs" ext-link-type="uri">https://cran.r-project.org/package=sivs</ext-link>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>European Research Council</institution>
            <institution-id institution-id-type="DOI">10.13039/100010663</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>677943</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>European Union's Horizon 2020 Research and Innovation Programme</institution>
          </institution-wrap>
        </funding-source>
        <award-id>675395</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Academy of Finland</institution>
            <institution-id institution-id-type="DOI">10.13039/501100002341</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>296801</award-id>
        <award-id>304995</award-id>
        <award-id>310561</award-id>
        <award-id>314443</award-id>
        <award-id>329278</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Sigrid Juselius Foundation</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Academy of Finland</institution>
            <institution-id institution-id-type="DOI">10.13039/501100002341</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>322123</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Due to more and more cost-efficient data generation and collection methods, we have seen a substantial rise in the data volume of database submissions during the past decade. To put it in perspective, only in the Gene Expression Omnibus (GEO) database alone, there has been about a 9-fold increase in the number of omics datasets submitted from 2009 to 2019 compared to its preceding decade (1999–2009). The complexity of biological data and the high-dimensionality of the datasets impose a challenge in the analysis and interpretation of these datasets (<xref rid="btab501-B5" ref-type="bibr">Braun, 2014</xref>). Furthermore, with the current pace of technological advancements, we are getting more and more measurable features to be added to enrich our datasets, which leads to a constant increase in the dimensionality of the feature space. All these make it crucial to find the most effective and influential features from the feature spaces in order to reduce the number of measured features, which ultimately will reduce the data collection costs. On top of reducing the feature space, it is of utter importance to have a robust set of markers and models that are generalizable to other datasets beyond those that were used to train the models.</p>
    <p>Feature selection is a crucial part of machine learning in which the features that are most informative in relation to the response value are selected, while irrelevant and redundant features are discarded (<xref rid="btab501-B20" ref-type="bibr">Koller and Sahami, 1996</xref>; <xref rid="btab501-B23" ref-type="bibr">Lin <italic toggle="yes">et al.</italic>, 2014</xref>). One of the commonly used methods for high-dimensional data is generalized linear modeling in combination with a shrinkage method, namely Least Absolute Shrinkage and Selection Operator (LASSO) or Elastic Net (<xref rid="btab501-B35" ref-type="bibr">Tibshirani, 1996</xref>; <xref rid="btab501-B47" ref-type="bibr">Zou and Hastie, 2005</xref>), which efficiently reduces the feature space and also provides easily interpretable models. However, the major drawback of these methods is inconsistencies in the selected features and their number (<xref rid="btab501-B30" ref-type="bibr">Roberts and Nowak, 2014</xref>). This is mainly due to hyperparameter tuning that happens via cross-validation. Because of the nature of cross-validation, the resulting models are sensitive to the fold assignment causing inconsistencies between features obtained from multiple runs. Furthermore, in high-dimensional data, the massive difference between the feature space size versus the sample size can further increase this inconsistency. This, in turn, can drastically reduce the reproducibility of the study and cause vast disagreement between studies that have used the same or similar data and yet derived different conclusions and set of selected biomarkers, for example.</p>
    <p>LASSO and Elastic Net both fall into the category of embedded feature selection approaches in which the feature selection is made as a part of the classification algorithm (<xref rid="btab501-B14" ref-type="bibr">He and Yu, 2010</xref>; <xref rid="btab501-B25" ref-type="bibr">Mahendran <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btab501-B38" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2016</xref>). Recently, novel hybrid approaches have also been proposed that take advantage of multiple feature selection strategies, such as the conventional filter and wrapper approaches (<xref rid="btab501-B1" ref-type="bibr">Apolloni <italic toggle="yes">et al.</italic>, 2016</xref>). Despite their good applicability and improved performance in high-dimensional data compared to conventional feature selection algorithms (<xref rid="btab501-B24" ref-type="bibr">Lu <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btab501-B41" ref-type="bibr">Wei <italic toggle="yes">et al.</italic>, 2020</xref>), there is a need for novel, robust approaches that have a publicly available implementation that any researcher can easily apply to their own datasets.</p>
    <p>In this paper, we present a feature selection method, Stable Iterative Variable Selection (SIVS), and its implementation in R to effectively reduce the feature space to a small subset without decreasing the accuracy. This is achieved by considering multiple configurations of cross-validation sample binning, aggregation of the results for feature ranking, and ultimately shrinking the feature space.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <p>The general idea of SIVS is to encapsulate methods with embedded feature selection that are not robust in converging to the same feature space, thus resulting in inconsistent model performance. This is done via performing model construction multiple times and aggregating the resulting selected features. By repeatedly constructing models using different cross-validation sample binnings, we ensure that we have covered most, if not all, sample-binning compositions. The overall workflow of SIVS is summarized in <xref rid="btab501-F1" ref-type="fig">Figure 1A</xref> and represented in detail in the following sections. From hereafter, the encapsulated method is referred to as the ‘internal method’. In this article, we focus on using a multivariable Generalized Linear Modeling method implemented in R with LASSO and Elastic-Net regularization (glmnet) (<xref rid="btab501-B10" ref-type="bibr">Friedman <italic toggle="yes">et al.</italic>, 2010</xref>; <xref rid="btab501-B34" ref-type="bibr">Simon <italic toggle="yes">et al.</italic>, 2011</xref>) as the internal method, but the general concept can be extended to basically any method with cross-validation-dependent embedded feature selection.</p>
    <fig position="float" id="btab501-F1">
      <label>Fig. 1.</label>
      <caption>
        <p>Internal steps of SIVS method. (<bold>A</bold>) The general schema of the SIVS method. (<bold>B</bold>) Frequency of each feature having nonzero coefficient in the ‘iterative model building’ step. (<bold>C</bold>) Distribution of nonzero coefficients each feature has got in the ‘iterative model building’ step. Features are illustrated in a sorted order based on the median of their nonzero coefficients from high to low. (<bold>D</bold>) The main plot of the SIVS method, presenting an overview of the ‘RFE’ step. This plot is composed of three main elements: the bar chart that shows the VIMP, the box plots to show the distribution of AUROC after removal of each feature, and ultimately the two vertical dashed lines marking the two suggested strictness</p>
      </caption>
      <graphic xlink:href="btab501f1" position="float"/>
    </fig>
    <sec>
      <title>3.1 Step 1: general preprocessing</title>
      <p>The SIVS algorithm starts with preprocessing of the data, which includes removing redundant features and standardizing numeric values for the following steps. First, the numeric features with zero variance or categorical features containing a single class are removed from the data. Finally, all numeric features are standardized to have a mean of zero and a standard deviation of one. This is to make the models' coefficients comparable.</p>
      <p>At this point, we perform method-specific preprocessing if required by the internal method. For instance, glmnet prefers to have the input matrix as a ‘data.matrix’ object. Furthermore, glmnet is sensitive to missing values and, therefore, any sample with missing values is removed. Alternatively, imputation could be applied before applying SIVS by the user to retain more samples in the analysis.</p>
    </sec>
    <sec>
      <title>3.2 Step 2: iterative model building</title>
      <p>At the second step, a predefined number <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mi>k</mml:mi></mml:math></inline-formula> of models (by default <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mi>k</mml:mi><mml:mo> </mml:mo><mml:mo>=</mml:mo><mml:mo> </mml:mo></mml:math></inline-formula>100) are built using different cross-validation binnings and all the resulting models are collected, in addition to their prediction performance against the training data. In the implementation of the SIVS R package, the number of iterations can be configured by the user with regard to the sample size. A relatively high number of iterations would result in having the same binning configuration multiple times and consequently does not provide new information. On the other hand, a relatively low number of iterations would lead to not covering all binning arrangements. In practice, we have observed that 100 iterations is sufficient for obtaining a stable set of features in a range of datasets (<xref rid="btab501-B18" ref-type="bibr">Klén <italic toggle="yes">et al.</italic>, 2019</xref>, <xref rid="btab501-B16" ref-type="bibr">2020</xref>; <xref rid="btab501-B36" ref-type="bibr">Venäläinen <italic toggle="yes">et al.</italic>, 2020</xref>, <xref rid="btab501-B37" ref-type="bibr">2021</xref>).</p>
    </sec>
    <sec>
      <title>3.3 Step 3: variable importance (VIMP) scoring</title>
      <p>Based on the set of models built during the iterative step, VIMP score is calculated for each feature. The main idea is to assign a higher score to features that are selected by the majority of the models and are contributing the most in the model to predict the response value. Let us denote the features by <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, where <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mi>n</mml:mi><mml:mo> </mml:mo><mml:mo>&gt;</mml:mo><mml:mo> </mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> is the total number of features. For each feature <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo> </mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, the coefficients of the models built in SIVS are denoted by <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> and the vector of elements of <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> with nonzero values is denoted by <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>. The VIMP score is calculated by the following equation for features where <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> has a length greater than zero:
<disp-formula id="E1"><mml:math id="M1" display="block" overflow="scroll"><mml:mi mathvariant="normal">VIMP</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">abs</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="italic">IQR</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula>
 <disp-formula id="E2"><label>(1)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal">for</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal">all</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal">for</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal">all</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal">otherwise</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math></disp-formula>where <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math></inline-formula> is a binary value indicating if all elements of <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> are either positive or negative, <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">abs</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the median of absolute values, <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi mathvariant="normal"> </mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the number of elements, and <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mi>I</mml:mi><mml:mi>Q</mml:mi><mml:mi>R</mml:mi></mml:math></inline-formula> is the interquartile range.</p>
    </sec>
    <sec>
      <title>3.4 Step 4: recursive feature elimination (RFE)</title>
      <p>For the last step, all features with nonzero VIMP scores based on <xref rid="E2" ref-type="disp-formula">Equation (1)</xref> are kept in the analysis and fed into the last step, and the rest of the features get eliminated. During the RFE step, the features are removed one by one from the model building in the increasing order of their VIMP score calculated in the previous step. Upon eliminating each feature, a set of models (default = 100) are built using the remaining features and different cross-validation seeds to provide an unbiased view of the effect of the eliminated feature on the overall performance of the model.</p>
      <p>In the implementation of the SIVS R package, the output of all the aforementioned steps is returned as an S3 R object. There are also helper functions to assist in the interpretation and plotting of the results.</p>
      <p>To assist in choosing an appropriate cutoff for the features on the basis of their decreasing VIMP scores, we have implemented a method using the RFE output in the R package (<xref rid="btab501-F1" ref-type="fig">Fig. 1D</xref>). This method has a parameter <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mo>γ</mml:mo></mml:math></inline-formula> to adjust the strictness of the cutoff suggestion (<inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mo>γ</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>). The cutoff is calculated according to <xref rid="E3" ref-type="disp-formula">Equation (2)</xref>:
<disp-formula id="E3"><label>(2)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mi>suggested</mml:mi><mml:mo>.</mml:mo><mml:mi>cutoff</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mo>γ</mml:mo></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>max</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>min</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>min</mml:mi></mml:msub></mml:math></disp-formula>where <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>min</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>max</mml:mi></mml:msub></mml:math></inline-formula> are, respectively, the minimum and maximum of the Area Under the Receiver Operating Characteristic curves (AUROC) across the features in the feature elimination step. This cutoff practically defines the minimum acceptable AUROC over the training set. The higher the strictness parameter <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mo>γ</mml:mo></mml:math></inline-formula> is, the lower the suggested cutoff gets. By default, <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mo>γ</mml:mo><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:math></inline-formula> which is considered as loose and retains more features.</p>
    </sec>
    <sec>
      <title>3.5 Data and test design</title>
      <p>To evaluate the performance of SIVS for feature selection, we applied it on three different types of biomedical data in a binary classification setup using glmnet with 10-fold cross-validation and binomial family as the internal method. The goodness of the resulting features was assessed by comparing the models built using features suggested by SIVS, with corresponding models built without SIVS. For each of the training data, 100 different logistic regression models were built using the binomial family of glmnet with different cross-validation random seeds to assess the consistency of the resulting performance in the independent validation data. All used datasets are summarized in <xref rid="btab501-T1" ref-type="table">Table 1</xref> and described in more detail below.</p>
      <table-wrap position="float" id="btab501-T1">
        <label>Table 1.</label>
        <caption>
          <p>Data that has been used in this study</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Disease</th>
              <th rowspan="1" colspan="1">Response value</th>
              <th rowspan="1" colspan="1">Data type (platform)</th>
              <th rowspan="1" colspan="1">Accession ID</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Breast cancer</td>
              <td rowspan="1" colspan="1">Relapse-free survival</td>
              <td rowspan="1" colspan="1">Microarray (GPL96)</td>
              <td rowspan="1" colspan="1">GSE2034, GSE7390</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Lung cancer</td>
              <td rowspan="1" colspan="1">Subtype classification</td>
              <td rowspan="1" colspan="1">RNA-seq</td>
              <td rowspan="1" colspan="1">TCGA_LUAD, TCGA_LUSC</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Cardiovascular</td>
              <td rowspan="1" colspan="1">Occurrence of cardiovascular outcome</td>
              <td rowspan="1" colspan="1">Clinical</td>
              <td rowspan="1" colspan="1">SPRINT, ACCORD-BP</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Arcene</td>
              <td rowspan="1" colspan="1">Cancer versus healthy</td>
              <td rowspan="1" colspan="1">Mass-spectrometry</td>
              <td rowspan="1" colspan="1">ARCENE</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic toggle="yes">Note:</italic> To compare the method introduced in this article, four types of data have been used. This table presents the various data types that have been used in this article, in addition to the information on what has been used as response values.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>As a performance metric, we used AUROC. To test whether the observed difference in AUROC was significant, we performed a pair-wise AUROC comparison between each of the ROC curves of (1) the standard glmnet model and (2) the glmnet model that was built using SIVS suggested features when the same cross-validation seed was used. The pair-wise statistical comparison was performed using the DeLong method (<xref rid="btab501-B8" ref-type="bibr">DeLong <italic toggle="yes">et al.</italic>, 1988</xref>) implemented in the roc.test function in the pROC (<xref rid="btab501-B31" ref-type="bibr">Robin <italic toggle="yes">et al.</italic>, 2011</xref>) package. A list of all used R packages is presented in <xref rid="sup1" ref-type="supplementary-material">Table S1</xref> in Appendix.</p>
      <sec>
        <title>3.5.1 Breast cancer</title>
        <p>For breast cancer classification, we used two gene expression microarray datasets from the GEO database, namely GSE2034 (<xref rid="btab501-B39" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2005</xref>) and GSE7390 (<xref rid="btab501-B9" ref-type="bibr">Desmedt <italic toggle="yes">et al.</italic>, 2007</xref>). Both datasets have been generated using the Affymetrix Human Genome U133A Array platform and contained lymph-node negative breast cancer samples and their relapse-free survival information. The GSE2034 data consisted of a total of 286 patients, where 179 were relapse-free, and the other 107 were relapsed patients, whereas the GSE7390 consisted of 107 relapse-free and 91 relapsed patients. The relapse-free status of the patients was used as a binary response value in the analysis. The dataset with the larger sample size, GSE2034, was used as the training set, whereas the GSE7390 data was used for independent validation.</p>
        <p>To make both datasets comparable, the microarray datasets were independently preprocessed using the Oligo R package ( <xref rid="btab501-B7" ref-type="bibr">Carvalho and Irizarry, 2010</xref>) and then independently normalized by variance stabilization (<xref rid="btab501-B16" ref-type="bibr">Huber <italic toggle="yes">et al.</italic>, 2002</xref>) using vsn R package (<xref rid="btab501-B17" ref-type="bibr">Huber <italic toggle="yes">et al.</italic>, 2021</xref>).</p>
      </sec>
      <sec>
        <title>3.5.2 Lung cancer</title>
        <p>For lung cancer classification, we used two RNA-seq datasets from The Cancer Genome Atlas (TCGA) database, namely Lung Adenocarcinoma (TCGA-LUAD) and Lung Squamous Cell Carcinoma (TCGA-LUSC). Both datasets were downloaded in the Fragments Per Kilobase of transcript per Million mapped reads upper quartile (FPKM-UQ) normalized (Bioinformatics Pipeline: mRNA Analysis—GDC Docs; HTSeq-FPKM-UQ—GDC Docs; <xref rid="btab501-B33" ref-type="bibr">Shahriyari, 2019</xref>) format using the following two queries:
</p>
        <list list-type="bullet">
          <list-item>
            <p>cases.primary_site in [‘bronchus and lung’] and cases.project.project_id in [‘TCGA-LUAD’] and files.access in [‘open’] and files.analysis.workflow_type in [‘HTSeq—FPKM-UQ’] and files.data_type in [‘Gene Expression Quantification’]</p>
          </list-item>
          <list-item>
            <p>cases.primary_site in [‘bronchus and lung’] and cases.project.project_id in [‘TCGA-LUSC’] and files.access in [‘open’] and files.analysis.workflow_type in [‘HTSeq—FPKM-UQ’] and files.data_type in [‘Gene Expression Quantification’]</p>
          </list-item>
        </list>
        <p>The TCGA-LUAD and TCGA-LUSC contained 594 and 551 samples, respectively. Considering that these two datasets contained samples from different subtypes of Lung cancer, we used their combination and built a model to differentiate the two subtypes. To form the training and validation sets, we randomly selected 100 samples from each subtype to create the validation set and used the rest of the samples (<italic toggle="yes">N</italic> = 945) as the training set.</p>
      </sec>
      <sec>
        <title>3.5.3 Cardiovascular disease</title>
        <p>For prediction of cardiovascular disease events, we used data from two clinical trials, namely the Systolic Blood Pressure Intervention Trial (SPRINT) and the Action to Control Cardiovascular Risk in Diabetes Blood Pressure (ACCORD-BP) trial, both of which compared two antihypertensive treatment strategies and their effects on cardiovascular outcomes (<xref rid="btab501-B4" ref-type="bibr">Buse, 2007</xref>; <xref rid="btab501-B43" ref-type="bibr">Wright <italic toggle="yes">et al.</italic>, 2015</xref>). The SPRINT and ACCORD-BP datasets involved 9361 and 4733 participants, respectively. Here, we applied SIVS on SPRINT data to predict the occurrence of primary composite cardiovascular disease outcome (the first occurrence of myocardial infarction, acute coronary syndrome, stroke, heart failure or death from cardiovascular causes) and validated the performance of the models against ACCORD-BP data. Both datasets were available on request from the National Heart, Lung and Blood Institute's (NHLBI) Biologic Specimen and Data Repository Information Coordinating Center (BioLINCC, <ext-link xlink:href="https://biolincc.nhlbi.nih.gov/" ext-link-type="uri">https://biolincc.nhlbi.nih.gov/</ext-link>). Here, we used similar variable preprocessing as described before (<xref rid="btab501-B36" ref-type="bibr">Venäläinen <italic toggle="yes">et al.</italic>, 2020</xref>).</p>
      </sec>
      <sec>
        <title>3.5.4 Arcene data</title>
        <p>In addition to the datasets above, we used a publicly available benchmarking dataset. This dataset is a benchmarking dataset made by aggregating three different mass-spectrometry datasets, and has been designed for testing the performance of feature selection methods and has been used in the NIPS 2003 feature selection challenge (<xref rid="btab501-B13" ref-type="bibr">Guyon <italic toggle="yes">et al.</italic>, 2005</xref>). This dataset has 1000 anonymized features and separate training set and validation set, each with 100 anonymized samples. Both the training and validation set have 44 samples with a positive response and 56 samples with a negative response.</p>
      </sec>
      <sec>
        <title>3.5.5 Comparison to available feature selection methods</title>
        <p>We compared the performance of SIVS against two publicly available, widely used feature selection methods: Boruta and RFE. Boruta is an iterative feature selection algorithm based on the random forest classification algorithm (<xref rid="btab501-B21" ref-type="bibr">Kursa and Rudnicki, 2010</xref>). In each iteration, it uses shuffled shadow variables and calculates <italic toggle="yes">Z</italic> scores to determine feature importance. RFE is a feature ranking method, which starts from a model with all features and in each iteration drops out a certain number of least important features (<xref rid="btab501-B12" ref-type="bibr">Guyon <italic toggle="yes">et al.</italic>, 2002</xref>). For these methods, we used implementations available in R packages <italic toggle="yes">Boruta</italic> and <italic toggle="yes">caret</italic>, respectively. RFE could only be applied to cardiovascular disease and Arcene datasets due to memory issues occurring with the handling of RNA-seq and microarray data with over 20 000 variables.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <p>For the breast cancer data, the standard glmnet models with median AUROC of 0.63 using the median of 76 features (range: 59–107) in the 100 different runs on the full feature space, whereas SIVS built models with median AUROC of 0.61 by constantly using 41 features (<xref rid="btab501-T2" ref-type="table">Table 2</xref> and <xref rid="btab501-F2" ref-type="fig">Fig. 2A–C</xref>). On lung cancer data, a median AUROC of 0.99 was achieved with the standard glmnet using the median of 114 features (range: 76–158), while with SIVS, the median AUROC of 0.99 was achieved using the median of 43 features (range: 41–45). On the cardiovascular disease data, the standard glmnet obtained a median AUROC of 0.70 using the median feature of 15 (range: 14–15), whereas with SIVS, the median AUROC of 0.69 was obtained using 13 features throughout all the 100 models. Thus, SIVS on average selected 49.6% fewer features compared to standard glmnet in all datasets, and on average 61.7% fewer features in high-dimensional datasets, while the models built using these features produced similar AUROC values (<xref rid="btab501-T2" ref-type="table">Table 2</xref> and <xref rid="btab501-F2" ref-type="fig">Fig. 2B</xref>). The number of features with SIVS was significantly lower than with standard glmnet (paired Wilcoxon test <italic toggle="yes">P</italic>-values 3.50e−18, 3.88e−18, and 4.71e−20, for breast cancer, lung cancer, and cardiovascular, respectively) without significantly affecting the AUROC values of the final models (DeLong median of <italic toggle="yes">P</italic>-values 0.42, 0.76, and 0.09, for breast cancer, lung cancer, and cardiovascular, respectively) (<xref rid="btab501-F3" ref-type="fig">Fig. 3</xref>). By looking closer at the performances of each type of run, we see that while the models built based on features selected by SIVS use fewer features compared to their counterparts, their performance is more uniform across different runs with different cross-validation random seeds (<xref rid="btab501-F2" ref-type="fig">Fig. 2C</xref>) which indicates the stability and robustness of the models built using features selected by SIVS. We observed similar behavior and performance on the Arcene benchmarking dataset as compared to the aforementioned results (<xref rid="btab501-F2" ref-type="fig">Fig. 2</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>). Additional performance metrics for all datasets that show similar trends as AUROC are available in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>.</p>
    <fig position="float" id="btab501-F2">
      <label>Fig. 2.</label>
      <caption>
        <p>Side-by-side comparison of glmnet and SIVS. (<bold>A</bold>) The number of features that were used in each of the 100 glmnet models built using SIVS features (SIVS + glmnet), Boruta features (Boruta + glmnet) and plain glmnet. For each dataset, all three types of runs were performed 100 times with 100 different cross-validation seeds to assess the stability of the outcomes. (<bold>B</bold> and <bold>C</bold>) Performance of these models on the test sets. The plots on the second row (panel B) illustrate that there is no significant difference in the performance between the models that were built using features selected by SIVS and models that were built without despite the fact that the models built using SIVS use far fewer features as illustrated in panel A. Additionally, the plots in panel C illustrate the same data points as panel B, but are zoomed-in to show the performance robustness of models that are built using SIVS selected features compared to glmnet and Boruta + glmnet. (<bold>D</bold>) Venn diagrams depicting the overlap of the selected features via their intersection (∩) and union (∪), showing that the feature space suggested by SIVS is always a subset of standard glmnet feature space, and typically the feature space of SIVS is so robust that the intersect and union are the same set</p>
      </caption>
      <graphic xlink:href="btab501f2" position="float"/>
    </fig>
    <fig position="float" id="btab501-F3">
      <label>Fig. 3.</label>
      <caption>
        <p>Significance of SIVS feature reduction on the final model. The AUROC of the glmnet models built using the full feature space and built using only SIVS suggested features were tested in a pair-wise fashion where models that were built using the same cross-validation seeds were compared together using the Delong method with two-sided alternative hypothesis (<xref rid="btab501-B8" ref-type="bibr">DeLong <italic toggle="yes">et al.</italic>, 1988</xref>)</p>
      </caption>
      <graphic xlink:href="btab501f3" position="float"/>
    </fig>
    <table-wrap position="float" id="btab501-T2">
      <label>Table 2.</label>
      <caption>
        <p>Run-type comparison and their models’ consistency</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col valign="top" align="left" span="1"/>
          <col valign="top" align="left" span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th rowspan="1" colspan="1"/>
            <th rowspan="1" colspan="1">Run-type</th>
            <th colspan="4" rowspan="1">glmnet<hr/></th>
            <th colspan="4" rowspan="1">SIVS + glmnet<hr/></th>
            <th colspan="4" rowspan="1">Boruta + glmnet<hr/></th>
          </tr>
          <tr>
            <th rowspan="1" colspan="1">Metric</th>
            <th rowspan="1" colspan="1">Detail</th>
            <th rowspan="1" colspan="1">Breast cancer</th>
            <th rowspan="1" colspan="1">Lung cancer</th>
            <th rowspan="1" colspan="1">Cardiovascular</th>
            <th rowspan="1" colspan="1">Arcene</th>
            <th rowspan="1" colspan="1">Breast cancer</th>
            <th rowspan="1" colspan="1">Lung cancer</th>
            <th rowspan="1" colspan="1">Cardiovascular</th>
            <th rowspan="1" colspan="1">Arcene</th>
            <th rowspan="1" colspan="1">Breast cancer</th>
            <th rowspan="1" colspan="1">Lung cancer</th>
            <th rowspan="1" colspan="1">Cardiovascular</th>
            <th rowspan="1" colspan="1">Arcene</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="7" colspan="1">Number of selected features</td>
            <td rowspan="1" colspan="1">Maximum</td>
            <td rowspan="1" colspan="1">107</td>
            <td rowspan="1" colspan="1">158</td>
            <td rowspan="1" colspan="1">15</td>
            <td rowspan="1" colspan="1">59</td>
            <td rowspan="1" colspan="1">41</td>
            <td rowspan="1" colspan="1">45</td>
            <td rowspan="1" colspan="1">13</td>
            <td rowspan="1" colspan="1">10</td>
            <td rowspan="1" colspan="1">7</td>
            <td rowspan="1" colspan="1">35</td>
            <td rowspan="1" colspan="1">11</td>
            <td rowspan="1" colspan="1">7</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Median</td>
            <td rowspan="1" colspan="1">76</td>
            <td rowspan="1" colspan="1">114</td>
            <td rowspan="1" colspan="1">15</td>
            <td rowspan="1" colspan="1">43</td>
            <td rowspan="1" colspan="1">41</td>
            <td rowspan="1" colspan="1">43</td>
            <td rowspan="1" colspan="1">13</td>
            <td rowspan="1" colspan="1">10</td>
            <td rowspan="1" colspan="1">7</td>
            <td rowspan="1" colspan="1">34</td>
            <td rowspan="1" colspan="1">11</td>
            <td rowspan="1" colspan="1">6</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Mean</td>
            <td rowspan="1" colspan="1">79.12</td>
            <td rowspan="1" colspan="1">114.08</td>
            <td rowspan="1" colspan="1">14.74</td>
            <td rowspan="1" colspan="1">41.88</td>
            <td rowspan="1" colspan="1">41</td>
            <td rowspan="1" colspan="1">42.42</td>
            <td rowspan="1" colspan="1">13</td>
            <td rowspan="1" colspan="1">10</td>
            <td rowspan="1" colspan="1">7</td>
            <td rowspan="1" colspan="1">33.62</td>
            <td rowspan="1" colspan="1">11</td>
            <td rowspan="1" colspan="1">6.44</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Minimum</td>
            <td rowspan="1" colspan="1">59</td>
            <td rowspan="1" colspan="1">76</td>
            <td rowspan="1" colspan="1">14</td>
            <td rowspan="1" colspan="1">28</td>
            <td rowspan="1" colspan="1">41</td>
            <td rowspan="1" colspan="1">41</td>
            <td rowspan="1" colspan="1">13</td>
            <td rowspan="1" colspan="1">10</td>
            <td rowspan="1" colspan="1">7</td>
            <td rowspan="1" colspan="1">32</td>
            <td rowspan="1" colspan="1">11</td>
            <td rowspan="1" colspan="1">6</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Standard deviation</td>
            <td rowspan="1" colspan="1">9.6685</td>
            <td rowspan="1" colspan="1">15.9524</td>
            <td rowspan="1" colspan="1">0.4408</td>
            <td rowspan="1" colspan="1">7.9089</td>
            <td rowspan="1" colspan="1">0</td>
            <td rowspan="1" colspan="1">1.165</td>
            <td rowspan="1" colspan="1">0</td>
            <td rowspan="1" colspan="1">0</td>
            <td rowspan="1" colspan="1">0</td>
            <td rowspan="1" colspan="1">0.8261</td>
            <td rowspan="1" colspan="1">0</td>
            <td rowspan="1" colspan="1">0.4989</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Intersect</td>
            <td rowspan="1" colspan="1">54</td>
            <td rowspan="1" colspan="1">58</td>
            <td rowspan="1" colspan="1">14</td>
            <td rowspan="1" colspan="1">26</td>
            <td rowspan="1" colspan="1">41</td>
            <td rowspan="1" colspan="1">41</td>
            <td rowspan="1" colspan="1">13</td>
            <td rowspan="1" colspan="1">10</td>
            <td rowspan="1" colspan="1">7</td>
            <td rowspan="1" colspan="1">30</td>
            <td rowspan="1" colspan="1">11</td>
            <td rowspan="1" colspan="1">6</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Union</td>
            <td rowspan="1" colspan="1">112</td>
            <td rowspan="1" colspan="1">177</td>
            <td rowspan="1" colspan="1">16</td>
            <td rowspan="1" colspan="1">60</td>
            <td rowspan="1" colspan="1">41</td>
            <td rowspan="1" colspan="1">45</td>
            <td rowspan="1" colspan="1">13</td>
            <td rowspan="1" colspan="1">10</td>
            <td rowspan="1" colspan="1">7</td>
            <td rowspan="1" colspan="1">35</td>
            <td rowspan="1" colspan="1">11</td>
            <td rowspan="1" colspan="1">7</td>
          </tr>
          <tr>
            <td rowspan="5" colspan="1">AUROC [validation]</td>
            <td rowspan="1" colspan="1">Maximum</td>
            <td rowspan="1" colspan="1">64.05%</td>
            <td rowspan="1" colspan="1">99.36%</td>
            <td rowspan="1" colspan="1">69.58%</td>
            <td rowspan="1" colspan="1">75.53%</td>
            <td rowspan="1" colspan="1">61.16%</td>
            <td rowspan="1" colspan="1">99.37%</td>
            <td rowspan="1" colspan="1">69.43%</td>
            <td rowspan="1" colspan="1">72.73%</td>
            <td rowspan="1" colspan="1">56.96%</td>
            <td rowspan="1" colspan="1">99.28%</td>
            <td rowspan="1" colspan="1">69.38%</td>
            <td rowspan="1" colspan="1">69.48%</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Median</td>
            <td rowspan="1" colspan="1">62.51%</td>
            <td rowspan="1" colspan="1">99.19%</td>
            <td rowspan="1" colspan="1">69.51%</td>
            <td rowspan="1" colspan="1">74.84%</td>
            <td rowspan="1" colspan="1">60.88%</td>
            <td rowspan="1" colspan="1">99.30%</td>
            <td rowspan="1" colspan="1">69.37%</td>
            <td rowspan="1" colspan="1">71.39%</td>
            <td rowspan="1" colspan="1">56.93%</td>
            <td rowspan="1" colspan="1">99.16%</td>
            <td rowspan="1" colspan="1">69.31%</td>
            <td rowspan="1" colspan="1">69.32%</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Mean</td>
            <td rowspan="1" colspan="1">62.01%</td>
            <td rowspan="1" colspan="1">99.17%</td>
            <td rowspan="1" colspan="1">69.52%</td>
            <td rowspan="1" colspan="1">74.74%</td>
            <td rowspan="1" colspan="1">60.93%</td>
            <td rowspan="1" colspan="1">99.30%</td>
            <td rowspan="1" colspan="1">69.37%</td>
            <td rowspan="1" colspan="1">71.58%</td>
            <td rowspan="1" colspan="1">56.93%</td>
            <td rowspan="1" colspan="1">99.16%</td>
            <td rowspan="1" colspan="1">69.32%</td>
            <td rowspan="1" colspan="1">69.24%</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Minimum</td>
            <td rowspan="1" colspan="1">59.42%</td>
            <td rowspan="1" colspan="1">98.83%</td>
            <td rowspan="1" colspan="1">69.45%</td>
            <td rowspan="1" colspan="1">74.19%</td>
            <td rowspan="1" colspan="1">60.74%</td>
            <td rowspan="1" colspan="1">99.25%</td>
            <td rowspan="1" colspan="1">69.37%</td>
            <td rowspan="1" colspan="1">70.74%</td>
            <td rowspan="1" colspan="1">56.89%</td>
            <td rowspan="1" colspan="1">98.99%</td>
            <td rowspan="1" colspan="1">69.31%</td>
            <td rowspan="1" colspan="1">68.59%</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Standard deviation</td>
            <td rowspan="1" colspan="1">0.0108</td>
            <td rowspan="1" colspan="1">0.0009</td>
            <td rowspan="1" colspan="1">0.0003</td>
            <td rowspan="1" colspan="1">0.0041</td>
            <td rowspan="1" colspan="1">0.001</td>
            <td rowspan="1" colspan="1">0.0002</td>
            <td rowspan="1" colspan="1">0.0001</td>
            <td rowspan="1" colspan="1">0.0053</td>
            <td rowspan="1" colspan="1">0.0002</td>
            <td rowspan="1" colspan="1">0.0005</td>
            <td rowspan="1" colspan="1">0.0002</td>
            <td rowspan="1" colspan="1">0.0023</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="tblfn2">
          <p><italic toggle="yes">Note</italic>: For each data type that is used in this article and for each method, 100 modelings and testings have been done using 100 different cross-validation seeds. This table presents the consistency of each method in terms of the number of selected features and AUROC.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <p>While SIVS produced a smaller feature space than the standard glmnet, the overlap of the selected features by SIVS across the different runs was markedly higher (<xref rid="btab501-F2" ref-type="fig">Fig. 2D</xref>). In addition, the small standard deviation of the AUROC values across the 100 different runs, further supported the stability of the SIVS-based models. Moreover, it is worth noting that the features selected by SIVS were a subset of the features selected by the standard glmnet (<xref rid="btab501-F2" ref-type="fig">Fig. 2D</xref>).</p>
    <p>Comparisons with Boruta and RFE revealed that SIVS performed consistently as well as them or even slightly outperformed them in terms of the number of selected variables (<xref rid="btab501-F2" ref-type="fig">Fig. 2</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S1</xref>). Most importantly, SIVS produced substantially more stable feature sets compared to stock glmnet (<xref rid="btab501-F2" ref-type="fig">Fig. 2</xref> and <xref rid="btab501-T2" ref-type="table">Table 2</xref>) but also compared to RFE, especially in Arcene dataset (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S1</xref>).</p>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <p>This study introduces SIVS, a novel feature selection method that can effectively reduce the feature space, especially in high-dimensional data, and provides insight into each feature's impact with regards to the response value. SIVS starts from aggregating the results of multiple multivariable modeling runs using different cross-validation random seeds. As a result, it provides feature importance scores for the features and consequently orders them accordingly. This score is then utilized in an RFE step to inspect the effect of each feature's removal on the stability and predictive power of the resulting model, which is ultimately used in narrowing down the list of important features to a much smaller subset.</p>
    <p>To assess the performance of SIVS and the goodness of the selected features, 100 models built using SIVS’ features were compared with 100 models built using plain glmnet. These models were compared based on their predictive power on a separate test set, considering the number of features they have and the variability of these features among the 100 models. This procedure was applied to three different datasets. The presented results demonstrate the effectiveness of SIVS as a feature selection method on various high- and low-dimensional biomedical data, where SIVS reduced the feature space down to 38% of the features that LASSO can typically select, without having any significant drawback in the predictive power of the model over independent test sets. Moreover, the features selected by SIVS were markedly more stable over multiple runs than those selected by standard glmnet.</p>
    <p>Feature selection methods can be divided into three categories: filter methods, wrapper methods, and embedded methods (<xref rid="btab501-B32" ref-type="bibr">Saeys <italic toggle="yes">et al.</italic>, 2007</xref>). In general, filter methods are independent of the machine learning method, i.e. model-agnostic, whereas wrapper and embedded methods are model-dependent. There have been some attempts to address model-agnostic feature selection on high-dimensional data (<xref rid="btab501-B22" ref-type="bibr">Labani <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btab501-B29" ref-type="bibr">Reggiani <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btab501-B45" ref-type="bibr">Yu and Liu, 2003</xref>), but the majority of available methods are model-dependent. Among these, some are designed to work for specific types of application (<xref rid="btab501-B40" ref-type="bibr">Wehrens and Franceschi, 2015</xref>), and some are more general (<xref rid="btab501-B27" ref-type="bibr">Perrot-Dockès <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btab501-B44" ref-type="bibr">Xu and Chen, 2014</xref>; <xref rid="btab501-B40" ref-type="bibr">Wehrens and Franceschi, 2015</xref>). SIVS is a method that falls into the model-dependent wrapper category, but due to the usage of internal methods, it is not dependent on one specific algorithm and is, therefore, more versatile. Although in the present study, we focused on glmnet as the underlying feature selection and model building method, the general concept can, in theory, be extended to methods with embedded feature selection/importance that depends on cross-validation for evaluating the weights for features such as random forests or Generalized Boosted Regression Models.</p>
    <p>At the time of writing this article, there are 51 packages dependent on glmnet, out of which seven address feature selection [BioMark (<xref rid="btab501-B40" ref-type="bibr">Wehrens and Franceschi, 2015</xref>), elasso (<xref rid="btab501-B11" ref-type="bibr">Guo, 2015</xref>), EstHer (<xref rid="btab501-B4" ref-type="bibr">Bonnet and Levy-Leduc, 2015</xref>), glmvsd (<xref rid="btab501-B26" ref-type="bibr">Nan <italic toggle="yes">et al.</italic>, 2016</xref>), GRridge (<xref rid="btab501-B42" ref-type="bibr">van de Wiel and Novianti, 2020</xref>), MultiVarSel (<xref rid="btab501-B28" ref-type="bibr">Perrot-Dockès <italic toggle="yes">et al.</italic>, 2019</xref>) and SMLE (<xref rid="btab501-B46" ref-type="bibr">Zang <italic toggle="yes">et al.</italic>, 2021</xref>)]. As is suggested by our results, standard glmnet models were inherently not consistent in terms of the selected features or accuracy and, therefore, methods that use glmnet models internally without building multiple glmnet models and somehow aggregate their results are also susceptible to inherit this inconsistency.</p>
    <p>SIVS is shipped with a method to suggest an appropriate cutoff for the exclusion of features with lower importance. The strictness of this suggestion method can be tuned (default = 0.01), and it is important to note that there is no one size fits all solution. The strictness threshold is subjective, and we encourage users to choose the threshold based on the RFE plot. To stay fair in this study, we consistently used the default parameters without modifications or tuning, but this is not to undermine the fact that the SIVS should not be treated as a blackbox feature selection method, and the parameters should be tuned according to the specification of data and the question in hand.</p>
    <p>A major strength of this study is that we have focused on real-world datasets instead of synthetic data to demonstrate the practical utility of the method. Moreover, we have used independent validation datasets to show how much the selected features generalize to other datasets. Lastly, we provide a ready-to-use implementation of the method. The method is implemented in R language and in compliance with the Comprehensive R Archive Network (CRAN) standards and regulation and is published on CRAN. Therefore, SIVS can be freely accessed, installed, and tested. Additionally, the SIVS source code is published under General Public License v3.0 (GPL3) and is publicly available on Github.</p>
    <p>A drawback of SIVS is that despite the multithreaded implementation of the method, it is relatively slow to compute due to multiple iterations (<italic toggle="yes">k</italic> = 100 by default). Additionally, considering that SIVS is wrapping the internal method, consequently it will inherit the limitation of that method as well (in the case of current implementation, glmnet). For example, due to L1 regularization, LASSO is known to have issues with colinear features which was not explored here. However, the aim of this study was to show the feasibility of the proposed method with a working implementation. For further optimization in the future, various possible alternatives could be considered. For example, the effect of replacing LASSO with a smooth function or exploring the effect of colinear features on SIVS performance as well as implementing alternative performance metrics and testing other prediction scenarios, such as multilabel classification, will be left as next steps for future research.</p>
    <p>In this article, we have showcased SIVS with glmnet as an internal method via three binary classifications, but in theory, SIVS can be applied on any of the model families that glmnet can be used for, as long as the predicted outcome can be used in receiver operating characteristic (ROC) curve calculation in pROC package. This limitation could also be loosened by the addition of other performance metrics into SIVS. In addition to glmnet, SIVS also naturally extends to other forms of modeling with embedded feature selection or shrinkage methods, such as random forest. Implementing these additional internal methods and other performance metrics will be done in the next versions of the SIVS R package.</p>
  </sec>
  <sec>
    <title>5 Conclusion</title>
    <p>This study shows how a single run of glmnet is not an optimal solution for finding the best feature space in terms of consistency in performance and the number of incorporated features in the final model. SIVS, the method presented in this article, is a feature selection method that can drastically reduce the feature space without substantially sacrificing the performance and produces consistent results across multiple runs. This indicates that the ‘true signal’ is more effectively captured by SIVS compared to the standard glmnet.</p>
    <p>All the scripts for data preprocessing and analysis are available upon request. The SIVS can be directly installed from CRAN, and the source code can be accessed through the following webpage:
</p>
    <list list-type="bullet">
      <list-item>
        <p>
          <ext-link xlink:href="https://cran.r-project.org/web/packages/sivs/" ext-link-type="uri">https://cran.r-project.org/package=sivs</ext-link>
        </p>
      </list-item>
    </list>
  </sec>
  <sec>
    <title>Author contributions</title>
    <p>M.M. participated in the study design, conducted the analyses, developed the method, wrote the manuscript and developed the SIVS R package. M.S.V. participated in method development, participated in study design, preprocessed the clinical data, participated in the analyses and participated in writing the manuscript. R.K. participated in the development of the method, participated in study design and edited the manuscript. L.L.E. supervised the study, participated in the study design and participated in writing the manuscript.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btab501_Supplementary_Data</label>
      <media xlink:href="btab501_supplementary_data.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>The authors would like to acknowledge the National Institutes of Health (NIH) for providing the SPRINT trial dataset. The authors would also like to thank Maria Jaakkola and Sohrab Saraei for their thoughtful discussions. The results presented here are in part based upon data generated by the TCGA Research Network: <ext-link xlink:href="https://www.cancer.gov/tcga" ext-link-type="uri">https://www.cancer.gov/tcga</ext-link></p>
    <sec>
      <title>Funding</title>
      <p>Prof. Elo reports grants from the European Research Council ERC (677943), European Union's Horizon 2020 Research and Innovation Programme (675395), Academy of Finland (296801, 304995, 310561, 314443 and 329278) and Sigrid Juselius Foundation, during the conduct of the study. Dr. Venäläinen reports funding from the Academy of Finland (grant 322123). Our research is also supported by the University of Turku, Åbo Akademi University, Turku Graduate School (UTUGS), Biocenter Finland and ELIXIR Finland.</p>
      <p><italic toggle="yes">Conflict of Interest</italic>: The authors declare no competing interests.</p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btab501-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Apolloni</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <article-title>Two hybrid wrapper-filter feature selection algorithms applied to high-dimensional microarray experiments</article-title>. <source>Appl. Soft Comput</source>., <volume>38</volume>, <fpage>922</fpage>–<lpage>932</lpage>.</mixed-citation>
    </ref>
    <ref id="btab501-B3">
      <mixed-citation publication-type="other">Bioinformatics Pipeline: mRNA Analysis-GDC Docs, mRNA Analysis Pipeline, <ext-link xlink:href="https://docs.gdc.cancer.gov/Data/Bioinformatics_Pipelines/Expression_mRNA_Pipeline/#fpkm" ext-link-type="uri">https://docs.gdc.cancer.gov/Data/Bioinformatics_Pipelines/Expression_mRNA_Pipeline/#fpkm</ext-link> (17 May 2021, date last accessed).</mixed-citation>
    </ref>
    <ref id="btab501-B4">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bonnet</surname><given-names>A.</given-names></string-name>, <string-name><surname>Levy-Leduc</surname><given-names>C.</given-names></string-name></person-group> (<year>2015</year>) EstHer: estimation of heritability in high dimensional sparse linear mixed models using variable selection, version 1.0, <ext-link xlink:href="https://CRAN.R-project.org/package=EstHer" ext-link-type="uri">https://CRAN.R-project.org/package=EstHer</ext-link>.</mixed-citation>
    </ref>
    <ref id="btab501-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Braun</surname><given-names>R.</given-names></string-name></person-group> (<year>2014</year>) <article-title>Systems analysis of high-throughput data</article-title>. <source>Adv. Exp. Med. Biol</source>., <volume>844</volume>, <fpage>153</fpage>–<lpage>187</lpage>.<pub-id pub-id-type="pmid">25480641</pub-id></mixed-citation>
    </ref>
    <ref id="btab501-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buse</surname><given-names>J.B.</given-names></string-name></person-group> (<year>2007</year>) <article-title>Action to Control Cardiovascular Risk in Diabetes (ACCORD) Trial: design and methods</article-title>. <source>Am. J. Cardiol</source>., <volume>99</volume>, <fpage>S21</fpage>–<lpage>S33</lpage>.</mixed-citation>
    </ref>
    <ref id="btab501-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carvalho</surname><given-names>B.S.</given-names></string-name>, <string-name><surname>Irizarry</surname><given-names>R.A.</given-names></string-name></person-group> (<year>2010</year>) <article-title>A framework for oligonucleotide microarray preprocessing</article-title>. <source>Bioinformatics</source>, <volume>26</volume>, <fpage>2363</fpage>–<lpage>2367</lpage>.<pub-id pub-id-type="pmid">20688976</pub-id></mixed-citation>
    </ref>
    <ref id="btab501-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>DeLong</surname><given-names>E.R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>1988</year>) <article-title>Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach</article-title>. <source>Biometrics</source>, <volume>44</volume>, <fpage>837</fpage>–<lpage>845</lpage>.<pub-id pub-id-type="pmid">3203132</pub-id></mixed-citation>
    </ref>
    <ref id="btab501-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Desmedt</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal>; TRANSBIG Consortium. (<year>2007</year>) <article-title>Strong time dependence of the 76-gene prognostic signature for node-negative breast cancer patients in the TRANSBIG multicenter independent validation series</article-title>. <source>Clin. Cancer Res</source>., <volume>13</volume>, <fpage>3207</fpage>–<lpage>3214</lpage>.<pub-id pub-id-type="pmid">17545524</pub-id></mixed-citation>
    </ref>
    <ref id="btab501-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friedman</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2010</year>) <article-title>Regularization paths for generalized linear models via coordinate descent</article-title>. <source>J. Stat. Softw</source>., <volume>33</volume>, <fpage>1</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">20808728</pub-id></mixed-citation>
    </ref>
    <ref id="btab501-B11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Guo</surname><given-names>P.</given-names></string-name></person-group> (<year>2015</year>) elasso: Enhanced Least Absolute Shrinkage and Selection Operator Regression Model, version 1.1, <ext-link xlink:href="https://cran.r-project.org/package=elasso" ext-link-type="uri">https://cran.r-project.org/package=elasso</ext-link>.</mixed-citation>
    </ref>
    <ref id="btab501-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guyon</surname><given-names>I.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2002</year>) <article-title>Gene selection for cancer classification using support vector machines</article-title>. <source>Mach. Learn</source>., <volume>46</volume>, <fpage>389</fpage>–<lpage>422</lpage>.</mixed-citation>
    </ref>
    <ref id="btab501-B13">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Guyon</surname><given-names>I.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2005</year>) <part-title>Result analysis of the NIPS 2003 feature selection challenge</part-title>. In: <person-group person-group-type="editor"><string-name><surname>Saul</surname><given-names>L.</given-names></string-name></person-group>  <etal>et al</etal> (eds.), <source>Advances in Neural Information Processing Systems</source>. <publisher-name>MIT Press</publisher-name>, <publisher-loc>Cambridge, Massachusetts</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btab501-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>He</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Yu</surname><given-names>W.</given-names></string-name></person-group> (<year>2010</year>) <article-title>Stable feature selection for biomarker discovery</article-title>. <source>Comput. Biol. Chem</source>., <volume>34</volume>, <fpage>215</fpage>–<lpage>225</lpage>.<pub-id pub-id-type="pmid">20702140</pub-id></mixed-citation>
    </ref>
    <ref id="btab501-B15">
      <mixed-citation publication-type="other">HTSeq-FPKM-UQ-GDC Docs, <ext-link xlink:href="https://docs.gdc.cancer.gov/Encyclopedia/pages/HTSeq-FPKM-UQ/" ext-link-type="uri">https://docs.gdc.cancer.gov/Encyclopedia/pages/HTSeq-FPKM-UQ/</ext-link> (17 May 2021, date last accessed).</mixed-citation>
    </ref>
    <ref id="btab501-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huber</surname><given-names>W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2002</year>) <article-title>Variance stabilization applied to microarray data calibration and to the quantification of differential expression</article-title>. <source>Bioinformatics</source>, <volume>18</volume>, <fpage>S96</fpage>–<lpage>S104</lpage>.<pub-id pub-id-type="pmid">12169536</pub-id></mixed-citation>
    </ref>
    <ref id="btab501-B17">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Huber</surname><given-names>W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) vsn: variance stabilization and calibration for microarray data bioconductor version: Release (3.13).</mixed-citation>
    </ref>
    <ref id="btab501-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Klén</surname><given-names>R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Prediction of complication related death after radical cystectomy for bladder cancer with machine learning methodology</article-title>. <source>Scand. J. Urol</source>., <volume>53</volume>, <fpage>325</fpage>–<lpage>331</lpage>.<pub-id pub-id-type="pmid">31552774</pub-id></mixed-citation>
    </ref>
    <ref id="btab501-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Klén</surname><given-names>R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Predicting skeletal muscle and whole-body insulin sensitivity using NMR-metabolomic profiling</article-title>. <source>J. Endocr. Soc</source>., <volume>4</volume>, <fpage>bvaa026</fpage>.<pub-id pub-id-type="pmid">32232183</pub-id></mixed-citation>
    </ref>
    <ref id="btab501-B20">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Koller</surname><given-names>D.</given-names></string-name>, <string-name><surname>Sahami</surname><given-names>M.</given-names></string-name></person-group> (<year>1996</year>) <article-title>Toward optimal feature selection</article-title>. In: <source><italic toggle="yes">International Conference on Machine Learning, Morgan Kaufmann Publishers Inc., Bari Italy, July 3-6 1996</italic></source>, pp. <fpage>284</fpage>–<lpage>292</lpage>.</mixed-citation>
    </ref>
    <ref id="btab501-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kursa</surname><given-names>M.B.</given-names></string-name>, <string-name><surname>Rudnicki</surname><given-names>W.R.</given-names></string-name></person-group> (<year>2010</year>) <article-title>Feature selection with the Boruta Package</article-title>. <source>J. Stat. Softw</source>., <volume>36</volume>, <fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation>
    </ref>
    <ref id="btab501-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Labani</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>A novel multivariate filter method for feature selection in text classification problems</article-title>. <source>Eng. Appl. Artif. Intell</source>., <volume>70</volume>, <fpage>25</fpage>–<lpage>37</lpage>.</mixed-citation>
    </ref>
    <ref id="btab501-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) <article-title>Variable selection in regression with compositional covariates</article-title>. <source>Biometrika</source>, <volume>101</volume>, <fpage>785</fpage>–<lpage>797</lpage>.</mixed-citation>
    </ref>
    <ref id="btab501-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lu</surname><given-names>H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>A hybrid feature selection algorithm for gene expression data classification</article-title>. <source>Neurocomputing</source>, <volume>256</volume>, <fpage>56</fpage>–<lpage>62</lpage>.</mixed-citation>
    </ref>
    <ref id="btab501-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mahendran</surname><given-names>N.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Machine learning based computational gene selection models: a survey, performance evaluation, open issues, and future research directions</article-title>. <source>Front. Genet</source>., <volume>11</volume>, <fpage>603808</fpage>.<pub-id pub-id-type="pmid">33362861</pub-id></mixed-citation>
    </ref>
    <ref id="btab501-B26">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Nan</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) glmvsd: variable selection deviation measures and instability tests for high-dimensional generalized linear models, version 1.4, <ext-link xlink:href="https://cran.r-project.org/package=glmvsd" ext-link-type="uri">https://cran.r-project.org/package=glmvsd</ext-link>.</mixed-citation>
    </ref>
    <ref id="btab501-B27">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Perrot-Dockès</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) A multivariate variable selection approach for analyzing LC-MS, <italic toggle="yes">arXiv:1704.00076 [stat].</italic></mixed-citation>
    </ref>
    <ref id="btab501-B28">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Perrot-Dockès</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) MultiVarSel: variable selection in a multivariate linear model, version 1.1.3, <ext-link xlink:href="https://CRAN.R-project.org/package=MultiVarSel" ext-link-type="uri">https://CRAN.R-project.org/package=MultiVarSel</ext-link>.</mixed-citation>
    </ref>
    <ref id="btab501-B29">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Reggiani</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <part-title>Feature selection in high-dimensional dataset using MapReduce</part-title>. In: <person-group person-group-type="editor"><string-name><surname>Verheij</surname><given-names>B.</given-names></string-name>, <string-name><surname>Wiering</surname><given-names>M.</given-names></string-name></person-group> (eds). <source>Artificial Intelligence. BNAIC 2017. Communications in Computer and Information Science</source>, Vol. <volume>823</volume>. <publisher-name>Springer International Publishing</publisher-name>, <publisher-loc>Cham</publisher-loc>, pp. <fpage>101</fpage>–<lpage>115</lpage>.</mixed-citation>
    </ref>
    <ref id="btab501-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Roberts</surname><given-names>S.</given-names></string-name>, <string-name><surname>Nowak</surname><given-names>G.</given-names></string-name></person-group> (<year>2014</year>) <article-title>Stabilizing the lasso against cross-validation variability</article-title>. <source>Comput. Stat. Data Anal</source>., <volume>70</volume>, <fpage>198</fpage>–<lpage>211</lpage>.</mixed-citation>
    </ref>
    <ref id="btab501-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Robin</surname><given-names>X.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2011</year>) <article-title>pROC: an open-source package for R and S+ to analyze and compare ROC curves</article-title>. <source>BMC Bioinform</source>., <volume>12</volume>, <fpage>77</fpage>.</mixed-citation>
    </ref>
    <ref id="btab501-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saeys</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2007</year>) <article-title>A review of feature selection techniques in bioinformatics</article-title>. <source>Bioinformatics</source>, <volume>23</volume>, <fpage>2507</fpage>–<lpage>2517</lpage>.<pub-id pub-id-type="pmid">17720704</pub-id></mixed-citation>
    </ref>
    <ref id="btab501-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shahriyari</surname><given-names>L.</given-names></string-name></person-group> (<year>2019</year>) <article-title>Effect of normalization methods on the performance of supervised learning algorithms applied to HTSeq-FPKM-UQ data sets: 7SK RNA expression as a predictor of survival in patients with colon adenocarcinoma</article-title>. <source>Brief. Bioinform</source>., <volume>20</volume>, <fpage>985</fpage>–<lpage>994</lpage>.<pub-id pub-id-type="pmid">29112707</pub-id></mixed-citation>
    </ref>
    <ref id="btab501-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Simon</surname><given-names>N.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2011</year>) <article-title>Regularization paths for Cox’s proportional hazards model via coordinate descent</article-title>. <source>J. Stat. Softw</source>., <volume>39</volume>, <fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation>
    </ref>
    <ref id="btab501-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tibshirani</surname><given-names>R.</given-names></string-name></person-group> (<year>1996</year>) <article-title>Regression shrinkage and selection via the lasso</article-title>. <source>J. R. Stat. Soc. Ser. B</source>, <volume>58</volume>, <fpage>267</fpage>–<lpage>288</lpage>.</mixed-citation>
    </ref>
    <ref id="btab501-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Venäläinen</surname><given-names>M.S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Easy-to-use tool for evaluating the elevated acute kidney injury risk against reduced cardiovascular disease risk during intensive blood pressure control</article-title>. <source>J. Hypertens</source>., <volume>38</volume>, <fpage>511</fpage>–<lpage>518</lpage>.<pub-id pub-id-type="pmid">31977572</pub-id></mixed-citation>
    </ref>
    <ref id="btab501-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Venäläinen</surname><given-names>M.S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>Preoperative risk prediction models for short-term revision and death after total hip arthroplasty: data from the Finnish Arthroplasty Register</article-title>. <source>JB JS Open Access</source>, <volume>6</volume>, <fpage>e20.00091</fpage>.</mixed-citation>
    </ref>
    <ref id="btab501-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>L.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <article-title>Feature selection methods for big data bioinformatics: a survey from the search perspective</article-title>. <source>Methods</source>, <volume>111</volume>, <fpage>21</fpage>–<lpage>31</lpage>.<pub-id pub-id-type="pmid">27592382</pub-id></mixed-citation>
    </ref>
    <ref id="btab501-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2005</year>) <article-title>Gene-expression profiles to predict distant metastasis of lymph-node-negative primary breast cancer</article-title>. <source>Lancet</source>, <volume>365</volume>, <fpage>671</fpage>–<lpage>679</lpage>.<pub-id pub-id-type="pmid">15721472</pub-id></mixed-citation>
    </ref>
    <ref id="btab501-B40">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Wehrens</surname><given-names>R.</given-names></string-name>, <string-name><surname>Franceschi</surname><given-names>P.</given-names></string-name></person-group> (<year>2015</year>) BioMark: find biomarkers in two-class discrimination problems.</mixed-citation>
    </ref>
    <ref id="btab501-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wei</surname><given-names>G.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>A novel hybrid feature selection method based on dynamic feature importance</article-title>. <source>Appl. Soft Comput</source>., <volume>93</volume>, <fpage>106337</fpage>.</mixed-citation>
    </ref>
    <ref id="btab501-B42">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>van de Wiel</surname><given-names>M.A.</given-names></string-name>, <string-name><surname>Novianti</surname><given-names>P.W.</given-names></string-name></person-group> (<year>2020</year>) GRridge: better prediction by use of co-data: adaptive group-regularized ridge regression, version 1.16.0, doi:10.18129/B9.bioc.GRridge.</mixed-citation>
    </ref>
    <ref id="btab501-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wright</surname><given-names>J.T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) <article-title>A randomized trial of intensive versus standard blood-pressure control</article-title>. <source>N. Engl. J. Med</source>., <volume>373</volume>, <fpage>2103</fpage>–<lpage>2116</lpage>.<pub-id pub-id-type="pmid">26551272</pub-id></mixed-citation>
    </ref>
    <ref id="btab501-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname><given-names>C.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>J.</given-names></string-name></person-group> (<year>2014</year>) <article-title>The sparse MLE for ultrahigh-dimensional feature screening</article-title>. <source>J. Am. Stat. Assoc</source>., <volume>109</volume>, <fpage>1257</fpage>–<lpage>1269</lpage>.<pub-id pub-id-type="pmid">25382886</pub-id></mixed-citation>
    </ref>
    <ref id="btab501-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname><given-names>L.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>H.</given-names></string-name></person-group> (<year>2003</year>) <article-title>Feature selection for high-dimensional data: a fast correlation-based filter solution</article-title>. In: <source>Proceedings, Twentieth International Conference on Machine Learning</source><italic toggle="yes">, The AAAI Press, Menlo Park</italic>, California, Washington, DC, August 21–24, 2003. pp. <fpage>856</fpage>–<lpage>863</lpage>.</mixed-citation>
    </ref>
    <ref id="btab501-B46">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zang</surname><given-names>Q.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) SMLE: joint feature screening via sparse MLE, version 1.1.1, <ext-link xlink:href="https://CRAN.R-project.org/package=SMLE" ext-link-type="uri">https://CRAN.R-project.org/package=SMLE</ext-link>.</mixed-citation>
    </ref>
    <ref id="btab501-B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zou</surname><given-names>H.</given-names></string-name>, <string-name><surname>Hastie</surname><given-names>T.</given-names></string-name></person-group> (<year>2005</year>) <article-title>Regularization and variable selection via the elastic net</article-title>. <source>J. R. Stati. Soc. Ser. B</source>, <volume>67</volume>, <fpage>301</fpage>–<lpage>320</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
