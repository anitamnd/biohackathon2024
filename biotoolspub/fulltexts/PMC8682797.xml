<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nucleic Acids Res</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nucleic Acids Res</journal-id>
    <journal-id journal-id-type="publisher-id">nar</journal-id>
    <journal-title-group>
      <journal-title>Nucleic Acids Research</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0305-1048</issn>
    <issn pub-type="epub">1362-4962</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8682797</article-id>
    <article-id pub-id-type="pmid">34581805</article-id>
    <article-id pub-id-type="doi">10.1093/nar/gkab829</article-id>
    <article-id pub-id-type="publisher-id">gkab829</article-id>
    <article-categories>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00010</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>Narese/8</subject>
        <subject>Narese/24</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Methods Online</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>BioSeq-BLM: a platform for analyzing DNA, RNA and protein sequences based on biological language models</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Hong-Liang</given-names>
        </name>
        <aff><institution>School of Computer Science and Technology, Beijing Institute of Technology</institution>, <addr-line>Beijing</addr-line>, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pang</surname>
          <given-names>Yi-He</given-names>
        </name>
        <aff><institution>School of Computer Science and Technology, Beijing Institute of Technology</institution>, <addr-line>Beijing</addr-line>, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-3685-9469</contrib-id>
        <name>
          <surname>Liu</surname>
          <given-names>Bin</given-names>
        </name>
        <!--bliu@bliulab.net-->
        <aff><institution>School of Computer Science and Technology, Beijing Institute of Technology</institution>, <addr-line>Beijing</addr-line>, <country country="CN">China</country></aff>
        <aff><institution>Advanced Research Institute of Multidisciplinary Science, Beijing Institute of Technology</institution>, <addr-line>Beijing</addr-line>, <country country="CN">China</country></aff>
        <xref rid="COR1" ref-type="corresp"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="COR1">To whom correspondence should be addressed. Tel: +86 10 68911310; Email: <email>bliu@bliulab.net</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>16</day>
      <month>12</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2021-09-28">
      <day>28</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>28</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <volume>49</volume>
    <issue>22</issue>
    <fpage>e129</fpage>
    <lpage>e129</lpage>
    <history>
      <date date-type="accepted">
        <day>09</day>
        <month>9</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>24</day>
        <month>8</month>
        <year>2021</year>
      </date>
      <date date-type="received">
        <day>18</day>
        <month>7</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021. Published by Oxford University Press on behalf of Nucleic Acids Research.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact <email>journals.permissions@oup.com</email></license-p>
      </license>
    </permissions>
    <self-uri xlink:href="gkab829.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>In order to uncover the meanings of ‘book of life’, 155 different biological language models (BLMs) for DNA, RNA and protein sequence analysis are discussed in this study, which are able to extract the linguistic properties of ‘book of life’. We also extend the BLMs into a system called BioSeq-BLM for automatically representing and analyzing the sequence data. Experimental results show that the predictors generated by BioSeq-BLM achieve comparable or even obviously better performance than the exiting state-of-the-art predictors published in literatures, indicating that BioSeq-BLM will provide new approaches for biological sequence analysis based on natural language processing technologies, and contribute to the development of this very important field. In order to help the readers to use BioSeq-BLM for their own experiments, the corresponding web server and stand-alone package are established and released, which can be freely accessed at <ext-link xlink:href="http://bliulab.net/BioSeq-BLM/" ext-link-type="uri">http://bliulab.net/BioSeq-BLM/</ext-link>.</p>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Key Research and Development Program of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100012166</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>2018AAA0100100</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>61822306</award-id>
        <award-id>61861146002</award-id>
        <award-id>61732012</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Beijing Natural Science Foundation</institution>
            <institution-id institution-id-type="DOI">10.13039/501100004826</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>JQ19019</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="17"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="SEC1">
    <title>INTRODUCTION</title>
    <p>The genome is the ‘book of life’, whose languages are the biological sequences (<xref rid="B1" ref-type="bibr">1</xref>). Natural languages and biological sequences are similar. For examples, the peptide bonds connect the amino acid residues to form a protein with certain structure and function. Similarly, words are combined by grammar and linguistic rules into a sentence with certain meanings. In this regard, the techniques grounded in linguistics are used to uncover the meanings of the ‘book of life’, and have greatly contributed to the development of biological sequence analysis. Protein domains can be considered as the words of proteins, and the rules for domain associations are the grammar of proteins (<xref rid="B2" ref-type="bibr">2</xref>). Inspired by these similarities between proteins and languages, the linguistic technique <italic toggle="yes">n</italic>-gram was employed to probe the proteome grammar, showing that a ‘quasi-universal grammar’ underlies the evolution of domain architectures (<xref rid="B3" ref-type="bibr">3</xref>). Biological sequences store all the information determining their structures and functions, and the sentences contain all the information defining their syntactic and semantic (<xref rid="B4" ref-type="bibr">4</xref>). Because the relationships among biological sequence, structure and function are similar as the relationships among sentence, syntactic and semantic in linguistics (see Figure <xref rid="F1" ref-type="fig">1</xref>), techniques for semantic analysis derived from natural language processing have been applied to predict the structures and functions of proteins (<xref rid="B5" ref-type="bibr">5</xref>), providing new ideas and approaches for solving these tasks.</p>
    <fig position="float" id="F1">
      <label>Figure 1.</label>
      <caption>
        <p>The similarities between protein sequence and natural language sentence.</p>
      </caption>
      <graphic xlink:href="gkab829fig1" position="float"/>
    </fig>
    <p>All these approaches based on natural language processing are playing important roles in uncovering the meanings of the ‘book of life’. Unfortunately, we still know only a little about its semantic. The existing studies focus on exploring the lexical, syntactic, or semantic of biological sequences. The biological sequences with various structures and functions share some common features with natural languages, but they also have their own linguistic properties. For examples, there are &gt;500 physiochemical properties for amino acids (<xref rid="B6" ref-type="bibr">6</xref>), and &gt;180 physiochemical properties for nucleotides (<xref rid="B7" ref-type="bibr">7</xref>). Even the most complicated polysemous word in a language will never have so many properties. As a result, the rule-based approaches show limited performance for some difficult tasks, such as protein disordered region prediction, enhancer identification, etc. Furthermore, these methods highly depend on the experience-based linguistic features. Therefore, models which are able to automatically and systematically capture the linguistic features are highly desired. They are critical for promoting the development of biological sequence analysis based on natural language processing. Language models can systematically and comprehensively represent and analyze the sentences, independent from the rule-based features, significantly contributing to the development of the natural language processing (<xref rid="B8" ref-type="bibr">8</xref>). Inspired by their successes, we are to propose the biological language models (BLMs) for DNA, RNA and protein sequences. Because the deep learning techniques have been demonstrated to be key methods in bioinformatics, such as protein structure prediction (<xref rid="B9" ref-type="bibr">9</xref>), and function analysis (<xref rid="B10" ref-type="bibr">10</xref>), BLMs mainly focuses on the biological neural language models to represent and analyze biological sequences based on deep learning techniques. We extend the BLMs to an automatic system called BioSeq-BLM (<ext-link xlink:href="http://bliulab.net/BioSeq-BLM" ext-link-type="uri">http://bliulab.net/BioSeq-BLM</ext-link>). Given the sequence data for a specific sequence analysis task, BioSeq-BLM will automatically construct the BLM, select the predictor, evaluate the performance, and analyze the results. BioSeq-BLM is particularly useful for solving the problems of extracting the linguistic features and designing the techniques derived from natural language processing, providing a new view to explore the meanings of ‘book of life’. It is anticipated that BioSeq-BLM will be a useful tool for biological sequence analysis, computational proteomics and genomics. As discussed in previous studies (<xref rid="B11" ref-type="bibr">11–13</xref>), a system which is able to automatically analyze the biological sequence data is highly desired, and several software tools have been established, such as BioSeq-Analysis (<xref rid="B11" ref-type="bibr">11</xref>), BioSeq-Analysis2.0 (<xref rid="B12" ref-type="bibr">12</xref>), protr (<xref rid="B14" ref-type="bibr">14</xref>), Rcpi (<xref rid="B15" ref-type="bibr">15</xref>), Kipoi (<xref rid="B16" ref-type="bibr">16</xref>), Janggu (<xref rid="B17" ref-type="bibr">17</xref>), Selene (<xref rid="B18" ref-type="bibr">18</xref>) and Pydna (<xref rid="B19" ref-type="bibr">19</xref>). However, among these existing tools, only BioSeq-BLM and BioSeq-Analysis2.0 are able to automatically construct the predictors with the benchmark datasets as the inputs. The other five tools focus on the individual steps, such as feature extraction, machine learning algorithm selection, or performance evaluation. The fundamental difference between BioSeq-BLM and other similar tools such as BioSeq-Analysis (<xref rid="B12" ref-type="bibr">12</xref>) is that BioSeq-BLM is the first study to define the BLMs and introduce 155 different BLMs for biological sequence analysis. Although some features or machine learning techniques also exist in BioSeq-Analysis2.0, BioSeq-BLM is the only existing tool for biological sequence analysis based on biological language models, providing new concepts and techniques for this very important field. These contributions make BioSeq-BLM unique, and more powerful than BioSeq-Analysis2.0. The comparisons between BioSeq-BLM and BioSeq-Analysis2.0 were listed in Table <xref rid="tbl1" ref-type="table">1</xref>, from which we can see that BioSeq-BLM is beyond the reach of BioSeq-Analysis2.0 and any other similar tools.</p>
    <table-wrap position="float" id="tbl1">
      <label>Table 1.</label>
      <caption>
        <p>The differences between BioSeq-BLM and BioSeq-Analysis2.0</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th rowspan="1" colspan="1">Modules</th>
            <th rowspan="1" colspan="1">Descriptions</th>
            <th rowspan="1" colspan="1">BioSeq-BLM</th>
            <th rowspan="1" colspan="1">BioSeq-Analysis2.0</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="1" colspan="1">BLMs</td>
            <td rowspan="1" colspan="1">Number of BGLMs</td>
            <td rowspan="1" colspan="1">58</td>
            <td rowspan="1" colspan="1">51</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1">Number of BSLMs</td>
            <td rowspan="1" colspan="1">48</td>
            <td rowspan="1" colspan="1">0</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1">Number of BNLMs</td>
            <td rowspan="1" colspan="1">41</td>
            <td rowspan="1" colspan="1">0</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1">Number of BSSLMs</td>
            <td rowspan="1" colspan="1">8</td>
            <td rowspan="1" colspan="1">0</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Predictor construction</td>
            <td rowspan="1" colspan="1">Number of machine learning algorithms</td>
            <td rowspan="1" colspan="1">9</td>
            <td rowspan="1" colspan="1">3</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1">Number of deep learning algorithms</td>
            <td rowspan="1" colspan="1">6</td>
            <td rowspan="1" colspan="1">0</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Performance evaluation</td>
            <td rowspan="1" colspan="1">Number of evaluation metrics</td>
            <td rowspan="1" colspan="1">10</td>
            <td rowspan="1" colspan="1">6</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Result analysis</td>
            <td rowspan="1" colspan="1">Number of methods for normalization</td>
            <td rowspan="1" colspan="1">4</td>
            <td rowspan="1" colspan="1">0</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1">Number of methods for clustering</td>
            <td rowspan="1" colspan="1">5</td>
            <td rowspan="1" colspan="1">0</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1">Number of algorithms for feature selection</td>
            <td rowspan="1" colspan="1">5</td>
            <td rowspan="1" colspan="1">0</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1">Number of models for dimension reduction</td>
            <td rowspan="1" colspan="1">3</td>
            <td rowspan="1" colspan="1">0</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Other</td>
            <td rowspan="1" colspan="1">Support GPU-accelerate or not</td>
            <td rowspan="1" colspan="1">Yes</td>
            <td rowspan="1" colspan="1">No</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>Our main contributions are as follows:</p>
    <list list-type="bullet">
      <list-item>
        <p>Based on the similarities between natural languages and biological sequences, we introduce the biological language models (BLMs) motivated by language models (LMs) in the field of natural language processing.</p>
      </list-item>
      <list-item>
        <p>We extend the BLMs into a platform called BioSeq-BLM, only requiring the benchmark datasets as inputs. The predictor will be automatically constructed and evaluated with the help of BioSeq-BLM. BioSeq-BLM is freely available at <ext-link xlink:href="http://bliulab.net/BioSeq-BLM/" ext-link-type="uri">http://bliulab.net/BioSeq-BLM/</ext-link>.</p>
      </list-item>
      <list-item>
        <p>Experimental results showed that the predictors constructed by BioSeq-BLM are able to improve the predictive performance for some biological sequence analysis tasks.</p>
      </list-item>
    </list>
  </sec>
  <sec sec-type="materials|methods" id="SEC2">
    <title>MATERIALS AND METHODS</title>
    <sec id="SEC2-1">
      <title>Biological sequence analysis tasks</title>
      <p>The aim of biological sequence analysis is to computationally analyze the sequences of DNA, RNA and proteins so as to identify their structures, functions and their associations with diseases. Given a benchmark dataset <bold>S</bold> for a specific biological sequence analysis task with <italic toggle="yes">N</italic> sequences:<disp-formula id="M1"><label>(1)</label><tex-math id="M0001" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}{{\bf S}}\ = \{ {{\rm{B}}_1},{\rm{\ }}{{\rm{B}}_{2,{\rm{\ }}}}\ {{\rm{B}}_{3,{\rm{\ }}}}{{\rm{B}}_{4,{\rm{\ }}}} \ldots ,{\rm{\ }}{{\rm{B}}_i},{\rm{\ }} \ldots ,{\rm{\ }}{{\rm{B}}_N}\} \end{equation*}$$\end{document}</tex-math></disp-formula>where B<italic toggle="yes"><sub>i</sub></italic> is the <italic toggle="yes">i</italic>-th biological sequence in <bold>S</bold> represented as:<disp-formula id="M2"><label>(2)</label><tex-math id="M0001a" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}\ {{\rm{B}}_i} = \ {\rm{R}}_1^i{\rm{R}}_2^i{\rm{R}}_3^i{\rm{R}}_4^i \ldots {\rm{\ R}}_j^i \ldots {\rm{\ R}}_M^i\end{equation*}$$\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M0002" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\rm{R}}_j^i$\end{document}</tex-math></inline-formula> represents the <italic toggle="yes">j</italic>-th word (the words of biological sequences will be introduced in the following sections) in <inline-formula><tex-math id="M0003" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${{\rm{B}}_i}$\end{document}</tex-math></inline-formula>.</p>
      <p>Biological sequence analysis tasks can be mainly divided into residue-level analysis and sequence-level analysis (<xref rid="B12" ref-type="bibr">12</xref>), aiming to identify the properties of each residue (<inline-formula><tex-math id="M0004" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\rm{R}}_j^i$\end{document}</tex-math></inline-formula>) and the whole sequence (<inline-formula><tex-math id="M0005" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${{\rm{B}}_i}$\end{document}</tex-math></inline-formula>), respectively. Their main difference is that the residue-level analysis treats each residue as a sample, while sequence-level analysis treats each biological sequence as a sample. For more information, please refer to (<xref rid="B12" ref-type="bibr">12</xref>). The BLMs will play key roles in these tasks, which will be introduced in the following sections.</p>
    </sec>
    <sec id="SEC2-2">
      <title>Biological language models</title>
      <p>The language model (LM) creates a statistical model for English sentences based on the Markov processes (<xref rid="B20" ref-type="bibr">20</xref>), which is a milestone in the field of natural language processing. LM determines the joint probability of a word sequence (<xref rid="B21" ref-type="bibr">21</xref>) so as to accurately represent and analyze the sentences. In this study, we are to propose the biological language models (BLMs) to represent and analyze the biological sequences following the ideas of LM. A BLM can be constructed for a specific task based on the corresponding benchmark dataset <bold>S</bold> (cf. Equation <xref rid="M1" ref-type="disp-formula">1</xref>), represented as:<disp-formula id="M3"><label>(3)</label><tex-math id="M0006" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*} BLM\ \left( {{{\rm{B}}_i}} \right) &amp;=&amp; \ BLM\ \left( {{{\bf V}}_1^i{{\bf V}}_2^i{{\bf V}}_3^i{{\bf V}}_4^i \ldots {{\bf V}}_j^i \ldots {{\bf V}}_M^i} \right) \nonumber\\ &amp;=&amp; \ BLM\left( {{{\bf V}}_1^i|{{\bf V}}_0^i} \right) \times {\rm{BLM}}\left( {{{\bf V}}_2^i|{{\bf V}}_0^i{{\bf V}}_1^i} \right) \nonumber\\ &amp;&amp; \times \ldots \times BLM\ \left( {{{\bf V}}_M^i|{{\bf V}}_0^iV_1^i{{\bf V}}_2^i \ldots {{\bf V}}_{M - 1}^i} \right) \nonumber\\ &amp;=&amp; \mathop \prod \limits_{j = 1}^M BLM\left( {{{\bf V}}_j^i|{{\bf V}}_0^i{{\bf V}}_1^i{{\bf V}}_2^i \ldots {{\bf V}}_{j - 1}^i} \right) \end{eqnarray*}$$\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M0007" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${{\bf V}}_j^i$\end{document}</tex-math></inline-formula> is feature vector of the word <inline-formula><tex-math id="M0008" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\rm{R}}_j^i$\end{document}</tex-math></inline-formula> in the sequence <inline-formula><tex-math id="M0009" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${{\rm{B}}_i}$\end{document}</tex-math></inline-formula> (cf. Equation <xref rid="M2" ref-type="disp-formula">2</xref>), represented as:<disp-formula id="M4"><label>(4)</label><tex-math id="M00010" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}{{\bf V}}_j^i = \ {{\bf \Phi }}\left( {{\rm{R}}_j^i} \right) + \ {{\bf \Psi }}\left( {{\rm{R}}_j^i} \right)\end{equation*}$$\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M00011" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${{\bf \Phi }}( {{\rm{R}}_j^i} )$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M00012" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${{\bf \Psi }}( {{\rm{R}}_j^i} )$\end{document}</tex-math></inline-formula> are the linguistics attributes and biological attributes (such as physiochemical properties, etc) for <inline-formula><tex-math id="M00013" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\rm{R}}_j^i$\end{document}</tex-math></inline-formula>, respectively.</p>
      <p>Inspired by the success of language models (LMs) in the field of natural language processing, we introduce the biological language models (BLMs) for biological sequence analysis. The main differences between LM and BLM are as follows:</p>
      <list list-type="bullet">
        <list-item>
          <p>Different inputs and methods. The inputs of LMs are sentences, while the inputs of BLMs are biological sequences. Furthermore, the words and word segmentation methods of BLMs are more diverse than those of LMs.</p>
        </list-item>
        <list-item>
          <p>More information in BLMs. BLMs not only consider the linguistic attributes of biological sequences, but also include the biological attributes, such as physical and chemical properties, evolutionary information, motifs, etc.</p>
        </list-item>
      </list>
      <p>The proposed BLMs are able to capture both the linguistic features and biological properties, which can be further divided into four categories according to different computational techniques and theories, including biological grammar language models (BGLMs), biological statistical language models (BSLMs), biological neural language models (BNLMs), and biological semantic similarity language models (BSSLMs). These BLMs represent the biological sequences based on different techniques and theories, and are playing complementary roles in biological sequence analysis. These BLMs will be introduced in the following sections.</p>
      <sec id="SEC2-2-1">
        <title>Biological grammar language models (BGLMs)</title>
        <p>Natural languages present the meanings of their utterances structured according to their syntax, knowing as compositional semantics (<xref rid="B22" ref-type="bibr">22</xref>). In natural language processing, the grammar language models formally implement natural language understanding and generation based on grammar rules and linguistic knowledge. The biological sequences also have their own grammar rules, such as the motif associations (<xref rid="B23" ref-type="bibr">23</xref>), word relationships (<xref rid="B24" ref-type="bibr">24</xref>), word properties (<xref rid="B6" ref-type="bibr">6</xref>), etc. These grammar rules of biological sequences are important for insightfully representing the sequence characteristics. In this regard, 58 biological grammar language models (BGLMs) are used to represent and analyze the biological sequences. Among these 58 BGLMs, there are 29 models based on the syntax rules (see Table <xref rid="tbl2" ref-type="table">2</xref>). Because the syntax rules reflect the relationships among residues along the biological sequences, these models are particularly useful for analyzing the structures and functions of biological sequences, such as protein disordered region prediction (<xref rid="B25" ref-type="bibr">25</xref>), splice site prediction (<xref rid="B26" ref-type="bibr">26</xref>), etc. Similar as sentences, biological sequences have their own words with more diverse properties reflecting evolutionary information, physicochemical values, structure information, etc. In order to incorporate the word properties into BGLMs, the other 29 BGLMs are based on word properties (<xref rid="B12" ref-type="bibr">12</xref>) (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S9</xref>). Because these BGLMs based on word properties are able to capture the physicochemical properties of residues and the evolutionary information of biological sequences, they are suitable for analyzing the residue properties, sequence properties, and the evolutionary relationships, such as protein remote homology detection (<xref rid="B27" ref-type="bibr">27</xref>), N6-Methyladenosine Sites (<xref rid="B28" ref-type="bibr">28</xref>), etc.</p>
        <table-wrap position="float" id="tbl2">
          <label>Table 2.</label>
          <caption>
            <p>29 BGLMs based on syntax rules</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Category</th>
                <th rowspan="1" colspan="1">Model</th>
                <th rowspan="1" colspan="1">Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">DNA</td>
                <td rowspan="1" colspan="1">DAC</td>
                <td rowspan="1" colspan="1">Dinucleotide-based auto covariance (<xref rid="B24" ref-type="bibr">24</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">DCC</td>
                <td rowspan="1" colspan="1">Dinucleotide-based cross covariance (<xref rid="B24" ref-type="bibr">24</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">DACC</td>
                <td rowspan="1" colspan="1">Dinucleotide-based auto-cross covariance (<xref rid="B24" ref-type="bibr">24</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">TAC</td>
                <td rowspan="1" colspan="1">Trinucleotide-based auto covariance (<xref rid="B24" ref-type="bibr">24</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">TCC</td>
                <td rowspan="1" colspan="1">Trinucleotide-based cross covariance (<xref rid="B24" ref-type="bibr">24</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">TACC</td>
                <td rowspan="1" colspan="1">Trinucleotide-based auto-cross covariance (<xref rid="B24" ref-type="bibr">24</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">MAC</td>
                <td rowspan="1" colspan="1">Moran autocorrelation (<xref rid="B104" ref-type="bibr">104</xref>,<xref rid="B105" ref-type="bibr">105</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">GAC</td>
                <td rowspan="1" colspan="1">Geary autocorrelation (<xref rid="B104" ref-type="bibr">104</xref>,<xref rid="B106" ref-type="bibr">106</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">NMBAC</td>
                <td rowspan="1" colspan="1">Normalized Moreau-Broto autocorrelation (<xref rid="B104" ref-type="bibr">104</xref>,<xref rid="B107" ref-type="bibr">107</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ZCPseKNC</td>
                <td rowspan="1" colspan="1">Z curve pseudo k tuple nucleotide composition (<xref rid="B108" ref-type="bibr">108</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ND</td>
                <td rowspan="1" colspan="1">Nucleotide Density (<xref rid="B26" ref-type="bibr">26</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">RNA</td>
                <td rowspan="1" colspan="1">DAC</td>
                <td rowspan="1" colspan="1">Dinucleotide-based auto covariance (<xref rid="B24" ref-type="bibr">24</xref>,<xref rid="B47" ref-type="bibr">47</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">DCC</td>
                <td rowspan="1" colspan="1">Dinucleotide-based cross covariance (<xref rid="B24" ref-type="bibr">24</xref>,<xref rid="B47" ref-type="bibr">47</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">DACC</td>
                <td rowspan="1" colspan="1">Dinucleotide-based auto-cross covariance (<xref rid="B24" ref-type="bibr">24</xref>,<xref rid="B47" ref-type="bibr">47</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">MAC</td>
                <td rowspan="1" colspan="1">Moran autocorrelation (<xref rid="B104" ref-type="bibr">104</xref>,<xref rid="B105" ref-type="bibr">105</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">GAC</td>
                <td rowspan="1" colspan="1">Geary autocorrelation (<xref rid="B104" ref-type="bibr">104</xref>,<xref rid="B106" ref-type="bibr">106</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">NMBAC</td>
                <td rowspan="1" colspan="1">Normalized Moreau-Broto autocorrelation (<xref rid="B104" ref-type="bibr">104</xref>,<xref rid="B107" ref-type="bibr">107</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ND</td>
                <td rowspan="1" colspan="1">Nucleotide Density (<xref rid="B26" ref-type="bibr">26</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Protein</td>
                <td rowspan="1" colspan="1">AC</td>
                <td rowspan="1" colspan="1">Auto covariance (<xref rid="B24" ref-type="bibr">24</xref>,<xref rid="B47" ref-type="bibr">47</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">CC</td>
                <td rowspan="1" colspan="1">Cross covariance (<xref rid="B24" ref-type="bibr">24</xref>,<xref rid="B47" ref-type="bibr">47</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ACC</td>
                <td rowspan="1" colspan="1">Auto-cross covariance (<xref rid="B24" ref-type="bibr">24</xref>,<xref rid="B47" ref-type="bibr">47</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">PDT</td>
                <td rowspan="1" colspan="1">Physicochemical distance transformation (<xref rid="B27" ref-type="bibr">27</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">PDT-Profile</td>
                <td rowspan="1" colspan="1">Profile-based physicochemical distance transformation (<xref rid="B27" ref-type="bibr">27</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">AC-PSSM</td>
                <td rowspan="1" colspan="1">Profile-based Auto covariance (<xref rid="B24" ref-type="bibr">24</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">CC-PSSM</td>
                <td rowspan="1" colspan="1">Profile-based Cross covariance (<xref rid="B24" ref-type="bibr">24</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ACC-PSSM</td>
                <td rowspan="1" colspan="1">Profile-based Auto-cross covariance (<xref rid="B24" ref-type="bibr">24</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">PSSM-DT</td>
                <td rowspan="1" colspan="1">PSSM distance transformation (<xref rid="B27" ref-type="bibr">27</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">PSSM-RT</td>
                <td rowspan="1" colspan="1">PSSM relation transformation (<xref rid="B109" ref-type="bibr">109</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Motif-PSSM</td>
                <td rowspan="1" colspan="1">Use PSSM as input and extract features by motifs-based CNN (<xref rid="B23" ref-type="bibr">23</xref>)</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec id="SEC2-2-2">
        <title>Biological statistical language models (BSLMs)</title>
        <p>In linguistics, the statistical language models (SLMs) reflect statistical rules of languages by using the distribution functions based on the statistical principles (<xref rid="B29" ref-type="bibr">29</xref>). As a result, the underlying intentions and topics of languages can be discovered. Inspired by SLMs, the biological statistical language models (BSLMs) are introduced to recognize the statistical rules of biological sequences based on bag-of-words (BOW) (see Table <xref rid="tbl3" ref-type="table">3</xref>), term frequency–inverse document frequency (TF-IDF) (<xref rid="B30" ref-type="bibr">30</xref>) (see Table <xref rid="tbl4" ref-type="table">4</xref>), TextRank (<xref rid="B31" ref-type="bibr">31</xref>) (see Table <xref rid="tbl5" ref-type="table">5</xref>), and topic models (<xref rid="B32" ref-type="bibr">32</xref>) (see Table <xref rid="tbl6" ref-type="table">6</xref>). In this study, Kmer (<xref rid="B33" ref-type="bibr">33</xref>), RevKmer (<xref rid="B33" ref-type="bibr">33–35</xref>), Mismatch (<xref rid="B36" ref-type="bibr">36–38</xref>) and Subsequence (<xref rid="B36" ref-type="bibr">36</xref>,<xref rid="B38" ref-type="bibr">38</xref>,<xref rid="B39" ref-type="bibr">39</xref>) are treated as the words of DNA. Particularly, RevKmer is able to capture the characteristics of two strands of the double helix of DNA sequences. Kmer (<xref rid="B40" ref-type="bibr">40</xref>), Mismatch (<xref rid="B36" ref-type="bibr">36–38</xref>) and Subsequence (<xref rid="B36" ref-type="bibr">36</xref>,<xref rid="B38" ref-type="bibr">38</xref>,<xref rid="B39" ref-type="bibr">39</xref>) are considered as the words of RNA; Kmer (<xref rid="B41" ref-type="bibr">41</xref>), Mismatch (<xref rid="B36" ref-type="bibr">36–38</xref>), Top-n-gram (<xref rid="B42" ref-type="bibr">42</xref>), Distance Residue (DR) (<xref rid="B43" ref-type="bibr">43</xref>) and Distance Top-n-gram (DT) (<xref rid="B43" ref-type="bibr">43</xref>) are considered as the words of proteins. Please note that Top-n-gram and DT are the words with the evolutionary information. BOW model represents sentences as the ‘bag’ of words by word occurrence frequencies, ignoring grammar and word orders (<xref rid="B44" ref-type="bibr">44</xref>). Therefore, these BSLMs based on BOW are suitable for analyzing simple functions of biological sequences, such as human nucleosome occupancy prediction (<xref rid="B34" ref-type="bibr">34</xref>), gene regulatory sequence prediction (<xref rid="B35" ref-type="bibr">35</xref>), etc. TF-IDF model (<xref rid="B45" ref-type="bibr">45</xref>) reflects the importance of words to the biological sequences. TextRank (<xref rid="B31" ref-type="bibr">31</xref>), a graph-based ranking model, recognizes key sentences by ranking the criticality of sentences in the text, and assigns higher weights indicating the influence of a word. Because both the TF-IDF model and TextRank model are able to detect the key features of the biological sequences and reduce the dimensions of the feature vectors, they are suitable for constructing efficient predictors for sequence-level analysis tasks, such as RNA-binding protein prediction (<xref rid="B46" ref-type="bibr">46</xref>), protein–protein interaction prediction (<xref rid="B47" ref-type="bibr">47</xref>), etc. These three models are performed on the words of biological sequences, and generate 12 BSLMs based on BOW (see Table <xref rid="tbl3" ref-type="table">3</xref>), 12 BSLMs based on TF-IDF (see Table <xref rid="tbl4" ref-type="table">4</xref>) and 12 BSLMs based on TextRank (see Table <xref rid="tbl5" ref-type="table">5</xref>). Furthermore, the topic model discovers the abstract ‘topics’ and the latent semantic structures of a ‘sequence document’ by using Latent Semantic Analysis (LSA) (<xref rid="B48" ref-type="bibr">48</xref>), Probabilistic Latent Semantic Analysis (PLSA) (<xref rid="B32" ref-type="bibr">32</xref>), Latent Dirichlet Allocation (LDA) (<xref rid="B49" ref-type="bibr">49</xref>) and Labeled-Latent Dirichlet Allocation (Labeled-LDA) (<xref rid="B50" ref-type="bibr">50</xref>), leading to 12 BSLMs based on topic models (see Table <xref rid="tbl6" ref-type="table">6</xref>).</p>
        <table-wrap position="float" id="tbl3">
          <label>Table 3.</label>
          <caption>
            <p>Twelve BSLMs based on BOW</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Category</th>
                <th rowspan="1" colspan="1">Model</th>
                <th rowspan="1" colspan="1">Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">DNA</td>
                <td rowspan="1" colspan="1">Kmer-BOW</td>
                <td rowspan="1" colspan="1">Kmer-based BOW (<xref rid="B33" ref-type="bibr">33</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">RevKmer-BOW</td>
                <td rowspan="1" colspan="1">Reverse-complementary-<break/>
Kmer-based BOW (<xref rid="B33" ref-type="bibr">33–35</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Mismatch-BOW</td>
                <td rowspan="1" colspan="1">Mismatch-based BOW (<xref rid="B36" ref-type="bibr">36–38</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Subsequence-BOW</td>
                <td rowspan="1" colspan="1">Subsequence-based BOW (<xref rid="B36" ref-type="bibr">36</xref>,<xref rid="B38" ref-type="bibr">38</xref>,<xref rid="B39" ref-type="bibr">39</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">RNA</td>
                <td rowspan="1" colspan="1">Kmer-BOW</td>
                <td rowspan="1" colspan="1">Kmer-based BOW (<xref rid="B40" ref-type="bibr">40</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Mismatch-BOW</td>
                <td rowspan="1" colspan="1">Mismatch-based BOW (<xref rid="B36" ref-type="bibr">36–38</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Subsequence-BOW</td>
                <td rowspan="1" colspan="1">Subsequence-based BOW (<xref rid="B36" ref-type="bibr">36</xref>,<xref rid="B38" ref-type="bibr">38</xref>,<xref rid="B39" ref-type="bibr">39</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Protein</td>
                <td rowspan="1" colspan="1">Kmer-BOW</td>
                <td rowspan="1" colspan="1">Kmer-based BOW (<xref rid="B41" ref-type="bibr">41</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Mismatch-BOW</td>
                <td rowspan="1" colspan="1">Mismatch-based BOW (<xref rid="B37" ref-type="bibr">37</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">DR-BOW</td>
                <td rowspan="1" colspan="1">Distance-Residue-based BOW (<xref rid="B43" ref-type="bibr">43</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Top-n-gram-BOW</td>
                <td rowspan="1" colspan="1">Top-n-gram-based BOW (<xref rid="B42" ref-type="bibr">42</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">DT-BOW</td>
                <td rowspan="1" colspan="1">Distance-Top-n-gram-based BOW (<xref rid="B43" ref-type="bibr">43</xref>)</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <table-wrap position="float" id="tbl4">
          <label>Table 4.</label>
          <caption>
            <p>Twelve BSLMs based on TF-IDF</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Category</th>
                <th rowspan="1" colspan="1">Model</th>
                <th rowspan="1" colspan="1">Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">DNA</td>
                <td rowspan="1" colspan="1">Kmer-TF-IDF</td>
                <td rowspan="1" colspan="1">Kmer-based TF-IDF (<xref rid="B30" ref-type="bibr">30</xref>,<xref rid="B33" ref-type="bibr">33</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">RevKmer-TF-IDF</td>
                <td rowspan="1" colspan="1">Reverse-complementary-Kmer-<break/>
based TF-IDF (<xref rid="B30" ref-type="bibr">30</xref>,<xref rid="B33" ref-type="bibr">33–35</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Mismatch-TF-IDF</td>
                <td rowspan="1" colspan="1">Mismatch-based TF-IDF (<xref rid="B30" ref-type="bibr">30</xref>,<xref rid="B36" ref-type="bibr">36–38</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Subsequence-TF-IDF</td>
                <td rowspan="1" colspan="1">Subsequence-based TF-IDF (<xref rid="B30" ref-type="bibr">30</xref>,<xref rid="B36" ref-type="bibr">36</xref>,<xref rid="B38" ref-type="bibr">38</xref>,<xref rid="B39" ref-type="bibr">39</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">RNA</td>
                <td rowspan="1" colspan="1">Kmer- TF-IDF</td>
                <td rowspan="1" colspan="1">Kmer-based TF-IDF (<xref rid="B30" ref-type="bibr">30</xref>,<xref rid="B40" ref-type="bibr">40</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Mismatch-TF-IDF</td>
                <td rowspan="1" colspan="1">Mismatch-based TF-IDF (<xref rid="B30" ref-type="bibr">30</xref>,<xref rid="B36" ref-type="bibr">36–38</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Subsequence-TF-IDF</td>
                <td rowspan="1" colspan="1">Subsequence-based TF-IDF (<xref rid="B30" ref-type="bibr">30</xref>,<xref rid="B36" ref-type="bibr">36</xref>,<xref rid="B38" ref-type="bibr">38</xref>,<xref rid="B39" ref-type="bibr">39</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Protein</td>
                <td rowspan="1" colspan="1">Kmer-TF-IDF</td>
                <td rowspan="1" colspan="1">Kmer-based TF-IDF (<xref rid="B30" ref-type="bibr">30</xref>,<xref rid="B41" ref-type="bibr">41</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Mismatch-TF-IDF</td>
                <td rowspan="1" colspan="1">Mismatch-based TF-IDF (<xref rid="B30" ref-type="bibr">30</xref>,<xref rid="B37" ref-type="bibr">37</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">DR-TF-IDF</td>
                <td rowspan="1" colspan="1">Distance-Residue-based TF-IDF (<xref rid="B30" ref-type="bibr">30</xref>,<xref rid="B43" ref-type="bibr">43</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Top-n-gram-TF-IDF</td>
                <td rowspan="1" colspan="1">Top-n-gram-based TF-IDF(<xref rid="B30" ref-type="bibr">30</xref>,<xref rid="B42" ref-type="bibr">42</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">DT-TF-IDF</td>
                <td rowspan="1" colspan="1">Distance-Top-n-gram-based TF-IDF (<xref rid="B30" ref-type="bibr">30</xref>,<xref rid="B43" ref-type="bibr">43</xref>)</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <table-wrap position="float" id="tbl5">
          <label>Table 5.</label>
          <caption>
            <p>Twelve BSLMs based on TextRank</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Category</th>
                <th rowspan="1" colspan="1">Model</th>
                <th rowspan="1" colspan="1">Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">DNA</td>
                <td rowspan="1" colspan="1">Kmer-TextRank</td>
                <td rowspan="1" colspan="1">Kmer-based TextRank (<xref rid="B31" ref-type="bibr">31</xref>,<xref rid="B33" ref-type="bibr">33</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">RevKmer-TextRank</td>
                <td rowspan="1" colspan="1">Reverse-complementary-Kmer-<break/>
based TextRank (<xref rid="B31" ref-type="bibr">31</xref>,<xref rid="B33" ref-type="bibr">33–35</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Mismatch-TextRank</td>
                <td rowspan="1" colspan="1">Mismatch-based TextRank (<xref rid="B31" ref-type="bibr">31</xref>,<xref rid="B36" ref-type="bibr">36–38</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Subsequence-TextRank</td>
                <td rowspan="1" colspan="1">Subsequence-based TextRank (<xref rid="B31" ref-type="bibr">31</xref>,<xref rid="B36" ref-type="bibr">36</xref>,<xref rid="B38" ref-type="bibr">38</xref>,<xref rid="B39" ref-type="bibr">39</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">RNA</td>
                <td rowspan="1" colspan="1">Kmer-TextRank</td>
                <td rowspan="1" colspan="1">Kmer-based TextRank (<xref rid="B31" ref-type="bibr">31</xref>,<xref rid="B40" ref-type="bibr">40</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Mismatch-TextRank</td>
                <td rowspan="1" colspan="1">Mismatch-based TextRank (<xref rid="B31" ref-type="bibr">31</xref>,<xref rid="B36" ref-type="bibr">36–38</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Subsequence-TextRank</td>
                <td rowspan="1" colspan="1">Subsequence-based TextRank (<xref rid="B31" ref-type="bibr">31</xref>,<xref rid="B36" ref-type="bibr">36</xref>,<xref rid="B38" ref-type="bibr">38</xref>,<xref rid="B39" ref-type="bibr">39</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Protein</td>
                <td rowspan="1" colspan="1">Kmer-TextRank</td>
                <td rowspan="1" colspan="1">Kmer-based TextRank (<xref rid="B31" ref-type="bibr">31</xref>,<xref rid="B41" ref-type="bibr">41</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Mismatch-TextRank</td>
                <td rowspan="1" colspan="1">Mismatch-based TextRank (<xref rid="B31" ref-type="bibr">31</xref>,<xref rid="B37" ref-type="bibr">37</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">DR-TextRank</td>
                <td rowspan="1" colspan="1">Distance-Residue-based TextRank (<xref rid="B31" ref-type="bibr">31</xref>,<xref rid="B43" ref-type="bibr">43</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Top-n-gram-TextRank</td>
                <td rowspan="1" colspan="1">Top-n-gram-based TextRank (<xref rid="B31" ref-type="bibr">31</xref>,<xref rid="B42" ref-type="bibr">42</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">DT-TextRank</td>
                <td rowspan="1" colspan="1">Distance-Top-n-gram-based TextRank (<xref rid="B31" ref-type="bibr">31</xref>,<xref rid="B43" ref-type="bibr">43</xref>)</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <table-wrap position="float" id="tbl6">
          <label>Table 6.</label>
          <caption>
            <p>Twelve BSLMs based on topic models</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Algorithm</th>
                <th rowspan="1" colspan="1">Model</th>
                <th rowspan="1" colspan="1">Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">LSA</td>
                <td rowspan="1" colspan="1">BOW-LSA</td>
                <td rowspan="1" colspan="1">Latent Semantic Analysis (<xref rid="B48" ref-type="bibr">48</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">TF-IDF-LSA</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">TextRank-LSA</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">LDA</td>
                <td rowspan="1" colspan="1">BOW-LDA</td>
                <td rowspan="1" colspan="1">Latent Dirichlet Allocation (<xref rid="B49" ref-type="bibr">49</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">TF-IDF-LDA</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">TextRank-LDA</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Labeled-LDA</td>
                <td rowspan="1" colspan="1">BOW-Labeled-LDA</td>
                <td rowspan="1" colspan="1">Labeled Latent Dirichlet Allocation Model (<xref rid="B50" ref-type="bibr">50</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">TF-IDF-Labeled-LDA</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">TextRank-Labeled-LDA</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">PLSA</td>
                <td rowspan="1" colspan="1">BOW-PLSA</td>
                <td rowspan="1" colspan="1">Probabilistic Latent Semantic Analysis (<xref rid="B32" ref-type="bibr">32</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">TF-IDF-PLSA</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">TextRank-PLSA</td>
                <td rowspan="1" colspan="1"/>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec id="SEC2-2-3">
        <title>Biological neural language models (BNLMs)</title>
        <p>In linguistics, the neural language models (NLMs) (<xref rid="B51" ref-type="bibr">51</xref>) employ deep neural networks to generate the distributed representations of words. Compared with other language models, the NLMs have the following advantages: (i) deep neural networks capture the local and global distance dependencies in a language; (ii) the distributed representation of words effectively avoids the problems of data sparse and dimensional disasters; (iii) the distributed representation of words captures the dependencies in a high-dimensional continuous space, leading to a better generalization ability. In order to incorporate these advantages into the biological language models, we introduce the biological neural language models (BNLMs) based on word embedding (Table <xref rid="tbl7" ref-type="table">7</xref>) and automatic features (Table <xref rid="tbl8" ref-type="table">8</xref>). Because linguistic objects with similar distributions have similar meanings (<xref rid="B52" ref-type="bibr">52</xref>), word embedding embeds each word into a continuous real-valued vector to represent the words. In this study, word2vec (<xref rid="B53" ref-type="bibr">53</xref>), GloVe (<xref rid="B54" ref-type="bibr">54</xref>) and fastText (<xref rid="B55" ref-type="bibr">55</xref>) are combined with the aforementioned words of biological sequences, and the corresponding 36 BNLMs based on word embedding are listed in Table <xref rid="tbl7" ref-type="table">7</xref>. Deep learning techniques are able to automatically extract the linguistic features independent from grammar rules and other experience knowledge. Because deep learning techniques require sufficient samples to train the predictive models with high performance, BNLMs are suitable for analyzing both the residue-level and sequence-level tasks with enough training samples, such as protein structure prediction (<xref rid="B9" ref-type="bibr">9</xref>), protein fold recognition (<xref rid="B56" ref-type="bibr">56</xref>), disordered region prediction (<xref rid="B57" ref-type="bibr">57</xref>), etc. In this study, autoencoder (<xref rid="B58" ref-type="bibr">58</xref>), CNN-BiLSTM (<xref rid="B56" ref-type="bibr">56</xref>) and DCNN-BiLSTM (<xref rid="B56" ref-type="bibr">56</xref>) are used to model the dependencies among residues/words in biological sequences. MotifCNN (<xref rid="B59" ref-type="bibr">59</xref>) and MotifDCNN (<xref rid="B59" ref-type="bibr">59</xref>) are used to capture the motif-based features. Finally, five BNLMs based on automatic features are shown in Table <xref rid="tbl8" ref-type="table">8</xref>.</p>
        <table-wrap position="float" id="tbl7">
          <label>Table 7.</label>
          <caption>
            <p>Thirty-six BNLMs based on word embedding</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Category</th>
                <th rowspan="1" colspan="1">Algorithm</th>
                <th rowspan="1" colspan="1">Model</th>
                <th rowspan="1" colspan="1">Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">DNA</td>
                <td rowspan="1" colspan="1">word2vec</td>
                <td rowspan="1" colspan="1">Kmer2vec</td>
                <td rowspan="1" colspan="1">Learn word representations via word2vec model (<xref rid="B53" ref-type="bibr">53</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">RevKmer2vec</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Mismatch2vec</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Subsequence2vec</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">GloVe</td>
                <td rowspan="1" colspan="1">Kmer-GloVe</td>
                <td rowspan="1" colspan="1">Learn word representations via Glove model (<xref rid="B54" ref-type="bibr">54</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">RevKmer-GloVe</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Mismatch-GloVe</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Subsequence-GloVe</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">fastText</td>
                <td rowspan="1" colspan="1">Kmer-fastText</td>
                <td rowspan="1" colspan="1">Learn word representations via fastText model (<xref rid="B55" ref-type="bibr">55</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">RevKmer-fastText</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Mismatch-fastText</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Subsequence-fastText</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">RNA</td>
                <td rowspan="1" colspan="1">word2vec</td>
                <td rowspan="1" colspan="1">Kmer2vec</td>
                <td rowspan="1" colspan="1">Learn word representations via word2vec model (<xref rid="B53" ref-type="bibr">53</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Mismatch2vec</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Subsequence2vec</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">GloVe</td>
                <td rowspan="1" colspan="1">Kmer-GloVe</td>
                <td rowspan="1" colspan="1">Learn word representations via Glove model (<xref rid="B54" ref-type="bibr">54</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Mismatch-GloVe</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Subsequence-GloVe</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">fastText</td>
                <td rowspan="1" colspan="1">Kmer-fastText</td>
                <td rowspan="1" colspan="1">Learn word representations via fastText model (<xref rid="B55" ref-type="bibr">55</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Mismatch-fastText</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Subsequence-fastText</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Protein</td>
                <td rowspan="1" colspan="1">word2vec</td>
                <td rowspan="1" colspan="1">Kmer2vec</td>
                <td rowspan="1" colspan="1">Learn word representations via word2vec model (<xref rid="B53" ref-type="bibr">53</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Mismatch2vec</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">DR2vec</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Top-n-gram2vec</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">DT2vec</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">GloVe</td>
                <td rowspan="1" colspan="1">Kmer-Glove</td>
                <td rowspan="1" colspan="1">Learn word representations via glove model (<xref rid="B54" ref-type="bibr">54</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Mismatch-Glove</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">DR-Glove</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Top-n-gram-Glove</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">DT-Glove</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">fastText</td>
                <td rowspan="1" colspan="1">Kmer-fastText</td>
                <td rowspan="1" colspan="1">Learn word representations via fastText model (<xref rid="B55" ref-type="bibr">55</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Mismatch-fastText</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">DR-fastText</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Top-n-gram-fastText</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">DT-fastText</td>
                <td rowspan="1" colspan="1"/>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <table-wrap position="float" id="tbl8">
          <label>Table 8.</label>
          <caption>
            <p>Five BNLMs based on automatic features</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Model</th>
                <th rowspan="1" colspan="1">Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">MotifCNN</td>
                <td rowspan="1" colspan="1">CNN construction with motifs initializing convolution kernel (<xref rid="B59" ref-type="bibr">59</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">MotifDCNN</td>
                <td rowspan="1" colspan="1">DCNN construction with motifs initializing convolution kernel (<xref rid="B59" ref-type="bibr">59</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CNN-BiLSTM</td>
                <td rowspan="1" colspan="1">Combine CNN and BiLSTM (<xref rid="B56" ref-type="bibr">56</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">DCNN-BiLSTM</td>
                <td rowspan="1" colspan="1">Combine DCNN and BiLSTM (<xref rid="B56" ref-type="bibr">56</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Autoencoder</td>
                <td rowspan="1" colspan="1">Learning Sequence Representations based on Autoencoders (<xref rid="B58" ref-type="bibr">58</xref>)</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec id="SEC2-2-4">
        <title>Biological semantic similarity language models (BSSLMs)</title>
        <p>Calculation of the sequence similarities of biological sequences is one of the keys in biological sequence analysis, which can be considered as the semantic similarities among sentences. The biological semantic similarity language models (BSSLMs) are able to represent the biological sequences based on the semantic similarities. The semantic similarities can be calculated by the feature vectors generated by the aforementioned three kinds of BLMs via Euclidean Distance (<xref rid="B60" ref-type="bibr">60–62</xref>), Manhattan Distance (<xref rid="B63" ref-type="bibr">63</xref>), Chebyshev Distance (<xref rid="B64" ref-type="bibr">64</xref>), Hamming Distance (<xref rid="B65" ref-type="bibr">65</xref>), Cosine Similarity (<xref rid="B60" ref-type="bibr">60–62</xref>), Pearson Correlation Coefficient (<xref rid="B60" ref-type="bibr">60–62</xref>), KL Divergence (Relative Entropy) (<xref rid="B60" ref-type="bibr">60–62</xref>), or Jaccard Similarity Coefficient (<xref rid="B60" ref-type="bibr">60–62</xref>). The resulting 8 BSSLMs are listed in Table <xref rid="tbl9" ref-type="table">9</xref>. Because the BSSLMs are able to accurately calculate the similarities among biological sequences, they are suitable for analyzing the relationships among biological sequences and the associations between diseases and biological sequences, such as homology detection (<xref rid="B60" ref-type="bibr">60</xref>), non-coding RNA-disease association identification (<xref rid="B66" ref-type="bibr">66</xref>), etc.</p>
        <table-wrap position="float" id="tbl9">
          <label>Table 9.</label>
          <caption>
            <p>Eight BSSLMs</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Model</th>
                <th rowspan="1" colspan="1">Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">ED</td>
                <td rowspan="1" colspan="1">Euclidean Distance (<xref rid="B60" ref-type="bibr">60–62</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">MD</td>
                <td rowspan="1" colspan="1">Manhattan Distance (<xref rid="B63" ref-type="bibr">63</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CD</td>
                <td rowspan="1" colspan="1">Chebyshev Distance (<xref rid="B64" ref-type="bibr">64</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">HD</td>
                <td rowspan="1" colspan="1">Hamming Distance (<xref rid="B65" ref-type="bibr">65</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CS</td>
                <td rowspan="1" colspan="1">Cosine Similarity (<xref rid="B60" ref-type="bibr">60–62</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">PCC</td>
                <td rowspan="1" colspan="1">Pearson Correlation Coefficient (<xref rid="B60" ref-type="bibr">60–62</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">KLD</td>
                <td rowspan="1" colspan="1">KL Divergence (Relative Entropy) (<xref rid="B60" ref-type="bibr">60–62</xref>)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">JSC</td>
                <td rowspan="1" colspan="1">Jaccard Similarity Coefficient (<xref rid="B60" ref-type="bibr">60–62</xref>)</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
    </sec>
    <sec id="SEC2-3">
      <title>Extension of BLMs to BioSeq-BLM system</title>
      <p>As introduced above, the BLMs represent the biological sequences in different aspects. We extend the BLMs to BioSeq-BLM system, making BLMs not only represent the biological sequences but also analyze the biological sequences, which is even out of the reach of any existing language model in linguistics. To achieve this goal, three other functions are added into BLMs, including predictor construction, performance evaluation, and result analysis, which will be introduced in the following sections. The overall flowchart of BioSeq-BLM is shown in Figure <xref rid="F2" ref-type="fig">2</xref>.</p>
      <fig position="float" id="F2">
        <label>Figure 2.</label>
        <caption>
          <p>The main components and their relationships of BioSeq-BLM. Inspired by the similarities between the natural languages and biological sequences, the BioSeq-BLM is constructed. There are four main components in BioSeq-BLM, including Biological Language Models (BLMs), predictor construction, performance evaluation and result analysis.</p>
        </caption>
        <graphic xlink:href="gkab829fig2" position="float"/>
      </fig>
      <sec id="SEC2-3-1">
        <title>Predictor construction</title>
        <p>We extend the BLMs to analyze the biological sequences by combining machine learning classifiers, which can be divided into three categories: classification algorithms, sequence labelling algorithm and deep learning algorithms.</p>
        <p>For classification algorithms, the Support Vector Machine (SVM) (<xref rid="B67" ref-type="bibr">67</xref>) and Random Forest (RF) (<xref rid="B68" ref-type="bibr">68</xref>) are employed. They are widely used in classification tasks and regression tasks because of their good generalization ability (<xref rid="B69" ref-type="bibr">69</xref>). For the sequence labelling algorithm, the Conditional Random Field (CRF) (<xref rid="B70" ref-type="bibr">70</xref>) is used for the residue-level analysis tasks. Compared with the classification algorithms, CRF is able to model the biological sequences in a global fashion by considering the dependency information of all the residues along the sequences. For deep learning algorithms, the convolutional neural network (CNN) (<xref rid="B57" ref-type="bibr">57</xref>) captures the localized semantic association features. Long short-term memory (LSTM) (<xref rid="B71" ref-type="bibr">71</xref>) and Gated recurrent units (GRU) (<xref rid="B72" ref-type="bibr">72</xref>) capture the long-term dependence features of sequences. Transformer (<xref rid="B73" ref-type="bibr">73</xref>), weighted transformer (<xref rid="B74" ref-type="bibr">74</xref>) and reformer (<xref rid="B75" ref-type="bibr">75</xref>) capture the dependencies at any distances in sequences. Compared with the classification algorithms and sequence labelling algorithms, deep learning algorithms can learn the deeper representation of the sequences and model more complex interactions, leading to better performance for the sequence analysis task.</p>
      </sec>
      <sec id="SEC2-3-2">
        <title>Performance evaluation</title>
        <p>Here two methods are employed to evaluate the performance of BioSeq-BLM, including <italic toggle="yes">N</italic>-fold cross-validation and independent test. 9 metrics are used to measure the performance of BioSeq-BLM for binary classification tasks, calculated by:<disp-formula id="M5"><label>(5)</label><tex-math id="M00014" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*} \left\{\begin{matrix} {{\rm{Acc}}\ = \ \frac{{TP\ + \ TN}}{{TP\ + \ FN\ + TN\ + \ FP}}} &amp; 0 \le {\rm{Acc}} \le 1\\ {{\rm{MCC}}\ = \ \frac{{TP\ *\ TN\ - \ FP\ *\ FN}}{{\sqrt {\left( {TP + FN} \right)\left( {TN + FN} \right)\left( {TP + FP} \right)\left( {TN + FP} \right)}}}} &amp; - 1\ \le \ {\rm{MCC}}\ \le \ 1\\ {{\rm{AUC}}\ :\ {\rm{Area\ Under\ ROC\ Curve\ }}} &amp; 0\ \le \ {\rm{AUC}}\ \le \ 1\\ {\rm{Sn}}\ = \ \frac{{TP}}{{TP\ + FN}} &amp; 0\ \le \ {\rm Sn}\ \le \ 1 \\ {\rm{Sp}}\ = \ \frac{{TN}}{{TN\ + \ FP}} &amp; 0\ \le \ {\rm{Sp}}\ \le \ 1\\ {\rm{Balanced\ Accuracy\ }} = \left( {{\rm{Sn}} + {\rm{Sp}}} \right){\rm{\ }}/2 &amp; 0 \le {\rm{Balanced\ Accuracy}} \le 1\\ {\rm{Precision\ }} = \frac{{TP}}{{TP + FP}}\ &amp; 0 \le {\rm{Precision}} \le 1\\ {\rm{AUPR}}:\ {\rm{Area\ Under\ PR\ Curve}} &amp; 0\ \le \ {\rm{AUPR}}\ \le \ 1\\ F1\ = \frac{{2*Precision*Recall}}{{Precision + Recall}} &amp; 0 \le {\rm{F}}1\ \le 1 \end{matrix}\right. \end{eqnarray*}$$\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M00015" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$TP$\end{document}</tex-math></inline-formula> represents the number of true positive samples; <inline-formula><tex-math id="M00016" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$TN$\end{document}</tex-math></inline-formula> represents the number of true negative samples; <inline-formula><tex-math id="M00017" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$FP$\end{document}</tex-math></inline-formula> represents the number of false positive samples; <inline-formula><tex-math id="M00018" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$FN$\end{document}</tex-math></inline-formula> represents the number of false negative samples. For multiclass classification tasks, multi-classification accuracy (<xref rid="B12" ref-type="bibr">12</xref>) is used, calculated by:<disp-formula id="M6"><label>(6)</label><tex-math id="M00019" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}{\rm{Acc}}\left( i \right){\rm{\ }} = {\rm{\ }}1{\rm{\ }} - {\rm{\ }}\frac{{N_ - ^ + \left( i \right) + \ N_ + ^ - \left( i \right)\ }}{{{N^ + }\left( i \right)\ + \ {N^ - }\left( i \right)}}\ 0 \le \ {\rm{Acc}}\left( i \right)\ \le \ 1\end{equation*}$$\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M00020" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${N^ + }( i )$\end{document}</tex-math></inline-formula> represents the total number of the samples in the <italic toggle="yes">i-</italic>th class, <inline-formula><tex-math id="M00021" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N_ - ^ + ( i )$\end{document}</tex-math></inline-formula> is the number of the samples in the <italic toggle="yes">i-</italic>th class wrongly predicted as the other classes, <inline-formula><tex-math id="M00022" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${N^ - }( i )$\end{document}</tex-math></inline-formula> represents the total number of the samples not in the <italic toggle="yes">i-</italic>th class and <inline-formula><tex-math id="M00023" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N_ + ^ - ( i )$\end{document}</tex-math></inline-formula> is the number of the samples not in the <italic toggle="yes">i-</italic>th class wrongly predicted to be the <italic toggle="yes">i-</italic>th class.</p>
        <p>The selection of performance measures is generally based on the characteristics of datasets. For most biological sequence analysis tasks, Acc, MCC, AUC and Balanced Accuracy are the most commonly used metrics for performance evaluation. For a balanced dataset with approximately equal number of samples for each label, Acc metric can accurately evaluate the performance of a predictor. For an unbalanced dataset, MCC, AUC and Balanced Accuracy can better evaluate the performance of the predictors. For example, MCC and AUC are used to evaluate the performance of different predictors for identification of intrinsically disordered regions in proteins (<xref rid="B76" ref-type="bibr">76</xref>) because of the imbalance of samples.</p>
        <p>BioSeq-BLM trained with imbalanced benchmark datasets will bias the class with fewer samples. In this regard, the sampling techniques are provided to solve this problem, including over-sampling method Synthetic Minority Oversampling Technique (SMOTE) (<xref rid="B77" ref-type="bibr">77</xref>), under-sampling method Tomek links (<xref rid="B78" ref-type="bibr">78</xref>) and the combination of over-sampling and under-sampling (<xref rid="B79" ref-type="bibr">79</xref>).</p>
      </sec>
      <sec id="SEC2-3-3">
        <title>Result analysis</title>
        <p>We provide a result analysis framework to interpret the predictive results with four modules: normalization, clustering, feature selection and dimension reduction. L1 regularization (<xref rid="B80" ref-type="bibr">80</xref>), L2 regularization (<xref rid="B81" ref-type="bibr">81</xref>), MinMaxScaler (<xref rid="B82" ref-type="bibr">82</xref>) and StandardScaler (<xref rid="B82" ref-type="bibr">82</xref>) in the normalization module can be used to normalize the features. Clustering module provides 5 cluster algorithms to visualize and validate if the corresponding BLM is able to accurately represent the data, including K-means (<xref rid="B83" ref-type="bibr">83</xref>), affinity propagation algorithms (<xref rid="B84" ref-type="bibr">84</xref>), Density-Based Spatial Clustering of Applications with Noise algorithm (DBSCAN) (<xref rid="B85" ref-type="bibr">85</xref>), Gaussian mixture model (<xref rid="B86" ref-type="bibr">86</xref>) and Agglomerative Nesting (<xref rid="B87" ref-type="bibr">87</xref>). Feature selection module provides 5 methods to analyze the importance of the features generated by BLMs, including chi-square (<xref rid="B88" ref-type="bibr">88</xref>,<xref rid="B89" ref-type="bibr">89</xref>), <italic toggle="yes">F</italic>-value (<xref rid="B88" ref-type="bibr">88</xref>,<xref rid="B89" ref-type="bibr">89</xref>), mutual information (<xref rid="B88" ref-type="bibr">88</xref>,<xref rid="B89" ref-type="bibr">89</xref>), recursive feature elimination (<xref rid="B90" ref-type="bibr">90</xref>) and tree mode (<xref rid="B91" ref-type="bibr">91</xref>). Three dimension reduction methods are incorporated into the dimension reduction module to remove the noise and reduce the dimensions of the feature vectors, including principal component analysis (PCA) (<xref rid="B92" ref-type="bibr">92</xref>), kernel principal component analysis (<xref rid="B93" ref-type="bibr">93</xref>) and truncated singular value decomposition (TSVD) (<xref rid="B94" ref-type="bibr">94</xref>).</p>
      </sec>
    </sec>
    <sec id="SEC2-4">
      <title>BioSeq-BLM web server and stand-alone package</title>
      <p>In order to help the researchers to use the biological language models for biological sequence analysis, we establish the web server and stand-alone tool of BioSeq-BLM, which can be freely accessed from <ext-link xlink:href="http://bliulab.net/BioSeq-BLM/" ext-link-type="uri">http://bliulab.net/BioSeq-BLM/</ext-link>.</p>
      <sec id="SEC2-4-1">
        <title>Web server</title>
        <p>After clicking the ‘Server’ tab, three kinds of BLMs (DNA-BLM, RNA-BLM and protein-BLM for DNA, RNA and protein sequence analysis, respectively) will be shown on the screen, and then the level of analysis (residue-level analysis and sequence-level analysis) and BLM should be selected. Next, choose to calculate semantic similarity based on BSSLMs or not for sequence-level analysis. After selecting the machine learning algorithm, the submit page will be shown on the screen (see Figure <xref rid="F3" ref-type="fig">3A</xref>–<xref rid="F3" ref-type="fig">D</xref>), where the users should set the parameters of the predictors, type the datasets in FASTA format into the input box or upload FASTA files (see Figure <xref rid="F3" ref-type="fig">3E</xref> and <xref rid="F3" ref-type="fig">F</xref>). Finally click the ‘Submit’ for calculation. The results will be shortly shown on the screen (see Figure <xref rid="F4" ref-type="fig">4</xref>).</p>
        <fig position="float" id="F3">
          <label>Figure 3.</label>
          <caption>
            <p>Screenshot of the input page of BioSeq-BLM web server. (<bold>A</bold>) A summary of the main parameters; (<bold>B</bold>) the parameters for BLM; (<bold>C</bold>) the parameters of result analysis; (<bold>D</bold>) the parameters for predictor construction and performance evaluation; (<bold>E</bold>) the input box of the datasets; (<bold>F</bold>) the functional buttons.</p>
          </caption>
          <graphic xlink:href="gkab829fig3" position="float"/>
        </fig>
        <fig position="float" id="F4">
          <label>Figure 4.</label>
          <caption>
            <p>Screenshot of the result page of BioSeq-BLM web server. It contains six sections: (<bold>A</bold>) the summary of main parameters; (<bold>B</bold>) and (<bold>C</bold>) the evaluation results; (<bold>D</bold>) the flowchart; (<bold>E</bold>) the output figures; (<bold>F</bold>) the output files.</p>
          </caption>
          <graphic xlink:href="gkab829fig4" position="float"/>
        </fig>
        <p>In this example, the BLM is set as Kmer-BOW (see Table <xref rid="tbl3" ref-type="table">3</xref>) combined with the SVM for DNA sequence analysis at sequence level. The results page contains six parts as shown in Figure <xref rid="F4" ref-type="fig">4</xref>, from which the users can easily see the performance of the BLM and the importance of different features. This information is a key for selecting the BLM for biological sequence analysis. Please note that for BLMs based on deep learning techniques with high computational costs, the command lines of the stand-alone package will be given, based on which the users can easily obtain the corresponding results with the help of the stand-alone package installed in their own computers.</p>
      </sec>
      <sec id="SEC2-4-2">
        <title>Stand-alone package</title>
        <p>The web server of BioSeq-BLM is easy to use. However, for high-throughput analysis, its computational cost is high, especially for the BLMs based on deep learning techniques. In this regard, the stand-alone package of BioSeq-BLM is provided, which can be downloaded from <ext-link xlink:href="http://bliulab.net/BioSeq-BLM/download/" ext-link-type="uri">http://bliulab.net/BioSeq-BLM/download/</ext-link>. Different from web server, the stand-alone package based on multithreading and GPU acceleration can make full use of local computing resources to implement computing. The trained models generated by BioSeq-BLM can be loaded to predict the unknown samples by using the stand-alone package of BioSeq-BLM. Furthermore, the bash scripts for automatically selecting the best models for specific biological sequence analysis tasks are incorporated into the stand-alone package, which will be particularly useful for biologists to choose the suitable models. For more information, please refer to the manual, which can be accessed at <ext-link xlink:href="http://bliulab.net/BioSeq-BLM/static/download/BioSeq-BLM_manual.pdf" ext-link-type="uri">http://bliulab.net/BioSeq-BLM/static/download/BioSeq-BLM_manual.pdf</ext-link>.</p>
      </sec>
      <sec id="SEC2-4-3">
        <title>How to choose web server or stand-alone package</title>
        <p>The Stand-alone package and web server are complementary. The choice of web server and stand-alone package is related to the number of input sequences and the selected model. For a small number of sequences, web server is generally recommended. When there are many input sequences, it is recommended to use stand-alone package for calculation. If we need to use the model based on deep learning for biological sequence analysis, we suggest the users to use the web server to generate the command lines, and then use the corresponding command lines to run the stand-alone package. If users want to batch select the best BLM and machine learning algorithm for a specific task, the stand-alone package provides relevant scripts to facilitate the relevant functions.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="results" id="SEC3">
    <title>RESULTS AND DISCUSSION</title>
    <p>The BioSeq-BLM incorporates 155 different BLMs for biological sequence analysis. In this section, we will show how to use BioSeq-BLM to solve some specific biological sequence analysis tasks, which will be particularly helpful for researchers to select the BLMs.</p>
    <sec id="SEC3-1">
      <title>Identification DNase I hypersensitive sites</title>
      <p>Identification of DNase I hypersensitive sites (DHSs) is important for understanding the functions of noncoding genomic regions (<xref rid="B95" ref-type="bibr">95</xref>), which is a DNA sequence analysis task at sequence level. Here, we will show how to construct computational predictors for this task based on different BLMs and SVMs with the help of the stand-alone package of BioSeq-BLM. The benchmark dataset (<xref rid="B95" ref-type="bibr">95</xref>) downloaded from <ext-link xlink:href="http://bliulab.net/iDHS-EL/data" ext-link-type="uri">http://bliulab.net/iDHS-EL/data</ext-link> is used as the inputs of BioSeq-BLM. For example, a predictor Subsequence-BOW combines the BSLM of Subsequence-BOW (see Table <xref rid="tbl3" ref-type="table">3</xref>) and SVM can be easily constructed with the help of the stand-alone package with the following command line:</p>
      <table-wrap position="anchor" id="utb1">
        <table frame="hsides" rules="groups">
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">python BioSeq-BLM_Seq.py -category DNA -mode BOW -words Subsequence -word_size 3 -cl Kmeans -nc 5 -fs F-value -nf 128 -rdb fs -ml SVM -sp combine -seq_file pos_file neg _file -label + 1 -1</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>The performance of the top 10 best predictors generated by BioSeq-BLM is shown in Figure <xref rid="F5" ref-type="fig">5</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>, from which we can see that Subsequence-BOW predictor is highly comparable with the state-of-the-art predictor iDHS-EL reported in (<xref rid="B95" ref-type="bibr">95</xref>). The BOW model based on Subsequence words (<xref rid="B36" ref-type="bibr">36</xref>,<xref rid="B38" ref-type="bibr">38</xref>,<xref rid="B39" ref-type="bibr">39</xref>) is able to extract the discriminative features leading to better performance. All the complicated processes for constructing a computational predictor can be easily implemented by BioSeq-BLM with only one command line. Furthermore, the different BLMs incorporated in BioSeq-BLM would be potential candidates for constructing more efficient predictors for this task.</p>
      <fig position="float" id="F5">
        <label>Figure 5.</label>
        <caption>
          <p>(<bold>A</bold>) The Receiver Operating Characteristic (ROC) curves of top 10 predictors constructed by SVMs and different BLMs for identification DNase I hypersensitive sites; (<bold>B</bold>) the corresponding Precision-Recall (PR) curves of these 10 predictors; (<bold>C</bold>) the clustering results by <italic toggle="yes">K</italic>-means algorithm; (<bold>D</bold>) the 10 most important features and their corresponding feature importance values evaluated in terms of <italic toggle="yes">F</italic>-value.</p>
        </caption>
        <graphic xlink:href="gkab829fig5" position="float"/>
      </fig>
    </sec>
    <sec id="SEC3-2">
      <title>Identification of real microRNA precursors</title>
      <p>As miRNAs are deeply implicated with many cancers and other diseases, it is important for both basic research and miRNA-based therapy to discriminate the real pre-miRNAs from the false ones (<xref rid="B96" ref-type="bibr">96</xref>), which is a RNA sequence analysis task at sequence level. Given the benchmark dataset (<xref rid="B96" ref-type="bibr">96</xref>), the RSS predictor for miRNA prediction based on the BGLM of RSS (RNA Secondary Structure) (<xref rid="B97" ref-type="bibr">97</xref>) and SVM can be easily constructed with the help of BioSeq-BLM by using the following command line:</p>
      <table-wrap position="anchor" id="utb2">
        <table frame="hsides" rules="groups">
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">python BioSeq-BLM_Seq.py -category RNA -mode OHE -method RSS -cl Kmeans -nc 5 -dr TSVD -np 128 -rdb dr -ml SVM -seq_file pos_file neg _file -label + 1 -1</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>The performance of the top 10 best predictors generated by BioSeq-BLM is shown in Figure <xref rid="F6" ref-type="fig">6</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>. Because the BGLM of RSS can capture the ‘hairpin’ characteristics of miRNAs in their secondary structures, the computational predictor RSS generated by BioSeq-BLM achieves comparable performance with the iMcRNA predictor reported in (<xref rid="B96" ref-type="bibr">96</xref>), further confirming the usefulness of BioSeq-BLM for RNA sequence analysis.</p>
      <fig position="float" id="F6">
        <label>Figure 6.</label>
        <caption>
          <p>(<bold>A</bold>) The ROC curves of top 10 predictors constructed by SVMs and different BLMs for identification of real microRNA precursors; (<bold>B</bold>) the corresponding PR curves of these 10 predictors; (<bold>C</bold>) the clustering result for <italic toggle="yes">K</italic>-means algorithm; (<bold>D</bold>) the 3D-figure for dimension reduction when applying TSVD method.</p>
        </caption>
        <graphic xlink:href="gkab829fig6" position="float"/>
      </fig>
    </sec>
    <sec id="SEC3-3">
      <title>Identification of DNA-binding proteins and RNA-binding proteins</title>
      <p>Identification of DNA-binding proteins (DBPs) and RNA-binding proteins (RBPs) are important protein sequence analysis tasks at the sequence level. Identification of DBPs and RBPs play important roles in biological processes, such as replication, translation and transcription of genetic material.</p>
      <p>A predictor BOW-LSA for DBP prediction can be generated by BioSeq-BLM by combining the BSLM of BOW-LSA (see Table <xref rid="tbl6" ref-type="table">6</xref>) and RF with the following command line:</p>
      <table-wrap position="anchor" id="utb3">
        <table frame="hsides" rules="groups">
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">python BioSeq-BLM_Seq.py -category Protein -mode TM -method LSA -in_tm BOW -words Top-N-Gram -top_n 2 -com_prop 0.7 -cl Kmeans -nc 5 -fs Tree -nf 128 -ml RF -seq_file pos_file neg _file -label + 1 -1</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>A predictor PDT-Profile for RBP prediction can be generated by BioSeq-BLM by combining the BGLM of PDT-Profile (see Table <xref rid="tbl2" ref-type="table">2</xref>) and SVM with the following command line:</p>
      <table-wrap position="anchor" id="utb4">
        <table frame="hsides" rules="groups">
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">python BioSeq-BLM_Seq.py -category Protein -mode SR -method PDT-Profile -ml SVM -seq_file pos_file neg _file -label + 1 -1</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Evaluated on the benchmark dataset (<xref rid="B98" ref-type="bibr">98</xref>) for DBP identification, the performance of the top 10 best predictors generated by BioSeq-BLM is shown in Figure <xref rid="F7" ref-type="fig">7</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S3</xref>. Because the BSLM of BOW-LSA is able to capture the global information, BOW-LSA shows the best performance and achieves an ACC of 81.58%, outperforming the predictor PseDNA-Pro reported in (<xref rid="B98" ref-type="bibr">98</xref>). Evaluated on the Salmonella benchmark dataset (<xref rid="B46" ref-type="bibr">46</xref>) for RBP identification, the performance of the top 10 best predictors generated by BioSeq-BLM is shown in Figure <xref rid="F8" ref-type="fig">8</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S4</xref>. Because the BGLM of PDT-Profile (<xref rid="B27" ref-type="bibr">27</xref>) is able to efficiently extract the evolutionary information from the profiles, the PDT-Profile predictor combining PDT-Profile and SVM achieves the best performance with an AUC of 0.915, outperforming other three existing state-of-the-art predictors (TriPepSVM (<xref rid="B46" ref-type="bibr">46</xref>), RNAPred (<xref rid="B99" ref-type="bibr">99</xref>) and RBPPred (<xref rid="B100" ref-type="bibr">100</xref>)) by 6–13% in terms of AUC (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S5</xref>). These results indicate that the predictors automatically generated by BioSeq-BLM can even achieve obviously better results than other existing approaches, which is another big step for the applications of the artificial intelligence to protein sequence analysis following the contributions of Alphfold2 (<xref rid="B101" ref-type="bibr">101</xref>) to the protein structure prediction.</p>
      <fig position="float" id="F7">
        <label>Figure 7.</label>
        <caption>
          <p>(<bold>A</bold>) The ROC curves of top 10 predictors constructed by RFs and different BLMs for identification of DNA binding proteins; (<bold>B</bold>) the corresponding PR curves of these 10 predictors; (<bold>C</bold>) the clustering result for <italic toggle="yes">K</italic>-means algorithm; (<bold>D</bold>) the 14 most important features and their corresponding feature importance evaluated by tree-based feature selection.</p>
        </caption>
        <graphic xlink:href="gkab829fig7" position="float"/>
      </fig>
      <fig position="float" id="F8">
        <label>Figure 8.</label>
        <caption>
          <p>(<bold>A</bold>) The ROC curves of top 10 predictors constructed by SVMs and different BLMs for identification of RNA binding proteins; (<bold>B</bold>) the corresponding PR curves of these 10 predictors; (<bold>C</bold>) the clustering result for <italic toggle="yes">K</italic>-means algorithm; (<bold>D</bold>) the 10 most important features and their corresponding feature importance evaluated by tree-based feature selection.</p>
        </caption>
        <graphic xlink:href="gkab829fig8" position="float"/>
      </fig>
    </sec>
    <sec id="SEC3-4">
      <title>Identification of intrinsically disordered regions in proteins</title>
      <p>Intrinsically disordered regions (IDRs) in proteins are important for protein structure and function analysis, which is a protein sequence analysis task at residue level. BioSeq-Analysis2.0 (<xref rid="B12" ref-type="bibr">12</xref>) is another software tool based on machine learning techniques to automatically analyze biological sequences. In this regard, we compare the predictors constructed by BioSeq-BLM for IDR prediction with the predictors generated by BioSeq-Analysis2.0 on the benchmark dataset (<xref rid="B76" ref-type="bibr">76</xref>). A predictor PSSM for IDR prediction can be generated by BioSeq-BLM via combining the BLM of PSSM (<xref rid="B102" ref-type="bibr">102</xref>) and LSTM with the following command line:</p>
      <table-wrap position="anchor" id="utb5">
        <table frame="hsides" rules="groups">
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">python BioSeq-BLM_Res.py -category Protein -method PSSM -ml LSTM -epoch 10 -batch_size 20 -n_layer 2 -hidden_dim 64 -seq_file protein_seq_file -label_file protein_label_file</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>The performance of the top 10 predictors built by BioSeq-BLM is shown in Figure <xref rid="F9" ref-type="fig">9</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S6</xref>. Because the LSTM captures the deep-level dependencies of residues in biological sequences in a global fashion, the PSSM predictor based on LSTM achieves the best performance among the 10 predictors. It also outperforms all the five top performing predictors generated by BioSeq-Analysis2.0 (<xref rid="B12" ref-type="bibr">12</xref>) by 8.7–12.6% in terms of AUC (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S7</xref>). These results are not surprising because the BioSeq-BLM is based on the biological language models and deep learning methods, which are able to more accurately represent and analyze the biological sequences, and therefore, it achieves better performance than BioSeq-Analysis2.0.</p>
      <fig position="float" id="F9">
        <label>Figure 9.</label>
        <caption>
          <p>(<bold>A</bold>) The ROC curves of top 10 predictors constructed by LSTMs and different BLMs for identification of intrinsically disordered regions in proteins; (<bold>B</bold>) the corresponding PR curves of these 10 predictors.</p>
        </caption>
        <graphic xlink:href="gkab829fig9" position="float"/>
      </fig>
    </sec>
    <sec id="SEC3-5">
      <title>RNA secondary structure prediction</title>
      <p>Identification of RNA secondary structure is an important step to understand RNA functions, which is a residue level analysis task. For example, we can use BioSeq-BLM to generate the One-hot predictor based on BGLM of One-hot and Random Forest by using the following command line:</p>
      <table-wrap position="anchor" id="utb6">
        <table frame="hsides" rules="groups">
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">python BioSeq-BLM_Seq.py -category RNA -mode OHE -method One-hot -ml RF -seq_file pos_file neg _file -label + 1 -1 -fixed_len 37</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Given the benchmark dataset (PARS-Yeast dataset (<xref rid="B103" ref-type="bibr">103</xref>)), the performance of the top 10 predictors built by BioSeq-BLM is shown in Figure <xref rid="F10" ref-type="fig">10</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S8</xref>. Because the combination of biological word properties and machine learning algorithms can improve the generalization ability of a predictor, the One-hot predictor achieves the best performance among the 10 predictors with an AUC of 0.960, which is highly comparable with the state-of-the-art predictor GRASP achieving an AUC of 0.967 (<xref rid="B103" ref-type="bibr">103</xref>).</p>
      <fig position="float" id="F10">
        <label>Figure 10.</label>
        <caption>
          <p>(<bold>A</bold>) The ROC curves of top 10 predictors constructed by RFs and different BLMs for RNA secondary structure prediction; (<bold>B</bold>) the corresponding PR curves of these 10 predictors.</p>
        </caption>
        <graphic xlink:href="gkab829fig10" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="SEC4">
    <title>CONCLUSION</title>
    <p>As discussed above, the techniques derived from natural language processing (NLP) are the keys to uncover the meanings of the ‘book of life’. As a result, with the rapid growth of the biological sequence data, the NLP techniques are playing more and more important roles in prediction of the structures and functions of these sequence data. Unfortunately, it is never an easy task to find the suitable NLP techniques to solve a specific task. In order to solve this challenging problem, in this study, we introduce 155 different BLMs for DNA, RNA and protein sequence analysis, and extend these BLMs into a system called BioSeq-BLM, which is able to automatically represent and analyze the sequence data only requiring the sequence data in FASTA format as inputs. With its help, the predictors can be easily constructed. Experimental results show that the predictor even outperforms the existing state-of-the-art approaches for specific tasks. BioSeq-BLM provides new approaches for biological sequence analysis based on the techniques from NLP, which is particularly useful for constructing the computational predictors, or at the very least, it will play a commentary role with the existing methods to contribute to the development of this very important field.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>gkab829_Supplemental_File</label>
      <media xlink:href="gkab829_supplemental_file.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ACK1">
    <title>ACKNOWLEDGEMENTS</title>
    <p>We are also very much indebted to the three anonymous reviewers, whose constructive comments are very helpful for strengthening the presentation of this paper.</p>
  </ack>
  <sec id="SEC5">
    <title>SUPPLEMENTARY DATA</title>
    <p><ext-link xlink:href="https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkab829#supplementary-data" ext-link-type="uri">Supplementary Data</ext-link> are available at NAR Online.</p>
  </sec>
  <sec id="SEC6">
    <title>FUNDING</title>
    <p>National Key R&amp;D Program of China [2018AAA0100100]; National Natural Science Foundation of China [61822306, 61861146002, 61732012]; Beijing Natural Science Foundation [JQ19019]. Funding for open access charge: National Key R&amp;D Program of China [2018AAA0100100].</p>
    <p><italic toggle="yes">Conflict of Interest statement</italic>. None declared.</p>
  </sec>
  <notes id="NT1">
    <title>Notes</title>
    <p>Present address: Bin Liu, Beijing Institute of Technology, No. 5, South Zhongguancun Street, Haidian District, Beijing 100081, China.</p>
  </notes>
  <ref-list id="REF1">
    <title>REFERENCES</title>
    <ref id="B1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Searls</surname><given-names>D.B.</given-names></string-name></person-group><article-title>The language of genes</article-title>. <source>Nature</source>. <year>2002</year>; <volume>420</volume>:<fpage>211</fpage>–<lpage>217</lpage>.<pub-id pub-id-type="pmid">12432405</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scaiewicz</surname><given-names>A.</given-names></string-name>, <string-name><surname>Levitt</surname><given-names>M.</given-names></string-name></person-group><article-title>The language of the protein universe</article-title>. <source>Curr. Opin. Genet. Dev.</source><year>2015</year>; <volume>35</volume>:<fpage>50</fpage>–<lpage>56</lpage>.<pub-id pub-id-type="pmid">26451980</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname><given-names>L.J.</given-names></string-name>, <string-name><surname>Tanwar</surname><given-names>D.K.</given-names></string-name>, <string-name><surname>Penha</surname><given-names>E.D.S.</given-names></string-name>, <string-name><surname>Wolf</surname><given-names>Y.I.</given-names></string-name>, <string-name><surname>Koonin</surname><given-names>E.V.</given-names></string-name>, <string-name><surname>Basu</surname><given-names>M.K.</given-names></string-name></person-group><article-title>Grammar of protein domain architectures</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <year>2019</year>; <volume>116</volume>:<fpage>3636</fpage>–<lpage>3645</lpage>.<pub-id pub-id-type="pmid">30733291</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Searls</surname><given-names>D.B.</given-names></string-name></person-group><article-title>Reading the book of life</article-title>. <source>Bioinformatics</source>. <year>2001</year>; <volume>17</volume>:<fpage>579</fpage>–<lpage>580</lpage>.<pub-id pub-id-type="pmid">11448875</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gimona</surname><given-names>M.</given-names></string-name></person-group><article-title>Protein linguistics - a grammar for modular protein assembly?</article-title>. <source>Nat. Rev. Mol. Cell Biol.</source><year>2006</year>; <volume>7</volume>:<fpage>68</fpage>–<lpage>73</lpage>.<pub-id pub-id-type="pmid">16493414</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kawashima</surname><given-names>S.</given-names></string-name>, <string-name><surname>Pokarowski</surname><given-names>P.</given-names></string-name>, <string-name><surname>Pokarowska</surname><given-names>M.</given-names></string-name>, <string-name><surname>Kolinski</surname><given-names>A.</given-names></string-name>, <string-name><surname>Katayama</surname><given-names>T.</given-names></string-name>, <string-name><surname>Kanehisa</surname><given-names>M.</given-names></string-name></person-group><article-title>AAindex: amino acid index database, progress report 2008</article-title>. <source>Nucleic Acids Res.</source><year>2008</year>; <volume>36</volume>:<fpage>D202</fpage>–<lpage>D205</lpage>.<pub-id pub-id-type="pmid">17998252</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friedel</surname><given-names>M.</given-names></string-name>, <string-name><surname>Nikolajewa</surname><given-names>S.</given-names></string-name>, <string-name><surname>Suhnel</surname><given-names>J.</given-names></string-name>, <string-name><surname>Wilhelm</surname><given-names>T.</given-names></string-name></person-group><article-title>DiProDB: a database for dinucleotide properties</article-title>. <source>Nucleic Acids Res.</source><year>2009</year>; <volume>37</volume>:<fpage>D37</fpage>–<lpage>D40</lpage>.<pub-id pub-id-type="pmid">18805906</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Eavani</surname><given-names>H.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>W.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>W.Y.</given-names></string-name></person-group><article-title>Few-Shot NLG with Pre-Trained Language Model</article-title>. <source>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</source>. <year>2020</year>; <fpage>183</fpage>–<lpage>190</lpage>.</mixed-citation>
    </ref>
    <ref id="B9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Senior</surname><given-names>A.W.</given-names></string-name>, <string-name><surname>Evans</surname><given-names>R.</given-names></string-name>, <string-name><surname>Jumper</surname><given-names>J.</given-names></string-name>, <string-name><surname>Kirkpatrick</surname><given-names>J.</given-names></string-name>, <string-name><surname>Sifre</surname><given-names>L.</given-names></string-name>, <string-name><surname>Green</surname><given-names>T.</given-names></string-name>, <string-name><surname>Qin</surname><given-names>C.</given-names></string-name>, <string-name><surname>Zidek</surname><given-names>A.</given-names></string-name>, <string-name><surname>Nelson</surname><given-names>A.W.R.</given-names></string-name>, <string-name><surname>Bridgland</surname><given-names>A.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Improved protein structure prediction using potentials from deep learning</article-title>. <source>Nature</source>. <year>2020</year>; <volume>577</volume>:<fpage>706</fpage>–<lpage>710</lpage>.<pub-id pub-id-type="pmid">31942072</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alipanahi</surname><given-names>B.</given-names></string-name>, <string-name><surname>Delong</surname><given-names>A.</given-names></string-name>, <string-name><surname>Weirauch</surname><given-names>M.T.</given-names></string-name>, <string-name><surname>Frey</surname><given-names>B.J.</given-names></string-name></person-group><article-title>Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning</article-title>. <source>Nat. Biotechnol.</source><year>2015</year>; <volume>33</volume>:<fpage>831</fpage>–<lpage>838</lpage>.<pub-id pub-id-type="pmid">26213851</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B.</given-names></string-name></person-group><article-title>BioSeq-Analysis: a platform for DNA, RNA and protein sequence analysis based on machine learning approaches</article-title>. <source>Brief. Bioinform.</source><year>2019</year>; <volume>20</volume>:<fpage>1280</fpage>–<lpage>1294</lpage>.<pub-id pub-id-type="pmid">29272359</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B.</given-names></string-name>, <string-name><surname>Gao</surname><given-names>X.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>H.Y.</given-names></string-name></person-group><article-title>BioSeq-Analysis2.0: an updated platform for analyzing DNA, RNA and protein sequences at sequence level and residue level based on machine learning approaches</article-title>. <source>Nucleic Acids Res.</source><year>2019</year>; <volume>47</volume>:<fpage>e127</fpage>.<pub-id pub-id-type="pmid">31504851</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>P.</given-names></string-name>, <string-name><surname>Li</surname><given-names>F.</given-names></string-name>, <string-name><surname>Marquez-Lago</surname><given-names>T.T.</given-names></string-name>, <string-name><surname>Leier</surname><given-names>A.</given-names></string-name>, <string-name><surname>Revote</surname><given-names>J.</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Powell</surname><given-names>D.R.</given-names></string-name>, <string-name><surname>Akutsu</surname><given-names>T.</given-names></string-name>, <string-name><surname>Webb</surname><given-names>G.I.</given-names></string-name><etal>et al</etal>.</person-group><article-title>iLearn: an integrated platform and meta-learner for feature engineering, machine-learning analysis and modeling of DNA, RNA and protein sequence data</article-title>. <source>Brief. Bioinform.</source><year>2019</year>; <volume>21</volume>:<fpage>1047</fpage>–<lpage>1057</lpage>.</mixed-citation>
    </ref>
    <ref id="B14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xiao</surname><given-names>N.</given-names></string-name>, <string-name><surname>Cao</surname><given-names>D.S.</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>M.F.</given-names></string-name>, <string-name><surname>Xu</surname><given-names>Q.S.</given-names></string-name></person-group><article-title>protr/ProtrWeb: R package and web server for generating various numerical representation schemes of protein sequences</article-title>. <source>Bioinformatics</source>. <year>2015</year>; <volume>31</volume>:<fpage>1857</fpage>–<lpage>1859</lpage>.<pub-id pub-id-type="pmid">25619996</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cao</surname><given-names>D.S.</given-names></string-name>, <string-name><surname>Xiao</surname><given-names>N.</given-names></string-name>, <string-name><surname>Xu</surname><given-names>Q.S.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>A.F.</given-names></string-name></person-group><article-title>Rcpi: R/Bioconductor package to generate various descriptors of proteins, compounds and their interactions</article-title>. <source>Bioinformatics</source>. <year>2015</year>; <volume>31</volume>:<fpage>279</fpage>–<lpage>281</lpage>.<pub-id pub-id-type="pmid">25246429</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Avsec</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Kreuzhuber</surname><given-names>R.</given-names></string-name>, <string-name><surname>Israeli</surname><given-names>J.</given-names></string-name>, <string-name><surname>Xu</surname><given-names>N.</given-names></string-name>, <string-name><surname>Cheng</surname><given-names>J.</given-names></string-name>, <string-name><surname>Shrikumar</surname><given-names>A.</given-names></string-name>, <string-name><surname>Banerjee</surname><given-names>A.</given-names></string-name>, <string-name><surname>Kim</surname><given-names>D.S.</given-names></string-name>, <string-name><surname>Beier</surname><given-names>T.</given-names></string-name>, <string-name><surname>Urban</surname><given-names>L.</given-names></string-name><etal>et al</etal>.</person-group><article-title>The Kipoi repository accelerates community exchange and reuse of predictive models for genomics</article-title>. <source>Nat. Biotechnol.</source><year>2019</year>; <volume>37</volume>:<fpage>592</fpage>–<lpage>600</lpage>.<pub-id pub-id-type="pmid">31138913</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kopp</surname><given-names>W.</given-names></string-name>, <string-name><surname>Monti</surname><given-names>R.</given-names></string-name>, <string-name><surname>Tamburrini</surname><given-names>A.</given-names></string-name>, <string-name><surname>Ohler</surname><given-names>U.</given-names></string-name>, <string-name><surname>Akalin</surname><given-names>A.</given-names></string-name></person-group><article-title>Deep learning for genomics using Janggu</article-title>. <source>Nat. Commun.</source><year>2020</year>; <volume>11</volume>:<fpage>3488</fpage>.<pub-id pub-id-type="pmid">32661261</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>K.M.</given-names></string-name>, <string-name><surname>Cofer</surname><given-names>E.M.</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>J.</given-names></string-name>, <string-name><surname>Troyanskaya</surname><given-names>O.G.</given-names></string-name></person-group><article-title>Selene: a PyTorch-based deep learning library for sequence data</article-title>. <source>Nat. Methods</source>. <year>2019</year>; <volume>16</volume>:<fpage>315</fpage>–<lpage>318</lpage>.<pub-id pub-id-type="pmid">30923381</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pereira</surname><given-names>F.</given-names></string-name>, <string-name><surname>Azevedo</surname><given-names>F.</given-names></string-name>, <string-name><surname>Carvalho</surname><given-names>Â.</given-names></string-name>, <string-name><surname>Ribeiro</surname><given-names>G.F.</given-names></string-name>, <string-name><surname>Budde</surname><given-names>M.W.</given-names></string-name>, <string-name><surname>Johansson</surname><given-names>B.</given-names></string-name></person-group><article-title>Pydna: a simulation and documentation tool for DNA assembly strategies using python</article-title>. <source>BMC Bioinformatics</source>. <year>2015</year>; <volume>16</volume>:<fpage>142</fpage>.<pub-id pub-id-type="pmid">25933606</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shannon</surname><given-names>C.E.</given-names></string-name></person-group><article-title>A mathematical theory of communication</article-title>. <source>Bell Syst. Tech. J.</source><year>1948</year>; <volume>27</volume>:<fpage>379</fpage>–<lpage>423</lpage>.</mixed-citation>
    </ref>
    <ref id="B21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goodman</surname><given-names>J.</given-names></string-name></person-group><article-title>A bit of progress in language modeling</article-title>. <source>Comput. Speech Lang.</source><year>2001</year>; <volume>15</volume>:<fpage>403</fpage>–<lpage>434</lpage>.</mixed-citation>
    </ref>
    <ref id="B22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chomsky</surname><given-names>N.</given-names></string-name></person-group><article-title>Three models for the description of language</article-title>. <source>IRE Trans. Inf. Theory</source>. <year>1956</year>; <volume>2</volume>:<fpage>113</fpage>–<lpage>124</lpage>.</mixed-citation>
    </ref>
    <ref id="B23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>J.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>Q.C.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>B.</given-names></string-name></person-group><article-title>iDRBP_MMC: identifying DNA-binding proteins and RNA-binding proteins based on multi-label learning model and motif-based convolutional neural network</article-title>. <source>J. Mol. Biol.</source><year>2020</year>; <volume>432</volume>:<fpage>5860</fpage>–<lpage>5875</lpage>.<pub-id pub-id-type="pmid">32920048</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dong</surname><given-names>Q.W.</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>S.G.</given-names></string-name>, <string-name><surname>Guan</surname><given-names>J.H.</given-names></string-name></person-group><article-title>A new taxonomy-based protein fold recognition approach based on autocross-covariance transformation</article-title>. <source>Bioinformatics</source>. <year>2009</year>; <volume>25</volume>:<fpage>2655</fpage>–<lpage>2662</lpage>.<pub-id pub-id-type="pmid">19706744</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hanson</surname><given-names>J.</given-names></string-name>, <string-name><surname>Paliwal</surname><given-names>K.K.</given-names></string-name>, <string-name><surname>Litfin</surname><given-names>T.</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>Y.</given-names></string-name></person-group><article-title>SPOT-Disorder2: improved protein intrinsic disorder prediction by ensembled deep learning</article-title>. <source>Genomics Proteomics Bioinformatics</source>. <year>2019</year>; <volume>17</volume>:<fpage>645</fpage>–<lpage>656</lpage>.<pub-id pub-id-type="pmid">32173600</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <label>26.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Bari</surname><given-names>A.T.</given-names></string-name>, <string-name><surname>Golam</surname><given-names>M.</given-names></string-name>, <string-name><surname>Reaz</surname><given-names>M.R.</given-names></string-name>, <string-name><surname>Choi</surname><given-names>H.-J.</given-names></string-name>, <string-name><surname>Jeong</surname><given-names>B.-S.</given-names></string-name></person-group><article-title>DNA Encoding for Splice Site Prediction in Large DNA Sequence</article-title>. <source>Proceedings of the 18th International Conference on Database Systems for Advanced Applications</source>. <year>2013</year>; <publisher-loc>Berlin, Heidelberg</publisher-loc><fpage>46</fpage>–<lpage>58</lpage>.<comment>Springer Berlin Heidelberg</comment>.</mixed-citation>
    </ref>
    <ref id="B27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.L.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>Q.C.</given-names></string-name>, <string-name><surname>Dong</surname><given-names>Q.W.</given-names></string-name>, <string-name><surname>Lan</surname><given-names>X.</given-names></string-name></person-group><article-title>Using amino acid physicochemical distance transformation for fast protein remote homology detection</article-title>. <source>PLoS One</source>. <year>2012</year>; <volume>7</volume>:<fpage>e46633</fpage>.<pub-id pub-id-type="pmid">23029559</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Qiang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>H.</given-names></string-name>, <string-name><surname>Ye</surname><given-names>X.</given-names></string-name>, <string-name><surname>Su</surname><given-names>R.</given-names></string-name>, <string-name><surname>Wei</surname><given-names>L.</given-names></string-name></person-group><article-title>M6AMRFS: robust prediction of N6-methyladenosine sites with sequence-based features in multiple species</article-title>. <source>Front. Genet.</source><year>2018</year>; <volume>9</volume>:<fpage>495</fpage>.<pub-id pub-id-type="pmid">30410501</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bahl</surname><given-names>L.R.</given-names></string-name>, <string-name><surname>Brown</surname><given-names>P.F.</given-names></string-name>, <string-name><surname>Souza</surname><given-names>P.V.</given-names></string-name>, <string-name><surname>Mercer</surname><given-names>R.L.</given-names></string-name></person-group><article-title>A tree-based statistical language model for natural language speech recognition</article-title>. <source>IEEE Trans. Acoust. Speech Signal Process.</source><year>1989</year>; <volume>37</volume>:<fpage>1001</fpage>–<lpage>1008</lpage>.</mixed-citation>
    </ref>
    <ref id="B30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>W.</given-names></string-name>, <string-name><surname>Yoshida</surname><given-names>T.</given-names></string-name>, <string-name><surname>Tang</surname><given-names>X.</given-names></string-name></person-group><article-title>A comparative study of TF*IDF, LSI and multi-words for text classification</article-title>. <source>Expert Syst. Appl.</source><year>2011</year>; <volume>38</volume>:<fpage>2758</fpage>–<lpage>2765</lpage>.</mixed-citation>
    </ref>
    <ref id="B31">
      <label>31.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Mihalcea</surname><given-names>R.</given-names></string-name>, <string-name><surname>Tarau</surname><given-names>P.</given-names></string-name></person-group><article-title>Textrank: Bringing order into text</article-title>. <source>Proceedings of the 2004 conference on Empirical Methods in Natural Language Processing</source>. <year>2004</year>; <publisher-loc>Barcelona, Spain</publisher-loc><fpage>404</fpage>–<lpage>411</lpage>.<comment>Association for Computational Linguistics</comment>.</mixed-citation>
    </ref>
    <ref id="B32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blei</surname><given-names>D.M.</given-names></string-name></person-group><article-title>Probabilistic topic models</article-title>. <source>Commun. ACM</source>. <year>2012</year>; <volume>55</volume>:<fpage>77</fpage>–<lpage>84</lpage>.</mixed-citation>
    </ref>
    <ref id="B33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>W.</given-names></string-name>, <string-name><surname>Lei</surname><given-names>T.Y.</given-names></string-name>, <string-name><surname>Jin</surname><given-names>D.C.</given-names></string-name>, <string-name><surname>Lin</surname><given-names>H.</given-names></string-name>, <string-name><surname>Chou</surname><given-names>K.C.</given-names></string-name></person-group><article-title>PseKNC: a flexible web server for generating pseudo K-tuple nucleotide composition</article-title>. <source>Anal. Biochem.</source><year>2014</year>; <volume>456</volume>:<fpage>53</fpage>–<lpage>60</lpage>.<pub-id pub-id-type="pmid">24732113</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gupta</surname><given-names>S.</given-names></string-name>, <string-name><surname>Dennis</surname><given-names>J.</given-names></string-name>, <string-name><surname>Thurman</surname><given-names>R.E.</given-names></string-name>, <string-name><surname>Kingston</surname><given-names>R.</given-names></string-name>, <string-name><surname>Stamatoyannopoulos</surname><given-names>J.A.</given-names></string-name>, <string-name><surname>Noble</surname><given-names>W.S.J.P.C.B.</given-names></string-name></person-group><article-title>Predicting human nucleosome occupancy from primary sequence</article-title>. <source>PLoS Comput. Biol.</source><year>2008</year>; <volume>4</volume>:<fpage>e1000134</fpage>.<pub-id pub-id-type="pmid">18725940</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Noble</surname><given-names>W.S.</given-names></string-name>, <string-name><surname>Kuehn</surname><given-names>S.</given-names></string-name>, <string-name><surname>Thurman</surname><given-names>R.</given-names></string-name>, <string-name><surname>Yu</surname><given-names>M.</given-names></string-name>, <string-name><surname>Stamatoyannopoulos</surname><given-names>J.</given-names></string-name></person-group><article-title>Predicting the in vivo signature of human gene regulatory sequences</article-title>. <source>Bioinformatics</source>. <year>2005</year>; <volume>21</volume>:<fpage>I338</fpage>–<lpage>I343</lpage>.<pub-id pub-id-type="pmid">15961476</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>El-Manzalawy</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Dobbs</surname><given-names>D.</given-names></string-name>, <string-name><surname>Honavar</surname><given-names>V.</given-names></string-name></person-group><article-title>Predicting flexible length linear B-cell epitopes</article-title>. <source>Comput. Syst. Bioinformatics Conf.</source><year>2008</year>; <volume>7</volume>:<fpage>121</fpage>–<lpage>132</lpage>.<pub-id pub-id-type="pmid">19642274</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leslie</surname><given-names>C.S.</given-names></string-name>, <string-name><surname>Eskin</surname><given-names>E.</given-names></string-name>, <string-name><surname>Cohen</surname><given-names>A.</given-names></string-name>, <string-name><surname>Weston</surname><given-names>J.</given-names></string-name>, <string-name><surname>Noble</surname><given-names>W.S.</given-names></string-name></person-group><article-title>Mismatch string kernels for discriminative protein classification</article-title>. <source>Bioinformatics</source>. <year>2004</year>; <volume>20</volume>:<fpage>467</fpage>–<lpage>476</lpage>.<pub-id pub-id-type="pmid">14990442</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Luo</surname><given-names>L.Q.</given-names></string-name>, <string-name><surname>Li</surname><given-names>D.F.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>W.</given-names></string-name>, <string-name><surname>Tu</surname><given-names>S.K.</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>X.P.</given-names></string-name>, <string-name><surname>Tian</surname><given-names>G.</given-names></string-name></person-group><article-title>Accurate prediction of transposon-derived piRNAs by integrating various sequential and physicochemical features</article-title>. <source>PLoS One</source>. <year>2016</year>; <volume>11</volume>:<fpage>e0153268</fpage>.<pub-id pub-id-type="pmid">27074043</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lodhi</surname><given-names>H.</given-names></string-name>, <string-name><surname>Saunders</surname><given-names>C.</given-names></string-name>, <string-name><surname>Shawe-Taylor</surname><given-names>J.</given-names></string-name>, <string-name><surname>Cristianini</surname><given-names>N.</given-names></string-name>, <string-name><surname>Watkins</surname><given-names>C.</given-names></string-name></person-group><article-title>Text classification using string kernels</article-title>. <source>J. Mach. Learn. Res.</source><year>2002</year>; <volume>2</volume>:<fpage>419</fpage>–<lpage>444</lpage>.</mixed-citation>
    </ref>
    <ref id="B40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>H.</given-names></string-name>, <string-name><surname>Deng</surname><given-names>E.Z.</given-names></string-name>, <string-name><surname>Ding</surname><given-names>H.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>W.</given-names></string-name>, <string-name><surname>Chou</surname><given-names>K.C.</given-names></string-name></person-group><article-title>iPro54-PseKNC: a sequence-based predictor for identifying sigma-54 promoters in prokaryote with pseudo k-tuple nucleotide composition</article-title>. <source>Nucleic Acids Res.</source><year>2014</year>; <volume>42</volume>:<fpage>12961</fpage>–<lpage>12972</lpage>.<pub-id pub-id-type="pmid">25361964</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>F.L.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.L.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>J.J.</given-names></string-name>, <string-name><surname>Fang</surname><given-names>L.Y.</given-names></string-name>, <string-name><surname>Chou</surname><given-names>K.C.</given-names></string-name></person-group><article-title>Pse-in-One: a web server for generating various modes of pseudo components of DNA, RNA, and protein sequences</article-title>. <source>Nucleic Acids Res.</source><year>2015</year>; <volume>43</volume>:<fpage>W65</fpage>–<lpage>W71</lpage>.<pub-id pub-id-type="pmid">25958395</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Lin</surname><given-names>L.</given-names></string-name>, <string-name><surname>Dong</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.</given-names></string-name></person-group><article-title>A discriminative method for protein remote homology detection and fold recognition combining Top-n-grams and latent semantic analysis</article-title>. <source>BMC Bioinformatics</source>. <year>2008</year>; <volume>9</volume>:<fpage>510</fpage>.<pub-id pub-id-type="pmid">19046430</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B.</given-names></string-name>, <string-name><surname>Xu</surname><given-names>J.</given-names></string-name>, <string-name><surname>Zou</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Xu</surname><given-names>R.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>Q.</given-names></string-name></person-group><article-title>Using distances between Top-n-gram and residue pairs for protein remote homology detection</article-title>. <source>BMC Bioinformatics</source>. <year>2014</year>; <volume>15</volume>:<fpage>S3</fpage>.</mixed-citation>
    </ref>
    <ref id="B44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harris</surname><given-names>Z.S.</given-names></string-name></person-group><article-title>Distributional structure</article-title>. <source>Word</source>. <year>1954</year>; <volume>10</volume>:<fpage>146</fpage>–<lpage>162</lpage>.</mixed-citation>
    </ref>
    <ref id="B45">
      <label>45.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ramos</surname><given-names>J.</given-names></string-name></person-group><article-title>Using tf-idf to determine word relevance in document queries</article-title>. <source>Proceedings of the First Instructional Conference on Machine Learning</source>. <year>2003</year>; <volume>242</volume>:<publisher-loc>New Jersey, USA</publisher-loc><fpage>133</fpage>–<lpage>142</lpage>.</mixed-citation>
    </ref>
    <ref id="B46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bressin</surname><given-names>A.</given-names></string-name>, <string-name><surname>Schulte-Sasse</surname><given-names>R.</given-names></string-name>, <string-name><surname>Figini</surname><given-names>D.</given-names></string-name>, <string-name><surname>Urdaneta</surname><given-names>E.C.</given-names></string-name>, <string-name><surname>Beckmann</surname><given-names>B.M.</given-names></string-name>, <string-name><surname>Marsico</surname><given-names>A.</given-names></string-name></person-group><article-title>TriPepSVM: de novo prediction of RNA-binding proteins based on short amino acid motifs</article-title>. <source>Nucleic Acids Res.</source><year>2019</year>; <volume>47</volume>:<fpage>4406</fpage>–<lpage>4417</lpage>.<pub-id pub-id-type="pmid">30923827</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <label>47.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guo</surname><given-names>Y.Z.</given-names></string-name>, <string-name><surname>Yu</surname><given-names>L.Z.</given-names></string-name>, <string-name><surname>Wen</surname><given-names>Z.N.</given-names></string-name>, <string-name><surname>Li</surname><given-names>M.L.</given-names></string-name></person-group><article-title>Using support vector machine combined with auto covariance to predict proteinprotein interactions from protein sequences</article-title>. <source>Nucleic Acids Res.</source><year>2008</year>; <volume>36</volume>:<fpage>3025</fpage>–<lpage>3030</lpage>.<pub-id pub-id-type="pmid">18390576</pub-id></mixed-citation>
    </ref>
    <ref id="B48">
      <label>48.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Landauer</surname><given-names>T.K.</given-names></string-name>, <string-name><surname>Foltz</surname><given-names>P.W.</given-names></string-name>, <string-name><surname>Laham</surname><given-names>D</given-names></string-name></person-group><article-title>An introduction to latent semantic analysis</article-title>. <source>Discourse Processes</source>. <year>1998</year>; <volume>25</volume>:<fpage>259</fpage>–<lpage>284</lpage>.</mixed-citation>
    </ref>
    <ref id="B49">
      <label>49.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blei</surname><given-names>D.M.</given-names></string-name>, <string-name><surname>Ng</surname><given-names>A.Y.</given-names></string-name>, <string-name><surname>Jordan</surname><given-names>M.I.</given-names></string-name></person-group><article-title>Latent dirichlet allocation</article-title>. <source>J. Mach. Learn. Res.</source><year>2003</year>; <volume>3</volume>:<fpage>993</fpage>–<lpage>1022</lpage>.</mixed-citation>
    </ref>
    <ref id="B50">
      <label>50.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ramage</surname><given-names>D.</given-names></string-name>, <string-name><surname>Hall</surname><given-names>D.</given-names></string-name>, <string-name><surname>Nallapati</surname><given-names>R.</given-names></string-name>, <string-name><surname>Manning</surname><given-names>C.D.</given-names></string-name></person-group><article-title>Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora</article-title>. <source>Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</source>. <year>2009</year>; <publisher-loc>Singapore</publisher-loc><fpage>248</fpage>–<lpage>256</lpage>.<comment>Association for Computational Linguistics</comment>.</mixed-citation>
    </ref>
    <ref id="B51">
      <label>51.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bengio</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Ducharme</surname><given-names>R.</given-names></string-name>, <string-name><surname>Vincent</surname><given-names>P.</given-names></string-name>, <string-name><surname>Jauvin</surname><given-names>C.</given-names></string-name></person-group><article-title>A neural probabilistic language model</article-title>. <source>J. Mach. Learn. Res.</source><year>2003</year>; <volume>3</volume>:<fpage>1137</fpage>–<lpage>1155</lpage>.</mixed-citation>
    </ref>
    <ref id="B52">
      <label>52.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>HARRIS</surname><given-names>Z.</given-names></string-name></person-group><article-title>Distributional Structure</article-title>. <source>Word</source>. <year>1954</year>; <volume>10</volume>:<fpage>142</fpage>–<lpage>146</lpage>.</mixed-citation>
    </ref>
    <ref id="B53">
      <label>53.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Mikolov</surname><given-names>T.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>K.</given-names></string-name>, <string-name><surname>Corrado</surname><given-names>G.</given-names></string-name>, <string-name><surname>Dean</surname><given-names>J.</given-names></string-name></person-group><article-title>Efficient estimation of word representations in vector space</article-title>. <year>2013</year>; <comment>arXiv doi:</comment><comment>07 September 2013, preprint: not peer reviewed</comment><uri xlink:href="https://www.arxiv.org/abs/1301.3781">https://arxiv.org/abs//1301.3781</uri>.</mixed-citation>
    </ref>
    <ref id="B54">
      <label>54.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pennington</surname><given-names>J.</given-names></string-name>, <string-name><surname>Socher</surname><given-names>R.</given-names></string-name>, <string-name><surname>Manning</surname><given-names>C.D.</given-names></string-name></person-group><article-title>Glove: Global vectors for word representation</article-title>. <source>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</source>. <year>2014</year>; <fpage>1532</fpage>–<lpage>1543</lpage>.<comment>Association for Computational Linguistics</comment>.</mixed-citation>
    </ref>
    <ref id="B55">
      <label>55.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Joulin</surname><given-names>A.</given-names></string-name>, <string-name><surname>Grave</surname><given-names>E.</given-names></string-name>, <string-name><surname>Bojanowski</surname><given-names>P.</given-names></string-name>, <string-name><surname>Mikolov</surname><given-names>T.</given-names></string-name></person-group><article-title>Bag of Tricks for Efficient Text Classification</article-title>. <source>Conference of the European Chapter of the Association for Computational Linguistics</source>. <year>2017</year>; <volume>2</volume>:<fpage>427</fpage>–<lpage>431</lpage>.</mixed-citation>
    </ref>
    <ref id="B56">
      <label>56.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B.</given-names></string-name>, <string-name><surname>Li</surname><given-names>C.C.</given-names></string-name>, <string-name><surname>Yan</surname><given-names>K.</given-names></string-name></person-group><article-title>DeepSVM-fold: protein fold recognition by combining support vector machines and pairwise sequence similarity scores generated by deep learning networks</article-title>. <source>Brief. Bioinform.</source><year>2020</year>; <volume>21</volume>:<fpage>1733</fpage>–<lpage>1741</lpage>.<pub-id pub-id-type="pmid">31665221</pub-id></mixed-citation>
    </ref>
    <ref id="B57">
      <label>57.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hanson</surname><given-names>J.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>Y.D.</given-names></string-name>, <string-name><surname>Paliwal</surname><given-names>K.</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>Y.Q.</given-names></string-name></person-group><article-title>Improving protein disorder prediction by deep bidirectional long short-term memory recurrent neural networks</article-title>. <source>Bioinformatics</source>. <year>2017</year>; <volume>33</volume>:<fpage>685</fpage>–<lpage>692</lpage>.<pub-id pub-id-type="pmid">28011771</pub-id></mixed-citation>
    </ref>
    <ref id="B58">
      <label>58.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Lebret</surname><given-names>R.</given-names></string-name>, <string-name><surname>Collobert</surname><given-names>R.</given-names></string-name></person-group><article-title>“The Sum of Its Parts”: joint learning of word and phrase representations with autoencoders</article-title>. <year>2015</year>; <comment>arXiv doi:</comment><comment>18 June 20155,preprint: not peer reviewed</comment><uri xlink:href="https://www.arxiv.org/abs/1506.05703">https://arxiv.org/abs/1506.05703</uri>.</mixed-citation>
    </ref>
    <ref id="B59">
      <label>59.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>C.C.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>B.</given-names></string-name></person-group><article-title>MotifCNN-fold: protein fold recognition based on fold-specific features extracted by motif-based convolutional neural networks</article-title>. <source>Brief. Bioinform.</source><year>2020</year>; <volume>21</volume>:<fpage>2133</fpage>–<lpage>2141</lpage>.<pub-id pub-id-type="pmid">31774907</pub-id></mixed-citation>
    </ref>
    <ref id="B60">
      <label>60.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ye</surname><given-names>X.G.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>G.L.</given-names></string-name>, <string-name><surname>Altschul</surname><given-names>S.F.</given-names></string-name></person-group><article-title>An assessment of substitution scores for protein profile-profile comparison</article-title>. <source>Bioinformatics</source>. <year>2011</year>; <volume>27</volume>:<fpage>3356</fpage>–<lpage>3363</lpage>.<pub-id pub-id-type="pmid">21998158</pub-id></mixed-citation>
    </ref>
    <ref id="B61">
      <label>61.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rangwala</surname><given-names>H.</given-names></string-name>, <string-name><surname>Karypis</surname><given-names>G.</given-names></string-name></person-group><article-title>Profile-based direct kernels for remote homology detection and fold recognition</article-title>. <source>Bioinformatics</source>. <year>2005</year>; <volume>21</volume>:<fpage>4239</fpage>–<lpage>4247</lpage>.<pub-id pub-id-type="pmid">16188929</pub-id></mixed-citation>
    </ref>
    <ref id="B62">
      <label>62.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mittelman</surname><given-names>D.</given-names></string-name>, <string-name><surname>Sadreyev</surname><given-names>R.</given-names></string-name>, <string-name><surname>Grishin</surname><given-names>N.</given-names></string-name></person-group><article-title>Probabilistic scoring measures for profile-profile comparison yield more accurate short seed alignments</article-title>. <source>Bioinformatics</source>. <year>2003</year>; <volume>19</volume>:<fpage>1531</fpage>–<lpage>1539</lpage>.<pub-id pub-id-type="pmid">12912834</pub-id></mixed-citation>
    </ref>
    <ref id="B63">
      <label>63.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Strauss</surname><given-names>T.</given-names></string-name>, <string-name><surname>von Maltitz</surname><given-names>M.J.</given-names></string-name></person-group><article-title>Generalising Ward's method for use with Manhattan distances</article-title>. <source>PLoS One</source>. <year>2017</year>; <volume>12</volume>:<fpage>e0168288</fpage>.<pub-id pub-id-type="pmid">28085891</pub-id></mixed-citation>
    </ref>
    <ref id="B64">
      <label>64.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weinberger</surname><given-names>K.Q.</given-names></string-name>, <string-name><surname>Saul</surname><given-names>L.K.</given-names></string-name></person-group><article-title>Distance metric learning for large margin nearest neighbor classification</article-title>. <source>J. Mach. Learn. Res.</source><year>2009</year>; <volume>10</volume>:<fpage>207</fpage>–<lpage>244</lpage>.</mixed-citation>
    </ref>
    <ref id="B65">
      <label>65.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Laboulais</surname><given-names>C.</given-names></string-name>, <string-name><surname>Ouali</surname><given-names>M.</given-names></string-name>, <string-name><surname>Le Bret</surname><given-names>M.</given-names></string-name>, <string-name><surname>Gabarro-Arpa</surname><given-names>J.</given-names></string-name></person-group><article-title>Hamming distance geometry of a protein conformational space: application to the clustering of a 4-ns molecular dynamics trajectory of the HIV-1 integrase catalytic core</article-title>. <source>Proteins-Struct. Funct. Genet.</source><year>2002</year>; <volume>47</volume>:<fpage>169</fpage>–<lpage>179</lpage>.<pub-id pub-id-type="pmid">11933064</pub-id></mixed-citation>
    </ref>
    <ref id="B66">
      <label>66.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>L.</given-names></string-name>, <string-name><surname>You</surname><given-names>Z.H.</given-names></string-name>, <string-name><surname>Huang</surname><given-names>Y.A.</given-names></string-name>, <string-name><surname>Huang</surname><given-names>D.S.</given-names></string-name>, <string-name><surname>Chan</surname><given-names>K.C.C.</given-names></string-name></person-group><article-title>An efficient approach based on multi-sources information to predict circRNA-disease associations using deep convolutional neural network</article-title>. <source>Bioinformatics</source>. <year>2020</year>; <volume>36</volume>:<fpage>4038</fpage>–<lpage>4046</lpage>.<pub-id pub-id-type="pmid">31793982</pub-id></mixed-citation>
    </ref>
    <ref id="B67">
      <label>67.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname><given-names>C.C.</given-names></string-name>, <string-name><surname>Lin</surname><given-names>C.J.</given-names></string-name></person-group><article-title>LIBSVM: a library for support vector machines</article-title>. <source>ACM Trans. Intell. Syst. Technol.</source><year>2011</year>; <volume>2</volume>:<fpage>27</fpage>.</mixed-citation>
    </ref>
    <ref id="B68">
      <label>68.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Biau</surname><given-names>G.</given-names></string-name></person-group><article-title>Analysis of a random forests model</article-title>. <source>J. Mach. Learn. Res.</source><year>2012</year>; <volume>13</volume>:<fpage>1063</fpage>–<lpage>1095</lpage>.</mixed-citation>
    </ref>
    <ref id="B69">
      <label>69.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>P.</given-names></string-name>, <string-name><surname>Li</surname><given-names>C.</given-names></string-name>, <string-name><surname>Li</surname><given-names>F.</given-names></string-name>, <string-name><surname>Xiang</surname><given-names>D.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>Y.Z.</given-names></string-name>, <string-name><surname>Akutsu</surname><given-names>T.</given-names></string-name>, <string-name><surname>Daly</surname><given-names>R.J.</given-names></string-name>, <string-name><surname>Webb</surname><given-names>G.I.</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>Q.</given-names></string-name><etal>et al</etal>.</person-group><article-title>iLearnPlus: a comprehensive and automated machine-learning platform for nucleic acid and protein sequence analysis, prediction and visualization</article-title>. <source>Nucleic. Acids. Res.</source><year>2021</year>; <volume>49</volume>:<fpage>e60</fpage>.<pub-id pub-id-type="pmid">33660783</pub-id></mixed-citation>
    </ref>
    <ref id="B70">
      <label>70.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sutton</surname><given-names>C.</given-names></string-name>, <string-name><surname>McCallum</surname><given-names>A.</given-names></string-name></person-group><article-title>An introduction to conditional random fields</article-title>. <source>Found. Trends Mach. Learn.</source><year>2012</year>; <volume>4</volume>:<fpage>267</fpage>–<lpage>373</lpage>.</mixed-citation>
    </ref>
    <ref id="B71">
      <label>71.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hochreiter</surname><given-names>S.</given-names></string-name>, <string-name><surname>Schmidhuber</surname><given-names>J.</given-names></string-name></person-group><article-title>Long short-term memory</article-title>. <source>Neural Comput.</source><year>1997</year>; <volume>9</volume>:<fpage>1735</fpage>–<lpage>1780</lpage>.<pub-id pub-id-type="pmid">9377276</pub-id></mixed-citation>
    </ref>
    <ref id="B72">
      <label>72.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cho</surname><given-names>K.</given-names></string-name>, <string-name><surname>van Merriënboer</surname><given-names>B.</given-names></string-name>, <string-name><surname>Gulcehre</surname><given-names>C.</given-names></string-name>, <string-name><surname>Bahdanau</surname><given-names>D.</given-names></string-name>, <string-name><surname>Bougares</surname><given-names>F.</given-names></string-name>, <string-name><surname>Schwenk</surname><given-names>H.</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y.</given-names></string-name></person-group><article-title>Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</article-title>. <source>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</source>. <year>2014</year>; <fpage>1724</fpage>–<lpage>1734</lpage>.<comment>Association for Computational Linguistics</comment>.</mixed-citation>
    </ref>
    <ref id="B73">
      <label>73.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Vaswani</surname><given-names>A.</given-names></string-name>, <string-name><surname>Shazeer</surname><given-names>N.</given-names></string-name>, <string-name><surname>Parmar</surname><given-names>N.</given-names></string-name>, <string-name><surname>Uszkoreit</surname><given-names>J.</given-names></string-name>, <string-name><surname>Jones</surname><given-names>L.</given-names></string-name>, <string-name><surname>Gomez</surname><given-names>A.N.</given-names></string-name>, <string-name><surname>Kaiser</surname><given-names>Ł.</given-names></string-name>, <string-name><surname>Polosukhin</surname><given-names>I.</given-names></string-name></person-group><article-title>Attention is all you need</article-title>. <source>Proceedings of the 31st International Conference on Neural Information Processing Systems</source>. <year>2017</year>; <publisher-loc>Long Beach, California, USA</publisher-loc><fpage>6000</fpage>–<lpage>6010</lpage>.<comment>Curran Associates Inc.</comment>.</mixed-citation>
    </ref>
    <ref id="B74">
      <label>74.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ahmed</surname><given-names>K.</given-names></string-name>, <string-name><surname>Keskar</surname><given-names>N.S.</given-names></string-name>, <string-name><surname>Socher</surname><given-names>R.</given-names></string-name></person-group><article-title>Weighted transformer network for machine translation</article-title>. <year>2017</year>; <comment>arXiv doi:</comment><comment>06 November 2017, preprint: not peer reviewed</comment><uri xlink:href="https://www.arxiv.org/abs/1711.02132">https://arxiv.org/abs/1711.02132</uri>.</mixed-citation>
    </ref>
    <ref id="B75">
      <label>75.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kitaev</surname><given-names>N.</given-names></string-name>, <string-name><surname>Kaiser</surname><given-names>Ł.</given-names></string-name>, <string-name><surname>Levskaya</surname><given-names>A.</given-names></string-name></person-group><article-title>Reformer: the efficient transformer</article-title>. <year>2020</year>; <comment>arXiv doi:</comment><comment>18 February 2020, preprint: not peer reviewed</comment><uri xlink:href="https://www.arxiv.org/abs/2001.04451">https://arxiv.org/abs/2001.04451</uri>.</mixed-citation>
    </ref>
    <ref id="B76">
      <label>76.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>B.</given-names></string-name></person-group><article-title>IDP–CRF: intrinsically disordered protein/region identification based on conditional random fields</article-title>. <source>Int. J. Mol. Sci.</source><year>2018</year>; <volume>19</volume>:<fpage>2483</fpage>.</mixed-citation>
    </ref>
    <ref id="B77">
      <label>77.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chawla</surname><given-names>N.V.</given-names></string-name>, <string-name><surname>Bowyer</surname><given-names>K.W.</given-names></string-name>, <string-name><surname>Hall</surname><given-names>L.O.</given-names></string-name>, <string-name><surname>Kegelmeyer</surname><given-names>W.P.</given-names></string-name></person-group><article-title>SMOTE: synthetic minority over-sampling technique</article-title>. <source>J. Artif. Intell. Res.</source><year>2002</year>; <volume>16</volume>:<fpage>321</fpage>–<lpage>357</lpage>.</mixed-citation>
    </ref>
    <ref id="B78">
      <label>78.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Farquad</surname><given-names>M.A.H.</given-names></string-name>, <string-name><surname>Bose</surname><given-names>I.</given-names></string-name></person-group><article-title>Preprocessing unbalanced data using support vector machine</article-title>. <source>Decision Support Systems</source>. <year>2012</year>; <volume>53</volume>:<fpage>226</fpage>–<lpage>233</lpage>.</mixed-citation>
    </ref>
    <ref id="B79">
      <label>79.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Junsomboon</surname><given-names>N.</given-names></string-name>, <string-name><surname>Phienthrakul</surname><given-names>T.</given-names></string-name></person-group><article-title>Combining Over-Sampling and Under-Sampling Techniques for Imbalance Dataset</article-title>. <source>Proceedings of the 9th International Conference on Machine Learning and Computing</source>. <year>2017</year>; <publisher-loc>Singapore, Singapore</publisher-loc><fpage>243</fpage>–<lpage>247</lpage>.<comment>Association for Computing Machinery</comment>.</mixed-citation>
    </ref>
    <ref id="B80">
      <label>80.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Schmidt</surname><given-names>M.</given-names></string-name>, <string-name><surname>Fung</surname><given-names>G.</given-names></string-name>, <string-name><surname>Rosales</surname><given-names>R.</given-names></string-name></person-group><article-title>Fast Optimization Methods for L1 Regularization: A Comparative Study and Two New Approaches</article-title>. <source>Proceedings of the 18th European conference on Machine Learning</source>. <year>2007</year>; <publisher-loc>Warsaw, Poland</publisher-loc><fpage>286</fpage>–<lpage>297</lpage>.<comment>Springer-Verlag</comment>.</mixed-citation>
    </ref>
    <ref id="B81">
      <label>81.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bilgic</surname><given-names>B.</given-names></string-name>, <string-name><surname>Chatnuntawech</surname><given-names>I.</given-names></string-name>, <string-name><surname>Fan</surname><given-names>A.P.</given-names></string-name>, <string-name><surname>Setsompop</surname><given-names>K.</given-names></string-name>, <string-name><surname>Cauley</surname><given-names>S.F.</given-names></string-name>, <string-name><surname>Wald</surname><given-names>L.L.</given-names></string-name>, <string-name><surname>Adalsteinsson</surname><given-names>E.</given-names></string-name></person-group><article-title>Fast image reconstruction with L2-regularization</article-title>. <source>J. Magn. Reson. Imaging</source>. <year>2014</year>; <volume>40</volume>:<fpage>181</fpage>–<lpage>191</lpage>.<pub-id pub-id-type="pmid">24395184</pub-id></mixed-citation>
    </ref>
    <ref id="B82">
      <label>82.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pedregosa</surname><given-names>F.</given-names></string-name>, <string-name><surname>Varoquaux</surname><given-names>G.</given-names></string-name>, <string-name><surname>Gramfort</surname><given-names>A.</given-names></string-name>, <string-name><surname>Michel</surname><given-names>V.</given-names></string-name>, <string-name><surname>Thirion</surname><given-names>B.</given-names></string-name>, <string-name><surname>Grisel</surname><given-names>O.</given-names></string-name>, <string-name><surname>Blondel</surname><given-names>M.</given-names></string-name>, <string-name><surname>Prettenhofer</surname><given-names>P.</given-names></string-name>, <string-name><surname>Weiss</surname><given-names>R.</given-names></string-name>, <string-name><surname>Dubourg</surname><given-names>V.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Scikit-learn: machine learning in Python</article-title>. <source>J. Mach. Learn. Res.</source><year>2011</year>; <volume>12</volume>:<fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="B83">
      <label>83.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jain</surname><given-names>A.K.</given-names></string-name>, <string-name><surname>Murty</surname><given-names>M.N.</given-names></string-name>, <string-name><surname>Flynn</surname><given-names>P.J.</given-names></string-name></person-group><article-title>Data clustering: a review</article-title>. <source>ACM computing surveys</source>. <year>1999</year>; <volume>31</volume>:<fpage>264</fpage>–<lpage>323</lpage>.</mixed-citation>
    </ref>
    <ref id="B84">
      <label>84.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frey</surname><given-names>B.J.</given-names></string-name>, <string-name><surname>Dueck</surname><given-names>D</given-names></string-name></person-group><article-title>Clustering by passing messages between data points</article-title>. <source>Science</source>. <year>2007</year>; <volume>315</volume>:<fpage>972</fpage>–<lpage>976</lpage>.<pub-id pub-id-type="pmid">17218491</pub-id></mixed-citation>
    </ref>
    <ref id="B85">
      <label>85.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ester</surname><given-names>M.</given-names></string-name>, <string-name><surname>Kriegel</surname><given-names>H.-P.</given-names></string-name>, <string-name><surname>Sander</surname><given-names>J.</given-names></string-name>, <string-name><surname>Xu</surname><given-names>X.</given-names></string-name></person-group><article-title>A density-based algorithm for discovering clusters in large spatial databases with noise</article-title>. <source>Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</source>. <year>1996</year>; <publisher-loc>Portland, Oregon</publisher-loc><fpage>226</fpage>–<lpage>231</lpage>.<comment>AAAI Press</comment>.</mixed-citation>
    </ref>
    <ref id="B86">
      <label>86.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname><given-names>S.C.</given-names></string-name>, <string-name><surname>Kang</surname><given-names>T.J.</given-names></string-name></person-group><article-title>Texture classification and segmentation using wavelet packet frame and Gaussian mixture model</article-title>. <source>Pattern Recogn</source>. <year>2007</year>; <volume>40</volume>:<fpage>1207</fpage>–<lpage>1221</lpage>.</mixed-citation>
    </ref>
    <ref id="B87">
      <label>87.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Skarmeta</surname><given-names>A.G.</given-names></string-name>, <string-name><surname>Bensaid</surname><given-names>A.</given-names></string-name>, <string-name><surname>Tazi</surname><given-names>N.</given-names></string-name></person-group><article-title>Data mining for text categorization with semi-supervised agglomerative hierarchical clustering</article-title>. <source>Int. J. Intell. Syst.</source><year>2000</year>; <volume>15</volume>:<fpage>633</fpage>–<lpage>646</lpage>.</mixed-citation>
    </ref>
    <ref id="B88">
      <label>88.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chandrashekar</surname><given-names>G.</given-names></string-name>, <string-name><surname>Sahin</surname><given-names>F.</given-names></string-name></person-group><article-title>A survey on feature selection methods</article-title>. <source>Comput. Electr. Eng.</source><year>2014</year>; <volume>40</volume>:<fpage>16</fpage>–<lpage>28</lpage>.</mixed-citation>
    </ref>
    <ref id="B89">
      <label>89.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guyon</surname><given-names>I.</given-names></string-name>, <string-name><surname>Elisseeff</surname><given-names>A.</given-names></string-name></person-group><article-title>An introduction to variable and feature selection</article-title>. <source>J. Mach. Learn. Res.</source><year>2003</year>; <volume>3</volume>:<fpage>1157</fpage>–<lpage>1182</lpage>.</mixed-citation>
    </ref>
    <ref id="B90">
      <label>90.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Darst</surname><given-names>B.F.</given-names></string-name>, <string-name><surname>Malecki</surname><given-names>K.C.</given-names></string-name>, <string-name><surname>Engelman</surname><given-names>C.D.</given-names></string-name></person-group><article-title>Using recursive feature elimination in random forest to account for correlated variables in high dimensional data</article-title>. <source>BMC Genet.</source><year>2018</year>; <volume>19</volume>:<fpage>353</fpage>–<lpage>363</lpage>.</mixed-citation>
    </ref>
    <ref id="B91">
      <label>91.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sugumaran</surname><given-names>V.</given-names></string-name>, <string-name><surname>Muralidharan</surname><given-names>V.</given-names></string-name>, <string-name><surname>Ramachandran</surname><given-names>K.</given-names></string-name></person-group><article-title>Feature selection using decision tree and classification through proximal support vector machine for fault diagnostics of roller bearing</article-title>. <source>Mech. Syst. Signal Process.</source><year>2007</year>; <volume>21</volume>:<fpage>930</fpage>–<lpage>942</lpage>.</mixed-citation>
    </ref>
    <ref id="B92">
      <label>92.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yeung</surname><given-names>K.Y.</given-names></string-name>, <string-name><surname>Ruzzo</surname><given-names>W.L.</given-names></string-name></person-group><article-title>Principal component analysis for clustering gene expression data</article-title>. <source>Bioinformatics</source>. <year>2001</year>; <volume>17</volume>:<fpage>763</fpage>–<lpage>774</lpage>.<pub-id pub-id-type="pmid">11590094</pub-id></mixed-citation>
    </ref>
    <ref id="B93">
      <label>93.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schölkopf</surname><given-names>B.</given-names></string-name>, <string-name><surname>Smola</surname><given-names>A.J.</given-names></string-name>, <string-name><surname>Müller</surname><given-names>K.-R.</given-names></string-name></person-group><article-title>Kernel Principal Component Analysis</article-title>. <source>Proceedings of the 7th International Conference on Artificial Neural Networks</source>. <year>1997</year>; <fpage>583</fpage>–<lpage>588</lpage>.<comment>Springer-Verlag</comment>.</mixed-citation>
    </ref>
    <ref id="B94">
      <label>94.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wei</surname><given-names>J.-J.</given-names></string-name>, <string-name><surname>Chang</surname><given-names>C.-J.</given-names></string-name>, <string-name><surname>Chou</surname><given-names>N.-K.</given-names></string-name>, <string-name><surname>Jan</surname><given-names>G.-J.</given-names></string-name></person-group><article-title>ECG data compression using truncated singular value decomposition</article-title>. <source>Trans. Info. Tech. Biomed.</source><year>2001</year>; <volume>5</volume>:<fpage>290</fpage>–<lpage>299</lpage>.</mixed-citation>
    </ref>
    <ref id="B95">
      <label>95.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B.</given-names></string-name>, <string-name><surname>Long</surname><given-names>R.</given-names></string-name>, <string-name><surname>Chou</surname><given-names>K.C.</given-names></string-name></person-group><article-title>iDHS-EL: identifying DNase I hypersensitive sites by fusing three different modes of pseudo nucleotide composition into an ensemble learning framework</article-title>. <source>Bioinformatics</source>. <year>2016</year>; <volume>32</volume>:<fpage>2411</fpage>–<lpage>2418</lpage>.<pub-id pub-id-type="pmid">27153623</pub-id></mixed-citation>
    </ref>
    <ref id="B96">
      <label>96.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B.</given-names></string-name>, <string-name><surname>Fang</surname><given-names>L.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>F.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>J.</given-names></string-name>, <string-name><surname>Chou</surname><given-names>K.-C.</given-names></string-name></person-group><article-title>Identification of real microRNA precursors with a pseudo structure status composition approach</article-title>. <source>PLoS One</source>. <year>2015</year>; <volume>10</volume>:<fpage>e0121501</fpage>.<pub-id pub-id-type="pmid">25821974</pub-id></mixed-citation>
    </ref>
    <ref id="B97">
      <label>97.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hofacker</surname><given-names>I.L.</given-names></string-name>, <string-name><surname>Fontana</surname><given-names>W.</given-names></string-name>, <string-name><surname>Stadler</surname><given-names>P.F.</given-names></string-name>, <string-name><surname>Bonhoeffer</surname><given-names>L.S.</given-names></string-name>, <string-name><surname>Tacker</surname><given-names>M.</given-names></string-name>, <string-name><surname>Schuster</surname><given-names>P.</given-names></string-name></person-group><article-title>Fast folding and comparison of rna secondary structures</article-title>. <source>Monatsh. Chem.</source><year>1994</year>; <volume>125</volume>:<fpage>167</fpage>–<lpage>188</lpage>.</mixed-citation>
    </ref>
    <ref id="B98">
      <label>98.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B.</given-names></string-name>, <string-name><surname>Xu</surname><given-names>J.H.</given-names></string-name>, <string-name><surname>Fan</surname><given-names>S.X.</given-names></string-name>, <string-name><surname>Xu</surname><given-names>R.F.</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>J.Y.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.L.</given-names></string-name></person-group><article-title>PseDNA-Pro: DNA-binding protein identification by combining Chou's PseAAC and physicochemical distance transformation</article-title>. <source>Mol. Inf.</source><year>2015</year>; <volume>34</volume>:<fpage>8</fpage>–<lpage>17</lpage>.</mixed-citation>
    </ref>
    <ref id="B99">
      <label>99.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kumar</surname><given-names>M.</given-names></string-name>, <string-name><surname>Gromiha</surname><given-names>M.M.</given-names></string-name>, <string-name><surname>Raghava</surname><given-names>G.P.S.</given-names></string-name></person-group><article-title>SVM based prediction of RNA-binding proteins using binding residues and evolutionary information</article-title>. <source>J. Mol. Recognit.</source><year>2011</year>; <volume>24</volume>:<fpage>303</fpage>–<lpage>313</lpage>.<pub-id pub-id-type="pmid">20677174</pub-id></mixed-citation>
    </ref>
    <ref id="B100">
      <label>100.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>S.</given-names></string-name></person-group><article-title>RBPPred: predicting RNA-binding proteins from sequence using SVM</article-title>. <source>Bioinformatics</source>. <year>2017</year>; <volume>33</volume>:<fpage>854</fpage>–<lpage>862</lpage>.<pub-id pub-id-type="pmid">27993780</pub-id></mixed-citation>
    </ref>
    <ref id="B101">
      <label>101.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Callaway</surname><given-names>E.</given-names></string-name></person-group><article-title>It will change everything’: DeepMind's AI makes gigantic leap in solving protein structures</article-title>. <source>Nature</source>. <year>2020</year>; <volume>588</volume>:<fpage>203</fpage>–<lpage>204</lpage>.<pub-id pub-id-type="pmid">33257889</pub-id></mixed-citation>
    </ref>
    <ref id="B102">
      <label>102.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Altschul</surname><given-names>S.F.</given-names></string-name>, <string-name><surname>Koonin</surname><given-names>E.V.</given-names></string-name></person-group><article-title>Iterated profile searches with PSI-BLAST - a tool for discovery in protein databases</article-title>. <source>Trends Biochem. Sci.</source><year>1998</year>; <volume>23</volume>:<fpage>444</fpage>–<lpage>447</lpage>.<pub-id pub-id-type="pmid">9852764</pub-id></mixed-citation>
    </ref>
    <ref id="B103">
      <label>103.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ke</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Rao</surname><given-names>J.</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>H.</given-names></string-name>, <string-name><surname>Lu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Xiao</surname><given-names>N.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>Y.</given-names></string-name></person-group><article-title>Accurate prediction of genome-wide RNA secondary structure profile based on extreme gradient boosting</article-title>. <source>Bioinformatics</source>. <year>2020</year>; <volume>36</volume>:<fpage>4576</fpage>–<lpage>4582</lpage>.<pub-id pub-id-type="pmid">32467966</pub-id></mixed-citation>
    </ref>
    <ref id="B104">
      <label>104.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>W.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Brooker</surname><given-names>J.</given-names></string-name>, <string-name><surname>Lin</surname><given-names>H.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>L.</given-names></string-name>, <string-name><surname>Chou</surname><given-names>K.C.</given-names></string-name></person-group><article-title>PseKNC-General: a cross-platform package for generating various modes of pseudo nucleotide compositions</article-title>. <source>Bioinformatics</source>. <year>2015</year>; <volume>31</volume>:<fpage>119</fpage>–<lpage>120</lpage>.<pub-id pub-id-type="pmid">25231908</pub-id></mixed-citation>
    </ref>
    <ref id="B105">
      <label>105.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Horne</surname><given-names>D.S.</given-names></string-name></person-group><article-title>Prediction of protein helix content from an auto-correlation analysis of sequence hydrophobicities</article-title>. <source>Biopolymers</source>. <year>1988</year>; <volume>27</volume>:<fpage>451</fpage>–<lpage>477</lpage>.<pub-id pub-id-type="pmid">3359010</pub-id></mixed-citation>
    </ref>
    <ref id="B106">
      <label>106.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sokal</surname><given-names>R.R.</given-names></string-name>, <string-name><surname>Thomson</surname><given-names>B.A.</given-names></string-name></person-group><article-title>Population structure inferred by local spatial autocorrelation: an example from an Amerindian tribal population</article-title>. <source>Am. J. Phys. Anthropol.</source><year>2006</year>; <volume>129</volume>:<fpage>121</fpage>–<lpage>131</lpage>.<pub-id pub-id-type="pmid">16261547</pub-id></mixed-citation>
    </ref>
    <ref id="B107">
      <label>107.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Feng</surname><given-names>Z.P.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>C.T.</given-names></string-name></person-group><article-title>Prediction of membrane protein types based on the hydrophobic index of amino acids</article-title>. <source>J. Protein Chem.</source><year>2000</year>; <volume>19</volume>:<fpage>269</fpage>–<lpage>275</lpage>.<pub-id pub-id-type="pmid">11043931</pub-id></mixed-citation>
    </ref>
    <ref id="B108">
      <label>108.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>J.H.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>Y.M.</given-names></string-name>, <string-name><surname>Liao</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>B.</given-names></string-name></person-group><article-title>iEsGene-ZCPseKNC: identify essential genes based on Z curve pseudo k-tuple nucleotide composition</article-title>. <source>Ieee Access</source>. <year>2019</year>; <volume>7</volume>:<fpage>165241</fpage>–<lpage>165247</lpage>.</mixed-citation>
    </ref>
    <ref id="B109">
      <label>109.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname><given-names>J.</given-names></string-name>, <string-name><surname>Lu</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Xu</surname><given-names>R.</given-names></string-name>, <string-name><surname>He</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>H.</given-names></string-name></person-group><article-title>EL_PSSM-RT: DNA-binding residue prediction by integrating ensemble learning with PSSM relation transformation</article-title>. <source>BMC Bioinformatics</source>. <year>2017</year>; <volume>18</volume>:<fpage>379</fpage>.<pub-id pub-id-type="pmid">28851273</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
