<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neuroinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neuroinform</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neuroinform.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1662-5196</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8222613</article-id>
    <article-id pub-id-type="doi">10.3389/fninf.2021.656486</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Characterizing Network Search Algorithms Developed for Dynamic Causal Modeling</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Aranyi</surname>
          <given-names>Sándor Csaba</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/1205316/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nagy</surname>
          <given-names>Marianna</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Opposits</surname>
          <given-names>Gábor</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Berényi</surname>
          <given-names>Ervin</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/216991/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Emri</surname>
          <given-names>Miklós</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/1196909/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Division of Nuclear Medicine and Translational Imaging, Department of Medical Imaging, Faculty of Medicine, University of Debrecen</institution>, <addr-line>Debrecen</addr-line>, <country>Hungary</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Division of Radiology and Imaging Science, Department of Medical Imaging, Faculty of Medicine, University of Debrecen</institution>, <addr-line>Debrecen</addr-line>, <country>Hungary</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Ludovico Minati, Tokyo Institute of Technology, Japan</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Karl Friston, University College London, United Kingdom; Peter Zeidman, University College London, United Kingdom</p>
      </fn>
      <corresp id="c001">*Correspondence: Sándor Csaba Aranyi <email>aranyi.csaba@med.unideb.hu</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>10</day>
      <month>6</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>15</volume>
    <elocation-id>656486</elocation-id>
    <history>
      <date date-type="received">
        <day>20</day>
        <month>1</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>07</day>
        <month>5</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2021 Aranyi, Nagy, Opposits, Berényi and Emri.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Aranyi, Nagy, Opposits, Berényi and Emri</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Dynamic causal modeling (DCM) is a widely used tool to estimate the effective connectivity of specified models of a brain network. Finding the model explaining measured data is one of the most important outstanding problems in Bayesian modeling. Using heuristic model search algorithms enables us to find an optimal model without having to define a model set a priori. However, the development of such methods is cumbersome in the case of large model-spaces. We aimed to utilize commonly used graph theoretical search algorithms for DCM to create a framework for characterizing them, and to investigate relevance of such methods for single-subject and group-level studies. Because of the enormous computational demand of DCM calculations, we separated the model estimation procedure from the search algorithm by providing a database containing the parameters of all models in a full model-space. For test data a publicly available fMRI dataset of 60 subjects was used. First, we reimplemented the deterministic bilinear DCM algorithm in the ReDCM R package, increasing computational speed during model estimation. Then, three network search algorithms have been adapted for DCM, and we demonstrated how modifications to these methods, based on DCM posterior parameter estimates, can enhance search performance. Comparison of the results are based on model evidence, structural similarities and the number of model estimations needed during search. An analytical approach using Bayesian model reduction (BMR) for efficient network discovery is already available for DCM. Comparing model search methods we found that topological algorithms often outperform analytical methods for single-subject analysis and achieve similar results for recovering common network properties of the winning model family, or set of models, obtained by multi-subject family-wise analysis. However, network search methods show their limitations in higher level statistical analysis of parametric empirical Bayes. Optimizing such linear modeling schemes the BMR methods are still considered the recommended approach. We envision the freely available database of estimated model-spaces to help further studies of the DCM model-space, and the ReDCM package to be a useful contribution for Bayesian inference within and beyond the field of neuroscience.</p>
    </abstract>
    <kwd-group>
      <kwd>fMRI</kwd>
      <kwd>dynamic causal modeling</kwd>
      <kwd>search algorithm</kwd>
      <kwd>network topology</kwd>
      <kwd>model-space</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">National Research, Development and Innovation Fund of Hungary</funding-source>
        <award-id rid="cn001">2020-4.1.1-TKP2020</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn002">National Brain Research Program</funding-source>
        <award-id rid="cn002">2017-1.2.1-NKP-2017-00002</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="7"/>
      <table-count count="3"/>
      <equation-count count="2"/>
      <ref-count count="29"/>
      <page-count count="14"/>
      <word-count count="9610"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>In recent years of neuroscience, increasing attention is drawn toward brain connectivity studies. Non-invasive techniques, like functional magnetic resonance imaging (fMRI) and electroencephalography (EEG), have gained popularity for this purpose. Dynamic causal modeling (DCM) is a continuously developing Bayesian framework for estimating effective neuronal connectivity between brain regions. It models neuronal signal alteration underlying the fMRI or EEG data for predicting network connectivity, their modulations and the effects of experimental inputs, while physiological parameters of the measured data are also accounted for. Initially, DCM was a hypothesis-driven method, useful to compare a small number of models to test neurobiologically relevant questions (Friston et al., <xref rid="B6" ref-type="bibr">2003</xref>), and using Bayesian model selection to decide which model has the highest evidence (Penny et al., <xref rid="B17" ref-type="bibr">2004</xref>). However, numerous studies focus on exploring a systematically built model-space to find the best fitting model for the data, and to draw an inference from it (Pool et al., <xref rid="B18" ref-type="bibr">2014</xref>; Warren et al., <xref rid="B27" ref-type="bibr">2019</xref>). More recent DCM development enables the comparison of model families of hundreds, or thousands of models along common network properties, and inferencing the parameters of the averaged model of the winning family using Bayesian model comparison and subsequent averaging (Penny et al., <xref rid="B16" ref-type="bibr">2010</xref>). For group analysis the currently recommended standard procedure involves the model inversion of a fully connected DCM model and compute parametric empirical Bayes (PEB) over subjects (Friston et al., <xref rid="B9" ref-type="bibr">2016</xref>). Then, we can test our hypotheses on model commonalities and differences by estimating posteriors of any nested model with a method called Bayesian model reduction (BMR), or perform an automatic search among all nested models to discover connections that most likely contribute to the final model evidence (Zeidman et al., <xref rid="B29" ref-type="bibr">2019b</xref>).</p>
    <p>Methods for searching for the model that most likely explains the measured data are also known as structure learning, which is one of the most important outstanding problems in Bayesian modeling. However, discovering large model-spaces is not a trivial task, considering that the number of alternative models grows exponentially with the number of network nodes and external effects. For this reason searching for an optimal solution is less advised in current research. A discovery method for causal networks have already been developed, in the Bayesian framework, to perform <italic>post-hoc</italic> model selection. This method refers to the greedy search to find parameters to remove from the pre-estimated fully connected model, which do not contribute to the final model evidence (Friston and Penny, <xref rid="B8" ref-type="bibr">2011</xref>). This procedure also exploits the efficiency of BMR. With this technique it is only necessary to invert the full model and then estimate any nested models in milliseconds, which is useful when estimating large number of models (Friston et al., <xref rid="B9" ref-type="bibr">2016</xref>). Nonetheless, the standard approach to estimate each model separately still remains relevant, because it is still unclear whether BMR remains robust to nonlinearities, such as the hemodynamic forward model DCM uses (Buxton et al., <xref rid="B3" ref-type="bibr">1998</xref>). Currently, searching methods to find the optimal model structure through fully inverting DCM models have not yet been thoroughly investigated for DCM.</p>
    <p>A search algorithm aims to find an optimal solution within the boundaries of a search space that meets or approximates predefined criteria. An iterative search algorithm may start with an initial structure within the search space (e.g., an empty or a fully connected network). Then, we construct topological alternatives to this network by adding or subtracting edges between network nodes. Finally, we evaluate the set of alternatives based on some approximation of the model evidence, for example, Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) to select the best ones for the next iteration. In DCM variational free-energy generally gives a better approximation of the log evidence, and is preferred over AIC and BIC (Penny, <xref rid="B15" ref-type="bibr">2012</xref>). For Bayesian networks, many procedures exist for the data-driven discovery of network graph structures (Smith et al., <xref rid="B23" ref-type="bibr">2011</xref>; Mumford and Ramsey, <xref rid="B14" ref-type="bibr">2014</xref>). These include simple greedy equivalence search (GES) methods (Ramsey et al., <xref rid="B20" ref-type="bibr">2011</xref>), and more complex multi-level algorithms combining greedy search with simulated annealing (Adabor et al., <xref rid="B1" ref-type="bibr">2015</xref>).</p>
    <p>In the case of DCM, every model search method attempts to find the model with the highest evidence based on fMRI data. As DCM models can be represented as graphs, some commonly used heuristic network optimization algorithms can easily be adapted to search through the DCM model-space. The computational difficulties limit the possibilities to develop searching methods for DCM. In the literature, only a few optimization methods are available to search for the best fitting model. Pyka et al. (<xref rid="B19" ref-type="bibr">2011</xref>) investigated the effectiveness of genetic algorithms (GA) compared to a fully randomized search. They demonstrated that GA found better DCM models by estimating fewer models than brute-force methods.</p>
    <p>In DCM, one can choose from multiple possibilities for network discovery. On the subject-level the most straightforward path to follow is to directly invert each model alternatives along the search path individually using the variational Laplace (VL) algorithm (Friston et al., <xref rid="B7" ref-type="bibr">2007</xref>). This method is the slowest and it is possible for different models to fall into different local minima during the estimation procedure. Alternatively, one can apply Bayesian model reduction on any DCM in relation to the fully connected model. This method also allows for using <italic>post-hoc</italic> model selection to find an optimal solution in a more analytic approach. The advantage of BMR is that it assumes that all models are evaluated around the same minima of free-energy. However, the approximation of model evidence is not known exactly for DCMs, as BMR assumes that the reduced posterior parameter distributions are Gaussian, which might not be appropriate due to nonlinearities in the model (Friston et al., <xref rid="B9" ref-type="bibr">2016</xref>). Lastly, model search methods can be performed on the group-level among nested PEB models.</p>
    <p>In practice searching through a large model-space assumes that each model is potentially equally likely a priori, while some models are usually more plausible than others, which is neglected during Bayesian model comparison. Furthermore, the prior probability density in DCM are composed to decrease the risk of overfitting the data. However, in large model-spaces it is possible to find a setting of priors that allows to fit the data, possibly overfitting it. Finally, interpretation of results of automatic searching methods without any hypothesis about the model is difficult, and hardly reproducible with different subjects or input data. For these reasons it is practical to form model families (of hundreds or thousands of models), each corresponding to a hypothesis about the underlying network, and assign each model alternative to one of these families. In this setup model search algorithms may prove useful if they can recover properties of the winning family.</p>
    <p>Considering the topological complexity of DCM models and the underlying neurobiological modeling, as well, the high computational demand of DCM still limits the efficiency to explore large model-spaces. The algorithm for model inversion, used by a variational Laplace scheme to estimate the physiological and connection parameters of the causal model, is computationally costly. This is especially true for DCM for fMRI, because they integrate the neuronal and the hemodynamic states of each region, as well. As an obvious solution to speeding up DCM by graphical processing units, GPU-enhanced calculations was accomplished previously for fMRI (Aponte et al., <xref rid="B2" ref-type="bibr">2016</xref>) as well as event-related potentials (Wang et al., <xref rid="B26" ref-type="bibr">2013</xref>). For our computations, we decided to follow the procedures of the original algorithm. We introduce a complete reimplementation of DCM facilitating efficient computing libraries to gain speed. We refer to this version of the software as ReDCM in the following.</p>
    <p>In this study, we aimed to create a computational framework for developing and characterizing different DCM-adapted model search methods, and investigate their uses in subject- and group-level scenarios. To focus on different searching methods we finessed the computational burden of DCM model inversion by pre-computing model evidence of every possible model in the model-space generated from the fMRI dataset used in Zeidman et al. (<xref rid="B28" ref-type="bibr">2019a</xref>). This enables to compare an arbitrary amount of DCM models without fitting them on the fly. Looking up estimation results from a pre-computed database helps efficient testing and development of search procedures. Furthermore, knowing the best fitting model of the model-space, we can easily measure the performance of the investigated methods. We adapted three model search algorithms for DCM that is available for network science. These are the above mentioned GES and GA algorithms, and a variant of the greedy method based on Hamming-distance of model structures. We refer to this algorithm as GHD from now on. We also looked into the possibilities to improve topological model search procedures by taking into account the DCM parameter estimates from previously reached models during the search. Finally, we characterized and compared the efficiency of these algorithms applied in relation to single subject analysis, family-wise inference and group-level PEB modeling.</p>
  </sec>
  <sec sec-type="materials and methods" id="s2">
    <title>2. Materials and Methods</title>
    <sec>
      <title>2.1. Mathematical Background of Dynamic Causal Modeling</title>
      <p>We reviewed the DCM mathematics from the point of view of full model-space generation and reimplementation. For the generation of all DCM models, we examined how the DCM implementation handles the topology of internal and external interactions of the neuronal networks. The mathematical background of this topology can be revealed in the neuronal state equation of DCM.</p>
      <p>DCM for fMRI models neural interactions between brain regions of a specified network. At any time point, the state of neuronal activity of each region depends on its neural state <italic>x</italic> at the previous time point and can be perturbed by experimentally driven stimuli <italic>u</italic>. In DCM the temporal change of the neuronal state vector is modeled using a bilinear Taylor series approximation, truncated to its linear terms (Stephan et al., <xref rid="B24" ref-type="bibr">2008</xref>). This scheme can model any nonlinear function <italic>f</italic>(<italic>x, u</italic>) around the system's resting point. These time-dependent dynamics can be expressed as the differential equation below:</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M1">
          <mml:mtable class="eqnarray" columnalign="left left left">
            <mml:mtr>
              <mml:mtd>
                <mml:mtext>f</mml:mtext>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mtext>x</mml:mtext>
                    <mml:mo>,</mml:mo>
                    <mml:mtext>u</mml:mtext>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>d</mml:mi>
                    <mml:mi>x</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>d</mml:mi>
                    <mml:mi>t</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mo>≈</mml:mo>
                <mml:mi>f</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mn>0</mml:mn>
                    <mml:mo>,</mml:mo>
                    <mml:mn>0</mml:mn>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>+</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munder class="msub">
                    <mml:mrow>
                      <mml:mstyle displaystyle="true">
                        <mml:munder accentunder="false">
                          <mml:mrow>
                            <mml:mfrac>
                              <mml:mrow>
                                <mml:mi>∂</mml:mi>
                                <mml:mi>f</mml:mi>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mi>∂</mml:mi>
                                <mml:mi>x</mml:mi>
                              </mml:mrow>
                            </mml:mfrac>
                          </mml:mrow>
                          <mml:mo>︸</mml:mo>
                        </mml:munder>
                      </mml:mstyle>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>A</mml:mi>
                    </mml:mrow>
                  </mml:munder>
                </mml:mstyle>
                <mml:mi>x</mml:mi>
                <mml:mo>+</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munder class="msub">
                    <mml:mrow>
                      <mml:mstyle displaystyle="true">
                        <mml:munder accentunder="false">
                          <mml:mrow>
                            <mml:mfrac>
                              <mml:mrow>
                                <mml:msup>
                                  <mml:mrow>
                                    <mml:mi>∂</mml:mi>
                                  </mml:mrow>
                                  <mml:mrow>
                                    <mml:mn>2</mml:mn>
                                  </mml:mrow>
                                </mml:msup>
                                <mml:mi>f</mml:mi>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mi>∂</mml:mi>
                                <mml:mi>x</mml:mi>
                                <mml:mi>∂</mml:mi>
                                <mml:mi>u</mml:mi>
                              </mml:mrow>
                            </mml:mfrac>
                          </mml:mrow>
                          <mml:mo>︸</mml:mo>
                        </mml:munder>
                      </mml:mstyle>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>B</mml:mi>
                    </mml:mrow>
                  </mml:munder>
                </mml:mstyle>
                <mml:mi>x</mml:mi>
                <mml:mi>u</mml:mi>
                <mml:mo>+</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munder class="msub">
                    <mml:mrow>
                      <mml:mstyle displaystyle="true">
                        <mml:munder accentunder="false">
                          <mml:mrow>
                            <mml:mfrac>
                              <mml:mrow>
                                <mml:mi>∂</mml:mi>
                                <mml:mi>f</mml:mi>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mi>∂</mml:mi>
                                <mml:mi>u</mml:mi>
                              </mml:mrow>
                            </mml:mfrac>
                          </mml:mrow>
                          <mml:mo>︸</mml:mo>
                        </mml:munder>
                      </mml:mstyle>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>C</mml:mi>
                    </mml:mrow>
                  </mml:munder>
                </mml:mstyle>
                <mml:mi>u</mml:mi>
                <mml:mo>,</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where the network dynamics are computed around the <italic>f</italic>(0, 0) point. Jacobian matrices <italic>A</italic>, <italic>B</italic> and <italic>C</italic> are parameters of the three different kinds of neuronal interactions modeled by DCM: the endogenous connectivity of the network, the modulatory effects of external input on the connections and the direct or driving effects of the stimuli (input) on the regions, respectively. These parameters define the model topology describing the inter-regional connections, and the external stimuli induced regional activity alterations and regional interaction modulations.</p>
      <p>The temporal neuronal activation needs to be combined with a modality-specific forward model to explain regional BOLD (blood oxygen level dependent) fMRI responses. DCM for fMRI uses a hemodynamic model based on the Balloon-Windkessel (or simply Balloon) model (Buxton et al., <xref rid="B3" ref-type="bibr">1998</xref>), that is adapted and extended for DCM (Friston et al., <xref rid="B10" ref-type="bibr">2000</xref>). In this model, the hemodynamic states are a function only of the neuronal state of the regions and represent the volume and deoxyhemoglobin content of the flowing blood. The full forward model of neuronal and hemodynamic state equations is used to predict the BOLD signal <italic>h</italic>(<italic>u</italic>, Θ) of network regions, where Θ are the parameters of the neuronal and the hemodynamic models. Because of the nonlinearities in the Balloon-model, the differential state equations need to be integrated numerically that can be extremely demanding on computational power.</p>
      <p>For parameter estimation, a Bayesian framework is used. An iterative variational Laplace (VL) algorithm optimizes the maximum a posteriori (MAP) estimate of the free model parameters (Friston et al., <xref rid="B7" ref-type="bibr">2007</xref>). By integrating out the dependencies between parameters we obtain model evidence, which can be used for model selection or comparison. In DCM, variational free-energy (Fe) is used to approximate model evidence. The free-energy balances between the fit of the model to the data and complexity, like the number of free parameters in the model (Stephan et al., <xref rid="B25" ref-type="bibr">2007</xref>). Hence, Fe is useful for comparing models while eliminating the effects of overfitting the data on model evidence.</p>
    </sec>
    <sec>
      <title>2.2. Optimized DCM Implementation</title>
      <p>Considering the computational demands of DCM, originally available as part of the Statistical Parametric Mapping (SPM, <ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/">http://www.fil.ion.ucl.ac.uk/spm/</ext-link>) Matlab toolbox, evaluating large amounts of models requires unmanageable processor time. To overcome this limitation we reimplemented the deterministic bilinear DCM12 algorithm (build v6225) in the R package ReDCM. (DCM12 refers to the actual implementation of the DCM algorithm in the given SPM version). The whole estimation procedure of variational Laplace and the DCM forward models for fMRI (neural and hemodynamic states) is available in ReDCM. The R programming language provided us a feasible environment to implement the Bayesian estimation framework, data analysis methods and model search algorithms as well. Besides DCM for fMRI, ReDCM also implements BOLD signal simulation, Bayesian model selection using fixed effects statistics or random effects with Gibbs-sampling (Penny et al., <xref rid="B16" ref-type="bibr">2010</xref>), and a separate tool to observe hemodynamic response function (HRF) for a set of Balloon-model parameters.</p>
      <p>Previous studies have identified the computation bottlenecks of DCM (Aponte et al., <xref rid="B2" ref-type="bibr">2016</xref>), namely the integration of differential state equations describing temporal neuronal and hemodynamic changes, which needs to be optimized. These parts of the code were ported into C language using the GNU Scientific Library (GSL) (Galassi et al., <xref rid="B11" ref-type="bibr">2006</xref>) for efficient matrix calculations. Four virtual machines with 48 CPU cores each were acquired in Microsoft's Azure cloud platform (Copeland et al., <xref rid="B5" ref-type="bibr">2015</xref>) to utilize high-performance computing facilities for estimating multiple DCM models simultaneously.</p>
      <p>We measured the performance of ReDCM and DCM12 without any parallelization techniques to quantify the computing efficiency we gained. For this analysis we generated synthetic BOLD-signal data of varying scan length between 200 and 1,200 time points and DCM models with different model sizes, containing 3, 5, and 7 interconnected regions of interest. We show the average runtime of iterative VL cycles, measured with both implementations, estimating parameters of each synthetic model. Each model had two external stimulating effect to drive regional state dynamics. For the BOLD time-series simulation we used the ReDCM implementation of appropriate functions from SPM.</p>
    </sec>
    <sec>
      <title>2.3. Model-Space Generation for the Semantic Decision Task</title>
      <p>In the case of a neural network, the number of all mathematically possible models (i.e., the cardinality of the full model-space, <italic>N</italic><sub><italic>ms</italic></sub>) increases hyper-exponentially, depending on the number of regions and experimental inputs. As the parameter priors can be expressed as binary variables (connected or not connected), the number of possible bilinear models can be computed with a simple expression:</p>
      <disp-formula id="E2">
        <label>(2)</label>
        <mml:math id="M2">
          <mml:mtable class="eqnarray" columnalign="left left left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>l</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>g</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mn>2</mml:mn>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>N</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>m</mml:mi>
                    <mml:mi>s</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mi>n</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mo>-</mml:mo>
                    <mml:mi>n</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>+</mml:mo>
                <mml:mi>i</mml:mi>
                <mml:mo>*</mml:mo>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mi>n</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:mrow>
                    </mml:msup>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>+</mml:mo>
                <mml:mi>i</mml:mi>
                <mml:mo>*</mml:mo>
                <mml:mi>n</mml:mi>
                <mml:mo>,</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where <italic>n</italic> is the number of regions and i is the number of input functions. The first additive term describes the endogenous network connectivity (<italic>A</italic>), the second adds the number of possible modulatory effects of inputs (<italic>B</italic>) and the third counts the direct effects on each region (<italic>C</italic>). We subtracted the number of regions from terms related to the A matrix, because self-connections always represent self-inhibitory effects which need to be estimated.</p>
      <p>For test data, we used the fMRI BOLD dataset freely available as supplementary data from Zeidman et al. (<xref rid="B28" ref-type="bibr">2019a</xref>), which is used to investigate laterality of semantic processing before (Seghier et al., <xref rid="B22" ref-type="bibr">2011</xref>). This consists of the same specified DCM model for 60 subjects, from which we used the first 10 for individual level computations, and data for group-wise PEB model. The experimental design involves three conditions: “Pictures,” “Words” includes onset for the corresponding semantic decision trials and “Task” includes all trials. The network architecture consists of four regions in the frontal cortex, responding to language processing: left ventral (lvF), left dorsal (ldF), right ventral (rvF), and right dorsal (rdF). For keeping consistency with previous work, and for computational reasons, we constrained the full model-space by fixing the <italic>C</italic> matrix so that only the “Task” condition is used as driving inputs for each region, while “Pictures” and “Words” are used for modulatory effects. In accordance with Zeidman et al. (<xref rid="B28" ref-type="bibr">2019a</xref>), <xref ref-type="fig" rid="F1">Figure 1</xref> shows the model considered as fully connected in this experiment. Keeping the constraints described here in mind this semantic decision network induces a model-space of 65,536 possible nested models to consider for each of the 10 subjects.</p>
      <fig id="F1" position="float">
        <label>Figure 1</label>
        <caption>
          <p>Network scheme for semantic decision task. A DCM model example to explain brain functions during semantic processing of words and pictures, originally investigated by /Seghier 2011/. The experiment consists of four frontal brain regions: left ventral (lvF), left dorsal (ldF), right ventral (rvF) and right dorsal (rdF). This network is examined in a task designed to involve three conditions: “Pictures” and “Words” includes onsets for the corresponding semantic decision trials and “Task” includes all trials. We generated a model-space of 65,536 models with every possible combination of endogenous connectivity matrix A, along with their experimental modulation, denoted by B1, B2, and B3. We fixed the direct effects, described by matrix C, to “Task” driving each region as experimental input. On the figure the network nodes are overlain on a coronal section of the brain captured from the average brain template of the Montreal Neurological Institute (Grabner et al., <xref rid="B12" ref-type="bibr">2006</xref>).</p>
        </caption>
        <graphic xlink:href="fninf-15-656486-g0001"/>
      </fig>
      <p>Group-level analysis is designed to inference on different lateral responses between left and right regions during the processing the semantic content of familiar words, quantified by the “Laterality Index” (LI). In the group-level PEB analysis several effects are modeled similarly to a general linear modeling scheme. Most importantly the commonalities, or main effect among all subjects and the LI to model subject variability. Handedness, gender and age are also included to capture variance no interest. Detailed description of the experiment is found in Seghier et al. (<xref rid="B22" ref-type="bibr">2011</xref>).</p>
      <p>Taking advantage of the ReDCM implementation and high performance computing, we estimated all models of the constrained model-space, and organized the estimated parameters of all models into an easy to handle data table. Description of this database is provided in <xref rid="T1" ref-type="table">Table 1</xref>. Alternatively, we also estimated the model-space using BMR. To examine both statistical and structural attributes of the model-space, we expanded this table with the DCM models' free-energy (Fe), and their Hamming-distance (Hd) relative to the model with the highest evidence. If we represent <italic>A</italic>, <italic>B</italic>, and <italic>C</italic> matrices as directed cyclic graph structures, we can describe the topological difference of any two models by their Hamming-distance. The Hamming-distance of two models is defined as the number of different graph edges that describe their connectivity.</p>
      <table-wrap id="T1" position="float">
        <label>Table 1</label>
        <caption>
          <p>Model-space database description.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Name</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Abbreviation</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Description</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Model identifier</td>
              <td valign="top" align="left" rowspan="1" colspan="1">ID</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Unique numerical identifier of a model.</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Free-energy</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Fe</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Estimated free-energy of a model.</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Free-energy difference</td>
              <td valign="top" align="left" rowspan="1" colspan="1">dFe</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Difference in free-energy relative to the best fitting model.</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Hamming-distance</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Hd</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Hamming-distance of a model to the best fitting model.</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Vectorized model priors</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Bitvector</td>
              <td valign="top" align="left" rowspan="1" colspan="1">A bitvector representing the model structure topologically, constructed from the Vectorized model priors of A, B, and C matrices.</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Endogenous connectivity</td>
              <td valign="top" align="left" rowspan="1" colspan="1">A</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Matrix of endogenous connection parameter estimates.</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Modulatory effects</td>
              <td valign="top" align="left" rowspan="1" colspan="1">B1, B2, B3</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Matrices of connection modulation parameters for each experimental input.</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Driving input</td>
              <td valign="top" align="left" rowspan="1" colspan="1">C</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Direct or driving effect of experimental stimuli.</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Connection probability</td>
              <td valign="top" align="left" rowspan="1" colspan="1">pA</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Matrix of endogenous connection parameter probabilities.</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Modulation probability</td>
              <td valign="top" align="left" rowspan="1" colspan="1">pB1, pB2, pB3</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Matrices of modulation parameter probabilities for each experimental input.</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Input probability</td>
              <td valign="top" align="left" rowspan="1" colspan="1">pC</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Probability of driving effect of experimental stimuli.</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>The database of the model-space consists of the parameter estimation results of each DCM model as a computer file that are connected to a summary table by a model identifier. This table collects the basis of model comparison information in statistical (free-energy, connection matrix parameter estimates) and graph theoretical (Hamming-distance, model priors) space</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>2.4. Model Search Methods</title>
      <p>Search algorithms aim to find the optimal model with the combination of connectivity parameters that yield the highest evidence of DCM model estimation on the fMRI data. We adapted three different model search algorithms to DCM and characterized their performance. As the connectivity parameters of <italic>A</italic>, <italic>B</italic>, and <italic>C</italic> matrices define the model-space, searching through it is actually about finding the optimal model with the combination of connectivity parameters that yields the highest evidence of DCM model estimation on the fMRI data. The comparison of model search algorithms is based on two main factors: the difference between the estimated Fe of the found model and the best fitting model of the model-space (i.e., how optimal is the result of search), and also the number of models estimated until convergence is reached (i.e., how fast the algorithm converges to the optimum). Based on the applied search method the number of models considered can still be relatively high. However, replacing DCM computations with looking up records from the already estimated model-space, as depicted in <xref ref-type="fig" rid="F2">Figure 2</xref>, makes developing and testing new search algorithms faster.</p>
      <fig id="F2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>Flowchart of model search algorithms for DCM. The flowchart on the left side panel depicts the most simple schematic of model search methods in a DCM model-space. On the right side we show the changes to the regular procedure that may optimize the algorithm in terms of number of estimated models or search results (green parts), and helps rapid characterization of model search methods using model-space lookup (red parts). In any case the algorithm starts with an initially selected model (or set of models) <italic>M</italic><sub>0</sub> that is used to select models <inline-formula><mml:math id="M3"><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> with an arbitrary method for DCM computation. Then we can select the best fitting model (or models) <italic>M</italic><sub>1</sub> of the selected population with Bayesian model selection, which is used to generate the next set of models to compare in the next iteration. This procedure continues until we cannot find an improved variation of the previously estimated models. As shown on the right side image, posterior parameter estimates of previously reached models <inline-formula><mml:math id="M4"><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> can also be used to reduce or manipulate the selected population and improve search efficiency.</p>
        </caption>
        <graphic xlink:href="fninf-15-656486-g0002"/>
      </fig>
      <p>Another way to improve model search efficiency is to omit models from the current iteration of the algorithm, that differs from previously computed DCM models only in parameters that don't effectively change model evidence based on the Bayesian estimation procedure. Thus, we created optimized versions of the following model search algorithms that skip DCM models that adds or subtracts parameters that are receding or exceeding a definite posterior parameter probability in previously estimated models. Doing this we ensure that connections with high (or low) average probability don't get deleted (or added) by any alternative models. This search space reduction method is outlined with green on <xref ref-type="fig" rid="F2">Figure 2</xref>. The optimization, informed by DCM model estimation, may significantly reduce the number of reached models during a search procedure. However, it needs to be applied with caution as it may introduce possibilities to deflect the search path into a local minima.</p>
      <sec>
        <title>2.4.1. Greedy Equivalence Search—GES</title>
        <p>Our greedy search algorithm is based on the Greedy Equivalence Search described by Ramsey et al. (<xref rid="B21" ref-type="bibr">2010</xref>). It starts off by selecting an initial, randomized dynamic causal model, and every model with one removed connectivity parameter is evaluated with DCM. Then we select the winning model structure with the highest Fe. After that, we estimate each nested model obtained by removing one possible connectivity parameter from the winning model, keeping consistency with model-space restrictions. When we reach a stage when removing the connections do not change the winning model, a backward procedure is started that adds connectivity parameters to the selected model. Forward and backward steps alternate until no model can be found to improve model evidence. The advantage of this method is it always converges at a particular local maximum based on the initial model; however, it cannot leap through them and may never find the best model.</p>
        <p>The optimized algorithm, denoted as GES', takes the posterior parameter probabilities into account when generating the set of models to be estimated. Connection parameters can be removed from the model only if their mean posterior probability is below 0.9. With this modification we fix all connections that have a significant impact on model evidence.</p>
      </sec>
      <sec>
        <title>2.4.2. Greedy Hamming-Distance Search—GHD</title>
        <p>A more general case of GES considers the models as vectors of binary connectivity parameters, rather than graph structures. Similarly, the Hamming-distance based Greedy Search (GHD) starts with a randomized initial model and estimates every model that is at most 1 Hamming-distance far from that model. At the next stage, the model with the highest evidence is selected to repeat the procedure. When no further winning model is found, the algorithm terminates. The idea behind this algorithm is that it is assumed that the models around the best fitting model also have high model evidence (Pyka et al., <xref rid="B19" ref-type="bibr">2011</xref>). Another advantage of this method is that it incorporates forward and backward search at the same time, as well. We applied the same optimization for GHD' that was applied on the GES' algorithm.</p>
      </sec>
      <sec>
        <title>2.4.3. Genetic Algorithm—GA</title>
        <p>Genetic algorithm is a widely used concept for different optimization and search problems otherwise challenging to solve procedurally. The candidates of the solution (called individuals or chromosomes) are represented as a set of attributes whose combinations describe the individual. These attributes are combined or swapped between individuals in a population of candidates and simulate genetic operations, like crossovers or mutations, to create more viable individuals. In our case, the models are the individuals and its connectivity parameters are the attributes.</p>
        <p>The genetic algorithm we use is described in details by Pyka et al. (<xref rid="B19" ref-type="bibr">2011</xref>). The genetic code of the models are represented as binary bit vectors of connectivity parameters. First, the procedure selects a random model from the search space and creates three variations of it by randomly changing some bits in the code. Second, using these four individuals, we generate 16 new codes by crossover and mutation genetic operators. Crossover is performed between two chromosomes by swapping a section of the code between two randomly chosen crossover points. A mutation occurs on the new genetic codes with a probability of 50% and changes two to eight randomly selected parameters. If a created model does not satisfy the model-space constraints described above or the code has already been considered during the algorithm, a new model is generated until we have a population of 20 models. Third, all models are estimated by DCM, and Bayesian model selection (Penny et al., <xref rid="B17" ref-type="bibr">2004</xref>) is performed to select the best four models that enter the next iteration of the GA. This particular use of model evidence or free energy for a genetic algorithm provides a nice metaphor for an natural selection as nature's way of performing Bayesian model selection. In other words, there are formulations of evolution in terms of minimizing free energy or maximizing adaptive fitness, where adaptive fitness is simply the marginal likelihood of a phenotype (Campbell, <xref rid="B4" ref-type="bibr">2016</xref>). This procedure stops when no new model is selected in the last three iterations.</p>
        <p>Similarly to previously specified search methods, our modified GA' algorithm is also informed by posterior parameter probabilities. Each model that contains connections with average probability <italic>p</italic> &lt; 0.3 is replaced with a new model that is mutated from the population.</p>
      </sec>
    </sec>
    <sec>
      <title>2.5. Model Search Characteristics</title>
      <p>Two routes of individual-level model search were followed. In the first one we performed search among separately estimated DCM models using the adapted algorithms. In this scheme we can compare search efficiency of the GES, GHD, and GA algorithms on the DCM model-space. Another approach involves using BMR to derive model evidence and posterior parameter estimates from the fully connected model. In this BMR model-space the same search methods can be applied. Additionally, <italic>post-hoc</italic> model search results can be compared with topological search methods.</p>
      <p>We analyzed each adapted graph-based model search method from the aspects of model fit (Fe), graph structure of the found model (Hd from the best model), and the number of estimated models (N) until the algorithm converges. Note that the GES and GHD procedures are inherently deterministic methods, and always find the same model with the same initialization. However, randomized initialization allows us to measure search performance more accurately. The stochastic methods such as GA, has a different convergence point each run regardless of the initial set of models. Consequently, we derived the efficiency of the implemented search algorithms from 20 consecutive runs for each subject's data, and assessed the model search characteristics by computing the results' mean and standard deviation.</p>
    </sec>
    <sec>
      <title>2.6. Family-Wise Inference</title>
      <p>The model families should be created to correspond to hypotheses about network structure attributes that are of interest by the experimenter. In case of BMS, both the null hypothesis and alternative hypotheses are compared to each other to make inference from the family that most likely to describe the structure of the network.</p>
      <p>In case of our dataset three meaningful separation of the model-space can be made (Zeidman et al., <xref rid="B28" ref-type="bibr">2019a</xref>). In the language related task of semantic processing of shown words and pictures it is more likely that words will have more impact modulating the connections in the network. Also, language processing is considered to dominantly activate regions in the left hemisphere, also with some right side activation, and it might be interesting to see dorsoventral separation of brain function during task. Along these observations three different separation of the model-space can be made: (1) network connectivity is modulated during processing words or pictures or both; (2) connectivity of left side, right side regions, or both sides is modulated; (3) and ventral or dorsal frontal regions are more involved during task or both. The dataset contains 27 base modulation models separated into three equally distributed sets of models for each of the three questions asked. Along these different settings of modulatory effects we assigned every combination of endogenous connectivity parameters to the corresponding model family. This means 2<sup>8</sup> = 256 models assigned to each base models, as there are eight free parameters found in the <italic>A</italic> matrix. We then performed a random effects analysis (RFX) of family-wise comparison along the group of ten subjects using the same set of models.</p>
      <p>Although these families do not cover the entire model-space we can decide for each model that to what extent they may belong to the base models. This is determined by comparing modulatory effects in the found model structure to the modulations defined by the 27 base models. The base model (or models) matching with the highest percentage determines the model family we assign model search results to. When more base models shares the connectivity of the found model, then corresponding families share the model accordingly. It is possible that the found model will show commonalities with more than one model family to some extent. Finally, we summarize model search results for all subjects to determine accuracy to recover family properties.</p>
    </sec>
    <sec>
      <title>2.7. Search Over Nested PEB Models</title>
      <p>For group-level analysis of DCM data the currently recommended procedure is to perform a linear PEB analysis of a fully connected network estimated for all subjects. A common tool to discover network structure on the group-level is based on the same greedy algorithm as <italic>post-hoc</italic> model selection, and uses BMR to evaluate a large number or nested PEB models. As reduced posterior probability can be analytically derived from linear models, BMR is safe to use in conjunction with PEB methods. Thus, using BMR estimated model evidence as decision criterion we compared the implemented topological search methods against the automatic search used in the SPM software. We then show the connectivity parameters for both commonalities and LI that are likely to contribute to the PEB model on the group-level.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>3. Results</title>
    <sec>
      <title>3.1. Performance Improvements to Estimate Model-Space</title>
      <p>The reimplemented model estimation procedure in ReDCM achieves a significant improvement in computational performance. We compared the variational iteration runtime of ReDCM and DCM12 by estimating the 18 test models by six different scan length (200, 400, 600, 800, 1,000, and 1,200 scans) by three models of varying model size (3, 5, and 7 regions of interest). <xref ref-type="fig" rid="F3">Figure 3</xref> shows the runtime comparison of every model estimation with model size 5 and length of 400 time points. Computation times of all 18 models are summarized in <xref rid="T2" ref-type="table">Table 2</xref>. Using ReDCM, without any parallelization or high-performance computing techniques, performance increased by 296–1,078%, depending on size and length.</p>
      <fig id="F3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Runtime comparison of DCM12 and ReDCM. We measured the average one-threaded computation runtime of the variational update cycles for both implementations. The comparison was based on varying scan length (200, 400, 600, 800, 1,000, and 1,200 frames) of a DCM model with five regions of interest (ROIs), and different model sizes (3, 5, and 7 ROIs) fitting fMRI data with scan length of 400 frames. There is a linear dependence between scan length and runtime, however, computations are exponentially longer with higher model sizes.</p>
        </caption>
        <graphic xlink:href="fninf-15-656486-g0003"/>
      </fig>
      <table-wrap id="T2" position="float">
        <label>Table 2</label>
        <caption>
          <p>Summary of runtime comparison.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>3 regions</bold>
              </th>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>5 regions</bold>
              </th>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>7 regions</bold>
              </th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>DCM12</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>ReDCM</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>DCM12</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>ReDCM</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>DCM12</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>ReDCM</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">200 scans</td>
              <td valign="top" align="center" rowspan="1" colspan="1">7.44</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.69</td>
              <td valign="top" align="center" rowspan="1" colspan="1">19.17</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3.61</td>
              <td valign="top" align="center" rowspan="1" colspan="1">53.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">17.19</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">400 scans</td>
              <td valign="top" align="center" rowspan="1" colspan="1">14.57</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.19</td>
              <td valign="top" align="center" rowspan="1" colspan="1">37.01</td>
              <td valign="top" align="center" rowspan="1" colspan="1">7.76</td>
              <td valign="top" align="center" rowspan="1" colspan="1">97.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">33.52</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">600 scans</td>
              <td valign="top" align="center" rowspan="1" colspan="1">21.59</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.57</td>
              <td valign="top" align="center" rowspan="1" colspan="1">53.96</td>
              <td valign="top" align="center" rowspan="1" colspan="1">11.36</td>
              <td valign="top" align="center" rowspan="1" colspan="1">136.37</td>
              <td valign="top" align="center" rowspan="1" colspan="1">46.27</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">800 scans</td>
              <td valign="top" align="center" rowspan="1" colspan="1">28.26</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.94</td>
              <td valign="top" align="center" rowspan="1" colspan="1">72.62</td>
              <td valign="top" align="center" rowspan="1" colspan="1">14.71</td>
              <td valign="top" align="center" rowspan="1" colspan="1">189.02</td>
              <td valign="top" align="center" rowspan="1" colspan="1">63.35</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">1,000 scans</td>
              <td valign="top" align="center" rowspan="1" colspan="1">34.94</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2.34</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90.06</td>
              <td valign="top" align="center" rowspan="1" colspan="1">17.83</td>
              <td valign="top" align="center" rowspan="1" colspan="1">233.77</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.83</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">1,200 scans</td>
              <td valign="top" align="center" rowspan="1" colspan="1">42.48</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2.78</td>
              <td valign="top" align="center" rowspan="1" colspan="1">107.04</td>
              <td valign="top" align="center" rowspan="1" colspan="1">21.17</td>
              <td valign="top" align="center" rowspan="1" colspan="1">272.53</td>
              <td valign="top" align="center" rowspan="1" colspan="1">91.84</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>All 18 synthetic models of different model size and varying scan length were estimated with both DCM12 and ReDCM. This table summarizes the running time of one iteration of the VL algorithm in seconds during parameter estimation</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>As ReDCM is intended to be an exact reimplementation of the DCM algorithm, we did not perform validation using the simulated data. Comparing posterior parameter estimates we found that the average difference between the two versions is lower than 1*10<sup>−4</sup>. Precision differences originate from different low level software libraries used for numerical methods.</p>
    </sec>
    <sec>
      <title>3.2. Attributes of the Full DCM Model-Space</title>
      <p>Running ReDCM on computers 4 * 48 CPU cores, we were able to estimate the full model-space of 10 subjects each containing 65,536 models in 12 days. We organized the estimated parameters of all models into an easy to handle data table. Description of this database is provided in <xref rid="T1" ref-type="table">Table 1</xref>. Alternatively, we also estimated the model-space using BMR. For investigating the model-space we introduced two attributes: (1) measuring the accuracy by <italic>dFe</italic> = <italic>Fe</italic>0−<italic>Fe</italic>, where <italic>Fe</italic>0 denotes the free-energy of the best model, and (2) the topological distance by <italic>Hd</italic>. Based on the estimated model-space of 10 subjects the <italic>dFe</italic> and <italic>Hd</italic> shows a significant, but moderate correlation with a coefficient of <italic>r</italic> = 0.3(<italic>p</italic> &lt; &lt; 0.001). This is partly in line with the hypothesis of Pyka et al. (<xref rid="B19" ref-type="bibr">2011</xref>), that the higher the log evidence difference between models is, the higher the Hamming-distance between them should be on any model-space. This indicates the usefulness of topological model search methods that use the negative free-energy as their fitness function. We also found that in the BMR model-space this correlation between model characteristics is even higher with <italic>r</italic> = 0.45(<italic>p</italic> &lt; &lt; 0.001). The main reason could be that with BMR every model is evaluated around the same minima of free-energy, which is the full model of the model-space.</p>
    </sec>
    <sec>
      <title>3.3. Subject-Level Characterization of Model Search Algorithms</title>
      <p>Results for model search in the DCM model-space are shown in <xref ref-type="fig" rid="F4">Figure 4</xref>. Averaged statistics for the 10 subjects can be found on the left side of <xref rid="T3" ref-type="table">Table 3</xref>. In our model-space the GA method slightly outperformed the deterministic GES and GHD by finding models having dFe 10.59 relative to the best model in average. However, GA estimates 202 models, roughly twice as much as GES (77) and GHD (118) until the algorithm finishes. Also, the stochastic method tends to find models slightly closer in structure, and having lower Hd (3.71) on average, than the two deterministic approaches (4.28 for GES and 4.07 for GHD).</p>
      <fig id="F4" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Summary of model search results for topological search algorithms. The accumulated number of estimated models, the dFe and the Hamming-distance of each of the three model search methods are displayed for all 20 runs on each of the 10 subject data. Box-and-whisker diagrams show the median, first and third quartiles of search results with the 95% confidence interval of the median. The improved version of the algorithms needed significantly lower number of models to calculate during search, with <italic>p</italic> &lt; 0.001 for each algorithm.</p>
        </caption>
        <graphic xlink:href="fninf-15-656486-g0004"/>
      </fig>
      <table-wrap id="T3" position="float">
        <label>Table 3</label>
        <caption>
          <p>Summary of model search results within the model-space estimated by DCM and BMR.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" colspan="5" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>DCM</bold>
              </th>
              <th valign="top" align="center" colspan="5" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>BMR</bold>
              </th>
            </tr>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>dFe</bold>
              </th>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Hd</bold>
              </th>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>N</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>dFe</bold>
              </th>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Hd</bold>
              </th>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>N</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>GES</italic>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">13.79</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 24.04)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.28</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 3.17)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">77</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.24</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 0.46)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.30</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 2.19)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>GES</italic>′</td>
              <td valign="top" align="center" rowspan="1" colspan="1">11.15</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 15.64)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.23</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 2.77)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">36</td>
              <td valign="top" align="center" rowspan="1" colspan="1">20.98</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 47.32)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.43</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 2.64)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">30</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>GHD</italic>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">11.45</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 15.24)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.07</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 3.37)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">118</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.25</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 0.43)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.35</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 2.30)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">129</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>GHD</italic>′</td>
              <td valign="top" align="center" rowspan="1" colspan="1">8.76</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 11.94)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.29</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 2.60)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">53</td>
              <td valign="top" align="center" rowspan="1" colspan="1">7.66</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 26.42)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.73</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 2.44)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">42</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>GA</italic>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">10.59</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 13.74)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>3.71</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 2.68)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">202</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.74</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 1.38)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.22</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 2.10)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">185</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>GA</italic>′</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>7.37</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 10.27)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3.83</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 2.46)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">140</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.54</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 19.75)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.51</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 2.24)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">97</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>post-hoc</italic>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">16.96</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 18.61)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.80</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 2.30)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">NA</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.73</td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 0.54)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>2.10</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">(sd = 1.66)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">NA</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>Model search algorithms are characterized based on statistical (Fe) and topological or structural (Hd) properties of the found models and the number of models needed (N) to be estimated until model search converges. Statistics are acquired and averaged for 10 subjects and 20 runs of each method. We included mean dFe, Hd and their standard deviation. Optimized modifications of the base algorithms are denoted as GES', GHD', and GA'. Bold text highlights the best performing method regarding mean dFe and Hd</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>Using the implemented modifications to the original methods we managed to exploit DCM's ability to estimate every connection strength parameter. For each of the three search procedures, the implemented modifications succeed to improve efficiency. Simply fixing parameters based on previously estimated model posteriors the optimized GA' method found the best model multiple times and reached an average of 7.37 dFe. Furthermore, each optimized algorithm need to compute significantly less models until convergence.</p>
      <p>Searching through the model-space using BMR allows us to compare the developed methods to the <italic>post-hoc</italic> model search implemented in SPM. Results are shown on the right side of <xref rid="T3" ref-type="table">Table 3</xref>. Interestingly, GES and GHD methods perform better in the BMR model-space than the other methods, but the optimized algorithms become unreliable. Fixing connectivity parameters in the BMR space with the same criteria used in the DCM space does not improve search efficiency. In terms of Hamming-distance, the <italic>post-hoc</italic> method finds the models closest to the best model in the model-space estimated by BMR. As an example <xref ref-type="fig" rid="F5">Figure 5</xref> depicts model search results for one subject over the joint distribution of dFe and Hd in both the DCM and the BMR model-space.</p>
      <fig id="F5" position="float">
        <label>Figure 5</label>
        <caption>
          <p>The joint distribution density of dFe and Hd of an example subject. Joint density images reveal any topological structure over the model-space by characterizing the distance between graph structure, as scored with the Hamming-distance, in terms of differences in model evidence or free-energy. The joint density for the model-space estimated by DCM is shown on <bold>(A)</bold>, and the distribution in the BMR space on <bold>(B)</bold>. In cases where there are no relationship between topological structure and model evidence, the joint distribution would look more evenly distributed over Hd in any range of dFe. In this shown example model structure appears to be correlated to model evidence by <italic>r</italic> = 0.33 in the DCM model-space and by <italic>r</italic> = 0.68 in the BMR space. This reflects the assumption of Pyka et al. (<xref rid="B19" ref-type="bibr">2011</xref>), being models with higher Fe are also close to the topological structure of the best model. On the (0,0) coordinates the best model of their corresponding space can be found. Model search results are labeled according to their description. As the Bayesian <italic>post-hoc</italic> model selection is not available in the fully estimated DCM model-space, we indicated the model with the same ID on both panels.</p>
        </caption>
        <graphic xlink:href="fninf-15-656486-g0005"/>
      </fig>
    </sec>
    <sec>
      <title>3.4. Family Inference and Model Search</title>
      <p>As a step toward group-level analysis, family-level inference is drawn from groups of models over the population. In many cases the models need to be estimated for selecting the winning family. Model search methods may be useful to recover the properties of families. <xref ref-type="fig" rid="F6">Figure 6</xref> shows the RFX model selection of all three partitioning of the model-space. In each case model search results showed commonalities with the winning family. The GA, GHD, and <italic>post-hoc</italic> model selection algorithms performed similarly between 75 and 99% accuracy to match task-based modulation patterns of the winning families.</p>
      <fig id="F6" position="float">
        <label>Figure 6</label>
        <caption>
          <p>Family-wise inference and family matched by model search methods. The top row shows random effects (RFX) family-wise model selection results over a group of subjects according to laterality, dorsoventral differences and task based differences. The bottom row shows percentage of model search results matching each of the families. The genetic algorithm (GA), the greedy Hamming-distance search (GHD), and <italic>post-hoc</italic> (PH) methods have the highest chance to match properties of the winning model family.</p>
        </caption>
        <graphic xlink:href="fninf-15-656486-g0006"/>
      </fig>
    </sec>
    <sec>
      <title>3.5. Model Search on Population-Level PEB Model</title>
      <p>We compared methods for searching among nested PEB models using the automatic greedy method (denoted as BMR in <xref ref-type="fig" rid="F7">Figure 7</xref>) used in SPM and the three topological model search algorithms. In the case of the group-level model we initiated GES and GHD methods with the full model and GA with randomized connectivity. As topological methods are not suitable to search group commonalities (group mean) and LI (differences) simultaneously, we only compared model structure in the group mean effect. We found that BMR removed only the modulatory effect of “Words” task on rdF self-inhibition. The GES and GHD methods also removed this effect along with the effect of “Pictures” on lvF. The GA method returned the full model in each of the 10 performed runs.</p>
      <fig id="F7" position="float">
        <label>Figure 7</label>
        <caption>
          <p>Model search in group-level PEB. The BMR method removed only the modulatory effect of “Words” on the self-inhibition of rdF region. The GES and GHD algorithms also removed the modulatory effect of “Pictures” from lvF self-connection. The GA method found the fully connected model each time regardless of the initial model.</p>
        </caption>
        <graphic xlink:href="fninf-15-656486-g0007"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>4. Discussion</title>
    <p>Investigating the neuronal interactions between brain regions is encumbered by several technical and methodological challenges. Dynamic causal modeling provides a methodological framework to estimate the influence of one region on another, while it also allows us to test our hypothesis on model structure and to decide which models are more plausible than others considering the data. However, little is known about optimal strategies for model search in DCM.</p>
    <p>One major hindrance in this field is the high computational demand that parameter estimation poses. To use the algorithm for model search purposes, performance improvements are required. Some solutions, available for fMRI (Aponte et al., <xref rid="B2" ref-type="bibr">2016</xref>) and EEG (Wang et al., <xref rid="B26" ref-type="bibr">2013</xref>) data, use the graphical processing unit (GPU) to perform matrix operations applied frequently. These methods usually utilize an alternative parameter estimation procedure to complement their massively parallel nature. We decided to implement the exact DCM12 algorithm from SPM to achieve the same estimation results of the posterior parameter densities as the original algorithm does. Using ReDCM with high-performance computing techniques we were also able to estimate all 10*65,536 models in acceptable time. We make the database of the entire computed model-space freely available for any research group to develop model search methods and to further investigate the properties of the DCM model-space. In this study, we separated the DCM model estimation from a model search to facilitate the development of search algorithms. Looking up results of a previously estimated model-space provides an efficient framework that developers of model search methods can exploit for testing and optimizing searching procedures.</p>
    <p>Another crucial point of DCM is that Fe is only an approximation of the log model evidence. As it estimates the log-evidence of a model under a Laplace approximation, this measure depends on prior parameter densities that are chosen to ensure the VL algorithm to converge. Hence the prior parameters of a model are defined by the structure of the model, and careful consideration is needed when comparing models with different numbers of connection parameters (Penny et al., <xref rid="B17" ref-type="bibr">2004</xref>). To address this problem, DCM minimizes the Kullback-Leibler divergence (Kullback and Leibler, <xref rid="B13" ref-type="bibr">1951</xref>) between the prior density and the approximate posterior (Friston et al., <xref rid="B7" ref-type="bibr">2007</xref>). This measure can be seen as the complexity of the model and is required to compare models with different structures. For searching models, our methods used Fe to compare models with different structures (and complexity), but we can also obtain useful information from a model's posterior connection parameter estimates. Here we demonstrated that model search algorithms can be improved by analyzing individual parameters of a model besides considering their free-energy.</p>
    <p>However, finding the one model with the highest Fe may not be meaningful. Even if the best balance between model fit and complexity is found and lower complexity models are more preferred than dense model structures, we cannot be sure whether the models are overfitting the data or not. In practice, it is encouraged to form a hypothesis about a network feature, and split the model-space into two (or more) groups or families around the tested connectivity patterns. In this study we showed that topological model search methods can successfully identify patterns of the winning model family constructed by a priori hypothesis about network structure.</p>
    <p>The model search algorithms we used to find the best model are based on iteratively changing the structure of the models to improve model fitting. This approach assumes that models close to the best one in Fe are also close in structure (or Hamming-distance in this case). This assumption holds to some extent, but it is still unclear how free-energy and Hamming-distance are distributed over an arbitrary dataset and how these characteristics are related to each other. We found that these methods can be applied to achieve useful models, although they are not always reliable. A limitation of using topological search algorithms, generally used for graph discovery, is the questionable generalizability for any input data or different model structures.</p>
    <p>A method for <italic>post-hoc</italic> model selection, described in the work by Friston and Penny (<xref rid="B8" ref-type="bibr">2011</xref>), finds an optimized reduction for any base model using a greedy approach removing free parameters from the model. While it efficiently and quickly optimizes model evidence by reducing connection parameters to shrinkage priors, it strictly remains in the Bayesian framework without considering graph structural aspects of network modeling. As the optimized models can violate model-space restrictions we set, it should be used on subject-level data with care. In most cases it means that even those endogenous connections (i.e., self-connections) can be removed from the model that are still modulated by any experimental input. However, reenabling these parameters later for easier interpretation or to match model-space restrictions moves results from their local minima, reducing search efficiency compared to topological methods. In contrast, the linear model of group-level PEB can be efficiently reduced with BMR, while topological methods are limited to search variations among parameters for common group effects rather than all parameters for explanatory variables within the PEB framework. For this reason BMR on PEB models can be considered the ‘gold standard’ and currently recommended way for group-level hypothesis testing and network discovery method. To mitigate limitations for structural issues one could extend model search algorithms to search over PEB model parameters rather than model structures defined by first-level DCMs.</p>
    <p>In summary, we characterized three graph theory based model search algorithms adapted for DCM and compared their efficiency based on free-energy difference and Hamming-distance relative to the best model in the model-space and the number of estimated models during search. We included the BMR-based <italic>post-hoc</italic> model selection in the comparison, and discussed advantages and disadvantages of different approaches. We found that topological algorithms often outperform analytic (BMR) methods while searching for the optimal model structure on the subject-level. Furthermore, each algorithm performs well in finding models that share properties described by the winning family in a family-wise model selection scheme. However, topological methods reveal their limitations in searching through nested PEB models, which confirms BMR to be the currently recommended way to optimize group-level models. To develop and test model search methods efficiently, we separated DCM computations from generating model alternatives, replacing model inversion with time efficient database lookups. We share model-space data used in this study, and the ReDCM R package, which reimplements deterministic bilinear DCM to support high-performance computing facilities. We hope that our work will help further studies of the DCM model-space and the ReDCM package will be a useful contribution for Bayesian inference in the field of neuroscience.</p>
  </sec>
  <sec sec-type="data-availability" id="s5">
    <title>Data Availability Statement</title>
    <p>The generated database of parameter estimates and model search results supporting the conclusions of this article are available at <ext-link ext-link-type="uri" xlink:href="https://aranyics.github.io/ReDCM/">https://aranyics.github.io/ReDCM/</ext-link>. The data table contains model description and DCM parameter estimation results for each model in the model-space. The current version of the ReDCM R package and source codes can be downloaded from the GitHub repository of the package: <ext-link ext-link-type="uri" xlink:href="https://github.com/aranyics/ReDCM/releases">https://github.com/aranyics/ReDCM/releases</ext-link>. The implemented algorithm is based on DCM12 from the v6225 build of the SPM toolbox.</p>
  </sec>
  <sec id="s6">
    <title>Author Contributions</title>
    <p>SA and ME took part in study conceptualization and development of model search methods. ReDCM software was developed by SA. GO managed hardware resources and data storage for model-space data. SA, MN, and ME contributed to writing the original manuscript and visualization. EB and ME was responsible for funding acquisition and project supervision. All authors revised and approved the submitted manuscript. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
</body>
<back>
  <ack>
    <p>We thank Tamás Spisák (University Hospital Essen) for insightful discussion on the subject and suggestions about presenting our results.</p>
  </ack>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> Project no. TKP2020-IKA-04 has been implemented with the support provided from the National Research, Development and Innovation Fund of Hungary, financed under the 2020-4.1.1-TKP2020 funding scheme. This study was also partially funded by the National Brain Research Program (2017-1.2.1-NKP-2017-00002).</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adabor</surname><given-names>E. S.</given-names></name><name><surname>Acquaah-Mensah</surname><given-names>G. K.</given-names></name><name><surname>Oduro</surname><given-names>F. T.</given-names></name></person-group> (<year>2015</year>). <article-title>SAGA: a hybrid search algorithm for Bayesian Network structure learning of transcriptional regulatory networks</article-title>. <source>J. Biomed. Inform</source>. <volume>53</volume>, <fpage>27</fpage>–<lpage>35</lpage>. <pub-id pub-id-type="doi">10.1016/j.jbi.2014.08.010</pub-id><?supplied-pmid 25181467?><pub-id pub-id-type="pmid">25181467</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aponte</surname><given-names>E. A.</given-names></name><name><surname>Raman</surname><given-names>S.</given-names></name><name><surname>Sengupta</surname><given-names>B.</given-names></name><name><surname>Penny</surname><given-names>W. D.</given-names></name><name><surname>Stephan</surname><given-names>K. E.</given-names></name><name><surname>Heinzle</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>MPDCM: A toolbox for massively parallel dynamic causal modeling</article-title>. <source>J. Neurosci. Methods</source>
<volume>257</volume>, <fpage>7</fpage>–<lpage>16</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2015.09.009</pub-id><?supplied-pmid 26384541?><pub-id pub-id-type="pmid">26384541</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buxton</surname><given-names>R. B.</given-names></name><name><surname>Wong</surname><given-names>E. C.</given-names></name><name><surname>Frank</surname><given-names>L. R.</given-names></name></person-group> (<year>1998</year>). <article-title>Dynamics of blood flow and oxygenation changes during brain activation: the balloon model</article-title>. <source>Magn. Reson. Med</source>. <volume>39</volume>, <fpage>855</fpage>–<lpage>864</lpage>. <pub-id pub-id-type="doi">10.1002/mrm.1910390602</pub-id><?supplied-pmid 9621908?><pub-id pub-id-type="pmid">9621908</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>J. O.</given-names></name></person-group> (<year>2016</year>). <article-title>Universal darwinism as a process of Bayesian inference</article-title>. <source>Front. Syst. Neurosci</source>. <volume>10</volume>:<fpage>49</fpage>. <pub-id pub-id-type="doi">10.3389/fnsys.2016.00049</pub-id><?supplied-pmid 27375438?><pub-id pub-id-type="pmid">27375438</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Copeland</surname><given-names>M.</given-names></name><name><surname>Soh</surname><given-names>J.</given-names></name><name><surname>Puca</surname><given-names>A.</given-names></name><name><surname>Manning</surname><given-names>M.</given-names></name><name><surname>Gollob</surname><given-names>D.</given-names></name></person-group> (<year>2015</year>). <source>Microsoft Azure: Planning, Deploying, and Managing Your Data Center in the Cloud, 1st Edn</source>. <publisher-loc>Berkeley, CA</publisher-loc>: <publisher-name>Apress</publisher-name>. <pub-id pub-id-type="doi">10.1007/978-1-4842-1043-7</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K.</given-names></name><name><surname>Harrison</surname><given-names>L.</given-names></name><name><surname>Penny</surname><given-names>W.</given-names></name></person-group> (<year>2003</year>). <article-title>Dynamic causal modelling</article-title>. <source>NeuroImage</source>
<volume>19</volume>, <fpage>1273</fpage>–<lpage>1302</lpage>. <pub-id pub-id-type="doi">10.1016/S1053-8119(03)00202-7</pub-id><pub-id pub-id-type="pmid">12948688</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K.</given-names></name><name><surname>Mattout</surname><given-names>J.</given-names></name><name><surname>Trujillo-Barreto</surname><given-names>N.</given-names></name><name><surname>Ashburner</surname><given-names>J.</given-names></name><name><surname>Penny</surname><given-names>W.</given-names></name></person-group> (<year>2007</year>). <article-title>Variational free energy and the Laplace approximation</article-title>. <source>NeuroImage</source>
<volume>34</volume>, <fpage>220</fpage>–<lpage>234</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.08.035</pub-id><?supplied-pmid 17055746?><pub-id pub-id-type="pmid">17055746</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K.</given-names></name><name><surname>Penny</surname><given-names>W.</given-names></name></person-group> (<year>2011</year>). <article-title><italic>Post hoc</italic> Bayesian model selection</article-title>. <source>NeuroImage</source>
<volume>56</volume>, <fpage>2089</fpage>–<lpage>2099</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.03.062</pub-id><?supplied-pmid 21459150?><pub-id pub-id-type="pmid">21459150</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K. J.</given-names></name><name><surname>Litvak</surname><given-names>V.</given-names></name><name><surname>Oswal</surname><given-names>A.</given-names></name><name><surname>Razi</surname><given-names>A.</given-names></name><name><surname>Stephan</surname><given-names>K. E.</given-names></name><name><surname>Van Wijk</surname><given-names>B. C.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>Bayesian model reduction and empirical Bayes for group (DCM) studies</article-title>. <source>NeuroImage</source><volume>128</volume>, <fpage>413</fpage>–<lpage>431</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.11.015</pub-id><?supplied-pmid 26569570?><pub-id pub-id-type="pmid">26569570</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K. J.</given-names></name><name><surname>Mechelli</surname><given-names>a.</given-names></name><name><surname>Turner</surname><given-names>R.</given-names></name><name><surname>Price</surname><given-names>C. J.</given-names></name></person-group> (<year>2000</year>). <article-title>Nonlinear responses in fMRI: the Balloon model, Volterra kernels, and other hemodynamics</article-title>. <source>NeuroImage</source>
<volume>12</volume>, <fpage>466</fpage>–<lpage>477</lpage>. <pub-id pub-id-type="doi">10.1006/nimg.2000.0630</pub-id><?supplied-pmid 10988040?><pub-id pub-id-type="pmid">10988040</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Galassi</surname><given-names>M.</given-names></name><name><surname>Davies</surname><given-names>J.</given-names></name><name><surname>Theiler</surname><given-names>J.</given-names></name><name><surname>Gough</surname><given-names>B.</given-names></name><name><surname>Jungman</surname><given-names>G.</given-names></name><name><surname>Alken</surname><given-names>P.</given-names></name><etal/></person-group>. (<year>2006</year>). <source>GNU Scientific Library Reference Manual</source>, <edition>2nd Edn</edition>. <publisher-loc>Bristol</publisher-loc>: <publisher-name>Network Theory Ltd</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Grabner</surname><given-names>G.</given-names></name><name><surname>Janke</surname><given-names>A. L.</given-names></name><name><surname>Budge</surname><given-names>M. M.</given-names></name><name><surname>Smith</surname><given-names>D.</given-names></name><name><surname>Pruessner</surname><given-names>J.</given-names></name><name><surname>Collins</surname><given-names>D. L.</given-names></name></person-group> (<year>2006</year>). <article-title>Symmetric atlasing and model based segmentation: an application to the hippocampus in older adults,</article-title> in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Berlin; Heidelberg</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>), <fpage>58</fpage>–<lpage>66</lpage>. <pub-id pub-id-type="doi">10.1007/11866763_8</pub-id><?supplied-pmid 17354756?></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kullback</surname><given-names>S.</given-names></name><name><surname>Leibler</surname><given-names>R. A.</given-names></name></person-group> (<year>1951</year>). <article-title>On information and sufficiency</article-title>. <source>Math. Stat</source>. <volume>22</volume>, <fpage>79</fpage>–<lpage>86</lpage>. <pub-id pub-id-type="doi">10.1214/aoms/1177729694</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mumford</surname><given-names>J. A.</given-names></name><name><surname>Ramsey</surname><given-names>J. D.</given-names></name></person-group> (<year>2014</year>). <article-title>Bayesian networks for fMRI: a primer</article-title>. <source>NeuroImage</source>
<volume>86</volume>, <fpage>573</fpage>–<lpage>582</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.020</pub-id><pub-id pub-id-type="pmid">24140939</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Penny</surname><given-names>W.</given-names></name></person-group> (<year>2012</year>). <article-title>Comparing dynamic causal models using AIC, BIC and free energy</article-title>. <source>NeuroImage</source>
<volume>59</volume>, <fpage>319</fpage>–<lpage>330</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.07.039</pub-id><?supplied-pmid 21864690?><pub-id pub-id-type="pmid">21864690</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Penny</surname><given-names>W. D.</given-names></name><name><surname>Stephan</surname><given-names>K. E.</given-names></name><name><surname>Daunizeau</surname><given-names>J.</given-names></name><name><surname>Rosa</surname><given-names>M. J.</given-names></name><name><surname>Friston</surname><given-names>K. J.</given-names></name><name><surname>Schofield</surname><given-names>T. M.</given-names></name><etal/></person-group>. (<year>2010</year>). <article-title>Comparing families of dynamic causal models</article-title>. <source>PLoS Comput. Biol</source>. <volume>6</volume>:<fpage>e1000709</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1000709</pub-id><?supplied-pmid 20300649?><pub-id pub-id-type="pmid">20300649</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Penny</surname><given-names>W. D.</given-names></name><name><surname>Stephan</surname><given-names>K. E.</given-names></name><name><surname>Mechelli</surname><given-names>A.</given-names></name><name><surname>Friston</surname><given-names>K. J.</given-names></name></person-group> (<year>2004</year>). <article-title>Comparing dynamic causal models</article-title>. <source>NeuroImage</source>
<volume>22</volume>, <fpage>1157</fpage>–<lpage>1172</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.03.026</pub-id><pub-id pub-id-type="pmid">15219588</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pool</surname><given-names>E.-M.</given-names></name><name><surname>Rehme</surname><given-names>A. K.</given-names></name><name><surname>Fink</surname><given-names>G. R.</given-names></name><name><surname>Eickhoff</surname><given-names>S. B.</given-names></name><name><surname>Grefkes</surname><given-names>C.</given-names></name></person-group> (<year>2014</year>). <article-title>Handedness and effective connectivity of the motor system</article-title>. <source>NeuroImage</source>
<volume>99</volume>, <fpage>451</fpage>–<lpage>460</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.05.048</pub-id><?supplied-pmid 24862079?><pub-id pub-id-type="pmid">24862079</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pyka</surname><given-names>M.</given-names></name><name><surname>Heider</surname><given-names>D.</given-names></name><name><surname>Hauke</surname><given-names>S.</given-names></name><name><surname>Kircher</surname><given-names>T.</given-names></name><name><surname>Jansen</surname><given-names>A.</given-names></name></person-group> (<year>2011</year>). <article-title>Dynamic causal modeling with genetic algorithms</article-title>. <source>J. Neurosci. Methods</source>
<volume>194</volume>, <fpage>402</fpage>–<lpage>406</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2010.11.007</pub-id><?supplied-pmid 21094663?><pub-id pub-id-type="pmid">21094663</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramsey</surname><given-names>J. D.</given-names></name><name><surname>Hanson</surname><given-names>S. J.</given-names></name><name><surname>Glymour</surname><given-names>C.</given-names></name></person-group> (<year>2011</year>). <article-title>Multi-subject search correctly identifies causal connections and most causal directions in the DCM models of the Smith et al. Simulation study</article-title>. <source>NeuroImage</source>
<volume>58</volume>, <fpage>838</fpage>–<lpage>848</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.06.068</pub-id><?supplied-pmid 21745580?><pub-id pub-id-type="pmid">21745580</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramsey</surname><given-names>J. D.</given-names></name><name><surname>Hanson</surname><given-names>S. J.</given-names></name><name><surname>Hanson</surname><given-names>C.</given-names></name><name><surname>Halchenko</surname><given-names>Y. O.</given-names></name><name><surname>Poldrack</surname><given-names>R. a.</given-names></name><name><surname>Glymour</surname><given-names>C.</given-names></name></person-group> (<year>2010</year>). <article-title>Six problems for causal inference from fMRI</article-title>. <source>NeuroImage</source>
<volume>49</volume>, <fpage>1545</fpage>–<lpage>1558</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.08.065</pub-id><?supplied-pmid 19747552?><pub-id pub-id-type="pmid">19747552</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seghier</surname><given-names>M. L.</given-names></name><name><surname>Josse</surname><given-names>G.</given-names></name><name><surname>Leff</surname><given-names>A. P.</given-names></name><name><surname>Price</surname><given-names>C. J.</given-names></name></person-group> (<year>2011</year>). <article-title>Lateralization is predicted by reduced coupling from the left to right prefrontal cortex during semantic decisions on written words</article-title>. <source>Cereb. Cortex</source>
<volume>21</volume>, <fpage>1519</fpage>–<lpage>1531</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhq203</pub-id><?supplied-pmid 21109578?><pub-id pub-id-type="pmid">21109578</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>S. M.</given-names></name><name><surname>Miller</surname><given-names>K. L.</given-names></name><name><surname>Salimi-Khorshidi</surname><given-names>G.</given-names></name><name><surname>Webster</surname><given-names>M.</given-names></name><name><surname>Beckmann</surname><given-names>C. F.</given-names></name><name><surname>Nichols</surname><given-names>T. E.</given-names></name><etal/></person-group>. (<year>2011</year>). <article-title>Network modelling methods for FMRI</article-title>. <source>NeuroImage</source><volume>54</volume>, <fpage>875</fpage>–<lpage>891</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.08.063</pub-id><pub-id pub-id-type="pmid">20817103</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephan</surname><given-names>K. E.</given-names></name><name><surname>Kasper</surname><given-names>L.</given-names></name><name><surname>Harrison</surname><given-names>L. M.</given-names></name><name><surname>Daunizeau</surname><given-names>J.</given-names></name><name><surname>den Ouden</surname><given-names>H. E. M.</given-names></name><name><surname>Breakspear</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2008</year>). <article-title>Nonlinear dynamic causal models for fMRI</article-title>. <source>NeuroImage</source><volume>42</volume>, <fpage>649</fpage>–<lpage>662</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.04.262</pub-id><?supplied-pmid 18565765?><pub-id pub-id-type="pmid">18565765</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephan</surname><given-names>K. E.</given-names></name><name><surname>Weiskopf</surname><given-names>N.</given-names></name><name><surname>Drysdale</surname><given-names>P. M.</given-names></name><name><surname>Robinson</surname><given-names>P. A.</given-names></name><name><surname>Friston</surname><given-names>K. J.</given-names></name></person-group> (<year>2007</year>). <article-title>Comparing hemodynamic models with DCM</article-title>. <source>NeuroImage</source>
<volume>38</volume>, <fpage>387</fpage>–<lpage>401</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.07.040</pub-id><pub-id pub-id-type="pmid">17884583</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>W.-J.</given-names></name><name><surname>Hsieh</surname><given-names>I.-F.</given-names></name><name><surname>Chen</surname><given-names>C.-C.</given-names></name></person-group> (<year>2013</year>). <article-title>Accelerating computation of DCM for ERP in MATLAB by external function calls to the GPU</article-title>. <source>PLoS ONE</source>
<volume>8</volume>:<fpage>e66599</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0066599</pub-id><?supplied-pmid 23840507?><pub-id pub-id-type="pmid">23840507</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>A. E.</given-names></name><name><surname>Harvey</surname><given-names>A. S.</given-names></name><name><surname>Vogrin</surname><given-names>S. J.</given-names></name><name><surname>Bailey</surname><given-names>C.</given-names></name><name><surname>Davidson</surname><given-names>A.</given-names></name><name><surname>Jackson</surname><given-names>G. D.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>The epileptic network of Lennox-Gastaut syndrome</article-title>. <source>Neurology</source><volume>93</volume>, <fpage>e215</fpage>–<lpage>e226</lpage>. <pub-id pub-id-type="doi">10.1212/WNL.0000000000007775</pub-id><pub-id pub-id-type="pmid">31227617</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeidman</surname><given-names>P.</given-names></name><name><surname>Jafarian</surname><given-names>A.</given-names></name><name><surname>Corbin</surname><given-names>N.</given-names></name><name><surname>Seghier</surname><given-names>M. L.</given-names></name><name><surname>Razi</surname><given-names>A.</given-names></name><name><surname>Price</surname><given-names>C. J.</given-names></name><etal/></person-group>. (<year>2019a</year>). <article-title>A guide to group effective connectivity analysis, part 1: first level analysis with DCM for fMRI</article-title>. <source>NeuroImage</source><volume>200</volume>, <fpage>174</fpage>–<lpage>190</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.06.031</pub-id><?supplied-pmid 31226497?><pub-id pub-id-type="pmid">31226497</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeidman</surname><given-names>P.</given-names></name><name><surname>Jafarian</surname><given-names>A.</given-names></name><name><surname>Seghier</surname><given-names>M. L.</given-names></name><name><surname>Litvak</surname><given-names>V.</given-names></name><name><surname>Cagnan</surname><given-names>H.</given-names></name><name><surname>Price</surname><given-names>C. J.</given-names></name><etal/></person-group>. (<year>2019b</year>). <article-title>A guide to group effective connectivity analysis, part 2: second level analysis with PEB</article-title>. <source>NeuroImage</source><volume>200</volume>, <fpage>12</fpage>–<lpage>25</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.06.032</pub-id><?supplied-pmid 31226492?><pub-id pub-id-type="pmid">31226492</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
