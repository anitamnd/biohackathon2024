<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8296748</article-id>
    <article-id pub-id-type="publisher-id">4295</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-021-04295-1</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methodology Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>GAT-LI: a graph attention network based learning and interpreting method for functional brain network classification</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3602-7603</contrib-id>
        <name>
          <surname>Hu</surname>
          <given-names>Jinlong</given-names>
        </name>
        <address>
          <email>jlhu@scut.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cao</surname>
          <given-names>Lijie</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Tenghui</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dong</surname>
          <given-names>Shoubin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Ping</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.79703.3a</institution-id><institution-id institution-id-type="ISNI">0000 0004 1764 3838</institution-id><institution>Guangdong Key Lab of Communication and Computer Network, School of Computer Science and Engineering, </institution><institution>South China University of Technology, </institution></institution-wrap>Guangzhou, China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.79703.3a</institution-id><institution-id institution-id-type="ISNI">0000 0004 1764 3838</institution-id><institution>Zhongshan Institute of Modern Industrial Technology, </institution><institution>South China University of Technology, </institution></institution-wrap>Zhongshan, China </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.16890.36</institution-id><institution-id institution-id-type="ISNI">0000 0004 1764 6123</institution-id><institution>Department of Chinese and Bilingual Studies, Faculty of Humanities, </institution><institution>The Hong Kong Polytechnic University, </institution></institution-wrap>Hong Kong, China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>22</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>22</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>22</volume>
    <elocation-id>379</elocation-id>
    <history>
      <date date-type="received">
        <day>6</day>
        <month>10</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>8</day>
        <month>7</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Autism spectrum disorders (ASD) imply a spectrum of symptoms rather than a single phenotype. ASD could affect brain connectivity at different degree based on the severity of the symptom. Given their excellent learning capability, graph neural networks (GNN) methods have recently been used to uncover functional connectivity patterns and biological mechanisms in neuropsychiatric disorders, such as ASD. However, there remain challenges to develop an accurate GNN learning model and understand how specific decisions of these graph models are made in brain network analysis.
</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In this paper, we propose a graph attention network based learning and interpreting method, namely GAT-LI, which learns to classify functional brain networks of ASD individuals versus healthy controls (HC), and interprets the learned graph model with feature importance. Specifically, GAT-LI includes a graph learning stage and an interpreting stage. First, in the graph learning stage, a new graph attention network model, namely GAT2, uses graph attention layers to learn the node representation, and a novel attention pooling layer to obtain the graph representation for functional brain network classification. We experimentally compared GAT2 model’s performance on the ABIDE I database from 1035 subjects against the classification performances of other well-known models, and the results showed that the GAT2 model achieved the best classification performance. We experimentally compared the influence of different construction methods of brain networks in GAT2 model. We also used a larger synthetic graph dataset with 4000 samples to validate the utility and power of GAT2 model. Second, in the interpreting stage, we used GNNExplainer to interpret learned GAT2 model with feature importance. We experimentally compared GNNExplainer with two well-known interpretation methods including Saliency Map and DeepLIFT to interpret the learned model, and the results showed GNNExplainer achieved the best interpretation performance. We further used the interpretation method to identify the features that contributed most in classifying ASD versus HC.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par3">We propose a two-stage learning and interpreting method GAT-LI to classify functional brain networks and interpret the feature importance in the graph model. The method should also be useful in the classification and interpretation tasks for graph data from other biomedical scenarios.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Graph attention networks</kwd>
      <kwd>Functional brain networks</kwd>
      <kwd>Resting-state functional connectivity data</kwd>
      <kwd>Classification</kwd>
      <kwd>Model interpretation</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>the Natural Science Foundation of Guangdong Province of China</institution>
        </funding-source>
        <award-id>2018A030313309</award-id>
        <award-id>2021A1515011942</award-id>
        <principal-award-recipient>
          <name>
            <surname>Hu</surname>
            <given-names>Jinlong</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>the Innovation fund of introduced high-end scientific research institutions of Zhongshan</institution>
        </funding-source>
        <award-id>2019AG031</award-id>
        <principal-award-recipient>
          <name>
            <surname>Dong</surname>
            <given-names>Shoubin</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>the Fundamental Research Funds for the Central Universities, SCUT</institution>
        </funding-source>
        <award-id>2019KZ20</award-id>
        <principal-award-recipient>
          <name>
            <surname>Hu</surname>
            <given-names>Jinlong</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>the Guangdong Pearl River Talents Plan Innovative and Entrepreneurial Team</institution>
        </funding-source>
        <award-id>2016ZT06S220</award-id>
        <principal-award-recipient>
          <name>
            <surname>Li</surname>
            <given-names>Ping</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par29">Autism spectrum disorders (ASD) is a spectrum disorder, which means that the symptoms are expressed along a spectrum rather than in a fixed single phenotype. Brain functional connectivity of ASD individuals could be affected at different degree based on the severity of the symptom. Functional connectivity is the statistical relationship between functional brain activities in voxels or regions of interests (ROIs), and it has been used to uncover the complex biological mechanisms in not only typically developing individuals but also neuropsychiatric disorders such as ASD. Given the excellent learning capability, deep learning methods have been used to examine and analyze functional connectivity [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR5">5</xref>]. Functional connectivity vectors are usually used as input data for deep learning models in classifying different phenotypes such as ASD versus healthy controls (HC) [<xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR7">7</xref>]. To further explore how specific decisions of these networks are made, some explanatory methods, such as piecewise linear neural networks [<xref ref-type="bibr" rid="CR5">5</xref>], and Shapley value explanation [<xref ref-type="bibr" rid="CR7">7</xref>], have recently been developed for deep learning models.</p>
    <p id="Par30">Graph neural networks (GNN) have become useful in brain network analyses [<xref ref-type="bibr" rid="CR8">8</xref>–<xref ref-type="bibr" rid="CR12">12</xref>]. Unlike standard neural networks using vectors as input data, GNN is a class of Neural Networks for graph data, which retains a state that can represent information of any depth from its neighborhood, and could explore the interactions between graph nodes [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR14">14</xref>]. GNN has great potential for improving the performance in classifying brain networks. For example, Ktena et al. [<xref ref-type="bibr" rid="CR8">8</xref>] constructed brain networks based on functional Magnetic Resonance Imaging (fMRI) data, and proposed a Siamese graph convolutional neural network to learn graph similarities for classification. Ma et al. [<xref ref-type="bibr" rid="CR9">9</xref>] applied similarity learning for brain connectivity networks, and further adopt a random walk strategy with sliding windows to capture the higher-order information of graphs to improve the classification performance. Zhang et al. [<xref ref-type="bibr" rid="CR10">10</xref>] presented a multi-view graph convolutional network for classifying Parkinson’s Disease cases from controls, where the graph convolutional networks (GCNs), a class of GNN, was applied to extract features from brain networks, and integrated Electronic Health Records with GCN based features for classification. Arslan et al. [<xref ref-type="bibr" rid="CR11">11</xref>] trained a GCN model for gender classification with brain networks as input, where the global average pooling was used as graph pooling method in the graph model. Gopinath et al. [<xref ref-type="bibr" rid="CR15">15</xref>] proposed a learnable pooling strategy in GCNs for brain surface analysis, where the neural networks were split to two separate paths, including computing latent features for each node and predicting the node clusters. Finally, Yang et al. [<xref ref-type="bibr" rid="CR12">12</xref>] developed an edge-weighted graph attention network (GAT) with brain networks as input for classifying Bipolar Disorder, where the dense hierarchical pooling (DHP) [<xref ref-type="bibr" rid="CR16">16</xref>] was used in the model. These studies attest to the utility and power of GNN and related models.</p>
    <p id="Par31">GAT follows a self-attention strategy and calculates the representation of each node in the graph by attending to its neighbors, and it further uses the multi-head attention [<xref ref-type="bibr" rid="CR17">17</xref>] to increase the representation capability of the model [<xref ref-type="bibr" rid="CR14">14</xref>]. To interpret GNN models, a few explanation methods have been applied to GNN classification models. For example, class activation mapping has been used to identify salient nodes (brain regions) [<xref ref-type="bibr" rid="CR11">11</xref>], and to visualize effective features by gradient sensitivity [<xref ref-type="bibr" rid="CR12">12</xref>]. These approaches have led to useful insights into the applications of graph neural networks for brain network analysis.</p>
    <p id="Par32">However, it is still challenging to construct accurate graph neural networks and to interpret the specific decisions of these networks for brain network analysis. For example, the pooling method on brain networks is challenging to perform and has room for improvement. In particular, pooling operations for graphs are used to scale down the size of graph representations, and thus reduce overfitting for GNN models [<xref ref-type="bibr" rid="CR18">18</xref>]. Most pooling methods, such as max-pooling, average-pooling, and DHP, usually follow artificial rules to summarize graph representation from node representation, which would limit the representation ability of the graph. There are also serious challenges to interpret GNN models, as the interpretation of GNNs need to leverage rich relational information and node features in the brain network data.</p>
    <p id="Par33">In this paper, we propose a new graph attention network based learning and interpreting method, namely GAT-LI, which is an accurate graph attention network model for learning to classify functional brain networks, and it interprets the learned graph model with feature importance. Specifically, GAT-LI includes two stages of learning and interpreting. First, in the learning stage, a graph attention network model, namely GAT2, learns to classify functional brain networks of ASD individuals versus healthy controls (HC). In GAT2 model, graph attention layers are used to learn the node representation, and a novel attention pooling layer is designed to obtain the functional brain network representation based on the node representation. Different from artificial rules, the proposed pooling method uses learnable parameters to summarize graph representation from every node’s representation with a unitary learnable standard. Second, in the interpreting stage, we use GNNExplainer [<xref ref-type="bibr" rid="CR19">19</xref>] to interpret learned GAT2 model with feature importance. GNNExplainer is a model-agnostic approach, which could generate consistent and concise interpretation for an entire class of instances.</p>
    <p id="Par34">We experimentally compared the GAT2 model’s performance against the performances of well-known classification models including support vector machine (SVM), random forest (RF), MultiLayer Perceptron (MLP), convolutional neural networks (CNN), GCN layers based GNN models, and GAT layers based on GNN models in a large dataset containing 1035 subjects from the Autism Brain Imaging Data Exchange I (ABIDE I) database [<xref ref-type="bibr" rid="CR20">20</xref>]. The results showed that the proposed GAT2 model achieved the highest classification performance. We also experimentally compared the influence of different construction methods of brain networks in the GAT2 model. To further demonstrate the utility and power of GAT2 model, we also experimentally validated the GAT2 model in a larger synthetic graph dataset including 4000 samples.</p>
    <p id="Par35">Finally, we experimentally compared GNNExplainer with two well-known interpretation methods, Saliency Map [<xref ref-type="bibr" rid="CR21">21</xref>] and DeepLIFT [<xref ref-type="bibr" rid="CR22">22</xref>], using feature perturbation to interpret the trained GAT2 model. The results showed that the GNNExplainer method interpreted the GAT2 model the best. We further used GNNExplainer to identify the features that have contributed most in classifying ASD cases from healthy controls.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <p id="Par36">In this section, we introduce the construction of functional brain networks, GAT-LI method including GAT2 model and interpretation method, and then we verify the proposed method through classification and interpretation experiments.</p>
    <sec id="Sec3">
      <title>Construction of functional brain networks</title>
      <p id="Par37">The process of functional brain network construction from resting-state fMRI data is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>.<fig id="Fig1"><label>Fig. 1</label><caption><p>Flow chart of functional brain network construction</p></caption><graphic xlink:href="12859_2021_4295_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par38"><italic>Node of network</italic> The whole brain is parcellated into N <italic>ROIs</italic> using the brain atlas. Therefore, each network has N nodes. We use the Harvard Oxford (HO) atlas [<xref ref-type="bibr" rid="CR23">23</xref>], so we have N = 110 nodes.</p>
      <p id="Par39"><italic>Edge and connectivity matrix</italic> The mean time series of each ROI are extracted, and the resting-state functional connectivity (rsFC) between ROIs are measured by computing the Pearson’s correlation coefficient of the extracted time-series. A <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{N}\times \mathrm{N}$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq1.gif"/></alternatives></inline-formula> connectivity matrix is constructed for each subject respectively, which can be represented as<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S=\left[\begin{array}{cc}\begin{array}{cc}{\rho }_{{r}_{1},{r}_{1}}&amp; {\rho }_{{r}_{1},{r}_{2}}\\ {\rho }_{{r}_{2},{r}_{1}}&amp; {\rho }_{{r}_{2},{r}_{2}}\end{array}&amp; \begin{array}{cc}\cdots &amp; {\rho }_{{r}_{1},{r}_{N}}\\ \cdots &amp; {\rho }_{2,{r}_{N}}\end{array}\\ \begin{array}{cc}\vdots &amp; \vdots \\ {\rho }_{{r}_{N},{r}_{1}}&amp; {\rho }_{{r}_{N},{r}_{2}}\end{array}&amp; \begin{array}{cc}\vdots &amp; \vdots \\ \cdots &amp; {\rho }_{{r}_{N},{r}_{N}}\end{array}\end{array}\right],$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mo>⋯</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mo>⋯</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="12859_2021_4295_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${r}_{i}$$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq2.gif"/></alternatives></inline-formula> represents the <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M8"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq3.gif"/></alternatives></inline-formula>th ROI.</p>
      <p id="Par40"><italic>Edge weight</italic> For the connected edges between two nodes, the edge weight is expressed by the absolute value of the Pearson correlation coefficient between the time series of the nodes. That is, for node <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${r}_{i}$$\end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq4.gif"/></alternatives></inline-formula> and node <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${r}_{j}$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq5.gif"/></alternatives></inline-formula>, the edge weight between the two nodes is <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$|{\rho }_{{r}_{i}{r}_{j}}|$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq6.gif"/></alternatives></inline-formula>.</p>
      <p id="Par41"><italic>Node feature</italic> The node feature (or node attribute) of each node (ROI) is represented by its functional connectivity profile with the rest of the regions [<xref ref-type="bibr" rid="CR8">8</xref>], corresponding row of the connectivity matrix, such as:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\varvec{h}}}_{i}=\left\{{\rho }_{{r}_{i},{r}_{1}},{\rho }_{{r}_{i},{r}_{2}},\dots ,{\rho }_{{r}_{i},{r}_{N}}\right\}.$$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="}" open="{"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="12859_2021_4295_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par42">Based on the number of nodes N = 110, a <inline-formula id="IEq7"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$110\times 110$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mn>110</mml:mn><mml:mo>×</mml:mo><mml:mn>110</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq7.gif"/></alternatives></inline-formula> connectivity matrix is constructed for each subject respectively, and the dimensions of node feature is 110.</p>
    </sec>
    <sec id="Sec4">
      <title>GAT2 model</title>
      <p id="Par43">The architecture of the GAT2 model is illustrated in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. The model is composed of two parts: the node representation learning part, and the pooling-and-prediction part. First, the node representation learning part learns the feature representation of the node with the graph attention networks. Then, the pooling-and-prediction part learns the graph representation based on node representation, and learns the prediction probability.<fig id="Fig2"><label>Fig. 2</label><caption><p>The architecture of GAT2</p></caption><graphic xlink:href="12859_2021_4295_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par44"><italic>Node representation learning</italic> The input to the layer is a set of node features, <inline-formula id="IEq8"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf{h}=\left\{{{\varvec{h}}}_{1},{{\varvec{h}}}_{2},\dots ,{{\varvec{h}}}_{N}\right\}$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="}" open="{"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq8.gif"/></alternatives></inline-formula>, <inline-formula id="IEq9"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\varvec{h}}}_{i}\in {\mathbb{R}}^{F}$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>F</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq9.gif"/></alternatives></inline-formula>, where N is the number of nodes, F is the dimensions of node features. The graph attention layer [<xref ref-type="bibr" rid="CR17">17</xref>] uses self-attention mechanism to aggregate the node’s 1-hop neighborhood nodes to compute the node representation. The attention coefficients are computed as follows:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${a}_{ij}=softmax\left(LeakyReLU\left({{\varvec{a}}}^{{\varvec{T}}}\left[\mathrm{W}{{\varvec{h}}}_{i}\Vert \mathrm{W}{{\varvec{h}}}_{j}\right]\right)\right),$$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced close=")" open="("><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced close=")" open="("><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:msup><mml:mfenced close="]" open="["><mml:mi mathvariant="normal">W</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mfenced></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="12859_2021_4295_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq10"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\varvec{a}}\in {\mathbb{R}}^{2{F}^{{\prime}}}$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq10.gif"/></alternatives></inline-formula> and the self-attention is included in <inline-formula id="IEq11"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\varvec{a}}$$\end{document}</tex-math><mml:math id="M28"><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq11.gif"/></alternatives></inline-formula>. Masked attention is used to introduce network structure information, and attention is only assigned to the neighbor node set <inline-formula id="IEq12"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${N}_{i}$$\end{document}</tex-math><mml:math id="M30"><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq12.gif"/></alternatives></inline-formula> of node <inline-formula id="IEq13"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M32"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq13.gif"/></alternatives></inline-formula>. The node representation generated from multi-head attention is computed as follows:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\varvec{h}}}_{i}^{{{\prime}}}={\parallel }_{k=1}^{K}\sigma \left(\sum\limits_{j\epsilon {N}_{i}}{a}_{ij}^{k}{\mathrm{W}}^{k}{{\varvec{h}}}_{j}\right),$$\end{document}</tex-math><mml:math id="M34" display="block"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mi>ϵ</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="12859_2021_4295_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\varvec{h}}}_{i}^{{\prime}}=\sigma \left(\frac{1}{K}\sum\limits_{k=1}^{K}\sum\limits_{j\in {N}_{i}}{a}_{ij}^{k}{\text{W}}^{k}{{\varvec{h}}}_{j}\right),$$\end{document}</tex-math><mml:math id="M36" display="block"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mtext>W</mml:mtext></mml:mrow><mml:mi>k</mml:mi></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="12859_2021_4295_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where the Eq. (<xref rid="Equ4" ref-type="">4</xref>) uses <inline-formula id="IEq14"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\parallel$$\end{document}</tex-math><mml:math id="M38"><mml:mo stretchy="false">‖</mml:mo></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq14.gif"/></alternatives></inline-formula> as the concatenation operation, connecting the feature representations obtained by each attention; Eq. (<xref rid="Equ5" ref-type="">5</xref>) is used to obtain the node representation of the last layer by averaging the features with multiple attentions; and <inline-formula id="IEq15"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma =\frac{1}{1+{e}^{-x}}$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq15.gif"/></alternatives></inline-formula>.</p>
      <p id="Par45"><italic>Graph attention pooling</italic> For summarizing graph representation from nodes representation, we provide a sharing weight vector for every node, and the new one-dimensional representation <inline-formula id="IEq16"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}$$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq16.gif"/></alternatives></inline-formula> of each node is obtained through function mapping, as shown in Eq. (<xref rid="Equ6" ref-type="">6</xref>). Finally, we get the graph representation <inline-formula id="IEq17"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf{P}=\left\{{P}_{1},{P}_{2},\dots ,{P}_{N}\right\}$$\end{document}</tex-math><mml:math id="M44"><mml:mrow><mml:mi mathvariant="bold">P</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="}" open="{"><mml:msub><mml:mi>P</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq17.gif"/></alternatives></inline-formula> whose dimensions are equal to the number of nodes.<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}=\sigma \left({\mathrm{W}}^{p}{{\varvec{h}}}_{i}^{{\prime}}\right),$$\end{document}</tex-math><mml:math id="M46" display="block"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:msup><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:msup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="12859_2021_4295_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq18"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{W}}^{{\varvec{p}}}\in 1\times {F}^{{\prime}}$$\end{document}</tex-math><mml:math id="M48"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq18.gif"/></alternatives></inline-formula>.</p>
      <p id="Par46"><italic>Prediction</italic> In order to pay attention to the contribution made by each node to the final prediction result, each node representation is given a weight, and the weight calculation is shown in Eq. (<xref rid="Equ7" ref-type="">7</xref>):<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf{A}=softmax\left({\mathrm{W}}^{A}\mathbf{P}\right),$$\end{document}</tex-math><mml:math id="M50" display="block"><mml:mrow><mml:mi mathvariant="bold">A</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced close=")" open="("><mml:msup><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msup><mml:mi mathvariant="bold">P</mml:mi></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="12859_2021_4295_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq19"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{W}}^{{\varvec{p}}}\in \mathrm{N}\times \mathrm{N}$$\end{document}</tex-math><mml:math id="M52"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">N</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq19.gif"/></alternatives></inline-formula> and <inline-formula id="IEq20"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf{P}=\left\{{P}_{1},{P}_{2},\dots ,{P}_{N}\right\}$$\end{document}</tex-math><mml:math id="M54"><mml:mrow><mml:mi mathvariant="bold">P</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="}" open="{"><mml:msub><mml:mi>P</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq20.gif"/></alternatives></inline-formula>. Then, using the contribution weights, the weighted sum of the node representation is used for the prediction of the model, as shown in Eq. (<xref rid="Equ8" ref-type="">8</xref>):<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{prob}=\sum\limits_{i=1}^{N}{\mathbf{A}}_{i}{P}_{i}.$$\end{document}</tex-math><mml:math id="M56" display="block"><mml:mrow><mml:mi mathvariant="normal">prob</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi mathvariant="bold">A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="12859_2021_4295_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula></p>
    </sec>
    <sec id="Sec5">
      <title>Interpretation methods</title>
      <p id="Par47">We use GNNExplainer [<xref ref-type="bibr" rid="CR19">19</xref>] to interpret the trained GAT2 model, and identify the important features in GAT2 model. We use the GNNExplainer to learn a feature mask that masks out unimportant node features, i.e., where if the value of an element in feature mask matrix is closely to zero, the corresponding feature would be considered unimportant. The dimension of the feature mask matrix is 110 × 110 in this study.</p>
    </sec>
    <sec id="Sec6">
      <title>Experiments</title>
      <sec id="Sec7">
        <title>Dataset and preprocessing</title>
        <p id="Par48">We used the resting-state fMRI data from 1035 subjects in the ABIDE I initiative [<xref ref-type="bibr" rid="CR20">20</xref>] for this study. The dataset includes 505 individuals diagnosed as having ASD and 530 HC. The preprocessed resting-state fMRI data were downloaded from the Preprocessed Connectomes Project (<ext-link ext-link-type="uri" xlink:href="http://preprocessed-connectomes-project.org/abide/download.html">http://preprocessed-connectomes-project.org/abide/download.html</ext-link>). The data were preprocessed by the Configurable Pipeline for the Analysis of Connectomes (CPAC) pipeline [<xref ref-type="bibr" rid="CR24">24</xref>] that included the following procedure: slice timing correction, motion realignment, intensity normalization, regression of nuisance signals, band-pass filtering (0.01–0.1 Hz) and registration of fMRI images to standard anatomical space (MNI152).</p>
      </sec>
      <sec id="Sec8">
        <title>Experimental setup</title>
        <p id="Par49">Given the above GAT2 model, we conducted experiments on the ABIDE I dataset with 1035 subjects and applied the interpretation method to explain the results.</p>
        <p id="Par50">To evaluate the performance of the proposed model, we used sensitivity, specificity, accuracy, F1 score, AUC, and Matthews correlation coefficient (MCC) as our metrics. These metrics are defined as follows:<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$sensitivity = \frac{TP}{{TP + FN}}$$\end{document}</tex-math><mml:math id="M58" display="block"><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2021_4295_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$specificity = \frac{TN}{{TN + FP}}$$\end{document}</tex-math><mml:math id="M60" display="block"><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2021_4295_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$accuracy = \frac{TP + TN}{{TP + FN + TN + FP}}$$\end{document}</tex-math><mml:math id="M62" display="block"><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2021_4295_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F1 = \frac{2 \times TP}{{2 \times TP + FP + FN}}$$\end{document}</tex-math><mml:math id="M64" display="block"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2021_4295_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MCC = \frac{TP \times TN - FP \times FN}{{\sqrt {(TP + FP) \times (TP + FN) \times (TN + FP) \times (TN + FN)} }}$$\end{document}</tex-math><mml:math id="M66" display="block"><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:msqrt><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2021_4295_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula>where true positive (TP) is defined as the number of ASD subjects that are correctly classified, false positive (FP) is the number of HC subjects that are misclassified as ASD subjects, true negative (TN) is defined as the number of HC subjects that are correctly classified, and false negative (FN) is defined as the number of ASD subjects that are misclassified as HC subjects. Sensitivity measures the proportion of correctly identified ASD subjects among all identified ASD subjects. Specificity measures the proportion of correctly identified HC subjects among all real HC subjects. AUC is defined as the area under the receiver operating characteristic curve.</p>
      </sec>
      <sec id="Sec9">
        <title>Classification comparison models and parameters</title>
        <p id="Par51">The comparison models include (i) traditional machine learning methods: SVM, PCA + SVM, and RF; (ii) non-graph deep learning model: MLP, CNN; (iii) GCN layer based GNN models: GCN-at (1st-order), and GCN-at (Cheby); (iv) GAT layer based GNN models: GAT2, GAT-average, and GAT-fc. The comparison models and their corresponding parameters are described as follows.</p>
        <p id="Par52"><italic>SVM</italic> Support vector machine (SVM) model with linear kernel. SVM method is an accepted benchmark method and has been widely used to classify fMRI data for brain disorders. SVM model sets the value of parameter C to 1.0.</p>
        <p id="Par53"><italic>PCA</italic> + <italic>SVM</italic> First use principal component analysis (PCA) to reduce the dimension of feature vector and then input into the SVM model for training and classification. Using PCA to retain 99% of the feature information, the dimension is reduced to 700 dimensions, and the dimensionality-reduced vector is input into the SVM with linear kernel for training and classification. The coefficient C is set to 1.0.</p>
        <p id="Par54"><italic>RF</italic> Random forest (RF) is an ensemble learning method for classification. We trained RF with 300 trees, and the maximum depth of the tree is set to 30.</p>
        <p id="Par55"><italic>MLP</italic> The MultiLayer Perceptron (MLP) model has two fully connected layers with LeakyReLU activation function. The number of units of the two fully connected hidden layers is 64, 32 respectively. Dropout layer is added to avoid overfitting and the dropout rate is 0.5. The output layer with one neuron is followed by a sigmoid activation function. The model training uses the Adam Optimizer, the learning rate is set to 0.0005, and the loss function uses the cross-entropy loss function.</p>
        <p id="Par56"><italic>CNN</italic> The convolutional neural networks (CNNs) model contains three convolutional layers and two fully connected layers, the number of convolutional kernels is 32, 64, 128 respectively, the size of all kernels is 3 * 3, and the activation function uses ReLU function. The number of neurons is 1024, 2 respectively, and the activation function uses ReLU function.</p>
        <p id="Par57"><italic>GCN-at (1st-order), GCN-at (Cheby)</italic> In order to verify the effectiveness of GAT layer for node representation learning in the GAT2 model, we designed GCN-at (1st-order) and GCN-at (Cheby) models to classify the functional brain networks. In these two models, the GCN layer is used for training to obtain the node representation, and then the node representation is input into the same pooling-and-prediction part of GAT2 for prediction.</p>
        <p id="Par58">According to the implementation of the GCN layer proposed in [<xref ref-type="bibr" rid="CR25">25</xref>], for the GCN-at (1st-order) model, the node representation is obtained from the GCN layer via a first-order approximation of localized spectral filters on graphs; for the GCN-at (Cheby) model, the node representation is obtained from the GCN layer via Chebyshev polynomials filter, the polynomial order is set to 3. The model contains one GCN layer, the number of units is set to 24, and the activation function uses the LeakyReLU function. The loss function uses the cross-entropy loss function.</p>
        <p id="Par59"><italic>GAT-fc, GAT-average, GAT-learn</italic> In order to verify the validity of the prediction part in the GAT2 model, we designed GAT-fc, GAT-learn, and GAT-average models to classify the functional brain networks.</p>
        <p id="Par60">In GAT-fc model, after obtaining the node representation vector through the GAT layer, the node representation vectors were spliced to obtain a one-dimensional vector, which is input into the fully connected layer for prediction.</p>
        <p id="Par61">The GAT-fc model contains two GAT layers, the number of attention heads is set to 5 and 3, the number of units is set to 24 and 3, respectively; the number of units of the fully connected layer is set to 64. The activation function uses LeakyReLU function. The output layer is followed by a softmax activation function. The loss function uses cross-entropy loss function.</p>
        <p id="Par62">In GAT-average model, after obtaining the node representation vector in the GAT layer, the node representation <inline-formula id="IEq21"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}$$\end{document}</tex-math><mml:math id="M68"><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq21.gif"/></alternatives></inline-formula> is mapped through the sigmoid function. Based on the average-pooling method in GCN [<xref ref-type="bibr" rid="CR11">11</xref>], the final prediction probability of GAT-average model is obtained by averaging the information of each node, as shown in Eq. (<xref rid="Equ14" ref-type="">14</xref>):<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{prob}=\frac{{\sum }_{i=1}^{N}{P}_{i}}{N}.$$\end{document}</tex-math><mml:math id="M70" display="block"><mml:mrow><mml:mi mathvariant="normal">prob</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="12859_2021_4295_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par63">The GAT-average model contains two GAT layers, the number of attention heads is set to 5 and 3, the number of units is set to 24 and 3, respectively, and the activation function uses LeakyReLU function. The loss function uses the cross-entropy loss function.</p>
        <p id="Par64">In GAT-learn model, we use the learnable pooling method in [<xref ref-type="bibr" rid="CR15">15</xref>] for GAT. The GAT-learn model comprises two GAT layers, one cascaded convolution-pooling blocks, and one fully-connected layer. The block generates an <inline-formula id="IEq22"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N\times 11$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mn>11</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq22.gif"/></alternatives></inline-formula> feature map (<inline-formula id="IEq23"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Y}^{(l)}$$\end{document}</tex-math><mml:math id="M74"><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq23.gif"/></alternatives></inline-formula>) and an <inline-formula id="IEq24"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N\times 1$$\end{document}</tex-math><mml:math id="M76"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq24.gif"/></alternatives></inline-formula> cluster assignment matrix (<inline-formula id="IEq25"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S}^{T}$$\end{document}</tex-math><mml:math id="M78"><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq25.gif"/></alternatives></inline-formula>) in two separate paths, and combines them using pooling formulation of Eq. (<xref rid="Equ15" ref-type="">15</xref>) to obtain a pooled feature map (<inline-formula id="IEq26"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y^{pool}$$\end{document}</tex-math><mml:math id="M80"><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="italic">pool</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq26.gif"/></alternatives></inline-formula>) of 1 * 11.<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y^{pool} = S^{T} Y^{(l)}$$\end{document}</tex-math><mml:math id="M82" display="block"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="italic">pool</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math><graphic xlink:href="12859_2021_4295_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par65"><italic>GAT2</italic> The model contains two GAT layers, the number of attention heads is set to 5 and 3, the number of neurons is set to 24 and 3, respectively, and the activation function uses LeakyReLU function. The node representation <inline-formula id="IEq27"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}$$\end{document}</tex-math><mml:math id="M84"><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq27.gif"/></alternatives></inline-formula> is obtained through the sigmoid function. Then the weighted sum of the node information is used for the prediction of the model. And we also set different number of GAT layers and different number of attention heads for comparing these hyper-parameters setting.</p>
        <p id="Par66">For inputs fed into non-graph learning models including SVM, PCA + SVM, RF, MLP, the upper triangle values of connectivity matrices are extracted and flattened into vectors, with the dimension of the feature vector being <inline-formula id="IEq28"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left(110\times \left(110-1\right)\right)/2=5995$$\end{document}</tex-math><mml:math id="M86"><mml:mrow><mml:mfenced close=")" open="("><mml:mn>110</mml:mn><mml:mo>×</mml:mo><mml:mfenced close=")" open="("><mml:mn>110</mml:mn><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mfenced></mml:mfenced><mml:mo stretchy="false">/</mml:mo><mml:mn>2</mml:mn><mml:mo>=</mml:mo><mml:mn>5995</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq28.gif"/></alternatives></inline-formula>. The whole connectivity matrices are used as inputs for CNN model.</p>
        <p id="Par67">All the above graph neural networks based models use Adam Optimizer for training and the learning rate is set to 0.0001. All the above deep learning models use the early stop mechanism, and the training is stopped if the test set for 15 consecutive rounds does not decrease in error rates.</p>
      </sec>
      <sec id="Sec10">
        <title>Comparison of classification with different network construction methods</title>
        <p id="Par68">We conducted more experiments to compare the classification performance of the GAT2 model with different network construction methods.<list list-type="simple"><list-item><label>(i)</label><p id="Par69">Influence of network construction via different brain atlases</p><p id="Par70">We used HO atlas [<xref ref-type="bibr" rid="CR23">23</xref>] and Automated Anatomical Labeling (AAL) atlas [<xref ref-type="bibr" rid="CR26">26</xref>] to divide brain regions, extracted functional connectivity features to construct brain networks, and compared the performance of classification with GAT2 model.</p></list-item></list></p>
        <p id="Par71">
          <list list-type="simple">
            <list-item>
              <label>(ii)</label>
              <p id="Par72">Influence of network sparsity</p>
              <p id="Par73">Considering that even weak connections between nodes may record some information, so we used dense network representation for classification in the classification experiments, where the dense network is the original network without using thresholds to eliminate weak connections.</p>
            </list-item>
          </list>
        </p>
        <p id="Par74">In this study, we set a threshold for the sparse brain network, and identified the influence of network sparsity. For the adjacency matrix, according to the edge weight value between nodes, only the connected edges whose edge weight value is greater than the threshold were retained. The GAT2 model was used for experimental comparison.</p>
      </sec>
      <sec id="Sec11">
        <title>Validating GAT2 in a larger dataset</title>
        <p id="Par75">We also validated the performance of GAT2 model in a larger synthetic dataset. We constructed a graph classification dataset with 4000 graphs, where each graph had 30 nodes and the weight of each connection was randomly selected from 0 to 1. The graph dataset was divided into two categories based on the following steps: (a) 15 nodes from the graph were randomly selected; (b) the sum of the connection weights between these 15 nodes was defined as W1, the sum of the connection weights between these 15 nodes and the rest 15 nodes was defined as W2, the sum of each graph was defined as <inline-formula id="IEq29"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{W}0=\mathrm{W}1\times 2+\mathrm{W}2$$\end{document}</tex-math><mml:math id="M88"><mml:mrow><mml:mi mathvariant="normal">W</mml:mi><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mi mathvariant="normal">W</mml:mi><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">W</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq29.gif"/></alternatives></inline-formula>, and the average value of W0 of 4000 graphs was then calculated; and (c) if W0 was larger than the average values, the category of this graph was set to Class-one, otherwise the category of the graph was set to Class-two. We also used corresponding row of the connectivity matrix to be node feature similar to the construction of brain networks described in “<xref rid="Sec4" ref-type="sec">Construction of functional brain networks</xref>” section.</p>
        <p id="Par76">We compared the classifying performance of GAT2 model against SVM, RF, and CNN, under the similar setting with the previous experiments of ABIDE dataset. Some specific model parameters used in this experiment are as follows: The GAT2 model contained two GAT layers, the number of attention heads was set to 4 and 4, the number of neurons was set to 16 and 16, respectively; the CNN model contained three convolutional layers and two fully connected layers, the number of convolutional kernels was 16, 32, 64 respectively; the RF had 128 trees, and the maximum depth of the tree was set to 20.</p>
      </sec>
      <sec id="Sec12">
        <title>Interpretation experiments</title>
        <p id="Par77">
          <list list-type="simple">
            <list-item>
              <label>(i)</label>
              <p id="Par78">Comparison methods</p>
              <p id="Par79">We also used Saliency Map [<xref ref-type="bibr" rid="CR21">21</xref>] and DeepLIFT [<xref ref-type="bibr" rid="CR22">22</xref>] as comparative interpretation methods. Saliency Map is a typical neural network interpretation method, which is based on gradient sensitivity. To apply Saliency Map to the GAT2 model, we calculated the gradient of the model loss relative to the input features, and analyzed the features according to the gradient value. The larger the gradient value, the greater the impact the corresponding feature has on the classification. DeepLIFT is a method that can decompose the output prediction of a neural network on a specific input by back propagating the contributions of all neurons in the network to each feature of the input.</p>
            </list-item>
          </list>
        </p>
        <p id="Par80">We explored the impact of features on classifying functional brain networks of the ASD individuals. The sample feature dimension of the input model is N × F, in which N represents the number of nodes, and F represents the node feature dimension. As described in “<xref rid="Sec5" ref-type="sec">GAT2 model</xref>” section, the constructed network has N = 110 network nodes and F = 110 features of each node. The steps of obtaining the characteristic gradient value are as follows: (a) for the test samples, the gradient of the model loss relative to the input features was calculated to obtain the gradient value of each feature; (b) for each feature, the average value of the gradient across all samples was identified and the absolute value of them was calculated.<list list-type="simple"><list-item><label>(ii)</label><p id="Par81">Interpretation experiments</p><p id="Par82">We applied Saliency Map, DeepLIFT, and GNNExplainer to interpret the trained GAT2 model, and estimated the classification performance impact of GAT2 models by the feature perturbation. We then compared the change of GAT2’s prediction when modifying the same number of features to compare the quality of the two interpretation methods.</p></list-item></list></p>
        <p id="Par83">We hacked the model by setting value of the nodal feature in instance x to zero, and observe the changes of prediction of GAT2 in one-fold data from the above fivefold cross-validation data division. We used metrics including sensitivity, specificity, accuracy, the change of prediction probability (CPP) which is the absolute change of probability of classifying <inline-formula id="IEq30"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{x}$$\end{document}</tex-math><mml:math id="M90"><mml:mi mathvariant="normal">x</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq30.gif"/></alternatives></inline-formula> as a positive instance, the number of label-changed instance (NLCI) which is the number of instances whose predicted label changes after being hacked.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec13">
    <title>Results</title>
    <sec id="Sec14">
      <title>Classification results</title>
      <sec id="Sec15">
        <title>Results of comparison models</title>
        <p id="Par84">The classification results of each model are shown in Table <xref rid="Tab1" ref-type="table">1</xref>. After randomly performing fivefold cross-validation data division, in each round of experiments, one-fold data were used for testing while other fourfold data were used for training the model. A specialized computer with i7-6700 K CPU, 64 GB RAM, and a NVIDIA GTX 1080 Ti GPU was used to train the models. For training GAT2 model, average number of epochs was 232, training batch size was 10, and the average training time was 329.9 s.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Classification performance of each model (mean <inline-formula id="IEq31"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\pm$$\end{document}</tex-math><mml:math id="M92"><mml:mo>±</mml:mo></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq31.gif"/></alternatives></inline-formula> std)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Accuracy</th><th align="left">Sensitivity</th><th align="left">Specificity</th><th align="left">F1</th><th align="left">AUC</th><th align="left">MCC</th></tr></thead><tbody><tr><td align="left">SVM</td><td char="±" align="char">0.6618 ± 0.0110</td><td char="±" align="char">0.6515 ± 0.0413</td><td char="±" align="char">0.6717 ± 0.0218</td><td char="±" align="char">0.6521 ± 0.0211</td><td char="±" align="char">0.7170 ± 0.0188</td><td char="±" align="char">0.3238 ± 0.0230</td></tr><tr><td align="left">PCA + SVM</td><td char="±" align="char">0.6686 ± 0.0195</td><td char="±" align="char">0.6554 ± 0.0561</td><td char="±" align="char">0.6811 ± 0.0334</td><td char="±" align="char">0.6576 ± 0.0300</td><td char="±" align="char">0.7184 ± 0.0156</td><td char="±" align="char">0.2793 ± 0.0339</td></tr><tr><td align="left">RF</td><td char="±" align="char">0.6599 ± 0.0309</td><td char="±" align="char">0.5921 ± 0.0309</td><td char="±" align="char"><bold>0.7245 ± 0.0324</bold></td><td char="±" align="char">0.6295 ± 0.0330</td><td char="±" align="char">0.7153 ± 0.0325</td><td char="±" align="char">0.2978 ± 0.0768</td></tr><tr><td align="left">MLP</td><td char="±" align="char">0.6754 ± 0.0309</td><td char="±" align="char">0.6634 ± 0.0401</td><td char="±" align="char">0.6868 ± 0.0601</td><td char="±" align="char">0.6660 ± 0.0297</td><td char="±" align="char"><bold>0.7535 ± 0.0297</bold></td><td char="±" align="char">0.2899 ± 0.0612</td></tr><tr><td align="left">CNN</td><td char="±" align="char">0.6550 ± 0.0312</td><td char="±" align="char">0.6316 ± 0.0466</td><td char="±" align="char">0.6774 ± 0.0345</td><td char="±" align="char">0.6407 ± 0.0364</td><td char="±" align="char">0.7111 ± 0.0314</td><td char="±" align="char">0.3098 ± 0.0615</td></tr><tr><td align="left">GCN-at (1st-order)</td><td char="±" align="char">0.5971 ± 0.0460</td><td char="±" align="char">0.6059 ± 0.0398</td><td char="±" align="char">0.5887 ± 0.0619</td><td char="±" align="char">0.5951 ± 0.0417</td><td char="±" align="char">0.6537 ± 0.0503</td><td char="±" align="char">0.2775 ± 0.0645</td></tr><tr><td align="left">GCN-at (Cheby)</td><td char="±" align="char">0.6357 ± 0.0217</td><td char="±" align="char">0.6812 ± 0.0558</td><td char="±" align="char">0.5925 ± 0.0558</td><td char="±" align="char">0.6452 ± 0.0262</td><td char="±" align="char">0.6926 ± 0.0368</td><td char="±" align="char">0.2975 ± 0.0600</td></tr><tr><td align="left">GAT-fc</td><td char="±" align="char">0.6184 ± 0.0332</td><td char="±" align="char">0.7089 ± 0.0507</td><td char="±" align="char">0.5321 ± 0.0927</td><td char="±" align="char">0.6445 ± 0.0209</td><td char="±" align="char">0.6547 ± 0.0426</td><td char="±" align="char">0.3155 ± 0.0713</td></tr><tr><td align="left">GAT-average</td><td char="±" align="char">0.6734 ± 0.0354</td><td char="±" align="char">0.7386 ± 0.0270</td><td char="±" align="char">0.6113 ± 0.0801</td><td char="±" align="char">0.6889 ± 0.0226</td><td char="±" align="char">0.7361 ± 0.0321</td><td char="±" align="char">0.3237 ± 0.0621</td></tr><tr><td align="left">GAT-learn</td><td char="±" align="char">0.5845 ± 0.0371</td><td char="±" align="char">0.6000 ± 0.1765</td><td char="±" align="char">0.5698 ± 0.1473</td><td char="±" align="char">0.5732 ± 0.0844</td><td char="±" align="char">0.5849 ± 0.0385</td><td char="±" align="char">0.1798 ± 0.0821</td></tr><tr><td align="left">GAT2</td><td char="±" align="char"><bold>0.6802 ± 0.0269</bold></td><td char="±" align="char"><bold>0.7406 ± 0.0408</bold></td><td char="±" align="char">0.6226 ± 0.0534</td><td char="±" align="char"><bold>0.6931 ± 0.0248</bold></td><td char="±" align="char">0.7358 ± 0.0373</td><td char="±" align="char"><bold>0.3426 ± 0.0628</bold></td></tr></tbody></table><table-wrap-foot><p>The bold means it is the best result for each metric (column of the table)</p></table-wrap-foot></table-wrap></p>
        <p id="Par85">The GAT2 model achieved the best results in accuracy, sensitivity, F1 score, and MCC indicators using fivefold cross-validation, with the accuracy of 68.02%, sensitivity of 74.06%, F1 score of 69.31%, and MCC of 0.3426.</p>
        <p id="Par86">From Table <xref rid="Tab1" ref-type="table">1</xref>, we could find that the deep learning models (MLP and GAT2) achieved better performance than the traditional machine learning methods (SVM, PCA + SVM). The MLP model achieved the highest AUC value of 0.7535. The accuracy, sensitivity, F1 score, and MCC of the GAT2 model were higher than the MLP model, and the total classification performance was slightly better than the MLP model.</p>
        <p id="Par87">Compared with GCN layer based graph models, the classification performance of the GAT2 model (with GAT layers) was better than GCN-at (1st-order) and GCN-at (Cheby) with GCN layers.</p>
        <p id="Par88">Compared the GAT layer based models, GAT2 model achieved the best results. The classification performance of the three was GAT2 &gt; GAT-average &gt; GAT-fc &gt; GAT-learn. In GAT-learn, there are two separate paths of neural networks to learn the pooling strategy, and the worst performance of this model may be due to the complex structure which makes it easy to overfit for this dataset. In GAT-fc, the node representation output from the GAT layer was flattened into a one-dimensional vector, and then entered to the fully connected layer for training and classification. The bad performance of GAT-fc may be due to the direct splicing of the node representation, which lost the information learned by each node. GAT-average, which retains the information of each node on average, does not consider that different nodes may contribute differently to the prediction results, so the classification effect was not as good as GAT2; GAT2 uses a weighted layer to learn each node representation, the information of each node was retained for final prediction, and the performance was significantly improved.</p>
        <p id="Par89">In summary, the proposed GAT2 model achieves the best results compared to other ten models, including SVM, PCA + SVM, RF, MLP, CNN, GCN-at (1st-order), GCN-at (Cheby), GAT-fc, GAT-average, and GAT-learn.</p>
      </sec>
      <sec id="Sec16">
        <title>Results of GAT2 with different neural network structures</title>
        <p id="Par90">The results of GAT2 with different neural network structures are shown in Table <xref rid="Tab2" ref-type="table">2</xref>. We compared different number of attention layers, and the number of attention multi-head for each layer.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Performance of GAT2 with different neural network structures (mean <inline-formula id="IEq32"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\pm$$\end{document}</tex-math><mml:math id="M94"><mml:mo>±</mml:mo></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq32.gif"/></alternatives></inline-formula> std)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Number of layers</th><th align="left">Number of multi-head for each layer</th><th align="left">Accuracy</th><th align="left">Sensitivity</th><th align="left">Specificity</th><th align="left">F1</th><th align="left">AUC</th><th align="left">MCC</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">5</td><td char="±" align="char">0.6696 ± 0.0332</td><td char="±" align="char">0.6416 ± 0.0440</td><td char="±" align="char"><bold>0.6962 ± 0.0307</bold></td><td char="±" align="char">0.6541 ± 0.0378</td><td char="±" align="char">0.7251 ± 0.0388</td><td char="±" align="char">0.3385 ± 0.0668</td></tr><tr><td align="left">3</td><td align="left">5, 5, 3</td><td char="±" align="char">0.6415 ± 0.0422</td><td char="±" align="char">0.6435 ± 0.1286</td><td char="±" align="char">0.6396 ± 0.0629</td><td char="±" align="char">0.6312 ± 0.0629</td><td char="±" align="char">0.7145 ± 0.0480</td><td char="±" align="char">0.2923 ± 0.0796</td></tr><tr><td align="left">2</td><td align="left">5, 5</td><td char="±" align="char">0.6676 ± 0.0409</td><td char="±" align="char">0.6812 ± 0.0706</td><td char="±" align="char">0.6547 ± 0.0935</td><td char="±" align="char">0.6660 ± 0.0404</td><td char="±" align="char">0.7261 ± 0.0384</td><td char="±" align="char">0.3390 ± 0.0787</td></tr><tr><td align="left">2</td><td align="left">3, 3</td><td char="±" align="char">0.6599 ± 0.0371</td><td char="±" align="char">0.6753 ± 0.0413</td><td char="±" align="char">0.6453 ± 0.0642</td><td char="±" align="char">0.6597 ± 0.0337</td><td char="±" align="char">0.7178 ± 0.0529</td><td char="±" align="char">0.3214 ± 0.0731</td></tr><tr><td align="left">2</td><td align="left">5, 3</td><td char="±" align="char"><bold>0.6802 ± 0.0269</bold></td><td char="±" align="char"><bold>0.7406 ± 0.0408</bold></td><td char="±" align="char">0.6226 ± 0.0534</td><td char="±" align="char"><bold>0.6931 ± 0.0248</bold></td><td char="±" align="char"><bold>0.7358 ± 0.0373</bold></td><td char="±" align="char"><bold>0.3426 ± 0.0628</bold></td></tr></tbody></table><table-wrap-foot><p>The bold means it is the best result for each metric (column of the table)</p></table-wrap-foot></table-wrap></p>
      </sec>
      <sec id="Sec17">
        <title>Results of classification with different network construction methods</title>
        <p id="Par91">
          <list list-type="simple">
            <list-item>
              <label>(i)</label>
              <p id="Par92">Influence of network construction via different brain atlases</p>
              <p id="Par93">The classification results of using AAL and HO atlas are shown in Table <xref rid="Tab3" ref-type="table">3</xref>. Compared with the AAL atlas, using the HO atlas for construction of the brain network, with the same model, the accuracy was increased by about 5%, the sensitivity was increased by about 2%, and the F1 value was increased by about 4%. All evaluation metrics have been significantly improved when using the HO atlas.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Classification performance on different brain atlases (mean <inline-formula id="IEq33"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\pm$$\end{document}</tex-math><mml:math id="M96"><mml:mo>±</mml:mo></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq33.gif"/></alternatives></inline-formula> std)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Atlas</th><th align="left">Accuracy</th><th align="left">Sensitivity</th><th align="left">Specificity</th><th align="left">F1</th><th align="left">AUC</th></tr></thead><tbody><tr><td align="left">AAL</td><td char="±" align="char">0.6300 ± 0.0428</td><td char="±" align="char">0.7188 ± 0.0298</td><td char="±" align="char">0.5453 ± 0.0988</td><td char="±" align="char">0.6556 ± 0.0236</td><td char="±" align="char">0.6763 ± 0.0499</td></tr><tr><td align="left">HO</td><td char="±" align="char"><bold>0.6802 ± 0.0269</bold></td><td char="±" align="char"><bold>0.7406 ± 0.0408</bold></td><td char="±" align="char"><bold>0.6226 ± 0.0534</bold></td><td char="±" align="char"><bold>0.6931 ± 0.0248</bold></td><td char="±" align="char"><bold>0.7358 ± 0.0373</bold></td></tr></tbody></table><table-wrap-foot><p>The bold means it is the best result for each metric (column of the table)</p></table-wrap-foot></table-wrap></p>
            </list-item>
            <list-item>
              <label>(ii)</label>
              <p id="Par94">Influence of brain network sparsity</p>
              <p id="Par95">The classification results of using different network sparsity are shown in Table <xref rid="Tab4" ref-type="table">4</xref>. The number of edges and sparsity of the brain network are shown with different threshold for edge weight. As can be seen from the table, when the network became more and more sparser, the accuracy, specificity and F1 value of the model continued to decline. For the two metrics of sensitivity and AUC value, as a whole, as the network became sparser, the value also showed a downward trend. When the threshold was greater than 0.3, the eliminated node connection edges increased, and each index decreased by a large extent. Even if the threshold value was 0.1, the classification accuracy of the model still decreased. It indicates that retaining the weak connection information of the network can enable the node to learn more information from neighboring nodes in this model, which allowed the model to achieve better classification performance.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Classification performance on networks with different sparsity (mean <inline-formula id="IEq34"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\pm$$\end{document}</tex-math><mml:math id="M98"><mml:mo>±</mml:mo></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq34.gif"/></alternatives></inline-formula> std)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Threshold</th><th align="left">Number of edges</th><th align="left">Sparisty</th><th align="left">Accuracy</th><th align="left">Sensitivity</th><th align="left">Specificity</th><th align="left">F1</th><th align="left">AUC</th></tr></thead><tbody><tr><td align="left">0.1</td><td char="." align="char">10,056</td><td align="left">0.1689</td><td char="±" align="char">0.6686 ± 0.0344</td><td char="±" align="char">0.7287 ± 0.0358</td><td char="±" align="char">0.6113 ± 0.0790</td><td char="±" align="char">0.6826 ± 0.0236</td><td char="±" align="char"><bold>0.7396 ± 0.0317</bold></td></tr><tr><td align="left">0.2</td><td char="." align="char">7976</td><td align="left">0.3408</td><td char="±" align="char">0.6512 ± 0.0604</td><td char="±" align="char">0.7327 ± 0.0360</td><td char="±" align="char">0.5736 ± 0.1110</td><td char="±" align="char">0.6738 ± 0.0433</td><td char="±" align="char">0.7165 ± 0.0603</td></tr><tr><td align="left">0.3</td><td char="." align="char">5877</td><td align="left">0.5142</td><td char="±" align="char">0.6377 ± 0.0684</td><td char="±" align="char">0.7149 ± 0.0449</td><td char="±" align="char">0.5642 ± 0.1255</td><td char="±" align="char">0.6600 ± 0.0467</td><td char="±" align="char">0.7005 ± 0.0648</td></tr><tr><td align="left">0.4</td><td char="." align="char">3933</td><td align="left">0.6750</td><td char="±" align="char">0.6232 ± 0.0693</td><td char="±" align="char">0.7129 ± 0.0429</td><td char="±" align="char">0.5377 ± 0.1303</td><td char="±" align="char">0.6506 ± 0.0460</td><td char="±" align="char">0.6878 ± 0.0681</td></tr><tr><td align="left">0.5</td><td char="." align="char">2330</td><td align="left">0.8074</td><td char="±" align="char">0.6145 ± 0.0516</td><td char="±" align="char">0.6594 ± 0.0409</td><td char="±" align="char">0.5717 ± 0.0992</td><td char="±" align="char">0.6263 ± 0.0370</td><td char="±" align="char">0.6831 ± 0.0542</td></tr><tr><td align="left">Dense network</td><td char="." align="char">12,100</td><td align="left">0</td><td char="±" align="char"><bold>0.6802 ± 0.0269</bold></td><td char="±" align="char"><bold>0.7406 ± 0.0408</bold></td><td char="±" align="char"><bold>0.6226 ± 0.0534</bold></td><td char="±" align="char"><bold>0.6931 ± 0.0248</bold></td><td char="±" align="char">0.7358 ± 0.0373</td></tr></tbody></table><table-wrap-foot><p>The bold means it is the best result for each metric (column of the table)</p></table-wrap-foot></table-wrap></p>
            </list-item>
          </list>
        </p>
      </sec>
      <sec id="Sec18">
        <title>Results of validating GAT2 in the larger dataset</title>
        <p id="Par96">The classification results in the larger constructed graph dataset are shown in Table <xref rid="Tab5" ref-type="table">5</xref>. The GAT2 model achieved the best results in accuracy, sensitivity, F1 score, AUC, and MCC indicators using fivefold cross-validation, with the accuracy of 95.18%, sensitivity of 95.68%, specificity of 94.66%, F1 score of 95.26%, AUC of 95.17%, and MCC of 99.78%.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Classification performance in the larger graph dataset (mean <inline-formula id="IEq35"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\pm$$\end{document}</tex-math><mml:math id="M100"><mml:mo>±</mml:mo></mml:math><inline-graphic xlink:href="12859_2021_4295_Article_IEq35.gif"/></alternatives></inline-formula> std)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Accuracy</th><th align="left">Sensitivity</th><th align="left">Specificity</th><th align="left">F1</th><th align="left">AUC</th><th align="left">MCC</th></tr></thead><tbody><tr><td align="left">SVM</td><td char="±" align="char">0.9242 ± 0.0114</td><td char="±" align="char">0.9235 ± 0.0133</td><td char="±" align="char">0.9249 ± 0.0129</td><td char="±" align="char">0.9247 ± 0.0113</td><td char="±" align="char">0.9242 ± 0.0114</td><td char="±" align="char">0.8485 ± 0.0228</td></tr><tr><td align="left">RF</td><td char="±" align="char">0.5975 ± 0.0127</td><td char="±" align="char">0.6367 ± 0.0210</td><td char="±" align="char">0.5541 ± 0.0094</td><td char="±" align="char">0.6133 ± 0.0151</td><td char="±" align="char">0.5954 ± 0.0127</td><td char="±" align="char">0.1916 ± 0.0257</td></tr><tr><td align="left">CNN</td><td char="±" align="char">0.5917 ± 0.0172</td><td char="±" align="char">0.6714 ± 0.0383</td><td char="±" align="char">0.5160 ± 0.0394</td><td char="±" align="char">0.5559 ± 0.1888</td><td char="±" align="char">0.5911 ± 0.0017</td><td char="±" align="char">0.3018 ± 0.0259</td></tr><tr><td align="left">GAT2</td><td char="±" align="char"><bold>0.9518 ± 0.0121</bold></td><td char="±" align="char"><bold>0.9568 ± 0.0344</bold></td><td char="±" align="char"><bold>0.9466 ± 0.0059</bold></td><td char="±" align="char"><bold>0.9526 ± 0.0099</bold></td><td char="±" align="char"><bold>0.9517 ± 0.0123</bold></td><td char="±" align="char"><bold>0.9978 ± 0.0006</bold></td></tr></tbody></table><table-wrap-foot><p>The bold means it is the best result for each metric (column of the table)</p></table-wrap-foot></table-wrap></p>
      </sec>
    </sec>
    <sec id="Sec19">
      <title>Explanation experiments</title>
      <p id="Par97">The results of using Saliency Map, DeepLIFT, and GNNExplainer methods for GAT2 model on the ABIDE dataset are shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. It’s shown that the average CPP and NLCI of GNNExplainer were higher than Saliency Map. And GNNExplainer achieved a bigger change of prediction in sensitivity, specificity and accuracy. It demonstrated that GNNExplainer performed better than Saliency Map when interpreting GAT2 model.<fig id="Fig3"><label>Fig. 3</label><caption><p>The performance of top features on Saliency Map and GNNExplaine</p></caption><graphic xlink:href="12859_2021_4295_Fig3_HTML" id="MO3"/></fig></p>
      <p id="Par98">We further analyzed the impact of top features of GNNExplainer method with Fig. <xref rid="Fig3" ref-type="fig">3</xref>, and it could be found that there is a significant impact on sensitivity, specificity, NLCI, and accuracy when hacking the top features. As seen in Fig. <xref rid="Fig3" ref-type="fig">3</xref>e, we could find that the decline curve of the accuracy had two stages, the first stage dropping faster, and the latter stage dropping more slowly. In the first stage, the accuracy would drop to 0.6470 when hacking the top 605 features; in the latter stage, the accuracy would drop to 0.5603 when hacking the top 2115 features. It indicates that these 605 features have contributed more to the classification of ASD from HC, and the rest of 1510 features, while also having significant impacts on the classification in GAT2 model, do not contribute as much as these 605 features.</p>
      <p id="Par99">We selected the top 10 connections (rsFCs) as shown in Table <xref rid="Tab6" ref-type="table">6</xref>. We computed the mean value of each rsFC of the ASD group and the HC group, respectively, as well as the mean difference of two groups. An independent two-sample t test was run on the means of the rsFC elements of two groups.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Analyses of 10 rsFCs</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Connection ID</th><th align="left">ROI number</th><th align="left">Regions</th><th align="left">ASD mean connection</th><th align="left">HC mean connection</th><th align="left">Mean difference</th><th align="left"><italic>p</italic> value</th></tr></thead><tbody><tr><td align="left" rowspan="2">1</td><td char="." align="char">31</td><td align="left">Right Superior Parietal Lobule</td><td char="." align="char" rowspan="2">0.5816</td><td char="." align="char" rowspan="2">0.5580</td><td char="." align="char" rowspan="2">0.0237</td><td char="." align="char" rowspan="2">0.3161</td></tr><tr><td char="." align="char">33</td><td align="left">Right Supramarginal Gyrus; posterior division</td></tr><tr><td align="left" rowspan="2">2</td><td char="." align="char">31</td><td align="left">Right Superior Parietal Lobule</td><td char="." align="char" rowspan="2">0.3633</td><td char="." align="char" rowspan="2">0.3536</td><td char="." align="char" rowspan="2">0.0097</td><td char="." align="char" rowspan="2">0.7276</td></tr><tr><td char="." align="char">38</td><td align="left">Right Frontal Medial Cortex</td></tr><tr><td align="left" rowspan="2">3</td><td char="." align="char">11</td><td align="left">Right Hippocampus</td><td char="." align="char" rowspan="2">0.2543</td><td char="." align="char" rowspan="2">0.2191</td><td char="." align="char" rowspan="2">0.0353</td><td char="." align="char" rowspan="2">0.2224</td></tr><tr><td char="." align="char">39</td><td align="left">Right Juxtapositional Lobule Cortex (formerly Supplementary Motor Cortex)</td></tr><tr><td align="left" rowspan="2">4</td><td char="." align="char">11</td><td align="left">Right Hippocampus</td><td char="." align="char" rowspan="2">0.3320</td><td char="." align="char" rowspan="2">0.2586</td><td char="." align="char" rowspan="2">0.0734</td><td char="." align="char" rowspan="2">0.0097**</td></tr><tr><td char="." align="char">38</td><td align="left">Right Frontal Medial Cortex</td></tr><tr><td align="left" rowspan="2">5</td><td char="." align="char">62</td><td align="left">Left Frontal Pole</td><td char="." align="char" rowspan="2">0.3600</td><td char="." align="char" rowspan="2">0.4201</td><td char="." align="char" rowspan="2">− 0.0600</td><td char="." align="char" rowspan="2">0.0109**</td></tr><tr><td char="." align="char">19</td><td align="left">Right Inferior Frontal Gyrus; pars opercularis</td></tr><tr><td align="left" rowspan="2">6</td><td char="." align="char">62</td><td align="left">Left Frontal Pole</td><td char="." align="char" rowspan="2">0.0138</td><td char="." align="char" rowspan="2">− 0.0081</td><td char="." align="char" rowspan="2">0.0219</td><td char="." align="char" rowspan="2">0.4854</td></tr><tr><td char="." align="char">51</td><td align="left">Right Temporal Fusiform Cortex; posterior division</td></tr><tr><td align="left" rowspan="2">7</td><td char="." align="char">62</td><td align="left">Left Frontal Pole</td><td char="." align="char" rowspan="2">0.4269</td><td char="." align="char" rowspan="2">0.4778</td><td char="." align="char" rowspan="2">− 0.0509</td><td char="." align="char" rowspan="2">0.0375**</td></tr><tr><td char="." align="char">68</td><td align="left">Left Precentral Gyrus</td></tr><tr><td align="left" rowspan="2">8</td><td char="." align="char">31</td><td align="left">Right Superior Parietal Lobule</td><td char="." align="char" rowspan="2">0.3830</td><td char="." align="char" rowspan="2">0.3622</td><td char="." align="char" rowspan="2">0.0208</td><td char="." align="char" rowspan="2">0.4224</td></tr><tr><td char="." align="char">5</td><td align="left">Left Amygdala</td></tr><tr><td align="left" rowspan="2">9</td><td char="." align="char">62</td><td align="left">Left Frontal Pole</td><td char="." align="char" rowspan="2">− 0.0692</td><td char="." align="char" rowspan="2">− 0.1137</td><td char="." align="char" rowspan="2">0.0445</td><td char="." align="char" rowspan="2">0.1646</td></tr><tr><td char="." align="char">94</td><td align="left">Left Frontal Orbital Cortex</td></tr><tr><td align="left" rowspan="2">10</td><td char="." align="char">62</td><td align="left">Left Frontal Pole</td><td char="." align="char" rowspan="2">0.4016</td><td char="." align="char" rowspan="2">0.3743</td><td char="." align="char" rowspan="2">0.0273</td><td char="." align="char" rowspan="2">0.3308</td></tr><tr><td char="." align="char">42</td><td align="left">Right Cingulate Gyrus; anterior division</td></tr></tbody></table><table-wrap-foot><p>**<italic>p</italic> &lt; 0.05</p></table-wrap-foot></table-wrap></p>
      <p id="Par100">In addition, we also used GNNExplainer to explain the GAT2 model of synthetic graph dataset, and the top 10 connections are shown in Table <xref rid="Tab7" ref-type="table">7</xref>. The mean value of each connection of the Class_one group and the Class_two group, the mean difference of two groups, and the P values were computed similarly as in Table <xref rid="Tab6" ref-type="table">6</xref>.<table-wrap id="Tab7"><label>Table 7</label><caption><p>Analyses of 10 connections in synthetic graph dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Connection ID</th><th align="left">Node ID</th><th align="left">Class_one mean connection</th><th align="left">Class_two mean connection</th><th align="left">Mean difference</th><th align="left"><italic>p</italic> value</th></tr></thead><tbody><tr><td align="left" rowspan="2">1</td><td char="." align="char">16</td><td char="." align="char" rowspan="2">0.5052</td><td char="." align="char" rowspan="2">0.4916</td><td char="." align="char" rowspan="2">0.0135</td><td char="." align="char" rowspan="2">0.0362**</td></tr><tr><td char="." align="char">5</td></tr><tr><td align="left" rowspan="2">2</td><td char="." align="char">13</td><td char="." align="char" rowspan="2">0.5160</td><td char="." align="char" rowspan="2">0.4962</td><td char="." align="char" rowspan="2">0.0197</td><td char="." align="char" rowspan="2">0.0023**</td></tr><tr><td char="." align="char">5</td></tr><tr><td align="left" rowspan="2">3</td><td char="." align="char">4</td><td char="." align="char" rowspan="2">0.5079</td><td char="." align="char" rowspan="2">0.4904</td><td char="." align="char" rowspan="2">0.0175</td><td char="." align="char" rowspan="2">0.0073**</td></tr><tr><td char="." align="char">1</td></tr><tr><td align="left" rowspan="2">4</td><td char="." align="char">15</td><td char="." align="char" rowspan="2">0.5061</td><td char="." align="char" rowspan="2">0.4995</td><td char="." align="char" rowspan="2">0.0066</td><td char="." align="char" rowspan="2">0.3090</td></tr><tr><td char="." align="char">5</td></tr><tr><td align="left" rowspan="2">5</td><td char="." align="char">22</td><td char="." align="char" rowspan="2">0.5068</td><td char="." align="char" rowspan="2">0.4978</td><td char="." align="char" rowspan="2">0.0091</td><td char="." align="char" rowspan="2">0.1554</td></tr><tr><td char="." align="char">5</td></tr><tr><td align="left" rowspan="2">6</td><td char="." align="char">27</td><td char="." align="char" rowspan="2">0.5144</td><td char="." align="char" rowspan="2">0.4864</td><td char="." align="char" rowspan="2">0.0272</td><td char="." align="char" rowspan="2">0.00002**</td></tr><tr><td char="." align="char">30</td></tr><tr><td align="left" rowspan="2">7</td><td char="." align="char">14</td><td char="." align="char" rowspan="2">0.5109</td><td char="." align="char" rowspan="2">0.4837</td><td char="." align="char" rowspan="2">0.0272</td><td char="." align="char" rowspan="2">0.00002**</td></tr><tr><td char="." align="char">5</td></tr><tr><td align="left" rowspan="2">8</td><td char="." align="char">28</td><td char="." align="char" rowspan="2">0.5121</td><td char="." align="char" rowspan="2">0.4860</td><td char="." align="char" rowspan="2">0.0260</td><td char="." align="char" rowspan="2">0.0039**</td></tr><tr><td char="." align="char">28</td></tr><tr><td align="left" rowspan="2">9</td><td char="." align="char">27</td><td char="." align="char" rowspan="2">0.5064</td><td char="." align="char" rowspan="2">0.4900</td><td char="." align="char" rowspan="2">0.0164</td><td char="." align="char" rowspan="2">0.0109**</td></tr><tr><td char="." align="char">1</td></tr><tr><td align="left" rowspan="2">10</td><td char="." align="char">26</td><td char="." align="char" rowspan="2">0.5059</td><td char="." align="char" rowspan="2">0.4846</td><td char="." align="char" rowspan="2">0.0219</td><td char="." align="char" rowspan="2">0.0011**</td></tr><tr><td char="." align="char">19</td></tr></tbody></table><table-wrap-foot><p>**<italic>p</italic> &lt; 0.05</p></table-wrap-foot></table-wrap></p>
    </sec>
  </sec>
  <sec id="Sec20">
    <title>Discussion</title>
    <p id="Par101">The superior performance of GAT2 model in classifying functional brain networks stems from two key aspects of the graph neural networks: graph attention learning layers for node representation, and attention learning in graph pooling. Graph attention layers are able to attend to neighborhoods' features, and enable specifying different weights for different nodes in a neighborhood. Compared with GCN layer based graph models, such as GCN-at (1st-order) and GCN-at (Cheby), GAT layer based graph models (GAT2 and GAT-average) yielded higher AUC score in the experiments. And the attention learning for graph pooling, which uses learnable parameters to summarize graph representation with a concise strategy, enhances the representation ability of graph. Compared with other pooling methods, such as in GAT-fc, and GAT-average models, the proposed graph attention pooling in GAT2 model achieves higher accuracy, sensitivity, specificity, and F1 score. To further demonstrate the utility and power of GAT2 model, we used more data to validate the GAT2 model in a larger graph dataset with 4000 samples, and the results showed that the performance of GAT2 model has been significantly better than the other comparison models.</p>
    <p id="Par102">For the construction of the brain network, we found that compared with the AAL atlas, GAT2 using HO atlas can capture the functional differences between the brain networks of ASD and HC in this dataset. It may be that numerical values of the underlying network metrics and the relation between nodal properties and region size were dependent on the atlas used [<xref ref-type="bibr" rid="CR27">27</xref>, <xref ref-type="bibr" rid="CR28">28</xref>], and compared with the AAL atlas, GAT2 using HO atlas can capture the functional differences between the brain networks of ASD individuals and HC in this dataset. Compared with sparse networks obtained by threshold, the dense network with weak connection information could enable the node to learn more information from neighboring nodes in GAT2 model.</p>
    <p id="Par103">For model interpretation, GNNExplainer performed better than Saliency Map and DeepLIFT when interpreting GAT2 model. We think that GNNExplainer is more powerful for interpreting the GAT2 model than Saliency and DeepLIFT. This is because the weights and attentions of features in the trained GAT2 model are similar, and the gradient values of features are similar, making it difficult to find the salient features by comparing gradient values with Saliency Map or DeepLIFT, while it is easier for GNNExplainer to learn the feature masks to obtain the salient features.</p>
    <p id="Par104">For interpreting results of the model from functional brain networks, as shown in the Table <xref rid="Tab6" ref-type="table">6</xref>, the top 10 connections (rsFCs) involved 12 ROIs (brain regions), and among these 10 rsFCs, 3 rsFCs (the connection 4, 5, and 7) were statistically significant (<italic>p</italic> &lt; 0.05) between the ASD and HC groups. The connection 1, 2, and 8 were associated with the Right Superior Parietal Lobule. In the ASD group, the Right Superior Parietal Lobule was strongly correlated with the Right Supramarginal Gyrus posterior division, and relatively weakly correlated with the Right Frontal Medial Cortex and the Left Amygdala. Such abormal rsFC connection patterns may result from increased or decreased key ROI/brain regions in information processing, as previous studies indicated. For example, decreased activation of the Right Superior Parietal Lobule has been observed in individuals with ASD during learning [<xref ref-type="bibr" rid="CR29">29</xref>]. Further, the connection 3 and 4 are associated with the Right Hippocampus. The connection of the Right Hippocampus with the Right Frontal Medial Cortex was stronger in the ASD group than in the HC group. It has been found that children with ASD show reduced working-memory-related activations in the right hippocampus [<xref ref-type="bibr" rid="CR30">30</xref>]. The connection 5, 6, 7, 9, and 10 are all associated with the Left Frontal Pole. The connections of the Left Frontal Pole with the Right Inferior Frontal Gyrus pars opercularis, and the Left Frontal Pole with the Left Precentral Gyrus, were weaker in the ASD group than in the HC group. Differences have been observed in Left Frontal Pole when studying the longitudinal changes of cortical thickness in autism and typical development [<xref ref-type="bibr" rid="CR31">31</xref>], along with greater activation of Left Frontal Pole in the ASD group during reward anticipation and outcomes for monetary and social rewards [<xref ref-type="bibr" rid="CR32">32</xref>]. Finally, it should be noted that among these 10 connections in Tables <xref rid="Tab6" ref-type="table">6</xref> and <xref rid="Tab7" ref-type="table">7</xref> connections are not statistically significant between the ASD and HC groups. That may be because the sample size of the groups was not large enough to reveal the statistical power [<xref ref-type="bibr" rid="CR33">33</xref>]. Nevertheless, they had contributed to the classification of ASD and HC in the GAT2 model found by the GNN explanation method.</p>
    <p id="Par105">The proposed GAT-LI method has the potential in assisting future diagnoses of brain neurological disorders such as ASD, in addition to understanding the neural bases of ASD, since the two-stage method could learn an accurate GNN model for graph data and interpret how specific decisions of these graph models are made by feature importance. Besides, GAT-LI could be generalized to the classification and interpretation tasks of graph data from other biomedical fields.</p>
    <p id="Par106">There are two limitations in the current work. First, the brain network dataset is limited to the ASD classification task. It would be important to see whether the proposed GAT-LI excels in classifying and interpreting other brain network data. Second, our brain network dataset is limited to 1035 participants, although we used the larger synthetic dataset to validate the utility of GAT2 model. Future studies should rely on large-scale real data of both typically developing individuals and individuals with neuropsychological disorders.</p>
  </sec>
  <sec id="Sec21">
    <title>Conclusions</title>
    <p id="Par107">This paper proposes a graph attention network based Learning and Interpreting method, namely GAT-LI, which uses a graph attention network model to learn to classify functional brain networks of ASD versus HC, and uses GNNExplainer to interpret the learned graph model. For the learning model, we proposed GAT2, which uses GAT layers to learn node representations and a novel attention pooling layer to obtain the functional brain network representation for classification. The results of our experiments showed that GAT2 model outperformed the other comparison models for classifying ASD from HC in the ABIDE database. We also compared the classification performance of our model in different brain networks, including the brain networks constructed with different brain atlases, and the sparsity of brain networks on different connection thresholds. We also further constructed a larger synthetic dataset to conduct more experiments to demonstrate the utility and power of GAT2 model. Finally, we used GNNExplainer to interpret the GAT2 model, and identified the significant features in classifying brain networks of ASD individuals from HC. Future work should focus on the accuracy and application of the GAT-LI method in analyzing other large-scale brain network data from both normal and disordered populations.
</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>AAL</term>
        <def>
          <p id="Par4">Automated Anatomical Labeling</p>
        </def>
      </def-item>
      <def-item>
        <term>ABIDE I</term>
        <def>
          <p id="Par5">Autism Brain Imaging Data Exchange I</p>
        </def>
      </def-item>
      <def-item>
        <term>ASD</term>
        <def>
          <p id="Par6">Autism spectrum disorder</p>
        </def>
      </def-item>
      <def-item>
        <term>AUC</term>
        <def>
          <p id="Par7">Area under the receiver operating characteristic curve</p>
        </def>
      </def-item>
      <def-item>
        <term>CNN</term>
        <def>
          <p id="Par8">Convolutional neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>CPAC</term>
        <def>
          <p id="Par9">Configurable Pipeline for the Analysis of Connectomes</p>
        </def>
      </def-item>
      <def-item>
        <term>CPP</term>
        <def>
          <p id="Par10">Change of prediction probability</p>
        </def>
      </def-item>
      <def-item>
        <term>DHP</term>
        <def>
          <p id="Par11">Dense hierarchical pooling</p>
        </def>
      </def-item>
      <def-item>
        <term>fMRI</term>
        <def>
          <p id="Par12">Functional Magnetic Resonance Imaging</p>
        </def>
      </def-item>
      <def-item>
        <term>FN</term>
        <def>
          <p id="Par13">False negative</p>
        </def>
      </def-item>
      <def-item>
        <term>FP</term>
        <def>
          <p id="Par14">False positive</p>
        </def>
      </def-item>
      <def-item>
        <term>GAT</term>
        <def>
          <p id="Par15">Graph attention network</p>
        </def>
      </def-item>
      <def-item>
        <term>GCN</term>
        <def>
          <p id="Par16">Graph convolutional networks</p>
        </def>
      </def-item>
      <def-item>
        <term>GNN</term>
        <def>
          <p id="Par17">Graph neural networks</p>
        </def>
      </def-item>
      <def-item>
        <term>HC</term>
        <def>
          <p id="Par18">Healthy controls</p>
        </def>
      </def-item>
      <def-item>
        <term>HO</term>
        <def>
          <p id="Par19">Harvard Oxford</p>
        </def>
      </def-item>
      <def-item>
        <term>MLP</term>
        <def>
          <p id="Par20">MultiLayer Perceptron</p>
        </def>
      </def-item>
      <def-item>
        <term>NLCI</term>
        <def>
          <p id="Par21">Number of label-changed instance</p>
        </def>
      </def-item>
      <def-item>
        <term>PCA</term>
        <def>
          <p id="Par22">Principal component analysis</p>
        </def>
      </def-item>
      <def-item>
        <term>RF</term>
        <def>
          <p id="Par23">Random forest</p>
        </def>
      </def-item>
      <def-item>
        <term>ROIs</term>
        <def>
          <p id="Par24">Regions of interests</p>
        </def>
      </def-item>
      <def-item>
        <term>rsFC</term>
        <def>
          <p id="Par25">Resting-state functional connectivity</p>
        </def>
      </def-item>
      <def-item>
        <term>SVM</term>
        <def>
          <p id="Par26">Support vector machine</p>
        </def>
      </def-item>
      <def-item>
        <term>TN</term>
        <def>
          <p id="Par27">True negative</p>
        </def>
      </def-item>
      <def-item>
        <term>TP</term>
        <def>
          <p id="Par28">True positive</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We are grateful to the anonymous reviewers for their valuable comments on the original manuscript, as well as all investigators who have worked to share their data through ABIDE.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors' contributions</title>
    <p>LC and TL collected the dataset, performed the experiments and drafted the manuscript. JH and SD analyzed the result. JH and PL wrote and modified the manuscript. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This research was supported in part by the Natural Science Foundation of Guangdong Province of China through the Grant 2018A030313309 and the Grant 2021A1515011942 (JH); the Fundamental Research Funds for the Central Universities, SCUT, through the grant 2019KZ20 (JH); the Innovation Fund of Introduced High-end Scientific Research Institutions of Zhongshan through the Grant 2019AG031 (SD); and the Guangdong Pearl River Talents Plan Innovative and Entrepreneurial Team through the Grant 2016ZT06S220 (PL).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The datasets analyzed during the current study are available in the ABIDE Preprocessed Connectomes Project website of <ext-link ext-link-type="uri" xlink:href="http://preprocessed-connectomes-project.org/abide/download.html">http://preprocessed-connectomes-project.org/abide/download.html</ext-link>. The source codes of GAT-LI are publicly available at the project website of <ext-link ext-link-type="uri" xlink:href="https://github.com/largeapp/gat-li">https://github.com/largeapp/gat-li</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1">
      <title>Ethics approval and consent to participate</title>
      <p id="Par108">No ethics approval was required for the study.</p>
    </notes>
    <notes id="FPar2">
      <title>Consent for publication</title>
      <p id="Par109">Not applicable.</p>
    </notes>
    <notes id="FPar3" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par110">None of the authors has any competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Khosla</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Jamison</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Ngo</surname>
            <given-names>GH</given-names>
          </name>
          <name>
            <surname>Kuceyeski</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sabuncu</surname>
            <given-names>MR</given-names>
          </name>
        </person-group>
        <article-title>Machine learning in resting-state fMRI analysis</article-title>
        <source>Magn Reson Imaging</source>
        <year>2019</year>
        <volume>64</volume>
        <fpage>101</fpage>
        <lpage>121</lpage>
        <pub-id pub-id-type="doi">10.1016/j.mri.2019.05.031</pub-id>
        <pub-id pub-id-type="pmid">31173849</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sólon</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Rosa</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Craddock</surname>
            <given-names>RC</given-names>
          </name>
          <name>
            <surname>Buchweitz</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Meneguzzi</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Identification of autism spectrum disorder using deep learning and the ABIDE dataset</article-title>
        <source>NeuroImage Clin</source>
        <year>2018</year>
        <volume>17</volume>
        <fpage>16</fpage>
        <lpage>23</lpage>
        <pub-id pub-id-type="doi">10.1016/j.nicl.2017.08.017</pub-id>
        <pub-id pub-id-type="pmid">29034163</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guo</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Dominick</surname>
            <given-names>KC</given-names>
          </name>
          <name>
            <surname>Minai</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Erickson</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>LJ</given-names>
          </name>
        </person-group>
        <article-title>Diagnosing autism spectrum disorder from brain resting-state functional connectivity patterns using a deep neural network with a novel feature selection method</article-title>
        <source>Front Neurosci</source>
        <year>2017</year>
        <volume>11</volume>
        <fpage>460</fpage>
        <pub-id pub-id-type="doi">10.3389/fnins.2017.00460</pub-id>
        <pub-id pub-id-type="pmid">28871217</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Eslami</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Mirjalili</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Fong</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Laird</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Saeed</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>ASD-DiagNet: a hybrid learning approach for detection of autism spectrum disorder using fMRI data</article-title>
        <source>Front Neuroinform</source>
        <year>2019</year>
        <volume>13</volume>
        <fpage>1</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.3389/fninf.2019.00070</pub-id>
        <pub-id pub-id-type="pmid">30792636</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Liao</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Dong</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Interpretable learning approaches in resting-state functional connectivity analysis: the case of autism spectrum disorder</article-title>
        <source>Comput Math Methods Med</source>
        <year>2020</year>
        <volume>2020</volume>
        <fpage>1394830</fpage>
        <pub-id pub-id-type="doi">10.1155/2020/1394830</pub-id>
        <pub-id pub-id-type="pmid">32508974</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Li X, Dvornek NC, Zhuang J, Ventola P, Duncan JS. Brain biomarker interpretation in ASD using deep learning and Fmri. In: International conference on medical image computing and computer-assisted intervention. Cham: Springer; 2018. p. 206–214.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Li X, Dvornek NC, Zhou Y, Zhuang J, Ventola P, Duncan JS. Efficient interpretation of deep learning models using graph structure and cooperative game theory: application to asd biomarker discovery. In: International conference on information processing in medical imaging. Cham: Springer; 2019. p. 718–730.</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ktena</surname>
            <given-names>SI</given-names>
          </name>
          <name>
            <surname>Parisot</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ferrante</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Rajchl</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Glocker</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Rueckert</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Metric learning with spectral graph convolutions on brain connectivity networks</article-title>
        <source>Neuroimage</source>
        <year>2018</year>
        <volume>169</volume>
        <fpage>431</fpage>
        <lpage>442</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.12.052</pub-id>
        <pub-id pub-id-type="pmid">29278772</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Ma G, Ahmed NK, Willke T, Sengupta D, Cole MW, Turk-Browne NB, Yu PS. Similarity learning with higher-order graph convolutions for brain network analysis. arXiv preprint <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/arXiv:1811.02662">arXiv:1811.02662</ext-link> (2018).</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Zhang X, Chou J, Wang F. Integrative analysis of patient health records and neuroimages via memory-based graph convolutional network. In: 2018 IEEE International conference on data mining (ICDM). IEEE; 2018. p. 767–776.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Arslan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ktena</surname>
            <given-names>SI</given-names>
          </name>
          <name>
            <surname>Glocker</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Rueckert</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Stoyanov</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Ferrante</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Dalca</surname>
            <given-names>AV</given-names>
          </name>
        </person-group>
        <article-title>Graph saliency maps through spectral convolutional networks: application to sex classification with brain connectivity</article-title>
        <source>Graphs in biomedical image analysis and integrating medical imaging and non-imaging modalities</source>
        <year>2018</year>
        <publisher-loc>Berlin</publisher-loc>
        <publisher-name>Springer</publisher-name>
        <fpage>3</fpage>
        <lpage>13</lpage>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Yang H, Li X, Wu Y, Li S, Lu S, Duncan JS, Gee JC, Gu S. Interpretable multimodality embedding of cerebral cortex using attention graph network for identifying bipolar disorder. In: International conference on medical image computing and computer-assisted intervention. Cham: Springer; 2019. p. 799–807.</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Zhou J, Cui G, Zhang Z, Yang C, Liu Z, Wang L, Li C, Sun M. Graph neural networks: a review of methods and applications. arXiv preprint <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/arXiv:1812.08434">arXiv:1812.08434</ext-link> (2018).</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Long</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Philip</surname>
            <given-names>SY</given-names>
          </name>
        </person-group>
        <article-title>A comprehensive survey on graph neural networks</article-title>
        <source>IEEE Trans Neural Netw Learn Syst</source>
        <year>2020</year>
        <volume>32</volume>
        <fpage>4</fpage>
        <lpage>24</lpage>
        <pub-id pub-id-type="doi">10.1109/TNNLS.2020.2978386</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Gopinath K, Desrosiers C, Lombaert H. Learnable pooling in graph convolution networks for brain surface analysis. IEEE Trans Pattern Anal Mach Intell (2020).</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Ying Z, You J, Morris C, Ren X, Hamilton W, Leskovec J. Hierarchical graph representation learning with differentiable pooling. In: Advances in neural information processing systems (NeurIPS 2018); 2018. p. 4800–4810.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Veličković P, Cucurull G, Casanova A, Romero A, Lio P, Bengio Y. Graph attention networks. arXiv preprint <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/arXiv:1710.10903">arXiv:1710.10903</ext-link> (2017).</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Lee J, Lee I, Kang J. Self-attention graph pooling. arXiv preprint <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/arXiv:1904.08082">arXiv:1904.08082</ext-link> (2019).</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Ying Z, Bourgeois D, You J, Zitnik M, Leskovec J. Gnnexplainer: generating explanations for graph neural networks. In: Advances in neural information processing systems (NeurIPS 2019); 2019. p. 9240–9251.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Di Martino</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>C-G</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Denio</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Castellanos</surname>
            <given-names>FX</given-names>
          </name>
          <name>
            <surname>Alaerts</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Anderson</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Assaf</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Bookheimer</surname>
            <given-names>SY</given-names>
          </name>
          <name>
            <surname>Dapretto</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism</article-title>
        <source>Mol Psychiatry</source>
        <year>2014</year>
        <volume>19</volume>
        <issue>6</issue>
        <fpage>659</fpage>
        <lpage>667</lpage>
        <pub-id pub-id-type="doi">10.1038/mp.2013.78</pub-id>
        <pub-id pub-id-type="pmid">23774715</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Simonyan K, Vedaldi A, Zisserman A. Deep inside convolutional networks: visualising image classification models and saliency maps. arXiv preprint <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/arXiv:1312.6034">arXiv:1312.6034</ext-link> (2013).</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Shrikumar A, Greenside P, Kundaje A. Learning important features through propagating activation differences. In: International conference on machine learning. PMLR; 2017. p. 3145–3153.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Desikan</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Ségonne</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Fischl</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Quinn</surname>
            <given-names>BT</given-names>
          </name>
          <name>
            <surname>Dickerson</surname>
            <given-names>BC</given-names>
          </name>
          <name>
            <surname>Blacker</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Buckner</surname>
            <given-names>RL</given-names>
          </name>
          <name>
            <surname>Dale</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Maguire</surname>
            <given-names>RP</given-names>
          </name>
          <name>
            <surname>Hyman</surname>
            <given-names>BT</given-names>
          </name>
        </person-group>
        <article-title>An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</article-title>
        <source>Neuroimage</source>
        <year>2006</year>
        <volume>31</volume>
        <issue>3</issue>
        <fpage>968</fpage>
        <lpage>980</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.021</pub-id>
        <pub-id pub-id-type="pmid">16530430</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Craddock</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Sikka</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Cheung</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Khanuja</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Ghosh</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Lurie</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Vogelstein</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Burns</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Towards automated analysis of connectomes: the configurable pipeline for the analysis of connectomes (C-PAC)</article-title>
        <source>Front Neuroinform</source>
        <year>2013</year>
        <volume>42</volume>
        <fpage>10-3389</fpage>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Kipf TN, Welling M. Semi-supervised classification with graph convolutional networks. arXiv preprint <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/arXiv:1609.02907">arXiv:1609.02907</ext-link> (2016).</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tzourio-Mazoyer</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Landeau</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Papathanassiou</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Crivello</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Etard</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Delcroix</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Mazoyer</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Joliot</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain</article-title>
        <source>Neuroimage</source>
        <year>2002</year>
        <volume>15</volume>
        <issue>1</issue>
        <fpage>273</fpage>
        <lpage>289</lpage>
        <pub-id pub-id-type="doi">10.1006/nimg.2001.0978</pub-id>
        <pub-id pub-id-type="pmid">11771995</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>de Reus</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Van den Heuvel</surname>
            <given-names>MP</given-names>
          </name>
        </person-group>
        <article-title>The parcellation-based connectome: limitations and extensions</article-title>
        <source>Neuroimage</source>
        <year>2013</year>
        <volume>80</volume>
        <fpage>397</fpage>
        <lpage>404</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.03.053</pub-id>
        <pub-id pub-id-type="pmid">23558097</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Zang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Gong</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Parcellation-dependent small-world brain functional networks: a resting-state fMRI study</article-title>
        <source>Hum Brain Mapp</source>
        <year>2009</year>
        <volume>30</volume>
        <issue>5</issue>
        <fpage>1511</fpage>
        <lpage>1523</lpage>
        <pub-id pub-id-type="doi">10.1002/hbm.20623</pub-id>
        <pub-id pub-id-type="pmid">18649353</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Travers</surname>
            <given-names>BG</given-names>
          </name>
          <name>
            <surname>Kana</surname>
            <given-names>RK</given-names>
          </name>
          <name>
            <surname>Klinger</surname>
            <given-names>LG</given-names>
          </name>
          <name>
            <surname>Klein</surname>
            <given-names>CL</given-names>
          </name>
          <name>
            <surname>Klinger</surname>
            <given-names>MR</given-names>
          </name>
        </person-group>
        <article-title>Motor learning in individuals with autism spectrum disorder: activation in superior parietal lobule related to learning and repetitive behaviors</article-title>
        <source>Autism Res</source>
        <year>2015</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>38</fpage>
        <lpage>51</lpage>
        <pub-id pub-id-type="doi">10.1002/aur.1403</pub-id>
        <pub-id pub-id-type="pmid">25258047</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Urbain</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Pang</surname>
            <given-names>EW</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>MJ</given-names>
          </name>
        </person-group>
        <article-title>Atypical spatiotemporal signatures of working memory brain processes in autism</article-title>
        <source>Transl Psychiatry</source>
        <year>2015</year>
        <volume>5</volume>
        <issue>8</issue>
        <fpage>e617</fpage>
        <lpage>e617</lpage>
        <pub-id pub-id-type="doi">10.1038/tp.2015.107</pub-id>
        <pub-id pub-id-type="pmid">26261885</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zielinski</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Prigge</surname>
            <given-names>MBD</given-names>
          </name>
          <name>
            <surname>Nielsen</surname>
            <given-names>JA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Longitudinal changes in cortical thickness in autism and typical development</article-title>
        <source>Brain</source>
        <year>2014</year>
        <volume>137</volume>
        <issue>6</issue>
        <fpage>1799</fpage>
        <lpage>1812</lpage>
        <pub-id pub-id-type="doi">10.1093/brain/awu083</pub-id>
        <pub-id pub-id-type="pmid">24755274</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dichter</surname>
            <given-names>GS</given-names>
          </name>
          <name>
            <surname>Richey</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Rittenberg</surname>
            <given-names>AM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Reward circuitry function in autism during face anticipation and outcomes</article-title>
        <source>J Autism Dev Disord</source>
        <year>2012</year>
        <volume>42</volume>
        <issue>2</issue>
        <fpage>147</fpage>
        <lpage>160</lpage>
        <pub-id pub-id-type="doi">10.1007/s10803-011-1221-1</pub-id>
        <pub-id pub-id-type="pmid">22187105</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Demšar</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Statistical comparisons of classifiers over multiple data sets</article-title>
        <source>J Mach Learn Res</source>
        <year>2006</year>
        <volume>7</volume>
        <fpage>1</fpage>
        <lpage>30</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
