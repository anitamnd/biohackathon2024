<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_PATTER100471 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEgr6 jpg ?>
<?FILEfx1 jpg ?>
<?FILEmmc1 pdf ?>
<?FILEmmc2 pdf ?>
<?FILEsi1 gif ?>
<?FILEsi2 gif ?>
<?FILEsi3 gif ?>
<?FILEsi4 gif ?>
<?FILEsi5 gif ?>
<?FILEsi6 gif ?>
<?FILEsi7 gif ?>
<?FILEsi8 gif ?>
<?FILEsi9 gif ?>
<?FILEsi10 gif ?>
<?FILEsi11 gif ?>
<?FILEsi12 gif ?>
<?FILEsi13 gif ?>
<?FILEsi14 gif ?>
<?FILEsi15 gif ?>
<?FILEsi16 gif ?>
<?FILEsi17 gif ?>
<?FILEsi18 gif ?>
<?FILEsi19 gif ?>
<?FILEsi20 gif ?>
<?FILEsi21 gif ?>
<?FILEsi22 gif ?>
<?FILEsi23 gif ?>
<?FILEsi24 gif ?>
<?FILEsi25 gif ?>
<?FILEsi26 gif ?>
<?FILEsi27 gif ?>
<?FILEsi28 gif ?>
<?FILEsi29 gif ?>
<?FILEsi30 gif ?>
<?FILEsi31 gif ?>
<?FILEsi32 gif ?>
<?FILEsi33 gif ?>
<?FILEsi34 gif ?>
<?FILEsi35 gif ?>
<?FILEsi36 gif ?>
<?FILEsi37 gif ?>
<?FILEsi38 gif ?>
<?FILEsi39 gif ?>
<?FILEsi40 gif ?>
<?FILEsi41 gif ?>
<?FILEsi42 gif ?>
<?FILEsi43 gif ?>
<?FILEsi44 gif ?>
<?FILEsi45 gif ?>
<?FILEsi46 gif ?>
<?FILEsi47 gif ?>
<?FILEsi48 gif ?>
<?FILEsi49 gif ?>
<?FILEsi50 gif ?>
<?FILEsi51 gif ?>
<?FILEsi52 gif ?>
<?FILEsi53 gif ?>
<?FILEsi54 gif ?>
<?FILEsi55 gif ?>
<?FILEsi56 gif ?>
<?FILEsi57 gif ?>
<?FILEsi58 gif ?>
<?FILEsi59 gif ?>
<?FILEsi60 gif ?>
<?FILEsi61 gif ?>
<?FILEsi62 gif ?>
<?FILEsi63 gif ?>
<?FILEsi64 gif ?>
<?FILEsi65 gif ?>
<?FILEsi66 gif ?>
<?FILEsi67 gif ?>
<?FILEsi68 gif ?>
<?FILEsi69 gif ?>
<?FILEsi70 gif ?>
<?FILEsi71 gif ?>
<?FILEsi72 gif ?>
<?FILEsi73 gif ?>
<?FILEsi74 gif ?>
<?FILEsi75 gif ?>
<?FILEsi76 gif ?>
<?FILEsi77 gif ?>
<?FILEsi78 gif ?>
<?FILEsi79 gif ?>
<?FILEsi80 gif ?>
<?FILEsi81 gif ?>
<?FILEsi82 gif ?>
<?FILEsi83 gif ?>
<?FILEsi84 gif ?>
<?FILEsi85 gif ?>
<?FILEsi86 gif ?>
<?FILEsi87 gif ?>
<?FILEsi88 gif ?>
<?FILEsi89 gif ?>
<?FILEsi90 gif ?>
<?FILEsi91 gif ?>
<?FILEsi92 gif ?>
<?FILEsi93 gif ?>
<?FILEsi94 gif ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Patterns (N Y)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Patterns (N Y)</journal-id>
    <journal-title-group>
      <journal-title>Patterns</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2666-3899</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9122960</article-id>
    <article-id pub-id-type="pii">S2666-3899(22)00051-4</article-id>
    <article-id pub-id-type="doi">10.1016/j.patter.2022.100471</article-id>
    <article-id pub-id-type="publisher-id">100471</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Relevance, redundancy, and complementarity trade-off (RRCT): A principled, generic, robust feature-selection tool</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au1">
        <name>
          <surname>Tsanas</surname>
          <given-names>Athanasios</given-names>
        </name>
        <email>tsanasthanasis@gmail.com</email>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="aff2" ref-type="aff">2</xref>
        <xref rid="aff3" ref-type="aff">3</xref>
        <xref rid="fn1" ref-type="fn">4</xref>
        <xref rid="cor1" ref-type="corresp">∗</xref>
      </contrib>
      <aff id="aff1"><label>1</label>Usher Institute, Edinburgh Medical School, University of Edinburgh, NINE Edinburgh BioQuarter, 9 Little France road, Edinburgh, UK</aff>
      <aff id="aff2"><label>2</label>School of Mathematics, University of Edinburgh, Edinburgh, UK</aff>
      <aff id="aff3"><label>3</label>Alan Turing Institute, British Library, London, UK</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>∗</label>Corresponding author <email>tsanasthanasis@gmail.com</email></corresp>
      <fn id="fn1">
        <label>4</label>
        <p id="ntpara0010">Lead contact</p>
      </fn>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>31</day>
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <day>13</day>
      <month>5</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>31</day>
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <volume>3</volume>
    <issue>5</issue>
    <elocation-id>100471</elocation-id>
    <history>
      <date date-type="received">
        <day>7</day>
        <month>12</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>19</day>
        <month>1</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>24</day>
        <month>2</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 The Author(s)</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="abs0010">
      <title>Summary</title>
      <p>We present a new heuristic feature-selection (FS) algorithm that integrates in a principled algorithmic framework the three key FS components: relevance, redundancy, and complementarity. Thus, we call it relevance, redundancy, and complementarity trade-off (RRCT). The association strength between each feature and the response and between feature pairs is quantified via an information theoretic transformation of rank correlation coefficients, and the feature complementarity is quantified using partial correlation coefficients. We empirically benchmark the performance of RRCT against 19 FS algorithms across four synthetic and eight real-world datasets in indicative challenging settings evaluating the following: (1) matching the true feature set and (2) out-of-sample performance in binary and multi-class classification problems when presenting selected features into a random forest. RRCT is very competitive in both tasks, and we tentatively make suggestions on the generalizability and application of the best-performing FS algorithms across settings where they may operate effectively.</p>
    </abstract>
    <abstract abstract-type="graphical" id="abs0015a">
      <title>Graphical abstract</title>
      <fig id="undfig1" position="anchor">
        <graphic xlink:href="fx1"/>
      </fig>
    </abstract>
    <abstract abstract-type="author-highlights" id="abs0015">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="ulist0010">
          <list-item id="u0010">
            <label>•</label>
            <p id="p0010">New feature-selection algorithm, RRCT, is robust and computationally efficient</p>
          </list-item>
          <list-item id="u0015">
            <label>•</label>
            <p id="p0015">RRCT is parameter-free and deployable off-the-shelf</p>
          </list-item>
          <list-item id="u0020">
            <label>•</label>
            <p id="p0020">Applicable in datasets with mixed-type variables, regression, and classification</p>
          </list-item>
          <list-item id="u0025">
            <label>•</label>
            <p id="p0025">Thorough empirical evaluation: 20 feature-selection algorithms, 12 diverse datasets</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <abstract abstract-type="editor-highlights" id="abs0020">
      <title>The bigger picture</title>
      <p>High-dimensional datasets are now increasingly met across a wide span of data-science applications, where the large number of variables may obstruct the extraction of useful patterns in the data and often prove detrimental to the subsequent supervised learning process (regression or classification). The problem is known as the curse of dimensionality and is often tackled using feature-selection algorithms. This study proposes an accurate, robust, computationally efficient feature-selection algorithm that is applicable in mixed-type variable settings across both regression and classification while inherently accounting for all key properties in determining a parsimonious feature subset: relevance, redundancy, and conditional relevance (or complementarity). Selecting a robust feature subset can save on data-collection resources, reduce computational cost and statistical-model portability, enhance interpretability, and often increase model generalization performance.</p>
    </abstract>
    <abstract abstract-type="teaser" id="abs0025">
      <p>Many practical datasets comprise a large number of variables, a setting that poses considerable challenges in terms of gaining insights into that particular domain. In this study, we introduce a robust algorithm that can identify a subset of the original variables, which should be retained for further analysis, and demonstrate its performance side by side against 19 state-of-art competing algorithms across 12 diverse datasets. Our findings have important implications toward reducing data-collection resources and interpretability as well as improving potential insights into the studied problem.</p>
    </abstract>
    <kwd-group id="kwrds0010">
      <title>Keywords</title>
      <kwd>curse of dimensionality</kwd>
      <kwd>dimensionality reduction</kwd>
      <kwd>feature selection</kwd>
      <kwd>statistical learning</kwd>
      <kwd>principle of parsimony</kwd>
      <kwd>variable selection</kwd>
      <kwd>information theory</kwd>
    </kwd-group>
  </article-meta>
  <notes>
    <p id="misc0010">Published: March 31, 2022</p>
  </notes>
</front>
<body>
  <sec id="sec1">
    <title>Introduction</title>
    <p id="p0040">There has been continuously growing research and commercial interest in collecting and processing data across diverse applications, ranging from healthcare to finance, military, and others. Typically, in most data-science applications, we want to infer the statistical and functional relationship between a set of features (characteristics of the dataset) and a (measured or assessed) quantity of interest known as the response (or outcome); this is commonly referred to as the supervised learning setup.<xref rid="bib1" ref-type="bibr"><sup>1</sup></xref> Increasingly, datasets are becoming more complex, often having an abundance of (recorded or extracted) features. The presence of a large number of features often obstructs the interpretation of useful patterns in the data and may be detrimental to the subsequent learning process of mapping features to the response.<xref rid="bib2" ref-type="bibr">2</xref>, <xref rid="bib3" ref-type="bibr">3</xref>, <xref rid="bib4" ref-type="bibr">4</xref> This problem, widely known as the curse of dimensionality,<xref rid="bib1" ref-type="bibr"><sup>1</sup></xref> occurs because the feature space is sparsely populated: the number of required data samples to adequately populate the feature space grows exponentially with the number of features. This is further exacerbated in applications where the number of features is considerably larger than the number of data samples (also known as fat datasets, e.g., in micro-array data analysis problems).<xref rid="bib1" ref-type="bibr"><sup>1</sup></xref></p>
    <p id="p0045">To mitigate the practical challenges arising because of the curse of dimensionality, researchers often employ feature-transformation or feature-selection (FS) methods. Feature transformation aims to build a new feature space of reduced dimensionality, producing a compact representation of the information that may be distributed across several of the original features. Although it has shown promising results in different applications,<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref><sup>,</sup><xref rid="bib5" ref-type="bibr"><sup>5</sup></xref><sup>,</sup><xref rid="bib6" ref-type="bibr"><sup>6</sup></xref> feature transformation is usually not easily interpretable because the physical meaning of the original features cannot be retrieved. In addition, it does not save on resources required during the data-collection process since all original features still need to be measured or computed. Moreover, in very high dimensional settings where the number of irrelevant features may exceed the number of relevant features, reliable feature transformation can be challenging.<xref rid="bib5" ref-type="bibr"><sup>5</sup></xref></p>
    <p id="p0050">FS algorithms abound in the literature, and there has been continued research interest in their development<xref rid="bib2" ref-type="bibr"><sup>2</sup></xref><sup>,</sup><xref rid="bib3" ref-type="bibr"><sup>3</sup></xref><sup>,</sup><xref rid="bib7" ref-type="bibr">7</xref>, <xref rid="bib8" ref-type="bibr">8</xref>, <xref rid="bib9" ref-type="bibr">9</xref>, <xref rid="bib10" ref-type="bibr">10</xref>, <xref rid="bib11" ref-type="bibr">11</xref>, <xref rid="bib12" ref-type="bibr">12</xref>, <xref rid="bib13" ref-type="bibr">13</xref>, <xref rid="bib14" ref-type="bibr">14</xref> and exploration to gain application-specific insights.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref><sup>,</sup><xref rid="bib15" ref-type="bibr">15</xref>, <xref rid="bib16" ref-type="bibr">16</xref>, <xref rid="bib17" ref-type="bibr">17</xref>, <xref rid="bib18" ref-type="bibr">18</xref> The FS algorithms reduce the original (high-dimensional) feature set onto a feature subset by discarding features aiming to (1) reduce computational time (depending on the application, also save on data-collection resources), (2) improve prediction performance in a standard supervised learning setup, and (3) provide new insights into the studied application (focusing on specific features of interest). FS algorithms can be broadly grouped into three categories: (1) filters, (2) embedded methods, and (3) wrappers. Wrappers and embedded methods incorporate a statistical learner (classifier or regressor), whereas filters are independent of the statistical learner. Wrappers incorporate a statistical learner and search the feature subset space using the performance of the statistical learner (formally assessed through the chosen loss function and using training and testing subsets). Embedded methods determine the feature subsets that best contribute to the performance of the statistical learning model while building the model itself (this is formalized through the loss function).<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref><sup>,</sup><xref rid="bib5" ref-type="bibr"><sup>5</sup></xref><sup>,</sup><xref rid="bib19" ref-type="bibr"><sup>19</sup></xref> Arguably, wrappers and embedded methods for FS have the following shortcomings compared with filters: (1) they often (but not always) have greater computational complexity, which is exacerbated as the dataset grows larger, (2) the selected feature subset for a specific statistical learner may be sub-optimal for a different statistical learner, a problem known as feature exportability, (3) controlling internal parameters (parameter fine-tuning) of the statistical learner requires experimentation and expertise and is time-consuming, and (4) there are inherent statistical-learner constraints; for example, some do not handle multi-class classification or regression problems. The problem with feature exportability arises because the features chosen in a wrapper or embedded algorithm are tailored to optimize the performance of the specific statistical learner. Therefore, the selected feature subset may not reflect the global properties of the dataset and might not generalize well in alternative statistical learners.<xref rid="bib20" ref-type="bibr"><sup>20</sup></xref> Filters attempt to overcome these limitations and commonly evaluate feature subsets based on their information content (for example, using statistical tests and statistical properties of the data) instead of optimizing the performance of specific statistical learners and are usually computationally more efficient. Henceforth, in this study, FS is used to refer exclusively to filters.</p>
    <p id="p0055">Historically, filter FS algorithms were developed to be computationally efficient using statistical properties of the data by applying statistical hypothesis tests and using correlation-based concepts to rank features and, progressively, have become considerably more sophisticated.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref><sup>,</sup><xref rid="bib21" ref-type="bibr"><sup>21</sup></xref> Some of the filter FS algorithms are computationally very demanding (for example, relying on high-dimensional density estimates, computationally intensive optimization, or computing mutual information [MI], which is computationally expensive and practically challenging with reduced data samples) and some require careful fine-tuning of internal parameters to optimize performance, while others are limited in their application because they can only address binary classification problems or cannot be generalized to regression settings. For an overview of these challenges, we refer to Guyon et al.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref> and Deng et al.<xref rid="bib21" ref-type="bibr"><sup>21</sup></xref> Crucially, some studies highlight the importance of using simple filters before experimenting with more sophisticated schemes, remarking that many promising but elementary concepts have been left unexplored.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref><sup>,</sup><xref rid="bib22" ref-type="bibr">22</xref>, <xref rid="bib23" ref-type="bibr">23</xref>, <xref rid="bib24" ref-type="bibr">24</xref></p>
    <p id="p0060">Motivated by the last statement, we pursued the development of a generic, computationally efficient FS algorithm that would be applicable across almost any data-science problem which is presented in the form of a data matrix and a response so that it could serve as an off-the-shelf FS algorithm. This led to the development of the new correlation-based filter FS algorithm that we propose here, which we call relevance, redundancy, and complementarity trade-off (RRCT). RRCT uses a simple nonlinear transformation of the correlation coefficients using information theoretic concepts to quantify the association of the features with the response and the overlapping information between features and also explicitly takes into account feature interactions as an integral component toward FS.</p>
    <p id="p0065">The aims of this study are to (1) introduce and empirically validate the new RRCT FS algorithm across a range of diverse datasets and (2) provide an empirical comparison of various widely used filter-based FS algorithms across datasets (including fat datasets) to benchmark performance.</p>
  </sec>
  <sec id="sec2">
    <title>Results</title>
    <p id="p0070">There are two approaches to assess the performance of FS algorithms. The first is by using a synthetic dataset where we know the ground truth (i.e., the features that are, by design, functionally associated with the response: these are known as the true features, and, correspondingly, the remaining features are known as false features or probes). The second approach is when we do not know the true features, e.g., in real-world datasets, and hence, we present the selected feature subsets into a statistical learner so that we infer FS performance on the basis of a chosen performance measure. For further details see <xref rid="sec4.5" ref-type="sec">FS assessment</xref> in the <xref rid="sec4" ref-type="sec">experimental procedures</xref>.</p>
    <sec id="sec2.1">
      <title>Validating FS algorithms using synthetic data</title>
      <p id="p0075">We begin the assessment of the 20 FS algorithms used in the study by reporting the false discovery rate (FDR) outputs for the four synthetic datasets (findings in <xref rid="fig1" ref-type="fig">Figure 1</xref>). These results should be interpreted sequentially: each step on the x axis denotes the iterative step in the FS algorithms, and the values in the y axis denote whether each FS algorithm’s choice identified true features in the subset or whether it selected a probe (false features, which are not members of the jointly minimal feature subset predicting the response). For example, a value of 1 in the y axis for the first iterative step for one of the FS algorithms would denote that the first feature that is selected for the given FS algorithm is a probe.<fig id="fig1"><label>Figure 1</label><caption><p>Comparison of the feature-selection algorithms in terms of true feature set recovery</p><p>The lower the false discovery rate (FDR), the better the feature-selection algorithm. The horizontal axis denotes the number of features selected during the incremental process for the number of true features in each dataset (the first synthetic dataset had 3 true features, the second had 8 true features, the third and fourth had 10 true features). Above each plot in parentheses, we present the size of the design matrix (in the form samples × features) followed by the number of classes (e.g., the first dataset contains 60 samples and 30 features in a 2-class classification problem).</p></caption><graphic xlink:href="gr1"/></fig></p>
      <p id="p0080">For convenience, <xref rid="tbl1" ref-type="table">Table 1</xref> summarizes the FDR for the number of true features in each of the datasets and can be studied along with <xref rid="fig1" ref-type="fig">Figure 1</xref>. The results in <xref rid="fig1" ref-type="fig">Figure 1</xref> and <xref rid="tbl1" ref-type="table">Table 1</xref> illustrate that GSO, IAMB, and RRCT are all very effective at discarding probes. On the contrary, we remark that popular algorithms such as RELIEF, mRMR Peng (which is the default implementation most studies use when referring to mRMR), correlation-based FS (CFS), and JMI may stumble and erroneously select probes.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>False discovery rate for the synthetic datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Synthetic 1 [60 × 30, C2]</th><th>Synthetic 2 [1,000 × 100, C2]</th><th>Synthetic 3 [1,000 × 500, C10]</th><th>Synthetic 4 [100 × 500, C8]</th></tr></thead><tbody><tr><td>GSO</td><td>0</td><td>0</td><td>0</td><td>0.10</td></tr><tr><td>mRMR Peng</td><td>0.67</td><td>0</td><td>0.30</td><td>0.90</td></tr><tr><td>mRMR Spearman</td><td>0.33</td><td>0</td><td>0.20</td><td>0.60</td></tr><tr><td>Information gain</td><td>0.67</td><td>0</td><td>0.50</td><td>1</td></tr><tr><td>RELIEF</td><td>0.33</td><td>0.38</td><td>0.30</td><td>0.70</td></tr><tr><td>CFS</td><td>0.67</td><td>0.13</td><td>0.50</td><td>1</td></tr><tr><td>CBF</td><td>0.67</td><td>0.13</td><td>0.50</td><td>1</td></tr><tr><td>SIMBA</td><td>0.33</td><td>0.50</td><td>1</td><td>0.80</td></tr><tr><td>LOGO</td><td>0.67</td><td>0.25</td><td>0.30</td><td>0.70</td></tr><tr><td>L1-LSMI</td><td>0.33</td><td>0.13</td><td>0.60</td><td>0.90</td></tr><tr><td>IAMB</td><td>0</td><td>0</td><td>0</td><td>0.10</td></tr><tr><td>HITON</td><td>0.67</td><td>0</td><td>0.90</td><td>1</td></tr><tr><td>JMI</td><td>0.33</td><td>0</td><td>0.30</td><td>0.90</td></tr><tr><td>DISR</td><td>0.33</td><td>0</td><td>0.30</td><td>0.90</td></tr><tr><td>QPFS</td><td>0.67</td><td>0</td><td>0.30</td><td>0.90</td></tr><tr><td>CMIM</td><td>0.66</td><td>1</td><td>0.40</td><td>0.90</td></tr><tr><td>CIFE</td><td>0.33</td><td>1</td><td>0.50</td><td>1</td></tr><tr><td>MIQ</td><td>0.67</td><td>0.13</td><td>0.40</td><td>0.80</td></tr><tr><td>SPECCMI</td><td>0.67</td><td>1</td><td>0.30</td><td>1</td></tr><tr><td>RRCT</td><td>0</td><td>0</td><td>0</td><td>0.10</td></tr></tbody></table><table-wrap-foot><fn><p>The design matrices are summarized in the form <italic>N×M</italic> [number of samples × number of features], and the following term indicates the problem and number of classes (e.g., C2 indicates that this is a classification problem with two classes). The presented results are the FDR scores for the number of true features in each of the datasets (see <xref rid="fig1" ref-type="fig">Figure 1</xref> and also the description of the synthetic datasets for details).</p></fn></table-wrap-foot></table-wrap></p>
      <p id="p0085">Specifically, the first synthetic dataset (using Breiman’s generator) has been challenging for many FS algorithms due to the limited number of samples (n = 60); methods that require the computation of feature densities or the MI require a large number of samples to be able to robustly compute these quantities, and hence, their internal criteria toward selecting the feature set are likely compromised. This would explain why, for example, methods such as mRMR Peng, MIQ, and others made a mistaken selection of feature on the second step (see <xref rid="fig1" ref-type="fig">Figure 1</xref>). Similarly, this simple synthetic dataset has been challenging for other advanced FS methods including QPFS and SPECCMI, which cannot obtain an overall global MI-based assessment of the problem.</p>
      <p id="p0090">The second synthetic dataset should similarly be relatively easy given we have binary features and a straightforward computation of the response using 8 features. However, we note that many of the FS algorithms, notably SIMBA, SPECCMI, CMIM, and CIFE, do not perform well.</p>
      <p id="p0095">The third and fourth synthetic datasets are more challenging in the way they were generated, with features spanning across different orders of magnitude and exhibiting more complicated relationships associating the features with the response. The third synthetic dataset, for example, may be challenging for some algorithms due to the relatively large number of classes. Despite having a large number of samples (e.g., as opposed to the first synthetic dataset) and given that the features had been generated using normal distributions, again, most FS algorithms started selecting probes from the fourth and fifth steps (see <xref rid="fig1" ref-type="fig">Figure 1</xref>). In particular, the fourth dataset was generated to assess how well FS algorithms can recover features in a fat dataset, which is a well-known and challenging problem, and we see that none of the studied FS algorithms could recover all true features. Nevertheless, GSO, IAMB, and RRCT were very promising and only missed the last true feature on the 10<sup>th</sup> step. All other FS algorithms investigated here appear to struggle to correctly discard probes.</p>
      <p id="p0100">Collectively, the experiments using synthetic datasets highlight some of the key weaknesses of FS algorithms across indicative types of data problems which could be seen in practical applications. We defer further elaboration on these findings for the <xref rid="sec3" ref-type="sec">Discussion</xref>.</p>
    </sec>
    <sec id="sec2.2">
      <title>Validating FS algorithms using real-world data</title>
      <p id="p0105"><xref rid="fig2" ref-type="fig">Figure 2</xref> presents the results for binary classification settings, and <xref rid="fig3" ref-type="fig">Figure 3</xref> presents the results for multi-class classification settings, as a function of the number of selected features presented into the random forest (RF). These results are summarized in <xref rid="tbl2" ref-type="table">Tables 2</xref> (binary classification problems) and <xref rid="tbl3" ref-type="table">3</xref> (multi-class classification problems), highlighting the number of features that led to the lowest misclassification error for each FS algorithm. The overall impression here is that there is no clear winner among the competing FS algorithms in terms of performance, although we remark that the algorithm proposed in this study, RRCT, works very well generally.<fig id="fig2"><label>Figure 2</label><caption><p>Comparison of the feature-selection algorithms based on RF out of sample performance for the binary-classification datasets</p><p>The horizontal axis denotes the number of features selected in the greedy feature-selection process. When the assessment was done using 10-fold cross validation, the results are presented in the form mean ± SD (where SD is in the form of error bars).</p></caption><graphic xlink:href="gr2"/></fig><fig id="fig3"><label>Figure 3</label><caption><p>Comparison of the feature-selection algorithms based on RF out of sample performance for the multi-class classification datasets</p><p>The horizontal axis denotes the number of features selected in the greedy feature-selection process. When the assessment was done using 10-fold cross validation, the results are presented in the form mean ± SD (where SD is in the form of error bars).</p></caption><graphic xlink:href="gr3"/></fig><table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>Out of sample RF, the percentage of misclassification for the binary-classification datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>SPECTF [80 × 44; 187 × 44]</th><th>Spambase [4,601 × 57]</th><th>Relathe [1,427 × 4,322]</th><th>Ovarian cancer [72 × 592]</th></tr></thead><tbody><tr><td>GSO</td><td>24.60 (23)</td><td>4.37 ± 0.87 (22)</td><td>14.25 ± 2.66 (28)</td><td>27.78 (3)</td></tr><tr><td>mRMR Peng</td><td>24.60 (20)</td><td>4.39 ± 0.57 (29)</td><td>16.48 ± 3.26 (28)</td><td>26.39 (15)</td></tr><tr><td>mRMR Spearman</td><td>23.53 (18)</td><td>4.78 ± 1.67 (27)</td><td>20.14 ± 3.56 (27)</td><td>20.83 (15)</td></tr><tr><td>Information gain</td><td>24.06 (16)</td><td>4.65 ± 0.59 (29)</td><td>20.63 ± 3.93 (23)</td><td>27.78 (4)</td></tr><tr><td>RELIEF</td><td>22.99 (27)</td><td>5.61 ± 1.00 (24)</td><td>29.30 ± 3.99 (27)</td><td>29.17 (7)</td></tr><tr><td>CFS</td><td>25.13 (12)</td><td>5.85 ± 0.53 (17)</td><td>17.18 ± 2.33 (30)</td><td>29.17 (3)</td></tr><tr><td>CBF</td><td>28.34 (2)</td><td>5.07 ± 0.48 (18)</td><td>16.06 ± 2.30 (27)</td><td>31.94 (3)</td></tr><tr><td>SIMBA</td><td>25.13 (25)</td><td>4.89 ± 0.83 (25)</td><td>32.25 ± 3.23 (26)</td><td>37.50 (20)</td></tr><tr><td>LOGO</td><td>25.67 (21)</td><td>4.87 ± 0.97 (28)</td><td>23.66 ± 4.24 (29)</td><td>27.78 (4)</td></tr><tr><td>L1-LSMI</td><td>22.46 (10)</td><td>4.33 ± 1.10 (17)</td><td>27.54 ± 2.24 (25)</td><td>26.39 (5)</td></tr><tr><td>IAMB</td><td>28.88 (6)</td><td>4.80 ± 0.89 (24)</td><td>14.73 ± 2.31 (30)</td><td>27.78 (3)</td></tr><tr><td>HITON</td><td>24.06 (22)</td><td>4.26 ± 0.66 (28)</td><td>28.24 ± 5.26 (23)</td><td>29.17 (3)</td></tr><tr><td>JMI</td><td>24.06 (30)</td><td>4.63 ± 0.58 (30)</td><td>20.70 ± 3.36 (16)</td><td>18.06 (14)</td></tr><tr><td>DISR</td><td>24.06 (22)</td><td>5.00 ± 0.83 (29)</td><td>19.86 ± 2.69 (21)</td><td>19.44 (4)</td></tr><tr><td>QPFS</td><td>24.60 (25)</td><td>4.37 ± 0.74 (28)</td><td>20.21 ± 1.97 (23)</td><td>23.61 (12)</td></tr><tr><td>CMIM</td><td>24.06 (15)</td><td>4.61 ± 0.88 (27)</td><td>16.06 ± 2.95 (30)</td><td>16.67 (24)</td></tr><tr><td>CIFE</td><td>23.53 (15)</td><td>5.89 ± 1.61 (30)</td><td>16.83 ± 3.26 (27)</td><td>29.17 (11)</td></tr><tr><td>MIQ</td><td>24.06 (16)</td><td>4.83 ± 0.84 (19)</td><td>18.52 ± 3.17 (28)</td><td>22.22 (15)</td></tr><tr><td>SPECCMI</td><td>24.06 (27)</td><td>4.61 ± 1.14 (28)</td><td>20.14 ± 4.67 (29)</td><td>19.44 (14)</td></tr><tr><td>RRCT</td><td>22.99 (18)</td><td>4.72 ± 0.56 (29)</td><td>13.87 ± 2.48 (27)</td><td>13.89 (20)</td></tr></tbody></table><table-wrap-foot><fn><p>The design matrices are summarized in the form <italic>N×M</italic> [number of samples × number of features]. When two design matrices are mentioned, the first was used for training (selecting features and training the statistical learner) and the second for testing performance. The presented results are the percentage of misclassification values, and the number in parentheses is the number of features that gave best-performance results searching for results with 1 … min(<italic>M</italic>,30) features (see <xref rid="fig2" ref-type="fig">Figure 2</xref> for details). When the assessment was done using 10-fold cross validation, the results are presented in the form mean ± SD.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="tbl3"><label>Table 3</label><caption><p>Out of sample RF, the percentage of misclassification for the multi-class classification datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Cardiotocography [2,129 × 21]</th><th>Handwriting [2,000 × 649]</th><th>Lymph [148 × 18]</th><th>SRBCT [63 × 2,308; 25 × 2,308]</th></tr></thead><tbody><tr><td>GSO</td><td>8.87 ± 1.62 (19)</td><td>2.00 ± 0.85 (30)</td><td>12.16 (14)</td><td>25.00 (10)</td></tr><tr><td>mRMR Peng</td><td>9.10 ± 1.75 (21)</td><td>1.05 ± 0.55 (29)</td><td>13.51 (15)</td><td>0.00 (10)</td></tr><tr><td>mRMR Spearman</td><td>9.06 ± 2.06 (15)</td><td>2.85 ± 1.06 (26)</td><td>13.51 (18)</td><td>0.00 (25)</td></tr><tr><td>Information gain</td><td>8.49 ± 2.32 (16)</td><td>1.50 ± 1.05 (26)</td><td>12.84 (17)</td><td>5.00 (18)</td></tr><tr><td>RELIEF</td><td>9.25 ± 1.54 (21)</td><td>5.85 ± 1.43 (29)</td><td>13.51 (16)</td><td>0.00 (19)</td></tr><tr><td>CFS</td><td>9.39 ± 2.69 (14)</td><td>2.90 ± 1.37 (30)</td><td>13.51 (10)</td><td>0.00 (9)</td></tr><tr><td>CBF</td><td>11.75 ± 2.50 (12)</td><td>1.45 ± 0.86 (22)</td><td>18.92 (4)</td><td>0.00 (27)</td></tr><tr><td>SIMBA</td><td>8.58 ± 1.87 (19)</td><td>2.60 ± 1.15 (30)</td><td>12.84 (12)</td><td>0.00 (6)</td></tr><tr><td>LOGO</td><td>9.25 ± 2.04 (14)</td><td>0.90 ± 0.81 (30)</td><td>12.16 (15)</td><td>5.00 (12)</td></tr><tr><td>L1-LSMI</td><td>9.53 ± 2.47 (18)</td><td>1.85 ± 1.03 (21)</td><td>12.16 (15)</td><td>5.00 (7)</td></tr><tr><td>IAMB</td><td>9.34 ± 2.65 (11)</td><td>2.15 ± 0.75 (27)</td><td>16.22 (4)</td><td>25.00 (10)</td></tr><tr><td>HITON</td><td>9.48 ± 1.71 (20)</td><td>3.90 ± 1.13 (27)</td><td>12.84 (18)</td><td>15.00 (4)</td></tr><tr><td>JMI</td><td>8.73 ± 1.38 (16)</td><td>1.35 ± 0.58 (30)</td><td>13.51 (14)</td><td>0.00 (24)</td></tr><tr><td>DISR</td><td>8.77 ± 1.64 (20)</td><td>1.80 ± 1.03 (24)</td><td>14.19 (17)</td><td>0.00 (23)</td></tr><tr><td>QPFS</td><td>9.25 ± 2.04 (13)</td><td>1.15 ± 0.91 (27)</td><td>13.51 (15)</td><td>5.00 (18)</td></tr><tr><td>CMIM</td><td>9.48 ± 2.10 (19)</td><td>1.15 ± 0.82 (30)</td><td>13.51 (15)</td><td>0.00 (27)</td></tr><tr><td>CIFE</td><td>9.72 ± 1.82 (21)</td><td>2.40 ± 0.81 (20)</td><td>13.51 (14)</td><td>15.00 (11)</td></tr><tr><td>MIQ</td><td>9.72 ± 1.49 (10)</td><td>1.90 ± 0.84 (30)</td><td>13.51 (15)</td><td>10.00 (30)</td></tr><tr><td>SPECCMI</td><td>8.87 ± 1.72 (17)</td><td>1.70 ± 1.09 (22)</td><td>12.84 (17)</td><td>0.00 (13)</td></tr><tr><td>RRCT</td><td>9.53 ± 1.37 (17)</td><td>2.30 ± 0.75 (28)</td><td>12.84 (15)</td><td>0.00 (18)</td></tr></tbody></table><table-wrap-foot><fn><p>The design matrices are summarized in the form <italic>N×M</italic> [number of samples × number of features]. When two design matrices are mentioned, the first was used for training (selecting features and training the statistical learner) and the second for testing performance. The presented results are the percentage of misclassification values, and the number in parentheses is the number of features that gave best-performance results searching for results with 1 … min(<italic>M</italic>,30) features (see <xref rid="fig3" ref-type="fig">Figure 3</xref> for details). When the assessment was done using 10-fold cross validation, the results are presented in the form mean ± SD.</p></fn></table-wrap-foot></table-wrap></p>
      <p id="p0110">Specifically, among the binary classification problems, RRCT is second best for the SPECTF dataset and best for Relathe and ovarian cancer, whereas for Spambase, things are less clear since there is some variability in the reported performance due to the randomness in the 10-fold cross-validation (CV) process (for example, the standard deviation is more than the difference between RRCT and the best-performing FS algorithm). For the ovarian-cancer dataset (which is a fat dataset) in particular, RRCT exhibits consistently better performance compared with all competing FS algorithms. This likely reflects that the interaction component that is computed with RRCT is crucial in this type of biological dataset.</p>
      <p id="p0115">For the multi-class classification datasets, findings were less clear, and again, there was no consistently dominating FS algorithm. Information gain was the best-performing algorithm for cardiotocography, LOGO was best performing for the handwriting dataset, and GSO, LOGO, and L1-LSMI were best performing for the lymph dataset. For the SRBCT dataset, many algorithms classified correctly all samples even though there were some differences in the number of features required to achieve that, and SIMBA should be considered the winner, having achieved that with the lowest number of features, thus reflecting a more parsimonious model. Overall, we remark that RRCT was very competitive for the multi-class classification problems except for the handwriting dataset, where LOGO was clearly better.</p>
    </sec>
    <sec id="sec2.3">
      <title>Computational complexity</title>
      <p id="p0120">This section briefly reviews the computational complexity of the 20 FS algorithms explored in this study. <xref rid="fig4" ref-type="fig">Figure 4</xref> provides the overall average timings to run each of the FS algorithms. Because of the way the functions for QPFS and SPECCMI were coded, the timings for these two algorithms in <xref rid="fig4" ref-type="fig">Figure 4</xref> reflect the required time to run QPFS and MIQ and SPECMI, CMIM, and CIFE, respectively. Overall, almost all algorithms scaled well with a large number of samples and features and provided outputs within a few seconds. L1-LSMI and IAMB were the computationally most-demanding approaches. RRCT is computationally efficient, exhibiting only slightly greater computational burden, e.g., to mRMR Spearman, and, on average, is less computationally intensive compared to the popular FS algorithm RELIEF. For further details, see <xref rid="sec4.5" ref-type="sec">FS assessment</xref> in the <xref rid="sec4" ref-type="sec">experimental procedures</xref>, and consult <xref rid="tbl4" ref-type="table">Table 4</xref> for the dataset used in the study.<fig id="fig4"><label>Figure 4</label><caption><p>Average timings for the feature-selection (FS) algorithms explored in this study across the 8 real-world datasets</p><p>The y axis is presented in a logarithmic scale for convenience. All experiments were run on a desktop Windows 10 machine with an Intel i9-9900K CPU at 3.6 GHz with 64 GB RAM using MATLAB 2021b.</p></caption><graphic xlink:href="gr4"/></fig><table-wrap position="float" id="tbl4"><label>Table 4</label><caption><p>Summary of the 12 datasets used in the study</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th>Data matrix</th><th>Associated task</th><th>Type</th></tr></thead><tbody><tr><td>Synthetic 1 (Breiman)</td><td>60 × 30</td><td>classification (2 classes)</td><td>C (30)</td></tr><tr><td>Synthetic 2</td><td>1,000 × 100</td><td>classification (2 classes)</td><td>D (100)</td></tr><tr><td>Synthetic 3 (Guyon)</td><td>1,000 × 500</td><td>classification (10 classes)</td><td>C (500)</td></tr><tr><td>Synthetic 4 (Guyon)</td><td>100 × 500</td><td>classification (8 classes)</td><td>C (500)</td></tr><tr><td>SPECTF<xref rid="tblfn1" ref-type="table-fn">a</xref></td><td>80 × 44; 187 × 44</td><td>classification (2 classes)</td><td>C (44)</td></tr><tr><td>Spambase<xref rid="tblfn1" ref-type="table-fn">a</xref></td><td>4,601 × 57</td><td>classification (2 classes)</td><td>C (57)</td></tr><tr><td>Relathe<xref rid="tblfn2" ref-type="table-fn">b</xref></td><td>1,427 × 4,322</td><td>classification (2 classes)</td><td>D (4,322)</td></tr><tr><td>Ovarian cancer<xref rid="tblfn3" ref-type="table-fn">c</xref></td><td>72 × 592</td><td>classification (2 classes)</td><td>C (592)</td></tr><tr><td>Cardiotocography<xref rid="tblfn1" ref-type="table-fn">a</xref></td><td>2,129 × 21</td><td>classification (10 classes)</td><td>C (14), D (7)</td></tr><tr><td>Handwriting<xref rid="tblfn1" ref-type="table-fn">a</xref></td><td>2,000 × 649</td><td>classification (10 classes)</td><td>C (649)</td></tr><tr><td>Lymph<xref rid="tblfn1" ref-type="table-fn">a</xref></td><td>148 × 18</td><td>classification (4 classes)</td><td>D (18)</td></tr><tr><td>SRBCT<xref rid="tblfn4" ref-type="table-fn">d</xref></td><td>63 × 2,308; 25 × 2,308</td><td>classification (4 classes)</td><td>C (2,308)</td></tr></tbody></table><table-wrap-foot><fn><p>The size of each data matrix is <inline-formula><mml:math id="M1" altimg="si83.gif"><mml:mrow><mml:mi>N</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M2" altimg="si4.gif"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> denotes the number of instances (samples) and <inline-formula><mml:math id="M3" altimg="si5.gif"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> denotes the number of features. The last column denotes the type of variables: continuous (C) or discrete (D). Where two design matrices are mentioned, the first is used for training (selecting features and training the statistical learner) and the second data matrix for testing performance.</p></fn></table-wrap-foot><table-wrap-foot><fn id="tblfn1"><label>a</label><p id="ntpara0015">Downloaded from the UCI Machine Learning Repository (<ext-link ext-link-type="uri" xlink:href="http://archive.ics.uci.edu/ml/datasets.html" id="intref0120">http://archive.ics.uci.edu/ml/datasets.html</ext-link>).</p></fn></table-wrap-foot><table-wrap-foot><fn id="tblfn2"><label>b</label><p id="ntpara0020">Downloaded from the ASU FS Repository (<ext-link ext-link-type="uri" xlink:href="https://jundongl.github.io/scikit-feature/datasets.html" id="intref0125">https://jundongl.github.io/scikit-feature/datasets.html</ext-link><ext-link ext-link-type="uri" xlink:href="http://archive.ics.uci.edu/ml/datasets.html" id="intref0130">http://archive.ics.uci.edu/ml/datasets.html</ext-link>.)</p></fn></table-wrap-foot><table-wrap-foot><fn id="tblfn3"><label>c</label><p id="ntpara0025">Downloaded from <ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/10/259/additional" id="intref0135">http://www.biomedcentral.com/1471-2105/10/259/additional</ext-link>.</p></fn></table-wrap-foot><table-wrap-foot><fn id="tblfn4"><label>d</label><p id="ntpara0030">Download from <ext-link ext-link-type="uri" xlink:href="http://www-stat.stanford.edu/%7Etibs/ElemStatLearn/" id="intref0140">http://www-stat.stanford.edu/∼tibs/ElemStatLearn/</ext-link>.</p></fn></table-wrap-foot></table-wrap></p>
    </sec>
  </sec>
  <sec id="sec3">
    <title>Discussion</title>
    <sec id="sec3.1">
      <title>Study overview and primary findings</title>
      <p id="p0125">We proposed a new filter FS algorithm, RRCT, which was directly inspired by previous theoretical exploration drawing on the three main components required for effective FS: relevance, redundancy, and complementarity (feature interactions). RRCT builds on an intuitively appealing conceptual formulation using partial correlation coefficients and an information-theoretic inspired transformation of rank correlations (see <xref rid="fig5" ref-type="fig">Figures 5</xref> and <xref rid="fig6" ref-type="fig">6</xref>). We investigated the potential of RRCT’s benchmarking performance against 19 filter FS algorithms across four synthetic datasets and eight real-world datasets (see <xref rid="tbl4" ref-type="table">Table 4</xref>). The datasets were chosen to be broadly representative of different practical challenges (e.g., very small number of samples, fat datasets, continuous and discrete features, multi-class classification). RRCT was shown to be very robust, in some datasets coming clearly on top and in other datasets performing very well and relatively close to the best-performing algorithms. We demonstrated that by generalizing point estimates of shared information content (quantified via correlation coefficients) and by accounting for multi-variable complementarity (using partial correlation coefficients), we developed a very effective, generic FS algorithm that improves on other classical filter schemes. RRCT is very fast and does not require fine-tuning of parameters, making it a useful tool that can be easily used off the shelf.<fig id="fig5"><label>Figure 5</label><caption><p>Information theoretic (IT) quantity (relevance or redundancy) as a function of the rank (Spearman) correlation coefficient <inline-formula><mml:math id="M4" altimg="si38.gif"><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:math></inline-formula>, computed as <inline-formula><mml:math id="M5" altimg="si81.gif"><mml:mrow><mml:mtext>I</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>ρ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>0.5</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">⋅</mml:mo><mml:mtext>log</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p><p>Asymptotically, as the absolute value of the correlation coefficient tends to <inline-formula><mml:math id="M6" altimg="si82.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">±</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, the IT quantity becomes infinite (in practice, we set this to a very large value: 1,000).</p></caption><graphic xlink:href="gr5"/></fig><fig id="fig6"><label>Figure 6</label><caption><p>Graphical representation of the effect of the partial correlation coefficient</p><p>The lowercase letters represent the shared information between the random variables.</p></caption><graphic xlink:href="gr6"/></fig></p>
    </sec>
    <sec id="sec3.2">
      <title>Results in context</title>
      <p id="p0130">A major strength of this study is the empirical comparison of 20 FS algorithms across 12 datasets, whereas most studies in the FS literature typically restrict comparisons to a limited number (often less than 10) or type of FS algorithms, e.g., only MI-based.<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref> We started our empirical exploration by focusing on synthetic datasets, where the ground truth of the contributing features toward estimating the response is known <italic>a priori</italic>. The four synthetically generated datasets investigated here were designed to study well-known practical challenges, including using datasets with a very small number of samples and a very small number of contributing features compared with the overall number (i.e., many probes in a dataset), and an increasingly emerging challenge with a small number of samples for a large number of features in a multi-class classification setting. We found that GSO, IAMB, and RRCT were consistently well suited to discarding probes across the four synthetic datasets, whereas more advanced FS algorithms generally performed poorly, particularly for the fourth synthetic dataset (the fat dataset). Overall, this finding is in agreement with previous suggestions encouraging the careful exploration of FS-algorithmic approaches.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref><sup>,</sup><xref rid="bib22" ref-type="bibr">22</xref>, <xref rid="bib23" ref-type="bibr">23</xref>, <xref rid="bib24" ref-type="bibr">24</xref></p>
      <p id="p0135">Similarly, the choice of the eight real-world datasets was driven by identifying types of challenges and using datasets, many of which had been previously used in the FS literature. Findings were less decisively clear compared with the synthetic datasets in terms of FS algorithm performance inferred by the RF outputs. We remark that there is no clearly superior FS algorithm for all datasets, which can be seen as one manifestation of the no free lunch theorem: no one method dominates all others over all possible datasets. This is something that previous studies in FS have similarly reported, indicatively.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref><sup>,</sup><xref rid="bib9" ref-type="bibr"><sup>9</sup></xref><sup>,</sup><xref rid="bib10" ref-type="bibr"><sup>10</sup></xref><sup>,</sup><xref rid="bib13" ref-type="bibr"><sup>13</sup></xref></p>
      <p id="p0140">Exploring the FS algorithms in more detail side by side, GSO came out on top for the lymph dataset; however, its performance was much worse in datasets with a smaller number of samples compared with the best-performing FS algorithms (particularly for the two fat datasets). We believe that GSO is an FS algorithm that should be more carefully considered in practical applications when having sufficiently large sample sizes, e.g., this verifies a further large-scale application where GSO performed well compared with other FS algorithms.<xref rid="bib25" ref-type="bibr"><sup>25</sup></xref> The mRMR Peng algorithm is probably one of the most popular FS algorithms in practice. Overall, we found that mRMR Peng was reasonably competitive, although, again, its performance degraded in two of the fat datasets (fourth synthetic dataset and ovarian-cancer dataset). On the other hand, it was among the FS algorithms that led to zero misclassifications on SRBCT (the second real-world fat dataset). It performed well on datasets with a large number of samples (Spambase, Relathe, cardiotocography, handwriting), which is unsurprising given that it relies on the computation of the MI. From a theoretical perspective, mRMR (as a concept) does not take complementarity into account, so it is likely that this is what caused its relatively lower performance, e.g., in the SPECTF and ovarian-cancer datasets. mRMR Spearman was reasonably competitive and even better performing that mRMR Peng for the datasets with small sample sizes, including for the fat datasets. Unsurprisingly, in datasets with a larger number of samples, mRMR Peng had a clear edge (especially the Relathe and handwriting datasets). Therefore, as expected, mRMR Spearman can be considered a computationally simpler approach that should probably only be preferred over mRMR Peng on small-sample-size studies; intuitively, this makes sense, because the computation of the correlation coefficient is more robust compared with the computation of MI. We empirically verified that MIQ, which is based on the same principle and basic formation as mRMR (the only difference being that the criterion operates on the ratio of relevance over average redundancy rather than their difference as in mRMR), has been justifiably overshadowed by mRMR since it performed worse by comparison. L1-LSMI has performed well overall, coming out on top for the SPECTF and lymph datasets. However, for the Relathe and ovarian-cancer datasets, it was still considerably worse than many of its competitors. We can tentatively infer that L1-LSMI is an algorithm that should be considered primarily in settings with a sufficiently large sample size (which is intuitively expected as it is an MI-based approach) and not in very sparse datasets where, again, the computation of the squared MI would be challenging. RELIEF is another popular FS algorithm; however, its performance overall was worse compared with some of the more successful FS algorithms, particularly where redundancy is a key property that needs to be accounted for (e.g., the handwriting dataset). Instead, the conceptually similar nearest hit (NH)- and nearest miss (NM)-based approaches, SIMBA and LOGO (the latter especially), performed better. LOGO was the top algorithm for handwriting and lymph and generally performed well in the real-world datasets, apart from the ovarian-cancer dataset. Therefore, LOGO appears to operate well in large datasets with many noisy or redundant continuous features. It is possible LOGO could be adjusted to perform better by changing the distance metric internally used as a hyper-parameter (e.g., for the Relathe dataset); however, we wanted to keep to the premise of working on vanilla-based implementations. The FS approaches aiming to determine Markov blankets (IAMB and HITON) had very variable performance: HITON was the top-performing algorithm for Spambase (largest dataset in terms of number of samples investigated here). However, it did not perform well for the other large datasets (Relathe and handwriting) which also comprise a very large number of features. Therefore, we tentatively suggest that HITON might be a good approach to determine the Markov blanket of the response in datasets with a relatively limited number of features. On the contrary, IAMB appeared to perform better in those settings with a large number of features compared with HITON.</p>
      <p id="p0145">The algorithmic approaches aiming to address FS globally (QPFS and SPECCMI), although theoretically well-founded, in practice did not demonstrate any superiority in terms of the simpler-based filter FS approaches in the different types of problems investigated here. By design, these FS algorithms aim to compute high-dimensional interactions among the features and the response, and hence, we can anticipate that they will likely require a large number of samples to operate well. Some of the practical challenges with QPFS have been previously discussed in Vinh et al.,<xref rid="bib26" ref-type="bibr"><sup>26</sup></xref> including not adapting well to datasets with a small number of samples because the MI cannot be efficiently computed. This is something that we verified in this study, e.g., with the first synthetic dataset where both methods selected many false features, whereas they both were more competitive against competing FS algorithms when presented with thousands of samples (e.g., Spambase and handwriting). Moreover, we have found that these global approaches are also likely not performing well in sparse datasets (see <xref rid="sec2" ref-type="sec">results</xref> for Relathe), which again can be attributed to practical challenges in accurately computing higher-order interactions aiming to jointly find a predictive feature subset. From a practical perspective, an additional challenge with QPFS and SPECCMI is that we need to discretize the continuous features in order to compute high-dimensional feature interactions computationally efficiently, which adds another level of complexity.</p>
      <p id="p0150">We remark that many of the advanced FS schemes examined here (including QPFS and SPECCMI) rely on MI, and its computation is very challenging in practice, particularly in relatively small- to middle-sized sample datasets for continuous features.<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref><sup>,</sup><xref rid="bib27" ref-type="bibr"><sup>27</sup></xref><sup>,</sup><xref rid="bib28" ref-type="bibr"><sup>28</sup></xref> This is why, in practice, most FS algorithms first discretize the continuous features and operate on the discretized features.<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref><sup>,</sup><xref rid="bib26" ref-type="bibr"><sup>26</sup></xref> There is no single rule of thumb on the number of discretized feature values or indeed the approach that should be used to discretize features (e.g., univariate or multivariate), and in practice, this was usually predefined to be a single number, e.g., in the context of FS, this was usually taken to be 5.<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref><sup>,</sup><xref rid="bib26" ref-type="bibr"><sup>26</sup></xref> This topic is of broader interest in ML<xref rid="bib29" ref-type="bibr"><sup>29</sup></xref> and is an area that needs to be explored further in combination with MI-based FS algorithms.</p>
      <p id="p0155">Previously, Brown et al.,<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref> in their extensive MI-based FS investigation, reported that JMI is their recommended algorithm, at least for relatively small data samples, which was generally in agreement with our findings here for MI-based approaches. We found that JMI can indeed be competitive in some datasets (e.g., we note its good performance in the handwriting and lymph datasets); however, overall, there were non-MI-based filters that were better performing.</p>
      <p id="p0160">RRCT was the clear winner in the Relathe and ovarian-cancer datasets and was tied with a few competing algorithms on the SRBCT dataset, while generally performing well across datasets. RRCT is well suited to datasets where feature complementarity is prominent, such as in micro-array datasets (ovarian cancer and SRBCT). It also outperformed competing algorithms in the sparse dataset (Relathe) because, intrinsically, this is well handled using correlation coefficients, and joint interactions with sparse data can be captured similarly well using the partial correlation coefficients. Since it is a correlation-based filter, its weakness is in datasets where the relationship between features and the response can only be captured by higher-order moments (e.g., the cardiotocography and handwriting datasets). Therefore, when there is a dataset with a fairly large sample size available and continuous features with likely non-linear underlying statistical relationships (with the response and within features), it is likely that an FS method such as LOGO would provide better results. We emphasize that RRCT has the desirable practical property that it is robust: it will generally be reasonably competitive even when not performing among the best FS algorithms for a particular dataset, whereas some of the competing FS algorithms are considerably more variable in their resulting performance across datasets. Indicatively, see HITON, which may be very good in some datasets (top algorithm in Spambase) and very bad performance in others (e.g., Relathe and ovarian cancer). Moreover, although not tested in this study, RRCT has the additional advantage that it can be readily deployed for both classification and regression applications, whereas some of the investigated FS algorithms cannot be readily generalized to such settings.</p>
      <p id="p0165">To account for inherent variability in FS when used in a CV setting and before presenting the selected feature subsets into the statistical learner, we used a robust voting strategy (see <xref rid="sec4.4.5" ref-type="sec">FS strategy</xref> and <xref rid="tbl5" ref-type="table">Table 5</xref>). As we argued previously,<xref rid="bib15" ref-type="bibr"><sup>15</sup></xref><sup>,</sup><xref rid="bib62" ref-type="bibr"><sup>30</sup></xref><sup>,</sup><xref rid="bib63" ref-type="bibr"><sup>31</sup></xref> this voting strategy approach ensures we can decide on the feature subset that can be used on the basis of perturbed versions of the original dataset and is applicable out-of-the-box with any greedy FS algorithm. We stress this is an important step to enable side-by-side comparisons of the selected feature subsets across FS algorithms in terms of FS consistency, which could be further explored.<table-wrap position="float" id="tbl5"><label>Table 5</label><caption><p>Proposed methodology for selecting features using the greedy feature-selection algorithms and a voting strategy</p></caption><table frame="hsides" rules="groups"><tbody><tr><td><bold>Input:</bold><inline-formula><mml:math id="M7" altimg="si2.gif"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mspace width="0.25em"/><mml:mi>ε</mml:mi><mml:mspace width="0.25em"/><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M8" altimg="si3.gif"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mspace width="0.25em"/><mml:mi>ε</mml:mi><mml:mspace width="0.25em"/><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M9" altimg="si4.gif"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> is the number of data samples and <inline-formula><mml:math id="M10" altimg="si5.gif"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> is the number of features. Depending on the FS algorithm, you may need additional hyper-parameters and/or pre-processing of the data (e.g., discretization).<break/><bold>Process</bold><list list-type="simple" id="olist0020"><list-item id="o0035"><label>1.</label><p id="p0595">For each FS algorithm, create an empty set <inline-formula><mml:math id="M11" altimg="si8.gif"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula>, which will contain the indices of the selected features.</p></list-item><list-item id="o0040"><label>2.</label><p id="p0600">Randomly select 90% of the data samples from the data matrix <inline-formula><mml:math id="M12" altimg="si11.gif"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:math></inline-formula> along with their responses, <inline-formula><mml:math id="M13" altimg="si60.gif"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item id="o0045"><label>3.</label><p id="p0605">Run the FS algorithm using the 90% randomly selected samples. The result is an ordered sequence of features (often you can choose the number of features <inline-formula><mml:math id="M14" altimg="si84.gif"><mml:mrow><mml:mi>m</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">≤</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> as the output to save on computational time), where the first feature is considered the most important for the chosen FS algorithm.</p></list-item><list-item id="o0050"><label>4.</label><p id="p0610">Repeat steps 2 and 3 multiple times, say <inline-formula><mml:math id="M15" altimg="si85.gif"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and store the results in a matrix <inline-formula><mml:math id="M16" altimg="si86.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (of size <inline-formula><mml:math id="M17" altimg="si87.gif"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mspace width="0.25em"/><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula>), In each of the <inline-formula><mml:math id="M18" altimg="si88.gif"><mml:mrow><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> rows of <inline-formula><mml:math id="M19" altimg="si86.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, we store the selected feature subset.</p></list-item><list-item id="o0055"><label>5.</label><p id="p0615">Voting to decide on the final feature subset for each FS algorithm: feature indices are incrementally included, one at a time, in <inline-formula><mml:math id="M20" altimg="si8.gif"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula>. For each of the <inline-formula><mml:math id="M21" altimg="si89.gif"><mml:mrow><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> steps, we find the indices corresponding to the features selected until that step for all the repetitions in step 4 (i.e., use the <inline-formula><mml:math id="M22" altimg="si90.gif"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mspace width="0.25em"/><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> subset of <inline-formula><mml:math id="M23" altimg="si86.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M24" altimg="si91.gif"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> corresponds to the features selected in the first <inline-formula><mml:math id="M25" altimg="si91.gif"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> FS steps [in the last step <inline-formula><mml:math id="M26" altimg="si92.gif"><mml:mrow><mml:mi>L</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula>]).</p></list-item><list-item id="o0060"><label>6.</label><p id="p0620">We select the feature index that appears most frequently among these <inline-formula><mml:math id="M27" altimg="si93.gif"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> elements and that is also not already included in <inline-formula><mml:math id="M28" altimg="si8.gif"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula>. This index is now included as the <inline-formula><mml:math id="M29" altimg="si91.gif"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula><sup>th</sup> element in <inline-formula><mml:math id="M30" altimg="si8.gif"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula>. Ties are resolved by including the lowest index number.</p></list-item><list-item id="o0065"><label>7.</label><p id="p0625">Repeat steps 5 and 6 for the number of features we want to ultimately use (i.e., <inline-formula><mml:math id="M31" altimg="si6.gif"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula>).</p></list-item></list><bold>Output:</bold><inline-formula><mml:math id="M32" altimg="si94.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">φ</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi></mml:msub><mml:mspace width="0.25em"/><mml:mi>ε</mml:mi><mml:mspace width="0.25em"/><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> vector with the ordered sequence of selected features in descending order of importance. The indices in this sequence correspond to the columns in the data matrix <inline-formula><mml:math id="M33" altimg="si11.gif"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:math></inline-formula>.</td></tr></tbody></table></table-wrap></p>
      <p id="p0665">In terms of computational demands (see <xref rid="fig4" ref-type="fig">Figure 4</xref>), we found that most FS algorithms generally provided feature ranking within a few seconds for almost all investigated datasets here. L1-LSMI was, in general, the computationally most-demanding algorithm, with IAMB surpassing it for the Relathe dataset, i.e. IAMB scales relatively poorly with a large number of features. RRCT scales very well, and for the Relathe dataset, it provided outputs within 14 s; indicatively, it was almost always faster compared with RELIEF running within a few seconds, so it can be considered a computationally very efficient FS approach.</p>
    </sec>
    <sec id="sec3.3">
      <title>Limitations and future work</title>
      <p id="p0170">The main limitation of RRCT is that it cannot quantify highly non-linear statistical relationships between variables. This is by design as a practical trade off to develop a computationally efficient, robust, parameter-free FS algorithm. RRCT implicitly considers that statistical relationships between pairs of variables can be adequately captured using monotonic relationships for the relevance and redundancy terms (quantified using the Spearman correlation coefficient), which could be considered a major simplification for practical real-world problems. Along these lines, the information theoretic (IT) formulation of RRCT presumes that the underlying distributions are quasi-Gaussian, which seems rather restrictive and rigid. Nevertheless, across machine-learning applications, many theoretically strong assumptions actually still lead to surprisingly good results in practice. One example is with principal-component analysis (PCA),<xref rid="bib1" ref-type="bibr"><sup>1</sup></xref> which often leads to generally good outcomes in feature transformation applications even though it makes quite restrictive assumptions in terms of the underlying data relationships compared with alternatives.<xref rid="bib30" ref-type="bibr"><sup>32</sup></xref> Conceptually, PCA only operates on the first two central-order moments (mean and variance), thus intrinsically not considering more complicated aspects of the statistical distributions for the variable relationships (linearity assumption). Similarly, the Naive Bayes classifier works surprisingly well given the rigid assumption of feature independence (which typically does not hold in practical applications).<xref rid="bib31" ref-type="bibr"><sup>33</sup></xref><sup>,</sup><xref rid="bib32" ref-type="bibr"><sup>34</sup></xref> Intuitively, prior transformation of the continuous features to shape into more quasi-normal distributions suggests itself as a potentially useful pre-processing step before feeding features into RRCT. Power transformations of the features, of which the Box-Cox transform<xref rid="bib33" ref-type="bibr"><sup>35</sup></xref> is one of the most popular, is therefore an appealing option to explore. In our experiments (data not shown), applying the Box-Cox transformation across continuous features, which were not normally distributed, did not seem to lead to consistent performance improvement across datasets, and we did not find a theoretical or empirical rule to decide on when this could be effectively employed. It is likely that this is not straightforward because the maximum likelihood approach used for the power transformation is challenging in the presence of outliers; this is an area that could be explored further, e.g., along the lines of Marazzi and Yohai.<xref rid="bib34" ref-type="bibr"><sup>36</sup></xref> Other density-normalization techniques might be more appropriate pre-processing steps prior to FS with RRCT (and possibly other FS algorithms), which could be explored in future work.</p>
      <p id="p0175">We attribute the overall success of RRCT across many of the practical problems despite its simplifying assumptions regarding the underlying variable distributions to the combination of three key components: (1) using the ranks within variables (implicitly used when computing rank correlation coefficients such as Spearman, thus overcoming certain problems with variable distributions, e.g., outliers skewing correlations), (2) the non-linear IT transformation of the correlation coefficients to form the relevance and redundancy terms, and (3) the complementarity term, which quantifies higher-order feature interactions toward estimating the response. Future work could explore building on the RRCT framework by integrating a different statistical-association measure instead of the Spearman correlation coefficient so that it can capture more general statistical relationships (although this would introduce additional computational complexity).</p>
      <p id="p0180">We attempted to provide plausible explanations as to why particular FS algorithms might be suited to specific datasets (or domains), which might give a good indication of their performance in similar settings, e.g., in terms of the number of classes, features, and samples (or possibly their ratio). This study has provided some useful tentative insights comparing a large range of filter FS algorithms across different indicative types of problems; however, we stress that these findings need to be evaluated more extensively. A very large empirical study using diverse datasets to identify the settings where particular FS algorithms excel and fail would be very useful in this regard.</p>
      <p id="p0185">It is useful to consider FS within the wider context of data processing and see how it fits with other methods that may be complementary in related tasks. Some FS algorithms, including some examined in this study, aim to provide feature weights toward determining the relative importance of features for a statistical learning task (e.g., RELIEF, SIMBA, LOGO). Although usually all samples are assigned equal weight, under certain circumstances, we may want to vary those weights to assign relative importance to different samples. Applying similar reasoning to FS, some research work has explored ways toward assigning different weights to samples rather than features. Inverse probability weighting (IPW) is one approach that can be used to achieve that, and we refer to Mansournia and Altman<xref rid="bib35" ref-type="bibr"><sup>37</sup></xref> for indicative applications. In principle, it would be possible to jointly explore IPW and FS, e.g., in unbalanced high-dimensional problems to assign weights both to samples and features toward specific statistical-learning tasks. Another research area that has attracted considerable research interest is interpretable machine learning. FS is a step toward facilitating interpretability by reducing the number of features presented into a statistical learner and hence enabling researchers to focus on interpreting the key characteristics and properties of the modeled system or the process characterized by the chosen lower-dimensional feature set. In addition to this, the research community has developed methods toward interpreting the outputs of statistical-learning models, and a convenient unified framework is provided using Shapley Additive Explanations (SHAP).<xref rid="bib36" ref-type="bibr"><sup>38</sup></xref> SHAP assigns each feature an importance score for a particular estimate for the statistical-learning model used (e.g., a trained RF), i.e., SHAP values explain how features impact the output of the model. Therefore, we can view FS as the first step in identifying the feature subset that is presented into a statistical learner and SHAP as the post-processing method to interpret the local feature importance toward the model output for a specific query sample. Future work could investigate further the synergies between FS and SHAP toward providing a streamlined approach to building parsimonious, robust, and interpretable models.</p>
    </sec>
    <sec id="sec3.4">
      <title>Concluding remarks</title>
      <p id="p0190">Collectively, this study’s findings indicate that RRCT is a very competitive algorithm across diverse settings, can be readily deployed when having datasets with mixed-type variables without any further pre-processing, and is directly applicable in both classification and regression applications (although in this study, we have only focused on classification applications because many of the FS algorithms explored herein only operate in classification). We envisage RRCT finding use as a robust, computationally efficient, off-the-shelf FS algorithm across any type of dataset, particularly in datasets with a small-medium number of samples and in fat datasets. Similarly, we provided some insights and tentative ideas regarding the investigated FS algorithms and the settings where they may (or not) be well suited for a range of typical types of data problems.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <title>Experimental procedures</title>
    <sec id="sec4.1">
      <title>Resource availability</title>
      <sec id="sec4.1.1">
        <title>Lead contact</title>
        <p id="p0195">The lead contact is Athanasios Tsanas (Thanasis; <ext-link ext-link-type="uri" xlink:href="mailto:atsanas@ed.ac.uk" id="intref0050">atsanas@ed.ac.uk</ext-link> and <ext-link ext-link-type="uri" xlink:href="mailto:tsanasthanasis@gmail.com" id="intref0055">tsanasthanasis@gmail.com</ext-link>).</p>
      </sec>
      <sec id="sec4.1.2">
        <title>Materials availability</title>
        <p id="p0200">This work generated no non-code materials.</p>
      </sec>
    </sec>
    <sec id="sec4.2">
      <title>Hardware and software</title>
      <p id="p0205">All experiments were run on a desktop Windows 10 machine with an Intel i9-9900K CPU at 3.6 GHz with 64 GB RAM. The source code has been tested on MATLAB 2021b.</p>
    </sec>
    <sec id="sec4.3">
      <title>Datasets</title>
      <p id="p0210"><xref rid="tbl4" ref-type="table">Table 4</xref> summarizes the datasets used in the study for easy reference. The key information herein is presented in terms of the (1) size of the data matrix in the form number of samples <italic>N</italic> × number of features <italic>M</italic>, (2) associated task (regression or classification), and (3) variable type (discrete or continuous variables, indicating where there is a data matrix with mixed-type variables). Overall, we used 4 synthetic datasets and 8 real-world datasets to evaluate the performance of the FS algorithms.</p>
      <sec id="sec4.3.1">
        <title>Synthetic data</title>
        <p id="p0215">The rationale for the choice of the synthetic datasets was to explore the performance of the 20 FS algorithms across indicative types of problems that may be considered broadly representative of what can be seen in practice (datasets with a small number of samples, datasets with binary features, fat datasets, multi-class classification datasets). Evaluating the performance of the FS algorithms in these synthetic datasets where we know <italic>a priori</italic> the true features provides direct insights on how well they can recover the minimal feature set that is jointly predictive of the response and can serve to highlight types of problems where specific FS algorithms would likely not perform well. For transparency, we used established synthetic data generators in three out of the four synthetic datasets.</p>
        <p id="p0220">The first synthetic dataset was generated using Breiman’s sample generator, which produces 60 samples and 30 features drawn from a multivariate normal distribution, where the resulting continuous response was binarized using a quantile-based transformation to obtain a balanced binary classification dataset. Only 3 features contribute to the response (true features), and the remaining 27 features are probes (false features). The challenge with this dataset is that there are very few samples, so methods that rely on large sample sizes for their internal selection criteria would likely not perform well. Therefore, the underlying motivation for starting our empirical exploration with this dataset was to evaluate how well the 20 FS algorithms operate on an indicative dataset with limited samples.</p>
        <p id="p0225">The second synthetic dataset was generated to assess how well FS algorithms cope with binary features. Specifically, we generated a dataset with 1,000 samples and 100 uniformly sampled binary features. The response was computed as <inline-formula><mml:math id="M34" altimg="si1.gif"><mml:mrow><mml:mi>y</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>5</mml:mn></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>11</mml:mn></mml:mrow><mml:mn>15</mml:mn></mml:msubsup><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>16</mml:mn></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">+</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>17</mml:mn></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">⋅</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>18</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, which was subsequently binarized using quantile-based transformation to obtain a balanced binary dataset. The underlying motivation to create the second synthetic dataset was to explore whether the setting with exclusively binary features (this type of dataset occurs in some fields of medicine and other practical problems) poses challenges for FS algorithms in recovering the true features.</p>
        <p id="p0230">The third and fourth synthetic datasets were generated using Guyon’s sample generator, which we have adapted to extend its use toward generating multi-class classification outputs. The generator requires users to specify the numbers of samples, of independent features (a subset of which is the useful features, i.e., they are used for the statistical-learning task), and of linearly dependent features upon the independent features. Guyon’s generator draws independent features normally distributed with additive white Gaussian noise (standard deviation = 0.1), and the features are shifted and re-scaled randomly to span three orders of magnitude. The response was computed by multiplying the useful features with a random weight vector, the components of which were drawn from a normal distribution, and quantizing the result using quantile-based transformation to obtain balanced datasets. The third synthetic dataset comprises 1,000 samples and 500 features, of which 10 are useful (predictive of the response) for a 10-class classification problem. The motivation for including the third dataset was to assess whether FS algorithms can perform well in a high-dimensional multi-class classification problem toward recovering the true features. Finally, the fourth synthetic dataset comprises only 100 samples and 500 features (of which 10 are useful) for an 8-class classification problem to test the performance of FS algorithms with an indicative fat dataset. Fat datasets are met increasingly frequently in different practical applications (e.g., with micro-array datasets), and therefore the motivation for generating the fourth synthetic dataset was to assess whether the FS algorithms can correctly recover the true features in such a challenging problem.</p>
        <p id="p0235">For convenience when working on the synthetic datasets, we use the terms true features to refer to the feature set that is jointly the minimal subset of features predicting the response and false features to refer to the remaining features.</p>
      </sec>
      <sec id="sec4.3.2">
        <title>Real-world data</title>
        <p id="p0240">Similar to the synthetic datasets, the rationale for the choice of the real-world datasets used in this study was to explore the performance of the 20 FS algorithms across indicative types of problems that may be considered broadly representative. By pursuing this empirical investigation, we gain insight into the types of problems where specific FS algorithms perform well (or not) and hence can tentatively draw conclusions on which FS algorithm(s) we may opt to use in related types of problems. This empirical exploration serves to help better understand challenges and limitations FS algorithms may have when presented with complicated real-world problems, beyond the carefully generated synthetic datasets introduced in the preceding section. Specifically, we wanted to explore indicative sparse datasets, fat datasets, and datasets with a varying number of samples and features. We used 8 indicative datasets that reflect a range of practical settings, including binary classification and multi-class classification problems, and datasets that range from a few samples and features to thousands of samples and features. Many of these datasets have been previously used in FS research literature.</p>
        <p id="p0245">The SPECTF dataset focuses on heart disease by processing cardiac single-proton emission computed tomography (SPECT) images, where each sample has been clinically assessed as normal or abnormal. The features have been extracted by processing the SPECT images by focusing on specific clinical regions of interest. This is a dataset that has been often used in FS research literature, and the rationale for including it in this empirical investigation is that we have a relatively limited number of samples in the training set that we use to select features, a setting that may be challenging for FS algorithms (akin to the first synthetic dataset).</p>
        <p id="p0250">The Spambase dataset contains 57 features to characterize a collection of emails coming from filed work and personal emails. The binary response denotes whether the e-mail was considered spam (1) or not (0), i.e., an unsolicited commercial email. The underlying motivation for including this dataset is that it represents the type of problem where we have a large number of samples and a relatively small number of features.</p>
        <p id="p0255">The Relathe dataset is a large sparse dataset with discrete and binary features characterizing text. The rationale for including this challenging dataset is both because of the very large number of features and because it represents a sparse problem (which is representative of some specific applications).</p>
        <p id="p0260">The ovarian-cancer dataset is one of the two real-world fat datasets used in this study. The features that have been extracted characterize metabolomic data produced from sera of ovarian-cancer patients and benign control participants. For details, we refer to Guan et al.<xref rid="bib37" ref-type="bibr"><sup>39</sup></xref> Similar to the fourth synthetic dataset, the rationale for including this dataset in the analysis is that it represents an emerging type of problem, which is increasingly seen with relatively few samples and a large number of features.</p>
        <p id="p0265">The cardiotocography dataset contains information from processing fetal cardiotocograms. They were assessed by three expert obstetricians, and a consensus classification label of the pattern class code constitutes the 10-class response. The motivation for including this dataset was to use it as a representative type of problem with a large number of samples in an unbalanced multi-class classification setting.</p>
        <p id="p0270">The handwriting dataset consists of 649 features extracted from a collection of Dutch utility maps to identify handwritten numerals (0–9). This is a nicely balanced dataset with 200 samples per class. The motivation for including this dataset is because we can use it to compare findings against the cardiotocography dataset given that we have a similar number of samples and classes, whereas handwriting contains a large number of features.</p>
        <p id="p0275">The lymphography dataset (abbreviated as “lymph” here to conform with other studies in the machine-learning literature) is derived from the field of oncology and comprises binary and multi-level discrete features aiming to estimate a four-class response: normal find, metastases, malign lymph, and fibrosis. The motivation for including this dataset was to explore how well FS algorithms might perform in a type of problem with relatively few samples and discrete features in a multi-class classification setting.</p>
        <p id="p0280">The SRBCT dataset is derived from a set of micro-array experiments where samples arose from small, round blue-cell tumors (SRBCTs) found in children, which have been classified into four major types: Burkitt lymphoma (BL), Ewing’s sarcoma (EWS), neuroblastoma (NB), and rhabdomyosarcoma (RMS). The dataset has been used as an indicative fat dataset in the standard statistical-learning book by Hastie et al.<xref rid="bib1" ref-type="bibr"><sup>1</sup></xref> The motivation for including SRBCT was similar to the use of the fourth synthetic dataset and the ovarian-cancer dataset: to explore the performance of FS algorithms in another fat dataset (compared with ovarian cancer, which represented a binary-class classification problem, this represents a multi-class classification problem).</p>
        <p id="p0285">Overall, these datasets provide good indicative examples of diverse types of data problems to assess the performance of FS algorithms.</p>
      </sec>
    </sec>
    <sec id="sec4.4">
      <title>FS</title>
      <p id="p0290">This section provides some background on the terminology and key concepts in FS and summarizes the FS algorithms used in the study, including the development of the new FS algorithm proposed herein (RRCT), along with the FS strategy.</p>
      <sec id="sec4.4.1">
        <title>Key terminology in FS and main concepts of filter approaches</title>
        <p id="p0295">Given the data matrix (or design matrix) <inline-formula><mml:math id="M35" altimg="si2.gif"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mspace width="0.25em"/><mml:mi>ε</mml:mi><mml:mspace width="0.25em"/><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and the response <inline-formula><mml:math id="M36" altimg="si3.gif"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mspace width="0.25em"/><mml:mi>ε</mml:mi><mml:mspace width="0.25em"/><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M37" altimg="si4.gif"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> is the number of samples (instances) and <inline-formula><mml:math id="M38" altimg="si5.gif"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> is the number of features, the FS algorithms aim to reduce the input feature space <inline-formula><mml:math id="M39" altimg="si5.gif"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> into <inline-formula><mml:math id="M40" altimg="si6.gif"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> features, where <inline-formula><mml:math id="M41" altimg="si7.gif"><mml:mrow><mml:mi>m</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">&lt;</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="M42" altimg="si6.gif"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> can be chosen based on prior knowledge and possible constraints of the application or can be determined via CV). That is, we want to select a feature set <inline-formula><mml:math id="M43" altimg="si8.gif"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula> comprising <inline-formula><mml:math id="M44" altimg="si6.gif"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> features <inline-formula><mml:math id="M45" altimg="si9.gif"><mml:mrow><mml:mspace width="0.25em"/><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mi>j</mml:mi><mml:mspace width="0.25em"/><mml:mi>ε</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where each <inline-formula><mml:math id="M46" altimg="si10.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a column vector in the data matrix <inline-formula><mml:math id="M47" altimg="si11.gif"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:math></inline-formula>. The optimal feature subset maximizes the joint information content of the selected features with respect to the response. However, this is a complex combinatorial problem, and the optimal solution can only be found by a brute-force search. Since a brute-force search is extremely computationally demanding, particularly for large datasets, sub-optimal alternatives are typically sought. Although in principle combinatorial optimization methods (e.g., genetic algorithms) can be applied to the FS problem, these techniques are also computationally expensive.</p>
        <p id="p0300">As an approximate solution to the combinatorial one, researchers often assess each feature individually in order to determine the overall information content of the feature subset from each individual feature in the subset. There are two FS approaches to incrementally decide on the selected feature subset, one step at a time: (1) sequential-forward process (features are sequentially added to the selected feature subset) and (2) backward elimination (starting from the entire feature set and eliminating one feature at each step). Forward FS is often used in many filter applications because it is computationally more efficient than backward elimination<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref><sup>,</sup><xref rid="bib7" ref-type="bibr"><sup>7</sup></xref><sup>,</sup><xref rid="bib23" ref-type="bibr"><sup>23</sup></xref><sup>,</sup><xref rid="bib38" ref-type="bibr"><sup>40</sup></xref> and is particularly suitable for those problems where we want to reduce a dataset comprising many features to a dataset with a fairly small number of features.</p>
        <p id="p0305">One of the simplest FS algorithms is to use only those features that are maximally related to the response, where the association strength of the features with the response can be quantified using a suitable criterion, <inline-formula><mml:math id="M48" altimg="si12.gif"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo linebreak="badbreak">⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (not necessarily a distance metric in the mathematical sense). One straightforward criterion is the Pearson correlation coefficient: this assumes that the association strength between the response and each of the features can be characterized using the mean and covariance (first two joint statistical moments) alone and that the higher-order moments are zero or at least sufficiently small enough that they can be neglected. Alternatively, the Spearman rank correlation coefficient, which is a more general criterion, can be used to quantify the relationship between each feature and the response. More advanced criteria can also be used to characterize potentially non-linear (and non-monotonic) relationships between the features and the response, such as MI. In fact, MI has attracted extensive and systematic interest in the FS literature.<xref rid="bib38" ref-type="bibr">40</xref>, <xref rid="bib39" ref-type="bibr">41</xref>, <xref rid="bib40" ref-type="bibr">42</xref> However, the computation of MI is computationally intensive, particularly in domains with continuous variables.<xref rid="bib5" ref-type="bibr"><sup>5</sup></xref> Conceptually, the simple approach discussed thus far, which relies solely on the association strength between individual features and the response variable, works well in the presence of independent (orthogonal) features. It is now well established that in most practical applications, a good feature subset needs to account for overlapping information shared among features useful in predicting the response.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref></p>
        <p id="p0310">There are three main concepts that researchers working in this field typically consider: relevance, redundancy, and feature interaction (also known as complementarity or conditional relevance). The first term, relevance<italic>,</italic> is defined as the univariate association strength of a feature with the response, which can be expressed using any approach that can express the statistical relationship between two variables (e.g., correlation coefficients, MI, etc.) and ideally needs to be maximized. The second term, redundancy, refers to the overlapping information shared among features in the feature subset toward predicting the response<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref><sup>,</sup><xref rid="bib4" ref-type="bibr"><sup>4</sup></xref><sup>,</sup><xref rid="bib23" ref-type="bibr"><sup>23</sup></xref> and ideally needs to be minimized. The third key concept, interaction, or complementarity, quantifies the extent to which two or more features are strongly associated with the response variable jointly (it is possible the same features may be only moderately associated with the response individually). This third concept was previously largely ignored by many FS algorithms; however, it has been explicitly considered in a number of recent studies, both from a theoretical perspective<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref> and also in practical FS implementations.<xref rid="bib26" ref-type="bibr"><sup>26</sup></xref><sup>,</sup><xref rid="bib41" ref-type="bibr"><sup>43</sup></xref><sup>,</sup><xref rid="bib42" ref-type="bibr"><sup>44</sup></xref> An intuitive way to motivate the use of feature interaction is that, in practical problems, it may be that the combination of two or more features is (highly) predictive of the response, whereas single features on their own may or may not be (highly) predictive. An extreme example that is well known comes from Boolean algebra with the use of the “exclusive OR” function (commonly referred to as “XOR”). Readers not familiar with this area can look into the truth table of the XOR function with two or more inputs in any standard textbook or website on Boolean algebra (for example, see the book by Whitesitt<xref rid="bib43" ref-type="bibr"><sup>45</sup></xref>). In the XOR example, each feature (in Boolean algebra terminology, features are typically referred to as inputs) is not predictive of the response (in Boolean algebra, the response is referred to as output). On the contrary, if we jointly consider together the (two or more) inputs in the XOR function, we can perfectly estimate the response. Similarly, it can be intuitively understood that in some applications, the joint consideration of two or more features may be more predictive of the response than their individual parts (for example, in biology one gene mutation might not be leading to a harmful phenotype; however, a gene mutation or deficiency that appears jointly across genes may be leading to an adverse outcome). From a computational perspective, it is more challenging to compute the feature-interaction component compared with relevance and redundancy because it requires an algorithmic expression that considers multiple variables at the same time. For example, if using an algorithmic approach that operates on densities, it is computationally much easier to compute marginal densities and conditional densities with two variables (e.g., a feature and the response) rather than joint densities and high-dimensional conditional densities for the features explored. We will see in the following section how FS algorithms, including the new FS algorithm proposed in this study, attempt to overcome these challenges by different computational means.</p>
        <p id="p0315">For further background on FS concepts, we refer to some standard articles in the topic.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref><sup>,</sup><xref rid="bib10" ref-type="bibr"><sup>10</sup></xref><sup>,</sup><xref rid="bib44" ref-type="bibr"><sup>46</sup></xref></p>
      </sec>
      <sec id="sec4.4.2">
        <title>Known FS algorithms for performance benchmarking</title>
        <p id="p0320">This section briefly summarizes 19 filter FS algorithms, many of which have been widely used in practical applications to determine feature subsets. The aim here was to evaluate how well these FS algorithms perform across indicative synthetic and real-world data problems, against which we can benchmark performance of RRCT, the new FS algorithm that will be introduced in the following section. Due to space constraints, we keep this section brief, summarizing the FS algorithms and their key properties and refer interested readers to the cited research literature for further details. Many of the MI-based FS algorithms have been brought under a unifying framework in Brown et al.;<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref> we also refer to a technical report that summarizes some of the FS methods used herein as part of the ASU FS repository.<xref rid="bib45" ref-type="bibr"><sup>47</sup></xref></p>
        <p id="p0325">The Gram-Schmidt orthogonalization (GSO) algorithm incrementally selects features at each step on the basis of being maximally correlated with the response and minimally correlated with the existing feature subset. The GSO algorithm projects the candidate features for selection at each step onto the null space of those features already selected in previous steps: the feature that is maximally correlated with the target in that projection is selected next. The procedure iterates until the number of desired features has been selected. Further details of the GSO algorithm used for FS can be found in Stoppiglia et al.<xref rid="bib46" ref-type="bibr"><sup>48</sup></xref> and Guyon et al.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref></p>
        <p id="p0330">Information gain is another generic concept (like GSO) that has been adopted toward FS.<xref rid="bib45" ref-type="bibr"><sup>47</sup></xref> It aims to identify the feature that maximizes the information gained by including it in the feature set compared with the joint information by the already selected features.</p>
        <p id="p0335">The correlation-based filter (CBF)<xref rid="bib47" ref-type="bibr"><sup>49</sup></xref> extends the concept of information gain using the symmetrical uncertainty (normalizing information gain by the sum of entropies) to mitigate bias favoring multi-level features. The CFS is another heuristic FS approach that also relies on symmetrical uncertainty to evaluate the amount of additional information adding a candidate feature brings to the already selected feature subset.</p>
        <p id="p0340">RELIEF was proposed by Kira and Rendell<xref rid="bib8" ref-type="bibr"><sup>8</sup></xref> as a heuristic FS algorithm for binary classification applications, extended to multi-class classification applications by Kononenko,<xref rid="bib48" ref-type="bibr"><sup>50</sup></xref> and later investigated more thoroughly by Robnik-Sikonja and Kononenko.<xref rid="bib49" ref-type="bibr"><sup>51</sup></xref> RELIEF is a feature-weighting algorithm, where each feature is assigned a weight depending on how “useful” it is in the context of predicting the response. Conceptually, features that do not contribute toward predicting the response will be associated with very small weights. The principle of RELIEF is similar to the k-nearest neighbor classifier, making use of the concept of NH and NM. RELIEF is related to hypothesis margin maximization, a concept that is also central in other machine-learning algorithms such as support-vector machines.<xref rid="bib50" ref-type="bibr"><sup>52</sup></xref> RELIEF intrinsically takes feature interactions into account toward their contribution to the separation of samples into differing classes; however, it does not explicitly integrate a mechanism to address redundancy.<disp-formula id="ufd1"><mml:math id="M49" altimg="si13.gif"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>w</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mover><mml:mo>=</mml:mo><mml:mtext>def</mml:mtext></mml:mover><mml:mfrac><mml:mn>1</mml:mn><mml:mi>q</mml:mi></mml:mfrac><mml:mspace width="0.25em"/><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>q</mml:mi></mml:munderover><mml:mrow><mml:mo>{</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mtext>NH</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:mo linebreak="badbreak">⋅</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mspace width="0.5em"/><mml:mo linebreak="badbreak">∈</mml:mo><mml:mspace width="0.25em"/><mml:mtext>NH</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mrow><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.5em"/><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.5em"/><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.5em"/><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:munder></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:munder><mml:munder><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo linebreak="badbreak">≠</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mtext>NM</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:mo linebreak="badbreak">⋅</mml:mo><mml:mfrac><mml:mrow><mml:mtext>P</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mtext>P</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mrow><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mspace width="0.5em"/><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.5em"/><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mspace width="0.5em"/><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.5em"/><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:munder><mml:mo linebreak="badbreak">⋅</mml:mo><mml:munder><mml:munder><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mspace width="0.5em"/><mml:mo linebreak="badbreak">∈</mml:mo><mml:mspace width="0.5em"/><mml:mtext>NM</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mrow><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.5em"/><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.5em"/><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.5em"/><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M50" altimg="si14.gif"><mml:mrow><mml:mi>w</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> refers to the weight associated with the <italic>j</italic><sup>th</sup> feature, <inline-formula><mml:math id="M51" altimg="si15.gif"><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:math></inline-formula> represents the number of instances randomly sampled from the data, <inline-formula><mml:math id="M52" altimg="si16.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> refers to a data sample (row in the data matrix <inline-formula><mml:math id="M53" altimg="si11.gif"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:math></inline-formula>), <inline-formula><mml:math id="M54" altimg="si17.gif"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo linebreak="badbreak">⋅</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> refers to the size of NHs or NMs, <inline-formula><mml:math id="M55" altimg="si18.gif"><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mo linebreak="badbreak">⋅</mml:mo><mml:mo>‖</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a distance metric (the Euclidean or Manhattan distance are typically used). The size of NHs <inline-formula><mml:math id="M56" altimg="si19.gif"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mtext>NH</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the size of NMs <inline-formula><mml:math id="M57" altimg="si20.gif"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mtext>NM</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are fixed to some pre-specified value, e.g., 10.</p>
        <p id="p0345">SIMBA is an extension of RELIEF and was developed by Gilad-Bachrach et al.<xref rid="bib50" ref-type="bibr"><sup>52</sup></xref> RELIEF does not iteratively re-evaluate the distances in the computation of the weight vector, i.e., the nearest neighbors of a sample are pre-defined in the original feature space. SIMBA integrates the iteratively computed feature weights in its computation of NHs and NMs, thus being more adaptive in re-evaluating sample distances and thus accounting for local information. However, this means that the convex-optimization problem that is being solved with RELIEF becomes a constrained non-linear optimization problem in SIMBA, which poses practical challenges and risks in identifying local minima.</p>
        <p id="p0350">LOGO<xref rid="bib7" ref-type="bibr"><sup>7</sup></xref> aims to decompose the intractable, exhaustive combinatorial problem of FS into a set of locally linear problems through local learning and can be thought of as an extension of RELIEF and SIMBA, with the key concept being to identify NHs and NMs. The local linearization of the global problem of selecting the most appropriate features for predicting the response stems from the use of a margin function, which focuses on the neighborhood of the investigated data samples. The probabilities of hit or miss are obtained from probability density functions, which are computed using kernel density estimation.<xref rid="bib1" ref-type="bibr"><sup>1</sup></xref> LOGO also uses a regularization parameter with the L1-norm to induce sparsity in the resulting weights.</p>
        <p id="p0355">The iterative associative Markov blanket (IAMB) algorithm<xref rid="bib51" ref-type="bibr"><sup>53</sup></xref> is a heuristic approach aiming to identify the Markov blanket (MB) of the response (defined as the set of features conditioned on which all other features are probabilistically independent of the response). IAMB is a sequential-search algorithm that considers features one by one for addition or removal, which more recently was shown to be a greedy iterative maximization of the conditional likelihood.<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref> The underlying assumption in the IAMB algorithmic family is that the data is faithful to some unknown Bayesian network. HITON is a related FS algorithm developed by the same research team that also aims to identify the MB without requiring a large sample size.<xref rid="bib52" ref-type="bibr"><sup>54</sup></xref></p>
        <p id="p0360">The minimum redundancy maximum relevance (mRMR) explicitly takes into account relevance and redundancy using an empirical form where the relevance is computed using the MI between each candidate feature and the response and the relevance is computed using the mean of the pairwise MI between features.<xref rid="bib38" ref-type="bibr"><sup>40</sup></xref> Specifically, it takes the form:<disp-formula id="ufd2"><mml:math id="M58" altimg="si21.gif"><mml:mrow><mml:mtext>mRMR</mml:mtext><mml:mover><mml:mo>=</mml:mo><mml:mtext>def</mml:mtext></mml:mover><mml:munder><mml:mtext>max</mml:mtext><mml:mrow><mml:mi>j</mml:mi><mml:mspace width="0.5em"/><mml:mo linebreak="badbreak">∈</mml:mo><mml:mspace width="0.5em"/><mml:mi>Q</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:munder><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:munder><mml:mo linebreak="badbreak">−</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mspace width="0.5em"/><mml:mo linebreak="badbreak">∈</mml:mo><mml:mspace width="0.5em"/><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M59" altimg="si10.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the <italic>j</italic><sup>th</sup> variable in the initial <inline-formula><mml:math id="M60" altimg="si5.gif"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>-dimensional feature space, <inline-formula><mml:math id="M61" altimg="si22.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a variable that has been already selected in the feature index subset <inline-formula><mml:math id="M62" altimg="si8.gif"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M63" altimg="si23.gif"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> is an integer, <inline-formula><mml:math id="M64" altimg="si24.gif"><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:math></inline-formula> contains the indices of all the features in the initial feature space, that is, 1 … <inline-formula><mml:math id="M65" altimg="si25.gif"><mml:mrow><mml:mspace width="0.25em"/><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M66" altimg="si8.gif"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula> contains the indices of selected features, <inline-formula><mml:math id="M67" altimg="si26.gif"><mml:mrow><mml:mi>Q</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula> denotes the indices of the features not in the selected subset, and <inline-formula><mml:math id="M68" altimg="si27.gif"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the cardinality of the selected subset. <inline-formula><mml:math id="M69" altimg="si12.gif"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo linebreak="badbreak">⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the criterion used, which, for the original mRMR, is the MI. In this study, we used the original mRMR developer’s implementation for the mRMR, which discretizes features using adaptive histograms, and refer to this FS algorithm as <italic>mRMR Peng</italic>. Using the mRMR formula, we can apply a different criterion for determining features, and a computationally very-efficient approach is to use the Spearman correlation coefficient instead.<xref rid="bib53" ref-type="bibr"><sup>55</sup></xref> Hence, we refer to that algorithm as <italic>mRMR Spearman</italic>. A modification of mRMR is using the quotient (ratio) instead of the difference between relevance and redundancy, which gives rise to the MIQ FS algorithm.<xref rid="bib54" ref-type="bibr"><sup>56</sup></xref></p>
        <p id="p0365">An alternative approach using MI is the joint MI (JMI),<xref rid="bib55" ref-type="bibr"><sup>57</sup></xref> which focuses on the complementary information between features toward estimating the response, which for a candidate feature <inline-formula><mml:math id="M70" altimg="si10.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is<disp-formula id="ufd3"><mml:math id="M71" altimg="si28.gif"><mml:mrow><mml:mtext>JMI</mml:mtext><mml:mover><mml:mo>=</mml:mo><mml:mtext>def</mml:mtext></mml:mover><mml:munder><mml:mtext>max</mml:mtext><mml:mrow><mml:mi>j</mml:mi><mml:mspace width="0.5em"/><mml:mo linebreak="badbreak">∈</mml:mo><mml:mspace width="0.25em"/><mml:mi>Q</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mspace width="0.5em"/><mml:mo linebreak="badbreak">∈</mml:mo><mml:mspace width="0.5em"/><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0370">The underlying concept in JMI is to include candidate features that are complementary with already existing features in the feature subset <inline-formula><mml:math id="M72" altimg="si8.gif"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula> since, intrinsically, it computes the pairwise information of features taken jointly with the response.</p>
        <p id="p0375">The double input symmetrical relevance (DISR)<xref rid="bib56" ref-type="bibr"><sup>58</sup></xref> is a modification of the JMI criterion by normalizing MI using the joint entropy:<disp-formula id="ufd4"><mml:math id="M73" altimg="si29.gif"><mml:mrow><mml:mtext>DISR</mml:mtext><mml:mover><mml:mo>=</mml:mo><mml:mtext>def</mml:mtext></mml:mover><mml:munder><mml:mtext>max</mml:mtext><mml:mrow><mml:mi>j</mml:mi><mml:mspace width="0.5em"/><mml:mo linebreak="badbreak">∈</mml:mo><mml:mspace width="0.5em"/><mml:mi>Q</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mspace width="0.5em"/><mml:mo linebreak="badbreak">∈</mml:mo><mml:mspace width="0.5em"/><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0380">The conditional MI maximization criterion (CMIM)<xref rid="bib57" ref-type="bibr"><sup>59</sup></xref> is another approach that relies on MI to select a candidate feature conditioning upon the features that have already been selected in the feature subset:<disp-formula id="ufd5"><mml:math id="M74" altimg="si30.gif"><mml:mrow><mml:mtext>CMIM</mml:mtext><mml:mover><mml:mo>=</mml:mo><mml:mtext>def</mml:mtext></mml:mover><mml:munder><mml:mtext>max</mml:mtext><mml:mrow><mml:mi>j</mml:mi><mml:mspace width="0.5em"/><mml:mo linebreak="badbreak">∈</mml:mo><mml:mspace width="0.5em"/><mml:mi>Q</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:munder><mml:mi>min</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mspace width="0.5em"/><mml:mo linebreak="badbreak">∈</mml:mo><mml:mspace width="0.25em"/><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0385">The conditional infomax feature extraction (CIFE)<xref rid="bib58" ref-type="bibr"><sup>60</sup></xref> explicitly attempts to integrate all three key concepts in FS, relevance, redundancy, and complementarity and takes the form<disp-formula id="ufd6"><mml:math id="M75" altimg="si31.gif"><mml:mrow><mml:mtext>CIFE</mml:mtext><mml:mover><mml:mo>=</mml:mo><mml:mtext>def</mml:mtext></mml:mover><mml:munder><mml:mtext>max</mml:mtext><mml:mrow><mml:mi>j</mml:mi><mml:mspace width="0.5em"/><mml:mo linebreak="badbreak">∈</mml:mo><mml:mspace width="0.5em"/><mml:mi>Q</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mspace width="0.5em"/><mml:mo linebreak="badbreak">∈</mml:mo><mml:mspace width="0.5em"/><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mspace width="0.5em"/><mml:mo linebreak="badbreak">∈</mml:mo><mml:mspace width="0.5em"/><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0390">The L1-squared-loss MI (L1-LSMI)<xref rid="bib9" ref-type="bibr"><sup>9</sup></xref> has been conceptually developed to integrate feature interaction and L1-regularization to maximize the squared-loss variant of MI between selected features and the response. This builds on the premise that estimating the density ratio may be a more efficient approach toward assessing variable dependencies compared with estimating the marginal and joint densities (or similarly entropies), which are computationally challenging for the accurate estimation of MI.<xref rid="bib59" ref-type="bibr"><sup>61</sup></xref></p>
        <p id="p0395">The quadratic programming feature selection (QPFS)<xref rid="bib60" ref-type="bibr"><sup>62</sup></xref> expresses the FS task as a quadratic-programming problem attempting to provide a global solution compared with the greedy approaches summarized above. Conceptually, QPFS attempts to make a global decision considering the interaction across all features jointly; however, Vinh et al.<xref rid="bib26" ref-type="bibr"><sup>26</sup></xref> have highlighted some challenges with the QPFS framework including tackling problems with small number of samples or potentially resultant non-convex formulations in empirical experiments.</p>
        <p id="p0400">The SPECCMI algorithm<xref rid="bib26" ref-type="bibr"><sup>26</sup></xref> has a similar conceptual grounding to QPFS aiming for a global MI-based FS solution. They proposed a spectral relaxation for efficiently solving a quadratic-integer-programming problem, which jointly considers relevance, unconditional redundancy, and class-conditional redundancy.</p>
      </sec>
      <sec id="sec4.4.3">
        <title>Implementations of FS algorithms</title>
        <p id="p0405">The performance details of the FS algorithms may differ using different implementations, so it is important to also highlight the source code used. For GSO, we used Guyon’s implementation, which can be found in the appendix of that study.<xref rid="bib23" ref-type="bibr"><sup>23</sup></xref> For mRMR, we used Peng’s implementation (<ext-link ext-link-type="uri" xlink:href="https://uk.mathworks.com/matlabcentral/fileexchange/14608-mrmr-feature-selection-using-mutual-information-computation" id="intref0060">https://uk.mathworks.com/matlabcentral/fileexchange/14608-mrmr-feature-selection-using-mutual-information-computation</ext-link>) and for mRMR Spearman the implementation by Tsanas (<ext-link ext-link-type="uri" xlink:href="https://github.com/ThanasisTsanas/mRMR_Spearman" id="intref0065">https://github.com/ThanasisTsanas/mRMR_Spearman</ext-link>). We remark that Peng’s code discretizes continuous features to efficiently compute MI (as most implementations computing MI do). For information gain, CFS, and CBF, we used the WEKA implementations (<ext-link ext-link-type="uri" xlink:href="https://www.cs.waikato.ac.nz/ml/weka/" id="intref0070">https://www.cs.waikato.ac.nz/ml/weka/</ext-link>) through a MATLAB interface from the ASU FS repository. The ASU FS repository provides a range of FS algorithms and was originally implemented in MATLAB (<ext-link ext-link-type="uri" xlink:href="https://jundongl.github.io/scikit-feature/OLD/home_old.html" id="intref0075">https://jundongl.github.io/scikit-feature/OLD/home_old.html</ext-link>) and, more recently, in Python (<ext-link ext-link-type="uri" xlink:href="https://jundongl.github.io/scikit-feature/" id="intref0080">https://jundongl.github.io/scikit-feature/</ext-link>). For the purposes of this study, we used the MATLAB implementation. For JMI and DISR, we used the fast implementation in FEAST (<ext-link ext-link-type="uri" xlink:href="https://github.com/Craigacp/FEAST" id="intref0085">https://github.com/Craigacp/FEAST</ext-link>).<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref> This implementation requires the discretization of continuous features for the computation of MI and following the authors' experimental setup that was set to 5 levels. Similarly, for QPFS, MIQ, CMIM, CIFE, and SPECMI, we used the implementation of Vinh (<ext-link ext-link-type="uri" xlink:href="https://uk.mathworks.com/matlabcentral/fileexchange/47129-information-theoretic-feature-selection" id="intref0090">https://uk.mathworks.com/matlabcentral/fileexchange/47129-information-theoretic-feature-selection</ext-link>), who also recommended 5 levels of discretization. For LOGO, we used the MATLAB implementation by the developer of the algorithm (<ext-link ext-link-type="uri" xlink:href="https://www.acsu.buffalo.edu/%7Eyijunsun/lab/LOGO.html" id="intref0095">https://www.acsu.buffalo.edu/∼yijunsun/lab/LOGO.html</ext-link>). For IAMB and HITON, we used the MATLAB implementation in the Causal Explorer by the developers of the algorithms (<ext-link ext-link-type="uri" xlink:href="https://github.com/mensxmachina/CausalExplorer_1.5" id="intref0100">https://github.com/mensxmachina/CausalExplorer_1.5</ext-link>), although we need to note that this is not open-source code (in the sense that the functions are executable but the implementations are not visible to developers). For L1-LSMI, we used the code by the developer of the package (Jitkrittum) (<ext-link ext-link-type="uri" xlink:href="https://github.com/wittawatj/l1lsmi" id="intref0105">https://github.com/wittawatj/l1lsmi</ext-link>). For RELIEF, we used MATLAB’s native implementation (command “relief”, <ext-link ext-link-type="uri" xlink:href="https://uk.mathworks.com/help/stats/relieff.html" id="intref0110">https://uk.mathworks.com/help/stats/relieff.html</ext-link>).</p>
      </sec>
      <sec id="sec4.4.4">
        <title>Novel FS algorithm: RRCT</title>
        <p id="p0410">We propose a new principled FS heuristic algorithm, RRCT, which attempts to effectively integrate all three major components outlined above for effective FS (relevance, redundancy, and complementarity) in a computationally efficient scheme. The proposed CBF FS algorithm extends the mRMR Spearman concept discussed in the preceding section by adjusting the original relevance and redundancy terms and incorporating a complementarity term. It relies on the computation of correlation coefficients, which are subsequently transformed using a function inspired by IT concepts. We invoke these IT concepts under the assumption that the features are normally distributed, which is common in diverse machine-learning applications and often works well in practice.<xref rid="bib61" ref-type="bibr"><sup>63</sup></xref> This assumption greatly facilitates analysis since important IT concepts that are of central importance to this new algorithm are simple to compute and to work with analytically.</p>
        <p id="p0415">The first step is to standardize features to have zero mean and unit standard deviation before further processing to ensure there is no feature clearly dominating others due to the measurement scales. Subsequently, we compute the Spearman rank correlation coefficient between the features and the response to obtain the vector of rank correlations <inline-formula><mml:math id="M76" altimg="si32.gif"><mml:mrow><mml:mspace width="0.25em"/><mml:mi mathvariant="bold">r</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where each entry denotes the correlation of each feature with the response. We used the Spearman rank correlation coefficient over the linear correlation coefficient as a more general method to express the relationship between variables. Then, we compute the covariance matrix, <inline-formula><mml:math id="M77" altimg="si33.gif"><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow></mml:math></inline-formula>, and denote its entries, <inline-formula><mml:math id="M78" altimg="si34.gif"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>ij</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>: these entries are the Spearman rank correlation coefficients computed between the features <inline-formula><mml:math id="M79" altimg="si35.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M80" altimg="si10.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M81" altimg="si36.gif"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mspace width="0.25em"/><mml:mi>ε</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.<disp-formula id="fd1"><label>(Equation 1)</label><mml:math id="M82" altimg="si37.gif"><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>12</mml:mn></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mo>…</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>12</mml:mn></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mo>…</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mo>…</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
        <p id="p0420">For the Gaussian distribution, there is an analytic expression for MI that depends only on the linear correlation coefficient <inline-formula><mml:math id="M83" altimg="si38.gif"><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:math></inline-formula><xref rid="bib27" ref-type="bibr"><sup>27</sup></xref> (strictly speaking, MI also relies on the variance, but this is 1 due to the standardization step):<disp-formula id="fd2"><label>(Equation 2)</label><mml:math id="M84" altimg="si39.gif"><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mo linebreak="goodbreak">−</mml:mo><mml:mn>0.5</mml:mn><mml:mo linebreak="goodbreak">⋅</mml:mo><mml:mi>ln</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0425"><xref rid="fd2" ref-type="disp-formula">Equation 2</xref> leads to an IT quantity (MI) that is obtained using the linear correlation coefficient. Here, we will use the same notion to define an IT quantity exactly as in <xref rid="fd2" ref-type="disp-formula">Equation 2</xref>, except this time the Spearman correlation coefficient will be used. For convenience, we will use the notation <inline-formula><mml:math id="M85" altimg="si40.gif"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>IT</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>0.5</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">⋅</mml:mo><mml:mtext>log</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to refer to the non-linearly transformed rank correlation coefficient <inline-formula><mml:math id="M86" altimg="si41.gif"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> between two random variables, <inline-formula><mml:math id="M87" altimg="si42.gif"><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula>. Now, we can write in compact vector form all the relevance terms using the IT-inspired transform in <xref rid="fd3" ref-type="disp-formula">Equation 3</xref>:<disp-formula id="fd3"><label>(Equation 3)</label><mml:math id="M88" altimg="si43.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mtext>ITL</mml:mtext></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mo linebreak="goodbreak">−</mml:mo><mml:mn>0.5</mml:mn><mml:mo linebreak="goodbreak">⋅</mml:mo><mml:mi>log</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:msubsup><mml:mi>r</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd columnalign="left"><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0430">Similarly, using the covariance matrix <inline-formula><mml:math id="M89" altimg="si33.gif"><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow></mml:math></inline-formula> and <xref rid="fd3" ref-type="disp-formula">Equation 3</xref>, the redundancy between pairs of features can be conveniently expressed as a matrix, where each <inline-formula><mml:math id="M90" altimg="si44.gif"><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> entry denotes the information that two features share in predicting the response:<disp-formula id="fd4"><label>(Equation 4)</label><mml:math id="M91" altimg="si45.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mtext>IT</mml:mtext></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mo linebreak="goodbreak">−</mml:mo><mml:mn>0.5</mml:mn><mml:mo linebreak="goodbreak">⋅</mml:mo><mml:mi>log</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mn>12</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>…</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>Μ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mn>12</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>Μ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>Μ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>Μ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>…</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0435">Now, inserting the relevance terms in <xref rid="fd3" ref-type="disp-formula">Equation 3</xref> across the main diagonal of <inline-formula><mml:math id="M92" altimg="si46.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mtext>IT</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> in <xref rid="fd4" ref-type="disp-formula">Equation 4</xref>, we obtain a matrix that will be used to compute the compromise between relevance and redundancy:<disp-formula id="fd5"><label>(Equation 5)</label><mml:math id="M93" altimg="si47.gif"><mml:mrow><mml:mi mathvariant="bold">D</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mo linebreak="goodbreak">−</mml:mo><mml:mn>0.5</mml:mn><mml:mo linebreak="goodbreak">⋅</mml:mo><mml:mi>log</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mn>12</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>…</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>Μ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mn>12</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>Μ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>Μ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>Μ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>…</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0440">The matrix <inline-formula><mml:math id="M94" altimg="si48.gif"><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow></mml:math></inline-formula> is essentially a compact form of mRMR, which alleviates the need for repeated computation of the relevance and complementarity terms in the iterative steps (therefore, this expedites the incremental FS process in large datasets). Conceptually, the IT transformation of the rank correlation coefficient assigns greater weight to coefficients above the absolute value 0.5 (see <xref rid="fig5" ref-type="fig">Figure 5</xref>). The effect is that weak statistical associations (between a feature and the response or between features) are penalized; conversely, strong associations (large absolute correlation coefficients) are enhanced. If the absolute value of the rank correlation coefficient is 1, we set the MI quantity to a very large value (we chose 1,000).</p>
        <p id="p0445">The proposed algorithm developed thus far can be seen as a computationally simpler version of the classical mRMR, which can be computationally efficiently computed because the information has been succinctly summarized in matrix <bold>D</bold>, so that for the computation of the new candidate feature <inline-formula><mml:math id="M95" altimg="si10.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (which corresponds to a feature not in the existing feature subset), we focus on the <inline-formula><mml:math id="M96" altimg="si49.gif"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula><sup>th</sup> row. The relevance of the feature <inline-formula><mml:math id="M97" altimg="si10.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> lies on the main diagonal of the matrix <bold>D</bold>, and the redundancy is computed from the average of the terms that appear in the column <inline-formula><mml:math id="M98" altimg="si23.gif"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> (the D<sub><italic>i</italic>,<italic>s</italic></sub> entries), where <inline-formula><mml:math id="M99" altimg="si23.gif"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> corresponds to features in the already selected subset <inline-formula><mml:math id="M100" altimg="si8.gif"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula> (which contains the indices <inline-formula><mml:math id="M101" altimg="si23.gif"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> of the selected features).</p>
        <p id="p0450">The following step is crucial for the development of RRCT: we embrace the concept of quantifying the conditional relevance (complementarity) of a feature as the usefulness of that feature in predicting the response conditional upon the already selected feature subset. This is achieved using the rank partial correlation coefficient, which quantifies the statistical association between two random variables, <inline-formula><mml:math id="M102" altimg="si42.gif"><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula>, while controlling for the effect of a set of a conditioning random variable, <inline-formula><mml:math id="M103" altimg="si50.gif"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:math></inline-formula>. This is defined as<disp-formula id="fd6"><label>(Equation 6)</label><mml:math id="M104" altimg="si51.gif"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>p</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>Z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo linebreak="badbreak">⋅</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">⋅</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">⋅</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mi>N</mml:mi><mml:mo linebreak="badbreak">⋅</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo linebreak="badbreak">−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo linebreak="badbreak">⋅</mml:mo><mml:msqrt><mml:mrow><mml:mi>N</mml:mi><mml:mo linebreak="badbreak">⋅</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo linebreak="badbreak">−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M105" altimg="si52.gif"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M106" altimg="si53.gif"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denote the residuals of <inline-formula><mml:math id="M107" altimg="si42.gif"><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula>, respectively, on <inline-formula><mml:math id="M108" altimg="si50.gif"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:math></inline-formula>. That is, the partial correlation coefficient is computed by first solving the two associated linear-regression problems and calculating the correlation between their residuals. Alternatively, the partial correlation coefficient can be computed using a recursive formula working directly with correlation coefficients: the <inline-formula><mml:math id="M109" altimg="si54.gif"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula><sup><italic>th</italic></sup>-order partial correlation (that is, the conditioning random variable <inline-formula><mml:math id="M110" altimg="si50.gif"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:math></inline-formula> contains <inline-formula><mml:math id="M111" altimg="si54.gif"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> features) is computed from three (<inline-formula><mml:math id="M112" altimg="si55.gif"><mml:mrow><mml:mi>n</mml:mi><mml:mspace width="0.25em"/><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mspace width="0.25em"/><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>)-order partial correlations (the 0<sup>th</sup>-order partial correlations are, by definition, the correlation coefficients). For the simplest case where the conditioning random variable <inline-formula><mml:math id="M113" altimg="si50.gif"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:math></inline-formula> comprises a single feature, this reduces to <xref rid="fd7" ref-type="disp-formula">Equation 7</xref>:<disp-formula id="fd7"><label>(Equation 7)</label><mml:math id="M114" altimg="si56.gif"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>p</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>Z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">⋅</mml:mo><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>Z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt><mml:mo linebreak="badbreak">⋅</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>Z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0455">The partial correlation coefficient expresses the contribution of the independent random variable <inline-formula><mml:math id="M115" altimg="si57.gif"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> over and above the contributions of the conditioning random variable <inline-formula><mml:math id="M116" altimg="si50.gif"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:math></inline-formula> for predicting the dependent random variable <inline-formula><mml:math id="M117" altimg="si58.gif"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula> and accounts for the additional explanation of the variance observed in <inline-formula><mml:math id="M118" altimg="si58.gif"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula> as a result of including <inline-formula><mml:math id="M119" altimg="si57.gif"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> in the regression setting. <xref rid="fig6" ref-type="fig">Figure 6</xref> presents a Venn diagram to graphically illustrate this point where the different regions denote the information captured by each random variable, and the overlapping regions denote the shared information between the random variables.</p>
        <p id="p0460">In the context of the developed FS algorithm, the partial correlation coefficient <inline-formula><mml:math id="M120" altimg="si59.gif"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>p</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> is defined as the rank correlation coefficient between a new candidate feature, <inline-formula><mml:math id="M121" altimg="si10.gif"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and the response <inline-formula><mml:math id="M122" altimg="si60.gif"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:math></inline-formula>, controlling for the existing features in the subset, i.e., <inline-formula><mml:math id="M123" altimg="si61.gif"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>p</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>|</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This approach aims to incorporate how well the candidate feature pairs up with the existing features that have already been chosen. Then, we transform the computed partial correlation coefficient using the IT-inspired transformation in <xref rid="fd2" ref-type="disp-formula">Equation 2</xref>, which gives<disp-formula id="fd8"><label>(Equation 8)</label><mml:math id="M124" altimg="si62.gif"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>p</mml:mtext><mml:mo>,</mml:mo><mml:mspace width="0.5em"/><mml:mtext>IT</mml:mtext></mml:mrow></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mo linebreak="goodbreak">−</mml:mo><mml:mn>0.5</mml:mn><mml:mo linebreak="goodbreak">⋅</mml:mo><mml:mi>log</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0465">Since the controlling variables <inline-formula><mml:math id="M125" altimg="si8.gif"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula> (whose effect needs to be removed to compute the partial correlation coefficient) are not known and will vary at each step, it is not possible to express this quantity in compact vector or matrix form as we did previously for <bold>D</bold>. The term in <xref rid="fd8" ref-type="disp-formula">Equation 8</xref> is thus computed separately in each step, giving rise to the final equation that is used toward selecting features in RRCT:<disp-formula id="fd9"><label>(Equation 9)</label><mml:math id="M126" altimg="si63.gif"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mtext>RRCT</mml:mtext><mml:mover><mml:mo>=</mml:mo><mml:mtext>def</mml:mtext></mml:mover><mml:munder><mml:mi>max</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mspace width="0.5em"/><mml:mo linebreak="badbreak">∈</mml:mo><mml:mspace width="0.5em"/><mml:mi>Q</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:munder><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>IT</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:munder><mml:mo linebreak="badbreak">−</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mspace width="0.25em"/><mml:mo linebreak="badbreak">∈</mml:mo><mml:mspace width="0.25em"/><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>IT</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:munder></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mspace width="1em"/><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mtext>sign</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>p</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>|</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">⋅</mml:mo><mml:mtext>sign</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>p</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>|</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">⋅</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>p</mml:mtext><mml:mo>,</mml:mo><mml:mspace width="0.5em"/><mml:mtext>IT</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><inline-formula><mml:math id="M127" altimg="si64.gif"><mml:mrow><mml:mtext>sign</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mo linebreak="badbreak">⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> returns +1 if the quantity <inline-formula><mml:math id="M128" altimg="si65.gif"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo linebreak="badbreak">⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is positive and −1 if <inline-formula><mml:math id="M129" altimg="si65.gif"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo linebreak="badbreak">⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is negative and is used to determine whether <inline-formula><mml:math id="M130" altimg="si66.gif"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>p</mml:mtext><mml:mo>,</mml:mo><mml:mtext> IT</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is added or subtracted in <xref rid="fd9" ref-type="disp-formula">Equation 9</xref>.</p>
        <p id="p0470">Care needs to be exercised in the RRCT expression when including the <inline-formula><mml:math id="M131" altimg="si66.gif"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>p</mml:mtext><mml:mo>,</mml:mo><mml:mtext> IT</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> term. Given that this term is non-negative due to the IT transformation, we need to determine whether the inclusion of the candidate feature to the existing subset actually contributes additional information conditional on the features in the selected subset (conditionally relevant). Consideration must be made for both the sign of the partial correlation coefficient and for the sign of the difference in magnitudes between <inline-formula><mml:math id="M132" altimg="si61.gif"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>p</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>|</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M133" altimg="si67.gif"><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The <inline-formula><mml:math id="M134" altimg="si68.gif"><mml:mrow><mml:mtext>sign</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>p</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>|</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> term in <xref rid="fd9" ref-type="disp-formula">Equation 9</xref> is used to determine whether the conditional relevance <inline-formula><mml:math id="M135" altimg="si61.gif"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>p</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>|</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is larger than <inline-formula><mml:math id="M136" altimg="si67.gif"><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> magnitude; that would suggest that including the candidate feature has additional (conditional) relevance given the features in the selected subset. The <inline-formula><mml:math id="M137" altimg="si69.gif"><mml:mrow><mml:mtext>sign</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>p</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>|</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> term is used to make the overall complementarity contribution positive in the case that <inline-formula><mml:math id="M138" altimg="si70.gif"><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M139" altimg="si71.gif"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>p</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>|</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M140" altimg="si72.gif"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>p</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>|</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> because then the term <inline-formula><mml:math id="M141" altimg="si68.gif"><mml:mrow><mml:mtext>sign</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>p</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>|</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> would indicate that the additional contribution offered by the complementarity term is negative.</p>
        <p id="p0475">RRCT uses <xref rid="fd9" ref-type="disp-formula">Equation 9</xref> to incrementally select a feature and place it in the selected set of features <inline-formula><mml:math id="M142" altimg="si8.gif"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula>. Specifically,</p>
        <p id="p0480">RRCT computation<list list-type="simple" id="olist0010"><list-item id="o0010"><label>1.</label><p id="p0485">Select and place the first feature with index <inline-formula><mml:math id="M143" altimg="si73.gif"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>: <inline-formula><mml:math id="M144" altimg="si74.gif"><mml:mrow><mml:munder><mml:mtext>max</mml:mtext><mml:mrow><mml:mi>j</mml:mi><mml:mspace width="0.25em"/><mml:mo>∈</mml:mo><mml:mspace width="0.25em"/><mml:mi>Q</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mtext>IT</mml:mtext></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the initially empty set <inline-formula><mml:math id="M145" altimg="si8.gif"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula>, that is <inline-formula><mml:math id="M146" altimg="si75.gif"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi>j</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">→</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="M147" altimg="si24.gif"><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:math></inline-formula> contains the indices of all the features in the initial <italic>M</italic>-dimensional feature space.</p></list-item><list-item id="o0015"><label>2.</label><p id="p0490">Selecting the next <inline-formula><mml:math id="M148" altimg="si76.gif"><mml:mrow><mml:mi>m</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> features, one at each step, by repeating the following: apply the criterion in <xref rid="fd9" ref-type="disp-formula">Equation 9</xref> to select the next feature index <inline-formula><mml:math id="M149" altimg="si73.gif"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> not already selected (i.e., from the <inline-formula><mml:math id="M150" altimg="si26.gif"><mml:mrow><mml:mi>Q</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">−</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula> set) and include it in the set <inline-formula><mml:math id="M151" altimg="si77.gif"><mml:mrow><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>j</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">→</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula>. The relevance and redundancy terms are conveniently pre-computed and directly used from matrix <inline-formula><mml:math id="M152" altimg="si48.gif"><mml:mrow><mml:mi mathvariant="bold">D</mml:mi></mml:mrow></mml:math></inline-formula> (see <xref rid="fd5" ref-type="disp-formula">Equation 5</xref>).</p></list-item><list-item id="o0020"><label>3.</label><p id="p0495">Obtain the feature subset by selecting the features <inline-formula><mml:math id="M153" altimg="si78.gif"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M154" altimg="si79.gif"><mml:mrow><mml:mi>j</mml:mi><mml:mspace width="0.25em"/><mml:mi>ε</mml:mi><mml:mspace width="0.25em"/><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula> from the original data matrix <inline-formula><mml:math id="M155" altimg="si11.gif"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:math></inline-formula>.</p></list-item></list></p>
        <p id="p0500">This reduced feature subset is the new data matrix <inline-formula><mml:math id="M156" altimg="si80.gif"><mml:mrow><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mspace width="0.25em"/><mml:mi>ε</mml:mi><mml:mspace width="0.25em"/><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, which can be used for further processing.</p>
      </sec>
      <sec id="sec4.4.5">
        <title>FS strategy</title>
        <p id="p0505">Each of the FS algorithms used herein provide a ranked output of the selected feature subset in descending order, where the first selected feature is deemed to be the most predictive of the response (where each FS algorithm has their own internal criteria for achieving this) and progressively working toward the successively less predictive features of the response. Optionally, some of the investigated algorithms also provide feature weights.</p>
        <p id="p0510">When we apply FS algorithms to determine a feature subset to assess whether the true set of features have been selected (e.g., in synthetic datasets), we can proceed with using the entire data matrix <inline-formula><mml:math id="M157" altimg="si11.gif"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:math></inline-formula>. However, when selecting a feature subset aiming to assess performance and when limited to a single dataset for selecting features and assessing model generalization performance, we need to be more careful. This is because we risk potentially biasing findings if we use the entire data matrix to select the feature subset and only subsequently proceed with standard model validation methods such as CV. Instead, feature sets need to be selected using a training set and evaluated on the testing set, for example, using CV, which is the standard model validation approach that has been typically used in FS literature for determining the feature set.</p>
        <p id="p0515">Ideally, we should obtain the same feature subset across all CV replications; this would clearly indicate what features should be selected in the dataset for a given FS algorithm. However, in practice, the selected features for any given FS algorithm may be different across different CV replicates. Hence, we need to develop a strategy to determine the selected features and the order with which they appear in the selected feature subset for each FS algorithm. Specifically, we follow the methodology we have previously described,<xref rid="bib15" ref-type="bibr"><sup>15</sup></xref><sup>,</sup><xref rid="bib62" ref-type="bibr"><sup>30</sup></xref><sup>,</sup><xref rid="bib63" ref-type="bibr"><sup>31</sup></xref> which is summarized in <xref rid="tbl5" ref-type="table">Table 5</xref>. This methodology is generic and can be readily applied with any greedy FS algorithm, i.e., all those FS algorithms that select features one at a time (moreover, it can be extended to alternative, non-greedy FS algorithms, see Tsanas<xref rid="bib15" ref-type="bibr"><sup>15</sup></xref>).</p>
      </sec>
    </sec>
    <sec id="sec4.5">
      <title>FS assessment</title>
      <p id="p0520">This section describes the methodology for the assessment of the FS algorithms when applied to the datasets summarized in <xref rid="tbl4" ref-type="table">Table 4</xref>.</p>
      <p id="p0525">There are two approaches to evaluate the performance of FS algorithms: (1) assessing whether the “optimal” feature subset was selected, i.e., identifying the true features contributing to estimating the response and discarding probes and redundant features, and (2) presenting the selected feature subsets into a subsequent supervised-learning algorithm (classifier or regressor, depending on the problem) and using a pre-defined performance measure for comparison. The former is only possible in synthetic datasets, where the true features are known in advance. The latter is a surrogate approach to evaluate the performance of FS algorithms<xref rid="bib64" ref-type="bibr"><sup>64</sup></xref> by introducing an additional layer into the FS problem and does not necessarily correspond to selecting the true feature subset. In practice, some weakly relevant or redundant features could improve the learners' performance; conversely, the benefit of discarding relevant features may outweigh loss in information content.<xref rid="bib22" ref-type="bibr"><sup>22</sup></xref> Moreover, it is possible that using different learners might lead to different conclusions regarding the superiority of the FS algorithms.<xref rid="bib20" ref-type="bibr"><sup>20</sup></xref> In practice, both approaches are commonly used in FS literature<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref><sup>,</sup><xref rid="bib7" ref-type="bibr"><sup>7</sup></xref><sup>,</sup><xref rid="bib64" ref-type="bibr"><sup>64</sup></xref> and will be used in this study as well to empirically compare the performance of RRCT against established widely used filter FS algorithms.</p>
      <sec id="sec4.5.1">
        <title>FS assessment using synthetic data</title>
        <p id="p0530">In synthetic datasets, the optimal feature subset is known <italic>a priori</italic>, and therefore we can evaluate whether the FS algorithms identify the true feature subset. Given that the number of true and false (collectively referring to redundant, irrelevant, and noisy) features in a synthetic dataset is known, we can progressively assess how well FS algorithms identify the true features. Specifically, we used the FDR, defined as the ratio of the number of false features identified by the FS algorithm as belonging to the selected feature subset over the total (true and false) number of selected features (i.e., number of false features estimated out of three features, out of four features, etc.) at a given step. The FDR lies in the range [0 … 1], where 0 indicates correctly identifying all features and 1 indicates selecting only false features (probes). Ideally, FDR should remain at zero for as many steps as the true features in the dataset, and we can use a plot to identify at what step FS algorithms select false features.</p>
        <p id="p0535">For the synthetic data, all samples were used to determine the final feature subset.</p>
      </sec>
      <sec id="sec4.5.2">
        <title>FS assessment using real-world data</title>
        <p id="p0540">To conform with the research literature on evaluating FS algorithms, we employ a statistical learner into which we present the selected features from the different FS algorithms. Given that we use this statistical-learning step to infer the performance of the FS algorithms, we wanted to use a statistical learner that would provide good performance, making full use of potential feature interactions, and not require any further tuning so that it provides a fair assessment of the FS step. For these reasons, we opted to use a RF herein because it is a powerful statistical learner that is very robust to the tuning of its hyper-parameters.<xref rid="bib1" ref-type="bibr"><sup>1</sup></xref><sup>,</sup><xref rid="bib65" ref-type="bibr"><sup>65</sup></xref> We used the default RF parameters: 500 trees, growing all trees fully, searching for the best split of the data by accessing in the tree nodes only the square root of the number of features (randomly selected), and using majority voting for the final RF estimate. This is because ultimately we did not aim to maximize performance but rather to objectively infer FS performance on the basis of the statistical learner’s outputs.</p>
        <p id="p0545">As mentioned previously, each FS algorithm used in the study provides the selected feature subset in descending order of preference, and here we presented the 1 … <italic>K</italic> selected features from each FS algorithm into the RF, where we set <italic>K</italic> to be the smallest number between 30 and the dataset dimensionality. In each step, we recorded the misclassification rate (number of samples assigned into the wrong class over the total number of samples presented into RF) and, for convenience, expressed that rate as a percentage. Ideally, we want this misclassification percentage score to be zero and can infer that an FS algorithm outperforms competing approaches when exhibiting a lower score.</p>
        <p id="p0550">For both the FS and the model validation in the real-world datasets, we used the following rules:<list list-type="simple" id="olist0015"><list-item id="o0025"><label>(1)</label><p id="p0555">If separate training and testing subsets were provided (see <xref rid="tbl4" ref-type="table">Table 4</xref>), then we used the training data to select features and train the model then evaluated performance on the testing data.</p></list-item><list-item id="o0030"><label>(2)</label><p id="p0560">If no separate training and testing subsets were provided, we used 10-fold CV to select features and assess performance when having more than 150 samples; otherwise, we used a leave-one-sample-out approach to select features and evaluate model performance.</p></list-item></list></p>
      </sec>
    </sec>
    <sec id="sec4.6">
      <title>Time-complexity analysis</title>
      <p id="p0565">A key practical consideration when choosing algorithms in general (and for the purposes of this study, FS algorithms in particular) includes understanding their time complexity, i.e., the running time. The running time depends on the computer configuration (computer and compiler or software) used, and therefore that needs to be reported along with the running time of specific algorithms across the examined practical tasks.<xref rid="bib66" ref-type="bibr"><sup>66</sup></xref></p>
      <p id="p0570">It is also possible to express the time complexity of an algorithm using the “big-oh” notation, which aims to represent the time complexity, e.g., as a function of the inputs into the algorithm.<xref rid="bib66" ref-type="bibr"><sup>66</sup></xref> However, as Aho and Ullman have noted, “quite often, the running time of a program depends on a particular input, not just on the size of the input,” in which case we need to consider the worst-case running time.<xref rid="bib66" ref-type="bibr"><sup>66</sup></xref> Therefore, when time complexity is considered in FS literature, researchers typically report the actual running time of algorithms along with the used computer configuration. We similarly provide this empirical comparison of FS algorithms in this study, reporting their running time across each of the 8 real-world datasets.</p>
    </sec>
  </sec>
</body>
<back>
  <ref-list id="cebib0010">
    <title>References</title>
    <ref id="bib1">
      <label>1</label>
      <element-citation publication-type="book" id="sref1">
        <person-group person-group-type="author">
          <name>
            <surname>Hastie</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Friedman</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>Elements of Statistical Learning</part-title>
        <edition>Second edition</edition>
        <year>2009</year>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib2">
      <label>2</label>
      <element-citation publication-type="journal" id="sref2">
        <person-group person-group-type="author">
          <name>
            <surname>Guyon</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Elisseeff</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>An introduction to variable and feature selection</article-title>
        <source>J. Mach. Learn. Res.</source>
        <volume>3</volume>
        <year>2003</year>
        <fpage>1157</fpage>
        <lpage>1182</lpage>
      </element-citation>
    </ref>
    <ref id="bib3">
      <label>3</label>
      <element-citation publication-type="book" id="sref3">
        <person-group person-group-type="editor">
          <name>
            <surname>Guyon</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Gunn</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Nikravesh</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Zadeh</surname>
            <given-names>L.A.</given-names>
          </name>
        </person-group>
        <source>Feature Extraction Foundations and Applications</source>
        <year>2006</year>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib4">
      <label>4</label>
      <element-citation publication-type="journal" id="sref4">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Efficient feature selection via analysis of relevance and redundancy</article-title>
        <source>J. Mach. Learn. Res.</source>
        <volume>5</volume>
        <year>2004</year>
        <fpage>1205</fpage>
        <lpage>1224</lpage>
      </element-citation>
    </ref>
    <ref id="bib5">
      <label>5</label>
      <element-citation publication-type="journal" id="sref5">
        <person-group person-group-type="author">
          <name>
            <surname>Torkkola</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Feature extraction by non-parametric mutual information maximization</article-title>
        <source>J. Mach. Learn. Res.</source>
        <volume>3</volume>
        <year>2003</year>
        <fpage>1415</fpage>
        <lpage>1438</lpage>
      </element-citation>
    </ref>
    <ref id="bib6">
      <label>6</label>
      <element-citation publication-type="journal" id="sref6">
        <person-group person-group-type="author">
          <name>
            <surname>Tsanas</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Saunders</surname>
            <given-names>K.E.</given-names>
          </name>
          <name>
            <surname>Bilderbeck</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Palmius</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Goodwin</surname>
            <given-names>G.M.</given-names>
          </name>
          <name>
            <surname>De Vos</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Clinical insight into latent variables of psychiatric questionnaires for mood symptom self-assessment</article-title>
        <source>JMIR Ment. Heal.</source>
        <volume>4</volume>
        <year>2017</year>
        <fpage>e15</fpage>
      </element-citation>
    </ref>
    <ref id="bib7">
      <label>7</label>
      <element-citation publication-type="journal" id="sref7">
        <person-group person-group-type="author">
          <name>
            <surname>Sun</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Todorovic</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Goodison</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Local-learning-based feature selection for high-dimensional data analysis</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <volume>32</volume>
        <year>2010</year>
        <fpage>1610</fpage>
        <lpage>1626</lpage>
        <pub-id pub-id-type="pmid">20634556</pub-id>
      </element-citation>
    </ref>
    <ref id="bib8">
      <label>8</label>
      <element-citation publication-type="book" id="sref8">
        <person-group person-group-type="author">
          <name>
            <surname>Kira</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Rendell</surname>
            <given-names>L.A.</given-names>
          </name>
        </person-group>
        <part-title>A practical approach to feature selection</part-title>
        <source>Proceedings of the Ninth International Conference on Machine Learning</source>
        <year>1992</year>
        <publisher-name>Morgan Kaufmann Publishers, Inc.</publisher-name>
        <fpage>249</fpage>
        <lpage>256</lpage>
      </element-citation>
    </ref>
    <ref id="bib9">
      <label>9</label>
      <element-citation publication-type="book" id="sref9">
        <person-group person-group-type="author">
          <name>
            <surname>Jitkrittum</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Hachiya</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Sugiyama</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <part-title>Feature Selection via L1-Penalized Squared-Loss Mutual Information</part-title>
        <year>2013</year>
        <publisher-name>IEICE Trans. Inf. Syst. <italic>E96-D</italic></publisher-name>
        <fpage>1513</fpage>
        <lpage>1524</lpage>
      </element-citation>
    </ref>
    <ref id="bib10">
      <label>10</label>
      <element-citation publication-type="journal" id="sref10">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Morstatter</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Trevino</surname>
            <given-names>R.P.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Feature selection: a data perspective</article-title>
        <source>ACM Comput. Surv.</source>
        <volume>50</volume>
        <year>2018</year>
        <fpage>94</fpage>
      </element-citation>
    </ref>
    <ref id="bib11">
      <label>11</label>
      <element-citation publication-type="journal" id="sref11">
        <person-group person-group-type="author">
          <name>
            <surname>Aliferis</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Statnikov</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Tsamardinos</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Mani</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Koutsoukos</surname>
            <given-names>X.D.</given-names>
          </name>
        </person-group>
        <article-title>Local causal and Markov Blanket induction for causal Discovery and feature selection for classification Part II: analysis and extensions</article-title>
        <source>J. Mach. Learn. Res.</source>
        <volume>11</volume>
        <year>2010</year>
        <fpage>171</fpage>
        <lpage>234</lpage>
      </element-citation>
    </ref>
    <ref id="bib12">
      <label>12</label>
      <element-citation publication-type="journal" id="sref12">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Fan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Tao</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Top-k feature selection framework using robust 0-1 integer programming</article-title>
        <source>IEEE Trans. Neural Network. Learn. Syst.</source>
        <volume>32</volume>
        <year>2021</year>
        <fpage>3005</fpage>
        <lpage>3019</lpage>
      </element-citation>
    </ref>
    <ref id="bib13">
      <label>13</label>
      <element-citation publication-type="journal" id="sref13">
        <person-group person-group-type="author">
          <name>
            <surname>Brown</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Pocock</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Zhaoo</surname>
            <given-names>M.-J.</given-names>
          </name>
          <name>
            <surname>Lujan</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Conditional likelihood maximisation: a unifying framework for information theoretic feature selection</article-title>
        <source>J. Mach. Learn. Res.</source>
        <volume>13</volume>
        <year>2012</year>
        <fpage>1</fpage>
        <lpage>40</lpage>
      </element-citation>
    </ref>
    <ref id="bib14">
      <label>14</label>
      <element-citation publication-type="journal" id="sref14">
        <person-group person-group-type="author">
          <name>
            <surname>Cai</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Feature selection in machine learning: a new perspective</article-title>
        <source>Neurocomputing</source>
        <volume>300</volume>
        <year>2018</year>
        <fpage>70</fpage>
        <lpage>79</lpage>
      </element-citation>
    </ref>
    <ref id="bib15">
      <label>15</label>
      <element-citation publication-type="book" id="sref15">
        <person-group person-group-type="author">
          <name>
            <surname>Tsanas</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>Accurate Telemonitoring of Parkinson’s Disease Using Nonlinear Speech Signal Processing and Statistical Machine Learning</part-title>
        <year>2012</year>
        <publisher-name>University of Oxford</publisher-name>
        <ext-link ext-link-type="uri" xlink:href="https://ora.ox.ac.uk/objects/uuid:2a43b92a-9cd5-4646-8f0f-81dbe2ba9d74/download_file?file_format=pdf&amp;safe_filename=DPhil%2Bthesis_post_viva_v8.pdf&amp;type_of_work=Thesis" id="interef5000">https://ora.ox.ac.uk/objects/uuid:2a43b92a-9cd5-4646-8f0f-81dbe2ba9d74/download_file?file_format=pdf&amp;safe_filename=DPhil%2Bthesis_post_viva_v8.pdf&amp;type_of_work=Thesis</ext-link>
      </element-citation>
    </ref>
    <ref id="bib16">
      <label>16</label>
      <element-citation publication-type="journal" id="sref16">
        <person-group person-group-type="author">
          <name>
            <surname>Naydenova</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Tsanas</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Howie</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Casals-Pascual</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>De Vos</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>The power of data mining in diagnosis of childhood pneumonia</article-title>
        <source>J. R. Soc. Interf.</source>
        <volume>13</volume>
        <year>2016</year>
        <fpage>20160266</fpage>
      </element-citation>
    </ref>
    <ref id="bib17">
      <label>17</label>
      <element-citation publication-type="journal" id="sref17">
        <person-group person-group-type="author">
          <name>
            <surname>Tsanas</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Little</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Ramig</surname>
            <given-names>L.O.</given-names>
          </name>
        </person-group>
        <article-title>Remote assessment of Parkinson’s disease symptom severity using the simulated cellular mobile telephone network</article-title>
        <source>IEEE Access</source>
        <volume>9</volume>
        <year>2021</year>
        <fpage>11024</fpage>
        <lpage>11036</lpage>
        <pub-id pub-id-type="pmid">33495722</pub-id>
      </element-citation>
    </ref>
    <ref id="bib18">
      <label>18</label>
      <element-citation publication-type="journal" id="sref18">
        <person-group person-group-type="author">
          <name>
            <surname>Vogel</surname>
            <given-names>A.P.</given-names>
          </name>
          <name>
            <surname>Tsanas</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Scattoni</surname>
            <given-names>M.L.</given-names>
          </name>
        </person-group>
        <article-title>Quantifying ultrasonic mouse vocalizations using acoustic analysis in a supervised statistical machine learning framework</article-title>
        <source>Sci. Rep.</source>
        <volume>9</volume>
        <year>2019</year>
        <fpage>e8100</fpage>
      </element-citation>
    </ref>
    <ref id="bib19">
      <label>19</label>
      <element-citation publication-type="journal" id="sref19">
        <person-group person-group-type="author">
          <name>
            <surname>Kohavi</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>John</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Wrappers for feature subset selection</article-title>
        <source>Artif. Intell.</source>
        <volume>97</volume>
        <year>1997</year>
        <fpage>273</fpage>
        <lpage>324</lpage>
      </element-citation>
    </ref>
    <ref id="bib20">
      <label>20</label>
      <element-citation publication-type="journal" id="sref20">
        <person-group person-group-type="author">
          <name>
            <surname>Hilario</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kalousis</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Approaches to dimensionality reduction in proteomic biomarker studies</article-title>
        <source>Brief. Bioinf.</source>
        <volume>9</volume>
        <year>2008</year>
        <fpage>102</fpage>
        <lpage>118</lpage>
      </element-citation>
    </ref>
    <ref id="bib21">
      <label>21</label>
      <element-citation publication-type="book" id="sref21">
        <person-group person-group-type="author">
          <name>
            <surname>Deng</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Weng</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Liu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Motoda</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <source>Computational methods of feature selection</source>
        <year>2008</year>
        <publisher-name>Chapman and Hall/CRC</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib22">
      <label>22</label>
      <element-citation publication-type="journal" id="sref22">
        <person-group person-group-type="author">
          <name>
            <surname>Guyon</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Mader</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Pletscher</surname>
            <given-names>P.a.</given-names>
          </name>
          <name>
            <surname>Schneider</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Uhr</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Competitive baseline methods set new standards for the NIPS 2003 feature selection benchmark</article-title>
        <source>Pattern Recogn. Lett.</source>
        <volume>28</volume>
        <year>2007</year>
        <fpage>1438</fpage>
        <lpage>1444</lpage>
      </element-citation>
    </ref>
    <ref id="bib23">
      <label>23</label>
      <element-citation publication-type="book" id="sref23">
        <person-group person-group-type="author">
          <name>
            <surname>Guyon</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <part-title>Practical Feature Selection : From Correlation to Causality</part-title>
        <volume>19</volume>
        <year>2008</year>
        <publisher-name>IOS Press</publisher-name>
        <ext-link ext-link-type="uri" xlink:href="https://ebooks.iospress.nl/volumearticle/23669" id="intereref5100">https://ebooks.iospress.nl/volumearticle/23669</ext-link>
      </element-citation>
    </ref>
    <ref id="bib24">
      <label>24</label>
      <element-citation publication-type="book" id="sref24">
        <person-group person-group-type="author">
          <name>
            <surname>Brown</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <part-title>A new perspective for information theoretic feature selection</part-title>
        <source>12th International Conference on Artificial Intelligence and Statistics (AISTATS)</source>
        <year>2009</year>
        <fpage>49</fpage>
        <lpage>56</lpage>
      </element-citation>
    </ref>
    <ref id="bib25">
      <label>25</label>
      <element-citation publication-type="journal" id="sref25">
        <person-group person-group-type="author">
          <name>
            <surname>Arora</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Baghai-Ravary</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Tsanas</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Developing a large scale population screening tool for the assessment of Parkinson’s disease using telephone-quality voice</article-title>
        <source>J. Acoust. Soc. Am.</source>
        <volume>145</volume>
        <year>2019</year>
        <fpage>2871</fpage>
        <lpage>2884</lpage>
        <pub-id pub-id-type="pmid">31153319</pub-id>
      </element-citation>
    </ref>
    <ref id="bib26">
      <label>26</label>
      <element-citation publication-type="book" id="sref26">
        <person-group person-group-type="author">
          <name>
            <surname>Vinh</surname>
            <given-names>X.N.</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Romano</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Bailey</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>Effective global approaches for mutual information based feature selection</part-title>
        <source>Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</source>
        <year>2014</year>
        <fpage>512</fpage>
        <lpage>521</lpage>
      </element-citation>
    </ref>
    <ref id="bib27">
      <label>27</label>
      <element-citation publication-type="book" id="sref27">
        <person-group person-group-type="author">
          <name>
            <surname>Cover</surname>
            <given-names>T.M.</given-names>
          </name>
          <name>
            <surname>Thomas</surname>
            <given-names>J.A.</given-names>
          </name>
        </person-group>
        <part-title>Elements of Information Theory</part-title>
        <edition>2nd</edition>
        <year>2005</year>
        <publisher-name>Wiley</publisher-name>
        <ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X" id="intereref5120">https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X</ext-link>
      </element-citation>
    </ref>
    <ref id="bib28">
      <label>28</label>
      <element-citation publication-type="book" id="sref28">
        <person-group person-group-type="author">
          <name>
            <surname>Vinh</surname>
            <given-names>N.X.</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Bailey</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>Reconsidering mutual information based feature selection : a statistical significance view</part-title>
        <source>Twenty-Eighth AAAI Conference on Artificial Intelligence</source>
        <year>2014</year>
        <publisher-name>Association for the Advancement of Artificial Intelligence</publisher-name>
        <fpage>2092</fpage>
        <lpage>2098</lpage>
      </element-citation>
    </ref>
    <ref id="bib29">
      <label>29</label>
      <element-citation publication-type="journal" id="sref29">
        <person-group person-group-type="author">
          <name>
            <surname>García</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Luengo</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Sáez</surname>
            <given-names>J.A.</given-names>
          </name>
          <name>
            <surname>López</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Herrera</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>A survey of discretization techniques: taxonomy and empirical analysis in supervised learning</article-title>
        <source>IEEE Trans. Knowl. Data Eng.</source>
        <volume>25</volume>
        <year>2013</year>
        <fpage>734</fpage>
        <lpage>750</lpage>
      </element-citation>
    </ref>
    <ref id="bib62">
      <label>30</label>
      <element-citation publication-type="journal" id="sref62">
        <person-group person-group-type="author">
          <name>
            <surname>Tsanas</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Little</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>McSharry</surname>
            <given-names>P.E.</given-names>
          </name>
          <name>
            <surname>Spielman</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ramig</surname>
            <given-names>L.O.</given-names>
          </name>
        </person-group>
        <article-title>Novel speech signal processing algorithms for high-accuracy classification of Parkinsons disease</article-title>
        <source>IEEE Trans. Biomed. Eng.</source>
        <volume>59</volume>
        <year>2012</year>
        <fpage>1264</fpage>
        <lpage>1271</lpage>
        <pub-id pub-id-type="pmid">22249592</pub-id>
      </element-citation>
    </ref>
    <ref id="bib63">
      <label>31</label>
      <element-citation publication-type="journal" id="sref63">
        <person-group person-group-type="author">
          <name>
            <surname>Tsanas</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Little</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Fox</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Ramig</surname>
            <given-names>L.O.</given-names>
          </name>
        </person-group>
        <article-title>Objective automatic assessment of rehabilitative speech treatment in Parkinson’s disease</article-title>
        <source>IEEE Trans. Neural Syst. Rehabil. Eng.</source>
        <volume>22</volume>
        <year>2014</year>
        <fpage>181</fpage>
        <lpage>190</lpage>
        <pub-id pub-id-type="pmid">26271131</pub-id>
      </element-citation>
    </ref>
    <ref id="bib30">
      <label>32</label>
      <element-citation publication-type="book" id="sref30">
        <person-group person-group-type="author">
          <name>
            <surname>van der Maaten</surname>
            <given-names>L.J.P.</given-names>
          </name>
          <name>
            <surname>Postma</surname>
            <given-names>E.O.</given-names>
          </name>
          <name>
            <surname>van den Herik</surname>
            <given-names>H.J.</given-names>
          </name>
        </person-group>
        <part-title>Dimensionality Reduction: A Comparative Review</part-title>
        <year>2009</year>
        <publisher-name>Tilburg University Technical Report</publisher-name>
        <ext-link ext-link-type="uri" xlink:href="https://lvdmaaten.github.io/publications/papers/TR_Dimensionality_Reduction_Review_2009.pdf" id="intereref5130">https://lvdmaaten.github.io/publications/papers/TR_Dimensionality_Reduction_Review_2009.pdf</ext-link>
      </element-citation>
    </ref>
    <ref id="bib31">
      <label>33</label>
      <element-citation publication-type="journal" id="sref31">
        <person-group person-group-type="author">
          <name>
            <surname>Kuncheva</surname>
            <given-names>L.I.</given-names>
          </name>
        </person-group>
        <article-title>On the optimality of Naïve Bayes with dependent binary features</article-title>
        <source>Pattern Recognit. Lett.</source>
        <volume>27</volume>
        <year>2006</year>
        <fpage>830</fpage>
        <lpage>837</lpage>
      </element-citation>
    </ref>
    <ref id="bib32">
      <label>34</label>
      <element-citation publication-type="journal" id="sref32">
        <person-group person-group-type="author">
          <name>
            <surname>Hand</surname>
            <given-names>D.J.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Idiot’s bayes-not so stupid after all?</article-title>
        <source>Int. Stat. Rev.</source>
        <volume>69</volume>
        <year>2001</year>
        <fpage>385</fpage>
        <lpage>398</lpage>
      </element-citation>
    </ref>
    <ref id="bib33">
      <label>35</label>
      <element-citation publication-type="journal" id="sref33">
        <person-group person-group-type="author">
          <name>
            <surname>Box</surname>
            <given-names>G.E.</given-names>
          </name>
          <name>
            <surname>Cox</surname>
            <given-names>D.R.</given-names>
          </name>
        </person-group>
        <article-title>An analysis of transformations</article-title>
        <source>J. R. Stat. Soc. Ser. B</source>
        <volume>26</volume>
        <year>1964</year>
        <fpage>211</fpage>
        <lpage>252</lpage>
      </element-citation>
    </ref>
    <ref id="bib34">
      <label>36</label>
      <element-citation publication-type="journal" id="sref34">
        <person-group person-group-type="author">
          <name>
            <surname>Marazzi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Yohai</surname>
            <given-names>V.J.</given-names>
          </name>
        </person-group>
        <article-title>Robust Box-Cox transformations based on minimum residual autocorrelation</article-title>
        <source>Comput. Stat. Data Anal.</source>
        <volume>50</volume>
        <year>2006</year>
        <fpage>2752</fpage>
        <lpage>2768</lpage>
      </element-citation>
    </ref>
    <ref id="bib35">
      <label>37</label>
      <element-citation publication-type="journal" id="sref35">
        <person-group person-group-type="author">
          <name>
            <surname>Mansournia</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Altman</surname>
            <given-names>D.G.</given-names>
          </name>
        </person-group>
        <article-title>Inverse probability weighting</article-title>
        <source>BMJ</source>
        <volume>352</volume>
        <year>2016</year>
        <fpage>i189</fpage>
        <pub-id pub-id-type="pmid">26773001</pub-id>
      </element-citation>
    </ref>
    <ref id="bib36">
      <label>38</label>
      <element-citation publication-type="book" id="sref36">
        <person-group person-group-type="author">
          <name>
            <surname>Lundberg</surname>
            <given-names>S.M.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>S.-I.</given-names>
          </name>
        </person-group>
        <part-title>A unified approach to interpreting model predictions</part-title>
        <source>NIPS</source>
        <year>2017</year>
        <fpage>4765</fpage>
        <lpage>4774</lpage>
      </element-citation>
    </ref>
    <ref id="bib37">
      <label>39</label>
      <element-citation publication-type="journal" id="sref37">
        <person-group person-group-type="author">
          <name>
            <surname>Guan</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Hampton</surname>
            <given-names>C.Y.</given-names>
          </name>
          <name>
            <surname>Benigno</surname>
            <given-names>B.B.</given-names>
          </name>
          <name>
            <surname>Walker</surname>
            <given-names>L.D.E.</given-names>
          </name>
          <name>
            <surname>Gray</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>McDonald</surname>
            <given-names>J.F.</given-names>
          </name>
          <name>
            <surname>Fernández</surname>
            <given-names>F.M.</given-names>
          </name>
        </person-group>
        <article-title>Ovarian cancer detection from metabolomic liquid chromatography/mass spectrometry data by support vector machines</article-title>
        <source>BMC Bioinf.</source>
        <volume>10</volume>
        <year>2009</year>
        <fpage>259</fpage>
      </element-citation>
    </ref>
    <ref id="bib38">
      <label>40</label>
      <element-citation publication-type="journal" id="sref38">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Long</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Feature selection based on mutual information</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <volume>27</volume>
        <year>2005</year>
        <fpage>1226</fpage>
        <lpage>1238</lpage>
        <pub-id pub-id-type="pmid">16119262</pub-id>
      </element-citation>
    </ref>
    <ref id="bib39">
      <label>41</label>
      <element-citation publication-type="journal" id="sref39">
        <person-group person-group-type="author">
          <name>
            <surname>Estévez</surname>
            <given-names>P.A.</given-names>
          </name>
          <name>
            <surname>Tesmer</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Perez</surname>
            <given-names>C.A.</given-names>
          </name>
          <name>
            <surname>Zurada</surname>
            <given-names>J.M.</given-names>
          </name>
        </person-group>
        <article-title>Normalized mutual information feature selection</article-title>
        <source>IEEE Trans. Neural Network.</source>
        <volume>20</volume>
        <year>2009</year>
        <fpage>189</fpage>
        <lpage>201</lpage>
      </element-citation>
    </ref>
    <ref id="bib40">
      <label>42</label>
      <element-citation publication-type="journal" id="sref40">
        <person-group person-group-type="author">
          <name>
            <surname>Herman</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Mutual information-based method for selecting informative feature sets</article-title>
        <source>Pattern Recogn.</source>
        <volume>46</volume>
        <year>2013</year>
        <fpage>3315</fpage>
        <lpage>3327</lpage>
      </element-citation>
    </ref>
    <ref id="bib41">
      <label>43</label>
      <element-citation publication-type="book" id="sref41">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <part-title>Searching for interacting features</part-title>
        <source>20th International Joint Conference on Artifical Intelligence</source>
        <year>2007</year>
        <fpage>1156</fpage>
        <lpage>1161</lpage>
      </element-citation>
    </ref>
    <ref id="bib42">
      <label>44</label>
      <element-citation publication-type="journal" id="sref42">
        <person-group person-group-type="author">
          <name>
            <surname>Zebari</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Abdulazeez</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Zeebaree</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Zebari</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Saeed</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>A comprehensive review of dimensionality reduction techniques for feature selection and feature extraction</article-title>
        <source>J. Appl. Sci. Technol. Trends</source>
        <volume>1</volume>
        <year>2020</year>
        <fpage>56</fpage>
        <lpage>70</lpage>
      </element-citation>
    </ref>
    <ref id="bib43">
      <label>45</label>
      <element-citation publication-type="book" id="sref43">
        <person-group person-group-type="author">
          <name>
            <surname>Whitesitt</surname>
            <given-names>J.E.</given-names>
          </name>
        </person-group>
        <part-title>Boolean Algebra and its Applications</part-title>
        <year>2010</year>
        <publisher-name>Dover</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib44">
      <label>46</label>
      <element-citation publication-type="journal" id="sref44">
        <person-group person-group-type="author">
          <name>
            <surname>Tang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Alelyani</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Feature selection for classification: a review</article-title>
        <source>Data Classif. Algorithms Appl.</source>
        <year>2014</year>
        <fpage>37</fpage>
        <lpage>64</lpage>
      </element-citation>
    </ref>
    <ref id="bib45">
      <label>47</label>
      <element-citation publication-type="book" id="sref45">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Morstatter</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Alelyani</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Anand</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <part-title>Advancing Feature Selection Research - ASU Feature Selection Repository</part-title>
        <year>2010</year>
        <publisher-name>Arizona State University</publisher-name>
        <ext-link ext-link-type="uri" xlink:href="https://www.public.asu.edu/~huanliu/papers/tr-10-007.pdf" id="intereref5400">https://www.public.asu.edu/∼huanliu/papers/tr-10-007.pdf</ext-link>
      </element-citation>
    </ref>
    <ref id="bib46">
      <label>48</label>
      <element-citation publication-type="journal" id="sref46">
        <person-group person-group-type="author">
          <name>
            <surname>Stoppiglia</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Dreyfus</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Dubois</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Oussar</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Ranking a random feature for variable and feature selection Hervé Stoppiglia Gérard Dreyfus</article-title>
        <source>J. Mach. Learn. Res.</source>
        <volume>3</volume>
        <year>2003</year>
        <fpage>1399</fpage>
        <lpage>1414</lpage>
      </element-citation>
    </ref>
    <ref id="bib47">
      <label>49</label>
      <element-citation publication-type="journal" id="sref47">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Feature selection for high-dimensional data: a fast correlation-based filter solution</article-title>
        <source>Proc. Twent. Int. Conf. Mach. Learn.</source>
        <volume>2</volume>
        <year>2003</year>
        <fpage>856</fpage>
        <lpage>863</lpage>
      </element-citation>
    </ref>
    <ref id="bib48">
      <label>50</label>
      <element-citation publication-type="book" id="sref48">
        <person-group person-group-type="author">
          <name>
            <surname>Kononenko</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <part-title>Estimating attributes: analysis and extensions of RELIEF</part-title>
        <source>Proceedings of the European Conference on Machine Learning</source>
        <year>1994</year>
        <fpage>171</fpage>
        <lpage>182</lpage>
      </element-citation>
    </ref>
    <ref id="bib49">
      <label>51</label>
      <element-citation publication-type="journal" id="sref49">
        <person-group person-group-type="author">
          <name>
            <surname>Robnik-Sikonja</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kononenko</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <article-title>Theoretical and empirical analysis of ReliefF and RReliefF</article-title>
        <source>Mach. Learn.</source>
        <volume>53</volume>
        <year>2003</year>
        <fpage>23</fpage>
        <lpage>69</lpage>
      </element-citation>
    </ref>
    <ref id="bib50">
      <label>52</label>
      <element-citation publication-type="book" id="sref50">
        <person-group person-group-type="author">
          <name>
            <surname>Gilad-Bachrach</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Navot</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Tishby</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <part-title>Margin based feature selection - theory and algorithms</part-title>
        <source>21st International Conference Maching Learning (ICML)</source>
        <year>2004</year>
        <fpage>43</fpage>
        <lpage>50</lpage>
      </element-citation>
    </ref>
    <ref id="bib51">
      <label>53</label>
      <element-citation publication-type="book" id="sref51">
        <person-group person-group-type="author">
          <name>
            <surname>Tsamardinos</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Aliferis</surname>
            <given-names>C.F.</given-names>
          </name>
        </person-group>
        <part-title>Towards principled feature selection: relevancy, filters, and wrappers</part-title>
        <source>Ninth international workshop on Artificial Intelligence and Statistics</source>
        <year>2003</year>
        <fpage>300</fpage>
        <lpage>307</lpage>
      </element-citation>
    </ref>
    <ref id="bib52">
      <label>54</label>
      <element-citation publication-type="book" id="sref52">
        <person-group person-group-type="author">
          <name>
            <surname>Aliferis</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Tsamardinos</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Statnikov</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>HITON: a novel Markov Blanket algorithm for optimal variable selection</part-title>
        <source>AMIA 2003 Annual Symposium proceedings</source>
        <year>2003</year>
        <fpage>21</fpage>
        <lpage>25</lpage>
      </element-citation>
    </ref>
    <ref id="bib53">
      <label>55</label>
      <element-citation publication-type="book" id="sref53">
        <person-group person-group-type="author">
          <name>
            <surname>Tsanas</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Little</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>McSharry</surname>
            <given-names>P.E.</given-names>
          </name>
        </person-group>
        <part-title>A methodology for the analysis of medical data</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Sturmberg</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>C.M.</given-names>
          </name>
        </person-group>
        <source>Handbook of Systems and Complexity in Health</source>
        <year>2013</year>
        <publisher-name>Springer</publisher-name>
        <fpage>113</fpage>
        <lpage>125</lpage>
      </element-citation>
    </ref>
    <ref id="bib54">
      <label>56</label>
      <element-citation publication-type="journal" id="sref54">
        <person-group person-group-type="author">
          <name>
            <surname>Ding</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Minimum redundancy feature selection from microarray gene expression data</article-title>
        <source>Proc. IEEE Bioinforma. Conf. CSB 2003</source>
        <year>2003</year>
        <fpage>523</fpage>
        <lpage>528</lpage>
      </element-citation>
    </ref>
    <ref id="bib55">
      <label>57</label>
      <element-citation publication-type="book" id="sref55">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>H.H.</given-names>
          </name>
          <name>
            <surname>Moody</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>Data visualization and feature selection: new algorithms for nongaussian data</part-title>
        <source>Advances in Neural Information Processing Systems</source>
        <year>1999</year>
        <fpage>687</fpage>
        <lpage>693</lpage>
      </element-citation>
    </ref>
    <ref id="bib56">
      <label>58</label>
      <element-citation publication-type="journal" id="sref56">
        <person-group person-group-type="author">
          <name>
            <surname>Meyer</surname>
            <given-names>P.E.</given-names>
          </name>
          <name>
            <surname>Schretter</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Bontempi</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Information-theoretic feature selection in microarray data using variable complementarity</article-title>
        <source>IEEE J. Sel. Top. Signal Process.</source>
        <volume>2</volume>
        <year>2008</year>
        <fpage>261</fpage>
        <lpage>274</lpage>
      </element-citation>
    </ref>
    <ref id="bib57">
      <label>59</label>
      <element-citation publication-type="journal" id="sref57">
        <person-group person-group-type="author">
          <name>
            <surname>Fleuret</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Fast binary feature selection with conditional mutual information</article-title>
        <source>J. Mach. Learn. Res.</source>
        <volume>5</volume>
        <year>2004</year>
        <fpage>1531</fpage>
        <lpage>1555</lpage>
      </element-citation>
    </ref>
    <ref id="bib58">
      <label>60</label>
      <element-citation publication-type="book" id="sref58">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <part-title>Conditional infomax learning: an integrated framework for feature extraction and fusion</part-title>
        <source>Lecture Notes in Computer Science</source>
        <year>2006</year>
        <fpage>68</fpage>
        <lpage>82</lpage>
      </element-citation>
    </ref>
    <ref id="bib59">
      <label>61</label>
      <element-citation publication-type="journal" id="sref59">
        <person-group person-group-type="author">
          <name>
            <surname>Suzuki</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Sugiyama</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Sufficient dimension reduction via squared-loss mutual information estimation</article-title>
        <source>Neural Comput.</source>
        <volume>25</volume>
        <year>2013</year>
        <fpage>725</fpage>
        <lpage>758</lpage>
        <pub-id pub-id-type="pmid">23272920</pub-id>
      </element-citation>
    </ref>
    <ref id="bib60">
      <label>62</label>
      <element-citation publication-type="journal" id="sref60">
        <person-group person-group-type="author">
          <name>
            <surname>Rodriguez-Lujan</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Huerta</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Elkan</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Cruz</surname>
            <given-names>C.S.</given-names>
          </name>
        </person-group>
        <article-title>Quadratic programming feature selection</article-title>
        <source>J. Mach. Learn. Res.</source>
        <volume>11</volume>
        <year>2010</year>
        <fpage>1491</fpage>
        <lpage>1516</lpage>
      </element-citation>
    </ref>
    <ref id="bib61">
      <label>63</label>
      <element-citation publication-type="book" id="sref61">
        <person-group person-group-type="author">
          <name>
            <surname>Bishop</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <part-title>Pattern Recognition and Machine Learning</part-title>
        <year>2006</year>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib64">
      <label>64</label>
      <element-citation publication-type="journal" id="sref64">
        <person-group person-group-type="author">
          <name>
            <surname>Tuv</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Borisov</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Runger</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Torkkola</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Feature selection with ensembles, artificial variables, and redundancy elimination</article-title>
        <source>J. Mach. Learn. Res.</source>
        <volume>10</volume>
        <year>2009</year>
        <fpage>1341</fpage>
        <lpage>1366</lpage>
      </element-citation>
    </ref>
    <ref id="bib65">
      <label>65</label>
      <element-citation publication-type="journal" id="sref65">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Random forests</article-title>
        <source>Mach. Learn.</source>
        <volume>45</volume>
        <year>2001</year>
        <fpage>5</fpage>
        <lpage>32</lpage>
      </element-citation>
    </ref>
    <ref id="bib66">
      <label>66</label>
      <element-citation publication-type="book" id="sref66">
        <person-group person-group-type="author">
          <name>
            <surname>Aho</surname>
            <given-names>A.V.</given-names>
          </name>
          <name>
            <surname>Ullman</surname>
            <given-names>J.D.</given-names>
          </name>
        </person-group>
        <part-title>The running time of programs</part-title>
        <source>Foundations of Computer Science</source>
        <year>1994</year>
        <publisher-name>Computer Science Press</publisher-name>
        <fpage>89</fpage>
        <lpage>155</lpage>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="appsec2" sec-type="supplementary-material">
    <title>Supplemental information</title>
    <p id="p0590">
      <supplementary-material content-type="local-data" id="mmc1">
        <caption>
          <title>Document S1. MATLAB source code for the RRCT algorithm</title>
        </caption>
        <media xlink:href="mmc1.pdf"/>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="mmc2">
        <caption>
          <title>Document S2. Article plus supplemental information</title>
        </caption>
        <media xlink:href="mmc2.pdf"/>
      </supplementary-material>
    </p>
  </sec>
  <sec sec-type="data-availability" id="da0010">
    <title>Data and code availability</title>
    <p id="p0030">All datasets used in the study are freely and publicly available. Most are available from the UCI ML Repository (<ext-link ext-link-type="uri" xlink:href="https://archive.ics.uci.edu/ml/index.php" id="intref0010">https://archive.ics.uci.edu/ml/index.php</ext-link>). Additionally, the ovarian-cancer dataset is freely available from <ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/10/259/additional" id="intref0015">http://www.biomedcentral.com/1471-2105/10/259/additional</ext-link>, and the SCRBCT dataset is available from <ext-link ext-link-type="uri" xlink:href="http://www-stat.stanford.edu/%7Etibs/ElemStatLearn/" id="intref0020">http://www-stat.stanford.edu/∼tibs/ElemStatLearn/</ext-link>. The Relathe dataset is freely available from <ext-link ext-link-type="uri" xlink:href="https://jundongl.github.io/scikit-feature/datasets.html" id="intref0025">https://jundongl.github.io/scikit-feature/datasets.html</ext-link>. We have provided specific links to where each of the datasets can be downloaded when describing the data (see <xref rid="tbl4" ref-type="table">Table 4</xref>). The synthetic data were generated by adjusting publicly available data generators and are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/ThanasisTsanas/RRCT/tree/main/Data" id="intref0030">https://github.com/ThanasisTsanas/RRCT/tree/main/Data</ext-link>. For convenience and easier reference, all synthetic and real-world datasets used in this study are included in that Github link in MATLAB (∗.mat) file format and Excel (∗.xlsx) file format, the latter being a more generic and accessible filetype that can be used across programming languages.</p>
    <p id="p0035">The MATLAB code developed and used in this study is freely available on the author’s group website under <ext-link ext-link-type="uri" xlink:href="https://www.darth-group.com/software" id="intref0035">https://www.darth-group.com/software</ext-link> and the author’s Github project page (<ext-link ext-link-type="uri" xlink:href="https://github.com/ThanasisTsanas/RRCT" id="intref0040">https://github.com/ThanasisTsanas/RRCT</ext-link>). Furthermore, RRCT has been implemented in Python (currently slower than the MATLAB version) and is available at the aforementioned Github link. The intention is to keep the code in the Github link up to date; for the frozen version, which was used to generate the results in this study using the MATLAB code, see the Zenodo (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6139462" id="intref0045">https://doi.org/10.5281/zenodo.6139462</ext-link>).</p>
  </sec>
  <ack id="ack0010">
    <title>Acknowledgments</title>
    <p id="p0575">A.T. is grateful to Drs. Max Little (MIT) and Patrick McSharry (University of Oxford) for early discussions on feature selection and a preliminary draft many years ago. This work was supported by the <funding-source id="gs1">Health Data Research UK</funding-source>, which receives its funding from HDR UK, Ltd. (HDR-5012), funded by the <funding-source id="gs2">UK Medical Research Council</funding-source>, <funding-source id="gs3"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source>, <funding-source id="gs4"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100000269</institution-id><institution>Economic and Social Research Council</institution></institution-wrap></funding-source>, <funding-source id="gs5"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100000276</institution-id><institution>Department of Health &amp; Social Care</institution></institution-wrap></funding-source> (UK), <funding-source id="gs6">Chief Scientist Office of the Scottish Government Health and Social Care Directorates</funding-source>, <funding-source id="gs7"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100010756</institution-id><institution>Health and Social Care Research and Development Division</institution></institution-wrap></funding-source> (Welsh government), <funding-source id="gs8"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100001626</institution-id><institution>Public Health Agency</institution></institution-wrap></funding-source> (Northern Ireland), <funding-source id="gs9"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100000274</institution-id><institution>British Heart Foundation</institution></institution-wrap></funding-source>, and the <funding-source id="gs10"><institution-wrap><institution-id institution-id-type="doi">10.13039/100010269</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source>.</p>
    <sec sec-type="COI-statement" id="sec5">
      <title>Declaration of interests</title>
      <p id="p0580">The author declares no competing interests.</p>
    </sec>
  </ack>
  <fn-group>
    <fn id="appsec1" fn-type="supplementary-material">
      <p id="p0585">Supplemental information can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.patter.2022.100471" id="intref0115">https://doi.org/10.1016/j.patter.2022.100471</ext-link>.</p>
    </fn>
  </fn-group>
</back>
