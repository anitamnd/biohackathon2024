<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">NAR Genom Bioinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">NAR Genom Bioinform</journal-id>
    <journal-id journal-id-type="publisher-id">nargab</journal-id>
    <journal-title-group>
      <journal-title>NAR Genomics and Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2631-9268</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10481254</article-id>
    <article-id pub-id-type="doi">10.1093/nargab/lqad063</article-id>
    <article-id pub-id-type="publisher-id">lqad063</article-id>
    <article-categories>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00030</subject>
        <subject>AcademicSubjects/SCI00980</subject>
        <subject>AcademicSubjects/SCI01060</subject>
        <subject>AcademicSubjects/SCI01140</subject>
        <subject>AcademicSubjects/SCI01180</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Standard Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SUPREME: multiomics data integration using graph convolutional networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-8592-4365</contrib-id>
        <name>
          <surname>Kesimoglu</surname>
          <given-names>Ziynet Nesibe</given-names>
        </name>
        <aff><institution>Department of Computer Science and Engineering, University of North Texas</institution>, Denton, TX, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-4813-4310</contrib-id>
        <name>
          <surname>Bozdag</surname>
          <given-names>Serdar</given-names>
        </name>
        <!--Serdar.Bozdag@unt.edu-->
        <aff><institution>Department of Computer Science and Engineering, University of North Texas</institution>, Denton, TX, <country country="US">USA</country></aff>
        <aff><institution>Department of Mathematics, University of North Texas</institution>, Denton, TX, <country country="US">USA</country></aff>
        <aff><institution>BioDiscovery Institute, University of North Texas</institution>, Denton, TX, <country country="US">USA</country></aff>
        <xref rid="COR1" ref-type="corresp"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="COR1">To whom correspondence should be addressed. Tel: +1 940 369 7581; Email: <email>Serdar.Bozdag@unt.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>6</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-06-28">
      <day>28</day>
      <month>6</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>28</day>
      <month>6</month>
      <year>2023</year>
    </pub-date>
    <volume>5</volume>
    <issue>2</issue>
    <elocation-id>lqad063</elocation-id>
    <history>
      <date date-type="received">
        <day>19</day>
        <month>1</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>08</day>
        <month>5</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>07</day>
        <month>6</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023. Published by Oxford University Press on behalf of NAR Genomics and Bioinformatics.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact <email>journals.permissions@oup.com</email></license-p>
      </license>
    </permissions>
    <self-uri xlink:href="lqad063.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>To pave the road towards precision medicine in cancer, patients with similar biology ought to be grouped into same cancer subtypes. Utilizing high-dimensional multiomics datasets, integrative approaches have been developed to uncover cancer subtypes. Recently, Graph Neural Networks have been discovered to learn node embeddings utilizing node features and associations on graph-structured data. Some integrative prediction tools have been developed leveraging these advances on multiple networks with some limitations. Addressing these limitations, we developed SUPREME, a node classification framework, which integrates multiple data modalities on graph-structured data. On breast cancer subtyping, unlike existing tools, SUPREME generates patient embeddings from multiple similarity networks utilizing multiomics features and integrates them with raw features to capture complementary signals. On breast cancer subtype prediction tasks from three datasets, SUPREME outperformed other tools. SUPREME-inferred subtypes had significant survival differences, mostly having more significance than ground truth, and outperformed nine other approaches. These results suggest that with proper multiomics data utilization, SUPREME could demystify undiscovered characteristics in cancer subtypes that cause significant survival differences and could improve ground truth label, which depends mainly on one datatype. In addition, to show model-agnostic property of SUPREME, we applied it to two additional datasets and had a clear outperformance.</p>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institute of General Medical Sciences</institution>
            <institution-id institution-id-type="DOI">10.13039/100000057</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>R35GM133657</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>University of North Texas</institution>
            <institution-id institution-id-type="DOI">10.13039/100008973</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="12"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="SEC1">
    <title>INTRODUCTION</title>
    <p>Cancer is one of the deadliest diseases for which cancer-causing agents such as oncogenes, mutations, and gene regulatory associations have not been fully demystified. Cancer patients show different characteristics in terms of the progression of disease and response to treatment (<xref rid="B1" ref-type="bibr">1</xref>). Various biological datasets from cancer tissues have been generated to better characterize cancer biology. For instance, The Cancer Genome Atlas (TCGA) project generated over 2.5 petabytes of multiple omics (multiomics) data for thousands of patients from 33 different cancer types (data are available at <ext-link xlink:href="https://portal.gdc.cancer.gov/" ext-link-type="uri">https://portal.gdc.cancer.gov/</ext-link>). Specifically for breast cancer, the Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) has generated four types of multiomics data for thousands of breast tumor samples (<xref rid="B2" ref-type="bibr">2</xref>). Utilizing high-dimensional biological datasets in public databases, computational approaches have been developed to discover subtypes of various cancers (<xref rid="B3" ref-type="bibr">3–5</xref>). Several of the cancer subtype prediction studies rely only on one type of biological datatype (<xref rid="B4" ref-type="bibr">4</xref>,<xref rid="B6" ref-type="bibr">6</xref>,<xref rid="B7" ref-type="bibr">7</xref>). However, each of these datatypes captures a different part of the underlying biology, thus developing integrative computational methods has been an important research area in bioinformatics.</p>
    <p>Breast cancer is currently the most commonly-diagnosed cancer worldwide (<xref rid="B8" ref-type="bibr">8</xref>). Therapeutic groups in breast cancer (i.e., estrogen receptor-positive, progesterone receptor-positive, human epidermal growth factor receptor 2 (HER2) amplified group, and triple-negative breast cancer) mainly depend on three receptors. Even though these receptors are very impactful in determining the breast cancer subtypes, they are not solely sufficient to classify a patient. Some other studies showed that genomic and clinical features such as race, age, and some mutations are also important in breast cancer subtyping (<xref rid="B9" ref-type="bibr">9</xref>,<xref rid="B10" ref-type="bibr">10</xref>).</p>
    <p>Genomic datatypes are found informative for differentiating subgroups in breast cancer. In 2009, Parker <italic toggle="yes">et al.</italic> (<xref rid="B11" ref-type="bibr">11</xref>) found a clear difference in the expression of 50 genes for breast cancer and introduced breast cancer molecular subtypes, called <italic toggle="yes">PAM50 subtypes</italic>. In 2012, the TCGA group published a study analyzing breast cancer subgroups and their associations with single datatypes, obtaining subtype-specific patterns in each datatype (<xref rid="B12" ref-type="bibr">12</xref>) and supporting the importance of gene expression-based models such as PAM50 (<xref rid="B11" ref-type="bibr">11</xref>). Even though there are important signals from both clinical and genomic features to determine the subtype of a patient, relying on a single data modality is not sufficient to differentiate subtypes clearly. As we get more samples and datatypes to analyze, it is important to integrate all the available datatypes properly with advanced approaches to understand differences in the characteristics of cancer patients.</p>
    <p>Recently several groups have developed unsupervised computational tools to integrate multiple datatypes to discover cancer subtypes. For instance, iClusterPlus (<xref rid="B13" ref-type="bibr">13</xref>) uses a joint latent variable model concatenating multiple datatypes with dimension reduction to cluster cancer patients. Similarity Network Fusion (SNF) (<xref rid="B14" ref-type="bibr">14</xref>) builds a patient similarity network based on each datatype, obtains a fused patient network by applying a nonlinear fusion step, and performs the clustering on that final network. PINSPlus (<xref rid="B15" ref-type="bibr">15</xref>) assumes that samples that are truly in the same subtype are clustered together despite small changes in the data. PINSPlus discovers the subtypes if the samples are highly connected for different datatypes applying data perturbation. The authors demonstrated that PINSPlus had robust results with significant survival differences across different cancer types. Those studies focus on unsupervised multiomics data integration without the utilization of found subtype labels such as PAM50 subtype labels. Furthermore, these tools utilize patient similarity networks or features, but not both simultaneously, while there are recent improvements in graph representation learning allowing the utilization of both at the same time (<xref rid="B16" ref-type="bibr">16–18</xref>).</p>
    <p>Graphs (networks) are suitable data structures to store multiomics datasets, however, machine learning (ML)-based approaches are challenging on graph data. Deep learning-based architectures have been used extensively for grid-like data (e.g., image), however, these methods are not directly applicable to graph data. Graphs are unstructured as each node has a varying number of neighbors and there is no fixed ordering of nodes. To train ML models on graph data, <italic toggle="yes">embedding</italic> (a fixed low-dimensional vector) is used and some shallow embedding methods emerged by encoding every node into embedding, representing the position and the local relationships in the graph (<xref rid="B19" ref-type="bibr">19–21</xref>). However, these shallow embedding methods are not scalable for large graphs and cannot utilize the node features that we have plenty of, thus, these methods have been replaced with more advanced deep learning-based methods such as Graph Neural Networks (GNNs) (<xref rid="B16" ref-type="bibr">16</xref>,<xref rid="B17" ref-type="bibr">17</xref>). The main difference in GNN-based architectures is how the features are aggregated from the local structure. Graph Convolutional Network (GCN) is one of the most popular GNNs that uses a modified aggregation involving self edges with normalization across neighbors (<xref rid="B18" ref-type="bibr">18</xref>). GNNs have recently been applied to biological problems such as cancer type/subtype prediction and drug response prediction (<xref rid="B18" ref-type="bibr">18</xref>, <xref rid="B22" ref-type="bibr">22–25</xref>).</p>
    <p>Even though there are some studies applying convolution to graph-structured data for cancer subtyping, these models are mostly applicable to a single network or had some limitations for integrative approaches. In (<xref rid="B26" ref-type="bibr">26</xref>), cancer type prediction of patients from 33 cancer and non-cancer types (i.e., all normal samples from all 33 available cancer types) was performed using GCNs. The input network was based on gene coexpression or protein-protein interaction, but the convolution was done on the gene expression dataset only, thus, missing the information of multiple data modalities. Multiomics GCN (MOGONET) is a supervised multiomics integration framework using GCNs with a patient similarity network for mRNA expression, DNA methylation, and microRNA expression separately (<xref rid="B27" ref-type="bibr">27</xref>). MOGONET gets the label independently from three different models, then uses them to get the final prediction. However, it does not consider multiple features for networks. We call this kind of embedding <italic toggle="yes">datatype-specific patient embedding</italic> where the methodology generates datatype-specific networks with datatype-specific node features and considers only the prediction labels from separate GCN models. However, these embeddings could be improved by utilizing all the multiomics patient features on each local network structure, making the embedding <italic toggle="yes">network-specific patient embedding</italic>. Moreover, it is possible to utilize GCN not only to get the prediction label but also to obtain the embeddings and integrate them. Going further, we can also integrate the patient features (called <italic toggle="yes">raw features</italic>) with embeddings to capture any diluted signals from features. To utilize more from available knowledge, it is important to properly integrate multiple network representations and multiomics features simultaneously.</p>
    <p>To address the aforementioned limitations, we developed a computational tool named SUPREME integrating multiple types of datasets using GCNs. SUPREME generated similarity networks using features from multi-modal datasets where node features include features from all data modalities, assuming that nodes with a similar local neighborhood are likely to belong to the same class. SUPREME encodes the relations on a network from each datatype and obtains network-specific node embeddings incorporating node features on each network. Then SUPREME integrates these embeddings providing extensive evaluations of all combinations of node embeddings. For each combination, SUPREME integrates the selected embeddings with raw features to utilize all the knowledge at the same time. SUPREME utilizes all available datatypes from public datasets and can interpret each datatype’s effectiveness in terms of features and networks. Being model-agnostic, SUPREME could be easily adapted to any model, any prediction task handling any number of datatypes, and could be easily modified by changing the embedding integration method, network generation strategy, and feature extraction approach.</p>
    <p>In this study, SUPREME was applied to three different prediction tasks from five different datasets. We applied SUPREME to predict subtypes of breast cancer patients using multiomics datasets (from TCGA and METABRIC datasets separately and together). Our results on cancer subtype prediction tasks showed that SUPREME outperformed other integrative supervised cancer (sub)type prediction tools and baseline methods. SUPREME had improved performance showing the importance of GCN-based approaches, network-specific patient embeddings, and raw feature integration. SUPREME was robust showing high and consistent prediction performance. We observed that the gene expression (EXP)-based features were the most significant features, as expected for breast cancer. Importantly, SUPREME-inferred cancer subtypes had consistently significant survival differences and were mostly more significant than the survival differences between ground truth subtypes, which were based on gene expression datatype. These results suggest that SUPREME can differentiate the characteristics of cancer subtypes properly utilizing the multiple network relations and multiple datatypes. To demonstrate the model-agnostic property of our tool, we also applied SUPREME to ACM and IMDB datasets and SUPREME outperformed other methods on both datasets.</p>
  </sec>
  <sec sec-type="materials|methods" id="SEC2">
    <title>MATERIALS AND METHODS</title>
    <p>SUPREME is a computational tool for node classification tasks integrating multiple data modalities using GCNs. Briefly, the first step is data preparation. In the second step, SUPREME extracts features from each datatype. Using those features, SUPREME generates individual similarity networks per datatype where features from all datatypes are used as node attributes. In the third step, using the obtained networks and features, SUPREME generates the network-specific node embeddings by running GCN on each network. In the last step, SUPREME does prediction by integrating individual network-specific embeddings and raw features. In the following part, we explain each step of SUPREME in detail.</p>
    <sec id="SEC2-1">
      <title>Data preparation</title>
      <p>We applied SUPREME on three datasets for the breast cancer subtype prediction task. We collected the data and generated seven datatypes (i.e., clinical, copy number aberration, coexpression, gene expression, DNA methylation, microRNA expression, and mutation) across 1022 breast tumor samples from TCGA (<xref rid="B12" ref-type="bibr">12</xref>), five datatypes (i.e., clinical, copy number aberration, coexpression, gene expression, and mutation) across 1699 breast tumor samples from METABRIC (<xref rid="B2" ref-type="bibr">2</xref>) and three datatypes (clinical, gene expression, and mutation) across a total of 2721 breast tumor samples from the combined datasets of TCGA and METABRIC. As ground truth for the prediction task, we obtained the PAM50 subtype labels, namely Basal-like, HER2-Enriched, Luminal-A, Luminal-B, and Normal-like (<xref rid="B11" ref-type="bibr">11</xref>). Data preprocessing details are in <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods 1.1</xref>.</p>
      <p>We also collected ACM and IMDB datasets for two additional tasks: movie genre prediction from IMDB dataset (<ext-link xlink:href="https://www.imdb.com" ext-link-type="uri">https://www.imdb.com</ext-link>) and paper area prediction task from ACM dataset (<ext-link xlink:href="http://dl.acm.org" ext-link-type="uri">http://dl.acm.org</ext-link>). IMDB dataset has a heterogeneous network with three node types (movie, actor, and director) along with two associations: movie-actor and movie-director. The movies have three genre classes: action, comedy, and drama. ACM dataset has also three node types (paper, author, and subject) on a heterogeneous network along with two associations: paper-author and paper-subject. The papers have three classes: database, wireless communication, and data mining.</p>
      <p>The number of features and samples for each dataset are shown in Table <xref rid="tbl1" ref-type="table">1</xref>.</p>
      <table-wrap position="float" id="tbl1">
        <label>Table 1.</label>
        <caption>
          <p>Number of features and samples for each dataset. Subtypes are abbreviated as BL: basal-like, HER2: HER2-Enriched, LA: luminal-A, LB: luminal-B, NL: normal-like</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Dataset</th>
              <th rowspan="1" colspan="1">Number of raw features</th>
              <th rowspan="1" colspan="1">Number of samples</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">TCGA</td>
              <td rowspan="1" colspan="1">3088</td>
              <td rowspan="1" colspan="1">1022 samples: 172 BL (17%), 78 HER2 (8%), 538 LA (53%), 195 LB (19%), 39 NL (4%)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">METABRIC</td>
              <td rowspan="1" colspan="1">1761</td>
              <td rowspan="1" colspan="1">1699 samples: 199 BL (12%), 220 HER2 (13%), 679 LA (40%), 461 LB (27%), 140 NL (8%)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Combined (TCGA+ METABRIC)</td>
              <td rowspan="1" colspan="1">1229</td>
              <td rowspan="1" colspan="1">2721 samples: 371 BL (14%), 298 HER2 (11%), 1217 LA (45%), 656 LB (24%), 179 NL (7%)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">IMDB</td>
              <td rowspan="1" colspan="1">3066</td>
              <td rowspan="1" colspan="1">4278 samples: 1135 (27%), 1584 (37%), 1559 (36%)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ACM</td>
              <td rowspan="1" colspan="1">1870</td>
              <td rowspan="1" colspan="1">3025 samples: 1061 (35%), 965 (32%), 999 (33%)</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="SEC2-2">
      <title>Feature extraction &amp; network generation</title>
      <sec id="SEC2-2-1">
        <title>Breast cancer subtyping</title>
        <p>SUPREME incorporates seven datatypes for TCGA data, five datatypes for METABRIC data, and three datatypes for the combined data. We utilized a Random Forest-based feature selection algorithm, called Boruta (<xref rid="B28" ref-type="bibr">28</xref>), to extract features from high dimensional datatypes. The selected features in the data preprocessing step (i.e., multiomics features) were used to compute the similarity between patients when generating the patient similarity networks, as node features in the patient similarity networks, and to integrate as raw features before the prediction task. To compute patient similarities in datatype-specific patient similarity networks, we used Pearson correlation for gene expression, copy number aberration, DNA methylation, microRNA expression, and coexpression datatypes; the Gower metric (<xref rid="B29" ref-type="bibr">29</xref>) from the daisy function of cluster R package (<xref rid="B30" ref-type="bibr">30</xref>) for clinical features; and Jaccard distance for binary mutation features. After selecting the top edges, the edge weights were eliminated to generate an unweighted network. We used 2500 edges for the datatypes of TCGA, 4500 for METABRIC and 7000 for the combined data (having approximately 2.5 times the sample size). Details of feature extraction and network generation are in <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods 1.2</xref>.</p>
      </sec>
      <sec id="SEC2-2-2">
        <title>Movie genre prediction</title>
        <p>We did not apply any feature selection for the IMDB dataset and used node features processed in (<xref rid="B31" ref-type="bibr">31</xref>). Using two associations (i.e., movie-actor and movie-director) in the data, two movie similarity networks were generated based on two meta-paths using (<xref rid="B32" ref-type="bibr">32</xref>): movie-director-movie with 17 446 edges and movie-actor-movie with 85 358 edges. Meta-path-based similarity networks connect nodes based on a given association. For instance, the meta-path movie-actor-movie defines similarity as the existence of at least one common actor between two movies.</p>
      </sec>
      <sec id="SEC2-2-3">
        <title>Paper area prediction</title>
        <p>For the ACM dataset, we did not apply any feature selection and used the node features processed in (<xref rid="B31" ref-type="bibr">31</xref>). Utilizing two associations (i.e., paper-author and paper-subject) in the data, two meta-paths were used to generate two paper similarity networks using (<xref rid="B33" ref-type="bibr">33</xref>): paper-author-paper with 29 281 edges and paper-subject-paper with 2 210 761 edges. The meta-path-based similarity definition is the same as in the IMDB dataset.</p>
        <p>When there is a high number of raw features and many networks to integrate, this might affect the prediction performance, and model training could be time-consuming. Thus, we added another optional feature selection step to further reduce the number of raw features integrated with the node embeddings for the prediction task. We enabled this additional feature selection for TCGA data where we had a high number of raw features and networks and observed that it reduced running time without affecting the prediction performance. We did not apply optional feature selection for ACM and IMDB datasets since we have only two networks. Similarly, we did not apply any reduction for the number of edges for these datasets since we do not have any quantitative similarities to prioritize the edges on meta-path-based similarity networks.</p>
      </sec>
    </sec>
    <sec id="SEC2-3">
      <title>Node embedding generation</title>
      <p>After extracting features and generating networks, we obtained network-specific node embeddings, which capture the topology of the network as well as node features to be utilized in a downstream ML task.</p>
      <p>In this study, we used the GCN model of Kipf and Welling (<xref rid="B18" ref-type="bibr">18</xref>) involving self edges in convolution and scaling the sum of aggregated features across the neighbors. GCN models learn the data by performing convolution on networks, considering one-hop local neighbors with equal contribution, and encoding the local topology of the network. Stacked layers involve recursive neighborhood diffusion considering more than a one-hop neighborhood.</p>
      <p>Let’s call an undirected graph as <inline-formula><tex-math id="M0001" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {G}= (\mathcal {V},\mathcal {E})$\end{document}</tex-math></inline-formula> where <inline-formula><tex-math id="M0001a" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {V}$\end{document}</tex-math></inline-formula> is a set of <italic toggle="yes">n</italic> nodes, i.e., <inline-formula><tex-math id="M0002" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {V} = \lbrace v_1, v_2, ..., v_n\rbrace$\end{document}</tex-math></inline-formula>, and <inline-formula><tex-math id="M0003" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {E}$\end{document}</tex-math></inline-formula> is a set of edges between nodes where <inline-formula><tex-math id="M0004" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$(v_i,v_j) \in \mathcal {E}$\end{document}</tex-math></inline-formula> when <inline-formula><tex-math id="M0005" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_i \in \mathcal {V}$\end{document}</tex-math></inline-formula>, <inline-formula><tex-math id="M0006" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_j \in \mathcal {V}$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M0007" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_i$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M0008" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_j$\end{document}</tex-math></inline-formula> have an association based on the graph <inline-formula><tex-math id="M0009" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {G}$\end{document}</tex-math></inline-formula>. Since the graph <inline-formula><tex-math id="M00010" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {G}$\end{document}</tex-math></inline-formula> is undirected, <inline-formula><tex-math id="M00011" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$(v_i,v_j) \in \mathcal {E} \iff (v_j,v_i) \in \mathcal {E}$\end{document}</tex-math></inline-formula>.</p>
      <p>The input for a GCN model is a feature matrix <inline-formula><tex-math id="M00012" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {X} \in \mathbb {R}^{nxk}$\end{document}</tex-math></inline-formula> where <italic toggle="yes">k</italic> is the feature size, and the adjacency matrix <inline-formula><tex-math id="M00013" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {A} \in \mathbb {R}^{nxn}$\end{document}</tex-math></inline-formula> with added self edges defined as:</p>
      <disp-formula>
        <tex-math id="M00014" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{equation*} \mathcal {A}[i,j]=\left\lbrace \begin{array}{lr}1 &amp; \text{ if }\left(v_{i}, v_{j}\right) \in \mathcal {E} \text{ or } i=j \\ 0 &amp; \text{ otherwise } \end{array}\right. \end{equation*}\end{document}</tex-math>
      </disp-formula>
      <p>The iteration process is defined as:</p>
      <disp-formula>
        <tex-math id="M00015" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{equation*} \mathcal {H}^{(l+1)}=\sigma \left(\mathcal {D}^{-\frac{1}{2}} \mathcal {A} \mathcal {D}^{-\frac{1}{2}} \mathcal {H}^{(l)} \mathcal {W}^{(l)}\right) \end{equation*}\end{document}</tex-math>
      </disp-formula>
      <p>with <inline-formula><tex-math id="M00016" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {H}^{(0)} = \mathcal {X}$\end{document}</tex-math></inline-formula> where</p>
      <disp-formula>
        <tex-math id="M00017" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{equation*} \mathcal {D}[i,i]=\sum _{j=1}^{n} \mathcal {A}[i,j], \end{equation*}\end{document}</tex-math>
      </disp-formula>
      <p><inline-formula><tex-math id="M00018" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {H}^{(l)}$\end{document}</tex-math></inline-formula> is the activation matrix in the <italic toggle="yes">l</italic>th layer, <inline-formula><tex-math id="M00019" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {W}^{(l)}$\end{document}</tex-math></inline-formula> is the trainable weight matrix in the <italic toggle="yes">l</italic>th layer and σ is the activation function.</p>
      <p>Considering breast cancer subtyping task using TCGA data, SUPREME setup for the single model generation was as follows: there were seven networks (i.e., patient similarity networks), each obtained from a different datatype. All networks had nodes as breast cancer patients and edges based on the patient similarities from the corresponding data. For instance, let us consider <inline-formula><tex-math id="M00020" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {G}$\end{document}</tex-math></inline-formula> as a gene expression-derived patient similarity network. This network connects patient nodes with a high correlation between their gene expression profile. As node features, <inline-formula><tex-math id="M00021" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {G}$\end{document}</tex-math></inline-formula> has the combined features, which were extracted from all the seven datatypes. Features of <inline-formula><tex-math id="M00022" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_i$\end{document}</tex-math></inline-formula> are denoted as <inline-formula><tex-math id="M00023" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$x_i \in \mathbb {R}^k$\end{document}</tex-math></inline-formula> where <italic toggle="yes">k</italic> is the total feature size. So, the stacked feature matrix <inline-formula><tex-math id="M00024" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {X} \in \mathbb {R}^{nxk}$\end{document}</tex-math></inline-formula> is:</p>
      <disp-formula>
        <tex-math id="M00025" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{equation*} \mathcal {X}=\begin{bmatrix}x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix} \end{equation*}\end{document}</tex-math>
      </disp-formula>
      <p>The local one-hop neighborhood of a node <inline-formula><tex-math id="M00026" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_i$\end{document}</tex-math></inline-formula> is <inline-formula><tex-math id="M00027" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {N}_{i}=\left\lbrace v_{j}:\left(v_{i}, v_{j}\right) \in \mathcal {E}\right\rbrace$\end{document}</tex-math></inline-formula> that included the set of nodes having an association with the node <inline-formula><tex-math id="M00028" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_i$\end{document}</tex-math></inline-formula>. Feature aggregation on the local neighborhood of each node was done by multiplying <inline-formula><tex-math id="M00029" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {X}$\end{document}</tex-math></inline-formula> by the <italic toggle="yes">nxn</italic>-sized scaled adjacency matrix <inline-formula><tex-math id="M00030" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {A}^{^{\prime }}$\end{document}</tex-math></inline-formula> where</p>
      <disp-formula>
        <tex-math id="M00031" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{equation*} \mathcal {A}^{\prime }=\mathcal {D}^{-\frac{1}{2}} \mathcal {A} \mathcal {D}^{-\frac{1}{2}}. \end{equation*}\end{document}</tex-math>
      </disp-formula>
      <p>Using 2-layered GCN in SUPREME, we had the form of the forward model giving the output <inline-formula><tex-math id="M00032" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {Z}$\end{document}</tex-math></inline-formula> where</p>
      <disp-formula>
        <tex-math id="M00033" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{equation*} \mathcal {Z}=\operatorname{softmax}\left(\mathcal {A}^{\prime } \operatorname{ReLU}\left(\mathcal {A}^{\prime } \mathcal {X} \mathcal {W}^{(1)}\right) \mathcal {W}^{(2)}\right) \end{equation*}\end{document}</tex-math>
      </disp-formula>
      <p>and <inline-formula><tex-math id="M00034" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {W}^{(1)} \in \mathbb {R}^{kxh}$\end{document}</tex-math></inline-formula>, <inline-formula><tex-math id="M00035" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {W}^{(2)} \in \mathbb {R}^{hxc}$\end{document}</tex-math></inline-formula> were the trainable weights for the first and second layers, respectively, where <italic toggle="yes">h</italic> was the hidden layer size and <italic toggle="yes">c</italic> was the number of classes to predict (namely, Basal-like, Luminal-A, Luminal-B, HER2-Enriched, and Normal-like, with <italic toggle="yes">c</italic> = 5). The loss function was calculated by cross-entropy error. Adam optimization (<xref rid="B34" ref-type="bibr">34</xref>) was used as the state-of-the-art for stochastic gradient descent algorithm and dropout was added for the first GCN layer. Early stopping was used with the patience of 30 forced to have at least 200 epochs.</p>
      <p>We split the total samples into training, validation, and test sets. This splitting was stratified, that is, keeping the same ratio of the subtype labels in the original data for each split. We kept the test set only for final evaluation of the tool. Training and validation splits are randomly selected for each run as stratified. For the breast cancer subtyping, we split 20% of the total samples as a test set. The remaining 80% of the samples were used for training (60%) and validation (20%). For IMDB and ACM datasets, we used the same data splits in (<xref rid="B31" ref-type="bibr">31</xref>). To tune the hyperparameters of the GCN model (i.e., hidden layer size and learning rate), for each run, SUPREME repeated an evaluation metric (i.e., macro-averaged F1 (macro F1) score) 10 times for each hyperparameter combination (<xref rid="sup1" ref-type="supplementary-material">Supplemental File 2</xref>) and selected the hyperparameter combination giving the best median macro F1 score on the validation data to generate the final model.</p>
      <p>Similarly applying the methodology for other datatypes, we generated seven different GCN models on TCGA data. Repeating the same procedure for other datasets, we obtained five models on METABRIC data, three models on the combined data, and two models on ACM and IMDB data. These final models were used to extract network-specific patient embeddings to use in the downstream prediction task.</p>
    </sec>
    <sec id="SEC2-4">
      <title>Training predictive models using node embedding combinations</title>
      <p>For each combination of node embeddings from <inline-formula><tex-math id="M00036" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$d$\end{document}</tex-math></inline-formula> datatypes, we concatenated them with the raw features and trained prediction models (having <inline-formula><tex-math id="M00037" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$2^{d}-1$\end{document}</tex-math></inline-formula> models). Specifically, we had 127, 31, seven, three, and three SUPREME models for TCGA, METABRIC, the combined data (TCGA+METABRIC), ACM, and IMDB datasets, respectively.</p>
      <p>We tested SUPREME with several ML methods namely, XGBoost, Support Vector Machine (SVM), Random Forest (RF), and Multi-layer Perceptron (MLP). For all datasets, we decided to use MLP as it gave consistently high performance (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref> and discussion section for the details).</p>
      <p>We did hyperparameter tuning for the prediction task, similar to GCN hyperparameter tuning in the previous step. We used the training and validation cohort to tune the hyperparameters (e.g., hidden layer size and learning rate) of the final model, where training and validation splits were randomly selected as stratified. We repeated the SUPREME run 10 times for each hyperparameter combination and used the hyperparameter combination giving the best median macro F1 score on the validation data. Using this hyperparameter combination, the final model was built and evaluated 10 times on the test data, which was never seen during training and hyperparameter tuning. The evaluation metrics (macro F1, weighted-average F1 (weighted F1) score, and accuracy) were obtained from the median of these 10 runs.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="SEC3">
    <title>RESULTS</title>
    <p>We introduced a novel node classification framework, called SUPREME, that utilizes graph convolutions on multiple datatype-specific networks that are annotated with multi-modal datatypes as node features. This framework is model-agnostic and could be applied to any classification problem with properly processed datatypes and networks. In this work, SUPREME was applied specifically to the breast cancer subtype prediction problem by applying convolution on patient similarity networks constructed based on multiple biological datatypes from breast tumor samples (Figure <xref rid="F1" ref-type="fig">1</xref>). We also evaluated SUPREME on ACM and IMDB datasets demonstrating the outperformance of SUPREME in different domains.</p>
    <fig position="float" id="F1">
      <label>Figure 1.</label>
      <caption>
        <p>SUPREME pipeline for breast cancer subtype prediction. SUPREME extracts feature from available datatypes and generates patient similarity networks where nodes are annotated with features from all datatypes. Utilizing graph convolutions on each patient similarity network, patient embeddings are generated. To provide extensive evaluations of subtype prediction, a machine learning model is trained for each combination of patient embeddings and raw multiomics features. [<italic toggle="yes">u</italic><sub><italic toggle="yes">k</italic></sub> is <italic toggle="yes">k</italic>th patient, <italic toggle="yes">i</italic><sup>(<italic toggle="yes">j</italic>)</sup> is a raw feature matrix for the <italic toggle="yes">j</italic>th datatype where each row is <inline-formula><tex-math id="M00038" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$i_k^{(j)}$\end{document}</tex-math></inline-formula> corresponding to the feature vector of the <italic toggle="yes">k</italic>th patient for the <italic toggle="yes">j</italic>th datatype. Similarly, <italic toggle="yes">z</italic><sup>(<italic toggle="yes">j</italic>)</sup> is a node embedding matrix for the <italic toggle="yes">j</italic>th datatype-specific network where each row is <inline-formula><tex-math id="M00039" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$z_k^{(j)}$\end{document}</tex-math></inline-formula> corresponding to the embedding of the <italic toggle="yes">k</italic><sup><italic toggle="yes">th</italic></sup> patient.]</p>
      </caption>
      <graphic xlink:href="lqad063fig1" position="float"/>
    </fig>
    <sec id="SEC3-1">
      <title>SUPREME outperformed the cancer subtype prediction tools and baseline methods</title>
      <p>For the breast cancer subtyping task, we compared the performance of SUPREME on three different datasets with seven other cancer (sub)type prediction tools and baseline methods, namely Deep cancer subtype classification (DeepCC) (<xref rid="B35" ref-type="bibr">35</xref>), GCN-based classification (GCNC) (<xref rid="B26" ref-type="bibr">26</xref>), MOGONET (<xref rid="B27" ref-type="bibr">27</xref>), MLP, RF, SVM, and XGBoost. For each dataset combination, SUPREME builds a separate cancer subtype prediction model. For ML-based baseline methods (i.e., MLP, RF, SVM, and XGBoost), we integrated only the raw features from the selected combination and did the prediction with those features. MOGONET utilizes GCN on multiomics data utilizing datatype-specific embedding predictions. GCNC leverages GCN with gene expression features on protein-protein interaction (PPI)- or coexpression-based gene network, while DeepCC utilizes only gene expression datatype with pathway activity transformation through an MLP model. Therefore, we had only two classification models for GCNC: <italic toggle="yes">GCNC</italic><sub><italic toggle="yes">PPI</italic></sub> with the PPI network and <italic toggle="yes">GCNC</italic><sub><italic toggle="yes">COE</italic></sub> with the coexpression network, and one model for DeepCC. To see the impact of the integration of raw features into the embeddings, we also trained models without integrating raw features with patient embeddings, called <italic toggle="yes">SUPREME-</italic>. We ran SUPREME, SUPREME-, and the other tools for all the combinations of available datatypes. Even though MOGONET is applicable to any number of datatypes, we could not run the tool for the models with more than five datatypes (waiting time was more than two days per combination), thus we had only 31 different models for TCGA data, while we had all models for METABRIC and the combined data.</p>
      <p>SUPREME and SUPREME- outperformed all other multiomics integration methods for three datasets in terms of macro F1, accuracy, and weighted F1 (Figure <xref rid="F2" ref-type="fig">2</xref>, Supplementary Figures S3 and S4). SUPREME significantly outperformed MLP, which utilizes raw features only in all datasets, showing the importance of GCN utilization. We observed that SUPREME significantly outperformed SUPREME- for all three datasets.</p>
      <fig position="float" id="F2">
        <label>Figure 2.</label>
        <caption>
          <p>Classification results. Violin plot of macro F1 scores obtained from 127 different models including all different combinations of datatypes as compared to the cancer subtype prediction tools and baseline supervised methods on TCGA data. DeepCC and GCNC violin plots show the distribution of macro F1 scores of ten runs of a single model as they can only utilize gene expression datatype. The significance level was measured with respect to SUPREME (Wilcoxon rank-sum test p-value to compare the distribution of violin plots representing the significance &lt; 0.001 by ***, else if &lt; 0.01 by **, and else if &lt; 0.05 by *). [MLP: Multi-layer Perceptron, RF: Random Forest, SUPREME-: SUPREME without raw feature integration, SVM: Support Vector Machine]</p>
        </caption>
        <graphic xlink:href="lqad063fig2" position="float"/>
      </fig>
      <p>We ran the tools that utilize only gene expression datatype and evaluated their performance (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>). For TCGA data, SUPREME achieved significantly higher performance than DeepCC and GCNC models, while performance on METABRIC and the combined data was comparable or superior (Figures <xref rid="F2" ref-type="fig">2</xref> and S3).</p>
      <p>In addition, we checked the subtype-specific F1 scores, and had consistent and higher performance across all subtypes, mostly having significant differences (Supplementary Figure S5). Specifically, on TCGA data, we had significantly better performance than all other tools for all subtypes in terms of subtype-specific F1 scores. Particularly, SUPREME had a significantly higher subtype-specific F1 score than all other tools on the Normal-like subtype for all three datasets. Considering that the Normal-like subtype had the smallest sample size in all three datasets (4% of the samples from TCGA, 8% from METABRIC, and 7% from the combined data), achieving this performance increase indicates SUPREME’s robustness even for minority classes.</p>
    </sec>
    <sec id="SEC3-2">
      <title>SUPREME had consistently high performance even with single models</title>
      <p>To see SUPREME’s performance with a single datatype, we investigated models generated with only one datatype, called <italic toggle="yes">single model</italic>. We compared SUPREME with an MLP-based model trained using a single datatype to show the impact of our GCN-based approach. To show the impact of different approaches with one datatype, we compared our single models against MOGONET and our EXP-based model with DeepCC and GCNC models. We conducted these experiments for all three datasets.</p>
      <p>Based on the single model results, SUPREME outperformed MOGONET for all single models from all three datasets (Table <xref rid="tbl2" ref-type="table">2</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S3–S5</xref>). Also, SUPREME outperformed MLP (six out of seven models for TCGA data, three out of five for METABRIC data, and two out of three models for the combined data), or had comparable performance, while MLP had extremely poor performance on some datatypes, showing the importance of GCN-based approach.</p>
      <table-wrap position="float" id="tbl2">
        <label>Table 2.</label>
        <caption>
          <p>Single model results on TCGA data. Macro F1 scores for each model with a single dataype. See Figure <xref rid="F1" ref-type="fig">1</xref> for the abbreviations of the datatypes. [MLP: multi-layer perceptron]</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Method</th>
              <th rowspan="1" colspan="1">CLI</th>
              <th rowspan="1" colspan="1">CNA</th>
              <th rowspan="1" colspan="1">COE</th>
              <th rowspan="1" colspan="1">EXP</th>
              <th rowspan="1" colspan="1">MET</th>
              <th rowspan="1" colspan="1">MIR</th>
              <th rowspan="1" colspan="1">MUT</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">SUPREME</td>
              <td rowspan="1" colspan="1">0.68 ± 0.04</td>
              <td rowspan="1" colspan="1">
                <bold>0.80 ± 0.03</bold>
              </td>
              <td rowspan="1" colspan="1">0.76 ± 0.04</td>
              <td rowspan="1" colspan="1">
                <bold>0.84 ± 0.02</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.79 ± 0.03</bold>
              </td>
              <td rowspan="1" colspan="1">0.73 ± 0.02</td>
              <td rowspan="1" colspan="1">
                <bold>0.75 ± 0.03</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SUPREME-</td>
              <td rowspan="1" colspan="1">
                <bold>0.72 ± 0.02</bold>
              </td>
              <td rowspan="1" colspan="1">0.77 ± 0.02</td>
              <td rowspan="1" colspan="1">
                <bold>0.77 ± 0.04</bold>
              </td>
              <td rowspan="1" colspan="1">0.77 ± 0.05</td>
              <td rowspan="1" colspan="1">
                <bold>0.79 ± 0.04</bold>
              </td>
              <td rowspan="1" colspan="1">0.70 ± 0.02</td>
              <td rowspan="1" colspan="1">0.74 ± 0.04</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MLP</td>
              <td rowspan="1" colspan="1">0.46 ± 0.07</td>
              <td rowspan="1" colspan="1">0.53 ± 0.04</td>
              <td rowspan="1" colspan="1">0.59 ± 0.02</td>
              <td rowspan="1" colspan="1">0.82 ± 0.03</td>
              <td rowspan="1" colspan="1">0.69 ± 0.04</td>
              <td rowspan="1" colspan="1">
                <bold>0.74 ± 0.04</bold>
              </td>
              <td rowspan="1" colspan="1">0.28 ± 0.06</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MOGONET</td>
              <td rowspan="1" colspan="1">0.41 ± 0.01</td>
              <td rowspan="1" colspan="1">0.52 ± 0.01</td>
              <td rowspan="1" colspan="1">0.57 ± 0.01</td>
              <td rowspan="1" colspan="1">0.75 ± 0.01</td>
              <td rowspan="1" colspan="1">0.61 ± 0.03</td>
              <td rowspan="1" colspan="1">0.71 ± 0.03</td>
              <td rowspan="1" colspan="1">0.34 ± 0.01</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>There was no clear winner for the comparison of the SUPREME EXP-based model with DeepCC and GCNC models. In terms of macro F1 score, SUPREME outperformed both methods on TCGA data and GCNC (1 draw, 1 win) on the combined data. (Table <xref rid="tbl2" ref-type="table">2</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S3–S5</xref>). This could be because DeepCC and GCNC utilize pathway activation, PPI network, or coexpression network in addition to gene expression datatype. Nonetheless, by utilizing more datatypes SUPREME outperformed or was on par with both tools for all datasets (Figure <xref rid="F2" ref-type="fig">2</xref>, Supplementary Figures S3 and S4).</p>
      <p>EXP-based models had the highest macro F1 score for all three datasets for all methods (Table <xref rid="tbl2" ref-type="table">2</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tabless S3, S4, and S5</xref>). The only exception is that SUPREME- MET-based model had slightly higher performance than SUPREME- EXP-based model on TCGA data. High performance of EXP-based models is not surprising as the breast cancer subtype labels are based on gene expression data. We observed that SUPREME usually outperformed SUPREME- on single models, which indicates that utilizing raw features usually improves the model performance. On the other hand, there were few cases where adding raw features dropped the performance (e.g., CLI-based models on TCGA data). By examining SUPREME- and MLP model performances, we compared the predictive power of patient embeddings with raw features. We observed that patient embedding features were more useful than raw features with few exceptions, such as microRNA expression- (MIR) and EXP-based models on TCGA data, copy number aberration (CNA)-based on METABRIC data, and CLI-based model on the combined data. Specifically on TCGA, we see that CLI-based embedding was more informative than CLI-based features. For CNA- and mutation (MUT)-based models, embeddings were more useful than raw features, but we observed that integrating raw features to embeddings further improved the performance. Similarly, although for the EXP-based model on TCGA data, embeddings were less informative than raw features, integrating them improved the performance.</p>
    </sec>
    <sec id="SEC3-3">
      <title>SUPREME had significant survival differences between predicted subtypes consistently</title>
      <p>To measure the ability of the supervised methods to differentiate samples based on survival, we predicted the subtype labels for each data modality combination and performed the survival analysis. In addition to the supervised methods, we also included the state-of-the-art unsupervised tools that are specifically applied to cancer subtyping (i.e., iClusterPlus (<xref rid="B13" ref-type="bibr">13</xref>), SNF (<xref rid="B14" ref-type="bibr">14</xref>), and PINSPlus (<xref rid="B15" ref-type="bibr">15</xref>)) and an algorithmically-relevant clustering method (i.e. affinity propagation (AP) clustering). AP clustering is relevant because it uses a message-passing strategy to find the cluster representatives and the best representative for each node. We obtained five clusters from the unsupervised methods to match the number of PAM50 subtypes and checked the survival differences for these obtained clusters. This analysis was only applied to the results on TCGA data where patient survival data were available. To check the statistical significance of survival differences between subtypes, we applied the log-rank test to compute p-values. Details of survival analysis are in <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods 1.3</xref>.</p>
      <p>The results showed that SUPREME’s predicted subtypes consistently had significant differences in survival rates and significantly outperformed all other nine methods in terms of the <italic toggle="yes">P</italic>-value (Figure <xref rid="F3" ref-type="fig">3</xref>). SUPREME had 0.0035 as the lowest <italic toggle="yes">P</italic>-value (when integrating CNA-, COE-, MET- and MUT-based patient embeddings) and 0.0131 as the median <italic toggle="yes">P</italic>-value (Supplementary Figure S6A for the Kaplan–Meier plot). Similarly for SUPREME-, we had 0.0018 as the lowest <italic toggle="yes">P</italic>-value (when integrating CNA- and COE-based patient embeddings), and 0.0147 as the median <italic toggle="yes">P</italic>-value. Interestingly, SUPREME had a more significant survival difference than the survival difference between ground truth (i.e., PAM50) labels (Supplementary Figure S6B for the Kaplan–Meier plot for PAM50 subtypes).</p>
      <fig position="float" id="F3">
        <label>Figure 3.</label>
        <caption>
          <p>Survival analysis results violin plot of the log-rank <italic toggle="yes">P</italic>-value obtained from survival analysis for the SUPREME models as compared to the cancer subtype prediction/clustering tools and baseline methods. Significance level was measured with respect to SUPREME (Wilcoxon rank-sum test <italic toggle="yes">P</italic>-value to compare the distribution of violin plots representing the significance &lt;0.001 by ***, else if &lt;0.01 by **, and else if &lt;0.05 by *). The continuous line shows the significance level of 0.05 and the dashed line shows the ground truth’s significance level. The below figure focuses on the significant survival <italic toggle="yes">P</italic>-values (&lt;0.05) [AP: affinity propagation, MLP: multi-layer perceptron, RF: random forest, SNF: similarity network fusion, SUPREME: SUPREME without raw feature integration, SVM: support vector machine].</p>
        </caption>
        <graphic xlink:href="lqad063fig3" position="float"/>
      </fig>
      <p>Specifically, 106 out of 127 SUPREME models had a lower <italic toggle="yes">P</italic>-value than the p-value for ground truth. For 57% of those models, we had CNA-based embedding selected. It is followed by 52% from COE-, CLI- and MET-based embeddings. This might suggest that those embeddings could contribute more to differentiating survival differences between subtypes.</p>
      <p>AP, iClusterPlus, MOGONET, and SNF methods had a wide range of <italic toggle="yes">P</italic>-values, while SUPREME, MLP, SVM, and XGBoost had mostly significant <italic toggle="yes">P</italic>-values (≤0.05) with a median lower than the significance level of the ground truth. SUPREME was better than SUPREME-, but the difference was not significant.</p>
      <p>Using support from the predicted subtypes by each model in SUPREME, we computed an ensembled consensus subtype based on majority voting for each patient (<xref rid="sup1" ref-type="supplementary-material">Supplemental File 3</xref>) and checked the survival difference between these consensus subtypes. Once again, we observed a significant (<italic toggle="yes">P</italic>-value = 0.01) survival difference between consensus subtypes (Supplementary Figure S6C). We also observed that 882 out of 1022 patients had the same subtype prediction across all 127 models showing the robustness of SUPREME predictions.</p>
    </sec>
    <sec id="SEC3-4">
      <title>Feature/omics importance analysis</title>
      <p>In this section, we investigated the importance of each network-specific embedding and datatype-specific features.</p>
      <sec id="SEC3-4-1">
        <title>Impact of network-specific patient embeddings</title>
        <p>We investigated the contribution of each patient embedding on the model performance by comparing the models built using a patient embedding from datatype <inline-formula><tex-math id="M00040" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}$\end{document}</tex-math></inline-formula> and without using that embedding. Among all <inline-formula><tex-math id="M00041" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$2^{d}-1$\end{document}</tex-math></inline-formula> models, <inline-formula><tex-math id="M00042" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$2^{d-1}$\end{document}</tex-math></inline-formula> models had the patient embedding obtained from a datatype <inline-formula><tex-math id="M00043" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}$\end{document}</tex-math></inline-formula>, called <inline-formula><tex-math id="M00044" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$with\mathbb {X}_n$\end{document}</tex-math></inline-formula>. The remaining <inline-formula><tex-math id="M00045" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$2^{d-1}-1$\end{document}</tex-math></inline-formula> models did not have the patient embedding obtained from <inline-formula><tex-math id="M00046" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}$\end{document}</tex-math></inline-formula>, called <inline-formula><tex-math id="M00047" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$no\mathbb {X}_n$\end{document}</tex-math></inline-formula>. For each datatype <inline-formula><tex-math id="M00048" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}$\end{document}</tex-math></inline-formula>, we compared <inline-formula><tex-math id="M00049" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$no\mathbb {X}_n$\end{document}</tex-math></inline-formula> models against <inline-formula><tex-math id="M00050" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$with\mathbb {X}_n$\end{document}</tex-math></inline-formula> models, showing the importance of <inline-formula><tex-math id="M00051" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}$\end{document}</tex-math></inline-formula>-specific patient embedding. We did this analysis on SUPREME- (i.e., without integrating the raw features) to ensure that differences were due to the patient embeddings only.</p>
        <p>The results on TCGA data showed that the performance of models increased or stayed the same with the inclusion of patient embeddings from all datatypes except for gene expression (Figure <xref rid="F4" ref-type="fig">4</xref>). The inclusion of EXP-based embedding showed a significant decrease in the model performance. The exclusion of CLI- and CNA-based patient embeddings had a significant drop in the model performance. Those findings agree with single model results.</p>
        <fig position="float" id="F4">
          <label>Figure 4.</label>
          <caption>
            <p>Analysis of network-specific patient embeddings. Violin plot of macro F1 scores of SUPREME- performance for the models integrated with a specific patient embedding from each datatype (<italic toggle="yes">with</italic><inline-formula><tex-math id="M00052" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}_n$\end{document}</tex-math></inline-formula> models, where <inline-formula><tex-math id="M00053" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}$\end{document}</tex-math></inline-formula> is the datatype whose embedding is included) versus excluding that embedding (<italic toggle="yes">no</italic><inline-formula><tex-math id="M00054" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}_n$\end{document}</tex-math></inline-formula> models) on TCGA data. Significance level was measured between <italic toggle="yes">with</italic> and <italic toggle="yes">no</italic> cases of the same datatype (Wilcoxon rank-sum test p-value to compare the distribution of violin plots representing the significance &lt;0.001 by ***, else if &lt;0.01 by **, and else if &lt;0.05 by *). See Figure <xref rid="F1" ref-type="fig">1</xref> for the abbreviations of datatypes. [SUPREME-: SUPREME without raw feature integration]</p>
          </caption>
          <graphic xlink:href="lqad063fig4" position="float"/>
        </fig>
        <p>For METABRIC data, the inclusion of COE- and EXP-based embeddings increased the performance, while the other embeddings did not affect the performance much (Supplementary Figure S7A). For the combined data, MUT- and EXP-based embeddings showed higher performance when included, whereas the inclusion of CLI-based embedding did not affect the performance much (Supplementary Figure S7B).</p>
        <p>In addition, we analyzed SUPREME results for TCGA data in terms of the best- and worst-performing models. Specifically, we had 31 top models with a macro F1 score is ≥0.88, and 30 bottom models with a macro F1 score is ≤0.83. We counted how many times each datatype occurred in the top and bottom models. CNA- and CLI-based embeddings were used for 28 and 19 out of 31 top models, respectively. The least occurred embedding was EXP-based with only six models out of 31. For the bottom models, we had 25 models from EXP-based embedding, while we had the least occurred embedding from CNA-based embedding with only five models. This analysis showed that CNA-based embedding was the most selected to have higher performance, while EXP-based embedding was rarely selected, supporting our findings in this section and in single model analysis.</p>
      </sec>
      <sec id="SEC3-4-2">
        <title>Impact of features from each datatype</title>
        <p>To see the impact of the features from each datatype, we ran SUPREME excluding the features from every single datatype separately. For each datatype <inline-formula><tex-math id="M00055" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula>, we excluded <inline-formula><tex-math id="M00056" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula>-specific node features from patient similarity networks and also did not integrate them with node embeddings during subtype prediction, called <inline-formula><tex-math id="M00057" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$no\mathbb {Y}_f$\end{document}</tex-math></inline-formula>. Considering that <inline-formula><tex-math id="M00058" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula>-specific patient similarity network was generated based on <inline-formula><tex-math id="M00059" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula>-specific features, we compared only the combinations without <inline-formula><tex-math id="M00060" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula> (<inline-formula><tex-math id="M00061" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$2^{d-1}-1$\end{document}</tex-math></inline-formula> models) to ensure the differences were due to the <inline-formula><tex-math id="M00062" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula>-specific features. We compared <inline-formula><tex-math id="M00063" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$no\mathbb {Y}_f$\end{document}</tex-math></inline-formula> models against the corresponding SUPREME models (called <inline-formula><tex-math id="M00064" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$with\mathbb {Y}_f$\end{document}</tex-math></inline-formula>), to show the importance of <inline-formula><tex-math id="M00065" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula>-specific features.</p>
        <p>When we excluded features from any datatype, we observed a lower or comparable performance (Figures <xref rid="F5" ref-type="fig">5</xref> and Supplementary Figure S8). The performance drop was significant for all the datatypes on TCGA, and gene expression and copy number aberration datatypes on METABRIC (Supplementary Figure S8A). The drop with the exclusion of the gene expression features was more drastic and it was consistent for all three datasets (Supplementary Figure S8B), supporting the importance of gene expression features for breast cancer (in agreement with findings in single model analysis).</p>
        <fig position="float" id="F5">
          <label>Figure 5.</label>
          <caption>
            <p>Analysis of features from each datatype. Violin plot of macro F1 scores for the models excluding the features from each datatype (<italic toggle="yes">no</italic><inline-formula><tex-math id="M00066" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}_f$\end{document}</tex-math></inline-formula> models, where <inline-formula><tex-math id="M00067" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula> is the datatype whose features are completely excluded) versus corresponding SUPREME models (<italic toggle="yes">with</italic><inline-formula><tex-math id="M00068" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}_f$\end{document}</tex-math></inline-formula> models) on TCGA data. Significance level was measured between <italic toggle="yes">with</italic> and <italic toggle="yes">no</italic> cases of the same datatype (Wilcoxon rank-sum test p-value to compare the distribution of violin plots representing the significance &lt;0.001 by ***, else if &lt;0.01 by **, and else if &lt;0.05 by *). See Figure <xref rid="F1" ref-type="fig">1</xref> for the abbreviations of datatypes.</p>
          </caption>
          <graphic xlink:href="lqad063fig5" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec id="SEC3-5">
      <title>Ablation studies</title>
      <p>We compared our tool with its variations when some steps were skipped to assess their importance (Table <xref rid="tbl3" ref-type="table">3</xref>). A comparison of SUPREME with SUPREME- showed the importance of raw feature integration. Also, to show the importance of GCN-based approaches, we trained the same ML algorithm (MLP in our case) using only the raw features and compared it with the SUPREME, which was based on the same raw features and additional patient embeddings.</p>
      <table-wrap position="float" id="tbl3">
        <label>Table 3.</label>
        <caption>
          <p>Ablation studies</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Comparison/Section</th>
              <th rowspan="1" colspan="1">Measures impact of</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">SUPREME vs. SUPREME-</td>
              <td rowspan="1" colspan="1">Raw feature integration</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SUPREME vs. MLP</td>
              <td rowspan="1" colspan="1">GCN utilization</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Single model section</td>
              <td rowspan="1" colspan="1">The used method with only one datatype</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SUPREME vs. MLP in single model section</td>
              <td rowspan="1" colspan="1">GCN utilization with only one datatype</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">This section</td>
              <td rowspan="1" colspan="1">Node features</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>To show the impact of each datatype separately, we demonstrated the performance of SUPREME models based on a single data type. We also compared SUPREME with other methods that can work with a single data modality only. To show the importance of embeddings at a single datatype level, we compared SUPREME with the MLP model trained on the features from the corresponding datatype.</p>
      <p>In addition to these studies, here, we also checked the overall impact of node features on the prediction tasks. To do that, instead of node features, we generated one-hot encoded features and evaluated SUPREME on TCGA data. We had macro F1 score as 0.75 ± 0.01, weighted F1 score as 0.83 ± 0.01, and accuracy as 0.84 ± 0.01. These results suggest that node features were important, dropping the evaluation performance drastically. This was expected as biological features are highly effective in determining the subtypes of breast cancer.</p>
    </sec>
    <sec id="SEC3-6">
      <title>SUPREME was model-agnostic outperforming other approaches in different domains</title>
      <p>To show the model-agnostic feature, we evaluated SUPREME on different domains. For that purpose, we generated two meta-path-based networks from the heterogeneous network of ACM and IMDB data. Since we had only two networks for these datasets, we shared the results for individual networks and the integrated one. Based on all six evaluation metrics, SUPREME outperformed other baseline methods on both datasets (Table <xref rid="tbl4" ref-type="table">4</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S6</xref>). As compared to MLP, we had increased performance showing the importance of graph utilization. Single models from GCN and SUPREME were not as good as the integrated one, showing the importance of SUPREME’s integrative nature.</p>
      <table-wrap position="float" id="tbl4">
        <label>Table 4.</label>
        <caption>
          <p>Macro F1 scores for IMDB and ACM datasets. [Macro F1: Macro-averaged F1 scores, GCN<sub><italic toggle="yes">x</italic></sub>: Result with x<sup>th</sup> network]. *Three results: First row with the first network, second row with the second network, and third row integrating the first and second networks. The first networks are based on movie-director-movie and paper-subject-paper meta-paths; and the second networks are based on movie-actor-movie and paper-author-paper meta-paths in IMDB and ACM datasets, respectively</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Method</th>
              <th rowspan="1" colspan="1">IMDB</th>
              <th rowspan="1" colspan="1">ACM</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">MLP</td>
              <td rowspan="1" colspan="1">0.53 ± 0.01</td>
              <td rowspan="1" colspan="1">0.90 ± 0.01</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SVM</td>
              <td rowspan="1" colspan="1">0.55 ± 0.00</td>
              <td rowspan="1" colspan="1">0.89 ± 0.00</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RF</td>
              <td rowspan="1" colspan="1">0.48 ± 0.00</td>
              <td rowspan="1" colspan="1">0.89 ± 0.00</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GCN<sub>1</sub></td>
              <td rowspan="1" colspan="1">0.56 ± 0.00</td>
              <td rowspan="1" colspan="1">0.70 ± 0.00</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GCN<sub>2</sub></td>
              <td rowspan="1" colspan="1">0.51 ± 0.00</td>
              <td rowspan="1" colspan="1">0.91 ± 0.00</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">0.58 ± 0.01</td>
              <td rowspan="1" colspan="1">0.91 ± 0.01</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SUPREME*</td>
              <td rowspan="1" colspan="1">0.55 ± 0.02</td>
              <td rowspan="1" colspan="1">0.92 ± 0.01</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">
                <bold>0.61 ± 0.02</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.94 ± 0.00</bold>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>According to these results, the first network of IMDB data (the network based on movie-director-movie meta-path) and the second network of ACM data (the network based on paper-author-paper meta-path) were more informative. This is not surprising that movie-director-movie association was more important than movie-actor-movie on movie genre prediction task on IMDB data. This was consistent based on the GCN runs and single models of SUPREME runs. Even though there is not a big difference on individual networks of ACM data for SUPREME, we see a big difference on GCN runs, showing the importance of our methodology utilizing embedding along with node features.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="SEC4">
    <title>DISCUSSION</title>
    <p>In this study, we introduced SUPREME, a novel integrative approach utilizing GCNs on multiple similarity networks where nodes are attributed with multi-modal node features. We applied SUPREME to three different prediction tasks from five different datasets. We observed that SUPREME outperformed other methods on ACM and IMDB data based on six evaluation metrics (Table <xref rid="tbl4" ref-type="table">4</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S6</xref>). On breast cancer subtyping, we compared SUPREME with seven cancer (sub)type prediction tools and baseline methods and observed that SUPREME substantially outperformed or was on par with them based on macro F1 score, accuracy, and weighted F1 score (Figures <xref rid="F2" ref-type="fig">2</xref>, Supplementary Figures S3 and S4, and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>). To demonstrate the consistency of the performance for individual SUPREME models, we shared the distribution of standard deviation of SUPREME models (Supplementary Figures S9, S10, and S11). We differentiated Normal-like subtype, which has the smallest sample size for three datasets, significantly better than all other tools on all three datasets showing SUPREME’s robustness even for minority classes (Supplementary Figure S5). We made SUPREME a publicly available tool at <ext-link xlink:href="https://github.com/bozdaglab/SUPREME" ext-link-type="uri">https://github.com/bozdaglab/SUPREME</ext-link> (under Creative Commons Attribution Non Commercial 4.0 International Public License) for researchers, biologists, and clinicians to utilize.</p>
    <p>We applied survival analysis to see the power of the methods to differentiate subtypes having significant survival differences. Using TCGA data, we compared our tool with nine popular integrative cancer subtype differentiating tools and baseline methods and SUPREME had consistently significant survival differences between predicted subtypes outperforming the other tools (Figure <xref rid="F3" ref-type="fig">3</xref>).</p>
    <p>Based on the majority of predictions, we determined ensemble subtype labels, most of which had high support from individual models (<xref rid="sup1" ref-type="supplementary-material">Supplemental File 3</xref>). We observed that survival difference between these ensemble subtypes was more significant than survival difference between gene expression-based ground truth (i.e., PAM50) subtypes (Supplementary Figure S6). These results suggest that some survival-related characteristics cannot be explained by gene expression data alone. SUPREME was able to extract these survival-related characteristics utilizing additional data modalities. SUPREME’s ensemble label predictions that were different from ground truth with high support could be further examined by biologists and clinicians.</p>
    <p>To show the effect of main steps of SUPREME, we performed an ablation study. In addition, we analyzed datatype-specific embeddings and datatype-specific features. We found that gene expression features were highly important for single models and overall, as expected for breast cancer. Findings about the important embeddings of datasets were supported by SUPREME- single models, where models were fed by only one embedding. We observed that patient embeddings were mostly more informative than raw features. Integrating raw features with patient embeddings usually improved the model performance (Figures <xref rid="F2" ref-type="fig">2</xref> and Supplementary Figure S3) except for raw features from few datatypes in single datatype-based models (Table <xref rid="tbl2" ref-type="table">2</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S3, S4, and S5</xref>).</p>
    <p>To compare the performance when we do not utilize the local neighborhood, we ran SUPREME- on TCGA data with the EXP-based single model when we do not have any neighbors than the patient itself. In that model, we had a macro F1 score of 0.85 ± 0.02 for SUPREME-, which was much higher than the original EXP-based model of SUPREME-. This model was even better than the EXP-based single model of SUPREME. This might suggest that EXP-based patient features themselves could perform better than neighborhood-convolved features because the ground truth utilizes patient features themselves to decide the subtype labels. Similarly, because of that, we might see a performance improvement when we add EXP-based raw features.</p>
    <p>SUPREME provides four options of ML algorithms to integrate embeddings and raw features, namely MLP, RF, SVM, and XGBoost. We ran SUPREME with all these choices and compared performances (<xref rid="sup1" ref-type="supplementary-material">Supplemental File 1</xref>, Supplementary Figures S1 and S2, and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>). RF and XGBoost had a low performance for some models. Overall, SVM had a good performance on every three datasets, however, it did not converge for some models. For this study, we chose MLP due to its high and consistent prediction performance for all three datasets and its low running time.</p>
    <p>In our experiments, we observed a high number of edges in MUT-based patient similarity networks as there were many patient pairs with the same similarity. Furthermore, the MUT-based models on TCGA data had high predictive performance, whereas these models had low predictive performance on METABRIC and the combined datasets. These discrepancies were mainly due to the sparse nature of the binary mutation features. For the special datatypes with binary-like sparse values like mutation, patient similarity networks and extracted features could be generated in a more sophisticated way such as based on the functional effect of these mutations (<xref rid="B36" ref-type="bibr">36–39</xref>).</p>
    <p>SUPREME is extendable to any number of datatypes to integrate. For cases where many datasets are integrated, to avoid potential overfitting, SUPREME provides an optional feature selection step for raw features before training the final prediction model. Users could skip raw feature integration altogether when network-specific patient embeddings provide sufficient discriminatory power. Users could run SUPREME on their training/validation data by enabling/disabling these features to optimize their models. In addition, users could perform ablation studies on SUPREME to determine the most effective data modalities and their combinations. Depending on these results, for the final prediction, users could rely on the most effective model or an ensemble model utilizing the most promising features and networks.</p>
    <p>As a future direction, SUPREME could utilize attention mechanisms (<xref rid="B40" ref-type="bibr">40–42</xref>), which allows getting weighted contributions from different datatypes, and also weighted neighborhoods from networks. In addition to multiomics datatypes, there are some regulatory relations such as competing endogenous RNA (ceRNA) regulation, which has been recently discovered with important insights into cancer (<xref rid="B43" ref-type="bibr">43</xref>). In our recent work, we inferred ceRNA interactions in breast cancer (<xref rid="B44" ref-type="bibr">44</xref>). To adopt this kind of regulatory relations, SUPREME could be improved to utilize patient similarity networks based on gene regulatory interactions and more complex patient relations. By improving the existing methodologies with recent advances in the literature, we can obtain more clear cancer subtype groups to pave the way for precision medicine.</p>
  </sec>
  <sec sec-type="data-availability" id="SEC5">
    <title>DATA AVAILABILITY</title>
    <p>TCGA data used in this article are available in The Cancer Genome Atlas project data portal at <ext-link xlink:href="https://portal.gdc.cancer.gov/" ext-link-type="uri">https://portal.gdc.cancer.gov/</ext-link>. The METABRIC data used in this article are deposited at the European Genome-Phenome Archive <ext-link xlink:href="http://www.ebi.ac.uk/ega/" ext-link-type="uri">http://www.ebi.ac.uk/ega/</ext-link>, which is hosted by the European Bioinformatics Institute, under accession number EGAS00000000083 [2]. The source code for our tool is available at <ext-link xlink:href="https://github.com/bozdaglab/SUPREME" ext-link-type="uri">https://github.com/bozdaglab/SUPREME</ext-link> (permanent doi: 10.6084/m9.figshare.23304062).</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>lqad063_Supplemental_Files</label>
      <media xlink:href="lqad063_supplemental_files.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ACK1">
    <title>ACKNOWLEDGEMENTS</title>
    <p>The authors would like to thank Ethan Rebello for his contributions to testing SUPREME and working on adding GPU support to it.</p>
  </ack>
  <sec id="SEC6">
    <title>SUPPLEMENTARY DATA</title>
    <p><ext-link xlink:href="https://academic.oup.com/nargab/article-lookup/doi/10.1093/nargab/lqad063#supplementary-data" ext-link-type="uri">Supplementary Data</ext-link> are available at NARGAB Online.</p>
  </sec>
  <sec id="SEC7">
    <title>FUNDING</title>
    <p>National Institute of General Medical Sciences of the National Institutes of Health [R35GM133657 to S.B.]; a summer research fellowship (to Z.N.K.) by the BioDiscovery Institute at the University of North Texas.</p>
    <p><italic toggle="yes">Conflict of interest statement</italic>. None declared.</p>
  </sec>
  <ref-list id="REF1">
    <title>REFERENCES</title>
    <ref id="B1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Waks</surname><given-names>A.G.</given-names></string-name>, <string-name><surname>Winer</surname><given-names>E.P.</given-names></string-name></person-group><article-title>Breast cancer treatment: a review</article-title>. <source>JAMA</source>. <year>2019</year>; <volume>321</volume>:<fpage>288</fpage>–<lpage>300</lpage>.<pub-id pub-id-type="pmid">30667505</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Curtis</surname><given-names>C.</given-names></string-name>, <string-name><surname>Shah</surname><given-names>S.P.</given-names></string-name>, <string-name><surname>Chin</surname><given-names>S.-F.</given-names></string-name>, <string-name><surname>Turashvili</surname><given-names>G.</given-names></string-name>, <string-name><surname>Rueda</surname><given-names>O.M.</given-names></string-name>, <string-name><surname>Dunning</surname><given-names>M.J.</given-names></string-name>, <string-name><surname>Speed</surname><given-names>D.</given-names></string-name>, <string-name><surname>Lynch</surname><given-names>A.G.</given-names></string-name>, <string-name><surname>Samarajiwa</surname><given-names>S.</given-names></string-name>, <string-name><surname>Yuan</surname><given-names>Y.</given-names></string-name><etal>et al</etal>.</person-group><article-title>The genomic and transcriptomic architecture of 2,000 breast tumours reveals novel subgroups</article-title>. <source>Nature</source>. <year>2012</year>; <volume>486</volume>:<fpage>346</fpage>–<lpage>352</lpage>.<pub-id pub-id-type="pmid">22522925</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Verhaak</surname><given-names>R.G.</given-names></string-name>, <string-name><surname>Hoadley</surname><given-names>K.A.</given-names></string-name>, <string-name><surname>Purdom</surname><given-names>E.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>V.</given-names></string-name>, <string-name><surname>Qi</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Wilkerson</surname><given-names>M.D.</given-names></string-name>, <string-name><surname>Miller</surname><given-names>C.R.</given-names></string-name>, <string-name><surname>Ding</surname><given-names>L.</given-names></string-name>, <string-name><surname>Golub</surname><given-names>T.</given-names></string-name>, <string-name><surname>Mesirov</surname><given-names>J.P.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Integrated genomic analysis identifies clinically relevant subtypes of glioblastoma characterized by abnormalities in PDGFRA, IDH1, EGFR, and NF1</article-title>. <source>Cancer Cell</source>. <year>2010</year>; <volume>17</volume>:<fpage>98</fpage>–<lpage>110</lpage>.<pub-id pub-id-type="pmid">20129251</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Noushmehr</surname><given-names>H.</given-names></string-name>, <string-name><surname>Weisenberger</surname><given-names>D.J.</given-names></string-name>, <string-name><surname>Diefes</surname><given-names>K.</given-names></string-name>, <string-name><surname>Phillips</surname><given-names>H.S.</given-names></string-name>, <string-name><surname>Pujara</surname><given-names>K.</given-names></string-name>, <string-name><surname>Berman</surname><given-names>B.P.</given-names></string-name>, <string-name><surname>Pan</surname><given-names>F.</given-names></string-name>, <string-name><surname>Pelloski</surname><given-names>C.E.</given-names></string-name>, <string-name><surname>Sulman</surname><given-names>E.P.</given-names></string-name>, <string-name><surname>Bhat</surname><given-names>K.P.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Identification of a CpG island methylator phenotype that defines a distinct subgroup of glioma</article-title>. <source>Cancer Cell</source>. <year>2010</year>; <volume>17</volume>:<fpage>510</fpage>–<lpage>522</lpage>.<pub-id pub-id-type="pmid">20399149</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baysan</surname><given-names>M.</given-names></string-name>, <string-name><surname>Bozdag</surname><given-names>S.</given-names></string-name>, <string-name><surname>Cam</surname><given-names>M.C.</given-names></string-name>, <string-name><surname>Kotliarova</surname><given-names>S.</given-names></string-name>, <string-name><surname>Ahn</surname><given-names>S.</given-names></string-name>, <string-name><surname>Walling</surname><given-names>J.</given-names></string-name>, <string-name><surname>Killian</surname><given-names>J.K.</given-names></string-name>, <string-name><surname>Stevenson</surname><given-names>H.</given-names></string-name>, <string-name><surname>Meltzer</surname><given-names>P.</given-names></string-name>, <string-name><surname>Fine</surname><given-names>H.A.</given-names></string-name></person-group><article-title>G-cimp status prediction of glioblastoma samples using mRNA expression data</article-title>. <source>PloS One</source>. <year>2012</year>; <volume>7</volume>:<fpage>e47839</fpage>.<pub-id pub-id-type="pmid">23139755</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vural</surname><given-names>S.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Guda</surname><given-names>C.</given-names></string-name></person-group><article-title>Classification of breast cancer patients using somatic mutation profiles and machine learning approaches</article-title>. <source>BMC Syst. Biol.</source><year>2016</year>; <volume>10</volume>:<fpage>263</fpage>–<lpage>276</lpage>.</mixed-citation>
    </ref>
    <ref id="B7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Youssef</surname><given-names>Y.M.</given-names></string-name>, <string-name><surname>White</surname><given-names>N.M.</given-names></string-name>, <string-name><surname>Grigull</surname><given-names>J.</given-names></string-name>, <string-name><surname>Krizova</surname><given-names>A.</given-names></string-name>, <string-name><surname>Samy</surname><given-names>C.</given-names></string-name>, <string-name><surname>Mejia-Guerrero</surname><given-names>S.</given-names></string-name>, <string-name><surname>Evans</surname><given-names>A.</given-names></string-name>, <string-name><surname>Yousef</surname><given-names>G.M.</given-names></string-name></person-group><article-title>Accurate molecular classification of kidney cancer subtypes using microRNA signature</article-title>. <source>Eur. Urol.</source><year>2011</year>; <volume>59</volume>:<fpage>721</fpage>–<lpage>730</lpage>.<pub-id pub-id-type="pmid">21272993</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <label>8.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ferlay</surname><given-names>J.</given-names></string-name>, <string-name><surname>Ervik</surname><given-names>M.</given-names></string-name>, <string-name><surname>Lam</surname><given-names>F.</given-names></string-name>, <string-name><surname>Colombet</surname><given-names>M.</given-names></string-name>, <string-name><surname>Mery</surname><given-names>L.</given-names></string-name>, <string-name><surname>Piñeros</surname><given-names>M.</given-names></string-name>, <string-name><surname>Znaor</surname><given-names>A.</given-names></string-name>, <string-name><surname>Soerjomataram</surname><given-names>I.</given-names></string-name>, <string-name><surname>Bray</surname><given-names>F.</given-names></string-name></person-group><article-title>Global cancer observatory: cancer today. Lyon: International Agency for Research on Cancer, 2018</article-title>. <year>2020</year>; </mixed-citation>
    </ref>
    <ref id="B9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anderson</surname><given-names>W.F.</given-names></string-name>, <string-name><surname>Chatterjee</surname><given-names>N.</given-names></string-name>, <string-name><surname>Ershler</surname><given-names>W.B.</given-names></string-name>, <string-name><surname>Brawley</surname><given-names>O.W.</given-names></string-name></person-group><article-title>Estrogen receptor breast cancer phenotypes in the Surveillance, Epidemiology, and End Results database</article-title>. <source>Breast Cancer Res. Treat.</source><year>2002</year>; <volume>76</volume>:<fpage>27</fpage>–<lpage>36</lpage>.<pub-id pub-id-type="pmid">12408373</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dietze</surname><given-names>E.C.</given-names></string-name>, <string-name><surname>Sistrunk</surname><given-names>C.</given-names></string-name>, <string-name><surname>Miranda-Carboni</surname><given-names>G.</given-names></string-name>, <string-name><surname>O’regan</surname><given-names>R.</given-names></string-name>, <string-name><surname>Seewaldt</surname><given-names>V.L.</given-names></string-name></person-group><article-title>Triple-negative breast cancer in African-American women: disparities versus biology</article-title>. <source>Nat. Rev. Cancer</source>. <year>2015</year>; <volume>15</volume>:<fpage>248</fpage>–<lpage>254</lpage>.<pub-id pub-id-type="pmid">25673085</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parker</surname><given-names>J.S.</given-names></string-name>, <string-name><surname>Mullins</surname><given-names>M.</given-names></string-name>, <string-name><surname>Cheang</surname><given-names>M.C.</given-names></string-name>, <string-name><surname>Leung</surname><given-names>S.</given-names></string-name>, <string-name><surname>Voduc</surname><given-names>D.</given-names></string-name>, <string-name><surname>Vickery</surname><given-names>T.</given-names></string-name>, <string-name><surname>Davies</surname><given-names>S.</given-names></string-name>, <string-name><surname>Fauron</surname><given-names>C.</given-names></string-name>, <string-name><surname>He</surname><given-names>X.</given-names></string-name>, <string-name><surname>Hu</surname><given-names>Z.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Supervised risk predictor of breast cancer based on intrinsic subtypes</article-title>. <source>J. Clin. Oncol.</source><year>2009</year>; <volume>27</volume>:<fpage>1160</fpage>.<pub-id pub-id-type="pmid">19204204</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koboldt</surname><given-names>D.</given-names></string-name>, <string-name><surname>Fulton</surname><given-names>R.</given-names></string-name>, <string-name><surname>McLellan</surname><given-names>M.</given-names></string-name>, <string-name><surname>Schmidt</surname><given-names>H.</given-names></string-name>, <string-name><surname>Kalicki-Veizer</surname><given-names>J.</given-names></string-name>, <string-name><surname>McMichael</surname><given-names>J.</given-names></string-name>, <string-name><surname>Fulton</surname><given-names>L.</given-names></string-name>, <string-name><surname>Dooling</surname><given-names>D.</given-names></string-name>, <string-name><surname>Ding</surname><given-names>L.</given-names></string-name>, <string-name><surname>Mardis</surname><given-names>E.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Comprehensive molecular portraits of human breast tumours</article-title>. <source>Nature</source>. <year>2012</year>; <volume>490</volume>:<fpage>61</fpage>–<lpage>70</lpage>.<pub-id pub-id-type="pmid">23000897</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname><given-names>R.</given-names></string-name>, <string-name><surname>Olshen</surname><given-names>A.B.</given-names></string-name>, <string-name><surname>Ladanyi</surname><given-names>M.</given-names></string-name></person-group><article-title>Integrative clustering of multiple genomic data types using a joint latent variable model with application to breast and lung cancer subtype analysis</article-title>. <source>Bioinformatics</source>. <year>2009</year>; <volume>25</volume>:<fpage>2906</fpage>–<lpage>2912</lpage>.<pub-id pub-id-type="pmid">19759197</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>B.</given-names></string-name>, <string-name><surname>Mezlini</surname><given-names>A.M.</given-names></string-name>, <string-name><surname>Demir</surname><given-names>F.</given-names></string-name>, <string-name><surname>Fiume</surname><given-names>M.</given-names></string-name>, <string-name><surname>Tu</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Brudno</surname><given-names>M.</given-names></string-name>, <string-name><surname>Haibe-Kains</surname><given-names>B.</given-names></string-name>, <string-name><surname>Goldenberg</surname><given-names>A.</given-names></string-name></person-group><article-title>Similarity network fusion for aggregating data types on a genomic scale</article-title>. <source>Nat. Methods</source>. <year>2014</year>; <volume>11</volume>:<fpage>333</fpage>–<lpage>337</lpage>.<pub-id pub-id-type="pmid">24464287</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nguyen</surname><given-names>H.</given-names></string-name>, <string-name><surname>Shrestha</surname><given-names>S.</given-names></string-name>, <string-name><surname>Draghici</surname><given-names>S.</given-names></string-name>, <string-name><surname>Nguyen</surname><given-names>T.</given-names></string-name></person-group><article-title>PINSPlus: a tool for tumor subtype discovery in integrated genomic data</article-title>. <source>Bioinformatics</source>. <year>2019</year>; <volume>35</volume>:<fpage>2843</fpage>–<lpage>2846</lpage>.<pub-id pub-id-type="pmid">30590381</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gori</surname><given-names>M.</given-names></string-name>, <string-name><surname>Monfardini</surname><given-names>G.</given-names></string-name>, <string-name><surname>Scarselli</surname><given-names>F.</given-names></string-name></person-group><article-title>A new model for learning in graph domains</article-title>. <source>Proceedings. 2005 IEEE International Joint Conference on Neural Networks</source>. <year>2005</year>; <volume>2</volume>:<fpage>729</fpage>–<lpage>734</lpage>.</mixed-citation>
    </ref>
    <ref id="B17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scarselli</surname><given-names>F.</given-names></string-name>, <string-name><surname>Gori</surname><given-names>M.</given-names></string-name>, <string-name><surname>Tsoi</surname><given-names>A.C.</given-names></string-name>, <string-name><surname>Hagenbuchner</surname><given-names>M.</given-names></string-name>, <string-name><surname>Monfardini</surname><given-names>G.</given-names></string-name></person-group><article-title>The graph neural network model</article-title>. <source>IEEE Trans. Neur. Networ.</source><year>2008</year>; <volume>20</volume>:<fpage>61</fpage>–<lpage>80</lpage>.</mixed-citation>
    </ref>
    <ref id="B18">
      <label>18.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kipf</surname><given-names>T.N.</given-names></string-name>, <string-name><surname>Welling</surname><given-names>M.</given-names></string-name></person-group><article-title>Semi-supervised classification with graph convolutional networks</article-title>. <year>2016</year>; <comment>arXiv doi:</comment><comment>22 February 2017, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.1609.02907</pub-id>.</mixed-citation>
    </ref>
    <ref id="B19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoff</surname><given-names>P.D.</given-names></string-name>, <string-name><surname>Raftery</surname><given-names>A.E.</given-names></string-name>, <string-name><surname>Handcock</surname><given-names>M.S.</given-names></string-name></person-group><article-title>Latent space approaches to social network analysis</article-title>. <source>J. Am. Stat. Assoc.</source><year>2002</year>; <volume>97</volume>:<fpage>1090</fpage>–<lpage>1098</lpage>.</mixed-citation>
    </ref>
    <ref id="B20">
      <label>20.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Perozzi</surname><given-names>B.</given-names></string-name>, <string-name><surname>Al-Rfou</surname><given-names>R.</given-names></string-name>, <string-name><surname>Skiena</surname><given-names>S.</given-names></string-name></person-group><article-title>Deepwalk: online learning of social representations</article-title>. <source>Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</source>. <year>2014</year>; <fpage>701</fpage>–<lpage>710</lpage>.</mixed-citation>
    </ref>
    <ref id="B21">
      <label>21.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Grover</surname><given-names>A.</given-names></string-name>, <string-name><surname>Leskovec</surname><given-names>J.</given-names></string-name></person-group><article-title>node2vec: scalable feature learning for networks</article-title>. <source>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</source>. <year>2016</year>; <fpage>855</fpage>–<lpage>864</lpage>.</mixed-citation>
    </ref>
    <ref id="B22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hamilton</surname><given-names>W.</given-names></string-name>, <string-name><surname>Ying</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Leskovec</surname><given-names>J.</given-names></string-name></person-group><article-title>Inductive representation learning on large graphs</article-title>. <source>Adv. Neural Inf. Process. Syst.</source><year>2017</year>; <volume>30</volume>:<uri xlink:href="https://proceedings.neurips.cc/paper_files/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html">https://proceedings.neurips.cc/paper_files/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html</uri>.</mixed-citation>
    </ref>
    <ref id="B23">
      <label>23.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Rhee</surname><given-names>S.</given-names></string-name>, <string-name><surname>Seo</surname><given-names>S.</given-names></string-name>, <string-name><surname>Kim</surname><given-names>S.</given-names></string-name></person-group><article-title>Hybrid approach of relation network and localized graph convolutional filtering for breast cancer subtype classification</article-title>. <year>2017</year>; <comment>bioRxiv doi:</comment><comment>15 June 2018, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.1711.05859</pub-id>.</mixed-citation>
    </ref>
    <ref id="B24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mohamed</surname><given-names>S.K.</given-names></string-name>, <string-name><surname>Nováček</surname><given-names>V.</given-names></string-name>, <string-name><surname>Nounu</surname><given-names>A.</given-names></string-name></person-group><article-title>Discovering protein drug targets using knowledge graph embeddings</article-title>. <source>Bioinformatics</source>. <year>2020</year>; <volume>36</volume>:<fpage>603</fpage>–<lpage>610</lpage>.<pub-id pub-id-type="pmid">31368482</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zitnik</surname><given-names>M.</given-names></string-name>, <string-name><surname>Agrawal</surname><given-names>M.</given-names></string-name>, <string-name><surname>Leskovec</surname><given-names>J.</given-names></string-name></person-group><article-title>Modeling polypharmacy side effects with graph convolutional networks</article-title>. <source>Bioinformatics</source>. <year>2018</year>; <volume>34</volume>:<fpage>i457</fpage>–<lpage>i466</lpage>.<pub-id pub-id-type="pmid">29949996</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ramirez</surname><given-names>R.</given-names></string-name>, <string-name><surname>Chiu</surname><given-names>Y.-C.</given-names></string-name>, <string-name><surname>Hererra</surname><given-names>A.</given-names></string-name>, <string-name><surname>Mostavi</surname><given-names>M.</given-names></string-name>, <string-name><surname>Ramirez</surname><given-names>J.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Huang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Jin</surname><given-names>Y.-F.</given-names></string-name></person-group><article-title>Classification of cancer types using graph convolutional neural networks</article-title>. <source>Front. Phys.</source><year>2020</year>; <volume>8</volume>:<fpage>203</fpage>.<pub-id pub-id-type="pmid">33437754</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>T.</given-names></string-name>, <string-name><surname>Shao</surname><given-names>W.</given-names></string-name>, <string-name><surname>Huang</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Tang</surname><given-names>H.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>J.</given-names></string-name>, <string-name><surname>Ding</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Huang</surname><given-names>K.</given-names></string-name></person-group><article-title>MOGONET integrates multi-omics data using graph convolutional networks allowing patient classification and biomarker identification</article-title>. <source>Nat. Commun.</source><year>2021</year>; <volume>12</volume>:<fpage>1</fpage>–<lpage>13</lpage>.<pub-id pub-id-type="pmid">33397941</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kursa</surname><given-names>M.B.</given-names></string-name>, <string-name><surname>Rudnicki</surname><given-names>W.R.</given-names></string-name></person-group><article-title>Feature selection with the Boruta package</article-title>. <source>J. Stat. Softw.</source><year>2010</year>; <volume>36</volume>:<fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation>
    </ref>
    <ref id="B29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gower</surname><given-names>J.C.</given-names></string-name></person-group><article-title>A general coefficient of similarity and some of its properties</article-title>. <source>Biometrics</source>. <year>1971</year>; <volume>27</volume>:<fpage>857</fpage>–<lpage>871</lpage>.</mixed-citation>
    </ref>
    <ref id="B30">
      <label>30.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Maechler</surname><given-names>M.</given-names></string-name>, <string-name><surname>Rousseeuw</surname><given-names>P.</given-names></string-name>, <string-name><surname>Struyf</surname><given-names>A.</given-names></string-name>, <string-name><surname>Hubert</surname><given-names>M.</given-names></string-name>, <string-name><surname>Hornik</surname><given-names>K.</given-names></string-name></person-group><article-title>cluster: cluster analysis basics and extensions, R package version 2.1.3</article-title>. <year>2022</year>; <uri xlink:href="https://cran.r-project.org/web/packages/cluster/citation.html">https://cran.r-project.org/web/packages/cluster/citation.html</uri>.</mixed-citation>
    </ref>
    <ref id="B31">
      <label>31.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kesimoglu</surname><given-names>Z.N.</given-names></string-name>, <string-name><surname>Bozdag</surname><given-names>S.</given-names></string-name></person-group><article-title>GRAF: graph attention-aware fusion networks</article-title>. <year>2023</year>; <comment>bioRxiv doi:</comment><comment>29 March 2023, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/pdf/2303.16781.pdf">https://arxiv.org/pdf/2303.16781.pdf</uri>.</mixed-citation>
    </ref>
    <ref id="B32">
      <label>32.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Fey</surname><given-names>M.</given-names></string-name>, <string-name><surname>Lenssen</surname><given-names>J.E.</given-names></string-name></person-group><article-title>Fast graph representation learning with PyTorch Geometric</article-title>. <year>2019</year>; <comment>bioRxiv doi:</comment><comment>25 April 2019, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.1903.02428</pub-id>.</mixed-citation>
    </ref>
    <ref id="B33">
      <label>33.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>M.</given-names></string-name>, <string-name><surname>Zheng</surname><given-names>D.</given-names></string-name>, <string-name><surname>Ye</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Gan</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Li</surname><given-names>M.</given-names></string-name>, <string-name><surname>Song</surname><given-names>X.</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>J.</given-names></string-name>, <string-name><surname>Ma</surname><given-names>C.</given-names></string-name>, <string-name><surname>Yu</surname><given-names>L.</given-names></string-name>, <string-name><surname>Gai</surname><given-names>Y.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Deep graph library: a graph-centric, highly-performant package for graph neural networks</article-title>. <year>2019</year>; <comment>bioRxiv doi:</comment><comment>25 August 2020, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.1909.01315</pub-id>.</mixed-citation>
    </ref>
    <ref id="B34">
      <label>34.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kingma</surname><given-names>D.P.</given-names></string-name>, <string-name><surname>Ba</surname><given-names>J.</given-names></string-name></person-group><article-title>Adam: a method for stochastic optimization</article-title>. <year>2014</year>; <comment>bioRxiv doi:</comment><comment>30 January 2017, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.1412.6980</pub-id>.</mixed-citation>
    </ref>
    <ref id="B35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gao</surname><given-names>F.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>W.</given-names></string-name>, <string-name><surname>Tan</surname><given-names>M.</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>L.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Fessler</surname><given-names>E.</given-names></string-name>, <string-name><surname>Vermeulen</surname><given-names>L.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.</given-names></string-name></person-group><article-title>DeepCC: a novel deep learning-based framework for cancer molecular subtype classification</article-title>. <source>Oncogenesis</source>. <year>2019</year>; <volume>8</volume>:<fpage>1</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">30631034</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khurana</surname><given-names>E.</given-names></string-name>, <string-name><surname>Fu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>J.</given-names></string-name>, <string-name><surname>Gerstein</surname><given-names>M.</given-names></string-name></person-group><article-title>Interpretation of genomic variants using a unified biological network approach</article-title>. <source>PLoS Comput. Biol.</source><year>2013</year>; <volume>9</volume>:<fpage>e1002886</fpage>.<pub-id pub-id-type="pmid">23505346</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kircher</surname><given-names>M.</given-names></string-name>, <string-name><surname>Witten</surname><given-names>D.M.</given-names></string-name>, <string-name><surname>Jain</surname><given-names>P.</given-names></string-name>, <string-name><surname>O’roak</surname><given-names>B.J.</given-names></string-name>, <string-name><surname>Cooper</surname><given-names>G.M.</given-names></string-name>, <string-name><surname>Shendure</surname><given-names>J.</given-names></string-name></person-group><article-title>A general framework for estimating the relative pathogenicity of human genetic variants</article-title>. <source>Nat. Genet.</source><year>2014</year>; <volume>46</volume>:<fpage>310</fpage>–<lpage>315</lpage>.<pub-id pub-id-type="pmid">24487276</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leiserson</surname><given-names>M.D.</given-names></string-name>, <string-name><surname>Vandin</surname><given-names>F.</given-names></string-name>, <string-name><surname>Wu</surname><given-names>H.-T.</given-names></string-name>, <string-name><surname>Dobson</surname><given-names>J.R.</given-names></string-name>, <string-name><surname>Eldridge</surname><given-names>J.V.</given-names></string-name>, <string-name><surname>Thomas</surname><given-names>J.L.</given-names></string-name>, <string-name><surname>Papoutsaki</surname><given-names>A.</given-names></string-name>, <string-name><surname>Kim</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Niu</surname><given-names>B.</given-names></string-name>, <string-name><surname>McLellan</surname><given-names>M.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Pan-cancer network analysis identifies combinations of rare somatic mutations across pathways and protein complexes</article-title>. <source>Nat. Genet.</source><year>2015</year>; <volume>47</volume>:<fpage>106</fpage>–<lpage>114</lpage>.<pub-id pub-id-type="pmid">25501392</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tokheim</surname><given-names>C.J.</given-names></string-name>, <string-name><surname>Papadopoulos</surname><given-names>N.</given-names></string-name>, <string-name><surname>Kinzler</surname><given-names>K.W.</given-names></string-name>, <string-name><surname>Vogelstein</surname><given-names>B.</given-names></string-name>, <string-name><surname>Karchin</surname><given-names>R.</given-names></string-name></person-group><article-title>Evaluating the evaluation of cancer driver genes</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><year>2016</year>; <volume>113</volume>:<fpage>14330</fpage>–<lpage>14335</lpage>.<pub-id pub-id-type="pmid">27911828</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vaswani</surname><given-names>A.</given-names></string-name>, <string-name><surname>Shazeer</surname><given-names>N.</given-names></string-name>, <string-name><surname>Parmar</surname><given-names>N.</given-names></string-name>, <string-name><surname>Uszkoreit</surname><given-names>J.</given-names></string-name>, <string-name><surname>Jones</surname><given-names>L.</given-names></string-name>, <string-name><surname>Gomez</surname><given-names>A.N.</given-names></string-name>, <string-name><surname>Kaiser</surname><given-names>Ł.</given-names></string-name>, <string-name><surname>Polosukhin</surname><given-names>I.</given-names></string-name></person-group><article-title>Attention is all you need</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2017</year>; <volume>30</volume>:<fpage>1</fpage>–<lpage>11</lpage>.</mixed-citation>
    </ref>
    <ref id="B41">
      <label>41.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Veličković</surname><given-names>P.</given-names></string-name>, <string-name><surname>Cucurull</surname><given-names>G.</given-names></string-name>, <string-name><surname>Casanova</surname><given-names>A.</given-names></string-name>, <string-name><surname>Romero</surname><given-names>A.</given-names></string-name>, <string-name><surname>Lio</surname><given-names>P.</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y.</given-names></string-name></person-group><article-title>Graph attention networks</article-title>. <year>2017</year>; <comment>bioRxiv doi:</comment><comment>4 February 2018, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.1710.10903</pub-id>.</mixed-citation>
    </ref>
    <ref id="B42">
      <label>42.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Brody</surname><given-names>S.</given-names></string-name>, <string-name><surname>Alon</surname><given-names>U.</given-names></string-name>, <string-name><surname>Yahav</surname><given-names>E.</given-names></string-name></person-group><article-title>How attentive are graph attention networks</article-title>. <year>2021</year>; <comment>bioRxiv doi:</comment><comment>31 January 2022, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.2105.14491</pub-id>.</mixed-citation>
    </ref>
    <ref id="B43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Salmena</surname><given-names>L.</given-names></string-name>, <string-name><surname>Poliseno</surname><given-names>L.</given-names></string-name>, <string-name><surname>Tay</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Kats</surname><given-names>L.</given-names></string-name>, <string-name><surname>Pandolfi</surname><given-names>P.P.</given-names></string-name></person-group><article-title>A ceRNA hypothesis: the Rosetta Stone of a hidden RNA language</article-title>. <source>Cell</source>. <year>2011</year>; <volume>146</volume>:<fpage>353</fpage>–<lpage>358</lpage>.<pub-id pub-id-type="pmid">21802130</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kesimoglu</surname><given-names>Z.N.</given-names></string-name>, <string-name><surname>Bozdag</surname><given-names>S.</given-names></string-name></person-group><article-title>Crinet: a computational tool to infer genome-wide competing endogenous RNA (ceRNA) interactions</article-title>. <source>Plos One</source>. <year>2021</year>; <volume>16</volume>:<fpage>e0251399</fpage>.<pub-id pub-id-type="pmid">33983999</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">NAR Genom Bioinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">NAR Genom Bioinform</journal-id>
    <journal-id journal-id-type="publisher-id">nargab</journal-id>
    <journal-title-group>
      <journal-title>NAR Genomics and Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2631-9268</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10481254</article-id>
    <article-id pub-id-type="doi">10.1093/nargab/lqad063</article-id>
    <article-id pub-id-type="publisher-id">lqad063</article-id>
    <article-categories>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00030</subject>
        <subject>AcademicSubjects/SCI00980</subject>
        <subject>AcademicSubjects/SCI01060</subject>
        <subject>AcademicSubjects/SCI01140</subject>
        <subject>AcademicSubjects/SCI01180</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Standard Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SUPREME: multiomics data integration using graph convolutional networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-8592-4365</contrib-id>
        <name>
          <surname>Kesimoglu</surname>
          <given-names>Ziynet Nesibe</given-names>
        </name>
        <aff><institution>Department of Computer Science and Engineering, University of North Texas</institution>, Denton, TX, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-4813-4310</contrib-id>
        <name>
          <surname>Bozdag</surname>
          <given-names>Serdar</given-names>
        </name>
        <!--Serdar.Bozdag@unt.edu-->
        <aff><institution>Department of Computer Science and Engineering, University of North Texas</institution>, Denton, TX, <country country="US">USA</country></aff>
        <aff><institution>Department of Mathematics, University of North Texas</institution>, Denton, TX, <country country="US">USA</country></aff>
        <aff><institution>BioDiscovery Institute, University of North Texas</institution>, Denton, TX, <country country="US">USA</country></aff>
        <xref rid="COR1" ref-type="corresp"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="COR1">To whom correspondence should be addressed. Tel: +1 940 369 7581; Email: <email>Serdar.Bozdag@unt.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>6</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-06-28">
      <day>28</day>
      <month>6</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>28</day>
      <month>6</month>
      <year>2023</year>
    </pub-date>
    <volume>5</volume>
    <issue>2</issue>
    <elocation-id>lqad063</elocation-id>
    <history>
      <date date-type="received">
        <day>19</day>
        <month>1</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>08</day>
        <month>5</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>07</day>
        <month>6</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023. Published by Oxford University Press on behalf of NAR Genomics and Bioinformatics.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact <email>journals.permissions@oup.com</email></license-p>
      </license>
    </permissions>
    <self-uri xlink:href="lqad063.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>To pave the road towards precision medicine in cancer, patients with similar biology ought to be grouped into same cancer subtypes. Utilizing high-dimensional multiomics datasets, integrative approaches have been developed to uncover cancer subtypes. Recently, Graph Neural Networks have been discovered to learn node embeddings utilizing node features and associations on graph-structured data. Some integrative prediction tools have been developed leveraging these advances on multiple networks with some limitations. Addressing these limitations, we developed SUPREME, a node classification framework, which integrates multiple data modalities on graph-structured data. On breast cancer subtyping, unlike existing tools, SUPREME generates patient embeddings from multiple similarity networks utilizing multiomics features and integrates them with raw features to capture complementary signals. On breast cancer subtype prediction tasks from three datasets, SUPREME outperformed other tools. SUPREME-inferred subtypes had significant survival differences, mostly having more significance than ground truth, and outperformed nine other approaches. These results suggest that with proper multiomics data utilization, SUPREME could demystify undiscovered characteristics in cancer subtypes that cause significant survival differences and could improve ground truth label, which depends mainly on one datatype. In addition, to show model-agnostic property of SUPREME, we applied it to two additional datasets and had a clear outperformance.</p>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institute of General Medical Sciences</institution>
            <institution-id institution-id-type="DOI">10.13039/100000057</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>R35GM133657</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>University of North Texas</institution>
            <institution-id institution-id-type="DOI">10.13039/100008973</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="12"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="SEC1">
    <title>INTRODUCTION</title>
    <p>Cancer is one of the deadliest diseases for which cancer-causing agents such as oncogenes, mutations, and gene regulatory associations have not been fully demystified. Cancer patients show different characteristics in terms of the progression of disease and response to treatment (<xref rid="B1" ref-type="bibr">1</xref>). Various biological datasets from cancer tissues have been generated to better characterize cancer biology. For instance, The Cancer Genome Atlas (TCGA) project generated over 2.5 petabytes of multiple omics (multiomics) data for thousands of patients from 33 different cancer types (data are available at <ext-link xlink:href="https://portal.gdc.cancer.gov/" ext-link-type="uri">https://portal.gdc.cancer.gov/</ext-link>). Specifically for breast cancer, the Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) has generated four types of multiomics data for thousands of breast tumor samples (<xref rid="B2" ref-type="bibr">2</xref>). Utilizing high-dimensional biological datasets in public databases, computational approaches have been developed to discover subtypes of various cancers (<xref rid="B3" ref-type="bibr">3–5</xref>). Several of the cancer subtype prediction studies rely only on one type of biological datatype (<xref rid="B4" ref-type="bibr">4</xref>,<xref rid="B6" ref-type="bibr">6</xref>,<xref rid="B7" ref-type="bibr">7</xref>). However, each of these datatypes captures a different part of the underlying biology, thus developing integrative computational methods has been an important research area in bioinformatics.</p>
    <p>Breast cancer is currently the most commonly-diagnosed cancer worldwide (<xref rid="B8" ref-type="bibr">8</xref>). Therapeutic groups in breast cancer (i.e., estrogen receptor-positive, progesterone receptor-positive, human epidermal growth factor receptor 2 (HER2) amplified group, and triple-negative breast cancer) mainly depend on three receptors. Even though these receptors are very impactful in determining the breast cancer subtypes, they are not solely sufficient to classify a patient. Some other studies showed that genomic and clinical features such as race, age, and some mutations are also important in breast cancer subtyping (<xref rid="B9" ref-type="bibr">9</xref>,<xref rid="B10" ref-type="bibr">10</xref>).</p>
    <p>Genomic datatypes are found informative for differentiating subgroups in breast cancer. In 2009, Parker <italic toggle="yes">et al.</italic> (<xref rid="B11" ref-type="bibr">11</xref>) found a clear difference in the expression of 50 genes for breast cancer and introduced breast cancer molecular subtypes, called <italic toggle="yes">PAM50 subtypes</italic>. In 2012, the TCGA group published a study analyzing breast cancer subgroups and their associations with single datatypes, obtaining subtype-specific patterns in each datatype (<xref rid="B12" ref-type="bibr">12</xref>) and supporting the importance of gene expression-based models such as PAM50 (<xref rid="B11" ref-type="bibr">11</xref>). Even though there are important signals from both clinical and genomic features to determine the subtype of a patient, relying on a single data modality is not sufficient to differentiate subtypes clearly. As we get more samples and datatypes to analyze, it is important to integrate all the available datatypes properly with advanced approaches to understand differences in the characteristics of cancer patients.</p>
    <p>Recently several groups have developed unsupervised computational tools to integrate multiple datatypes to discover cancer subtypes. For instance, iClusterPlus (<xref rid="B13" ref-type="bibr">13</xref>) uses a joint latent variable model concatenating multiple datatypes with dimension reduction to cluster cancer patients. Similarity Network Fusion (SNF) (<xref rid="B14" ref-type="bibr">14</xref>) builds a patient similarity network based on each datatype, obtains a fused patient network by applying a nonlinear fusion step, and performs the clustering on that final network. PINSPlus (<xref rid="B15" ref-type="bibr">15</xref>) assumes that samples that are truly in the same subtype are clustered together despite small changes in the data. PINSPlus discovers the subtypes if the samples are highly connected for different datatypes applying data perturbation. The authors demonstrated that PINSPlus had robust results with significant survival differences across different cancer types. Those studies focus on unsupervised multiomics data integration without the utilization of found subtype labels such as PAM50 subtype labels. Furthermore, these tools utilize patient similarity networks or features, but not both simultaneously, while there are recent improvements in graph representation learning allowing the utilization of both at the same time (<xref rid="B16" ref-type="bibr">16–18</xref>).</p>
    <p>Graphs (networks) are suitable data structures to store multiomics datasets, however, machine learning (ML)-based approaches are challenging on graph data. Deep learning-based architectures have been used extensively for grid-like data (e.g., image), however, these methods are not directly applicable to graph data. Graphs are unstructured as each node has a varying number of neighbors and there is no fixed ordering of nodes. To train ML models on graph data, <italic toggle="yes">embedding</italic> (a fixed low-dimensional vector) is used and some shallow embedding methods emerged by encoding every node into embedding, representing the position and the local relationships in the graph (<xref rid="B19" ref-type="bibr">19–21</xref>). However, these shallow embedding methods are not scalable for large graphs and cannot utilize the node features that we have plenty of, thus, these methods have been replaced with more advanced deep learning-based methods such as Graph Neural Networks (GNNs) (<xref rid="B16" ref-type="bibr">16</xref>,<xref rid="B17" ref-type="bibr">17</xref>). The main difference in GNN-based architectures is how the features are aggregated from the local structure. Graph Convolutional Network (GCN) is one of the most popular GNNs that uses a modified aggregation involving self edges with normalization across neighbors (<xref rid="B18" ref-type="bibr">18</xref>). GNNs have recently been applied to biological problems such as cancer type/subtype prediction and drug response prediction (<xref rid="B18" ref-type="bibr">18</xref>, <xref rid="B22" ref-type="bibr">22–25</xref>).</p>
    <p>Even though there are some studies applying convolution to graph-structured data for cancer subtyping, these models are mostly applicable to a single network or had some limitations for integrative approaches. In (<xref rid="B26" ref-type="bibr">26</xref>), cancer type prediction of patients from 33 cancer and non-cancer types (i.e., all normal samples from all 33 available cancer types) was performed using GCNs. The input network was based on gene coexpression or protein-protein interaction, but the convolution was done on the gene expression dataset only, thus, missing the information of multiple data modalities. Multiomics GCN (MOGONET) is a supervised multiomics integration framework using GCNs with a patient similarity network for mRNA expression, DNA methylation, and microRNA expression separately (<xref rid="B27" ref-type="bibr">27</xref>). MOGONET gets the label independently from three different models, then uses them to get the final prediction. However, it does not consider multiple features for networks. We call this kind of embedding <italic toggle="yes">datatype-specific patient embedding</italic> where the methodology generates datatype-specific networks with datatype-specific node features and considers only the prediction labels from separate GCN models. However, these embeddings could be improved by utilizing all the multiomics patient features on each local network structure, making the embedding <italic toggle="yes">network-specific patient embedding</italic>. Moreover, it is possible to utilize GCN not only to get the prediction label but also to obtain the embeddings and integrate them. Going further, we can also integrate the patient features (called <italic toggle="yes">raw features</italic>) with embeddings to capture any diluted signals from features. To utilize more from available knowledge, it is important to properly integrate multiple network representations and multiomics features simultaneously.</p>
    <p>To address the aforementioned limitations, we developed a computational tool named SUPREME integrating multiple types of datasets using GCNs. SUPREME generated similarity networks using features from multi-modal datasets where node features include features from all data modalities, assuming that nodes with a similar local neighborhood are likely to belong to the same class. SUPREME encodes the relations on a network from each datatype and obtains network-specific node embeddings incorporating node features on each network. Then SUPREME integrates these embeddings providing extensive evaluations of all combinations of node embeddings. For each combination, SUPREME integrates the selected embeddings with raw features to utilize all the knowledge at the same time. SUPREME utilizes all available datatypes from public datasets and can interpret each datatype’s effectiveness in terms of features and networks. Being model-agnostic, SUPREME could be easily adapted to any model, any prediction task handling any number of datatypes, and could be easily modified by changing the embedding integration method, network generation strategy, and feature extraction approach.</p>
    <p>In this study, SUPREME was applied to three different prediction tasks from five different datasets. We applied SUPREME to predict subtypes of breast cancer patients using multiomics datasets (from TCGA and METABRIC datasets separately and together). Our results on cancer subtype prediction tasks showed that SUPREME outperformed other integrative supervised cancer (sub)type prediction tools and baseline methods. SUPREME had improved performance showing the importance of GCN-based approaches, network-specific patient embeddings, and raw feature integration. SUPREME was robust showing high and consistent prediction performance. We observed that the gene expression (EXP)-based features were the most significant features, as expected for breast cancer. Importantly, SUPREME-inferred cancer subtypes had consistently significant survival differences and were mostly more significant than the survival differences between ground truth subtypes, which were based on gene expression datatype. These results suggest that SUPREME can differentiate the characteristics of cancer subtypes properly utilizing the multiple network relations and multiple datatypes. To demonstrate the model-agnostic property of our tool, we also applied SUPREME to ACM and IMDB datasets and SUPREME outperformed other methods on both datasets.</p>
  </sec>
  <sec sec-type="materials|methods" id="SEC2">
    <title>MATERIALS AND METHODS</title>
    <p>SUPREME is a computational tool for node classification tasks integrating multiple data modalities using GCNs. Briefly, the first step is data preparation. In the second step, SUPREME extracts features from each datatype. Using those features, SUPREME generates individual similarity networks per datatype where features from all datatypes are used as node attributes. In the third step, using the obtained networks and features, SUPREME generates the network-specific node embeddings by running GCN on each network. In the last step, SUPREME does prediction by integrating individual network-specific embeddings and raw features. In the following part, we explain each step of SUPREME in detail.</p>
    <sec id="SEC2-1">
      <title>Data preparation</title>
      <p>We applied SUPREME on three datasets for the breast cancer subtype prediction task. We collected the data and generated seven datatypes (i.e., clinical, copy number aberration, coexpression, gene expression, DNA methylation, microRNA expression, and mutation) across 1022 breast tumor samples from TCGA (<xref rid="B12" ref-type="bibr">12</xref>), five datatypes (i.e., clinical, copy number aberration, coexpression, gene expression, and mutation) across 1699 breast tumor samples from METABRIC (<xref rid="B2" ref-type="bibr">2</xref>) and three datatypes (clinical, gene expression, and mutation) across a total of 2721 breast tumor samples from the combined datasets of TCGA and METABRIC. As ground truth for the prediction task, we obtained the PAM50 subtype labels, namely Basal-like, HER2-Enriched, Luminal-A, Luminal-B, and Normal-like (<xref rid="B11" ref-type="bibr">11</xref>). Data preprocessing details are in <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods 1.1</xref>.</p>
      <p>We also collected ACM and IMDB datasets for two additional tasks: movie genre prediction from IMDB dataset (<ext-link xlink:href="https://www.imdb.com" ext-link-type="uri">https://www.imdb.com</ext-link>) and paper area prediction task from ACM dataset (<ext-link xlink:href="http://dl.acm.org" ext-link-type="uri">http://dl.acm.org</ext-link>). IMDB dataset has a heterogeneous network with three node types (movie, actor, and director) along with two associations: movie-actor and movie-director. The movies have three genre classes: action, comedy, and drama. ACM dataset has also three node types (paper, author, and subject) on a heterogeneous network along with two associations: paper-author and paper-subject. The papers have three classes: database, wireless communication, and data mining.</p>
      <p>The number of features and samples for each dataset are shown in Table <xref rid="tbl1" ref-type="table">1</xref>.</p>
      <table-wrap position="float" id="tbl1">
        <label>Table 1.</label>
        <caption>
          <p>Number of features and samples for each dataset. Subtypes are abbreviated as BL: basal-like, HER2: HER2-Enriched, LA: luminal-A, LB: luminal-B, NL: normal-like</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Dataset</th>
              <th rowspan="1" colspan="1">Number of raw features</th>
              <th rowspan="1" colspan="1">Number of samples</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">TCGA</td>
              <td rowspan="1" colspan="1">3088</td>
              <td rowspan="1" colspan="1">1022 samples: 172 BL (17%), 78 HER2 (8%), 538 LA (53%), 195 LB (19%), 39 NL (4%)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">METABRIC</td>
              <td rowspan="1" colspan="1">1761</td>
              <td rowspan="1" colspan="1">1699 samples: 199 BL (12%), 220 HER2 (13%), 679 LA (40%), 461 LB (27%), 140 NL (8%)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Combined (TCGA+ METABRIC)</td>
              <td rowspan="1" colspan="1">1229</td>
              <td rowspan="1" colspan="1">2721 samples: 371 BL (14%), 298 HER2 (11%), 1217 LA (45%), 656 LB (24%), 179 NL (7%)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">IMDB</td>
              <td rowspan="1" colspan="1">3066</td>
              <td rowspan="1" colspan="1">4278 samples: 1135 (27%), 1584 (37%), 1559 (36%)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ACM</td>
              <td rowspan="1" colspan="1">1870</td>
              <td rowspan="1" colspan="1">3025 samples: 1061 (35%), 965 (32%), 999 (33%)</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="SEC2-2">
      <title>Feature extraction &amp; network generation</title>
      <sec id="SEC2-2-1">
        <title>Breast cancer subtyping</title>
        <p>SUPREME incorporates seven datatypes for TCGA data, five datatypes for METABRIC data, and three datatypes for the combined data. We utilized a Random Forest-based feature selection algorithm, called Boruta (<xref rid="B28" ref-type="bibr">28</xref>), to extract features from high dimensional datatypes. The selected features in the data preprocessing step (i.e., multiomics features) were used to compute the similarity between patients when generating the patient similarity networks, as node features in the patient similarity networks, and to integrate as raw features before the prediction task. To compute patient similarities in datatype-specific patient similarity networks, we used Pearson correlation for gene expression, copy number aberration, DNA methylation, microRNA expression, and coexpression datatypes; the Gower metric (<xref rid="B29" ref-type="bibr">29</xref>) from the daisy function of cluster R package (<xref rid="B30" ref-type="bibr">30</xref>) for clinical features; and Jaccard distance for binary mutation features. After selecting the top edges, the edge weights were eliminated to generate an unweighted network. We used 2500 edges for the datatypes of TCGA, 4500 for METABRIC and 7000 for the combined data (having approximately 2.5 times the sample size). Details of feature extraction and network generation are in <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods 1.2</xref>.</p>
      </sec>
      <sec id="SEC2-2-2">
        <title>Movie genre prediction</title>
        <p>We did not apply any feature selection for the IMDB dataset and used node features processed in (<xref rid="B31" ref-type="bibr">31</xref>). Using two associations (i.e., movie-actor and movie-director) in the data, two movie similarity networks were generated based on two meta-paths using (<xref rid="B32" ref-type="bibr">32</xref>): movie-director-movie with 17 446 edges and movie-actor-movie with 85 358 edges. Meta-path-based similarity networks connect nodes based on a given association. For instance, the meta-path movie-actor-movie defines similarity as the existence of at least one common actor between two movies.</p>
      </sec>
      <sec id="SEC2-2-3">
        <title>Paper area prediction</title>
        <p>For the ACM dataset, we did not apply any feature selection and used the node features processed in (<xref rid="B31" ref-type="bibr">31</xref>). Utilizing two associations (i.e., paper-author and paper-subject) in the data, two meta-paths were used to generate two paper similarity networks using (<xref rid="B33" ref-type="bibr">33</xref>): paper-author-paper with 29 281 edges and paper-subject-paper with 2 210 761 edges. The meta-path-based similarity definition is the same as in the IMDB dataset.</p>
        <p>When there is a high number of raw features and many networks to integrate, this might affect the prediction performance, and model training could be time-consuming. Thus, we added another optional feature selection step to further reduce the number of raw features integrated with the node embeddings for the prediction task. We enabled this additional feature selection for TCGA data where we had a high number of raw features and networks and observed that it reduced running time without affecting the prediction performance. We did not apply optional feature selection for ACM and IMDB datasets since we have only two networks. Similarly, we did not apply any reduction for the number of edges for these datasets since we do not have any quantitative similarities to prioritize the edges on meta-path-based similarity networks.</p>
      </sec>
    </sec>
    <sec id="SEC2-3">
      <title>Node embedding generation</title>
      <p>After extracting features and generating networks, we obtained network-specific node embeddings, which capture the topology of the network as well as node features to be utilized in a downstream ML task.</p>
      <p>In this study, we used the GCN model of Kipf and Welling (<xref rid="B18" ref-type="bibr">18</xref>) involving self edges in convolution and scaling the sum of aggregated features across the neighbors. GCN models learn the data by performing convolution on networks, considering one-hop local neighbors with equal contribution, and encoding the local topology of the network. Stacked layers involve recursive neighborhood diffusion considering more than a one-hop neighborhood.</p>
      <p>Let’s call an undirected graph as <inline-formula><tex-math id="M0001" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {G}= (\mathcal {V},\mathcal {E})$\end{document}</tex-math></inline-formula> where <inline-formula><tex-math id="M0001a" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {V}$\end{document}</tex-math></inline-formula> is a set of <italic toggle="yes">n</italic> nodes, i.e., <inline-formula><tex-math id="M0002" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {V} = \lbrace v_1, v_2, ..., v_n\rbrace$\end{document}</tex-math></inline-formula>, and <inline-formula><tex-math id="M0003" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {E}$\end{document}</tex-math></inline-formula> is a set of edges between nodes where <inline-formula><tex-math id="M0004" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$(v_i,v_j) \in \mathcal {E}$\end{document}</tex-math></inline-formula> when <inline-formula><tex-math id="M0005" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_i \in \mathcal {V}$\end{document}</tex-math></inline-formula>, <inline-formula><tex-math id="M0006" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_j \in \mathcal {V}$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M0007" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_i$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M0008" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_j$\end{document}</tex-math></inline-formula> have an association based on the graph <inline-formula><tex-math id="M0009" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {G}$\end{document}</tex-math></inline-formula>. Since the graph <inline-formula><tex-math id="M00010" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {G}$\end{document}</tex-math></inline-formula> is undirected, <inline-formula><tex-math id="M00011" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$(v_i,v_j) \in \mathcal {E} \iff (v_j,v_i) \in \mathcal {E}$\end{document}</tex-math></inline-formula>.</p>
      <p>The input for a GCN model is a feature matrix <inline-formula><tex-math id="M00012" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {X} \in \mathbb {R}^{nxk}$\end{document}</tex-math></inline-formula> where <italic toggle="yes">k</italic> is the feature size, and the adjacency matrix <inline-formula><tex-math id="M00013" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {A} \in \mathbb {R}^{nxn}$\end{document}</tex-math></inline-formula> with added self edges defined as:</p>
      <disp-formula>
        <tex-math id="M00014" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{equation*} \mathcal {A}[i,j]=\left\lbrace \begin{array}{lr}1 &amp; \text{ if }\left(v_{i}, v_{j}\right) \in \mathcal {E} \text{ or } i=j \\ 0 &amp; \text{ otherwise } \end{array}\right. \end{equation*}\end{document}</tex-math>
      </disp-formula>
      <p>The iteration process is defined as:</p>
      <disp-formula>
        <tex-math id="M00015" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{equation*} \mathcal {H}^{(l+1)}=\sigma \left(\mathcal {D}^{-\frac{1}{2}} \mathcal {A} \mathcal {D}^{-\frac{1}{2}} \mathcal {H}^{(l)} \mathcal {W}^{(l)}\right) \end{equation*}\end{document}</tex-math>
      </disp-formula>
      <p>with <inline-formula><tex-math id="M00016" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {H}^{(0)} = \mathcal {X}$\end{document}</tex-math></inline-formula> where</p>
      <disp-formula>
        <tex-math id="M00017" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{equation*} \mathcal {D}[i,i]=\sum _{j=1}^{n} \mathcal {A}[i,j], \end{equation*}\end{document}</tex-math>
      </disp-formula>
      <p><inline-formula><tex-math id="M00018" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {H}^{(l)}$\end{document}</tex-math></inline-formula> is the activation matrix in the <italic toggle="yes">l</italic>th layer, <inline-formula><tex-math id="M00019" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {W}^{(l)}$\end{document}</tex-math></inline-formula> is the trainable weight matrix in the <italic toggle="yes">l</italic>th layer and σ is the activation function.</p>
      <p>Considering breast cancer subtyping task using TCGA data, SUPREME setup for the single model generation was as follows: there were seven networks (i.e., patient similarity networks), each obtained from a different datatype. All networks had nodes as breast cancer patients and edges based on the patient similarities from the corresponding data. For instance, let us consider <inline-formula><tex-math id="M00020" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {G}$\end{document}</tex-math></inline-formula> as a gene expression-derived patient similarity network. This network connects patient nodes with a high correlation between their gene expression profile. As node features, <inline-formula><tex-math id="M00021" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {G}$\end{document}</tex-math></inline-formula> has the combined features, which were extracted from all the seven datatypes. Features of <inline-formula><tex-math id="M00022" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_i$\end{document}</tex-math></inline-formula> are denoted as <inline-formula><tex-math id="M00023" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$x_i \in \mathbb {R}^k$\end{document}</tex-math></inline-formula> where <italic toggle="yes">k</italic> is the total feature size. So, the stacked feature matrix <inline-formula><tex-math id="M00024" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {X} \in \mathbb {R}^{nxk}$\end{document}</tex-math></inline-formula> is:</p>
      <disp-formula>
        <tex-math id="M00025" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{equation*} \mathcal {X}=\begin{bmatrix}x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix} \end{equation*}\end{document}</tex-math>
      </disp-formula>
      <p>The local one-hop neighborhood of a node <inline-formula><tex-math id="M00026" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_i$\end{document}</tex-math></inline-formula> is <inline-formula><tex-math id="M00027" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {N}_{i}=\left\lbrace v_{j}:\left(v_{i}, v_{j}\right) \in \mathcal {E}\right\rbrace$\end{document}</tex-math></inline-formula> that included the set of nodes having an association with the node <inline-formula><tex-math id="M00028" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_i$\end{document}</tex-math></inline-formula>. Feature aggregation on the local neighborhood of each node was done by multiplying <inline-formula><tex-math id="M00029" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {X}$\end{document}</tex-math></inline-formula> by the <italic toggle="yes">nxn</italic>-sized scaled adjacency matrix <inline-formula><tex-math id="M00030" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {A}^{^{\prime }}$\end{document}</tex-math></inline-formula> where</p>
      <disp-formula>
        <tex-math id="M00031" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{equation*} \mathcal {A}^{\prime }=\mathcal {D}^{-\frac{1}{2}} \mathcal {A} \mathcal {D}^{-\frac{1}{2}}. \end{equation*}\end{document}</tex-math>
      </disp-formula>
      <p>Using 2-layered GCN in SUPREME, we had the form of the forward model giving the output <inline-formula><tex-math id="M00032" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {Z}$\end{document}</tex-math></inline-formula> where</p>
      <disp-formula>
        <tex-math id="M00033" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{equation*} \mathcal {Z}=\operatorname{softmax}\left(\mathcal {A}^{\prime } \operatorname{ReLU}\left(\mathcal {A}^{\prime } \mathcal {X} \mathcal {W}^{(1)}\right) \mathcal {W}^{(2)}\right) \end{equation*}\end{document}</tex-math>
      </disp-formula>
      <p>and <inline-formula><tex-math id="M00034" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {W}^{(1)} \in \mathbb {R}^{kxh}$\end{document}</tex-math></inline-formula>, <inline-formula><tex-math id="M00035" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {W}^{(2)} \in \mathbb {R}^{hxc}$\end{document}</tex-math></inline-formula> were the trainable weights for the first and second layers, respectively, where <italic toggle="yes">h</italic> was the hidden layer size and <italic toggle="yes">c</italic> was the number of classes to predict (namely, Basal-like, Luminal-A, Luminal-B, HER2-Enriched, and Normal-like, with <italic toggle="yes">c</italic> = 5). The loss function was calculated by cross-entropy error. Adam optimization (<xref rid="B34" ref-type="bibr">34</xref>) was used as the state-of-the-art for stochastic gradient descent algorithm and dropout was added for the first GCN layer. Early stopping was used with the patience of 30 forced to have at least 200 epochs.</p>
      <p>We split the total samples into training, validation, and test sets. This splitting was stratified, that is, keeping the same ratio of the subtype labels in the original data for each split. We kept the test set only for final evaluation of the tool. Training and validation splits are randomly selected for each run as stratified. For the breast cancer subtyping, we split 20% of the total samples as a test set. The remaining 80% of the samples were used for training (60%) and validation (20%). For IMDB and ACM datasets, we used the same data splits in (<xref rid="B31" ref-type="bibr">31</xref>). To tune the hyperparameters of the GCN model (i.e., hidden layer size and learning rate), for each run, SUPREME repeated an evaluation metric (i.e., macro-averaged F1 (macro F1) score) 10 times for each hyperparameter combination (<xref rid="sup1" ref-type="supplementary-material">Supplemental File 2</xref>) and selected the hyperparameter combination giving the best median macro F1 score on the validation data to generate the final model.</p>
      <p>Similarly applying the methodology for other datatypes, we generated seven different GCN models on TCGA data. Repeating the same procedure for other datasets, we obtained five models on METABRIC data, three models on the combined data, and two models on ACM and IMDB data. These final models were used to extract network-specific patient embeddings to use in the downstream prediction task.</p>
    </sec>
    <sec id="SEC2-4">
      <title>Training predictive models using node embedding combinations</title>
      <p>For each combination of node embeddings from <inline-formula><tex-math id="M00036" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$d$\end{document}</tex-math></inline-formula> datatypes, we concatenated them with the raw features and trained prediction models (having <inline-formula><tex-math id="M00037" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$2^{d}-1$\end{document}</tex-math></inline-formula> models). Specifically, we had 127, 31, seven, three, and three SUPREME models for TCGA, METABRIC, the combined data (TCGA+METABRIC), ACM, and IMDB datasets, respectively.</p>
      <p>We tested SUPREME with several ML methods namely, XGBoost, Support Vector Machine (SVM), Random Forest (RF), and Multi-layer Perceptron (MLP). For all datasets, we decided to use MLP as it gave consistently high performance (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref> and discussion section for the details).</p>
      <p>We did hyperparameter tuning for the prediction task, similar to GCN hyperparameter tuning in the previous step. We used the training and validation cohort to tune the hyperparameters (e.g., hidden layer size and learning rate) of the final model, where training and validation splits were randomly selected as stratified. We repeated the SUPREME run 10 times for each hyperparameter combination and used the hyperparameter combination giving the best median macro F1 score on the validation data. Using this hyperparameter combination, the final model was built and evaluated 10 times on the test data, which was never seen during training and hyperparameter tuning. The evaluation metrics (macro F1, weighted-average F1 (weighted F1) score, and accuracy) were obtained from the median of these 10 runs.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="SEC3">
    <title>RESULTS</title>
    <p>We introduced a novel node classification framework, called SUPREME, that utilizes graph convolutions on multiple datatype-specific networks that are annotated with multi-modal datatypes as node features. This framework is model-agnostic and could be applied to any classification problem with properly processed datatypes and networks. In this work, SUPREME was applied specifically to the breast cancer subtype prediction problem by applying convolution on patient similarity networks constructed based on multiple biological datatypes from breast tumor samples (Figure <xref rid="F1" ref-type="fig">1</xref>). We also evaluated SUPREME on ACM and IMDB datasets demonstrating the outperformance of SUPREME in different domains.</p>
    <fig position="float" id="F1">
      <label>Figure 1.</label>
      <caption>
        <p>SUPREME pipeline for breast cancer subtype prediction. SUPREME extracts feature from available datatypes and generates patient similarity networks where nodes are annotated with features from all datatypes. Utilizing graph convolutions on each patient similarity network, patient embeddings are generated. To provide extensive evaluations of subtype prediction, a machine learning model is trained for each combination of patient embeddings and raw multiomics features. [<italic toggle="yes">u</italic><sub><italic toggle="yes">k</italic></sub> is <italic toggle="yes">k</italic>th patient, <italic toggle="yes">i</italic><sup>(<italic toggle="yes">j</italic>)</sup> is a raw feature matrix for the <italic toggle="yes">j</italic>th datatype where each row is <inline-formula><tex-math id="M00038" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$i_k^{(j)}$\end{document}</tex-math></inline-formula> corresponding to the feature vector of the <italic toggle="yes">k</italic>th patient for the <italic toggle="yes">j</italic>th datatype. Similarly, <italic toggle="yes">z</italic><sup>(<italic toggle="yes">j</italic>)</sup> is a node embedding matrix for the <italic toggle="yes">j</italic>th datatype-specific network where each row is <inline-formula><tex-math id="M00039" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$z_k^{(j)}$\end{document}</tex-math></inline-formula> corresponding to the embedding of the <italic toggle="yes">k</italic><sup><italic toggle="yes">th</italic></sup> patient.]</p>
      </caption>
      <graphic xlink:href="lqad063fig1" position="float"/>
    </fig>
    <sec id="SEC3-1">
      <title>SUPREME outperformed the cancer subtype prediction tools and baseline methods</title>
      <p>For the breast cancer subtyping task, we compared the performance of SUPREME on three different datasets with seven other cancer (sub)type prediction tools and baseline methods, namely Deep cancer subtype classification (DeepCC) (<xref rid="B35" ref-type="bibr">35</xref>), GCN-based classification (GCNC) (<xref rid="B26" ref-type="bibr">26</xref>), MOGONET (<xref rid="B27" ref-type="bibr">27</xref>), MLP, RF, SVM, and XGBoost. For each dataset combination, SUPREME builds a separate cancer subtype prediction model. For ML-based baseline methods (i.e., MLP, RF, SVM, and XGBoost), we integrated only the raw features from the selected combination and did the prediction with those features. MOGONET utilizes GCN on multiomics data utilizing datatype-specific embedding predictions. GCNC leverages GCN with gene expression features on protein-protein interaction (PPI)- or coexpression-based gene network, while DeepCC utilizes only gene expression datatype with pathway activity transformation through an MLP model. Therefore, we had only two classification models for GCNC: <italic toggle="yes">GCNC</italic><sub><italic toggle="yes">PPI</italic></sub> with the PPI network and <italic toggle="yes">GCNC</italic><sub><italic toggle="yes">COE</italic></sub> with the coexpression network, and one model for DeepCC. To see the impact of the integration of raw features into the embeddings, we also trained models without integrating raw features with patient embeddings, called <italic toggle="yes">SUPREME-</italic>. We ran SUPREME, SUPREME-, and the other tools for all the combinations of available datatypes. Even though MOGONET is applicable to any number of datatypes, we could not run the tool for the models with more than five datatypes (waiting time was more than two days per combination), thus we had only 31 different models for TCGA data, while we had all models for METABRIC and the combined data.</p>
      <p>SUPREME and SUPREME- outperformed all other multiomics integration methods for three datasets in terms of macro F1, accuracy, and weighted F1 (Figure <xref rid="F2" ref-type="fig">2</xref>, Supplementary Figures S3 and S4). SUPREME significantly outperformed MLP, which utilizes raw features only in all datasets, showing the importance of GCN utilization. We observed that SUPREME significantly outperformed SUPREME- for all three datasets.</p>
      <fig position="float" id="F2">
        <label>Figure 2.</label>
        <caption>
          <p>Classification results. Violin plot of macro F1 scores obtained from 127 different models including all different combinations of datatypes as compared to the cancer subtype prediction tools and baseline supervised methods on TCGA data. DeepCC and GCNC violin plots show the distribution of macro F1 scores of ten runs of a single model as they can only utilize gene expression datatype. The significance level was measured with respect to SUPREME (Wilcoxon rank-sum test p-value to compare the distribution of violin plots representing the significance &lt; 0.001 by ***, else if &lt; 0.01 by **, and else if &lt; 0.05 by *). [MLP: Multi-layer Perceptron, RF: Random Forest, SUPREME-: SUPREME without raw feature integration, SVM: Support Vector Machine]</p>
        </caption>
        <graphic xlink:href="lqad063fig2" position="float"/>
      </fig>
      <p>We ran the tools that utilize only gene expression datatype and evaluated their performance (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>). For TCGA data, SUPREME achieved significantly higher performance than DeepCC and GCNC models, while performance on METABRIC and the combined data was comparable or superior (Figures <xref rid="F2" ref-type="fig">2</xref> and S3).</p>
      <p>In addition, we checked the subtype-specific F1 scores, and had consistent and higher performance across all subtypes, mostly having significant differences (Supplementary Figure S5). Specifically, on TCGA data, we had significantly better performance than all other tools for all subtypes in terms of subtype-specific F1 scores. Particularly, SUPREME had a significantly higher subtype-specific F1 score than all other tools on the Normal-like subtype for all three datasets. Considering that the Normal-like subtype had the smallest sample size in all three datasets (4% of the samples from TCGA, 8% from METABRIC, and 7% from the combined data), achieving this performance increase indicates SUPREME’s robustness even for minority classes.</p>
    </sec>
    <sec id="SEC3-2">
      <title>SUPREME had consistently high performance even with single models</title>
      <p>To see SUPREME’s performance with a single datatype, we investigated models generated with only one datatype, called <italic toggle="yes">single model</italic>. We compared SUPREME with an MLP-based model trained using a single datatype to show the impact of our GCN-based approach. To show the impact of different approaches with one datatype, we compared our single models against MOGONET and our EXP-based model with DeepCC and GCNC models. We conducted these experiments for all three datasets.</p>
      <p>Based on the single model results, SUPREME outperformed MOGONET for all single models from all three datasets (Table <xref rid="tbl2" ref-type="table">2</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S3–S5</xref>). Also, SUPREME outperformed MLP (six out of seven models for TCGA data, three out of five for METABRIC data, and two out of three models for the combined data), or had comparable performance, while MLP had extremely poor performance on some datatypes, showing the importance of GCN-based approach.</p>
      <table-wrap position="float" id="tbl2">
        <label>Table 2.</label>
        <caption>
          <p>Single model results on TCGA data. Macro F1 scores for each model with a single dataype. See Figure <xref rid="F1" ref-type="fig">1</xref> for the abbreviations of the datatypes. [MLP: multi-layer perceptron]</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Method</th>
              <th rowspan="1" colspan="1">CLI</th>
              <th rowspan="1" colspan="1">CNA</th>
              <th rowspan="1" colspan="1">COE</th>
              <th rowspan="1" colspan="1">EXP</th>
              <th rowspan="1" colspan="1">MET</th>
              <th rowspan="1" colspan="1">MIR</th>
              <th rowspan="1" colspan="1">MUT</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">SUPREME</td>
              <td rowspan="1" colspan="1">0.68 ± 0.04</td>
              <td rowspan="1" colspan="1">
                <bold>0.80 ± 0.03</bold>
              </td>
              <td rowspan="1" colspan="1">0.76 ± 0.04</td>
              <td rowspan="1" colspan="1">
                <bold>0.84 ± 0.02</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.79 ± 0.03</bold>
              </td>
              <td rowspan="1" colspan="1">0.73 ± 0.02</td>
              <td rowspan="1" colspan="1">
                <bold>0.75 ± 0.03</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SUPREME-</td>
              <td rowspan="1" colspan="1">
                <bold>0.72 ± 0.02</bold>
              </td>
              <td rowspan="1" colspan="1">0.77 ± 0.02</td>
              <td rowspan="1" colspan="1">
                <bold>0.77 ± 0.04</bold>
              </td>
              <td rowspan="1" colspan="1">0.77 ± 0.05</td>
              <td rowspan="1" colspan="1">
                <bold>0.79 ± 0.04</bold>
              </td>
              <td rowspan="1" colspan="1">0.70 ± 0.02</td>
              <td rowspan="1" colspan="1">0.74 ± 0.04</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MLP</td>
              <td rowspan="1" colspan="1">0.46 ± 0.07</td>
              <td rowspan="1" colspan="1">0.53 ± 0.04</td>
              <td rowspan="1" colspan="1">0.59 ± 0.02</td>
              <td rowspan="1" colspan="1">0.82 ± 0.03</td>
              <td rowspan="1" colspan="1">0.69 ± 0.04</td>
              <td rowspan="1" colspan="1">
                <bold>0.74 ± 0.04</bold>
              </td>
              <td rowspan="1" colspan="1">0.28 ± 0.06</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MOGONET</td>
              <td rowspan="1" colspan="1">0.41 ± 0.01</td>
              <td rowspan="1" colspan="1">0.52 ± 0.01</td>
              <td rowspan="1" colspan="1">0.57 ± 0.01</td>
              <td rowspan="1" colspan="1">0.75 ± 0.01</td>
              <td rowspan="1" colspan="1">0.61 ± 0.03</td>
              <td rowspan="1" colspan="1">0.71 ± 0.03</td>
              <td rowspan="1" colspan="1">0.34 ± 0.01</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>There was no clear winner for the comparison of the SUPREME EXP-based model with DeepCC and GCNC models. In terms of macro F1 score, SUPREME outperformed both methods on TCGA data and GCNC (1 draw, 1 win) on the combined data. (Table <xref rid="tbl2" ref-type="table">2</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S3–S5</xref>). This could be because DeepCC and GCNC utilize pathway activation, PPI network, or coexpression network in addition to gene expression datatype. Nonetheless, by utilizing more datatypes SUPREME outperformed or was on par with both tools for all datasets (Figure <xref rid="F2" ref-type="fig">2</xref>, Supplementary Figures S3 and S4).</p>
      <p>EXP-based models had the highest macro F1 score for all three datasets for all methods (Table <xref rid="tbl2" ref-type="table">2</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tabless S3, S4, and S5</xref>). The only exception is that SUPREME- MET-based model had slightly higher performance than SUPREME- EXP-based model on TCGA data. High performance of EXP-based models is not surprising as the breast cancer subtype labels are based on gene expression data. We observed that SUPREME usually outperformed SUPREME- on single models, which indicates that utilizing raw features usually improves the model performance. On the other hand, there were few cases where adding raw features dropped the performance (e.g., CLI-based models on TCGA data). By examining SUPREME- and MLP model performances, we compared the predictive power of patient embeddings with raw features. We observed that patient embedding features were more useful than raw features with few exceptions, such as microRNA expression- (MIR) and EXP-based models on TCGA data, copy number aberration (CNA)-based on METABRIC data, and CLI-based model on the combined data. Specifically on TCGA, we see that CLI-based embedding was more informative than CLI-based features. For CNA- and mutation (MUT)-based models, embeddings were more useful than raw features, but we observed that integrating raw features to embeddings further improved the performance. Similarly, although for the EXP-based model on TCGA data, embeddings were less informative than raw features, integrating them improved the performance.</p>
    </sec>
    <sec id="SEC3-3">
      <title>SUPREME had significant survival differences between predicted subtypes consistently</title>
      <p>To measure the ability of the supervised methods to differentiate samples based on survival, we predicted the subtype labels for each data modality combination and performed the survival analysis. In addition to the supervised methods, we also included the state-of-the-art unsupervised tools that are specifically applied to cancer subtyping (i.e., iClusterPlus (<xref rid="B13" ref-type="bibr">13</xref>), SNF (<xref rid="B14" ref-type="bibr">14</xref>), and PINSPlus (<xref rid="B15" ref-type="bibr">15</xref>)) and an algorithmically-relevant clustering method (i.e. affinity propagation (AP) clustering). AP clustering is relevant because it uses a message-passing strategy to find the cluster representatives and the best representative for each node. We obtained five clusters from the unsupervised methods to match the number of PAM50 subtypes and checked the survival differences for these obtained clusters. This analysis was only applied to the results on TCGA data where patient survival data were available. To check the statistical significance of survival differences between subtypes, we applied the log-rank test to compute p-values. Details of survival analysis are in <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods 1.3</xref>.</p>
      <p>The results showed that SUPREME’s predicted subtypes consistently had significant differences in survival rates and significantly outperformed all other nine methods in terms of the <italic toggle="yes">P</italic>-value (Figure <xref rid="F3" ref-type="fig">3</xref>). SUPREME had 0.0035 as the lowest <italic toggle="yes">P</italic>-value (when integrating CNA-, COE-, MET- and MUT-based patient embeddings) and 0.0131 as the median <italic toggle="yes">P</italic>-value (Supplementary Figure S6A for the Kaplan–Meier plot). Similarly for SUPREME-, we had 0.0018 as the lowest <italic toggle="yes">P</italic>-value (when integrating CNA- and COE-based patient embeddings), and 0.0147 as the median <italic toggle="yes">P</italic>-value. Interestingly, SUPREME had a more significant survival difference than the survival difference between ground truth (i.e., PAM50) labels (Supplementary Figure S6B for the Kaplan–Meier plot for PAM50 subtypes).</p>
      <fig position="float" id="F3">
        <label>Figure 3.</label>
        <caption>
          <p>Survival analysis results violin plot of the log-rank <italic toggle="yes">P</italic>-value obtained from survival analysis for the SUPREME models as compared to the cancer subtype prediction/clustering tools and baseline methods. Significance level was measured with respect to SUPREME (Wilcoxon rank-sum test <italic toggle="yes">P</italic>-value to compare the distribution of violin plots representing the significance &lt;0.001 by ***, else if &lt;0.01 by **, and else if &lt;0.05 by *). The continuous line shows the significance level of 0.05 and the dashed line shows the ground truth’s significance level. The below figure focuses on the significant survival <italic toggle="yes">P</italic>-values (&lt;0.05) [AP: affinity propagation, MLP: multi-layer perceptron, RF: random forest, SNF: similarity network fusion, SUPREME: SUPREME without raw feature integration, SVM: support vector machine].</p>
        </caption>
        <graphic xlink:href="lqad063fig3" position="float"/>
      </fig>
      <p>Specifically, 106 out of 127 SUPREME models had a lower <italic toggle="yes">P</italic>-value than the p-value for ground truth. For 57% of those models, we had CNA-based embedding selected. It is followed by 52% from COE-, CLI- and MET-based embeddings. This might suggest that those embeddings could contribute more to differentiating survival differences between subtypes.</p>
      <p>AP, iClusterPlus, MOGONET, and SNF methods had a wide range of <italic toggle="yes">P</italic>-values, while SUPREME, MLP, SVM, and XGBoost had mostly significant <italic toggle="yes">P</italic>-values (≤0.05) with a median lower than the significance level of the ground truth. SUPREME was better than SUPREME-, but the difference was not significant.</p>
      <p>Using support from the predicted subtypes by each model in SUPREME, we computed an ensembled consensus subtype based on majority voting for each patient (<xref rid="sup1" ref-type="supplementary-material">Supplemental File 3</xref>) and checked the survival difference between these consensus subtypes. Once again, we observed a significant (<italic toggle="yes">P</italic>-value = 0.01) survival difference between consensus subtypes (Supplementary Figure S6C). We also observed that 882 out of 1022 patients had the same subtype prediction across all 127 models showing the robustness of SUPREME predictions.</p>
    </sec>
    <sec id="SEC3-4">
      <title>Feature/omics importance analysis</title>
      <p>In this section, we investigated the importance of each network-specific embedding and datatype-specific features.</p>
      <sec id="SEC3-4-1">
        <title>Impact of network-specific patient embeddings</title>
        <p>We investigated the contribution of each patient embedding on the model performance by comparing the models built using a patient embedding from datatype <inline-formula><tex-math id="M00040" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}$\end{document}</tex-math></inline-formula> and without using that embedding. Among all <inline-formula><tex-math id="M00041" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$2^{d}-1$\end{document}</tex-math></inline-formula> models, <inline-formula><tex-math id="M00042" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$2^{d-1}$\end{document}</tex-math></inline-formula> models had the patient embedding obtained from a datatype <inline-formula><tex-math id="M00043" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}$\end{document}</tex-math></inline-formula>, called <inline-formula><tex-math id="M00044" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$with\mathbb {X}_n$\end{document}</tex-math></inline-formula>. The remaining <inline-formula><tex-math id="M00045" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$2^{d-1}-1$\end{document}</tex-math></inline-formula> models did not have the patient embedding obtained from <inline-formula><tex-math id="M00046" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}$\end{document}</tex-math></inline-formula>, called <inline-formula><tex-math id="M00047" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$no\mathbb {X}_n$\end{document}</tex-math></inline-formula>. For each datatype <inline-formula><tex-math id="M00048" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}$\end{document}</tex-math></inline-formula>, we compared <inline-formula><tex-math id="M00049" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$no\mathbb {X}_n$\end{document}</tex-math></inline-formula> models against <inline-formula><tex-math id="M00050" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$with\mathbb {X}_n$\end{document}</tex-math></inline-formula> models, showing the importance of <inline-formula><tex-math id="M00051" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}$\end{document}</tex-math></inline-formula>-specific patient embedding. We did this analysis on SUPREME- (i.e., without integrating the raw features) to ensure that differences were due to the patient embeddings only.</p>
        <p>The results on TCGA data showed that the performance of models increased or stayed the same with the inclusion of patient embeddings from all datatypes except for gene expression (Figure <xref rid="F4" ref-type="fig">4</xref>). The inclusion of EXP-based embedding showed a significant decrease in the model performance. The exclusion of CLI- and CNA-based patient embeddings had a significant drop in the model performance. Those findings agree with single model results.</p>
        <fig position="float" id="F4">
          <label>Figure 4.</label>
          <caption>
            <p>Analysis of network-specific patient embeddings. Violin plot of macro F1 scores of SUPREME- performance for the models integrated with a specific patient embedding from each datatype (<italic toggle="yes">with</italic><inline-formula><tex-math id="M00052" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}_n$\end{document}</tex-math></inline-formula> models, where <inline-formula><tex-math id="M00053" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}$\end{document}</tex-math></inline-formula> is the datatype whose embedding is included) versus excluding that embedding (<italic toggle="yes">no</italic><inline-formula><tex-math id="M00054" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}_n$\end{document}</tex-math></inline-formula> models) on TCGA data. Significance level was measured between <italic toggle="yes">with</italic> and <italic toggle="yes">no</italic> cases of the same datatype (Wilcoxon rank-sum test p-value to compare the distribution of violin plots representing the significance &lt;0.001 by ***, else if &lt;0.01 by **, and else if &lt;0.05 by *). See Figure <xref rid="F1" ref-type="fig">1</xref> for the abbreviations of datatypes. [SUPREME-: SUPREME without raw feature integration]</p>
          </caption>
          <graphic xlink:href="lqad063fig4" position="float"/>
        </fig>
        <p>For METABRIC data, the inclusion of COE- and EXP-based embeddings increased the performance, while the other embeddings did not affect the performance much (Supplementary Figure S7A). For the combined data, MUT- and EXP-based embeddings showed higher performance when included, whereas the inclusion of CLI-based embedding did not affect the performance much (Supplementary Figure S7B).</p>
        <p>In addition, we analyzed SUPREME results for TCGA data in terms of the best- and worst-performing models. Specifically, we had 31 top models with a macro F1 score is ≥0.88, and 30 bottom models with a macro F1 score is ≤0.83. We counted how many times each datatype occurred in the top and bottom models. CNA- and CLI-based embeddings were used for 28 and 19 out of 31 top models, respectively. The least occurred embedding was EXP-based with only six models out of 31. For the bottom models, we had 25 models from EXP-based embedding, while we had the least occurred embedding from CNA-based embedding with only five models. This analysis showed that CNA-based embedding was the most selected to have higher performance, while EXP-based embedding was rarely selected, supporting our findings in this section and in single model analysis.</p>
      </sec>
      <sec id="SEC3-4-2">
        <title>Impact of features from each datatype</title>
        <p>To see the impact of the features from each datatype, we ran SUPREME excluding the features from every single datatype separately. For each datatype <inline-formula><tex-math id="M00055" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula>, we excluded <inline-formula><tex-math id="M00056" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula>-specific node features from patient similarity networks and also did not integrate them with node embeddings during subtype prediction, called <inline-formula><tex-math id="M00057" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$no\mathbb {Y}_f$\end{document}</tex-math></inline-formula>. Considering that <inline-formula><tex-math id="M00058" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula>-specific patient similarity network was generated based on <inline-formula><tex-math id="M00059" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula>-specific features, we compared only the combinations without <inline-formula><tex-math id="M00060" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula> (<inline-formula><tex-math id="M00061" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$2^{d-1}-1$\end{document}</tex-math></inline-formula> models) to ensure the differences were due to the <inline-formula><tex-math id="M00062" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula>-specific features. We compared <inline-formula><tex-math id="M00063" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$no\mathbb {Y}_f$\end{document}</tex-math></inline-formula> models against the corresponding SUPREME models (called <inline-formula><tex-math id="M00064" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$with\mathbb {Y}_f$\end{document}</tex-math></inline-formula>), to show the importance of <inline-formula><tex-math id="M00065" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula>-specific features.</p>
        <p>When we excluded features from any datatype, we observed a lower or comparable performance (Figures <xref rid="F5" ref-type="fig">5</xref> and Supplementary Figure S8). The performance drop was significant for all the datatypes on TCGA, and gene expression and copy number aberration datatypes on METABRIC (Supplementary Figure S8A). The drop with the exclusion of the gene expression features was more drastic and it was consistent for all three datasets (Supplementary Figure S8B), supporting the importance of gene expression features for breast cancer (in agreement with findings in single model analysis).</p>
        <fig position="float" id="F5">
          <label>Figure 5.</label>
          <caption>
            <p>Analysis of features from each datatype. Violin plot of macro F1 scores for the models excluding the features from each datatype (<italic toggle="yes">no</italic><inline-formula><tex-math id="M00066" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}_f$\end{document}</tex-math></inline-formula> models, where <inline-formula><tex-math id="M00067" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula> is the datatype whose features are completely excluded) versus corresponding SUPREME models (<italic toggle="yes">with</italic><inline-formula><tex-math id="M00068" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}_f$\end{document}</tex-math></inline-formula> models) on TCGA data. Significance level was measured between <italic toggle="yes">with</italic> and <italic toggle="yes">no</italic> cases of the same datatype (Wilcoxon rank-sum test p-value to compare the distribution of violin plots representing the significance &lt;0.001 by ***, else if &lt;0.01 by **, and else if &lt;0.05 by *). See Figure <xref rid="F1" ref-type="fig">1</xref> for the abbreviations of datatypes.</p>
          </caption>
          <graphic xlink:href="lqad063fig5" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec id="SEC3-5">
      <title>Ablation studies</title>
      <p>We compared our tool with its variations when some steps were skipped to assess their importance (Table <xref rid="tbl3" ref-type="table">3</xref>). A comparison of SUPREME with SUPREME- showed the importance of raw feature integration. Also, to show the importance of GCN-based approaches, we trained the same ML algorithm (MLP in our case) using only the raw features and compared it with the SUPREME, which was based on the same raw features and additional patient embeddings.</p>
      <table-wrap position="float" id="tbl3">
        <label>Table 3.</label>
        <caption>
          <p>Ablation studies</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Comparison/Section</th>
              <th rowspan="1" colspan="1">Measures impact of</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">SUPREME vs. SUPREME-</td>
              <td rowspan="1" colspan="1">Raw feature integration</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SUPREME vs. MLP</td>
              <td rowspan="1" colspan="1">GCN utilization</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Single model section</td>
              <td rowspan="1" colspan="1">The used method with only one datatype</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SUPREME vs. MLP in single model section</td>
              <td rowspan="1" colspan="1">GCN utilization with only one datatype</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">This section</td>
              <td rowspan="1" colspan="1">Node features</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>To show the impact of each datatype separately, we demonstrated the performance of SUPREME models based on a single data type. We also compared SUPREME with other methods that can work with a single data modality only. To show the importance of embeddings at a single datatype level, we compared SUPREME with the MLP model trained on the features from the corresponding datatype.</p>
      <p>In addition to these studies, here, we also checked the overall impact of node features on the prediction tasks. To do that, instead of node features, we generated one-hot encoded features and evaluated SUPREME on TCGA data. We had macro F1 score as 0.75 ± 0.01, weighted F1 score as 0.83 ± 0.01, and accuracy as 0.84 ± 0.01. These results suggest that node features were important, dropping the evaluation performance drastically. This was expected as biological features are highly effective in determining the subtypes of breast cancer.</p>
    </sec>
    <sec id="SEC3-6">
      <title>SUPREME was model-agnostic outperforming other approaches in different domains</title>
      <p>To show the model-agnostic feature, we evaluated SUPREME on different domains. For that purpose, we generated two meta-path-based networks from the heterogeneous network of ACM and IMDB data. Since we had only two networks for these datasets, we shared the results for individual networks and the integrated one. Based on all six evaluation metrics, SUPREME outperformed other baseline methods on both datasets (Table <xref rid="tbl4" ref-type="table">4</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S6</xref>). As compared to MLP, we had increased performance showing the importance of graph utilization. Single models from GCN and SUPREME were not as good as the integrated one, showing the importance of SUPREME’s integrative nature.</p>
      <table-wrap position="float" id="tbl4">
        <label>Table 4.</label>
        <caption>
          <p>Macro F1 scores for IMDB and ACM datasets. [Macro F1: Macro-averaged F1 scores, GCN<sub><italic toggle="yes">x</italic></sub>: Result with x<sup>th</sup> network]. *Three results: First row with the first network, second row with the second network, and third row integrating the first and second networks. The first networks are based on movie-director-movie and paper-subject-paper meta-paths; and the second networks are based on movie-actor-movie and paper-author-paper meta-paths in IMDB and ACM datasets, respectively</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Method</th>
              <th rowspan="1" colspan="1">IMDB</th>
              <th rowspan="1" colspan="1">ACM</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">MLP</td>
              <td rowspan="1" colspan="1">0.53 ± 0.01</td>
              <td rowspan="1" colspan="1">0.90 ± 0.01</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SVM</td>
              <td rowspan="1" colspan="1">0.55 ± 0.00</td>
              <td rowspan="1" colspan="1">0.89 ± 0.00</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RF</td>
              <td rowspan="1" colspan="1">0.48 ± 0.00</td>
              <td rowspan="1" colspan="1">0.89 ± 0.00</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GCN<sub>1</sub></td>
              <td rowspan="1" colspan="1">0.56 ± 0.00</td>
              <td rowspan="1" colspan="1">0.70 ± 0.00</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GCN<sub>2</sub></td>
              <td rowspan="1" colspan="1">0.51 ± 0.00</td>
              <td rowspan="1" colspan="1">0.91 ± 0.00</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">0.58 ± 0.01</td>
              <td rowspan="1" colspan="1">0.91 ± 0.01</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SUPREME*</td>
              <td rowspan="1" colspan="1">0.55 ± 0.02</td>
              <td rowspan="1" colspan="1">0.92 ± 0.01</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">
                <bold>0.61 ± 0.02</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.94 ± 0.00</bold>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>According to these results, the first network of IMDB data (the network based on movie-director-movie meta-path) and the second network of ACM data (the network based on paper-author-paper meta-path) were more informative. This is not surprising that movie-director-movie association was more important than movie-actor-movie on movie genre prediction task on IMDB data. This was consistent based on the GCN runs and single models of SUPREME runs. Even though there is not a big difference on individual networks of ACM data for SUPREME, we see a big difference on GCN runs, showing the importance of our methodology utilizing embedding along with node features.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="SEC4">
    <title>DISCUSSION</title>
    <p>In this study, we introduced SUPREME, a novel integrative approach utilizing GCNs on multiple similarity networks where nodes are attributed with multi-modal node features. We applied SUPREME to three different prediction tasks from five different datasets. We observed that SUPREME outperformed other methods on ACM and IMDB data based on six evaluation metrics (Table <xref rid="tbl4" ref-type="table">4</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S6</xref>). On breast cancer subtyping, we compared SUPREME with seven cancer (sub)type prediction tools and baseline methods and observed that SUPREME substantially outperformed or was on par with them based on macro F1 score, accuracy, and weighted F1 score (Figures <xref rid="F2" ref-type="fig">2</xref>, Supplementary Figures S3 and S4, and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>). To demonstrate the consistency of the performance for individual SUPREME models, we shared the distribution of standard deviation of SUPREME models (Supplementary Figures S9, S10, and S11). We differentiated Normal-like subtype, which has the smallest sample size for three datasets, significantly better than all other tools on all three datasets showing SUPREME’s robustness even for minority classes (Supplementary Figure S5). We made SUPREME a publicly available tool at <ext-link xlink:href="https://github.com/bozdaglab/SUPREME" ext-link-type="uri">https://github.com/bozdaglab/SUPREME</ext-link> (under Creative Commons Attribution Non Commercial 4.0 International Public License) for researchers, biologists, and clinicians to utilize.</p>
    <p>We applied survival analysis to see the power of the methods to differentiate subtypes having significant survival differences. Using TCGA data, we compared our tool with nine popular integrative cancer subtype differentiating tools and baseline methods and SUPREME had consistently significant survival differences between predicted subtypes outperforming the other tools (Figure <xref rid="F3" ref-type="fig">3</xref>).</p>
    <p>Based on the majority of predictions, we determined ensemble subtype labels, most of which had high support from individual models (<xref rid="sup1" ref-type="supplementary-material">Supplemental File 3</xref>). We observed that survival difference between these ensemble subtypes was more significant than survival difference between gene expression-based ground truth (i.e., PAM50) subtypes (Supplementary Figure S6). These results suggest that some survival-related characteristics cannot be explained by gene expression data alone. SUPREME was able to extract these survival-related characteristics utilizing additional data modalities. SUPREME’s ensemble label predictions that were different from ground truth with high support could be further examined by biologists and clinicians.</p>
    <p>To show the effect of main steps of SUPREME, we performed an ablation study. In addition, we analyzed datatype-specific embeddings and datatype-specific features. We found that gene expression features were highly important for single models and overall, as expected for breast cancer. Findings about the important embeddings of datasets were supported by SUPREME- single models, where models were fed by only one embedding. We observed that patient embeddings were mostly more informative than raw features. Integrating raw features with patient embeddings usually improved the model performance (Figures <xref rid="F2" ref-type="fig">2</xref> and Supplementary Figure S3) except for raw features from few datatypes in single datatype-based models (Table <xref rid="tbl2" ref-type="table">2</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S3, S4, and S5</xref>).</p>
    <p>To compare the performance when we do not utilize the local neighborhood, we ran SUPREME- on TCGA data with the EXP-based single model when we do not have any neighbors than the patient itself. In that model, we had a macro F1 score of 0.85 ± 0.02 for SUPREME-, which was much higher than the original EXP-based model of SUPREME-. This model was even better than the EXP-based single model of SUPREME. This might suggest that EXP-based patient features themselves could perform better than neighborhood-convolved features because the ground truth utilizes patient features themselves to decide the subtype labels. Similarly, because of that, we might see a performance improvement when we add EXP-based raw features.</p>
    <p>SUPREME provides four options of ML algorithms to integrate embeddings and raw features, namely MLP, RF, SVM, and XGBoost. We ran SUPREME with all these choices and compared performances (<xref rid="sup1" ref-type="supplementary-material">Supplemental File 1</xref>, Supplementary Figures S1 and S2, and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>). RF and XGBoost had a low performance for some models. Overall, SVM had a good performance on every three datasets, however, it did not converge for some models. For this study, we chose MLP due to its high and consistent prediction performance for all three datasets and its low running time.</p>
    <p>In our experiments, we observed a high number of edges in MUT-based patient similarity networks as there were many patient pairs with the same similarity. Furthermore, the MUT-based models on TCGA data had high predictive performance, whereas these models had low predictive performance on METABRIC and the combined datasets. These discrepancies were mainly due to the sparse nature of the binary mutation features. For the special datatypes with binary-like sparse values like mutation, patient similarity networks and extracted features could be generated in a more sophisticated way such as based on the functional effect of these mutations (<xref rid="B36" ref-type="bibr">36–39</xref>).</p>
    <p>SUPREME is extendable to any number of datatypes to integrate. For cases where many datasets are integrated, to avoid potential overfitting, SUPREME provides an optional feature selection step for raw features before training the final prediction model. Users could skip raw feature integration altogether when network-specific patient embeddings provide sufficient discriminatory power. Users could run SUPREME on their training/validation data by enabling/disabling these features to optimize their models. In addition, users could perform ablation studies on SUPREME to determine the most effective data modalities and their combinations. Depending on these results, for the final prediction, users could rely on the most effective model or an ensemble model utilizing the most promising features and networks.</p>
    <p>As a future direction, SUPREME could utilize attention mechanisms (<xref rid="B40" ref-type="bibr">40–42</xref>), which allows getting weighted contributions from different datatypes, and also weighted neighborhoods from networks. In addition to multiomics datatypes, there are some regulatory relations such as competing endogenous RNA (ceRNA) regulation, which has been recently discovered with important insights into cancer (<xref rid="B43" ref-type="bibr">43</xref>). In our recent work, we inferred ceRNA interactions in breast cancer (<xref rid="B44" ref-type="bibr">44</xref>). To adopt this kind of regulatory relations, SUPREME could be improved to utilize patient similarity networks based on gene regulatory interactions and more complex patient relations. By improving the existing methodologies with recent advances in the literature, we can obtain more clear cancer subtype groups to pave the way for precision medicine.</p>
  </sec>
  <sec sec-type="data-availability" id="SEC5">
    <title>DATA AVAILABILITY</title>
    <p>TCGA data used in this article are available in The Cancer Genome Atlas project data portal at <ext-link xlink:href="https://portal.gdc.cancer.gov/" ext-link-type="uri">https://portal.gdc.cancer.gov/</ext-link>. The METABRIC data used in this article are deposited at the European Genome-Phenome Archive <ext-link xlink:href="http://www.ebi.ac.uk/ega/" ext-link-type="uri">http://www.ebi.ac.uk/ega/</ext-link>, which is hosted by the European Bioinformatics Institute, under accession number EGAS00000000083 [2]. The source code for our tool is available at <ext-link xlink:href="https://github.com/bozdaglab/SUPREME" ext-link-type="uri">https://github.com/bozdaglab/SUPREME</ext-link> (permanent doi: 10.6084/m9.figshare.23304062).</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>lqad063_Supplemental_Files</label>
      <media xlink:href="lqad063_supplemental_files.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ACK1">
    <title>ACKNOWLEDGEMENTS</title>
    <p>The authors would like to thank Ethan Rebello for his contributions to testing SUPREME and working on adding GPU support to it.</p>
  </ack>
  <sec id="SEC6">
    <title>SUPPLEMENTARY DATA</title>
    <p><ext-link xlink:href="https://academic.oup.com/nargab/article-lookup/doi/10.1093/nargab/lqad063#supplementary-data" ext-link-type="uri">Supplementary Data</ext-link> are available at NARGAB Online.</p>
  </sec>
  <sec id="SEC7">
    <title>FUNDING</title>
    <p>National Institute of General Medical Sciences of the National Institutes of Health [R35GM133657 to S.B.]; a summer research fellowship (to Z.N.K.) by the BioDiscovery Institute at the University of North Texas.</p>
    <p><italic toggle="yes">Conflict of interest statement</italic>. None declared.</p>
  </sec>
  <ref-list id="REF1">
    <title>REFERENCES</title>
    <ref id="B1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Waks</surname><given-names>A.G.</given-names></string-name>, <string-name><surname>Winer</surname><given-names>E.P.</given-names></string-name></person-group><article-title>Breast cancer treatment: a review</article-title>. <source>JAMA</source>. <year>2019</year>; <volume>321</volume>:<fpage>288</fpage>–<lpage>300</lpage>.<pub-id pub-id-type="pmid">30667505</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Curtis</surname><given-names>C.</given-names></string-name>, <string-name><surname>Shah</surname><given-names>S.P.</given-names></string-name>, <string-name><surname>Chin</surname><given-names>S.-F.</given-names></string-name>, <string-name><surname>Turashvili</surname><given-names>G.</given-names></string-name>, <string-name><surname>Rueda</surname><given-names>O.M.</given-names></string-name>, <string-name><surname>Dunning</surname><given-names>M.J.</given-names></string-name>, <string-name><surname>Speed</surname><given-names>D.</given-names></string-name>, <string-name><surname>Lynch</surname><given-names>A.G.</given-names></string-name>, <string-name><surname>Samarajiwa</surname><given-names>S.</given-names></string-name>, <string-name><surname>Yuan</surname><given-names>Y.</given-names></string-name><etal>et al</etal>.</person-group><article-title>The genomic and transcriptomic architecture of 2,000 breast tumours reveals novel subgroups</article-title>. <source>Nature</source>. <year>2012</year>; <volume>486</volume>:<fpage>346</fpage>–<lpage>352</lpage>.<pub-id pub-id-type="pmid">22522925</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Verhaak</surname><given-names>R.G.</given-names></string-name>, <string-name><surname>Hoadley</surname><given-names>K.A.</given-names></string-name>, <string-name><surname>Purdom</surname><given-names>E.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>V.</given-names></string-name>, <string-name><surname>Qi</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Wilkerson</surname><given-names>M.D.</given-names></string-name>, <string-name><surname>Miller</surname><given-names>C.R.</given-names></string-name>, <string-name><surname>Ding</surname><given-names>L.</given-names></string-name>, <string-name><surname>Golub</surname><given-names>T.</given-names></string-name>, <string-name><surname>Mesirov</surname><given-names>J.P.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Integrated genomic analysis identifies clinically relevant subtypes of glioblastoma characterized by abnormalities in PDGFRA, IDH1, EGFR, and NF1</article-title>. <source>Cancer Cell</source>. <year>2010</year>; <volume>17</volume>:<fpage>98</fpage>–<lpage>110</lpage>.<pub-id pub-id-type="pmid">20129251</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Noushmehr</surname><given-names>H.</given-names></string-name>, <string-name><surname>Weisenberger</surname><given-names>D.J.</given-names></string-name>, <string-name><surname>Diefes</surname><given-names>K.</given-names></string-name>, <string-name><surname>Phillips</surname><given-names>H.S.</given-names></string-name>, <string-name><surname>Pujara</surname><given-names>K.</given-names></string-name>, <string-name><surname>Berman</surname><given-names>B.P.</given-names></string-name>, <string-name><surname>Pan</surname><given-names>F.</given-names></string-name>, <string-name><surname>Pelloski</surname><given-names>C.E.</given-names></string-name>, <string-name><surname>Sulman</surname><given-names>E.P.</given-names></string-name>, <string-name><surname>Bhat</surname><given-names>K.P.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Identification of a CpG island methylator phenotype that defines a distinct subgroup of glioma</article-title>. <source>Cancer Cell</source>. <year>2010</year>; <volume>17</volume>:<fpage>510</fpage>–<lpage>522</lpage>.<pub-id pub-id-type="pmid">20399149</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baysan</surname><given-names>M.</given-names></string-name>, <string-name><surname>Bozdag</surname><given-names>S.</given-names></string-name>, <string-name><surname>Cam</surname><given-names>M.C.</given-names></string-name>, <string-name><surname>Kotliarova</surname><given-names>S.</given-names></string-name>, <string-name><surname>Ahn</surname><given-names>S.</given-names></string-name>, <string-name><surname>Walling</surname><given-names>J.</given-names></string-name>, <string-name><surname>Killian</surname><given-names>J.K.</given-names></string-name>, <string-name><surname>Stevenson</surname><given-names>H.</given-names></string-name>, <string-name><surname>Meltzer</surname><given-names>P.</given-names></string-name>, <string-name><surname>Fine</surname><given-names>H.A.</given-names></string-name></person-group><article-title>G-cimp status prediction of glioblastoma samples using mRNA expression data</article-title>. <source>PloS One</source>. <year>2012</year>; <volume>7</volume>:<fpage>e47839</fpage>.<pub-id pub-id-type="pmid">23139755</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vural</surname><given-names>S.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Guda</surname><given-names>C.</given-names></string-name></person-group><article-title>Classification of breast cancer patients using somatic mutation profiles and machine learning approaches</article-title>. <source>BMC Syst. Biol.</source><year>2016</year>; <volume>10</volume>:<fpage>263</fpage>–<lpage>276</lpage>.</mixed-citation>
    </ref>
    <ref id="B7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Youssef</surname><given-names>Y.M.</given-names></string-name>, <string-name><surname>White</surname><given-names>N.M.</given-names></string-name>, <string-name><surname>Grigull</surname><given-names>J.</given-names></string-name>, <string-name><surname>Krizova</surname><given-names>A.</given-names></string-name>, <string-name><surname>Samy</surname><given-names>C.</given-names></string-name>, <string-name><surname>Mejia-Guerrero</surname><given-names>S.</given-names></string-name>, <string-name><surname>Evans</surname><given-names>A.</given-names></string-name>, <string-name><surname>Yousef</surname><given-names>G.M.</given-names></string-name></person-group><article-title>Accurate molecular classification of kidney cancer subtypes using microRNA signature</article-title>. <source>Eur. Urol.</source><year>2011</year>; <volume>59</volume>:<fpage>721</fpage>–<lpage>730</lpage>.<pub-id pub-id-type="pmid">21272993</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <label>8.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ferlay</surname><given-names>J.</given-names></string-name>, <string-name><surname>Ervik</surname><given-names>M.</given-names></string-name>, <string-name><surname>Lam</surname><given-names>F.</given-names></string-name>, <string-name><surname>Colombet</surname><given-names>M.</given-names></string-name>, <string-name><surname>Mery</surname><given-names>L.</given-names></string-name>, <string-name><surname>Piñeros</surname><given-names>M.</given-names></string-name>, <string-name><surname>Znaor</surname><given-names>A.</given-names></string-name>, <string-name><surname>Soerjomataram</surname><given-names>I.</given-names></string-name>, <string-name><surname>Bray</surname><given-names>F.</given-names></string-name></person-group><article-title>Global cancer observatory: cancer today. Lyon: International Agency for Research on Cancer, 2018</article-title>. <year>2020</year>; </mixed-citation>
    </ref>
    <ref id="B9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anderson</surname><given-names>W.F.</given-names></string-name>, <string-name><surname>Chatterjee</surname><given-names>N.</given-names></string-name>, <string-name><surname>Ershler</surname><given-names>W.B.</given-names></string-name>, <string-name><surname>Brawley</surname><given-names>O.W.</given-names></string-name></person-group><article-title>Estrogen receptor breast cancer phenotypes in the Surveillance, Epidemiology, and End Results database</article-title>. <source>Breast Cancer Res. Treat.</source><year>2002</year>; <volume>76</volume>:<fpage>27</fpage>–<lpage>36</lpage>.<pub-id pub-id-type="pmid">12408373</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dietze</surname><given-names>E.C.</given-names></string-name>, <string-name><surname>Sistrunk</surname><given-names>C.</given-names></string-name>, <string-name><surname>Miranda-Carboni</surname><given-names>G.</given-names></string-name>, <string-name><surname>O’regan</surname><given-names>R.</given-names></string-name>, <string-name><surname>Seewaldt</surname><given-names>V.L.</given-names></string-name></person-group><article-title>Triple-negative breast cancer in African-American women: disparities versus biology</article-title>. <source>Nat. Rev. Cancer</source>. <year>2015</year>; <volume>15</volume>:<fpage>248</fpage>–<lpage>254</lpage>.<pub-id pub-id-type="pmid">25673085</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parker</surname><given-names>J.S.</given-names></string-name>, <string-name><surname>Mullins</surname><given-names>M.</given-names></string-name>, <string-name><surname>Cheang</surname><given-names>M.C.</given-names></string-name>, <string-name><surname>Leung</surname><given-names>S.</given-names></string-name>, <string-name><surname>Voduc</surname><given-names>D.</given-names></string-name>, <string-name><surname>Vickery</surname><given-names>T.</given-names></string-name>, <string-name><surname>Davies</surname><given-names>S.</given-names></string-name>, <string-name><surname>Fauron</surname><given-names>C.</given-names></string-name>, <string-name><surname>He</surname><given-names>X.</given-names></string-name>, <string-name><surname>Hu</surname><given-names>Z.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Supervised risk predictor of breast cancer based on intrinsic subtypes</article-title>. <source>J. Clin. Oncol.</source><year>2009</year>; <volume>27</volume>:<fpage>1160</fpage>.<pub-id pub-id-type="pmid">19204204</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koboldt</surname><given-names>D.</given-names></string-name>, <string-name><surname>Fulton</surname><given-names>R.</given-names></string-name>, <string-name><surname>McLellan</surname><given-names>M.</given-names></string-name>, <string-name><surname>Schmidt</surname><given-names>H.</given-names></string-name>, <string-name><surname>Kalicki-Veizer</surname><given-names>J.</given-names></string-name>, <string-name><surname>McMichael</surname><given-names>J.</given-names></string-name>, <string-name><surname>Fulton</surname><given-names>L.</given-names></string-name>, <string-name><surname>Dooling</surname><given-names>D.</given-names></string-name>, <string-name><surname>Ding</surname><given-names>L.</given-names></string-name>, <string-name><surname>Mardis</surname><given-names>E.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Comprehensive molecular portraits of human breast tumours</article-title>. <source>Nature</source>. <year>2012</year>; <volume>490</volume>:<fpage>61</fpage>–<lpage>70</lpage>.<pub-id pub-id-type="pmid">23000897</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname><given-names>R.</given-names></string-name>, <string-name><surname>Olshen</surname><given-names>A.B.</given-names></string-name>, <string-name><surname>Ladanyi</surname><given-names>M.</given-names></string-name></person-group><article-title>Integrative clustering of multiple genomic data types using a joint latent variable model with application to breast and lung cancer subtype analysis</article-title>. <source>Bioinformatics</source>. <year>2009</year>; <volume>25</volume>:<fpage>2906</fpage>–<lpage>2912</lpage>.<pub-id pub-id-type="pmid">19759197</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>B.</given-names></string-name>, <string-name><surname>Mezlini</surname><given-names>A.M.</given-names></string-name>, <string-name><surname>Demir</surname><given-names>F.</given-names></string-name>, <string-name><surname>Fiume</surname><given-names>M.</given-names></string-name>, <string-name><surname>Tu</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Brudno</surname><given-names>M.</given-names></string-name>, <string-name><surname>Haibe-Kains</surname><given-names>B.</given-names></string-name>, <string-name><surname>Goldenberg</surname><given-names>A.</given-names></string-name></person-group><article-title>Similarity network fusion for aggregating data types on a genomic scale</article-title>. <source>Nat. Methods</source>. <year>2014</year>; <volume>11</volume>:<fpage>333</fpage>–<lpage>337</lpage>.<pub-id pub-id-type="pmid">24464287</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nguyen</surname><given-names>H.</given-names></string-name>, <string-name><surname>Shrestha</surname><given-names>S.</given-names></string-name>, <string-name><surname>Draghici</surname><given-names>S.</given-names></string-name>, <string-name><surname>Nguyen</surname><given-names>T.</given-names></string-name></person-group><article-title>PINSPlus: a tool for tumor subtype discovery in integrated genomic data</article-title>. <source>Bioinformatics</source>. <year>2019</year>; <volume>35</volume>:<fpage>2843</fpage>–<lpage>2846</lpage>.<pub-id pub-id-type="pmid">30590381</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gori</surname><given-names>M.</given-names></string-name>, <string-name><surname>Monfardini</surname><given-names>G.</given-names></string-name>, <string-name><surname>Scarselli</surname><given-names>F.</given-names></string-name></person-group><article-title>A new model for learning in graph domains</article-title>. <source>Proceedings. 2005 IEEE International Joint Conference on Neural Networks</source>. <year>2005</year>; <volume>2</volume>:<fpage>729</fpage>–<lpage>734</lpage>.</mixed-citation>
    </ref>
    <ref id="B17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scarselli</surname><given-names>F.</given-names></string-name>, <string-name><surname>Gori</surname><given-names>M.</given-names></string-name>, <string-name><surname>Tsoi</surname><given-names>A.C.</given-names></string-name>, <string-name><surname>Hagenbuchner</surname><given-names>M.</given-names></string-name>, <string-name><surname>Monfardini</surname><given-names>G.</given-names></string-name></person-group><article-title>The graph neural network model</article-title>. <source>IEEE Trans. Neur. Networ.</source><year>2008</year>; <volume>20</volume>:<fpage>61</fpage>–<lpage>80</lpage>.</mixed-citation>
    </ref>
    <ref id="B18">
      <label>18.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kipf</surname><given-names>T.N.</given-names></string-name>, <string-name><surname>Welling</surname><given-names>M.</given-names></string-name></person-group><article-title>Semi-supervised classification with graph convolutional networks</article-title>. <year>2016</year>; <comment>arXiv doi:</comment><comment>22 February 2017, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.1609.02907</pub-id>.</mixed-citation>
    </ref>
    <ref id="B19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoff</surname><given-names>P.D.</given-names></string-name>, <string-name><surname>Raftery</surname><given-names>A.E.</given-names></string-name>, <string-name><surname>Handcock</surname><given-names>M.S.</given-names></string-name></person-group><article-title>Latent space approaches to social network analysis</article-title>. <source>J. Am. Stat. Assoc.</source><year>2002</year>; <volume>97</volume>:<fpage>1090</fpage>–<lpage>1098</lpage>.</mixed-citation>
    </ref>
    <ref id="B20">
      <label>20.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Perozzi</surname><given-names>B.</given-names></string-name>, <string-name><surname>Al-Rfou</surname><given-names>R.</given-names></string-name>, <string-name><surname>Skiena</surname><given-names>S.</given-names></string-name></person-group><article-title>Deepwalk: online learning of social representations</article-title>. <source>Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</source>. <year>2014</year>; <fpage>701</fpage>–<lpage>710</lpage>.</mixed-citation>
    </ref>
    <ref id="B21">
      <label>21.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Grover</surname><given-names>A.</given-names></string-name>, <string-name><surname>Leskovec</surname><given-names>J.</given-names></string-name></person-group><article-title>node2vec: scalable feature learning for networks</article-title>. <source>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</source>. <year>2016</year>; <fpage>855</fpage>–<lpage>864</lpage>.</mixed-citation>
    </ref>
    <ref id="B22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hamilton</surname><given-names>W.</given-names></string-name>, <string-name><surname>Ying</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Leskovec</surname><given-names>J.</given-names></string-name></person-group><article-title>Inductive representation learning on large graphs</article-title>. <source>Adv. Neural Inf. Process. Syst.</source><year>2017</year>; <volume>30</volume>:<uri xlink:href="https://proceedings.neurips.cc/paper_files/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html">https://proceedings.neurips.cc/paper_files/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html</uri>.</mixed-citation>
    </ref>
    <ref id="B23">
      <label>23.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Rhee</surname><given-names>S.</given-names></string-name>, <string-name><surname>Seo</surname><given-names>S.</given-names></string-name>, <string-name><surname>Kim</surname><given-names>S.</given-names></string-name></person-group><article-title>Hybrid approach of relation network and localized graph convolutional filtering for breast cancer subtype classification</article-title>. <year>2017</year>; <comment>bioRxiv doi:</comment><comment>15 June 2018, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.1711.05859</pub-id>.</mixed-citation>
    </ref>
    <ref id="B24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mohamed</surname><given-names>S.K.</given-names></string-name>, <string-name><surname>Nováček</surname><given-names>V.</given-names></string-name>, <string-name><surname>Nounu</surname><given-names>A.</given-names></string-name></person-group><article-title>Discovering protein drug targets using knowledge graph embeddings</article-title>. <source>Bioinformatics</source>. <year>2020</year>; <volume>36</volume>:<fpage>603</fpage>–<lpage>610</lpage>.<pub-id pub-id-type="pmid">31368482</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zitnik</surname><given-names>M.</given-names></string-name>, <string-name><surname>Agrawal</surname><given-names>M.</given-names></string-name>, <string-name><surname>Leskovec</surname><given-names>J.</given-names></string-name></person-group><article-title>Modeling polypharmacy side effects with graph convolutional networks</article-title>. <source>Bioinformatics</source>. <year>2018</year>; <volume>34</volume>:<fpage>i457</fpage>–<lpage>i466</lpage>.<pub-id pub-id-type="pmid">29949996</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ramirez</surname><given-names>R.</given-names></string-name>, <string-name><surname>Chiu</surname><given-names>Y.-C.</given-names></string-name>, <string-name><surname>Hererra</surname><given-names>A.</given-names></string-name>, <string-name><surname>Mostavi</surname><given-names>M.</given-names></string-name>, <string-name><surname>Ramirez</surname><given-names>J.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Huang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Jin</surname><given-names>Y.-F.</given-names></string-name></person-group><article-title>Classification of cancer types using graph convolutional neural networks</article-title>. <source>Front. Phys.</source><year>2020</year>; <volume>8</volume>:<fpage>203</fpage>.<pub-id pub-id-type="pmid">33437754</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>T.</given-names></string-name>, <string-name><surname>Shao</surname><given-names>W.</given-names></string-name>, <string-name><surname>Huang</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Tang</surname><given-names>H.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>J.</given-names></string-name>, <string-name><surname>Ding</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Huang</surname><given-names>K.</given-names></string-name></person-group><article-title>MOGONET integrates multi-omics data using graph convolutional networks allowing patient classification and biomarker identification</article-title>. <source>Nat. Commun.</source><year>2021</year>; <volume>12</volume>:<fpage>1</fpage>–<lpage>13</lpage>.<pub-id pub-id-type="pmid">33397941</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kursa</surname><given-names>M.B.</given-names></string-name>, <string-name><surname>Rudnicki</surname><given-names>W.R.</given-names></string-name></person-group><article-title>Feature selection with the Boruta package</article-title>. <source>J. Stat. Softw.</source><year>2010</year>; <volume>36</volume>:<fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation>
    </ref>
    <ref id="B29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gower</surname><given-names>J.C.</given-names></string-name></person-group><article-title>A general coefficient of similarity and some of its properties</article-title>. <source>Biometrics</source>. <year>1971</year>; <volume>27</volume>:<fpage>857</fpage>–<lpage>871</lpage>.</mixed-citation>
    </ref>
    <ref id="B30">
      <label>30.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Maechler</surname><given-names>M.</given-names></string-name>, <string-name><surname>Rousseeuw</surname><given-names>P.</given-names></string-name>, <string-name><surname>Struyf</surname><given-names>A.</given-names></string-name>, <string-name><surname>Hubert</surname><given-names>M.</given-names></string-name>, <string-name><surname>Hornik</surname><given-names>K.</given-names></string-name></person-group><article-title>cluster: cluster analysis basics and extensions, R package version 2.1.3</article-title>. <year>2022</year>; <uri xlink:href="https://cran.r-project.org/web/packages/cluster/citation.html">https://cran.r-project.org/web/packages/cluster/citation.html</uri>.</mixed-citation>
    </ref>
    <ref id="B31">
      <label>31.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kesimoglu</surname><given-names>Z.N.</given-names></string-name>, <string-name><surname>Bozdag</surname><given-names>S.</given-names></string-name></person-group><article-title>GRAF: graph attention-aware fusion networks</article-title>. <year>2023</year>; <comment>bioRxiv doi:</comment><comment>29 March 2023, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/pdf/2303.16781.pdf">https://arxiv.org/pdf/2303.16781.pdf</uri>.</mixed-citation>
    </ref>
    <ref id="B32">
      <label>32.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Fey</surname><given-names>M.</given-names></string-name>, <string-name><surname>Lenssen</surname><given-names>J.E.</given-names></string-name></person-group><article-title>Fast graph representation learning with PyTorch Geometric</article-title>. <year>2019</year>; <comment>bioRxiv doi:</comment><comment>25 April 2019, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.1903.02428</pub-id>.</mixed-citation>
    </ref>
    <ref id="B33">
      <label>33.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>M.</given-names></string-name>, <string-name><surname>Zheng</surname><given-names>D.</given-names></string-name>, <string-name><surname>Ye</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Gan</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Li</surname><given-names>M.</given-names></string-name>, <string-name><surname>Song</surname><given-names>X.</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>J.</given-names></string-name>, <string-name><surname>Ma</surname><given-names>C.</given-names></string-name>, <string-name><surname>Yu</surname><given-names>L.</given-names></string-name>, <string-name><surname>Gai</surname><given-names>Y.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Deep graph library: a graph-centric, highly-performant package for graph neural networks</article-title>. <year>2019</year>; <comment>bioRxiv doi:</comment><comment>25 August 2020, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.1909.01315</pub-id>.</mixed-citation>
    </ref>
    <ref id="B34">
      <label>34.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kingma</surname><given-names>D.P.</given-names></string-name>, <string-name><surname>Ba</surname><given-names>J.</given-names></string-name></person-group><article-title>Adam: a method for stochastic optimization</article-title>. <year>2014</year>; <comment>bioRxiv doi:</comment><comment>30 January 2017, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.1412.6980</pub-id>.</mixed-citation>
    </ref>
    <ref id="B35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gao</surname><given-names>F.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>W.</given-names></string-name>, <string-name><surname>Tan</surname><given-names>M.</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>L.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Fessler</surname><given-names>E.</given-names></string-name>, <string-name><surname>Vermeulen</surname><given-names>L.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.</given-names></string-name></person-group><article-title>DeepCC: a novel deep learning-based framework for cancer molecular subtype classification</article-title>. <source>Oncogenesis</source>. <year>2019</year>; <volume>8</volume>:<fpage>1</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">30631034</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khurana</surname><given-names>E.</given-names></string-name>, <string-name><surname>Fu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>J.</given-names></string-name>, <string-name><surname>Gerstein</surname><given-names>M.</given-names></string-name></person-group><article-title>Interpretation of genomic variants using a unified biological network approach</article-title>. <source>PLoS Comput. Biol.</source><year>2013</year>; <volume>9</volume>:<fpage>e1002886</fpage>.<pub-id pub-id-type="pmid">23505346</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kircher</surname><given-names>M.</given-names></string-name>, <string-name><surname>Witten</surname><given-names>D.M.</given-names></string-name>, <string-name><surname>Jain</surname><given-names>P.</given-names></string-name>, <string-name><surname>O’roak</surname><given-names>B.J.</given-names></string-name>, <string-name><surname>Cooper</surname><given-names>G.M.</given-names></string-name>, <string-name><surname>Shendure</surname><given-names>J.</given-names></string-name></person-group><article-title>A general framework for estimating the relative pathogenicity of human genetic variants</article-title>. <source>Nat. Genet.</source><year>2014</year>; <volume>46</volume>:<fpage>310</fpage>–<lpage>315</lpage>.<pub-id pub-id-type="pmid">24487276</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leiserson</surname><given-names>M.D.</given-names></string-name>, <string-name><surname>Vandin</surname><given-names>F.</given-names></string-name>, <string-name><surname>Wu</surname><given-names>H.-T.</given-names></string-name>, <string-name><surname>Dobson</surname><given-names>J.R.</given-names></string-name>, <string-name><surname>Eldridge</surname><given-names>J.V.</given-names></string-name>, <string-name><surname>Thomas</surname><given-names>J.L.</given-names></string-name>, <string-name><surname>Papoutsaki</surname><given-names>A.</given-names></string-name>, <string-name><surname>Kim</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Niu</surname><given-names>B.</given-names></string-name>, <string-name><surname>McLellan</surname><given-names>M.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Pan-cancer network analysis identifies combinations of rare somatic mutations across pathways and protein complexes</article-title>. <source>Nat. Genet.</source><year>2015</year>; <volume>47</volume>:<fpage>106</fpage>–<lpage>114</lpage>.<pub-id pub-id-type="pmid">25501392</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tokheim</surname><given-names>C.J.</given-names></string-name>, <string-name><surname>Papadopoulos</surname><given-names>N.</given-names></string-name>, <string-name><surname>Kinzler</surname><given-names>K.W.</given-names></string-name>, <string-name><surname>Vogelstein</surname><given-names>B.</given-names></string-name>, <string-name><surname>Karchin</surname><given-names>R.</given-names></string-name></person-group><article-title>Evaluating the evaluation of cancer driver genes</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><year>2016</year>; <volume>113</volume>:<fpage>14330</fpage>–<lpage>14335</lpage>.<pub-id pub-id-type="pmid">27911828</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vaswani</surname><given-names>A.</given-names></string-name>, <string-name><surname>Shazeer</surname><given-names>N.</given-names></string-name>, <string-name><surname>Parmar</surname><given-names>N.</given-names></string-name>, <string-name><surname>Uszkoreit</surname><given-names>J.</given-names></string-name>, <string-name><surname>Jones</surname><given-names>L.</given-names></string-name>, <string-name><surname>Gomez</surname><given-names>A.N.</given-names></string-name>, <string-name><surname>Kaiser</surname><given-names>Ł.</given-names></string-name>, <string-name><surname>Polosukhin</surname><given-names>I.</given-names></string-name></person-group><article-title>Attention is all you need</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2017</year>; <volume>30</volume>:<fpage>1</fpage>–<lpage>11</lpage>.</mixed-citation>
    </ref>
    <ref id="B41">
      <label>41.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Veličković</surname><given-names>P.</given-names></string-name>, <string-name><surname>Cucurull</surname><given-names>G.</given-names></string-name>, <string-name><surname>Casanova</surname><given-names>A.</given-names></string-name>, <string-name><surname>Romero</surname><given-names>A.</given-names></string-name>, <string-name><surname>Lio</surname><given-names>P.</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y.</given-names></string-name></person-group><article-title>Graph attention networks</article-title>. <year>2017</year>; <comment>bioRxiv doi:</comment><comment>4 February 2018, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.1710.10903</pub-id>.</mixed-citation>
    </ref>
    <ref id="B42">
      <label>42.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Brody</surname><given-names>S.</given-names></string-name>, <string-name><surname>Alon</surname><given-names>U.</given-names></string-name>, <string-name><surname>Yahav</surname><given-names>E.</given-names></string-name></person-group><article-title>How attentive are graph attention networks</article-title>. <year>2021</year>; <comment>bioRxiv doi:</comment><comment>31 January 2022, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.2105.14491</pub-id>.</mixed-citation>
    </ref>
    <ref id="B43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Salmena</surname><given-names>L.</given-names></string-name>, <string-name><surname>Poliseno</surname><given-names>L.</given-names></string-name>, <string-name><surname>Tay</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Kats</surname><given-names>L.</given-names></string-name>, <string-name><surname>Pandolfi</surname><given-names>P.P.</given-names></string-name></person-group><article-title>A ceRNA hypothesis: the Rosetta Stone of a hidden RNA language</article-title>. <source>Cell</source>. <year>2011</year>; <volume>146</volume>:<fpage>353</fpage>–<lpage>358</lpage>.<pub-id pub-id-type="pmid">21802130</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kesimoglu</surname><given-names>Z.N.</given-names></string-name>, <string-name><surname>Bozdag</surname><given-names>S.</given-names></string-name></person-group><article-title>Crinet: a computational tool to infer genome-wide competing endogenous RNA (ceRNA) interactions</article-title>. <source>Plos One</source>. <year>2021</year>; <volume>16</volume>:<fpage>e0251399</fpage>.<pub-id pub-id-type="pmid">33983999</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">NAR Genom Bioinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">NAR Genom Bioinform</journal-id>
    <journal-id journal-id-type="publisher-id">nargab</journal-id>
    <journal-title-group>
      <journal-title>NAR Genomics and Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2631-9268</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10481254</article-id>
    <article-id pub-id-type="doi">10.1093/nargab/lqad063</article-id>
    <article-id pub-id-type="publisher-id">lqad063</article-id>
    <article-categories>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00030</subject>
        <subject>AcademicSubjects/SCI00980</subject>
        <subject>AcademicSubjects/SCI01060</subject>
        <subject>AcademicSubjects/SCI01140</subject>
        <subject>AcademicSubjects/SCI01180</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Standard Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SUPREME: multiomics data integration using graph convolutional networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-8592-4365</contrib-id>
        <name>
          <surname>Kesimoglu</surname>
          <given-names>Ziynet Nesibe</given-names>
        </name>
        <aff><institution>Department of Computer Science and Engineering, University of North Texas</institution>, Denton, TX, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-4813-4310</contrib-id>
        <name>
          <surname>Bozdag</surname>
          <given-names>Serdar</given-names>
        </name>
        <!--Serdar.Bozdag@unt.edu-->
        <aff><institution>Department of Computer Science and Engineering, University of North Texas</institution>, Denton, TX, <country country="US">USA</country></aff>
        <aff><institution>Department of Mathematics, University of North Texas</institution>, Denton, TX, <country country="US">USA</country></aff>
        <aff><institution>BioDiscovery Institute, University of North Texas</institution>, Denton, TX, <country country="US">USA</country></aff>
        <xref rid="COR1" ref-type="corresp"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="COR1">To whom correspondence should be addressed. Tel: +1 940 369 7581; Email: <email>Serdar.Bozdag@unt.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>6</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-06-28">
      <day>28</day>
      <month>6</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>28</day>
      <month>6</month>
      <year>2023</year>
    </pub-date>
    <volume>5</volume>
    <issue>2</issue>
    <elocation-id>lqad063</elocation-id>
    <history>
      <date date-type="received">
        <day>19</day>
        <month>1</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>08</day>
        <month>5</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>07</day>
        <month>6</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023. Published by Oxford University Press on behalf of NAR Genomics and Bioinformatics.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact <email>journals.permissions@oup.com</email></license-p>
      </license>
    </permissions>
    <self-uri xlink:href="lqad063.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>To pave the road towards precision medicine in cancer, patients with similar biology ought to be grouped into same cancer subtypes. Utilizing high-dimensional multiomics datasets, integrative approaches have been developed to uncover cancer subtypes. Recently, Graph Neural Networks have been discovered to learn node embeddings utilizing node features and associations on graph-structured data. Some integrative prediction tools have been developed leveraging these advances on multiple networks with some limitations. Addressing these limitations, we developed SUPREME, a node classification framework, which integrates multiple data modalities on graph-structured data. On breast cancer subtyping, unlike existing tools, SUPREME generates patient embeddings from multiple similarity networks utilizing multiomics features and integrates them with raw features to capture complementary signals. On breast cancer subtype prediction tasks from three datasets, SUPREME outperformed other tools. SUPREME-inferred subtypes had significant survival differences, mostly having more significance than ground truth, and outperformed nine other approaches. These results suggest that with proper multiomics data utilization, SUPREME could demystify undiscovered characteristics in cancer subtypes that cause significant survival differences and could improve ground truth label, which depends mainly on one datatype. In addition, to show model-agnostic property of SUPREME, we applied it to two additional datasets and had a clear outperformance.</p>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institute of General Medical Sciences</institution>
            <institution-id institution-id-type="DOI">10.13039/100000057</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>R35GM133657</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>University of North Texas</institution>
            <institution-id institution-id-type="DOI">10.13039/100008973</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="12"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="SEC1">
    <title>INTRODUCTION</title>
    <p>Cancer is one of the deadliest diseases for which cancer-causing agents such as oncogenes, mutations, and gene regulatory associations have not been fully demystified. Cancer patients show different characteristics in terms of the progression of disease and response to treatment (<xref rid="B1" ref-type="bibr">1</xref>). Various biological datasets from cancer tissues have been generated to better characterize cancer biology. For instance, The Cancer Genome Atlas (TCGA) project generated over 2.5 petabytes of multiple omics (multiomics) data for thousands of patients from 33 different cancer types (data are available at <ext-link xlink:href="https://portal.gdc.cancer.gov/" ext-link-type="uri">https://portal.gdc.cancer.gov/</ext-link>). Specifically for breast cancer, the Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) has generated four types of multiomics data for thousands of breast tumor samples (<xref rid="B2" ref-type="bibr">2</xref>). Utilizing high-dimensional biological datasets in public databases, computational approaches have been developed to discover subtypes of various cancers (<xref rid="B3" ref-type="bibr">3–5</xref>). Several of the cancer subtype prediction studies rely only on one type of biological datatype (<xref rid="B4" ref-type="bibr">4</xref>,<xref rid="B6" ref-type="bibr">6</xref>,<xref rid="B7" ref-type="bibr">7</xref>). However, each of these datatypes captures a different part of the underlying biology, thus developing integrative computational methods has been an important research area in bioinformatics.</p>
    <p>Breast cancer is currently the most commonly-diagnosed cancer worldwide (<xref rid="B8" ref-type="bibr">8</xref>). Therapeutic groups in breast cancer (i.e., estrogen receptor-positive, progesterone receptor-positive, human epidermal growth factor receptor 2 (HER2) amplified group, and triple-negative breast cancer) mainly depend on three receptors. Even though these receptors are very impactful in determining the breast cancer subtypes, they are not solely sufficient to classify a patient. Some other studies showed that genomic and clinical features such as race, age, and some mutations are also important in breast cancer subtyping (<xref rid="B9" ref-type="bibr">9</xref>,<xref rid="B10" ref-type="bibr">10</xref>).</p>
    <p>Genomic datatypes are found informative for differentiating subgroups in breast cancer. In 2009, Parker <italic toggle="yes">et al.</italic> (<xref rid="B11" ref-type="bibr">11</xref>) found a clear difference in the expression of 50 genes for breast cancer and introduced breast cancer molecular subtypes, called <italic toggle="yes">PAM50 subtypes</italic>. In 2012, the TCGA group published a study analyzing breast cancer subgroups and their associations with single datatypes, obtaining subtype-specific patterns in each datatype (<xref rid="B12" ref-type="bibr">12</xref>) and supporting the importance of gene expression-based models such as PAM50 (<xref rid="B11" ref-type="bibr">11</xref>). Even though there are important signals from both clinical and genomic features to determine the subtype of a patient, relying on a single data modality is not sufficient to differentiate subtypes clearly. As we get more samples and datatypes to analyze, it is important to integrate all the available datatypes properly with advanced approaches to understand differences in the characteristics of cancer patients.</p>
    <p>Recently several groups have developed unsupervised computational tools to integrate multiple datatypes to discover cancer subtypes. For instance, iClusterPlus (<xref rid="B13" ref-type="bibr">13</xref>) uses a joint latent variable model concatenating multiple datatypes with dimension reduction to cluster cancer patients. Similarity Network Fusion (SNF) (<xref rid="B14" ref-type="bibr">14</xref>) builds a patient similarity network based on each datatype, obtains a fused patient network by applying a nonlinear fusion step, and performs the clustering on that final network. PINSPlus (<xref rid="B15" ref-type="bibr">15</xref>) assumes that samples that are truly in the same subtype are clustered together despite small changes in the data. PINSPlus discovers the subtypes if the samples are highly connected for different datatypes applying data perturbation. The authors demonstrated that PINSPlus had robust results with significant survival differences across different cancer types. Those studies focus on unsupervised multiomics data integration without the utilization of found subtype labels such as PAM50 subtype labels. Furthermore, these tools utilize patient similarity networks or features, but not both simultaneously, while there are recent improvements in graph representation learning allowing the utilization of both at the same time (<xref rid="B16" ref-type="bibr">16–18</xref>).</p>
    <p>Graphs (networks) are suitable data structures to store multiomics datasets, however, machine learning (ML)-based approaches are challenging on graph data. Deep learning-based architectures have been used extensively for grid-like data (e.g., image), however, these methods are not directly applicable to graph data. Graphs are unstructured as each node has a varying number of neighbors and there is no fixed ordering of nodes. To train ML models on graph data, <italic toggle="yes">embedding</italic> (a fixed low-dimensional vector) is used and some shallow embedding methods emerged by encoding every node into embedding, representing the position and the local relationships in the graph (<xref rid="B19" ref-type="bibr">19–21</xref>). However, these shallow embedding methods are not scalable for large graphs and cannot utilize the node features that we have plenty of, thus, these methods have been replaced with more advanced deep learning-based methods such as Graph Neural Networks (GNNs) (<xref rid="B16" ref-type="bibr">16</xref>,<xref rid="B17" ref-type="bibr">17</xref>). The main difference in GNN-based architectures is how the features are aggregated from the local structure. Graph Convolutional Network (GCN) is one of the most popular GNNs that uses a modified aggregation involving self edges with normalization across neighbors (<xref rid="B18" ref-type="bibr">18</xref>). GNNs have recently been applied to biological problems such as cancer type/subtype prediction and drug response prediction (<xref rid="B18" ref-type="bibr">18</xref>, <xref rid="B22" ref-type="bibr">22–25</xref>).</p>
    <p>Even though there are some studies applying convolution to graph-structured data for cancer subtyping, these models are mostly applicable to a single network or had some limitations for integrative approaches. In (<xref rid="B26" ref-type="bibr">26</xref>), cancer type prediction of patients from 33 cancer and non-cancer types (i.e., all normal samples from all 33 available cancer types) was performed using GCNs. The input network was based on gene coexpression or protein-protein interaction, but the convolution was done on the gene expression dataset only, thus, missing the information of multiple data modalities. Multiomics GCN (MOGONET) is a supervised multiomics integration framework using GCNs with a patient similarity network for mRNA expression, DNA methylation, and microRNA expression separately (<xref rid="B27" ref-type="bibr">27</xref>). MOGONET gets the label independently from three different models, then uses them to get the final prediction. However, it does not consider multiple features for networks. We call this kind of embedding <italic toggle="yes">datatype-specific patient embedding</italic> where the methodology generates datatype-specific networks with datatype-specific node features and considers only the prediction labels from separate GCN models. However, these embeddings could be improved by utilizing all the multiomics patient features on each local network structure, making the embedding <italic toggle="yes">network-specific patient embedding</italic>. Moreover, it is possible to utilize GCN not only to get the prediction label but also to obtain the embeddings and integrate them. Going further, we can also integrate the patient features (called <italic toggle="yes">raw features</italic>) with embeddings to capture any diluted signals from features. To utilize more from available knowledge, it is important to properly integrate multiple network representations and multiomics features simultaneously.</p>
    <p>To address the aforementioned limitations, we developed a computational tool named SUPREME integrating multiple types of datasets using GCNs. SUPREME generated similarity networks using features from multi-modal datasets where node features include features from all data modalities, assuming that nodes with a similar local neighborhood are likely to belong to the same class. SUPREME encodes the relations on a network from each datatype and obtains network-specific node embeddings incorporating node features on each network. Then SUPREME integrates these embeddings providing extensive evaluations of all combinations of node embeddings. For each combination, SUPREME integrates the selected embeddings with raw features to utilize all the knowledge at the same time. SUPREME utilizes all available datatypes from public datasets and can interpret each datatype’s effectiveness in terms of features and networks. Being model-agnostic, SUPREME could be easily adapted to any model, any prediction task handling any number of datatypes, and could be easily modified by changing the embedding integration method, network generation strategy, and feature extraction approach.</p>
    <p>In this study, SUPREME was applied to three different prediction tasks from five different datasets. We applied SUPREME to predict subtypes of breast cancer patients using multiomics datasets (from TCGA and METABRIC datasets separately and together). Our results on cancer subtype prediction tasks showed that SUPREME outperformed other integrative supervised cancer (sub)type prediction tools and baseline methods. SUPREME had improved performance showing the importance of GCN-based approaches, network-specific patient embeddings, and raw feature integration. SUPREME was robust showing high and consistent prediction performance. We observed that the gene expression (EXP)-based features were the most significant features, as expected for breast cancer. Importantly, SUPREME-inferred cancer subtypes had consistently significant survival differences and were mostly more significant than the survival differences between ground truth subtypes, which were based on gene expression datatype. These results suggest that SUPREME can differentiate the characteristics of cancer subtypes properly utilizing the multiple network relations and multiple datatypes. To demonstrate the model-agnostic property of our tool, we also applied SUPREME to ACM and IMDB datasets and SUPREME outperformed other methods on both datasets.</p>
  </sec>
  <sec sec-type="materials|methods" id="SEC2">
    <title>MATERIALS AND METHODS</title>
    <p>SUPREME is a computational tool for node classification tasks integrating multiple data modalities using GCNs. Briefly, the first step is data preparation. In the second step, SUPREME extracts features from each datatype. Using those features, SUPREME generates individual similarity networks per datatype where features from all datatypes are used as node attributes. In the third step, using the obtained networks and features, SUPREME generates the network-specific node embeddings by running GCN on each network. In the last step, SUPREME does prediction by integrating individual network-specific embeddings and raw features. In the following part, we explain each step of SUPREME in detail.</p>
    <sec id="SEC2-1">
      <title>Data preparation</title>
      <p>We applied SUPREME on three datasets for the breast cancer subtype prediction task. We collected the data and generated seven datatypes (i.e., clinical, copy number aberration, coexpression, gene expression, DNA methylation, microRNA expression, and mutation) across 1022 breast tumor samples from TCGA (<xref rid="B12" ref-type="bibr">12</xref>), five datatypes (i.e., clinical, copy number aberration, coexpression, gene expression, and mutation) across 1699 breast tumor samples from METABRIC (<xref rid="B2" ref-type="bibr">2</xref>) and three datatypes (clinical, gene expression, and mutation) across a total of 2721 breast tumor samples from the combined datasets of TCGA and METABRIC. As ground truth for the prediction task, we obtained the PAM50 subtype labels, namely Basal-like, HER2-Enriched, Luminal-A, Luminal-B, and Normal-like (<xref rid="B11" ref-type="bibr">11</xref>). Data preprocessing details are in <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods 1.1</xref>.</p>
      <p>We also collected ACM and IMDB datasets for two additional tasks: movie genre prediction from IMDB dataset (<ext-link xlink:href="https://www.imdb.com" ext-link-type="uri">https://www.imdb.com</ext-link>) and paper area prediction task from ACM dataset (<ext-link xlink:href="http://dl.acm.org" ext-link-type="uri">http://dl.acm.org</ext-link>). IMDB dataset has a heterogeneous network with three node types (movie, actor, and director) along with two associations: movie-actor and movie-director. The movies have three genre classes: action, comedy, and drama. ACM dataset has also three node types (paper, author, and subject) on a heterogeneous network along with two associations: paper-author and paper-subject. The papers have three classes: database, wireless communication, and data mining.</p>
      <p>The number of features and samples for each dataset are shown in Table <xref rid="tbl1" ref-type="table">1</xref>.</p>
      <table-wrap position="float" id="tbl1">
        <label>Table 1.</label>
        <caption>
          <p>Number of features and samples for each dataset. Subtypes are abbreviated as BL: basal-like, HER2: HER2-Enriched, LA: luminal-A, LB: luminal-B, NL: normal-like</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Dataset</th>
              <th rowspan="1" colspan="1">Number of raw features</th>
              <th rowspan="1" colspan="1">Number of samples</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">TCGA</td>
              <td rowspan="1" colspan="1">3088</td>
              <td rowspan="1" colspan="1">1022 samples: 172 BL (17%), 78 HER2 (8%), 538 LA (53%), 195 LB (19%), 39 NL (4%)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">METABRIC</td>
              <td rowspan="1" colspan="1">1761</td>
              <td rowspan="1" colspan="1">1699 samples: 199 BL (12%), 220 HER2 (13%), 679 LA (40%), 461 LB (27%), 140 NL (8%)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Combined (TCGA+ METABRIC)</td>
              <td rowspan="1" colspan="1">1229</td>
              <td rowspan="1" colspan="1">2721 samples: 371 BL (14%), 298 HER2 (11%), 1217 LA (45%), 656 LB (24%), 179 NL (7%)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">IMDB</td>
              <td rowspan="1" colspan="1">3066</td>
              <td rowspan="1" colspan="1">4278 samples: 1135 (27%), 1584 (37%), 1559 (36%)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ACM</td>
              <td rowspan="1" colspan="1">1870</td>
              <td rowspan="1" colspan="1">3025 samples: 1061 (35%), 965 (32%), 999 (33%)</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="SEC2-2">
      <title>Feature extraction &amp; network generation</title>
      <sec id="SEC2-2-1">
        <title>Breast cancer subtyping</title>
        <p>SUPREME incorporates seven datatypes for TCGA data, five datatypes for METABRIC data, and three datatypes for the combined data. We utilized a Random Forest-based feature selection algorithm, called Boruta (<xref rid="B28" ref-type="bibr">28</xref>), to extract features from high dimensional datatypes. The selected features in the data preprocessing step (i.e., multiomics features) were used to compute the similarity between patients when generating the patient similarity networks, as node features in the patient similarity networks, and to integrate as raw features before the prediction task. To compute patient similarities in datatype-specific patient similarity networks, we used Pearson correlation for gene expression, copy number aberration, DNA methylation, microRNA expression, and coexpression datatypes; the Gower metric (<xref rid="B29" ref-type="bibr">29</xref>) from the daisy function of cluster R package (<xref rid="B30" ref-type="bibr">30</xref>) for clinical features; and Jaccard distance for binary mutation features. After selecting the top edges, the edge weights were eliminated to generate an unweighted network. We used 2500 edges for the datatypes of TCGA, 4500 for METABRIC and 7000 for the combined data (having approximately 2.5 times the sample size). Details of feature extraction and network generation are in <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods 1.2</xref>.</p>
      </sec>
      <sec id="SEC2-2-2">
        <title>Movie genre prediction</title>
        <p>We did not apply any feature selection for the IMDB dataset and used node features processed in (<xref rid="B31" ref-type="bibr">31</xref>). Using two associations (i.e., movie-actor and movie-director) in the data, two movie similarity networks were generated based on two meta-paths using (<xref rid="B32" ref-type="bibr">32</xref>): movie-director-movie with 17 446 edges and movie-actor-movie with 85 358 edges. Meta-path-based similarity networks connect nodes based on a given association. For instance, the meta-path movie-actor-movie defines similarity as the existence of at least one common actor between two movies.</p>
      </sec>
      <sec id="SEC2-2-3">
        <title>Paper area prediction</title>
        <p>For the ACM dataset, we did not apply any feature selection and used the node features processed in (<xref rid="B31" ref-type="bibr">31</xref>). Utilizing two associations (i.e., paper-author and paper-subject) in the data, two meta-paths were used to generate two paper similarity networks using (<xref rid="B33" ref-type="bibr">33</xref>): paper-author-paper with 29 281 edges and paper-subject-paper with 2 210 761 edges. The meta-path-based similarity definition is the same as in the IMDB dataset.</p>
        <p>When there is a high number of raw features and many networks to integrate, this might affect the prediction performance, and model training could be time-consuming. Thus, we added another optional feature selection step to further reduce the number of raw features integrated with the node embeddings for the prediction task. We enabled this additional feature selection for TCGA data where we had a high number of raw features and networks and observed that it reduced running time without affecting the prediction performance. We did not apply optional feature selection for ACM and IMDB datasets since we have only two networks. Similarly, we did not apply any reduction for the number of edges for these datasets since we do not have any quantitative similarities to prioritize the edges on meta-path-based similarity networks.</p>
      </sec>
    </sec>
    <sec id="SEC2-3">
      <title>Node embedding generation</title>
      <p>After extracting features and generating networks, we obtained network-specific node embeddings, which capture the topology of the network as well as node features to be utilized in a downstream ML task.</p>
      <p>In this study, we used the GCN model of Kipf and Welling (<xref rid="B18" ref-type="bibr">18</xref>) involving self edges in convolution and scaling the sum of aggregated features across the neighbors. GCN models learn the data by performing convolution on networks, considering one-hop local neighbors with equal contribution, and encoding the local topology of the network. Stacked layers involve recursive neighborhood diffusion considering more than a one-hop neighborhood.</p>
      <p>Let’s call an undirected graph as <inline-formula><tex-math id="M0001" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {G}= (\mathcal {V},\mathcal {E})$\end{document}</tex-math></inline-formula> where <inline-formula><tex-math id="M0001a" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {V}$\end{document}</tex-math></inline-formula> is a set of <italic toggle="yes">n</italic> nodes, i.e., <inline-formula><tex-math id="M0002" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {V} = \lbrace v_1, v_2, ..., v_n\rbrace$\end{document}</tex-math></inline-formula>, and <inline-formula><tex-math id="M0003" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {E}$\end{document}</tex-math></inline-formula> is a set of edges between nodes where <inline-formula><tex-math id="M0004" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$(v_i,v_j) \in \mathcal {E}$\end{document}</tex-math></inline-formula> when <inline-formula><tex-math id="M0005" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_i \in \mathcal {V}$\end{document}</tex-math></inline-formula>, <inline-formula><tex-math id="M0006" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_j \in \mathcal {V}$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M0007" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_i$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M0008" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_j$\end{document}</tex-math></inline-formula> have an association based on the graph <inline-formula><tex-math id="M0009" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {G}$\end{document}</tex-math></inline-formula>. Since the graph <inline-formula><tex-math id="M00010" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {G}$\end{document}</tex-math></inline-formula> is undirected, <inline-formula><tex-math id="M00011" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$(v_i,v_j) \in \mathcal {E} \iff (v_j,v_i) \in \mathcal {E}$\end{document}</tex-math></inline-formula>.</p>
      <p>The input for a GCN model is a feature matrix <inline-formula><tex-math id="M00012" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {X} \in \mathbb {R}^{nxk}$\end{document}</tex-math></inline-formula> where <italic toggle="yes">k</italic> is the feature size, and the adjacency matrix <inline-formula><tex-math id="M00013" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {A} \in \mathbb {R}^{nxn}$\end{document}</tex-math></inline-formula> with added self edges defined as:</p>
      <disp-formula>
        <tex-math id="M00014" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{equation*} \mathcal {A}[i,j]=\left\lbrace \begin{array}{lr}1 &amp; \text{ if }\left(v_{i}, v_{j}\right) \in \mathcal {E} \text{ or } i=j \\ 0 &amp; \text{ otherwise } \end{array}\right. \end{equation*}\end{document}</tex-math>
      </disp-formula>
      <p>The iteration process is defined as:</p>
      <disp-formula>
        <tex-math id="M00015" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{equation*} \mathcal {H}^{(l+1)}=\sigma \left(\mathcal {D}^{-\frac{1}{2}} \mathcal {A} \mathcal {D}^{-\frac{1}{2}} \mathcal {H}^{(l)} \mathcal {W}^{(l)}\right) \end{equation*}\end{document}</tex-math>
      </disp-formula>
      <p>with <inline-formula><tex-math id="M00016" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {H}^{(0)} = \mathcal {X}$\end{document}</tex-math></inline-formula> where</p>
      <disp-formula>
        <tex-math id="M00017" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{equation*} \mathcal {D}[i,i]=\sum _{j=1}^{n} \mathcal {A}[i,j], \end{equation*}\end{document}</tex-math>
      </disp-formula>
      <p><inline-formula><tex-math id="M00018" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {H}^{(l)}$\end{document}</tex-math></inline-formula> is the activation matrix in the <italic toggle="yes">l</italic>th layer, <inline-formula><tex-math id="M00019" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {W}^{(l)}$\end{document}</tex-math></inline-formula> is the trainable weight matrix in the <italic toggle="yes">l</italic>th layer and σ is the activation function.</p>
      <p>Considering breast cancer subtyping task using TCGA data, SUPREME setup for the single model generation was as follows: there were seven networks (i.e., patient similarity networks), each obtained from a different datatype. All networks had nodes as breast cancer patients and edges based on the patient similarities from the corresponding data. For instance, let us consider <inline-formula><tex-math id="M00020" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {G}$\end{document}</tex-math></inline-formula> as a gene expression-derived patient similarity network. This network connects patient nodes with a high correlation between their gene expression profile. As node features, <inline-formula><tex-math id="M00021" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {G}$\end{document}</tex-math></inline-formula> has the combined features, which were extracted from all the seven datatypes. Features of <inline-formula><tex-math id="M00022" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_i$\end{document}</tex-math></inline-formula> are denoted as <inline-formula><tex-math id="M00023" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$x_i \in \mathbb {R}^k$\end{document}</tex-math></inline-formula> where <italic toggle="yes">k</italic> is the total feature size. So, the stacked feature matrix <inline-formula><tex-math id="M00024" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {X} \in \mathbb {R}^{nxk}$\end{document}</tex-math></inline-formula> is:</p>
      <disp-formula>
        <tex-math id="M00025" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{equation*} \mathcal {X}=\begin{bmatrix}x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix} \end{equation*}\end{document}</tex-math>
      </disp-formula>
      <p>The local one-hop neighborhood of a node <inline-formula><tex-math id="M00026" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_i$\end{document}</tex-math></inline-formula> is <inline-formula><tex-math id="M00027" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {N}_{i}=\left\lbrace v_{j}:\left(v_{i}, v_{j}\right) \in \mathcal {E}\right\rbrace$\end{document}</tex-math></inline-formula> that included the set of nodes having an association with the node <inline-formula><tex-math id="M00028" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_i$\end{document}</tex-math></inline-formula>. Feature aggregation on the local neighborhood of each node was done by multiplying <inline-formula><tex-math id="M00029" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {X}$\end{document}</tex-math></inline-formula> by the <italic toggle="yes">nxn</italic>-sized scaled adjacency matrix <inline-formula><tex-math id="M00030" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {A}^{^{\prime }}$\end{document}</tex-math></inline-formula> where</p>
      <disp-formula>
        <tex-math id="M00031" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{equation*} \mathcal {A}^{\prime }=\mathcal {D}^{-\frac{1}{2}} \mathcal {A} \mathcal {D}^{-\frac{1}{2}}. \end{equation*}\end{document}</tex-math>
      </disp-formula>
      <p>Using 2-layered GCN in SUPREME, we had the form of the forward model giving the output <inline-formula><tex-math id="M00032" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {Z}$\end{document}</tex-math></inline-formula> where</p>
      <disp-formula>
        <tex-math id="M00033" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{equation*} \mathcal {Z}=\operatorname{softmax}\left(\mathcal {A}^{\prime } \operatorname{ReLU}\left(\mathcal {A}^{\prime } \mathcal {X} \mathcal {W}^{(1)}\right) \mathcal {W}^{(2)}\right) \end{equation*}\end{document}</tex-math>
      </disp-formula>
      <p>and <inline-formula><tex-math id="M00034" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {W}^{(1)} \in \mathbb {R}^{kxh}$\end{document}</tex-math></inline-formula>, <inline-formula><tex-math id="M00035" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal {W}^{(2)} \in \mathbb {R}^{hxc}$\end{document}</tex-math></inline-formula> were the trainable weights for the first and second layers, respectively, where <italic toggle="yes">h</italic> was the hidden layer size and <italic toggle="yes">c</italic> was the number of classes to predict (namely, Basal-like, Luminal-A, Luminal-B, HER2-Enriched, and Normal-like, with <italic toggle="yes">c</italic> = 5). The loss function was calculated by cross-entropy error. Adam optimization (<xref rid="B34" ref-type="bibr">34</xref>) was used as the state-of-the-art for stochastic gradient descent algorithm and dropout was added for the first GCN layer. Early stopping was used with the patience of 30 forced to have at least 200 epochs.</p>
      <p>We split the total samples into training, validation, and test sets. This splitting was stratified, that is, keeping the same ratio of the subtype labels in the original data for each split. We kept the test set only for final evaluation of the tool. Training and validation splits are randomly selected for each run as stratified. For the breast cancer subtyping, we split 20% of the total samples as a test set. The remaining 80% of the samples were used for training (60%) and validation (20%). For IMDB and ACM datasets, we used the same data splits in (<xref rid="B31" ref-type="bibr">31</xref>). To tune the hyperparameters of the GCN model (i.e., hidden layer size and learning rate), for each run, SUPREME repeated an evaluation metric (i.e., macro-averaged F1 (macro F1) score) 10 times for each hyperparameter combination (<xref rid="sup1" ref-type="supplementary-material">Supplemental File 2</xref>) and selected the hyperparameter combination giving the best median macro F1 score on the validation data to generate the final model.</p>
      <p>Similarly applying the methodology for other datatypes, we generated seven different GCN models on TCGA data. Repeating the same procedure for other datasets, we obtained five models on METABRIC data, three models on the combined data, and two models on ACM and IMDB data. These final models were used to extract network-specific patient embeddings to use in the downstream prediction task.</p>
    </sec>
    <sec id="SEC2-4">
      <title>Training predictive models using node embedding combinations</title>
      <p>For each combination of node embeddings from <inline-formula><tex-math id="M00036" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$d$\end{document}</tex-math></inline-formula> datatypes, we concatenated them with the raw features and trained prediction models (having <inline-formula><tex-math id="M00037" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$2^{d}-1$\end{document}</tex-math></inline-formula> models). Specifically, we had 127, 31, seven, three, and three SUPREME models for TCGA, METABRIC, the combined data (TCGA+METABRIC), ACM, and IMDB datasets, respectively.</p>
      <p>We tested SUPREME with several ML methods namely, XGBoost, Support Vector Machine (SVM), Random Forest (RF), and Multi-layer Perceptron (MLP). For all datasets, we decided to use MLP as it gave consistently high performance (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref> and discussion section for the details).</p>
      <p>We did hyperparameter tuning for the prediction task, similar to GCN hyperparameter tuning in the previous step. We used the training and validation cohort to tune the hyperparameters (e.g., hidden layer size and learning rate) of the final model, where training and validation splits were randomly selected as stratified. We repeated the SUPREME run 10 times for each hyperparameter combination and used the hyperparameter combination giving the best median macro F1 score on the validation data. Using this hyperparameter combination, the final model was built and evaluated 10 times on the test data, which was never seen during training and hyperparameter tuning. The evaluation metrics (macro F1, weighted-average F1 (weighted F1) score, and accuracy) were obtained from the median of these 10 runs.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="SEC3">
    <title>RESULTS</title>
    <p>We introduced a novel node classification framework, called SUPREME, that utilizes graph convolutions on multiple datatype-specific networks that are annotated with multi-modal datatypes as node features. This framework is model-agnostic and could be applied to any classification problem with properly processed datatypes and networks. In this work, SUPREME was applied specifically to the breast cancer subtype prediction problem by applying convolution on patient similarity networks constructed based on multiple biological datatypes from breast tumor samples (Figure <xref rid="F1" ref-type="fig">1</xref>). We also evaluated SUPREME on ACM and IMDB datasets demonstrating the outperformance of SUPREME in different domains.</p>
    <fig position="float" id="F1">
      <label>Figure 1.</label>
      <caption>
        <p>SUPREME pipeline for breast cancer subtype prediction. SUPREME extracts feature from available datatypes and generates patient similarity networks where nodes are annotated with features from all datatypes. Utilizing graph convolutions on each patient similarity network, patient embeddings are generated. To provide extensive evaluations of subtype prediction, a machine learning model is trained for each combination of patient embeddings and raw multiomics features. [<italic toggle="yes">u</italic><sub><italic toggle="yes">k</italic></sub> is <italic toggle="yes">k</italic>th patient, <italic toggle="yes">i</italic><sup>(<italic toggle="yes">j</italic>)</sup> is a raw feature matrix for the <italic toggle="yes">j</italic>th datatype where each row is <inline-formula><tex-math id="M00038" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$i_k^{(j)}$\end{document}</tex-math></inline-formula> corresponding to the feature vector of the <italic toggle="yes">k</italic>th patient for the <italic toggle="yes">j</italic>th datatype. Similarly, <italic toggle="yes">z</italic><sup>(<italic toggle="yes">j</italic>)</sup> is a node embedding matrix for the <italic toggle="yes">j</italic>th datatype-specific network where each row is <inline-formula><tex-math id="M00039" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$z_k^{(j)}$\end{document}</tex-math></inline-formula> corresponding to the embedding of the <italic toggle="yes">k</italic><sup><italic toggle="yes">th</italic></sup> patient.]</p>
      </caption>
      <graphic xlink:href="lqad063fig1" position="float"/>
    </fig>
    <sec id="SEC3-1">
      <title>SUPREME outperformed the cancer subtype prediction tools and baseline methods</title>
      <p>For the breast cancer subtyping task, we compared the performance of SUPREME on three different datasets with seven other cancer (sub)type prediction tools and baseline methods, namely Deep cancer subtype classification (DeepCC) (<xref rid="B35" ref-type="bibr">35</xref>), GCN-based classification (GCNC) (<xref rid="B26" ref-type="bibr">26</xref>), MOGONET (<xref rid="B27" ref-type="bibr">27</xref>), MLP, RF, SVM, and XGBoost. For each dataset combination, SUPREME builds a separate cancer subtype prediction model. For ML-based baseline methods (i.e., MLP, RF, SVM, and XGBoost), we integrated only the raw features from the selected combination and did the prediction with those features. MOGONET utilizes GCN on multiomics data utilizing datatype-specific embedding predictions. GCNC leverages GCN with gene expression features on protein-protein interaction (PPI)- or coexpression-based gene network, while DeepCC utilizes only gene expression datatype with pathway activity transformation through an MLP model. Therefore, we had only two classification models for GCNC: <italic toggle="yes">GCNC</italic><sub><italic toggle="yes">PPI</italic></sub> with the PPI network and <italic toggle="yes">GCNC</italic><sub><italic toggle="yes">COE</italic></sub> with the coexpression network, and one model for DeepCC. To see the impact of the integration of raw features into the embeddings, we also trained models without integrating raw features with patient embeddings, called <italic toggle="yes">SUPREME-</italic>. We ran SUPREME, SUPREME-, and the other tools for all the combinations of available datatypes. Even though MOGONET is applicable to any number of datatypes, we could not run the tool for the models with more than five datatypes (waiting time was more than two days per combination), thus we had only 31 different models for TCGA data, while we had all models for METABRIC and the combined data.</p>
      <p>SUPREME and SUPREME- outperformed all other multiomics integration methods for three datasets in terms of macro F1, accuracy, and weighted F1 (Figure <xref rid="F2" ref-type="fig">2</xref>, Supplementary Figures S3 and S4). SUPREME significantly outperformed MLP, which utilizes raw features only in all datasets, showing the importance of GCN utilization. We observed that SUPREME significantly outperformed SUPREME- for all three datasets.</p>
      <fig position="float" id="F2">
        <label>Figure 2.</label>
        <caption>
          <p>Classification results. Violin plot of macro F1 scores obtained from 127 different models including all different combinations of datatypes as compared to the cancer subtype prediction tools and baseline supervised methods on TCGA data. DeepCC and GCNC violin plots show the distribution of macro F1 scores of ten runs of a single model as they can only utilize gene expression datatype. The significance level was measured with respect to SUPREME (Wilcoxon rank-sum test p-value to compare the distribution of violin plots representing the significance &lt; 0.001 by ***, else if &lt; 0.01 by **, and else if &lt; 0.05 by *). [MLP: Multi-layer Perceptron, RF: Random Forest, SUPREME-: SUPREME without raw feature integration, SVM: Support Vector Machine]</p>
        </caption>
        <graphic xlink:href="lqad063fig2" position="float"/>
      </fig>
      <p>We ran the tools that utilize only gene expression datatype and evaluated their performance (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>). For TCGA data, SUPREME achieved significantly higher performance than DeepCC and GCNC models, while performance on METABRIC and the combined data was comparable or superior (Figures <xref rid="F2" ref-type="fig">2</xref> and S3).</p>
      <p>In addition, we checked the subtype-specific F1 scores, and had consistent and higher performance across all subtypes, mostly having significant differences (Supplementary Figure S5). Specifically, on TCGA data, we had significantly better performance than all other tools for all subtypes in terms of subtype-specific F1 scores. Particularly, SUPREME had a significantly higher subtype-specific F1 score than all other tools on the Normal-like subtype for all three datasets. Considering that the Normal-like subtype had the smallest sample size in all three datasets (4% of the samples from TCGA, 8% from METABRIC, and 7% from the combined data), achieving this performance increase indicates SUPREME’s robustness even for minority classes.</p>
    </sec>
    <sec id="SEC3-2">
      <title>SUPREME had consistently high performance even with single models</title>
      <p>To see SUPREME’s performance with a single datatype, we investigated models generated with only one datatype, called <italic toggle="yes">single model</italic>. We compared SUPREME with an MLP-based model trained using a single datatype to show the impact of our GCN-based approach. To show the impact of different approaches with one datatype, we compared our single models against MOGONET and our EXP-based model with DeepCC and GCNC models. We conducted these experiments for all three datasets.</p>
      <p>Based on the single model results, SUPREME outperformed MOGONET for all single models from all three datasets (Table <xref rid="tbl2" ref-type="table">2</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S3–S5</xref>). Also, SUPREME outperformed MLP (six out of seven models for TCGA data, three out of five for METABRIC data, and two out of three models for the combined data), or had comparable performance, while MLP had extremely poor performance on some datatypes, showing the importance of GCN-based approach.</p>
      <table-wrap position="float" id="tbl2">
        <label>Table 2.</label>
        <caption>
          <p>Single model results on TCGA data. Macro F1 scores for each model with a single dataype. See Figure <xref rid="F1" ref-type="fig">1</xref> for the abbreviations of the datatypes. [MLP: multi-layer perceptron]</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Method</th>
              <th rowspan="1" colspan="1">CLI</th>
              <th rowspan="1" colspan="1">CNA</th>
              <th rowspan="1" colspan="1">COE</th>
              <th rowspan="1" colspan="1">EXP</th>
              <th rowspan="1" colspan="1">MET</th>
              <th rowspan="1" colspan="1">MIR</th>
              <th rowspan="1" colspan="1">MUT</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">SUPREME</td>
              <td rowspan="1" colspan="1">0.68 ± 0.04</td>
              <td rowspan="1" colspan="1">
                <bold>0.80 ± 0.03</bold>
              </td>
              <td rowspan="1" colspan="1">0.76 ± 0.04</td>
              <td rowspan="1" colspan="1">
                <bold>0.84 ± 0.02</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.79 ± 0.03</bold>
              </td>
              <td rowspan="1" colspan="1">0.73 ± 0.02</td>
              <td rowspan="1" colspan="1">
                <bold>0.75 ± 0.03</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SUPREME-</td>
              <td rowspan="1" colspan="1">
                <bold>0.72 ± 0.02</bold>
              </td>
              <td rowspan="1" colspan="1">0.77 ± 0.02</td>
              <td rowspan="1" colspan="1">
                <bold>0.77 ± 0.04</bold>
              </td>
              <td rowspan="1" colspan="1">0.77 ± 0.05</td>
              <td rowspan="1" colspan="1">
                <bold>0.79 ± 0.04</bold>
              </td>
              <td rowspan="1" colspan="1">0.70 ± 0.02</td>
              <td rowspan="1" colspan="1">0.74 ± 0.04</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MLP</td>
              <td rowspan="1" colspan="1">0.46 ± 0.07</td>
              <td rowspan="1" colspan="1">0.53 ± 0.04</td>
              <td rowspan="1" colspan="1">0.59 ± 0.02</td>
              <td rowspan="1" colspan="1">0.82 ± 0.03</td>
              <td rowspan="1" colspan="1">0.69 ± 0.04</td>
              <td rowspan="1" colspan="1">
                <bold>0.74 ± 0.04</bold>
              </td>
              <td rowspan="1" colspan="1">0.28 ± 0.06</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MOGONET</td>
              <td rowspan="1" colspan="1">0.41 ± 0.01</td>
              <td rowspan="1" colspan="1">0.52 ± 0.01</td>
              <td rowspan="1" colspan="1">0.57 ± 0.01</td>
              <td rowspan="1" colspan="1">0.75 ± 0.01</td>
              <td rowspan="1" colspan="1">0.61 ± 0.03</td>
              <td rowspan="1" colspan="1">0.71 ± 0.03</td>
              <td rowspan="1" colspan="1">0.34 ± 0.01</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>There was no clear winner for the comparison of the SUPREME EXP-based model with DeepCC and GCNC models. In terms of macro F1 score, SUPREME outperformed both methods on TCGA data and GCNC (1 draw, 1 win) on the combined data. (Table <xref rid="tbl2" ref-type="table">2</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S3–S5</xref>). This could be because DeepCC and GCNC utilize pathway activation, PPI network, or coexpression network in addition to gene expression datatype. Nonetheless, by utilizing more datatypes SUPREME outperformed or was on par with both tools for all datasets (Figure <xref rid="F2" ref-type="fig">2</xref>, Supplementary Figures S3 and S4).</p>
      <p>EXP-based models had the highest macro F1 score for all three datasets for all methods (Table <xref rid="tbl2" ref-type="table">2</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tabless S3, S4, and S5</xref>). The only exception is that SUPREME- MET-based model had slightly higher performance than SUPREME- EXP-based model on TCGA data. High performance of EXP-based models is not surprising as the breast cancer subtype labels are based on gene expression data. We observed that SUPREME usually outperformed SUPREME- on single models, which indicates that utilizing raw features usually improves the model performance. On the other hand, there were few cases where adding raw features dropped the performance (e.g., CLI-based models on TCGA data). By examining SUPREME- and MLP model performances, we compared the predictive power of patient embeddings with raw features. We observed that patient embedding features were more useful than raw features with few exceptions, such as microRNA expression- (MIR) and EXP-based models on TCGA data, copy number aberration (CNA)-based on METABRIC data, and CLI-based model on the combined data. Specifically on TCGA, we see that CLI-based embedding was more informative than CLI-based features. For CNA- and mutation (MUT)-based models, embeddings were more useful than raw features, but we observed that integrating raw features to embeddings further improved the performance. Similarly, although for the EXP-based model on TCGA data, embeddings were less informative than raw features, integrating them improved the performance.</p>
    </sec>
    <sec id="SEC3-3">
      <title>SUPREME had significant survival differences between predicted subtypes consistently</title>
      <p>To measure the ability of the supervised methods to differentiate samples based on survival, we predicted the subtype labels for each data modality combination and performed the survival analysis. In addition to the supervised methods, we also included the state-of-the-art unsupervised tools that are specifically applied to cancer subtyping (i.e., iClusterPlus (<xref rid="B13" ref-type="bibr">13</xref>), SNF (<xref rid="B14" ref-type="bibr">14</xref>), and PINSPlus (<xref rid="B15" ref-type="bibr">15</xref>)) and an algorithmically-relevant clustering method (i.e. affinity propagation (AP) clustering). AP clustering is relevant because it uses a message-passing strategy to find the cluster representatives and the best representative for each node. We obtained five clusters from the unsupervised methods to match the number of PAM50 subtypes and checked the survival differences for these obtained clusters. This analysis was only applied to the results on TCGA data where patient survival data were available. To check the statistical significance of survival differences between subtypes, we applied the log-rank test to compute p-values. Details of survival analysis are in <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods 1.3</xref>.</p>
      <p>The results showed that SUPREME’s predicted subtypes consistently had significant differences in survival rates and significantly outperformed all other nine methods in terms of the <italic toggle="yes">P</italic>-value (Figure <xref rid="F3" ref-type="fig">3</xref>). SUPREME had 0.0035 as the lowest <italic toggle="yes">P</italic>-value (when integrating CNA-, COE-, MET- and MUT-based patient embeddings) and 0.0131 as the median <italic toggle="yes">P</italic>-value (Supplementary Figure S6A for the Kaplan–Meier plot). Similarly for SUPREME-, we had 0.0018 as the lowest <italic toggle="yes">P</italic>-value (when integrating CNA- and COE-based patient embeddings), and 0.0147 as the median <italic toggle="yes">P</italic>-value. Interestingly, SUPREME had a more significant survival difference than the survival difference between ground truth (i.e., PAM50) labels (Supplementary Figure S6B for the Kaplan–Meier plot for PAM50 subtypes).</p>
      <fig position="float" id="F3">
        <label>Figure 3.</label>
        <caption>
          <p>Survival analysis results violin plot of the log-rank <italic toggle="yes">P</italic>-value obtained from survival analysis for the SUPREME models as compared to the cancer subtype prediction/clustering tools and baseline methods. Significance level was measured with respect to SUPREME (Wilcoxon rank-sum test <italic toggle="yes">P</italic>-value to compare the distribution of violin plots representing the significance &lt;0.001 by ***, else if &lt;0.01 by **, and else if &lt;0.05 by *). The continuous line shows the significance level of 0.05 and the dashed line shows the ground truth’s significance level. The below figure focuses on the significant survival <italic toggle="yes">P</italic>-values (&lt;0.05) [AP: affinity propagation, MLP: multi-layer perceptron, RF: random forest, SNF: similarity network fusion, SUPREME: SUPREME without raw feature integration, SVM: support vector machine].</p>
        </caption>
        <graphic xlink:href="lqad063fig3" position="float"/>
      </fig>
      <p>Specifically, 106 out of 127 SUPREME models had a lower <italic toggle="yes">P</italic>-value than the p-value for ground truth. For 57% of those models, we had CNA-based embedding selected. It is followed by 52% from COE-, CLI- and MET-based embeddings. This might suggest that those embeddings could contribute more to differentiating survival differences between subtypes.</p>
      <p>AP, iClusterPlus, MOGONET, and SNF methods had a wide range of <italic toggle="yes">P</italic>-values, while SUPREME, MLP, SVM, and XGBoost had mostly significant <italic toggle="yes">P</italic>-values (≤0.05) with a median lower than the significance level of the ground truth. SUPREME was better than SUPREME-, but the difference was not significant.</p>
      <p>Using support from the predicted subtypes by each model in SUPREME, we computed an ensembled consensus subtype based on majority voting for each patient (<xref rid="sup1" ref-type="supplementary-material">Supplemental File 3</xref>) and checked the survival difference between these consensus subtypes. Once again, we observed a significant (<italic toggle="yes">P</italic>-value = 0.01) survival difference between consensus subtypes (Supplementary Figure S6C). We also observed that 882 out of 1022 patients had the same subtype prediction across all 127 models showing the robustness of SUPREME predictions.</p>
    </sec>
    <sec id="SEC3-4">
      <title>Feature/omics importance analysis</title>
      <p>In this section, we investigated the importance of each network-specific embedding and datatype-specific features.</p>
      <sec id="SEC3-4-1">
        <title>Impact of network-specific patient embeddings</title>
        <p>We investigated the contribution of each patient embedding on the model performance by comparing the models built using a patient embedding from datatype <inline-formula><tex-math id="M00040" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}$\end{document}</tex-math></inline-formula> and without using that embedding. Among all <inline-formula><tex-math id="M00041" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$2^{d}-1$\end{document}</tex-math></inline-formula> models, <inline-formula><tex-math id="M00042" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$2^{d-1}$\end{document}</tex-math></inline-formula> models had the patient embedding obtained from a datatype <inline-formula><tex-math id="M00043" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}$\end{document}</tex-math></inline-formula>, called <inline-formula><tex-math id="M00044" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$with\mathbb {X}_n$\end{document}</tex-math></inline-formula>. The remaining <inline-formula><tex-math id="M00045" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$2^{d-1}-1$\end{document}</tex-math></inline-formula> models did not have the patient embedding obtained from <inline-formula><tex-math id="M00046" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}$\end{document}</tex-math></inline-formula>, called <inline-formula><tex-math id="M00047" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$no\mathbb {X}_n$\end{document}</tex-math></inline-formula>. For each datatype <inline-formula><tex-math id="M00048" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}$\end{document}</tex-math></inline-formula>, we compared <inline-formula><tex-math id="M00049" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$no\mathbb {X}_n$\end{document}</tex-math></inline-formula> models against <inline-formula><tex-math id="M00050" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$with\mathbb {X}_n$\end{document}</tex-math></inline-formula> models, showing the importance of <inline-formula><tex-math id="M00051" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}$\end{document}</tex-math></inline-formula>-specific patient embedding. We did this analysis on SUPREME- (i.e., without integrating the raw features) to ensure that differences were due to the patient embeddings only.</p>
        <p>The results on TCGA data showed that the performance of models increased or stayed the same with the inclusion of patient embeddings from all datatypes except for gene expression (Figure <xref rid="F4" ref-type="fig">4</xref>). The inclusion of EXP-based embedding showed a significant decrease in the model performance. The exclusion of CLI- and CNA-based patient embeddings had a significant drop in the model performance. Those findings agree with single model results.</p>
        <fig position="float" id="F4">
          <label>Figure 4.</label>
          <caption>
            <p>Analysis of network-specific patient embeddings. Violin plot of macro F1 scores of SUPREME- performance for the models integrated with a specific patient embedding from each datatype (<italic toggle="yes">with</italic><inline-formula><tex-math id="M00052" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}_n$\end{document}</tex-math></inline-formula> models, where <inline-formula><tex-math id="M00053" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}$\end{document}</tex-math></inline-formula> is the datatype whose embedding is included) versus excluding that embedding (<italic toggle="yes">no</italic><inline-formula><tex-math id="M00054" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {X}_n$\end{document}</tex-math></inline-formula> models) on TCGA data. Significance level was measured between <italic toggle="yes">with</italic> and <italic toggle="yes">no</italic> cases of the same datatype (Wilcoxon rank-sum test p-value to compare the distribution of violin plots representing the significance &lt;0.001 by ***, else if &lt;0.01 by **, and else if &lt;0.05 by *). See Figure <xref rid="F1" ref-type="fig">1</xref> for the abbreviations of datatypes. [SUPREME-: SUPREME without raw feature integration]</p>
          </caption>
          <graphic xlink:href="lqad063fig4" position="float"/>
        </fig>
        <p>For METABRIC data, the inclusion of COE- and EXP-based embeddings increased the performance, while the other embeddings did not affect the performance much (Supplementary Figure S7A). For the combined data, MUT- and EXP-based embeddings showed higher performance when included, whereas the inclusion of CLI-based embedding did not affect the performance much (Supplementary Figure S7B).</p>
        <p>In addition, we analyzed SUPREME results for TCGA data in terms of the best- and worst-performing models. Specifically, we had 31 top models with a macro F1 score is ≥0.88, and 30 bottom models with a macro F1 score is ≤0.83. We counted how many times each datatype occurred in the top and bottom models. CNA- and CLI-based embeddings were used for 28 and 19 out of 31 top models, respectively. The least occurred embedding was EXP-based with only six models out of 31. For the bottom models, we had 25 models from EXP-based embedding, while we had the least occurred embedding from CNA-based embedding with only five models. This analysis showed that CNA-based embedding was the most selected to have higher performance, while EXP-based embedding was rarely selected, supporting our findings in this section and in single model analysis.</p>
      </sec>
      <sec id="SEC3-4-2">
        <title>Impact of features from each datatype</title>
        <p>To see the impact of the features from each datatype, we ran SUPREME excluding the features from every single datatype separately. For each datatype <inline-formula><tex-math id="M00055" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula>, we excluded <inline-formula><tex-math id="M00056" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula>-specific node features from patient similarity networks and also did not integrate them with node embeddings during subtype prediction, called <inline-formula><tex-math id="M00057" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$no\mathbb {Y}_f$\end{document}</tex-math></inline-formula>. Considering that <inline-formula><tex-math id="M00058" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula>-specific patient similarity network was generated based on <inline-formula><tex-math id="M00059" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula>-specific features, we compared only the combinations without <inline-formula><tex-math id="M00060" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula> (<inline-formula><tex-math id="M00061" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$2^{d-1}-1$\end{document}</tex-math></inline-formula> models) to ensure the differences were due to the <inline-formula><tex-math id="M00062" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula>-specific features. We compared <inline-formula><tex-math id="M00063" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$no\mathbb {Y}_f$\end{document}</tex-math></inline-formula> models against the corresponding SUPREME models (called <inline-formula><tex-math id="M00064" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$with\mathbb {Y}_f$\end{document}</tex-math></inline-formula>), to show the importance of <inline-formula><tex-math id="M00065" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula>-specific features.</p>
        <p>When we excluded features from any datatype, we observed a lower or comparable performance (Figures <xref rid="F5" ref-type="fig">5</xref> and Supplementary Figure S8). The performance drop was significant for all the datatypes on TCGA, and gene expression and copy number aberration datatypes on METABRIC (Supplementary Figure S8A). The drop with the exclusion of the gene expression features was more drastic and it was consistent for all three datasets (Supplementary Figure S8B), supporting the importance of gene expression features for breast cancer (in agreement with findings in single model analysis).</p>
        <fig position="float" id="F5">
          <label>Figure 5.</label>
          <caption>
            <p>Analysis of features from each datatype. Violin plot of macro F1 scores for the models excluding the features from each datatype (<italic toggle="yes">no</italic><inline-formula><tex-math id="M00066" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}_f$\end{document}</tex-math></inline-formula> models, where <inline-formula><tex-math id="M00067" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}$\end{document}</tex-math></inline-formula> is the datatype whose features are completely excluded) versus corresponding SUPREME models (<italic toggle="yes">with</italic><inline-formula><tex-math id="M00068" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbb {Y}_f$\end{document}</tex-math></inline-formula> models) on TCGA data. Significance level was measured between <italic toggle="yes">with</italic> and <italic toggle="yes">no</italic> cases of the same datatype (Wilcoxon rank-sum test p-value to compare the distribution of violin plots representing the significance &lt;0.001 by ***, else if &lt;0.01 by **, and else if &lt;0.05 by *). See Figure <xref rid="F1" ref-type="fig">1</xref> for the abbreviations of datatypes.</p>
          </caption>
          <graphic xlink:href="lqad063fig5" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec id="SEC3-5">
      <title>Ablation studies</title>
      <p>We compared our tool with its variations when some steps were skipped to assess their importance (Table <xref rid="tbl3" ref-type="table">3</xref>). A comparison of SUPREME with SUPREME- showed the importance of raw feature integration. Also, to show the importance of GCN-based approaches, we trained the same ML algorithm (MLP in our case) using only the raw features and compared it with the SUPREME, which was based on the same raw features and additional patient embeddings.</p>
      <table-wrap position="float" id="tbl3">
        <label>Table 3.</label>
        <caption>
          <p>Ablation studies</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Comparison/Section</th>
              <th rowspan="1" colspan="1">Measures impact of</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">SUPREME vs. SUPREME-</td>
              <td rowspan="1" colspan="1">Raw feature integration</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SUPREME vs. MLP</td>
              <td rowspan="1" colspan="1">GCN utilization</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Single model section</td>
              <td rowspan="1" colspan="1">The used method with only one datatype</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SUPREME vs. MLP in single model section</td>
              <td rowspan="1" colspan="1">GCN utilization with only one datatype</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">This section</td>
              <td rowspan="1" colspan="1">Node features</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>To show the impact of each datatype separately, we demonstrated the performance of SUPREME models based on a single data type. We also compared SUPREME with other methods that can work with a single data modality only. To show the importance of embeddings at a single datatype level, we compared SUPREME with the MLP model trained on the features from the corresponding datatype.</p>
      <p>In addition to these studies, here, we also checked the overall impact of node features on the prediction tasks. To do that, instead of node features, we generated one-hot encoded features and evaluated SUPREME on TCGA data. We had macro F1 score as 0.75 ± 0.01, weighted F1 score as 0.83 ± 0.01, and accuracy as 0.84 ± 0.01. These results suggest that node features were important, dropping the evaluation performance drastically. This was expected as biological features are highly effective in determining the subtypes of breast cancer.</p>
    </sec>
    <sec id="SEC3-6">
      <title>SUPREME was model-agnostic outperforming other approaches in different domains</title>
      <p>To show the model-agnostic feature, we evaluated SUPREME on different domains. For that purpose, we generated two meta-path-based networks from the heterogeneous network of ACM and IMDB data. Since we had only two networks for these datasets, we shared the results for individual networks and the integrated one. Based on all six evaluation metrics, SUPREME outperformed other baseline methods on both datasets (Table <xref rid="tbl4" ref-type="table">4</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S6</xref>). As compared to MLP, we had increased performance showing the importance of graph utilization. Single models from GCN and SUPREME were not as good as the integrated one, showing the importance of SUPREME’s integrative nature.</p>
      <table-wrap position="float" id="tbl4">
        <label>Table 4.</label>
        <caption>
          <p>Macro F1 scores for IMDB and ACM datasets. [Macro F1: Macro-averaged F1 scores, GCN<sub><italic toggle="yes">x</italic></sub>: Result with x<sup>th</sup> network]. *Three results: First row with the first network, second row with the second network, and third row integrating the first and second networks. The first networks are based on movie-director-movie and paper-subject-paper meta-paths; and the second networks are based on movie-actor-movie and paper-author-paper meta-paths in IMDB and ACM datasets, respectively</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Method</th>
              <th rowspan="1" colspan="1">IMDB</th>
              <th rowspan="1" colspan="1">ACM</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">MLP</td>
              <td rowspan="1" colspan="1">0.53 ± 0.01</td>
              <td rowspan="1" colspan="1">0.90 ± 0.01</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SVM</td>
              <td rowspan="1" colspan="1">0.55 ± 0.00</td>
              <td rowspan="1" colspan="1">0.89 ± 0.00</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RF</td>
              <td rowspan="1" colspan="1">0.48 ± 0.00</td>
              <td rowspan="1" colspan="1">0.89 ± 0.00</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GCN<sub>1</sub></td>
              <td rowspan="1" colspan="1">0.56 ± 0.00</td>
              <td rowspan="1" colspan="1">0.70 ± 0.00</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GCN<sub>2</sub></td>
              <td rowspan="1" colspan="1">0.51 ± 0.00</td>
              <td rowspan="1" colspan="1">0.91 ± 0.00</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">0.58 ± 0.01</td>
              <td rowspan="1" colspan="1">0.91 ± 0.01</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SUPREME*</td>
              <td rowspan="1" colspan="1">0.55 ± 0.02</td>
              <td rowspan="1" colspan="1">0.92 ± 0.01</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">
                <bold>0.61 ± 0.02</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.94 ± 0.00</bold>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>According to these results, the first network of IMDB data (the network based on movie-director-movie meta-path) and the second network of ACM data (the network based on paper-author-paper meta-path) were more informative. This is not surprising that movie-director-movie association was more important than movie-actor-movie on movie genre prediction task on IMDB data. This was consistent based on the GCN runs and single models of SUPREME runs. Even though there is not a big difference on individual networks of ACM data for SUPREME, we see a big difference on GCN runs, showing the importance of our methodology utilizing embedding along with node features.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="SEC4">
    <title>DISCUSSION</title>
    <p>In this study, we introduced SUPREME, a novel integrative approach utilizing GCNs on multiple similarity networks where nodes are attributed with multi-modal node features. We applied SUPREME to three different prediction tasks from five different datasets. We observed that SUPREME outperformed other methods on ACM and IMDB data based on six evaluation metrics (Table <xref rid="tbl4" ref-type="table">4</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S6</xref>). On breast cancer subtyping, we compared SUPREME with seven cancer (sub)type prediction tools and baseline methods and observed that SUPREME substantially outperformed or was on par with them based on macro F1 score, accuracy, and weighted F1 score (Figures <xref rid="F2" ref-type="fig">2</xref>, Supplementary Figures S3 and S4, and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>). To demonstrate the consistency of the performance for individual SUPREME models, we shared the distribution of standard deviation of SUPREME models (Supplementary Figures S9, S10, and S11). We differentiated Normal-like subtype, which has the smallest sample size for three datasets, significantly better than all other tools on all three datasets showing SUPREME’s robustness even for minority classes (Supplementary Figure S5). We made SUPREME a publicly available tool at <ext-link xlink:href="https://github.com/bozdaglab/SUPREME" ext-link-type="uri">https://github.com/bozdaglab/SUPREME</ext-link> (under Creative Commons Attribution Non Commercial 4.0 International Public License) for researchers, biologists, and clinicians to utilize.</p>
    <p>We applied survival analysis to see the power of the methods to differentiate subtypes having significant survival differences. Using TCGA data, we compared our tool with nine popular integrative cancer subtype differentiating tools and baseline methods and SUPREME had consistently significant survival differences between predicted subtypes outperforming the other tools (Figure <xref rid="F3" ref-type="fig">3</xref>).</p>
    <p>Based on the majority of predictions, we determined ensemble subtype labels, most of which had high support from individual models (<xref rid="sup1" ref-type="supplementary-material">Supplemental File 3</xref>). We observed that survival difference between these ensemble subtypes was more significant than survival difference between gene expression-based ground truth (i.e., PAM50) subtypes (Supplementary Figure S6). These results suggest that some survival-related characteristics cannot be explained by gene expression data alone. SUPREME was able to extract these survival-related characteristics utilizing additional data modalities. SUPREME’s ensemble label predictions that were different from ground truth with high support could be further examined by biologists and clinicians.</p>
    <p>To show the effect of main steps of SUPREME, we performed an ablation study. In addition, we analyzed datatype-specific embeddings and datatype-specific features. We found that gene expression features were highly important for single models and overall, as expected for breast cancer. Findings about the important embeddings of datasets were supported by SUPREME- single models, where models were fed by only one embedding. We observed that patient embeddings were mostly more informative than raw features. Integrating raw features with patient embeddings usually improved the model performance (Figures <xref rid="F2" ref-type="fig">2</xref> and Supplementary Figure S3) except for raw features from few datatypes in single datatype-based models (Table <xref rid="tbl2" ref-type="table">2</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S3, S4, and S5</xref>).</p>
    <p>To compare the performance when we do not utilize the local neighborhood, we ran SUPREME- on TCGA data with the EXP-based single model when we do not have any neighbors than the patient itself. In that model, we had a macro F1 score of 0.85 ± 0.02 for SUPREME-, which was much higher than the original EXP-based model of SUPREME-. This model was even better than the EXP-based single model of SUPREME. This might suggest that EXP-based patient features themselves could perform better than neighborhood-convolved features because the ground truth utilizes patient features themselves to decide the subtype labels. Similarly, because of that, we might see a performance improvement when we add EXP-based raw features.</p>
    <p>SUPREME provides four options of ML algorithms to integrate embeddings and raw features, namely MLP, RF, SVM, and XGBoost. We ran SUPREME with all these choices and compared performances (<xref rid="sup1" ref-type="supplementary-material">Supplemental File 1</xref>, Supplementary Figures S1 and S2, and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>). RF and XGBoost had a low performance for some models. Overall, SVM had a good performance on every three datasets, however, it did not converge for some models. For this study, we chose MLP due to its high and consistent prediction performance for all three datasets and its low running time.</p>
    <p>In our experiments, we observed a high number of edges in MUT-based patient similarity networks as there were many patient pairs with the same similarity. Furthermore, the MUT-based models on TCGA data had high predictive performance, whereas these models had low predictive performance on METABRIC and the combined datasets. These discrepancies were mainly due to the sparse nature of the binary mutation features. For the special datatypes with binary-like sparse values like mutation, patient similarity networks and extracted features could be generated in a more sophisticated way such as based on the functional effect of these mutations (<xref rid="B36" ref-type="bibr">36–39</xref>).</p>
    <p>SUPREME is extendable to any number of datatypes to integrate. For cases where many datasets are integrated, to avoid potential overfitting, SUPREME provides an optional feature selection step for raw features before training the final prediction model. Users could skip raw feature integration altogether when network-specific patient embeddings provide sufficient discriminatory power. Users could run SUPREME on their training/validation data by enabling/disabling these features to optimize their models. In addition, users could perform ablation studies on SUPREME to determine the most effective data modalities and their combinations. Depending on these results, for the final prediction, users could rely on the most effective model or an ensemble model utilizing the most promising features and networks.</p>
    <p>As a future direction, SUPREME could utilize attention mechanisms (<xref rid="B40" ref-type="bibr">40–42</xref>), which allows getting weighted contributions from different datatypes, and also weighted neighborhoods from networks. In addition to multiomics datatypes, there are some regulatory relations such as competing endogenous RNA (ceRNA) regulation, which has been recently discovered with important insights into cancer (<xref rid="B43" ref-type="bibr">43</xref>). In our recent work, we inferred ceRNA interactions in breast cancer (<xref rid="B44" ref-type="bibr">44</xref>). To adopt this kind of regulatory relations, SUPREME could be improved to utilize patient similarity networks based on gene regulatory interactions and more complex patient relations. By improving the existing methodologies with recent advances in the literature, we can obtain more clear cancer subtype groups to pave the way for precision medicine.</p>
  </sec>
  <sec sec-type="data-availability" id="SEC5">
    <title>DATA AVAILABILITY</title>
    <p>TCGA data used in this article are available in The Cancer Genome Atlas project data portal at <ext-link xlink:href="https://portal.gdc.cancer.gov/" ext-link-type="uri">https://portal.gdc.cancer.gov/</ext-link>. The METABRIC data used in this article are deposited at the European Genome-Phenome Archive <ext-link xlink:href="http://www.ebi.ac.uk/ega/" ext-link-type="uri">http://www.ebi.ac.uk/ega/</ext-link>, which is hosted by the European Bioinformatics Institute, under accession number EGAS00000000083 [2]. The source code for our tool is available at <ext-link xlink:href="https://github.com/bozdaglab/SUPREME" ext-link-type="uri">https://github.com/bozdaglab/SUPREME</ext-link> (permanent doi: 10.6084/m9.figshare.23304062).</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>lqad063_Supplemental_Files</label>
      <media xlink:href="lqad063_supplemental_files.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ACK1">
    <title>ACKNOWLEDGEMENTS</title>
    <p>The authors would like to thank Ethan Rebello for his contributions to testing SUPREME and working on adding GPU support to it.</p>
  </ack>
  <sec id="SEC6">
    <title>SUPPLEMENTARY DATA</title>
    <p><ext-link xlink:href="https://academic.oup.com/nargab/article-lookup/doi/10.1093/nargab/lqad063#supplementary-data" ext-link-type="uri">Supplementary Data</ext-link> are available at NARGAB Online.</p>
  </sec>
  <sec id="SEC7">
    <title>FUNDING</title>
    <p>National Institute of General Medical Sciences of the National Institutes of Health [R35GM133657 to S.B.]; a summer research fellowship (to Z.N.K.) by the BioDiscovery Institute at the University of North Texas.</p>
    <p><italic toggle="yes">Conflict of interest statement</italic>. None declared.</p>
  </sec>
  <ref-list id="REF1">
    <title>REFERENCES</title>
    <ref id="B1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Waks</surname><given-names>A.G.</given-names></string-name>, <string-name><surname>Winer</surname><given-names>E.P.</given-names></string-name></person-group><article-title>Breast cancer treatment: a review</article-title>. <source>JAMA</source>. <year>2019</year>; <volume>321</volume>:<fpage>288</fpage>–<lpage>300</lpage>.<pub-id pub-id-type="pmid">30667505</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Curtis</surname><given-names>C.</given-names></string-name>, <string-name><surname>Shah</surname><given-names>S.P.</given-names></string-name>, <string-name><surname>Chin</surname><given-names>S.-F.</given-names></string-name>, <string-name><surname>Turashvili</surname><given-names>G.</given-names></string-name>, <string-name><surname>Rueda</surname><given-names>O.M.</given-names></string-name>, <string-name><surname>Dunning</surname><given-names>M.J.</given-names></string-name>, <string-name><surname>Speed</surname><given-names>D.</given-names></string-name>, <string-name><surname>Lynch</surname><given-names>A.G.</given-names></string-name>, <string-name><surname>Samarajiwa</surname><given-names>S.</given-names></string-name>, <string-name><surname>Yuan</surname><given-names>Y.</given-names></string-name><etal>et al</etal>.</person-group><article-title>The genomic and transcriptomic architecture of 2,000 breast tumours reveals novel subgroups</article-title>. <source>Nature</source>. <year>2012</year>; <volume>486</volume>:<fpage>346</fpage>–<lpage>352</lpage>.<pub-id pub-id-type="pmid">22522925</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Verhaak</surname><given-names>R.G.</given-names></string-name>, <string-name><surname>Hoadley</surname><given-names>K.A.</given-names></string-name>, <string-name><surname>Purdom</surname><given-names>E.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>V.</given-names></string-name>, <string-name><surname>Qi</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Wilkerson</surname><given-names>M.D.</given-names></string-name>, <string-name><surname>Miller</surname><given-names>C.R.</given-names></string-name>, <string-name><surname>Ding</surname><given-names>L.</given-names></string-name>, <string-name><surname>Golub</surname><given-names>T.</given-names></string-name>, <string-name><surname>Mesirov</surname><given-names>J.P.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Integrated genomic analysis identifies clinically relevant subtypes of glioblastoma characterized by abnormalities in PDGFRA, IDH1, EGFR, and NF1</article-title>. <source>Cancer Cell</source>. <year>2010</year>; <volume>17</volume>:<fpage>98</fpage>–<lpage>110</lpage>.<pub-id pub-id-type="pmid">20129251</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Noushmehr</surname><given-names>H.</given-names></string-name>, <string-name><surname>Weisenberger</surname><given-names>D.J.</given-names></string-name>, <string-name><surname>Diefes</surname><given-names>K.</given-names></string-name>, <string-name><surname>Phillips</surname><given-names>H.S.</given-names></string-name>, <string-name><surname>Pujara</surname><given-names>K.</given-names></string-name>, <string-name><surname>Berman</surname><given-names>B.P.</given-names></string-name>, <string-name><surname>Pan</surname><given-names>F.</given-names></string-name>, <string-name><surname>Pelloski</surname><given-names>C.E.</given-names></string-name>, <string-name><surname>Sulman</surname><given-names>E.P.</given-names></string-name>, <string-name><surname>Bhat</surname><given-names>K.P.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Identification of a CpG island methylator phenotype that defines a distinct subgroup of glioma</article-title>. <source>Cancer Cell</source>. <year>2010</year>; <volume>17</volume>:<fpage>510</fpage>–<lpage>522</lpage>.<pub-id pub-id-type="pmid">20399149</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baysan</surname><given-names>M.</given-names></string-name>, <string-name><surname>Bozdag</surname><given-names>S.</given-names></string-name>, <string-name><surname>Cam</surname><given-names>M.C.</given-names></string-name>, <string-name><surname>Kotliarova</surname><given-names>S.</given-names></string-name>, <string-name><surname>Ahn</surname><given-names>S.</given-names></string-name>, <string-name><surname>Walling</surname><given-names>J.</given-names></string-name>, <string-name><surname>Killian</surname><given-names>J.K.</given-names></string-name>, <string-name><surname>Stevenson</surname><given-names>H.</given-names></string-name>, <string-name><surname>Meltzer</surname><given-names>P.</given-names></string-name>, <string-name><surname>Fine</surname><given-names>H.A.</given-names></string-name></person-group><article-title>G-cimp status prediction of glioblastoma samples using mRNA expression data</article-title>. <source>PloS One</source>. <year>2012</year>; <volume>7</volume>:<fpage>e47839</fpage>.<pub-id pub-id-type="pmid">23139755</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vural</surname><given-names>S.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Guda</surname><given-names>C.</given-names></string-name></person-group><article-title>Classification of breast cancer patients using somatic mutation profiles and machine learning approaches</article-title>. <source>BMC Syst. Biol.</source><year>2016</year>; <volume>10</volume>:<fpage>263</fpage>–<lpage>276</lpage>.</mixed-citation>
    </ref>
    <ref id="B7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Youssef</surname><given-names>Y.M.</given-names></string-name>, <string-name><surname>White</surname><given-names>N.M.</given-names></string-name>, <string-name><surname>Grigull</surname><given-names>J.</given-names></string-name>, <string-name><surname>Krizova</surname><given-names>A.</given-names></string-name>, <string-name><surname>Samy</surname><given-names>C.</given-names></string-name>, <string-name><surname>Mejia-Guerrero</surname><given-names>S.</given-names></string-name>, <string-name><surname>Evans</surname><given-names>A.</given-names></string-name>, <string-name><surname>Yousef</surname><given-names>G.M.</given-names></string-name></person-group><article-title>Accurate molecular classification of kidney cancer subtypes using microRNA signature</article-title>. <source>Eur. Urol.</source><year>2011</year>; <volume>59</volume>:<fpage>721</fpage>–<lpage>730</lpage>.<pub-id pub-id-type="pmid">21272993</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <label>8.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ferlay</surname><given-names>J.</given-names></string-name>, <string-name><surname>Ervik</surname><given-names>M.</given-names></string-name>, <string-name><surname>Lam</surname><given-names>F.</given-names></string-name>, <string-name><surname>Colombet</surname><given-names>M.</given-names></string-name>, <string-name><surname>Mery</surname><given-names>L.</given-names></string-name>, <string-name><surname>Piñeros</surname><given-names>M.</given-names></string-name>, <string-name><surname>Znaor</surname><given-names>A.</given-names></string-name>, <string-name><surname>Soerjomataram</surname><given-names>I.</given-names></string-name>, <string-name><surname>Bray</surname><given-names>F.</given-names></string-name></person-group><article-title>Global cancer observatory: cancer today. Lyon: International Agency for Research on Cancer, 2018</article-title>. <year>2020</year>; </mixed-citation>
    </ref>
    <ref id="B9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anderson</surname><given-names>W.F.</given-names></string-name>, <string-name><surname>Chatterjee</surname><given-names>N.</given-names></string-name>, <string-name><surname>Ershler</surname><given-names>W.B.</given-names></string-name>, <string-name><surname>Brawley</surname><given-names>O.W.</given-names></string-name></person-group><article-title>Estrogen receptor breast cancer phenotypes in the Surveillance, Epidemiology, and End Results database</article-title>. <source>Breast Cancer Res. Treat.</source><year>2002</year>; <volume>76</volume>:<fpage>27</fpage>–<lpage>36</lpage>.<pub-id pub-id-type="pmid">12408373</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dietze</surname><given-names>E.C.</given-names></string-name>, <string-name><surname>Sistrunk</surname><given-names>C.</given-names></string-name>, <string-name><surname>Miranda-Carboni</surname><given-names>G.</given-names></string-name>, <string-name><surname>O’regan</surname><given-names>R.</given-names></string-name>, <string-name><surname>Seewaldt</surname><given-names>V.L.</given-names></string-name></person-group><article-title>Triple-negative breast cancer in African-American women: disparities versus biology</article-title>. <source>Nat. Rev. Cancer</source>. <year>2015</year>; <volume>15</volume>:<fpage>248</fpage>–<lpage>254</lpage>.<pub-id pub-id-type="pmid">25673085</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parker</surname><given-names>J.S.</given-names></string-name>, <string-name><surname>Mullins</surname><given-names>M.</given-names></string-name>, <string-name><surname>Cheang</surname><given-names>M.C.</given-names></string-name>, <string-name><surname>Leung</surname><given-names>S.</given-names></string-name>, <string-name><surname>Voduc</surname><given-names>D.</given-names></string-name>, <string-name><surname>Vickery</surname><given-names>T.</given-names></string-name>, <string-name><surname>Davies</surname><given-names>S.</given-names></string-name>, <string-name><surname>Fauron</surname><given-names>C.</given-names></string-name>, <string-name><surname>He</surname><given-names>X.</given-names></string-name>, <string-name><surname>Hu</surname><given-names>Z.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Supervised risk predictor of breast cancer based on intrinsic subtypes</article-title>. <source>J. Clin. Oncol.</source><year>2009</year>; <volume>27</volume>:<fpage>1160</fpage>.<pub-id pub-id-type="pmid">19204204</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koboldt</surname><given-names>D.</given-names></string-name>, <string-name><surname>Fulton</surname><given-names>R.</given-names></string-name>, <string-name><surname>McLellan</surname><given-names>M.</given-names></string-name>, <string-name><surname>Schmidt</surname><given-names>H.</given-names></string-name>, <string-name><surname>Kalicki-Veizer</surname><given-names>J.</given-names></string-name>, <string-name><surname>McMichael</surname><given-names>J.</given-names></string-name>, <string-name><surname>Fulton</surname><given-names>L.</given-names></string-name>, <string-name><surname>Dooling</surname><given-names>D.</given-names></string-name>, <string-name><surname>Ding</surname><given-names>L.</given-names></string-name>, <string-name><surname>Mardis</surname><given-names>E.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Comprehensive molecular portraits of human breast tumours</article-title>. <source>Nature</source>. <year>2012</year>; <volume>490</volume>:<fpage>61</fpage>–<lpage>70</lpage>.<pub-id pub-id-type="pmid">23000897</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname><given-names>R.</given-names></string-name>, <string-name><surname>Olshen</surname><given-names>A.B.</given-names></string-name>, <string-name><surname>Ladanyi</surname><given-names>M.</given-names></string-name></person-group><article-title>Integrative clustering of multiple genomic data types using a joint latent variable model with application to breast and lung cancer subtype analysis</article-title>. <source>Bioinformatics</source>. <year>2009</year>; <volume>25</volume>:<fpage>2906</fpage>–<lpage>2912</lpage>.<pub-id pub-id-type="pmid">19759197</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>B.</given-names></string-name>, <string-name><surname>Mezlini</surname><given-names>A.M.</given-names></string-name>, <string-name><surname>Demir</surname><given-names>F.</given-names></string-name>, <string-name><surname>Fiume</surname><given-names>M.</given-names></string-name>, <string-name><surname>Tu</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Brudno</surname><given-names>M.</given-names></string-name>, <string-name><surname>Haibe-Kains</surname><given-names>B.</given-names></string-name>, <string-name><surname>Goldenberg</surname><given-names>A.</given-names></string-name></person-group><article-title>Similarity network fusion for aggregating data types on a genomic scale</article-title>. <source>Nat. Methods</source>. <year>2014</year>; <volume>11</volume>:<fpage>333</fpage>–<lpage>337</lpage>.<pub-id pub-id-type="pmid">24464287</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nguyen</surname><given-names>H.</given-names></string-name>, <string-name><surname>Shrestha</surname><given-names>S.</given-names></string-name>, <string-name><surname>Draghici</surname><given-names>S.</given-names></string-name>, <string-name><surname>Nguyen</surname><given-names>T.</given-names></string-name></person-group><article-title>PINSPlus: a tool for tumor subtype discovery in integrated genomic data</article-title>. <source>Bioinformatics</source>. <year>2019</year>; <volume>35</volume>:<fpage>2843</fpage>–<lpage>2846</lpage>.<pub-id pub-id-type="pmid">30590381</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gori</surname><given-names>M.</given-names></string-name>, <string-name><surname>Monfardini</surname><given-names>G.</given-names></string-name>, <string-name><surname>Scarselli</surname><given-names>F.</given-names></string-name></person-group><article-title>A new model for learning in graph domains</article-title>. <source>Proceedings. 2005 IEEE International Joint Conference on Neural Networks</source>. <year>2005</year>; <volume>2</volume>:<fpage>729</fpage>–<lpage>734</lpage>.</mixed-citation>
    </ref>
    <ref id="B17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scarselli</surname><given-names>F.</given-names></string-name>, <string-name><surname>Gori</surname><given-names>M.</given-names></string-name>, <string-name><surname>Tsoi</surname><given-names>A.C.</given-names></string-name>, <string-name><surname>Hagenbuchner</surname><given-names>M.</given-names></string-name>, <string-name><surname>Monfardini</surname><given-names>G.</given-names></string-name></person-group><article-title>The graph neural network model</article-title>. <source>IEEE Trans. Neur. Networ.</source><year>2008</year>; <volume>20</volume>:<fpage>61</fpage>–<lpage>80</lpage>.</mixed-citation>
    </ref>
    <ref id="B18">
      <label>18.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kipf</surname><given-names>T.N.</given-names></string-name>, <string-name><surname>Welling</surname><given-names>M.</given-names></string-name></person-group><article-title>Semi-supervised classification with graph convolutional networks</article-title>. <year>2016</year>; <comment>arXiv doi:</comment><comment>22 February 2017, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.1609.02907</pub-id>.</mixed-citation>
    </ref>
    <ref id="B19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoff</surname><given-names>P.D.</given-names></string-name>, <string-name><surname>Raftery</surname><given-names>A.E.</given-names></string-name>, <string-name><surname>Handcock</surname><given-names>M.S.</given-names></string-name></person-group><article-title>Latent space approaches to social network analysis</article-title>. <source>J. Am. Stat. Assoc.</source><year>2002</year>; <volume>97</volume>:<fpage>1090</fpage>–<lpage>1098</lpage>.</mixed-citation>
    </ref>
    <ref id="B20">
      <label>20.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Perozzi</surname><given-names>B.</given-names></string-name>, <string-name><surname>Al-Rfou</surname><given-names>R.</given-names></string-name>, <string-name><surname>Skiena</surname><given-names>S.</given-names></string-name></person-group><article-title>Deepwalk: online learning of social representations</article-title>. <source>Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</source>. <year>2014</year>; <fpage>701</fpage>–<lpage>710</lpage>.</mixed-citation>
    </ref>
    <ref id="B21">
      <label>21.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Grover</surname><given-names>A.</given-names></string-name>, <string-name><surname>Leskovec</surname><given-names>J.</given-names></string-name></person-group><article-title>node2vec: scalable feature learning for networks</article-title>. <source>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</source>. <year>2016</year>; <fpage>855</fpage>–<lpage>864</lpage>.</mixed-citation>
    </ref>
    <ref id="B22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hamilton</surname><given-names>W.</given-names></string-name>, <string-name><surname>Ying</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Leskovec</surname><given-names>J.</given-names></string-name></person-group><article-title>Inductive representation learning on large graphs</article-title>. <source>Adv. Neural Inf. Process. Syst.</source><year>2017</year>; <volume>30</volume>:<uri xlink:href="https://proceedings.neurips.cc/paper_files/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html">https://proceedings.neurips.cc/paper_files/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html</uri>.</mixed-citation>
    </ref>
    <ref id="B23">
      <label>23.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Rhee</surname><given-names>S.</given-names></string-name>, <string-name><surname>Seo</surname><given-names>S.</given-names></string-name>, <string-name><surname>Kim</surname><given-names>S.</given-names></string-name></person-group><article-title>Hybrid approach of relation network and localized graph convolutional filtering for breast cancer subtype classification</article-title>. <year>2017</year>; <comment>bioRxiv doi:</comment><comment>15 June 2018, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.1711.05859</pub-id>.</mixed-citation>
    </ref>
    <ref id="B24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mohamed</surname><given-names>S.K.</given-names></string-name>, <string-name><surname>Nováček</surname><given-names>V.</given-names></string-name>, <string-name><surname>Nounu</surname><given-names>A.</given-names></string-name></person-group><article-title>Discovering protein drug targets using knowledge graph embeddings</article-title>. <source>Bioinformatics</source>. <year>2020</year>; <volume>36</volume>:<fpage>603</fpage>–<lpage>610</lpage>.<pub-id pub-id-type="pmid">31368482</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zitnik</surname><given-names>M.</given-names></string-name>, <string-name><surname>Agrawal</surname><given-names>M.</given-names></string-name>, <string-name><surname>Leskovec</surname><given-names>J.</given-names></string-name></person-group><article-title>Modeling polypharmacy side effects with graph convolutional networks</article-title>. <source>Bioinformatics</source>. <year>2018</year>; <volume>34</volume>:<fpage>i457</fpage>–<lpage>i466</lpage>.<pub-id pub-id-type="pmid">29949996</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ramirez</surname><given-names>R.</given-names></string-name>, <string-name><surname>Chiu</surname><given-names>Y.-C.</given-names></string-name>, <string-name><surname>Hererra</surname><given-names>A.</given-names></string-name>, <string-name><surname>Mostavi</surname><given-names>M.</given-names></string-name>, <string-name><surname>Ramirez</surname><given-names>J.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Huang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Jin</surname><given-names>Y.-F.</given-names></string-name></person-group><article-title>Classification of cancer types using graph convolutional neural networks</article-title>. <source>Front. Phys.</source><year>2020</year>; <volume>8</volume>:<fpage>203</fpage>.<pub-id pub-id-type="pmid">33437754</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>T.</given-names></string-name>, <string-name><surname>Shao</surname><given-names>W.</given-names></string-name>, <string-name><surname>Huang</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Tang</surname><given-names>H.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>J.</given-names></string-name>, <string-name><surname>Ding</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Huang</surname><given-names>K.</given-names></string-name></person-group><article-title>MOGONET integrates multi-omics data using graph convolutional networks allowing patient classification and biomarker identification</article-title>. <source>Nat. Commun.</source><year>2021</year>; <volume>12</volume>:<fpage>1</fpage>–<lpage>13</lpage>.<pub-id pub-id-type="pmid">33397941</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kursa</surname><given-names>M.B.</given-names></string-name>, <string-name><surname>Rudnicki</surname><given-names>W.R.</given-names></string-name></person-group><article-title>Feature selection with the Boruta package</article-title>. <source>J. Stat. Softw.</source><year>2010</year>; <volume>36</volume>:<fpage>1</fpage>–<lpage>13</lpage>.</mixed-citation>
    </ref>
    <ref id="B29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gower</surname><given-names>J.C.</given-names></string-name></person-group><article-title>A general coefficient of similarity and some of its properties</article-title>. <source>Biometrics</source>. <year>1971</year>; <volume>27</volume>:<fpage>857</fpage>–<lpage>871</lpage>.</mixed-citation>
    </ref>
    <ref id="B30">
      <label>30.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Maechler</surname><given-names>M.</given-names></string-name>, <string-name><surname>Rousseeuw</surname><given-names>P.</given-names></string-name>, <string-name><surname>Struyf</surname><given-names>A.</given-names></string-name>, <string-name><surname>Hubert</surname><given-names>M.</given-names></string-name>, <string-name><surname>Hornik</surname><given-names>K.</given-names></string-name></person-group><article-title>cluster: cluster analysis basics and extensions, R package version 2.1.3</article-title>. <year>2022</year>; <uri xlink:href="https://cran.r-project.org/web/packages/cluster/citation.html">https://cran.r-project.org/web/packages/cluster/citation.html</uri>.</mixed-citation>
    </ref>
    <ref id="B31">
      <label>31.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kesimoglu</surname><given-names>Z.N.</given-names></string-name>, <string-name><surname>Bozdag</surname><given-names>S.</given-names></string-name></person-group><article-title>GRAF: graph attention-aware fusion networks</article-title>. <year>2023</year>; <comment>bioRxiv doi:</comment><comment>29 March 2023, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/pdf/2303.16781.pdf">https://arxiv.org/pdf/2303.16781.pdf</uri>.</mixed-citation>
    </ref>
    <ref id="B32">
      <label>32.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Fey</surname><given-names>M.</given-names></string-name>, <string-name><surname>Lenssen</surname><given-names>J.E.</given-names></string-name></person-group><article-title>Fast graph representation learning with PyTorch Geometric</article-title>. <year>2019</year>; <comment>bioRxiv doi:</comment><comment>25 April 2019, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.1903.02428</pub-id>.</mixed-citation>
    </ref>
    <ref id="B33">
      <label>33.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>M.</given-names></string-name>, <string-name><surname>Zheng</surname><given-names>D.</given-names></string-name>, <string-name><surname>Ye</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Gan</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Li</surname><given-names>M.</given-names></string-name>, <string-name><surname>Song</surname><given-names>X.</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>J.</given-names></string-name>, <string-name><surname>Ma</surname><given-names>C.</given-names></string-name>, <string-name><surname>Yu</surname><given-names>L.</given-names></string-name>, <string-name><surname>Gai</surname><given-names>Y.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Deep graph library: a graph-centric, highly-performant package for graph neural networks</article-title>. <year>2019</year>; <comment>bioRxiv doi:</comment><comment>25 August 2020, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.1909.01315</pub-id>.</mixed-citation>
    </ref>
    <ref id="B34">
      <label>34.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kingma</surname><given-names>D.P.</given-names></string-name>, <string-name><surname>Ba</surname><given-names>J.</given-names></string-name></person-group><article-title>Adam: a method for stochastic optimization</article-title>. <year>2014</year>; <comment>bioRxiv doi:</comment><comment>30 January 2017, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.1412.6980</pub-id>.</mixed-citation>
    </ref>
    <ref id="B35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gao</surname><given-names>F.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>W.</given-names></string-name>, <string-name><surname>Tan</surname><given-names>M.</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>L.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Fessler</surname><given-names>E.</given-names></string-name>, <string-name><surname>Vermeulen</surname><given-names>L.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.</given-names></string-name></person-group><article-title>DeepCC: a novel deep learning-based framework for cancer molecular subtype classification</article-title>. <source>Oncogenesis</source>. <year>2019</year>; <volume>8</volume>:<fpage>1</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">30631034</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khurana</surname><given-names>E.</given-names></string-name>, <string-name><surname>Fu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>J.</given-names></string-name>, <string-name><surname>Gerstein</surname><given-names>M.</given-names></string-name></person-group><article-title>Interpretation of genomic variants using a unified biological network approach</article-title>. <source>PLoS Comput. Biol.</source><year>2013</year>; <volume>9</volume>:<fpage>e1002886</fpage>.<pub-id pub-id-type="pmid">23505346</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kircher</surname><given-names>M.</given-names></string-name>, <string-name><surname>Witten</surname><given-names>D.M.</given-names></string-name>, <string-name><surname>Jain</surname><given-names>P.</given-names></string-name>, <string-name><surname>O’roak</surname><given-names>B.J.</given-names></string-name>, <string-name><surname>Cooper</surname><given-names>G.M.</given-names></string-name>, <string-name><surname>Shendure</surname><given-names>J.</given-names></string-name></person-group><article-title>A general framework for estimating the relative pathogenicity of human genetic variants</article-title>. <source>Nat. Genet.</source><year>2014</year>; <volume>46</volume>:<fpage>310</fpage>–<lpage>315</lpage>.<pub-id pub-id-type="pmid">24487276</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Leiserson</surname><given-names>M.D.</given-names></string-name>, <string-name><surname>Vandin</surname><given-names>F.</given-names></string-name>, <string-name><surname>Wu</surname><given-names>H.-T.</given-names></string-name>, <string-name><surname>Dobson</surname><given-names>J.R.</given-names></string-name>, <string-name><surname>Eldridge</surname><given-names>J.V.</given-names></string-name>, <string-name><surname>Thomas</surname><given-names>J.L.</given-names></string-name>, <string-name><surname>Papoutsaki</surname><given-names>A.</given-names></string-name>, <string-name><surname>Kim</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Niu</surname><given-names>B.</given-names></string-name>, <string-name><surname>McLellan</surname><given-names>M.</given-names></string-name><etal>et al</etal>.</person-group><article-title>Pan-cancer network analysis identifies combinations of rare somatic mutations across pathways and protein complexes</article-title>. <source>Nat. Genet.</source><year>2015</year>; <volume>47</volume>:<fpage>106</fpage>–<lpage>114</lpage>.<pub-id pub-id-type="pmid">25501392</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tokheim</surname><given-names>C.J.</given-names></string-name>, <string-name><surname>Papadopoulos</surname><given-names>N.</given-names></string-name>, <string-name><surname>Kinzler</surname><given-names>K.W.</given-names></string-name>, <string-name><surname>Vogelstein</surname><given-names>B.</given-names></string-name>, <string-name><surname>Karchin</surname><given-names>R.</given-names></string-name></person-group><article-title>Evaluating the evaluation of cancer driver genes</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><year>2016</year>; <volume>113</volume>:<fpage>14330</fpage>–<lpage>14335</lpage>.<pub-id pub-id-type="pmid">27911828</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vaswani</surname><given-names>A.</given-names></string-name>, <string-name><surname>Shazeer</surname><given-names>N.</given-names></string-name>, <string-name><surname>Parmar</surname><given-names>N.</given-names></string-name>, <string-name><surname>Uszkoreit</surname><given-names>J.</given-names></string-name>, <string-name><surname>Jones</surname><given-names>L.</given-names></string-name>, <string-name><surname>Gomez</surname><given-names>A.N.</given-names></string-name>, <string-name><surname>Kaiser</surname><given-names>Ł.</given-names></string-name>, <string-name><surname>Polosukhin</surname><given-names>I.</given-names></string-name></person-group><article-title>Attention is all you need</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2017</year>; <volume>30</volume>:<fpage>1</fpage>–<lpage>11</lpage>.</mixed-citation>
    </ref>
    <ref id="B41">
      <label>41.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Veličković</surname><given-names>P.</given-names></string-name>, <string-name><surname>Cucurull</surname><given-names>G.</given-names></string-name>, <string-name><surname>Casanova</surname><given-names>A.</given-names></string-name>, <string-name><surname>Romero</surname><given-names>A.</given-names></string-name>, <string-name><surname>Lio</surname><given-names>P.</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y.</given-names></string-name></person-group><article-title>Graph attention networks</article-title>. <year>2017</year>; <comment>bioRxiv doi:</comment><comment>4 February 2018, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.1710.10903</pub-id>.</mixed-citation>
    </ref>
    <ref id="B42">
      <label>42.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Brody</surname><given-names>S.</given-names></string-name>, <string-name><surname>Alon</surname><given-names>U.</given-names></string-name>, <string-name><surname>Yahav</surname><given-names>E.</given-names></string-name></person-group><article-title>How attentive are graph attention networks</article-title>. <year>2021</year>; <comment>bioRxiv doi:</comment><comment>31 January 2022, preprint: not peer reviewed</comment><pub-id pub-id-type="doi">10.48550/arXiv.2105.14491</pub-id>.</mixed-citation>
    </ref>
    <ref id="B43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Salmena</surname><given-names>L.</given-names></string-name>, <string-name><surname>Poliseno</surname><given-names>L.</given-names></string-name>, <string-name><surname>Tay</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Kats</surname><given-names>L.</given-names></string-name>, <string-name><surname>Pandolfi</surname><given-names>P.P.</given-names></string-name></person-group><article-title>A ceRNA hypothesis: the Rosetta Stone of a hidden RNA language</article-title>. <source>Cell</source>. <year>2011</year>; <volume>146</volume>:<fpage>353</fpage>–<lpage>358</lpage>.<pub-id pub-id-type="pmid">21802130</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kesimoglu</surname><given-names>Z.N.</given-names></string-name>, <string-name><surname>Bozdag</surname><given-names>S.</given-names></string-name></person-group><article-title>Crinet: a computational tool to infer genome-wide competing endogenous RNA (ceRNA) interactions</article-title>. <source>Plos One</source>. <year>2021</year>; <volume>16</volume>:<fpage>e0251399</fpage>.<pub-id pub-id-type="pmid">33983999</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
