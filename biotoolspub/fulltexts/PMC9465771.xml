<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr Neuroimage?>
<?submitter-system nihms?>
<?submitter-canonical-name Elsevier?>
<?submitter-canonical-id ELSEVIERAM?>
<?submitter-userid 8068823?>
<?submitter-authority myNCBI?>
<?submitter-login elsevieram?>
<?submitter-name Elsevier Author Support?>
<?domain nihpa?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">9215515</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">20498</journal-id>
    <journal-id journal-id-type="nlm-ta">Neuroimage</journal-id>
    <journal-id journal-id-type="iso-abbrev">Neuroimage</journal-id>
    <journal-title-group>
      <journal-title>NeuroImage</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1053-8119</issn>
    <issn pub-type="epub">1095-9572</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9465771</article-id>
    <article-id pub-id-type="pmid">35842095</article-id>
    <article-id pub-id-type="doi">10.1016/j.neuroimage.2022.119474</article-id>
    <article-id pub-id-type="manuscript">nihpa1833287</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SynthStrip: skull-stripping for any brain image</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Hoopes</surname>
          <given-names>Andrew</given-names>
        </name>
        <xref rid="A1" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mora</surname>
          <given-names>Jocelyn S.</given-names>
        </name>
        <xref rid="A1" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dalca</surname>
          <given-names>Adrian V.</given-names>
        </name>
        <xref rid="A1" ref-type="aff">a</xref>
        <xref rid="A2" ref-type="aff">b</xref>
        <xref rid="A3" ref-type="aff">c</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fischl</surname>
          <given-names>Bruce</given-names>
        </name>
        <xref rid="A1" ref-type="aff">a</xref>
        <xref rid="A2" ref-type="aff">b</xref>
        <xref rid="A3" ref-type="aff">c</xref>
        <xref rid="A4" ref-type="aff">d</xref>
        <xref rid="FN1" ref-type="author-notes">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hoffmann</surname>
          <given-names>Malte</given-names>
        </name>
        <xref rid="A1" ref-type="aff">a</xref>
        <xref rid="A2" ref-type="aff">b</xref>
        <xref rid="FN1" ref-type="author-notes">1</xref>
        <xref rid="CR1" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="A1"><label>a</label>Athinoula A. Martinos Center for Biomedical Imaging,
Massachusetts General Hospital, 149 13<sup>th</sup> St, Charlestown, MA, USA</aff>
    <aff id="A2"><label>b</label>Department of Radiology, Harvard Medical School, 25
Shattuck St, Boston, MA, USA</aff>
    <aff id="A3"><label>c</label>Computer Science and Artificial Intelligence Lab,
Massachusetts Institute of Technology, 32 Vassar St, Cambridge, MA, USA</aff>
    <aff id="A4"><label>d</label>Harvard-MIT Division of Health Sciences and Technology, 77
Massachusetts Ave, Cambridge, MA, USA</aff>
    <author-notes>
      <fn fn-type="equal" id="FN1">
        <label>1</label>
        <p id="P1">These authors contributed equally.</p>
      </fn>
      <corresp id="CR1"><label>*</label>Corresponding author.
<email>mhoffmann@mgh.harvard.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>2</day>
      <month>9</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <day>15</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>13</day>
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>15</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <volume>260</volume>
    <fpage>119474</fpage>
    <lpage>119474</lpage>
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>)</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P2">The removal of non-brain signal from magnetic resonance imaging (MRI)
data, known as skull-stripping, is an integral component of many neuroimage
analysis streams. Despite their abundance, popular classical skull-stripping
methods are usually tailored to images with specific acquisition properties,
namely near-isotropic resolution and T1-weighted (T1w) MRI contrast, which are
prevalent in research settings. As a result, existing tools tend to adapt poorly
to other image types, such as stacks of thick slices acquired with fast
spin-echo (FSE) MRI that are common in the clinic. While learning-based
approaches for brain extraction have gained traction in recent years, these
methods face a similar burden, as they are only effective for image types seen
during the training procedure. To achieve robust skull-stripping across a
landscape of imaging protocols, we introduce SynthStrip, a rapid, learning-based
brain-extraction tool. By leveraging anatomical segmentations to generate an
entirely synthetic training dataset with anatomies, intensity distributions, and
artifacts that far exceed the realistic range of medical images, SynthStrip
learns to successfully generalize to a variety of real acquired brain images,
removing the need for training data with target contrasts. We demonstrate the
efficacy of SynthStrip for a diverse set of image acquisitions and resolutions
across subject populations, ranging from newborn to adult. We show substantial
improvements in accuracy over popular skull-stripping baselines – all
with a single trained model. Our method and labeled evaluation data are
available at <ext-link xlink:href="https://w3id.org/synthstrip" ext-link-type="uri">https://w3id.org/synthstrip</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>Skull stripping</kwd>
      <kwd>Brain extraction</kwd>
      <kwd>Image synthesis</kwd>
      <kwd>MRI-contrast agnosticism</kwd>
      <kwd>Deep learning</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>1.</label>
    <title>Introduction</title>
    <p id="P3">Skull-stripping, also known as brain extraction, involves the removal of
non-brain tissue signal from magnetic resonance imaging (MRI) data. This process is
useful for anonymizing brain scans and a fundamental component of many neuroimage
analysis pipelines, such as FreeSurfer (<xref rid="R23" ref-type="bibr">Fischl,
2012</xref>), FSL (<xref rid="R42" ref-type="bibr">Jenkinson et al.,
2012</xref>), AFNI (<xref rid="R13" ref-type="bibr">Cox, 1996</xref>), and ANTs
(<xref rid="R6" ref-type="bibr">Avants et al., 2011</xref>). These packages
include tools that typically require brain-extracted input images and might perform
inaccurately, or even fail, without removal of irrelevant and distracting tissue.
One such class of algorithms that benefits from this systematic tissue extraction is
image registration, a core element of atlas-based segmentation and other analyses.
Nonlinear registration (<xref rid="R3" ref-type="bibr">Ashburner, 2007</xref>; <xref rid="R5" ref-type="bibr">Avants et al., 2008</xref>; <xref rid="R69" ref-type="bibr">Rueckert et al., 1999</xref>; <xref rid="R76" ref-type="bibr">Vercauteren et al., 2009</xref>) estimates local deformations between pairs of
images, and these algorithms tend to produce more accurate estimates when they can
focus entirely on the anatomy of interest (<xref rid="R51" ref-type="bibr">Klein et
al., 2009</xref>; <xref rid="R61" ref-type="bibr">Ou et al., 2014</xref>).
Similarly, skull-stripping increases the reliability of linear registration (<xref rid="R14" ref-type="bibr">Cox and Jesmanowicz, 1999</xref>; <xref rid="R26" ref-type="bibr">Friston et al., 1995</xref>; <xref rid="R34" ref-type="bibr">Hoffmann et al., 2015</xref>; <xref rid="R43" ref-type="bibr">Jenkinson and
Smith, 2001</xref>; <xref rid="R44" ref-type="bibr">Jiang et al., 1995</xref>;
<xref rid="R59" ref-type="bibr">Modat et al., 2014</xref>; <xref rid="R65" ref-type="bibr">Reuter et al., 2010</xref>) by excluding anatomy that deforms
non-rigidly, such as the eyes, jaw, and tongue (<xref rid="R1" ref-type="bibr">Andrade et al., 2018</xref>; <xref rid="R22" ref-type="bibr">Fein et al.,
2006</xref>; <xref rid="R25" ref-type="bibr">Fischmeister et al., 2013</xref>;
<xref rid="R35" ref-type="bibr">Hoffmann et al., 2020</xref>).</p>
    <p id="P4">Classical skull-stripping techniques are well-explored and widespread, but
popular methods are often tailored to images with specific modalities or acquisition
properties. Most commonly, these methods focus on three-dimensional (3D) T1-weighted
(T1w) MRI scans acquired with MPRAGE sequences (<xref rid="R52" ref-type="bibr">van
der Kouwe et al., 2008</xref>; <xref rid="R56" ref-type="bibr">Marques et al.,
2010</xref>; <xref rid="R60" ref-type="bibr">Mugler and Brookeman, 1990</xref>),
which are ubiquitous in neuroimaging research. While some skull-stripping tools
accommodate additional contrasts, these methods are ultimately limited to a
predefined set of viable image types and do not properly adapt to inputs outside
this set. For example, skull-stripping tools developed for near-isotropic, adult
brain images may perform poorly when applied to infant subjects or clinical scans
with thick slices, such as stacks of 2D fast spin-echo (FSE) acquisitions.</p>
    <p id="P5">When a suitable brain extraction method is not available for a particular
scan type, a common workaround involves skull-stripping a compatible image of the
same subject and computing a co-registration to propagate the extracted brain mask
to the target image of interest (<xref rid="R40" ref-type="bibr">Iglesias et al.,
2011</xref>). Unfortunately, an accurate intra-subject alignment can require
significant manual tuning because the target image still includes extra-cerebral
matter that may impede linear registration quality (<xref rid="R65" ref-type="bibr">Reuter et al., 2010</xref>). Crucially, this procedure also requires the
existence of an additional, strip-able image, often a high-resolution isotropic T1w
or T2-weighted (T2w) scan, which is rare, for example, in clinical screening
protocols, introducing a barrier to the clinical adoption of analysis tools.</p>
    <p id="P6">While classical algorithms for skull-stripping are limited by their
assumptions about the spatial features and intensity distributions in the input
images, supervised deep-learning approaches, which leverage convolutional neural
networks (CNNs), can, in principle, learn to extract a region of interest from any
image type given sufficient anatomical contrast and resolution. In practice, these
networks achieve high accuracy for data types observed during training, but their
performance often deteriorates on images with characteristics unseen during training
(<xref rid="R31" ref-type="bibr">Hendrycks et al., 2021</xref>; Hoffmann et al.,
2021b; <xref rid="R45" ref-type="bibr">Jog et al., 2019</xref>; <xref rid="R48" ref-type="bibr">Karani et al., 2018</xref>). In consequence, robust, supervised
learning-based approaches depend on the availability of a representative training
dataset that contains accurate ground-truth annotations and exposes the network to a
landscape of image types. While numerous public datasets provide access to widely
used MRI acquisitions for which target brain masks can be easily derived with
classical methods, curating a diverse training dataset with uncommon sequences and
sufficient anatomical variability is a challenging task that requires substantial
human effort. As a result, current deep-learning skull-stripping methods are trained
with few different data types and deliver state-of-the-art results only for
particular subsets of image characteristics (<xref rid="R38" ref-type="bibr">Hwang
et al., 2019</xref>; <xref rid="R50" ref-type="bibr">Kleesiek et al.,
2016</xref>; <xref rid="R71" ref-type="bibr">Salehi et al., 2017</xref>).</p>
    <p id="P7">Recently, a novel learning strategy alleviates the requirement for
representative acquired training data by optimizing networks with a wide array of
synthetic images, each generated directly from a precomputed label map (<xref rid="R7" ref-type="bibr">Billot et al., 2020</xref>; Hoffmann et al., 2021b).
This synthesis scheme enables networks to accurately carry out tasks on any image
type at evaluation-time without ever sampling real target acquisitions during
training, and it has been effectively employed for segmentation (<xref rid="R7" ref-type="bibr">Billot et al., 2020</xref>) and deformable image registration
(Hoffmann et al., 2021b). To build on deep-learning methods for brain extraction
while addressing their shortcomings, we adapt the synthesis technique and introduce
SynthStrip, a flexible brain-extraction tool that can be deployed universally on a
variety of brain images. By exposing a CNN to an arbitrary and deliberately
unrealistic range of anatomies, contrasts, and artifacts, we obtain a model that is
agnostic to acquisition specifics, as it never samples any real data during
training. Consequently, this scheme enables SynthStrip to extract the brain from a
wide array of neuroimaging data types, and we demonstrate its viability and
improvement over popular baselines using a varied test set that spans both research
scans and clinical exams (<xref rid="F1" ref-type="fig">Fig. 1</xref>). The test set
includes T1w, T2w, T2w fluid attenuated inversion recovery (T2-FLAIR), and
proton-density (PDw) contrasts as well as clinical FSE scans with slices and high
in-plane resolution, and low-resolution EPI, ranging across age and pathology. We
demonstrate the ability of SynthStrip to generalize beyond structural MRI, to MR
angiography (MRA), diffusion-weighted imaging (DWI), fluorodeoxyglucose positron
emission tomography (FDG-PET), and even computed tomography (CT). We make our
validation set publicly available to promote further development and evaluation of
brain-extraction tools.</p>
  </sec>
  <sec id="S2">
    <label>2.</label>
    <title>Related work</title>
    <p id="P8">In this section, we briefly review the automated brain-extraction techniques
that we use as baseline methods. We include both classical and deep-learning
baselines introduced over the last two decades, focusing in particular on those with
high efficacy and popularity in the research domain. For an exhaustive overview of
skull-stripping methods, see <xref rid="R21" ref-type="bibr">Fatima et al.,
2020</xref>.</p>
    <sec id="S3">
      <label>2.1.</label>
      <title>Classical skull-stripping</title>
      <p id="P9">Classical, or traditional, algorithms that remove non-brain image signal
vary substantially in their implementation (<xref rid="R13" ref-type="bibr">Cox,
1996</xref>; <xref rid="R20" ref-type="bibr">Eskildsen et al., 2012</xref>;
<xref rid="R40" ref-type="bibr">Iglesias et al., 2011</xref>; <xref rid="R68" ref-type="bibr">Roy et al., 2017</xref>; <xref rid="R72" ref-type="bibr">Ségonne et al., 2004</xref>; <xref rid="R73" ref-type="bibr">Shattuck et al., 2001</xref>; <xref rid="R74" ref-type="bibr">Smith, 2002</xref>). One common class of approaches
leverages a deformable mesh model to reconstruct a smooth boundary of the brain
matter surface. The widely-used Brain Extraction Tool (BET; <xref rid="R74" ref-type="bibr">Smith, 2002</xref>), distributed as part of the FSL package
(<xref rid="R42" ref-type="bibr">Jenkinson et al., 2012</xref>), utilizes
this technique by initializing a spherical mesh at the barycenter of the brain
and projecting mesh vertices outwards to model the brain border. Since BET uses
locally adaptive intensity thresholds to distinguish brain and non-brain voxels,
it generalizes to a variety of contrasts, such as T1w, T2w, and PDw. To prevent
surface leaks beyond the brain boundary, 3dSkullStrip, a component of AFNI
(<xref rid="R13" ref-type="bibr">Cox, 1996</xref>), extends the BET strategy
by considering information on the surface exterior, accounting for eyes,
ventricles, and skull.</p>
      <p id="P10">The popular hybrid approach (<xref rid="R72" ref-type="bibr">Ségonne et al., 2004</xref>) available in FreeSurfer also leverages
a deformable surface paradigm, combing it with a watershed algorithm and
statistical atlas to improve robustness. First, the watershed establishes an
estimate of the white-matter mask, which is then refined to the brain boundary
using a surface mesh expansion. A probabilistic atlas of intensity distributions
helps prevent outliers during mesh fitting, and erroneous brain mask voxels are
removed during post-processing via a graph cuts algorithm (<xref rid="R28" ref-type="bibr">Greig et al., 1989</xref>; <xref rid="R70" ref-type="bibr">Sadananthan et al., 2010</xref>) that thresholds the cerebrospinal fluid
(CSF). While effective, this technique is optimized only for images with T1w
contrast, since it relies on the underlying assumption that white matter is
surrounded by darker gray matter and CSF. Another hybrid approach, ROBEX (<xref rid="R40" ref-type="bibr">Iglesias et al., 2011</xref>), exploits a joint
generative-discriminative model. A Random Forest classification (<xref rid="R9" ref-type="bibr">Breiman, 2001</xref>) detects the brain contour, which is
used to fit a point-distribution model to the brain target. The skull-stripping
tool BEaST (<xref rid="R20" ref-type="bibr">Eskildsen et al., 2012</xref>)
builds on patch-based, non-local segmentation techniques (<xref rid="R11" ref-type="bibr">Coupé et al., 2010</xref>; <xref rid="R12" ref-type="bibr">2011</xref>; <xref rid="R68" ref-type="bibr">Roy et al.,
2017</xref>) and assigns a label to each voxel by comparing its local
neighborhood to patches in a reference set with prior labels. With the exception
of BET and 3dSkullStrip, all of these tools were specifically developed for T1w
images.</p>
    </sec>
    <sec id="S4">
      <label>2.2.</label>
      <title>Deep-learning approaches</title>
      <p id="P11">Innovations in deep-learning have gained popularity as methodological
building blocks for an array of tasks in medical image analysis, including
skull-stripping. Various learning-based extraction methods have been proposed,
demonstrating accuracy and speed that often out-perform their classical
counterparts. These models are optimized in a supervised fashion, using a set of
acquired training images with corresponding ground-truth brain masks, derived
through classical methods or manual segmentation. An early, cross-contrast
approach, Deep MRI Brain Extraction (DMBE) (<xref rid="R50" ref-type="bibr">Kleesiek et al., 2016</xref>), trains a 3D CNN on combinations of T1w, T2w,
and FLAIR contrasts and matches the accuracy of classical baselines for several
datasets, including clinical scans with brain tumors. Conversely, Auto-Net
(<xref rid="R71" ref-type="bibr">Salehi et al., 2017</xref>) introduces two
separate 2.5D architectures that skull-strip volumes by individually segmenting
sagittal, coronal, and transverse views of same image and fusing the predictions
with an auto-context algorithm (<xref rid="R75" ref-type="bibr">Tu and Bai,
2009</xref>). The first architecture leverages convolutions on
single-resolution voxel-wise patches, while the second utilizes a scale-space
U-Net architecture (<xref rid="R66" ref-type="bibr">Ronneberger et al.,
2015</xref>) to predict the brain mask. Auto-Net is effective for both adult
and neonatal brain scans but only trained with T1w images. CONSNet (<xref rid="R53" ref-type="bibr">Lucena et al., 2019</xref>) similarly leverages a
2D U-Net, applied across image slices in each plane, to strip 3D T1w images.
More recently, implementations using full 3D U-Nets (<xref rid="R37" ref-type="bibr">Hsu et al., 2020</xref>; <xref rid="R38" ref-type="bibr">Hwang et al., 2019</xref>) have robustly matched or exceeded
start-of-the-art brain-extraction performance.</p>
    </sec>
    <sec id="S5">
      <label>2.3.</label>
      <title>Contribution</title>
      <p id="P12">SynthStrip builds on a solid foundation laid by prior studies of
deep-learning algorithms for brain extraction, enabling us to choose among
network architectures well suited for this particular task. We emphasize that
our goal is not to compare or make claims on the optimality of specific
architectures – the discussed algorithms may perform equally well.
Instead, our focus is on exploiting a novel training strategy using synthetic
data only, to build an easy-to-use skull-stripping tool that alleviates the
requirement of expanding the training set and re-optimizing network weights
every time a new image type is to be supported.</p>
    </sec>
  </sec>
  <sec id="S6">
    <label>3.</label>
    <title>Method</title>
    <p id="P13">To predict robust brain masks for an array of real image types, we train a
deep convolutional neural network on a vast landscape of images synthesized with a
deliberately unrealistic range of anatomies, acquisition parameters, and artifacts.
From a dataset <inline-formula><mml:math id="M1" display="inline"><mml:mrow><mml:mi>𝒟</mml:mi></mml:mrow></mml:math></inline-formula> of precomputed, whole-head segmentations with brain
and non-brain tissue labels, we sample a segmentation <inline-formula><mml:math id="M2" display="inline"><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi>𝒟</mml:mi></mml:mrow></mml:math></inline-formula><italic toggle="yes">s</italic> ∈ <italic toggle="yes">D</italic> at
each optimization step and use it to generate a gray-scale head scan
<italic toggle="yes">x</italic> with randomized acquisition characteristics. In effect, this
paradigm synthesizes a stream of training images used to optimize a SynthStrip
network <italic toggle="yes">g<sub>θ</sub></italic>, with trainable parameters
<italic toggle="yes">θ</italic>, in a supervised fashion: <disp-formula id="FD1"><label>(1)</label><mml:math id="M3" display="block"><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mspace/><mml:msub><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mi>𝒟</mml:mi></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ℒ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p>
    <p id="P14">where <italic toggle="yes">y</italic> is the predicted brain mask,
<inline-formula><mml:math id="M4" display="inline"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is the target brain mask derived by merging the
brain labels of <italic toggle="yes">s</italic>, and <inline-formula><mml:math id="M5" display="inline"><mml:mrow><mml:mo>ℒ</mml:mo></mml:mrow></mml:math></inline-formula> is the loss function that measures similarity
between <italic toggle="yes">y</italic> and <inline-formula><mml:math id="M6" display="inline"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>.</p>
    <sec id="S7">
      <label>3.1.</label>
      <title>Synthesis</title>
      <p id="P15">Building from previous work (<xref rid="R7" ref-type="bibr">Billot et
al., 2020</xref>; Hoffmann et al., 2021b), we use a generative model to
synthesize a stream of random images with substantial anatomical and intensity
variation, as exhibited in <xref rid="F2" ref-type="fig">Fig. 2</xref>. At each
training step, parameters that dictate synthesis components are randomly sampled
from predetermined ranges and probability distributions explicitly defined in
<xref rid="T1" ref-type="table">Table 1</xref>. We emphasize that while the
generated scans can appear implausible, these training images do not need to be
realistic in order for the SynthStrip model to accurately generalize to real
images at test-time.</p>
      <p id="P16">To generate a gray-scale image <italic toggle="yes">x</italic> from a whole-head
anatomical segmentation <italic toggle="yes">s</italic>, we first create spatial variability
to subject the network to a landscape of possible head positions and anatomical
irregularities. This is accomplished by manipulating <italic toggle="yes">s</italic> with a
spatial transformation <italic toggle="yes">t</italic>, composed of an affine transform (with
random translation, scaling, and rotation) and a nonlinear deformation. The
deformation is generated by sampling random 3D displacement vectors from a
normal distribution, with random scale, at an arbitrarily low image resolution.
This random displacement field is vector-integrated, using five <italic toggle="yes">scaling
and squaring</italic> steps to encourage a diffeomorphic warp (<xref rid="R2" ref-type="bibr">Arsigny et al., 2006</xref>; <xref rid="R16" ref-type="bibr">Dalca et al., 2019</xref>), and tri-linearly resampled to
match the resolution of <italic toggle="yes">s</italic>. After applying the randomized
transform, the resulting segmentation <italic toggle="yes">s<sub>t</sub></italic> serves as
the basis for deriving the image <italic toggle="yes">x</italic> and target brain mask
<inline-formula><mml:math id="M7" display="inline"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, which is obtained by merging the labels of
<italic toggle="yes">s<sub>t</sub></italic> into brain and non-brain classes.</p>
      <p id="P17">To compute <italic toggle="yes">x</italic>, we consider a Bayesian model of MR
contrast, which assumes that the voxel intensity of each tissue type in the
image can be represented by a single Gaussian distribution. Reversing this
generalization, we assign a random distribution of tissue intensity to every
anatomical label in <italic toggle="yes">s<sub>t</sub></italic> and use this artificial
mixture model to attain an image with arbitrary contrast by replacing each label
voxel in <italic toggle="yes">s<sub>t</sub></italic> with a random value drawn from its
corresponding intensity distribution. Following the synthesis, we aim to
simulate various artifacts and geometric properties that might exist across
modality and acquisition type. First, we corrupt the image with a spatially
varying intensity bias field, generated by resizing a low-resolution image
sampled from a normal distribution with zero mean. The corrupted image is
computed by an element-wise multiplication with the voxel-wise exponential of
the bias field. Second, we perform gamma augmentation by globally exponentiating
all voxels with a single value exp(<italic toggle="yes">γ</italic>), where
<italic toggle="yes">γ</italic> is a normally sampled parameter. Lastly, to
account for scans with a partial field of view (FOV) and varied resolution, we
randomly crop the image content and down-sample along an indiscriminate set of
axes. Before down-sampling by an arbitrary factor <italic toggle="yes">r</italic>, we
simulate partial-volume effects by blurring the image using a Gaussian kernel
with standard deviation σ = <italic toggle="yes">r</italic>/4. The image cropping and
down-sampling components are applied with a 50% probability rate during
synthesis.</p>
    </sec>
    <sec id="S8">
      <label>3.2.</label>
      <title>Loss</title>
      <p id="P18">We optimize <italic toggle="yes">g<sub>θ</sub></italic> using a loss function
<inline-formula><mml:math id="M8" display="inline"><mml:mrow><mml:mo>ℒ</mml:mo></mml:mrow></mml:math></inline-formula> that measures the similarity between predicted
and target brain masks. Unless otherwise stated, we employ a loss
<inline-formula><mml:math id="M9" display="inline"><mml:mrow><mml:mo>ℒ</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> that encourages the network to predict a signed
distance transform (SDT) <italic toggle="yes">d</italic> representing the minimum distance
(in <italic toggle="yes">mm</italic>) to the skull boundary at each voxel. Distances are
positive within the brain and negative outside, facilitating the extraction of a
binary brain mask <italic toggle="yes">y</italic> from <italic toggle="yes">d</italic> at test-time by
simple thresholding. The training paradigm is outlined in <xref rid="F3" ref-type="fig">Fig. 3</xref>. During training, an exact target Euclidean SDT
<inline-formula><mml:math id="M10" display="inline"><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is computed from the target brain mask
<inline-formula><mml:math id="M11" display="inline"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, and the similarity between <italic toggle="yes">d</italic>
and <inline-formula><mml:math id="M12" display="inline"><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is measured by their mean squared difference
(MSE). To concentrate optimization gradients to pertinent regions of the image
during training, <inline-formula><mml:math id="M13" display="inline"><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is banded such that voxel distances
<inline-formula><mml:math id="M14" display="inline"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> do not surpass a discrete threshold
<italic toggle="yes">t</italic>, and all voxels that exceed the distance
<italic toggle="yes">t</italic> are down-weighted in the MSE computation by a factor
<italic toggle="yes">b</italic>. Therefore, <disp-formula id="FD2"><label>(2)</label><mml:math id="M15" display="block"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>𝒫</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>𝒫</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>b</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="thickmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p id="P19">where <italic toggle="yes">i</italic> represents a voxel in the spatial image domain
<inline-formula><mml:math id="M16" display="inline"><mml:mrow><mml:mi>𝒫</mml:mi></mml:mrow></mml:math></inline-formula>, <italic toggle="yes">t</italic> = 5 <italic toggle="yes">mm</italic> and
<italic toggle="yes">b</italic> = 0.1 in our experiments, optimally determined via a grid
search.</p>
      <p id="P20">As a complimentary analysis, we compare the distance-based loss
<inline-formula><mml:math id="M17" display="inline"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> against a soft Dice loss (<xref rid="R18" ref-type="bibr">Dice, 1945</xref>; <xref rid="R58" ref-type="bibr">Milletari
et al., 2016</xref>), which is commonly used to optimize image segmentation
models and quantifies volume overlap for pairs of labels. We define the loss
<inline-formula><mml:math id="M18" display="inline"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> as <disp-formula id="FD3"><label>(3)</label><mml:math id="M19" display="block"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi mathvariant="italic">dice</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:mo>⊙</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:mo>⊕</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>⊙</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>⊕</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p id="P21">where <italic toggle="yes">y<sup>j</sup></italic> and <inline-formula><mml:math id="M20" display="inline"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> represent brain label maps,
<italic toggle="yes">y</italic><sup><italic toggle="yes">k</italic></sup> and
<inline-formula><mml:math id="M21" display="inline"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> represent non-brain label maps, and ⊙
and ⊕ represent voxel-wise multiplication and addition, respectively.
While <inline-formula><mml:math id="M22" display="inline"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M23" display="inline"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> both result in effective skull-stripping
networks, we favor the distance loss <inline-formula><mml:math id="M24" display="inline"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> due to its smoothing effect on the outline of
the predicted brain mask, as demonstrated in Experiment <xref rid="S24" ref-type="sec">4.4</xref>.</p>
    </sec>
    <sec id="S9">
      <label>3.3.</label>
      <title>Implementation</title>
      <p id="P22">We implement <italic toggle="yes">g<sub>θ</sub></italic> using a 3D U-Net
convolutional architecture, with down-sampling (encoder) and up-sampling
(decoder) components that facilitate the integration of features across large
spatial regions. The U-Net comprises seven resolution levels, which each include
two convolutional operations with leaky ReLU activations (parameter
<italic toggle="yes">α</italic> = 0.2) and filter numbers defined in <xref rid="F3" ref-type="fig">Fig. 3</xref>. Down-sampling is achieved through
max-pooling, and skip-connections are formed by concatenating the outputs of
each encoder level with the inputs of the decoder level with corresponding
resolution. In models using <inline-formula><mml:math id="M25" display="inline"><mml:mrow><mml:mo>ℒ</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, one final, single-feature convolutional layer
with linear activation outputs the predicted SDT <italic toggle="yes">d</italic>. In models
optimized with <inline-formula><mml:math id="M26" display="inline"><mml:mrow><mml:mo>ℒ</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, the final layer is a two-feature convolution,
with softmax activation, that outputs a probabilistic segmentation representing
non-brain and brain regions.</p>
      <p id="P23">We train SynthStrip using the Adam optimizer (<xref rid="R49" ref-type="bibr">Kingma and Ba, 2014</xref>) with a batch size of one and an
initial learning rate of 10<sup>−4</sup> . This rate is reduced by a
factor of two after every 20,000 optimization steps without a decrease in
validation loss. At test-time, all inputs to the model are internally conformed
to 1-<italic toggle="yes">mm</italic> isotropic voxel size using trilinear interpolation, and
intensities are scaled between 0 and 1. The U-Net outputs are resampled such
that the final brain mask is computed in the original input space. We implement
SynthStrip in Python, using the open-source PyTorch (<xref rid="R62" ref-type="bibr">Paszke et al., 2019</xref>) and Neurite (<xref rid="R17" ref-type="bibr">Dalca et al., 2018</xref>) libraries, and make our tool and
associated code available in the open-source FreeSurfer package (<ext-link xlink:href="https://w3id.org/synthstrip" ext-link-type="uri">https://w3id.org/synthstrip</ext-link>). All experiments
are conducted using Intel Xeon Silver 4214R CPUs and Nvidia RTX 8000 GPUs.</p>
    </sec>
    <sec id="S10">
      <label>3.4.</label>
      <title>Data</title>
      <p id="P24">In our experiments, we employ a small training dataset of adult and
infant brain segmentations and a separate, larger dataset of acquired images for
validation and testing that spans across age, health, resolution, and imaging
modality. All data are 3D images, acquired either directly or as stacks of 2D
MRI slices.</p>
      <sec id="S11">
        <label>3.4.1.</label>
        <title>Training data</title>
        <sec id="S12">
          <title>Datasets:</title>
          <p id="P25">We compose a set of 80 training subjects, each with whole-head
tissue segmentations, from the following three cohorts: 40 adult
subjects from the Buckner40 dataset (<xref rid="R24" ref-type="bibr">Fischl et al., 2002</xref>), 30 locally scanned adult subjects from
the Human Connectome Aging Project (HCP-A) (<xref rid="R8" ref-type="bibr">Bookheimer et al., 2019</xref>; <xref rid="R30" ref-type="bibr">Harms et al., 2018</xref>), and 10 infant subjects
born full-term, scanned at Boston Children’s Hospital at ages
between 0 and 18 months (<xref rid="R54" ref-type="bibr">de Macedo
Rodrigues et al., 2015</xref>).</p>
        </sec>
        <sec id="S13">
          <title>Processing:</title>
          <p id="P26">To compute anatomical segmentations of individual cerebral
regions, adult and infant T1w scans are processed with SAM-SEG (<xref rid="R64" ref-type="bibr">Puonti et al., 2016</xref>) and the Infant
FreeSurfer reconstruction pipeline (<xref rid="R77" ref-type="bibr">Zöllei et al., 2020</xref>), respectively. In order to build
complete segmentation maps for robust whole-head image synthesis, we
also generate six coarse labels of extra-cerebral tissue using a simple
intensity-based labeling strategy with thresholds that mark label
intensity boundaries. Considering only non-zero voxels without brain
labels, we fit threshold values to each image by maximizing the
similarity in number of voxels for each extra-cerebral label. These
extra-cerebral labels do not necessarily represent or differentiate
meaningful anatomical structures – their purpose is to provide
intensity and spatial variability to synthesized regions outside the
brain.</p>
          <p id="P27">In total, the training segmentations contain 46 individual
anatomical labels, with 40 brain-specific labels (including CSF), that
we merge into the target brain mask <inline-formula><mml:math id="M27" display="inline"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. All training segmentations are fit to
a 256<sup>3</sup> image shape with 1-<italic toggle="yes">mm</italic> isotropic
resolution. We emphasize that this geometric preprocessing is not
required at test-time.</p>
        </sec>
      </sec>
      <sec id="S14">
        <label>3.4.2.</label>
        <title>Evaluation data</title>
        <sec id="S15">
          <title>Datasets:</title>
          <p id="P28">Our evaluation data comprise 620 images, split into validation
and test subsets of sizes 22 and 598, respectively. We gather these
images across seven public datasets, with makeup, resolution, and
validation splits outlined in <xref rid="T2" ref-type="table">Table
2</xref>. The IXI<sup><xref rid="FN4" ref-type="fn">1</xref></sup>
dataset features a range of MRI contrasts and modalities, including T1w
and T2w as well as PDw, MRA, and DWI. To simplify the DWI evaluation, a
single diffusion direction is randomly extracted from each acquisition.
The FSM subset (<xref rid="R29" ref-type="bibr">Greve et al.,
2021</xref>) is derived from in-house data using standard
acquisitions as well as quantitative T1 maps (qT1). In-house,
pseudo-continuous ASL (PCASL) scans are acquired as stacks of 2D-EPI
slices with low resolution and a small FOV that often crops the ventral
brain region (<xref rid="R15" ref-type="bibr">Dai et al., 2008</xref>).
The QIN (<xref rid="R10" ref-type="bibr">Clark et al., 2013</xref>;
<xref rid="R55" ref-type="bibr">Mamonov and Kalpathy-Cramer,
2016</xref>; <xref rid="R63" ref-type="bibr">Prah et al.,
2015</xref>) dataset comprises precontrast, clinical stacks of thick
image slices from patients with newly diagnosed glioblastoma. We also
include a subset of the infant T1w image dataset, using subjects
held-out from training. Lastly, to evaluate the ability of SynthStrip to
adapt to imaging modalities beyond MR, we gather a test cohort of brain
CT and FDG-PET scans from the CERMEP-IDB-MRXFDG (CIM) database (<xref rid="R57" ref-type="bibr">Mérida et al., 2021</xref>).</p>
        </sec>
        <sec id="S16">
          <title>Ground-truth masks:</title>
          <p id="P29">For each image in the evaluation dataset, we derive a reference
brain mask using the following labelling strategy. Since every
evaluation subject includes a corresponding T1w image, we generate brain
masks for these scans using each <italic toggle="yes">classical</italic> baseline
method evaluated in our analysis. Then, an “average” brain
mask is computed for each subject by extracting the majority label value
at every voxel. We refine the average masks manually before propagating
the masks by rigidly aligning each subject’s T1w scan to the
remaining image types with a robust registration approach (<xref rid="R65" ref-type="bibr">Reuter et al., 2010</xref>). Poor
alignments are further refined by hand. We make the reference dataset
available online to facilitate future development of skull-stripping
techniques, including the original images if permitted by their
respective licenses.</p>
        </sec>
      </sec>
      <sec id="S17">
        <label>3.4.3.</label>
        <title>Ethics</title>
        <p id="P30">This retrospective study re-analyzes previously published or shared
datasets. The FSM and ASL studies were approved by the Mass General Brigham
Internal Review Board (IRB). The HCP-A study was approved by IRBs at
Washington University in St. Louis and Mass General Brigham. The infant
study was approved by the Committee on Clinical Investigation at Boston
Children’s Hospital and the Mass General Brigham IRB. All subjects
gave written informed consent. No ethical approval was required for
retrospective analysis of de-identified open-access data.</p>
      </sec>
    </sec>
  </sec>
  <sec id="S18">
    <label>4.</label>
    <title>Experiments</title>
    <p id="P31">We analyze the performance of SynthStrip on diverse whole-head images and
compare its 3D skull-stripping accuracy to classical and deep-learning baseline
tools.</p>
    <sec id="S19">
      <title>Baselines:</title>
      <p id="P32">We select a group of skull-stripping baselines based on their
popularity, determined by citation count, and effectiveness, as shown in prior
work (<xref rid="R21" ref-type="bibr">Fatima et al., 2020</xref>; <xref rid="R40" ref-type="bibr">Iglesias et al., 2011</xref>). As classical
baselines, we choose ROBEX 1.1, BET from FSL 6.0.4, 3dSkull-Strip (3DSS) from
AFNI 21.0.21, BEaST 1.15, and the FreeSurfer 7.2 watershed algorithm (FSW).
Unfortunately, many top-cited, learning based approaches do not make their
models available, even upon request to the authors. A notable exception is Deep
MRI Brain Extraction (DMBE), which we therefore include. Default parameters are
used for each method except BET, for which the –R option is provided for
more accurate brain center estimation. All inputs to FSW and DMBE are re-sampled
to 1-<italic toggle="yes">mm</italic> isotropic voxel sizes to accommodate the expected input
resolution for these methods.</p>
    </sec>
    <sec id="S20">
      <title>Metrics:</title>
      <p id="P33">We evaluate the similarity between computed and ground-truth brain masks
by measuring their Dice overlap, mean and maximum (Hausdorff) surface distances,
and percent difference in total volume. Baseline scores are compared to
SynthStrip with a paired sample <italic toggle="yes">t</italic>-test. Sensitivity and
specificity, which measure the percent of true positive and true negative brain
labels, respectively, provide further insight into the properties of the
computed brain masks.</p>
      <sec id="S21">
        <label>4.1.</label>
        <title>Skull-stripping accuracy</title>
        <p id="P34">We assess the broad skull-stripping capability of a SynthStrip model
trained using images synthesized from the label maps outlined in <xref rid="S11" ref-type="sec">Section 3.4.1</xref>. We compare the accuracy
of our method to each of the baselines across the test set of real brain
images defined in <xref rid="S14" ref-type="sec">Section 3.4.2</xref>.
Method runtime is compared for the FSM dataset.</p>
        <p id="P35">The comparison demonstrates SynthStrip’s accurate and robust
brain extraction, which substantially outperforms baseline methods (<xref rid="T3" ref-type="table">Tables 3</xref>, <xref rid="T4" ref-type="table">4</xref> and <xref rid="SD1" ref-type="supplementary-material">Supplementary Tables S1</xref>, <xref rid="SD1" ref-type="supplementary-material">S2</xref>). For every
evaluation metric, brain masks predicted by SynthStrip yield significantly
better scores than baseline masks (<italic toggle="yes">p</italic> &lt; 0.05) for the
<italic toggle="yes">vast</italic> majority of datasets. Importantly, no baseline
method significantly outperforms SynthStrip on any dataset. As shown in
<xref rid="F4" ref-type="fig">Fig. 4</xref>, SynthStrip achieves the
highest Dice score <italic toggle="yes">and</italic> lowest mean surface distance for
more than 80% of all test images, in stark contrast to the next best
performing method, BET, which yields the top result for less than 10% of
images. The superior performance of SynthStrip persists even when
considering only T1w, near-isotropic, adult-brain images, which all of the
baselines are tuned for. Across this particular subset of 127 T1w images
from the IXI, FSM, and ASL datasets, SynthStrip achieves the best mean Dice,
surface distance, Hausdorff distance, and volume difference (<xref rid="F5" ref-type="fig">Fig. 5</xref>), and it consistently extracts the brain
with high specificity and sensitivity, while other methods tend to
under-perform in either of those metrics due to tendencies to substantially
over- or under-label the brain. When considering the remaining non-T1w,
thick-slice, and infant image types, SynthStrip’s predominance is
similarly substantial (<xref rid="F6" ref-type="fig">Fig. 6</xref>). For FSM
T1w data, our method runs on the CPU in less than one minute (<xref rid="T5" ref-type="table">Table 5</xref>), trailing the fastest two baselines,
BET and FSW, by approximately 17 seconds on average. On the GPU, SynthStrip
runs substantially faster, requiring only 1.8 ± 0. 2 seconds.</p>
      </sec>
      <sec id="S22">
        <label>4.2.</label>
        <title>Qualitative brain-mask analysis</title>
        <p id="P36">Across the evaluation set, skull-stripping errors in SynthStrip
predictions are uncommon and typically involve minimally over-segmenting the
brain mask by including thin regions of extra-cerebral matter near the
dorsal cortex or pockets of tissue around the eye sockets, as shown in <xref rid="F8" ref-type="fig">Fig. 8</xref>. Considering only the
<italic toggle="yes">N</italic> images for which SynthStrip does not achieve the best
score in <xref rid="F4" ref-type="fig">Fig. 4</xref>, on average, SynthStrip
lags behind the best-performing baseline by only −0.53 ± 0.54
Dice percentage points (<italic toggle="yes">N</italic> = 111) and (0. 20 ± 0.18)
<italic toggle="yes">mm</italic> mean surface distance (<italic toggle="yes">N</italic> = 94).</p>
        <p id="P37">The top performing baseline method is ROBEX, which yields
high-quality brain extraction across many of the test datasets, with the
notable exception of the qT1 cohort. ROBEX produces spatially plausible
brain masks and evades drastic failure modes that exist in other base-lines,
similarly to SynthStrip. However, despite its generally good performance,
ROBEX has a tendency to include pockets of tissue surrounding the eyes and
remove regions of cortical gray matter near the superior surface (<xref rid="F8" ref-type="fig">Figs. 8</xref> and <xref rid="SD1" ref-type="supplementary-material">S1</xref>).</p>
        <p id="P38">BET and 3DSS also perform effective brain extraction across image
types, but tend to fail dramatically for outlier cases. For example, BET
locates the brain boundary with considerable precision when successful.
However, for some image subsets, especially those with abundant non-brain
matter, such as FSM, BET often includes large regions of inferior skull as
well as facial and neck tissue in the brain mask. While 3DSS largely avoids
such gross mislabeling, it tends to produce skull-strips that leak into neck
tissue or, conversely, remove small regions of the cortical surface.</p>
        <p id="P39">BEaST and FSW perform well for near-isotropic T1w images, such as
those in the IXI, FSM, and ASL datasets. But since they are heavily
optimized for the assumed spatial and intensity features of this acquisition
type, they generally perform poorly or even fail completely for other
contrasts. Common error modes of FSW involve the failure to remove bits of
skull or inferior non-brain matter, in contrast to BEaST, which is
susceptible to removing critical regions of the cortex.</p>
        <p id="P40">The learning-based method DMBE yields suitable brain masks for
near-isotropic image types with T1w contrast but frequently leaves
substantial, unconnected components of non-brain matter. While DMBE extracts
the brain tissue border as opposed to CSF, our analysis shows that the
predominant contributor to the discrepancy between DMBE and ground-truth
brain masks is the inclusion of neck and facial tissue (<xref rid="F8" ref-type="fig">Figs. 8</xref> and <xref rid="SD1" ref-type="supplementary-material">S1</xref>). DMBE model inference is
slow, consuming more than a half hour to skull-strip a standard image.</p>
      </sec>
      <sec id="S23">
        <label>4.3.</label>
        <title>Variability across time-series data</title>
        <p id="P41">We analyze the consistency of SynthStrip brain masks across
time-domain data by assessing the differences between diffusion-encoded
directions acquired in the same session. For each subject in the DWI
dataset, we affinely align and skull-strip all of the 16 diffusion-encoded
frames in a common, average space (<xref rid="R65" ref-type="bibr">Reuter et
al., 2010</xref>). We compute the number of discordant voxels across
brain masks for a given method, defining discordant voxels (DV) as voxel
locations with labels that differ in the time domain. We report the percent
of DV relative to the brain mask volume, determined by the number of voxels
labeled as brain in any frame. In this particular analysis, we only consider
ROBEX, BET, and 3DSS as baselines since they generalize to DWI acquisitions.
As shown in <xref rid="F7" ref-type="fig">Fig. 7</xref>, SynthStrip
demonstrates a high level of intra-subject consistency, as it predicts brain
masks with substantially lower % DV across DWI directions than the baselines
(<italic toggle="yes">p</italic> &lt; 10<sup>−12</sup>). Since the % DV
metric considers voxels labeled as brain for any direction, a single mask
with gross mislabeling will substantially increase the metric value, as is
the case with ROBEX, which over-segments the brain for only a few directions
per subject.</p>
      </sec>
      <sec id="S24">
        <label>4.4.</label>
        <title>Loss comparison</title>
        <p id="P42">During our experimentation, we find that training SynthStrip models
using a traditional soft Dice loss yields comparable results to those
trained with an SDT-based loss for <italic toggle="yes">nearly</italic> every metric.
However, despite similar global accuracy, we observe that models trained
with <inline-formula><mml:math id="M28" display="inline"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> predict brain masks characterized by
relatively noisy and rough boundaries, as illustrated in <xref rid="F7" ref-type="fig">Fig. 7</xref>. The high variability at the edge of the
brain mask is emphasized by a 6.4 ± 3.2 <italic toggle="yes">mm</italic> increase
in maximum surface distance when using <inline-formula><mml:math id="M29" display="inline"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> compared to <inline-formula><mml:math id="M30" display="inline"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. We further quantify this discrepancy in
brain-mask smoothness by computing the percent of exposed boundary voxels
(EBV) that neighbor more non-brain labels than brain labels. Brain masks
with noisier boundaries will exhibit larger EBV due to an increased mask
surface area and number of sporadic border voxels. We perform this
evaluation using the FSM data subset of 132 images with isotropic voxel
size. Models trained with <inline-formula><mml:math id="M31" display="inline"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> predict masks with 4.5× higher EBV
than models trained with <inline-formula><mml:math id="M32" display="inline"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. We hypothesize that as the network learns
to estimate an SDT, it is encouraged to focus more on the boundary of mask,
rather than the label as a whole, resulting in a smoother prediction of the
brain border.</p>
      </sec>
    </sec>
  </sec>
  <sec id="S25">
    <label>5.</label>
    <title>Discussion</title>
    <p id="P43">We present SynthStrip, a learning-based, universal brain-extraction tool
trained on diverse synthetic images. Subjected to training data that far exceeds the
realistic range of medical images, the model learns to generalize across imaging
modalities, anatomical variability, and acquisition schemes.</p>
    <sec id="S26">
      <label>5.1.</label>
      <title>Baseline comparison</title>
      <p id="P44">SynthStrip significantly improves upon baseline skull-stripping accuracy
for nearly every image cohort tested, and the few exceptions to this improvement
involve data subsets for which SynthStrip matches baseline performance. This
predominance is in part due to the ability of SynthStrip to generalize across a
wide variety of image types as well as its proclivity to avoid substantial
mislabeling. In particular, varying specific acquisition characteristics during
synthesis promotes network robustness to such characteristics across a range of
protocols. For example, simulating partial-volume effects with blurring and
randomizing the resolution enable SynthStrip to accurately generalize to
clinical thick slice acquisitions and those with large voxel sizes. By learning
robust, large-scale spatial features of representative brain masks, the model
consistently predicts masks of realistic and expected shape. Baseline
techniques, on the other hand, often rely on weak spatial priors and are
therefore prone to over- or under-segment brain tissue when confronted with
image features that are unexpected or unaccounted for (<xref rid="F8" ref-type="fig">Figs. 8</xref> and <xref rid="SD1" ref-type="supplementary-material">S1</xref>).</p>
      <p id="P45">ROBEX’s consistent performance across contrasts and modality is
somewhat unexpected since the discriminative edge detector is trained only for
T1w scans. We hypothesize that the coupled shape model is able to compensate for
any intensity bias encoded in the discriminative detector. The T1w-specific
approaches BEAST and FSW could be effective for other MRI contrasts if provided
known intensity priors of the brain matter. However, this work would require
substantial human effort as it needs to be repeated for every new image type.
The substantial, unconnected components of non-brain matter frequently left by
DMBE are likely a byproduct of its convolutional architecture, which does not
leverage multiple resolution levels to gather spatial features across large
distances.</p>
    </sec>
    <sec id="S27">
      <label>5.2.</label>
      <title>Use for brain-specific registration</title>
      <p id="P46">Consistent brain extraction across different images from the same
subject is critical for accurate analysis of time-series acquisitions. For
example, diffusion (<xref rid="R36" ref-type="bibr">Holdsworth et al.,
2012</xref>; <xref rid="R46" ref-type="bibr">Jones and Leemans, 2011</xref>)
and functional MRI analyses (<xref rid="R4" ref-type="bibr">Ashburner,
2009</xref>; <xref rid="R41" ref-type="bibr">Jenkinson et al., 2002</xref>)
depend on within-subject registration of individual frames acquired across time
to undo the effect of any head motion during the scan. Unfortunately, anatomical
structures that deform non-rigidly between frames, such as the neck or tongue,
can hamper brain-registration accuracy and thus impinge on downstream results.
While this effect can be accounted for by first removing non-brain tissue from
each frame to achieve brain-specific registration (<xref rid="R1" ref-type="bibr">Andrade et al., 2018</xref>; <xref rid="R25" ref-type="bibr">Fischmeister et al., 2013</xref>), it requires consistent brain extraction
across frames (<xref rid="R1" ref-type="bibr">Andrade et al., 2018</xref>; <xref rid="R22" ref-type="bibr">Fein et al., 2006</xref>; <xref rid="R25" ref-type="bibr">Fischmeister et al., 2013</xref>; <xref rid="R35" ref-type="bibr">Hoffmann et al., 2020</xref>). SynthStrip’s high
within-subject consistency despite substantial contrast differences across the
diffusion encoding demonstrates its potential for regularizing retrospective
motion correction of time-series data.</p>
    </sec>
    <sec id="S28">
      <label>5.3.</label>
      <title>Model and data availability</title>
      <p id="P47">Even as learning-based methods in neuroimaging analysis continue to grow
in popularity, developers of deep-learning skull-stripping tools are sometimes
disinclined to provide easy-to-use distributions of their work. Out of the three
promising methods discussed in this work, only DMBE makes its models and code
publicly available for use. In contrast, we make SynthStrip available as a
universal, cross-platform command-line utility, distributed both as a standalone
and as a built-in FreeSurfer tool. To facilitate further development and testing
of robust skull-stripping tools, we also make our evaluation data and
ground-truth labels available at <ext-link xlink:href="https://w3id.org/synthstrip" ext-link-type="uri">https://w3id.org/synthstrip</ext-link>.</p>
    </sec>
    <sec id="S29">
      <label>5.4.</label>
      <title>Future work</title>
      <p id="P48">While SynthStrip facilitates state-of-the-art brain extraction, we aim
to extend the tissue-extraction strategy to other applications both within and
beyond neuroimaging. One such application is fetal head extraction from in-utero
fetal MRI scans. Due to excessive motion, fetal MRI is limited to the
acquisition of sub-second 2D slices. However, stacks of several slices are
needed to cover the anatomy of interest, and while their inplane resolution is
typically of the order of 1 <italic toggle="yes">mm</italic> × 1 <italic toggle="yes">mm</italic>,
views across slices are hampered by slice thicknesses of 4–6
<italic toggle="yes">mm</italic> and between-slice motion (Hoffmann et al., 2021a). To
enable full 3D views of the fetal brain, post-processing tools for
super-resolution reconstruction have emerged, that aim to reconstruct a
high-quality volume of isotropic resolution from a number of slice stacks
acquired at different angles (<xref rid="R19" ref-type="bibr">Ebner et al.,
2020</xref>; <xref rid="R39" ref-type="bibr">Iglesias et al., 2021</xref>;
<xref rid="R47" ref-type="bibr">Kainz et al., 2015</xref>; <xref rid="R67" ref-type="bibr">Rousseau et al., 2006</xref>). Yet, these methods hinge on
successful brain extraction which is challenging due to frequent artifacts and
because the relatively small brain first needs to be localized within a wide FOV
encompassing the maternal anatomy (<xref rid="R27" ref-type="bibr">Gaudfernau et
al., 2021</xref>). In addition, substantially fewer public fetal datasets
are available for training in comparison to vast public adult brain datasets.
This presents an ideal problem to be addressed with SynthStrip, as our approach
synthesizes an endless stream of training data from only a handful of label
maps.</p>
    </sec>
  </sec>
  <sec id="S30">
    <label>6.</label>
    <title>Conclusion</title>
    <p id="P49">The removal of non-brain signal from neuroimaging data is a fundamental
first step for many quantitative analyses and its accuracy has a direct impact on
downstream results. However, popular skull-stripping utilities are typically
tailored to isotropic T1w scans and tend to fail, sometimes catastrophically, on
images with other MRI contrasts or stack-of-slices acquisitions that are common in
the clinic. We propose SynthStrip, a flexible tool that produces highly accurate
brain masks across a landscape of imaging paradigms with widely varying contrast and
resolution. We implement our method by leveraging anatomical label maps to
synthesize a broad set of training images, optimizing a robust convolutional neural
network that is agnostic to MRI contrasts and acquisition schemes.</p>
  </sec>
  <sec sec-type="supplementary-material" id="SM1">
    <title>Supplementary Material</title>
    <supplementary-material id="SD1" position="float" content-type="local-data">
      <label>1</label>
      <media xlink:href="NIHMS1833287-supplement-1.zip" id="d64e1597" position="anchor"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="S31">
    <title>Acknowledgments</title>
    <p id="P50">The authors thank Douglas Greve, Lilla Zöllei, and David Salat for
sharing FSM, infant, and ASL data, respectively, and for providing a mechanism for
distributing our reference dataset. Support for this research was provided in part
by a BRAIN Initiative Cell Census Network grant [U01 MH117023], the National
Institute of Biomedical Imaging and Bioengineering [P41 EB015896, R01 EB023281, R21
EB018907, R01 EB019956, P41 EB030006], the National Institute of Child Health and
Human Development [K99 HD101553], the National Institute on Aging [R56 AG064027, R01
AG016495, R01 AG070988], the National Institute of Mental Health [RF1 MH121885, RF1
MH123195], the National Institute of Neurological Disorders and Stroke [R01
NS070963, R01 NS083534, R01 NS105820], and was made possible by the resources
provided by Shared Instrumentation Grants [S10 RR023401, S10 RR019307, S10
RR023043]. Additional support was provided by the NIH Blueprint for Neuroscience
Research [U01 MH093765], part of the multi-institutional Human Connectome Project.
The research project benefitted from computational hardware generously provided by
the Massachusetts Life Sciences Center (<ext-link xlink:href="https://www.masslifesciences.com/" ext-link-type="uri">https://www.masslifesciences.com</ext-link>).</p>
  </ack>
  <fn-group>
    <fn fn-type="COI-statement" id="FN2">
      <p id="P51">Declaration of competing interest</p>
      <p id="P52">Bruce Fischl has a financial interest in CorticoMetrics, a company whose
medical pursuits focus on brain imaging and measurement technologies. This
interest is reviewed and managed by Massachusetts General Hospital and Mass
General Brigham in accordance with their conflict-of-interest policies. The
authors declare that they have no other known competing financial interests or
personal relationships that could have appeared to influence the work reported
in this paper.</p>
    </fn>
    <fn id="FN3">
      <p id="P53">Credit authorship contribution statement</p>
      <p id="P54"><bold>Andrew Hoopes:</bold> Methodology, Software, Validation, Formal
analysis, Investigation, Data curation, Writing – original draft, Writing
– review &amp; editing, Visualization. <bold>Jocelyn S. Mora:</bold>
Validation, Investigation, Resources, Data curation, Writing – review
&amp; editing, Visualization. <bold>Adrian V. Dalca:</bold> Conceptualization,
Methodology, Writing – review &amp; editing, Supervision. <bold>Bruce
Fischl:</bold> Conceptualization, Methodology, Software, Investigation,
Writing – review &amp; editing, Supervision, Funding acquisition.
<bold>Malte Hoffmann:</bold> Conceptualization, Methodology, Software,
Investigation, Data curation, Writing – original draft, Writing –
review &amp; editing, Supervision, Project administration.</p>
    </fn>
    <fn id="FN4">
      <label>1.</label>
      <p id="P55">Acquired from <ext-link xlink:href="http://brain-development.org/ixi-dataset" ext-link-type="uri">http://brain-development.org/ixi-dataset</ext-link>.</p>
    </fn>
    <fn id="FN5">
      <p id="P56">Supplementary materials</p>
      <p id="P57"><xref rid="SD1" ref-type="supplementary-material">Supplementary
material</xref> associated with this article can be found, in the online
version, at doi:<ext-link xlink:href="https://doi.org/10.1016/j.neuroimage.2022.119474" ext-link-type="uri">10.1016/j.neuroimage.2022.119474</ext-link>.</p>
    </fn>
  </fn-group>
  <glossary>
    <title>Abbreviation:</title>
    <def-list>
      <def-item>
        <term>MRI</term>
        <def>
          <p id="P58">Magnetic resonance imaging</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <mixed-citation publication-type="confproc"><name><surname>Andrade</surname><given-names>N</given-names></name>, <name><surname>Faria</surname><given-names>FA</given-names></name>, <name><surname>Cappabianco</surname><given-names>FAM</given-names></name>, <year>2018</year>. <source>A practical review on medical image registration: From rigid to deep learning based approaches</source>. In:
<conf-name>2018 31st SIBGRAPI Conference on Graphics, Patterns and Images
(SIBGRAPI)</conf-name>. <publisher-name>IEEE</publisher-name>, pp.
<fpage>463</fpage>–<lpage>470</lpage>.</mixed-citation>
    </ref>
    <ref id="R2">
      <mixed-citation publication-type="book"><name><surname>Arsigny</surname><given-names>V</given-names></name>, <name><surname>Commowick</surname><given-names>O</given-names></name>, <name><surname>Pennec</surname><given-names>X</given-names></name>, <name><surname>Ayache</surname><given-names>N</given-names></name>, <year>2006</year>. <part-title>A log-euclidean framework for
statistics on diffeomorphisms</part-title>. In: <source>MICCAI: Medical Image Computing and Computer Assisted Interventions</source>.
<publisher-name>Springer</publisher-name>, pp.
<fpage>924</fpage>–<lpage>931</lpage>.</mixed-citation>
    </ref>
    <ref id="R3">
      <mixed-citation publication-type="journal"><name><surname>Ashburner</surname><given-names>J</given-names></name>, <year>2007</year>. <article-title>A fast diffeomorphic image
registration algorithm</article-title>. <source>Neuroimage</source><volume>38</volume> (<issue>1</issue>),
<fpage>95</fpage>–<lpage>113</lpage> .<pub-id pub-id-type="pmid">17761438</pub-id></mixed-citation>
    </ref>
    <ref id="R4">
      <mixed-citation publication-type="book"><name><surname>Ashburner</surname><given-names>J</given-names></name>, <year>2009</year>. <part-title>Preparing fMRI Data for Statistical
Analysis</part-title>. In: <source>fMRI techniques and protocols</source>. <publisher-name>Springer</publisher-name>, pp.
<fpage>151</fpage>–<lpage>178</lpage>.</mixed-citation>
    </ref>
    <ref id="R5">
      <mixed-citation publication-type="journal"><name><surname>Avants</surname><given-names>BB</given-names></name>, <name><surname>Epstein</surname><given-names>CL</given-names></name>, <name><surname>Grossman</surname><given-names>M</given-names></name>, <name><surname>Gee</surname><given-names>JC</given-names></name>, <year>2008</year>. <article-title>Symmetric diffeomorphic image
registration with cross-correlation: evaluating automated labeling of
elderly and neurodegenerative brain</article-title>. <source>Med. Image Anal</source><volume>12</volume> (<issue>1</issue>),
<fpage>26</fpage>–<lpage>41</lpage>.<pub-id pub-id-type="pmid">17659998</pub-id></mixed-citation>
    </ref>
    <ref id="R6">
      <mixed-citation publication-type="journal"><name><surname>Avants</surname><given-names>BB</given-names></name>, <name><surname>Tustison</surname><given-names>NJ</given-names></name>, <name><surname>Song</surname><given-names>G</given-names></name>, <name><surname>Cook</surname><given-names>PA</given-names></name>, <name><surname>Klein</surname><given-names>A</given-names></name>, <name><surname>Gee</surname><given-names>JC</given-names></name>, <year>2011</year>. <article-title>A reproducible evaluation of ANTs
similarity metric performance in brain image registration</article-title>.
<source>Neuroimage</source><volume>54</volume> (<issue>3</issue>),
<fpage>2033</fpage>–<lpage>2044</lpage>.<pub-id pub-id-type="pmid">20851191</pub-id></mixed-citation>
    </ref>
    <ref id="R7">
      <mixed-citation publication-type="journal"><name><surname>Billot</surname><given-names>B</given-names></name>, <name><surname>Greve</surname><given-names>D</given-names></name>, <name><surname>Van Leemput</surname><given-names>K</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, <name><surname>Iglesias</surname><given-names>JE</given-names></name>, <name><surname>Dalca</surname><given-names>AV</given-names></name>, <year>2020</year>. <article-title>A learning strategy for
contrast-agnostic MRI segmentation</article-title>. <source>arXiv preprint arXiv:2003.01995</source>.</mixed-citation>
    </ref>
    <ref id="R8">
      <mixed-citation publication-type="journal"><name><surname>Bookheimer</surname><given-names>SY</given-names></name>, <name><surname>Salat</surname><given-names>DH</given-names></name>, <name><surname>Terpstra</surname><given-names>M</given-names></name>, <name><surname>Ances</surname><given-names>BM</given-names></name>, <name><surname>Barch</surname><given-names>DM</given-names></name>, <name><surname>Buckner</surname><given-names>RL</given-names></name>, <name><surname>Burgess</surname><given-names>GC</given-names></name>, <name><surname>Curtiss</surname><given-names>SW</given-names></name>, <name><surname>Diaz-Santos</surname><given-names>M</given-names></name>, <name><surname>Elam</surname><given-names>JS</given-names></name>, <etal/>, <year>2019</year>. <article-title>The lifespan human
connectome project in aging: an overview</article-title>.
<source>Neuroimage</source><volume>185</volume>,
<fpage>335</fpage>–<lpage>348</lpage>.<pub-id pub-id-type="pmid">30332613</pub-id></mixed-citation>
    </ref>
    <ref id="R9">
      <mixed-citation publication-type="journal"><name><surname>Breiman</surname><given-names>L</given-names></name>, <year>2001</year>. <article-title>Random forests</article-title>.
<source>Mach. Learn</source><volume>45</volume> (<issue>1</issue>),
<fpage>5</fpage>–<lpage>32</lpage>.</mixed-citation>
    </ref>
    <ref id="R10">
      <mixed-citation publication-type="journal"><name><surname>Clark</surname><given-names>K</given-names></name>, <name><surname>Vendt</surname><given-names>B</given-names></name>, <name><surname>Smith</surname><given-names>K</given-names></name>, <name><surname>Freymann</surname><given-names>J</given-names></name>, <name><surname>Kirby</surname><given-names>J</given-names></name>, <name><surname>Koppel</surname><given-names>P</given-names></name>, <name><surname>Moore</surname><given-names>S</given-names></name>, <name><surname>Phillips</surname><given-names>S</given-names></name>, <name><surname>Maffitt</surname><given-names>D</given-names></name>, <name><surname>Pringle</surname><given-names>M</given-names></name>, <etal/>, <year>2013</year>. <article-title>The cancer imaging archive
(TCIA): maintaining and operating a public information
repository</article-title>. <source>J. Digit. Image</source><volume>26</volume> (<issue>6</issue>),
<fpage>1045</fpage>–<lpage>1057</lpage>.</mixed-citation>
    </ref>
    <ref id="R11">
      <mixed-citation publication-type="confproc"><name><surname>Coupé</surname><given-names>P</given-names></name>, <name><surname>Manjón</surname><given-names>JV</given-names></name>, <name><surname>Fonov</surname><given-names>V</given-names></name>, <name><surname>Pruessner</surname><given-names>J</given-names></name>, <name><surname>Robles</surname><given-names>M</given-names></name>, <name><surname>Collins</surname><given-names>DL</given-names></name>, <year>2010</year>. <source>Non-local patch-based label fusion for hippocampus segmentation</source>. In: <conf-name>International Conference
on Medical Image Computing and Computer-Assisted Intervention</conf-name>.
<publisher-name>Springer</publisher-name>, pp.
<fpage>129</fpage>–<lpage>136</lpage>.</mixed-citation>
    </ref>
    <ref id="R12">
      <mixed-citation publication-type="journal"><name><surname>Coupé</surname><given-names>P</given-names></name>, <name><surname>Manjón</surname><given-names>JV</given-names></name>, <name><surname>Fonov</surname><given-names>V</given-names></name>, <name><surname>Pruessner</surname><given-names>J</given-names></name>, <name><surname>Robles</surname><given-names>M</given-names></name>, <name><surname>Collins</surname><given-names>DL</given-names></name>, <year>2011</year>. <article-title>Patch-based segmentation using expert
priors: application to hippocampus and ventricle
segmentation</article-title>. <source>Neuroimage</source><volume>54</volume> (<issue>2</issue>),
<fpage>940</fpage>–<lpage>954</lpage>.<pub-id pub-id-type="pmid">20851199</pub-id></mixed-citation>
    </ref>
    <ref id="R13">
      <mixed-citation publication-type="journal"><name><surname>Cox</surname><given-names>RW</given-names></name>, <year>1996</year>. <article-title>Afni: software for analysis and
visualization of functional magnetic resonance neuroimages</article-title>.
<source>Comput. Biomed. Res</source><volume>29</volume> (<issue>3</issue>),
<fpage>162</fpage>–<lpage>173</lpage>.<pub-id pub-id-type="pmid">8812068</pub-id></mixed-citation>
    </ref>
    <ref id="R14">
      <mixed-citation publication-type="journal"><name><surname>Cox</surname><given-names>RW</given-names></name>, <name><surname>Jesmanowicz</surname><given-names>A</given-names></name>, <year>1999</year>. <article-title>Real-time 3d image registration for
functional MRI</article-title>. <source>Magnet. Reson. Med</source><volume>42</volume> (<issue>6</issue>),
<fpage>1014</fpage>–<lpage>1018</lpage>.</mixed-citation>
    </ref>
    <ref id="R15">
      <mixed-citation publication-type="journal"><name><surname>Dai</surname><given-names>W</given-names></name>, <name><surname>Garcia</surname><given-names>D</given-names></name>, <name><surname>De Bazelaire</surname><given-names>C</given-names></name>, <name><surname>Alsop</surname><given-names>DC</given-names></name>, <year>2008</year>. <article-title>Continuous flow-driven inversion for
arterial spin labeling using pulsed radio frequency and gradient
fields</article-title>. <source>Magnet. Reson. Med</source><volume>60</volume> (<issue>6</issue>),
<fpage>1488</fpage>–<lpage>1497</lpage>.</mixed-citation>
    </ref>
    <ref id="R16">
      <mixed-citation publication-type="journal"><name><surname>Dalca</surname><given-names>AV</given-names></name>, <name><surname>Balakrishnan</surname><given-names>G</given-names></name>, <name><surname>Guttag</surname><given-names>J</given-names></name>, <name><surname>Sabuncu</surname><given-names>M</given-names></name>, <year>2019</year>. <article-title>Unsupervised learning of
probabilistic diffeomorphic registration for images and
surfaces</article-title>. <source>Med.I.A</source><volume>57</volume>,
<fpage>226</fpage>–<lpage>236</lpage>.</mixed-citation>
    </ref>
    <ref id="R17">
      <mixed-citation publication-type="confproc"><name><surname>Dalca</surname><given-names>AV</given-names></name>, <name><surname>Guttag</surname><given-names>J</given-names></name>, <name><surname>Sabuncu</surname><given-names>MR</given-names></name>, <year>2018</year>. <source>Anatomical priors in convolutional networks for unsupervised biomedical segmentation</source>. In:
<conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition</conf-name>, pp.
<fpage>9290</fpage>–<lpage>9299</lpage>.</mixed-citation>
    </ref>
    <ref id="R18">
      <mixed-citation publication-type="journal"><name><surname>Dice</surname><given-names>LR</given-names></name>, <year>1945</year>. <article-title>Measures of the amount of ecologic
association between species</article-title>. <source>Ecology</source><volume>26</volume> (<issue>3</issue>),
<fpage>297</fpage>–<lpage>302</lpage>.</mixed-citation>
    </ref>
    <ref id="R19">
      <mixed-citation publication-type="journal"><name><surname>Ebner</surname><given-names>M</given-names></name>, <name><surname>Wang</surname><given-names>G</given-names></name>, <name><surname>Li</surname><given-names>W</given-names></name>, <name><surname>Aertsen</surname><given-names>M</given-names></name>, <name><surname>Patel</surname><given-names>PA</given-names></name>, <name><surname>Aughwane</surname><given-names>R</given-names></name>, <name><surname>Melbourne</surname><given-names>A</given-names></name>, <name><surname>Doel</surname><given-names>T</given-names></name>, <name><surname>Dymarkowski</surname><given-names>S</given-names></name>, <name><surname>De Coppi</surname><given-names>P</given-names></name>, <etal/>, <year>2020</year>. <article-title>An automated framework for
localization, segmentation and super-resolution reconstruction of fetal
brain MRI</article-title>. <source>Neuroimage</source><volume>206</volume>, <fpage>116324</fpage>.<pub-id pub-id-type="pmid">31704293</pub-id></mixed-citation>
    </ref>
    <ref id="R20">
      <mixed-citation publication-type="journal"><name><surname>Eskildsen</surname><given-names>SF</given-names></name>, <name><surname>Coupé</surname><given-names>P</given-names></name>, <name><surname>Fonov</surname><given-names>V</given-names></name>, <name><surname>Manjón</surname><given-names>JV</given-names></name>, <name><surname>Leung</surname><given-names>KK</given-names></name>, <name><surname>Guizard</surname><given-names>N</given-names></name>, <name><surname>Wassef</surname><given-names>SN</given-names></name>, <name><surname>Østergaard</surname><given-names>LR</given-names></name>, <name><surname>Collins</surname><given-names>DL</given-names></name>, <name><surname>Initiative</surname><given-names>ADN</given-names></name>, <etal/>, <year>2012</year>. <article-title>Beast: brain extraction
based on nonlocal segmentation technique</article-title>.
<source>Neuroimage</source><volume>59</volume> (<issue>3</issue>),
<fpage>2362</fpage>–<lpage>2373</lpage>.<pub-id pub-id-type="pmid">21945694</pub-id></mixed-citation>
    </ref>
    <ref id="R21">
      <mixed-citation publication-type="journal"><name><surname>Fatima</surname><given-names>A</given-names></name>, <name><surname>Shahid</surname><given-names>AR</given-names></name>, <name><surname>Raza</surname><given-names>B</given-names></name>, <name><surname>Madni</surname><given-names>TM</given-names></name>, <name><surname>Janjua</surname><given-names>UI</given-names></name>, <year>2020</year>. <article-title>State-of-the-art traditional to the
machine-and deep-learning-based skull stripping techniques, models, and
algorithms</article-title>. <source>J. Digit. Imaging</source><volume>33</volume> (<issue>6</issue>),
<fpage>1443</fpage>–<lpage>1464</lpage>.<pub-id pub-id-type="pmid">32666364</pub-id></mixed-citation>
    </ref>
    <ref id="R22">
      <mixed-citation publication-type="journal"><name><surname>Fein</surname><given-names>G</given-names></name>, <name><surname>Landman</surname><given-names>B</given-names></name>, <name><surname>Tran</surname><given-names>H</given-names></name>, <name><surname>Barakos</surname><given-names>J</given-names></name>, <name><surname>Moon</surname><given-names>K</given-names></name>, <name><surname>Di Sclafani</surname><given-names>V</given-names></name>, <name><surname>Shumway</surname><given-names>R</given-names></name>, <year>2006</year>. <article-title>Statistical parametric mapping of
brain morphology: sensitivity is dramatically increased by using
brain-extracted images as inputs</article-title>.
<source>Neuroimage</source><volume>30</volume> (<issue>4</issue>),
<fpage>1187</fpage>–<lpage>1195</lpage>.<pub-id pub-id-type="pmid">16442817</pub-id></mixed-citation>
    </ref>
    <ref id="R23">
      <mixed-citation publication-type="journal"><name><surname>Fischl</surname><given-names>B</given-names></name>, <year>2012</year>. <article-title>Freesurfer</article-title>.
<source>Neuroimage</source><volume>62</volume> (<issue>2</issue>),
<fpage>774</fpage>–<lpage>781</lpage>. <comment>20 YEARS OF
fMRI</comment><pub-id pub-id-type="pmid">22248573</pub-id></mixed-citation>
    </ref>
    <ref id="R24">
      <mixed-citation publication-type="journal"><name><surname>Fischl</surname><given-names>B</given-names></name>, <etal/>, <year>2002</year>. <article-title>Whole brain segmentation:
automated labeling of neuroanatomical structures in the human
brain</article-title>. <source>Neuron</source><volume>33</volume> (<issue>3</issue>),
<fpage>341</fpage>–<lpage>355</lpage>.<pub-id pub-id-type="pmid">11832223</pub-id></mixed-citation>
    </ref>
    <ref id="R25">
      <mixed-citation publication-type="journal"><name><surname>Fischmeister</surname><given-names>FPS</given-names></name>, <name><surname>Höllinger</surname><given-names>I</given-names></name>, <name><surname>Klinger</surname><given-names>N</given-names></name>, <name><surname>Geissler</surname><given-names>A</given-names></name>, <name><surname>Wurnig</surname><given-names>MC</given-names></name>, <name><surname>Matt</surname><given-names>E</given-names></name>, <name><surname>Rath</surname><given-names>J</given-names></name>, <name><surname>Robinson</surname><given-names>SD</given-names></name>, <name><surname>Trattnig</surname><given-names>S</given-names></name>, <name><surname>Beisteiner</surname><given-names>R</given-names></name>, <year>2013</year>. <article-title>The benefits of skull stripping in
the normalization of clinical fMRI data</article-title>. <source>NeuroImage: Clinical</source><volume>3</volume>,
<fpage>369</fpage>–<lpage>380</lpage>.<pub-id pub-id-type="pmid">24273720</pub-id></mixed-citation>
    </ref>
    <ref id="R26">
      <mixed-citation publication-type="journal"><name><surname>Friston</surname><given-names>KJ</given-names></name>, <name><surname>Ashburner</surname><given-names>J</given-names></name>, <name><surname>Frith</surname><given-names>CD</given-names></name>, <name><surname>Poline</surname><given-names>J-B</given-names></name>, <name><surname>Heather</surname><given-names>JD</given-names></name>, <name><surname>Frackowiak</surname><given-names>RSJ</given-names></name>, <year>1995</year>. <article-title>Spatial registration and
normalization of images</article-title>. <source>Hum. Brain Mapp</source><volume>3</volume> (<issue>3</issue>),
<fpage>165</fpage>–<lpage>189</lpage>.</mixed-citation>
    </ref>
    <ref id="R27">
      <mixed-citation publication-type="book"><name><surname>Gaudfernau</surname><given-names>F</given-names></name>, <name><surname>Blondiaux</surname><given-names>E</given-names></name>, <name><surname>Allassonière</surname><given-names>S</given-names></name>, <year>2021</year>. <part-title>Analysis of the Anatomical
Variability of Fetal Brains with Corpus Callosum Agenesis</part-title>.
In: <source>Uncertainty for Safe Utilization of Machine Learning in Medical Imaging, and Perinatal Imaging, Placental and Preterm Image Analysis</source>. <publisher-name>Springer</publisher-name>, pp.
<fpage>274</fpage>–<lpage>283</lpage>.</mixed-citation>
    </ref>
    <ref id="R28">
      <mixed-citation publication-type="journal"><name><surname>Greig</surname><given-names>DM</given-names></name>, <name><surname>Porteous</surname><given-names>BT</given-names></name>, <name><surname>Seheult</surname><given-names>AH</given-names></name>, <year>1989</year>. <article-title>Exact maximum a posteriori estimation
for binary images</article-title>. <source>J. R. Stat. Soc.: Ser. B (Methodological)</source><volume>51</volume> (<issue>2</issue>),
<fpage>271</fpage>–<lpage>279</lpage>.</mixed-citation>
    </ref>
    <ref id="R29">
      <mixed-citation publication-type="journal"><name><surname>Greve</surname><given-names>DN</given-names></name>, <name><surname>Billot</surname><given-names>B</given-names></name>, <name><surname>Cordero</surname><given-names>D</given-names></name>, <name><surname>Hoopes</surname><given-names>A</given-names></name>, <name><surname>Hoffmann</surname><given-names>M</given-names></name>, <name><surname>Dalca</surname><given-names>AV</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, <name><surname>Iglesias</surname><given-names>JE</given-names></name>, <name><surname>Augustinack</surname><given-names>JC</given-names></name>, <year>2021</year>. <article-title>A deep learning toolbox for automatic
segmentation of subcortical limbic structures from MRI
images</article-title>. <source>Neuroimage</source><volume>244</volume>, <fpage>118610</fpage>.<pub-id pub-id-type="pmid">34571161</pub-id></mixed-citation>
    </ref>
    <ref id="R30">
      <mixed-citation publication-type="journal"><name><surname>Harms</surname><given-names>MP</given-names></name>, <etal/>, <year>2018</year>. <article-title>Extending the human
connectome project across ages: imaging protocols for the lifespan
development and aging projects</article-title>. <source>Neuroimage</source><volume>183</volume>,
<fpage>972</fpage>–<lpage>984</lpage>.<pub-id pub-id-type="pmid">30261308</pub-id></mixed-citation>
    </ref>
    <ref id="R31">
      <mixed-citation publication-type="confproc"><name><surname>Hendrycks</surname><given-names>D</given-names></name>, <name><surname>Basart</surname><given-names>S</given-names></name>, <name><surname>Mu</surname><given-names>N</given-names></name>, <name><surname>Kadavath</surname><given-names>S</given-names></name>, <name><surname>Wang</surname><given-names>F</given-names></name>, <name><surname>Dorundo</surname><given-names>E</given-names></name>, <name><surname>Desai</surname><given-names>R</given-names></name>, <name><surname>Zhu</surname><given-names>T</given-names></name>, <name><surname>Parajuli</surname><given-names>S</given-names></name>, <name><surname>Guo</surname><given-names>M</given-names></name>, <name><surname>Song</surname><given-names>D</given-names></name>, <name><surname>Steinhardt</surname><given-names>J</given-names></name>, <name><surname>Gilmer</surname><given-names>J</given-names></name>, <year>2021</year>. <source>The many faces of robustness: A critical analysis of out-of-distribution generalization</source>. In:
<conf-name>Proceedings of the IEEE/CVF International Conference on Computer
Vision (ICCV)</conf-name>, pp.
<fpage>8340</fpage>–<lpage>8349</lpage>.</mixed-citation>
    </ref>
    <ref id="R32">
      <mixed-citation publication-type="journal"><name><surname>Hoffmann</surname><given-names>M</given-names></name>, <name><surname>Abaci Turk</surname><given-names>E</given-names></name>, <name><surname>Gagoski</surname><given-names>B</given-names></name>, <name><surname>Morgan</surname><given-names>L</given-names></name>, <name><surname>Wighton</surname><given-names>P</given-names></name>, <name><surname>Tisdall</surname><given-names>MD</given-names></name>, <name><surname>Reuter</surname><given-names>M</given-names></name>, <name><surname>Adalsteinsson</surname><given-names>E</given-names></name>, <name><surname>Grant</surname><given-names>PE</given-names></name>, <name><surname>Wald</surname><given-names>LL</given-names></name>, <etal/>, <year>2021</year>. <article-title>Rapid head-pose detection
for automated slice prescription of fetal-brain MRI</article-title>.
<source>Int. J. Imag. Syst. Technol</source><volume>31</volume> (<issue>3</issue>),
<fpage>1136</fpage>–<lpage>1154</lpage>.</mixed-citation>
    </ref>
    <ref id="R33">
      <mixed-citation publication-type="confproc"><name><surname>Hoffmann</surname><given-names>M</given-names></name>, <name><surname>Billot</surname><given-names>B</given-names></name>, <name><surname>Iglesias</surname><given-names>JE</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, <name><surname>Dalca</surname><given-names>AV</given-names></name>, <year>2021</year>. <source>Learning mri contrast-agnostic registration</source>. In: <conf-name>2021 IEEE 18th International Symposium
on Biomedical Imaging (ISBI)</conf-name>.
<publisher-name>IEEE</publisher-name>, pp.
<fpage>899</fpage>–<lpage>903</lpage>.</mixed-citation>
    </ref>
    <ref id="R34">
      <mixed-citation publication-type="journal"><name><surname>Hoffmann</surname><given-names>M</given-names></name>, <name><surname>Carpenter</surname><given-names>TA</given-names></name>, <name><surname>Williams</surname><given-names>GB</given-names></name>, <name><surname>Sawiak</surname><given-names>SJ</given-names></name>, <year>2015</year>. <article-title>A survey of patient motion in
disorders of consciousness and optimization of its retrospective
correction</article-title>. <source>Magn. Reson. Imaging</source><volume>33</volume> (<issue>3</issue>),
<fpage>346</fpage>–<lpage>350</lpage>.<pub-id pub-id-type="pmid">25485789</pub-id></mixed-citation>
    </ref>
    <ref id="R35">
      <mixed-citation publication-type="confproc"><name><surname>Hoffmann</surname><given-names>M</given-names></name>, <name><surname>Frost</surname><given-names>R</given-names></name>, <name><surname>Salat</surname><given-names>D</given-names></name>, <name><surname>Tisdall</surname><given-names>MD</given-names></name>, <name><surname>Polimeni</surname><given-names>J</given-names></name>, <name><surname>van der Kouwe</surname><given-names>A</given-names></name>, <year>2020</year>. <source>Real-time brain masking algorithm improves motion tracking accuracy in scans with volumetric navigators (vNavs)</source>. In: <conf-name>International Society for Magnetic
Resonance in Medicine</conf-name>. <publisher-name>ISMRM</publisher-name>,
p. <fpage>3367</fpage>.</mixed-citation>
    </ref>
    <ref id="R36">
      <mixed-citation publication-type="journal"><name><surname>Holdsworth</surname><given-names>SJ</given-names></name>, <name><surname>Aksoy</surname><given-names>M</given-names></name>, <name><surname>Newbould</surname><given-names>RD</given-names></name>, <name><surname>Yeom</surname><given-names>K</given-names></name>, <name><surname>Van</surname><given-names>AT</given-names></name>, <name><surname>Ooi</surname><given-names>MB</given-names></name>, <name><surname>Barnes</surname><given-names>PD</given-names></name>, <name><surname>Bammer</surname><given-names>R</given-names></name>, <name><surname>Skare</surname><given-names>S</given-names></name>, <year>2012</year>. <article-title>Diffusion tensor imaging (DTI) with
retrospective motion correction for large-scale pediatric
imaging</article-title>. <source>J. Magn. Reson. Imaging</source><volume>36</volume> (<issue>4</issue>),
<fpage>961</fpage>–<lpage>971</lpage>.<pub-id pub-id-type="pmid">22689498</pub-id></mixed-citation>
    </ref>
    <ref id="R37">
      <mixed-citation publication-type="journal"><name><surname>Hsu</surname><given-names>L-M</given-names></name>, <name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Ranadive</surname><given-names>P</given-names></name>, <name><surname>Ban</surname><given-names>W</given-names></name>, <name><surname>Chao</surname><given-names>T-HH</given-names></name>, <name><surname>Song</surname><given-names>S</given-names></name>, <name><surname>Cerri</surname><given-names>DH</given-names></name>, <name><surname>Walton</surname><given-names>LR</given-names></name>, <name><surname>Broadwater</surname><given-names>MA</given-names></name>, <name><surname>Lee</surname><given-names>S-H</given-names></name>, <etal/>, <year>2020</year>. <article-title>Automatic skull stripping of
rat and mouse brain MRI data using u-net</article-title>. <source>Front. Neurosci</source><volume>14</volume>, <fpage>568614</fpage>.<pub-id pub-id-type="pmid">33117118</pub-id></mixed-citation>
    </ref>
    <ref id="R38">
      <mixed-citation publication-type="journal"><name><surname>Hwang</surname><given-names>H</given-names></name>, <name><surname>Rehman</surname><given-names>HZU</given-names></name>, <name><surname>Lee</surname><given-names>S</given-names></name>, <year>2019</year>. <article-title>3D u-net for skull stripping in brain
MRI</article-title>. <source>Appl. Sci</source><volume>9</volume> (<issue>3</issue>), <fpage>569</fpage>.</mixed-citation>
    </ref>
    <ref id="R39">
      <mixed-citation publication-type="journal"><name><surname>Iglesias</surname><given-names>JE</given-names></name>, <name><surname>Billot</surname><given-names>B</given-names></name>, <name><surname>Balbastre</surname><given-names>Y</given-names></name>, <name><surname>Tabari</surname><given-names>A</given-names></name>, <name><surname>Conklin</surname><given-names>J</given-names></name>, <name><surname>González</surname><given-names>RG</given-names></name>, <name><surname>Alexander</surname><given-names>DC</given-names></name>, <name><surname>Golland</surname><given-names>P</given-names></name>, <name><surname>Edlow</surname><given-names>BL</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, <etal/>, <year>2021</year>. <article-title>Joint super-resolution and
synthesis of 1 mm isotropic MP-RAGE volumes from clinical MRI exams with
scans of different orientation, resolution and contrast</article-title>.
<source>Neuroimage</source><volume>237</volume>, <fpage>118206</fpage>.<pub-id pub-id-type="pmid">34048902</pub-id></mixed-citation>
    </ref>
    <ref id="R40">
      <mixed-citation publication-type="journal"><name><surname>Iglesias</surname><given-names>JE</given-names></name>, <name><surname>Liu</surname><given-names>C-Y</given-names></name>, <name><surname>Thompson</surname><given-names>PM</given-names></name>, <name><surname>Tu</surname><given-names>Z</given-names></name>, <year>2011</year>. <article-title>Robust brain extraction across
datasets and comparison with publicly available methods</article-title>.
<source>IEEE Trans. Med. Imag</source><volume>30</volume> (<issue>9</issue>),
<fpage>1617</fpage>–<lpage>1634</lpage>.</mixed-citation>
    </ref>
    <ref id="R41">
      <mixed-citation publication-type="journal"><name><surname>Jenkinson</surname><given-names>M</given-names></name>, <name><surname>Bannister</surname><given-names>P</given-names></name>, <name><surname>Brady</surname><given-names>M</given-names></name>, <name><surname>Smith</surname><given-names>S</given-names></name>, <year>2002</year>. <article-title>Improved optimization for the robust
and accurate linear registration and motion correction of brain
images</article-title>. <source>Neuroimage</source><volume>17</volume> (<issue>2</issue>),
<fpage>825</fpage>–<lpage>841</lpage>.<pub-id pub-id-type="pmid">12377157</pub-id></mixed-citation>
    </ref>
    <ref id="R42">
      <mixed-citation publication-type="journal"><name><surname>Jenkinson</surname><given-names>M</given-names></name>, <name><surname>Beckmann</surname><given-names>CF</given-names></name>, <name><surname>Behrens</surname><given-names>TEJ</given-names></name>, <name><surname>Woolrich</surname><given-names>MW</given-names></name>, <name><surname>Smith</surname><given-names>SM</given-names></name>, <year>2012</year>. <source>Fsl. Neuroimage</source><volume>62</volume> (<issue>2</issue>),
<fpage>782</fpage>–<lpage>790</lpage>.<pub-id pub-id-type="pmid">21979382</pub-id></mixed-citation>
    </ref>
    <ref id="R43">
      <mixed-citation publication-type="journal"><name><surname>Jenkinson</surname><given-names>M</given-names></name>, <name><surname>Smith</surname><given-names>S</given-names></name>, <year>2001</year>. <article-title>A global optimisation method for
robust affine registration of brain images</article-title>. <source>Med. Image. Anal</source><volume>5</volume> (<issue>2</issue>),
<fpage>143</fpage>–<lpage>156</lpage>.<pub-id pub-id-type="pmid">11516708</pub-id></mixed-citation>
    </ref>
    <ref id="R44">
      <mixed-citation publication-type="journal"><name><surname>Jiang</surname><given-names>A</given-names></name>, <name><surname>Kennedy</surname><given-names>DN</given-names></name>, <name><surname>Baker</surname><given-names>JR</given-names></name>, <name><surname>Weisskoff</surname><given-names>RM</given-names></name>, <name><surname>Tootell</surname><given-names>RBH</given-names></name>, <name><surname>Woods</surname><given-names>RP</given-names></name>, <name><surname>Benson</surname><given-names>RR</given-names></name>, <name><surname>Kwong</surname><given-names>KK</given-names></name>, <name><surname>Brady</surname><given-names>TJ</given-names></name>, <name><surname>Rosen</surname><given-names>BR</given-names></name>, <etal/>, <year>1995</year>. <article-title>Motion detection and
correction in functional MR imaging</article-title>. <source>Hum. Brain Mapp</source><volume>3</volume> (<issue>3</issue>),
<fpage>224</fpage>–<lpage>235</lpage>.</mixed-citation>
    </ref>
    <ref id="R45">
      <mixed-citation publication-type="journal"><name><surname>Jog</surname><given-names>A</given-names></name>, <name><surname>Hoopes</surname><given-names>A</given-names></name>, <name><surname>Greve</surname><given-names>DN</given-names></name>, <name><surname>Van Leemput</surname><given-names>K</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, <year>2019</year>. <article-title>Psacnn: pulse sequence adaptive fast
whole brain segmentation</article-title>. <source>Neuroimage</source><volume>199</volume>,
<fpage>553</fpage>–<lpage>569</lpage>.<pub-id pub-id-type="pmid">31129303</pub-id></mixed-citation>
    </ref>
    <ref id="R46">
      <mixed-citation publication-type="book"><name><surname>Jones</surname><given-names>DK</given-names></name>, <name><surname>Leemans</surname><given-names>A</given-names></name>, <year>2011</year>. <part-title>Diffusion Tensor
Imaging</part-title>. In: <source>Magnetic resonance neuroimaging</source>. <publisher-name>Springer</publisher-name>, pp.
<fpage>127</fpage>–<lpage>144</lpage>.</mixed-citation>
    </ref>
    <ref id="R47">
      <mixed-citation publication-type="journal"><name><surname>Kainz</surname><given-names>B</given-names></name>, <name><surname>Steinberger</surname><given-names>M</given-names></name>, <name><surname>Wein</surname><given-names>W</given-names></name>, <name><surname>Kuklisova-Murgasova</surname><given-names>M</given-names></name>, <name><surname>Malamateniou</surname><given-names>C</given-names></name>, <name><surname>Keraudren</surname><given-names>K</given-names></name>, <name><surname>Torsney-Weir</surname><given-names>T</given-names></name>, <name><surname>Rutherford</surname><given-names>M</given-names></name>, <name><surname>Aljabar</surname><given-names>P</given-names></name>, <name><surname>Hajnal</surname><given-names>JV</given-names></name>, <etal/>, <year>2015</year>. <article-title>Fast volume reconstruction
from motion corrupted stacks of 2d slices</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>34</volume> (<issue>9</issue>),
<fpage>1901</fpage>–<lpage>1913</lpage>.<pub-id pub-id-type="pmid">25807565</pub-id></mixed-citation>
    </ref>
    <ref id="R48">
      <mixed-citation publication-type="book"><name><surname>Karani</surname><given-names>N</given-names></name>, <name><surname>Chaitanya</surname><given-names>K</given-names></name>, <name><surname>Baumgartner</surname><given-names>C</given-names></name>, <name><surname>Konukoglu</surname><given-names>E</given-names></name>, <year>2018</year>. <part-title>A lifelong learning approach to brain
MR segmentation across scanners and protocols</part-title>. In:
<source>Medical Image Computing and Computer Assisted Intervention – MICCAI 2018</source>. <publisher-name>Springer International Publishing,
Cham</publisher-name>, pp.
<fpage>476</fpage>–<lpage>484</lpage>.</mixed-citation>
    </ref>
    <ref id="R49">
      <mixed-citation publication-type="other"><name><surname>Kingma</surname><given-names>DP</given-names></name>, <name><surname>Ba</surname><given-names>J</given-names></name>, <year>2014</year>. <article-title>Adam: a method for stochastic
optimization</article-title>. <source>arXiv preprint arXiv:1412.6980</source>.</mixed-citation>
    </ref>
    <ref id="R50">
      <mixed-citation publication-type="journal"><name><surname>Kleesiek</surname><given-names>J</given-names></name>, <name><surname>Urban</surname><given-names>G</given-names></name>, <name><surname>Hubert</surname><given-names>A</given-names></name>, <name><surname>Schwarz</surname><given-names>D</given-names></name>, <name><surname>Maier-Hein</surname><given-names>K</given-names></name>, <name><surname>Bendszus</surname><given-names>M</given-names></name>, <name><surname>Biller</surname><given-names>A</given-names></name>, <year>2016</year>. <article-title>Deep MRI brain extraction: a 3d
convolutional neural network for skull stripping</article-title>.
<source>Neuroimage</source><volume>129</volume>,
<fpage>460</fpage>–<lpage>469</lpage>.<pub-id pub-id-type="pmid">26808333</pub-id></mixed-citation>
    </ref>
    <ref id="R51">
      <mixed-citation publication-type="journal"><name><surname>Klein</surname><given-names>A</given-names></name>, <name><surname>Andersson</surname><given-names>J</given-names></name>, <name><surname>Ardekani</surname><given-names>BA</given-names></name>, <name><surname>Ashburner</surname><given-names>J</given-names></name>, <name><surname>Avants</surname><given-names>B</given-names></name>, <name><surname>Chiang</surname><given-names>M-C</given-names></name>, <name><surname>Christensen</surname><given-names>GE</given-names></name>, <name><surname>Collins</surname><given-names>DL</given-names></name>, <name><surname>Gee</surname><given-names>J</given-names></name>, <name><surname>Hellier</surname><given-names>P</given-names></name>, <etal/>, <year>2009</year>. <article-title>Evaluation of 14 nonlinear
deformation algorithms applied to human brain MRI
registration</article-title>. <source>Neuroimage</source><volume>46</volume> (<issue>3</issue>),
<fpage>786</fpage>–<lpage>802</lpage>.<pub-id pub-id-type="pmid">19195496</pub-id></mixed-citation>
    </ref>
    <ref id="R52">
      <mixed-citation publication-type="journal"><name><surname>van der Kouwe</surname><given-names>AJW</given-names></name>, <name><surname>Benner</surname><given-names>T</given-names></name>, <name><surname>Salat</surname><given-names>DH</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, <year>2008</year>. <article-title>Brain morphometry with multiecho
MPRAGE</article-title>. <source>Neuroimage</source><volume>40</volume> (<issue>2</issue>),
<fpage>559</fpage>–<lpage>569</lpage>.<pub-id pub-id-type="pmid">18242102</pub-id></mixed-citation>
    </ref>
    <ref id="R53">
      <mixed-citation publication-type="journal"><name><surname>Lucena</surname><given-names>O</given-names></name>, <name><surname>Souza</surname><given-names>R</given-names></name>, <name><surname>Rittner</surname><given-names>L</given-names></name>, <name><surname>Frayne</surname><given-names>R</given-names></name>, <name><surname>Lotufo</surname><given-names>R</given-names></name>, <year>2019</year>. <article-title>Convolutional neural networks for
skull-stripping in brain MR imaging using silver standard
masks</article-title>. <source>Artif. Intell. Med</source><volume>98</volume>,
<fpage>48</fpage>–<lpage>58</lpage>.<pub-id pub-id-type="pmid">31521252</pub-id></mixed-citation>
    </ref>
    <ref id="R54">
      <mixed-citation publication-type="journal"><name><surname>de Macedo Rodrigues</surname><given-names>K</given-names></name>, <name><surname>Ben-Avi</surname><given-names>E</given-names></name>, <name><surname>Sliva</surname><given-names>DD</given-names></name>, <name><surname>Choe</surname><given-names>M.-s.</given-names></name>, <name><surname>Drottar</surname><given-names>M</given-names></name>, <name><surname>Wang</surname><given-names>R</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, <name><surname>Grant</surname><given-names>PE</given-names></name>, <name><surname>Zöllei</surname><given-names>L</given-names></name>, <year>2015</year>. <article-title>A freesurfer-compliant consistent
manual segmentation of infant brains spanning the 0–2 year age
range</article-title>. <source>Front. Hum. Neurosci</source><volume>9</volume>, <fpage>21</fpage>.<pub-id pub-id-type="pmid">25741260</pub-id></mixed-citation>
    </ref>
    <ref id="R55">
      <mixed-citation publication-type="journal"><name><surname>Mamonov</surname><given-names>AB</given-names></name>, <name><surname>Kalpathy-Cramer</surname><given-names>J</given-names></name>, <year>2016</year>. <source>Data from QIN GBM treatment response</source>. <pub-id pub-id-type="doi">10.7937/k9/tcia.2016.nQF4gpn2</pub-id></mixed-citation>
    </ref>
    <ref id="R56">
      <mixed-citation publication-type="journal"><name><surname>Marques</surname><given-names>JP</given-names></name>, <name><surname>Kober</surname><given-names>T</given-names></name>, <name><surname>Krueger</surname><given-names>G</given-names></name>, <name><surname>van der Zwaag</surname><given-names>W</given-names></name>, <name><surname>Van de Moortele</surname><given-names>P-F</given-names></name>, <name><surname>Gruetter</surname><given-names>R</given-names></name>, <year>2010</year>. <article-title>MP2Rage, a self bias-field corrected
sequence for improved segmentation and T1-mapping at high
field</article-title>. <source>Neuroimage</source><volume>49</volume> (<issue>2</issue>),
<fpage>1271</fpage>–<lpage>1281</lpage>.<pub-id pub-id-type="pmid">19819338</pub-id></mixed-citation>
    </ref>
    <ref id="R57">
      <mixed-citation publication-type="journal"><name><surname>Mérida</surname><given-names>I</given-names></name>, <name><surname>Jung</surname><given-names>J</given-names></name>, <name><surname>Bouvard</surname><given-names>S</given-names></name>, <name><surname>Le Bars</surname><given-names>D</given-names></name>, <name><surname>Lancelot</surname><given-names>S</given-names></name>, <name><surname>Lavenne</surname><given-names>F</given-names></name>, <name><surname>Bouillot</surname><given-names>C</given-names></name>, <name><surname>Redouté</surname><given-names>J</given-names></name>, <name><surname>Hammers</surname><given-names>A</given-names></name>, <name><surname>Costes</surname><given-names>N</given-names></name>, <year>2021</year>. <article-title>Cermep-idb-mrxfdg: a database of 37
normal adult human brain [18f] fdg pet, t1 and flair mri, and ct images
available for research</article-title>. <source>EJNMMI Res</source>.
<volume>11</volume> (<issue>1</issue>),
<fpage>1</fpage>–<lpage>10</lpage>.<pub-id pub-id-type="pmid">33394212</pub-id></mixed-citation>
    </ref>
    <ref id="R58">
      <mixed-citation publication-type="journal"><name><surname>Milletari</surname><given-names>F</given-names></name>, <name><surname>Navab</surname><given-names>N</given-names></name>, <name><surname>Ahmadi</surname><given-names>S-A</given-names></name>, <year>2016</year>. <article-title>V-net: Fully convolutional neural
networks for volumetric medical image segmentation</article-title>. In:
<source>3DV</source>, pp.
<fpage>565</fpage>–<lpage>571</lpage>.</mixed-citation>
    </ref>
    <ref id="R59">
      <mixed-citation publication-type="journal"><name><surname>Modat</surname><given-names>M</given-names></name>, <name><surname>Cash</surname><given-names>DM</given-names></name>, <name><surname>Daga</surname><given-names>P</given-names></name>, <name><surname>Winston</surname><given-names>GP</given-names></name>, <name><surname>Duncan</surname><given-names>JS</given-names></name>, <name><surname>Ourselin</surname><given-names>S</given-names></name>, <year>2014</year>. <article-title>Global image registration using a
symmetric block-matching approach</article-title>. <source>J. Med. Imaging</source><volume>1</volume> (<issue>2</issue>), <fpage>024003</fpage>.</mixed-citation>
    </ref>
    <ref id="R60">
      <mixed-citation publication-type="journal"><name><surname>Mugler III</surname><given-names>JP</given-names></name>, <name><surname>Brookeman</surname><given-names>JR</given-names></name>, <year>1990</year>. <article-title>Three-dimensional
magnetization-prepared rapid gradient-echo imaging (3D MP
RAGE)</article-title>. <source>Magn. Reson. Med</source><volume>15</volume> (<issue>1</issue>),
<fpage>152</fpage>–<lpage>157</lpage>. doi:<pub-id pub-id-type="doi">10.1002/mrm.1910150117</pub-id>.<pub-id pub-id-type="pmid">2374495</pub-id></mixed-citation>
    </ref>
    <ref id="R61">
      <mixed-citation publication-type="journal"><name><surname>Ou</surname><given-names>Y</given-names></name>, <name><surname>Akbari</surname><given-names>H</given-names></name>, <name><surname>Bilello</surname><given-names>M</given-names></name>, <name><surname>Da</surname><given-names>X</given-names></name>, <name><surname>Davatzikos</surname><given-names>C</given-names></name>, <year>2014</year>. <article-title>Comparative evaluation of
registration algorithms in different brain databases with varying
difficulty: results and insights</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>33</volume> (<issue>10</issue>),
<fpage>2039</fpage>–<lpage>2065</lpage>.<pub-id pub-id-type="pmid">24951685</pub-id></mixed-citation>
    </ref>
    <ref id="R62">
      <mixed-citation publication-type="journal"><name><surname>Paszke</surname><given-names>A</given-names></name>, <name><surname>Gross</surname><given-names>S</given-names></name>, <name><surname>Massa</surname><given-names>F</given-names></name>, <name><surname>Lerer</surname><given-names>A</given-names></name>, <name><surname>Bradbury</surname><given-names>J</given-names></name>, <name><surname>Chanan</surname><given-names>G</given-names></name>, <name><surname>Killeen</surname><given-names>T</given-names></name>, <name><surname>Lin</surname><given-names>Z</given-names></name>, <name><surname>Gimelshein</surname><given-names>N</given-names></name>, <name><surname>Antiga</surname><given-names>L</given-names></name>, <name><surname>Desmaison</surname><given-names>A</given-names></name>, <name><surname>Kopf</surname><given-names>A</given-names></name>, <name><surname>Yang</surname><given-names>E</given-names></name>, <name><surname>DeVito</surname><given-names>Z</given-names></name>, <name><surname>Raison</surname><given-names>M</given-names></name>, <name><surname>Tejani</surname><given-names>A</given-names></name>, <name><surname>Chilamkurthy</surname><given-names>S</given-names></name>, <name><surname>Steiner</surname><given-names>B</given-names></name>, <name><surname>Fang</surname><given-names>L</given-names></name>, <name><surname>Bai</surname><given-names>J</given-names></name>, <name><surname>Chintala</surname><given-names>S</given-names></name>, <year>2019</year>. <article-title>Pytorch: An Imperative Style,
High-performance Deep Learning Library</article-title>. In: <source>Advances in Neural Information Processing Systems</source><volume>32</volume>, pp.
<fpage>8024</fpage>–<lpage>8035</lpage>.</mixed-citation>
    </ref>
    <ref id="R63">
      <mixed-citation publication-type="journal"><name><surname>Prah</surname><given-names>MA</given-names></name>, <name><surname>Stufflebeam</surname><given-names>SM</given-names></name>, <name><surname>Paulson</surname><given-names>ES</given-names></name>, <name><surname>Kalpathy-Cramer</surname><given-names>J</given-names></name>, <name><surname>Gerstner</surname><given-names>ER</given-names></name>, <name><surname>Batchelor</surname><given-names>TT</given-names></name>, <name><surname>Barboriak</surname><given-names>DP</given-names></name>, <name><surname>Rosen</surname><given-names>BR</given-names></name>, <name><surname>Schmainda</surname><given-names>KM</given-names></name>, <year>2015</year>. <article-title>Repeatability of standardized and
normalized relative CBV in patients with newly diagnosed
glioblastoma</article-title>. <source>Am. J. Neuroradiol</source><volume>36</volume> (<issue>9</issue>),
<fpage>1654</fpage>–<lpage>1661</lpage>.<pub-id pub-id-type="pmid">26066626</pub-id></mixed-citation>
    </ref>
    <ref id="R64">
      <mixed-citation publication-type="journal"><name><surname>Puonti</surname><given-names>O</given-names></name>, <name><surname>Iglesias</surname><given-names>JE</given-names></name>, <name><surname>Leemput</surname><given-names>KV</given-names></name>, <year>2016</year>. <article-title>Fast and sequence-adaptive
whole-brain segmentation using parametric bayesian modeling</article-title>.
<source>Neuroimage</source><volume>143</volume>,
<fpage>235</fpage>–<lpage>249</lpage>.<pub-id pub-id-type="pmid">27612647</pub-id></mixed-citation>
    </ref>
    <ref id="R65">
      <mixed-citation publication-type="journal"><name><surname>Reuter</surname><given-names>M</given-names></name>, <name><surname>Rosas</surname><given-names>HD</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, <year>2010</year>. <article-title>Highly accurate inverse consistent
registration: arobust approach</article-title>. <source>Neuroimage</source><volume>53</volume> (<issue>4</issue>),
<fpage>1181</fpage>–<lpage>1196</lpage>.<pub-id pub-id-type="pmid">20637289</pub-id></mixed-citation>
    </ref>
    <ref id="R66">
      <mixed-citation publication-type="confproc"><name><surname>Ronneberger</surname><given-names>O</given-names></name>, <name><surname>Fischer</surname><given-names>P</given-names></name>, <name><surname>Brox</surname><given-names>T</given-names></name>, <year>2015</year>. <source>U-net: Convolutional networks for biomedical image segmentation</source>. In: <conf-name>International Conference on
Medical image computing and computer-assisted intervention</conf-name>.
<publisher-name>Springer</publisher-name>, pp.
<fpage>234</fpage>–<lpage>241</lpage>.</mixed-citation>
    </ref>
    <ref id="R67">
      <mixed-citation publication-type="journal"><name><surname>Rousseau</surname><given-names>F</given-names></name>, <name><surname>Glenn</surname><given-names>OA</given-names></name>, <name><surname>Iordanova</surname><given-names>B</given-names></name>, <name><surname>Rodriguez-Carranza</surname><given-names>C</given-names></name>, <name><surname>Vigneron</surname><given-names>DB</given-names></name>, <name><surname>Barkovich</surname><given-names>JA</given-names></name>, <name><surname>Studholme</surname><given-names>C</given-names></name>, <year>2006</year>. <article-title>Registration-based approach for
reconstruction of high-resolution in utero fetal MR brain
images</article-title>. <source>Acad. Radiol</source><volume>13</volume> (<issue>9</issue>),
<fpage>1072</fpage>–<lpage>1081</lpage> .<pub-id pub-id-type="pmid">16935719</pub-id></mixed-citation>
    </ref>
    <ref id="R68">
      <mixed-citation publication-type="journal"><name><surname>Roy</surname><given-names>S</given-names></name>, <name><surname>Butman</surname><given-names>JA</given-names></name>, <name><surname>Pham</surname><given-names>DL</given-names></name>, <name><surname>Initiative</surname><given-names>ADN</given-names></name>, <etal/>, <year>2017</year>. <article-title>Robust skull stripping using
multiple MR image contrasts insensitive to pathology</article-title>.
<source>Neuroimage</source><volume>146</volume>,
<fpage>132</fpage>–<lpage>147</lpage>.<pub-id pub-id-type="pmid">27864083</pub-id></mixed-citation>
    </ref>
    <ref id="R69">
      <mixed-citation publication-type="journal"><name><surname>Rueckert</surname><given-names>D</given-names></name>, <name><surname>Sonoda</surname><given-names>LI</given-names></name>, <name><surname>Hayes</surname><given-names>C</given-names></name>, <name><surname>Hill</surname><given-names>DLG</given-names></name>, <name><surname>Leach</surname><given-names>MO</given-names></name>, <name><surname>Hawkes</surname><given-names>DJ</given-names></name>, <year>1999</year>. <article-title>Non-rigid registration using
free-form deformations: application to breast MR images</article-title>.
<source>IEEE Trans. Med. Image</source><volume>18</volume> (<issue>8</issue>),
<fpage>712</fpage>–<lpage>721</lpage>.</mixed-citation>
    </ref>
    <ref id="R70">
      <mixed-citation publication-type="journal"><name><surname>Sadananthan</surname><given-names>SA</given-names></name>, <name><surname>Zheng</surname><given-names>W</given-names></name>, <name><surname>Chee</surname><given-names>MWL</given-names></name>, <name><surname>Zagorodnov</surname><given-names>V</given-names></name>, <year>2010</year>. <article-title>Skull stripping using graph
cuts</article-title>. <source>Neuroimage</source><volume>49</volume> (<issue>1</issue>),
<fpage>225</fpage>–<lpage>239</lpage>.<pub-id pub-id-type="pmid">19732839</pub-id></mixed-citation>
    </ref>
    <ref id="R71">
      <mixed-citation publication-type="journal"><name><surname>Salehi</surname><given-names>SSM</given-names></name>, <name><surname>Erdogmus</surname><given-names>D</given-names></name>, <name><surname>Gholipour</surname><given-names>A</given-names></name>, <year>2017</year>. <article-title>Auto-context convolutional neural
network (auto-net) for brain extraction in magnetic resonance
imaging</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>36</volume> (<issue>11</issue>),
<fpage>2319</fpage>–<lpage>2330</lpage>.<pub-id pub-id-type="pmid">28678704</pub-id></mixed-citation>
    </ref>
    <ref id="R72">
      <mixed-citation publication-type="journal"><name><surname>Ségonne</surname><given-names>F</given-names></name>, <name><surname>Dale</surname><given-names>AM</given-names></name>, <name><surname>Busa</surname><given-names>E</given-names></name>, <name><surname>Glessner</surname><given-names>M</given-names></name>, <name><surname>Salat</surname><given-names>D</given-names></name>, <name><surname>Hahn</surname><given-names>HK</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, <year>2004</year>. <article-title>A hybrid approach to the skull
stripping problem in MRI</article-title>. <source>Neuroimage</source><volume>22</volume> (<issue>3</issue>),
<fpage>1060</fpage>–<lpage>1075</lpage>.<pub-id pub-id-type="pmid">15219578</pub-id></mixed-citation>
    </ref>
    <ref id="R73">
      <mixed-citation publication-type="journal"><name><surname>Shattuck</surname><given-names>DW</given-names></name>, <name><surname>Sandor-Leahy</surname><given-names>SR</given-names></name>, <name><surname>Schaper</surname><given-names>KA</given-names></name>, <name><surname>Rottenberg</surname><given-names>DA</given-names></name>, <name><surname>Leahy</surname><given-names>RM</given-names></name>, <year>2001</year>. <article-title>Magnetic resonance image tissue
classification using a partial volume model</article-title>.
<source>Neuroimage</source><volume>13</volume> (<issue>5</issue>),
<fpage>856</fpage>–<lpage>876</lpage>.<pub-id pub-id-type="pmid">11304082</pub-id></mixed-citation>
    </ref>
    <ref id="R74">
      <mixed-citation publication-type="journal"><name><surname>Smith</surname><given-names>SM</given-names></name>, <year>2002</year>. <article-title>Fast robust automated brain
extraction</article-title>. <source>Hum. Brain Mapp</source><volume>17</volume> (<issue>3</issue>),
<fpage>143</fpage>–<lpage>155</lpage>.<pub-id pub-id-type="pmid">12391568</pub-id></mixed-citation>
    </ref>
    <ref id="R75">
      <mixed-citation publication-type="journal"><name><surname>Tu</surname><given-names>Z</given-names></name>, <name><surname>Bai</surname><given-names>X</given-names></name>, <year>2009</year>. <article-title>Auto-context and its application to
high-level vision tasks and 3d brain image segmentation</article-title>.
<source>IEEE Trans. Pattern Anal. Mach. Intell</source><volume>32</volume> (<issue>10</issue>),
<fpage>1744</fpage>–<lpage>1757</lpage>.</mixed-citation>
    </ref>
    <ref id="R76">
      <mixed-citation publication-type="journal"><name><surname>Vercauteren</surname><given-names>T</given-names></name>, <name><surname>Pennec</surname><given-names>X</given-names></name>, <name><surname>Perchant</surname><given-names>A</given-names></name>, <name><surname>Ayache</surname><given-names>N</given-names></name>, <year>2009</year>. <article-title>Diffeomorphic demons: efficient
non-parametric image registration</article-title>.
<source>Neuroimage</source><volume>45</volume> (<issue>1</issue>),
<fpage>S61</fpage>–<lpage>S72</lpage>.<pub-id pub-id-type="pmid">19041946</pub-id></mixed-citation>
    </ref>
    <ref id="R77">
      <mixed-citation publication-type="journal"><name><surname>Zöllei</surname><given-names>L</given-names></name>, <name><surname>Iglesias</surname><given-names>JE</given-names></name>, <name><surname>Ou</surname><given-names>Y</given-names></name>, <name><surname>Grant</surname><given-names>PE</given-names></name>, <name><surname>Fischl</surname><given-names>B</given-names></name>, <year>2020</year>. <article-title>Infant freesurfer: an automated
segmentation and surface extraction pipeline for t1-weighted neuroimaging
data of infants 0–2 years</article-title>.
<source>Neuroimage</source><volume>218</volume>, <fpage>116946</fpage>.<pub-id pub-id-type="pmid">32442637</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="F1">
    <label>Fig. 1.</label>
    <caption>
      <p id="P59">Examples of SynthStrip brain extractions (bottom) for a wide range of
image acquisitions and modalities (top). Powered by a strategy for synthesizing
diverse training data, SynthStrip learns to skull-strip brain images of any
type.</p>
    </caption>
    <graphic xlink:href="nihms-1833287-f0001" position="float"/>
  </fig>
  <fig position="float" id="F2">
    <label>Fig. 2.</label>
    <caption>
      <p id="P60">Samples of synthetic images used for SynthStrip training. To encourage
the network to generalize, we synthesize images that far exceed the realistic
range of whole-brain acquisitions. In this figure, each brain image is generated
from the same label map. In practice, we use label maps from several different
subjects.</p>
    </caption>
    <graphic xlink:href="nihms-1833287-f0002" position="float"/>
  </fig>
  <fig position="float" id="F3">
    <label>Fig. 3.</label>
    <caption>
      <p id="P61">SynthStrip training framework. At every optimization step, we sample a
randomly transformed brain segmentation <italic toggle="yes">s<sub>t</sub></italic>, from
which we synthesize a gray-scale image <italic toggle="yes">x</italic> with arbitrary
contrast. The skull-stripping 3D U-Net receives <italic toggle="yes">x</italic> as input and
predicts a thresholded signed distance transform (SDT) <italic toggle="yes">d</italic>
representing the distance of each voxel to the skull boundary. The U-Net
consists of skip-connected, multi-resolution convolutional layers illustrated by
gray bars, with their number of output filters indicated below. We train
SynthStrip in a supervised fashion, maximizing the similarity between
<italic toggle="yes">d</italic> and the ground-truth SDT <inline-formula><mml:math id="M33" display="inline"><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> within a ribbon of set distance around the
brain and derived directly from the segmentation labels of
<italic toggle="yes">s</italic><sub><italic toggle="yes">t</italic></sub>.</p>
    </caption>
    <graphic xlink:href="nihms-1833287-f0003" position="float"/>
  </fig>
  <fig position="float" id="F4">
    <label>Fig. 4.</label>
    <caption>
      <p id="P62">SynthStrip accuracy compared to baseline methods, across all images in
the test set. Images are sorted by the score of the top performing
skull-stripping method. Each dot represents a single brain mask derived with a
particular tool, and each column of dots represents the scores obtained for a
single image across tools. See <xref rid="SD1" ref-type="supplementary-material">Supplementary Fig. S2</xref> for a version showing each baseline in a
different color.</p>
    </caption>
    <graphic xlink:href="nihms-1833287-f0004" position="float"/>
  </fig>
  <fig position="float" id="F5">
    <label>Fig. 5.</label>
    <caption>
      <p id="P63">SynthStrip and baseline skull-stripping performance for near-isotropic,
T1w adult MR brain images. Median scores are represented by black dots. For all
metrics except sensitivity and specificity, SynthStrip yields optimal brain
masks. The high specificity achieved by ROBEX and BEaST comes at the cost of
substantial under-segmentation of the brain mask, as indicated by their low
sensitivity scores. The inverse is true for FSW, which tends to substantially
over-segment the brain. Black dots indicate median scores.</p>
    </caption>
    <graphic xlink:href="nihms-1833287-f0005" position="float"/>
  </fig>
  <fig position="float" id="F6">
    <label>Fig. 6.</label>
    <caption>
      <p id="P64">Considering all non-T1w, thick-slice, and infant images in the
evaluation set, SynthStrip surpasses baseline accuracy by a wide margin. In this
figure, we include only baselines that generalize to acquisition protocols and
modalities beyond the common structural T1w MRI scans. Black dots indicate
median scores.</p>
    </caption>
    <graphic xlink:href="nihms-1833287-f0006" position="float"/>
  </fig>
  <fig position="float" id="F7">
    <label>Fig. 7.</label>
    <caption>
      <p id="P65"><bold>A:</bold> SynthStrip variability across time-series data, measured
by percent of discordant voxel locations (DV) across diffusion-encoded
directions, relative to the brain mask volume. The ROBEX median % DV extends
beyond the chart axis, as indicated by the black arrow. <bold>B:</bold> Effect
of SDT- and Dice-based loss functions during training. A SynthStrip model
trained using <inline-formula><mml:math id="M34" display="inline"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> predicts substantially smoother brain masks
(boundaries indicated in orange) than a model trained with
<inline-formula><mml:math id="M35" display="inline"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, resulting in considerably lower maximum
surface distance (MD) to ground truth masks and percent of exposed boundary
voxels (EBV).</p>
    </caption>
    <graphic xlink:href="nihms-1833287-f0007" position="float"/>
  </fig>
  <fig position="float" id="F8">
    <label>Fig. 8.</label>
    <caption>
      <p id="P66">Representative skull-stripping errors for SynthStrip and baseline
methods. White arrows indicate over-labeling of the brain mask, while orange
arrows indicate removal of brain matter. SynthStrip errors are uncommon and
typically involve including small regions of dura or other extracerebral tissue
in the brain mask, if they occur..</p>
    </caption>
    <graphic xlink:href="nihms-1833287-f0008" position="float"/>
  </fig>
  <table-wrap position="float" id="T1">
    <label>Table 1</label>
    <caption>
      <p id="P67">Uniform hyperparameter sampling ranges used for synthesizing a training
image from a source segmentation map. The specific values were chosen by visual
inspection of the generated images to produce a landscape of image contrasts,
anatomies, and acquisition characteristics that far exceed the realistic range
of medical images. We sample fields with isotropic voxels of the indicated side
length, where SD abbreviates standard deviation.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Synthesis hyperparameter</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Uniform sampling range</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Affine translation</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0–50 <italic toggle="yes">mm</italic></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Affine rotation</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0–45°</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Affine scaling</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">80–120%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Deformation voxel length</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">8–16 <italic toggle="yes">mm</italic></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Deformation SD</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0–3 <italic toggle="yes">mm</italic></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Label intensity mean</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0–1</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Label intensity SD</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0–0.1</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Bias field voxel length</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4–64 <italic toggle="yes">mm</italic></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Bias field SD</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0–0.5</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Exponentiation parameter
<italic toggle="yes">γ</italic></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">−0.25–0.25</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">FOV cropping (any axis)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0–50 <italic toggle="yes">mm</italic></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Down-sample factor <italic toggle="yes">r</italic> (any
axis)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1–5</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T2">
    <label>Table 2</label>
    <caption>
      <p id="P68">We employ a diverse set of acquired evaluation data, spanning across
imaging modalities, MRI contrasts, and resolution (res.), where 2D indicates
stacks of slice-wise acquisitions. Each individual dataset is divided into a
small validation (val.) and a larger test set. For further details see <xref rid="S14" ref-type="sec">Section 3.4.2</xref>.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Dataset</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Modality</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Res. (<italic toggle="yes">mm</italic><sup>3</sup>)</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Val.</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Test</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">IXI</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">T1w MRI</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.9×0.9×1.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">48</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">T2w MRI</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.9×0.9×1.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">48</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">PDw MRI</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.9×0.9×1.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">48</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">MRA</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.5×0.5×0.8</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">48</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">DWI</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.8×1.8×2.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">32</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">FSM</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">T1w MPRAGE</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.0×1.0×1.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">38</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">T2w 3D-SPACE</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.0×1.0×1.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">34</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">PDw 3D-FLASH</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.0×1.0×1.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">30</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">qT1 MP2RAGE</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.0×1.0×1.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">30</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">ASL</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">T1w MPRAGE</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.0×1.0×1.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">41</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">PCASL 2D-EPI</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.4×3.4×5.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">41</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">QIN</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">T1w 2D-FLASH</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.4×0.4×6.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">52</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">T2-FLAIR 2D-FSE</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.4×0.4×6.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">15</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">T2w 2D-FSE</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.0×1.0×5.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">37</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Infant</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">T1w MPRAGE</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.0×1.0×1.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">16</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">CIM</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">FDG PET</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.0×2.0×2.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">20</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">CT</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.6×0.6×1.5</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">20</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T3" orientation="landscape">
    <label>Table 3</label>
    <caption>
      <p id="P69">SynthStrip and baseline method accuracy across datasets, as measured by
the mean surface distance (± SD) between computed and ground-truth binary
brain masks. <italic toggle="yes">p</italic>-values comparing baseline with SythStrip results
are presented below each score. SynthStrip stands out as a dominant
skull-stripping technique, significantly outperforming baselines for nearly
every dataset with the exception of those with <italic toggle="yes">p</italic>-values in
bold, for which <italic toggle="yes">p</italic> &gt; 0.05. FSW fails entirely for
multiple subsets of non-T1w images (metrics not shown).</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th colspan="8" align="left" valign="middle" rowspan="1">Mean surface distance
(<italic toggle="yes">mm</italic>)<hr/></th>
        </tr>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1"/>
          <th align="left" valign="middle" rowspan="1" colspan="1">SynthStrip</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">ROBEX</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">BET</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">3DSS</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">BEaST</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">FSW</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">DMBE</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">IXI T1w</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.0 ± 0.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.2 ± 0.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.4 ± 1.5</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">8.9 ± 6.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.3 ± 0.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.9 ± 2.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.6 ± 3.5</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.3 × 10<sup>−6</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>1.1 ×
10<sup>−1</sup></bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">8.6 × 10<sup>−12</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">7.3 × 10<sup>−33</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.2 × 10<sup>−7</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.4 × 10<sup>−6</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">FSM T1w</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.7 ± 0.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.4 ± 0.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">18.8 ± 6.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.0 ± 1.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.8 ± 0.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.5 ± 2.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">8.2 ± 5.0</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.7 × 10<sup>−17</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.2 × 10<sup>−18</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.7 × 10<sup>−12</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">7.2 × 10<sup>−36</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6.8 × 10<sup>−6</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.6 × 10<sup>−11</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">ASL T1w</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.9 ± 0.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.0 ± 0.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.7 ± 0.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.4 ± 1.5</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.9 ± 2.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.6 ± 0.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.3 ± 2.3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.1 × 10<sup>−2</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6.0 × 10<sup>−16</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.4 × 10<sup>−14</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">9.5 × 10<sup>−12</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.6 × 10<sup>−11</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.0 × 10<sup>−12</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">QIN T1w</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.1 ± 0.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.8 ± 2.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.3 ± 1.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">11.7 ± 5.6</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.8 ± 7.8</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">9.8 ± 8.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.7 ± 2.2</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">9.5 × 10<sup>−7</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.7 × 10<sup>−11</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.3 × 10<sup>−19</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.9 × 10<sup>−5</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.7 × 10<sup>−10</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.0 × 10<sup>−12</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">IXI T2w</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.2 ± 0.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.4 ± 1.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.2 ± 2.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">9.6 ± 5.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">20.5 ± 12.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">-</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">57.2 ± 19.7</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.8 × 10<sup>−17</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.6 × 10<sup>−7</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.0 × 10<sup>−15</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.8 × 10<sup>−14</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">-</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.8 × 10<sup>−25</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">FSM T2w</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.8 ± 0.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.6 ± 0.8</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.6 ± 0.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.9 ± 0.9</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">14.7 ± 10.6</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">-</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">72.4 ± 24.2</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.7 × 10<sup>−15</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.7 × 10<sup>−8</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">8.5 × 10<sup>−9</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.6 × 10<sup>−9</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">-</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6.4 × 10<sup>−19</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">QIN T2w</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.6 ± 0.8</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.6 ± 2.5</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.9 ± 2.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">14.9 ± 9.6</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">16.8 ± 9.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">-</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">11.8 ± 4.8</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.9 × 10<sup>−9</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">9.1 × 10<sup>−8</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.9 × 10<sup>−9</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">7.2 × 10<sup>−12</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">-</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.8 × 10<sup>−16</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">QIN FLAIR</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.0 ± 0.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.1 ± 0.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.2 ± 0.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">9.9 ± 5.9</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.4 ± 1.9</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">7.9 ± 3.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.5 ± 1.2</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.1 × 10<sup>−5</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.3 × 10<sup>−3</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.6 × 10<sup>−5</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.2 × 10<sup>−4</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.4 × 10<sup>−7</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.5 × 10<sup>−9</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">IXI PDw</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.2 ± 0.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.9 ± 0.6</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.6 ± 0.6</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">9.8 ± 5.5</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">11.2 ± 11.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">8.5 ± 5.8</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.1 ± 2.4</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.9 × 10<sup>−15</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.0 × 10<sup>−7</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.4 × 10<sup>−14</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.2 × 10<sup>−7</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.7 × 10<sup>−11</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.4 × 10<sup>−15</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">FSM PDw</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.0 ± 0.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.5 ± 0.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.8 ± 3.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.6 ± 0.6</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.3 ± 3.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">20.0 ± 7.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">17.9 ± 3.5</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">7.3 × 10<sup>−6</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>2.3 ×
10<sup>−1</sup></bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">8.9 × 10<sup>−7</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.5 × 10<sup>−6</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.2 × 10<sup>−15</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.6 × 10<sup>−23</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">IXI MRA</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.3 ± 0.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">10.7 ± 2.8</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">16.2 ± 6.5</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">9.1 ± 4.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.2 ± 2.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">-</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">34.8 ± 17.1</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.0 × 10<sup>−28</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">7.8 × 10<sup>−21</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.3 × 10<sup>−18</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">9.8 × 10<sup>−11</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">-</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.4 × 10<sup>−18</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">FSM qT1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.8 ± 0.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">21.8 ± 14.8</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">33.0 ± 10.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">33.2 ± 3.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">31.9 ± 23.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">26.0 ± 11.9</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">54.0 ± 17.7</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.2 × 10<sup>−8</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.2 × 10<sup>−17</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.3 × 10<sup>−32</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.4 × 10<sup>−8</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.4 × 10<sup>−12</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.2 × 10<sup>−17</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">ASL EPI</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.6 ± 0.6</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.8 ± 2.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.0 ± 0.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">9.6 ± 4.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">14.8 ± 11.8</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">16.1 ± 5.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.6 ± 0.6</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.6 × 10<sup>−10</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">9.9 × 10<sup>−5</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">9.8 × 10<sup>−16</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">9.8 × 10<sup>−9</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.0 × 10<sup>−20</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.5 × 10<sup>−15</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Infant T1w</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.0 ± 0.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.1 ± 6.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">14.3 ± 10.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">22.2 ± 12.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">19.0 ± 21.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">17.6 ± 15.5</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6.6 ± 3.4</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>5.9 ×
10<sup>−2</sup></bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.1 × 10<sup>−4</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.3 × 10<sup>−6</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.5 × 10<sup>−3</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.3 × 10<sup>−3</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.6 × 10<sup>−5</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">IXI DWI</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.6 ± 0.6</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6.2 ± 3.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.4 ± 0.9</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">8.5 ± 2.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">11.1 ± 9.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">11.2 ± 4.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6.8 ± 1.2</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.5 × 10<sup>−9</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.7 × 10<sup>−11</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">9.8 × 10<sup>−15</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.5 × 10<sup>−6</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.3 × 10<sup>−12</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.1 × 10<sup>−27</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">CIM PET</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.5 ± 0.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.9 ± 2.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">9.3 ± 4.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.2 ± 2.9</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">69.0 ± 21.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">16.2 ± 3.5</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">17.6 ± 6.4</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">7.7 × 10<sup>−5</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">8.0 × 10<sup>−8</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>2.4 ×
10<sup>−1</sup></bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.6 × 10<sup>−10</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.8 × 10<sup>−13</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.0 × 10<sup>−9</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">CIM CT</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.0 ± 0.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">11.4 ± 1.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">34.1 ± 3.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">20.6 ± 2.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">74.8 ± 18.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">29.1 ± 5.9</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">34.7 ± 8.2</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.1 × 10<sup>−20</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.6 × 10<sup>−19</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.1 × 10<sup>−20</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">7.7 × 10<sup>−12</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.9 × 10<sup>−12</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.5 × 10<sup>−13</sup></td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T4" orientation="landscape">
    <label>Table 4</label>
    <caption>
      <p id="P70">Skull-stripping accuracy across datasets, as measured by the mean Dice
overlap (± SD) between computed and ground-truth binary brain masks.
<italic toggle="yes">p</italic>-values comparing baseline with SythStrip results are
presented below each score. Across each dataset, SynthStrip significantly
outperforms most baselines except those with <italic toggle="yes">p</italic>-values in bold,
for which <italic toggle="yes">p</italic> &gt; 0.05.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th colspan="8" align="left" valign="middle" rowspan="1">Dice (%)<hr/></th>
        </tr>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1"/>
          <th align="left" valign="middle" rowspan="1" colspan="1">SynthStrip</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">ROBEX</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">BET</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">3DSS</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">BEaST</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">FSW</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">DMBE</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">IXI T1w</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">97.0 ± 0.5</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">96.2 ± 0.8</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">96.1 ± 3.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">95.4 ± 1.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">93.4 ± 0.8</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">92.6 ± 4.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">93.7 ± 3.0</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.5 × 10<sup>−8</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>5.1 ×
10<sup>−2</sup></bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.3 × 10<sup>−9</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.1 × 10<sup>−32</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.8 × 10<sup>−9</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">8.0 × 10<sup>−10</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">FSM T1w</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">97.8 ± 0.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">95.9 ± 0.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">65.8 ± 11.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">92.0 ± 3.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">92.1 ± 0.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">93.8 ± 3.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">89.5 ± 3.3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.3 × 10<sup>−18</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.7 × 10<sup>−19</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.4 × 10<sup>−13</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.9 × 10<sup>−35</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.2 × 10<sup>−7</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.6 × 10<sup>−18</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">ASL T1w</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">97.3 ± 0.5</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">96.8 ± 1.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">94.9 ± 1.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">90.8 ± 3.5</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">89.0 ± 5.5</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">95.5 ± 0.9</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">92.7 ± 1.7</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.0 × 10<sup>−3</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.7 × 10<sup>−17</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.5 × 10<sup>−15</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">7.7 × 10<sup>−13</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.1 × 10<sup>−11</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.1 × 10<sup>−19</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">QIN T1w</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">96.3 ± 0.8</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">92.8 ± 4.6</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">93.8 ± 2.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">92.4 ± 3.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">85.2 ± 17.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">79.9 ± 13.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">89.0 ± 7.3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.7 × 10<sup>−7</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.2 × 10<sup>−11</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.3 × 10<sup>−15</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.5 × 10<sup>−5</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.2 × 10<sup>−12</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">7.4 × 10<sup>−10</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">IXI T2w</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">96.4 ± 0.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">91.3 ± 2.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">91.0 ± 6.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">94.9 ± 1.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">63.0 ± 12.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">-</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6.7 ± 8.0</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.9 × 10<sup>−19</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.0 × 10<sup>−7</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.4 × 10<sup>−9</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">7.8 × 10<sup>−24</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">-</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.1 × 10<sup>−52</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">FSM T2w</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">97.7 ± 0.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">93.2 ± 1.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">95.7 ± 1.6</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">94.9 ± 2.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">69.8 ± 15.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">-</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.4 ± 7.2</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.9 × 10<sup>−18</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">9.8 × 10<sup>−9</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.1 × 10<sup>−10</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">7.3 × 10<sup>−13</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">-</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.1 × 10<sup>−40</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">QIN T2w</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">95.2 ± 1.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">87.3 ± 5.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">89.6 ± 4.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">71.4 ± 21.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">57.7 ± 19.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">-</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">61.5 ± 16.5</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.1 × 10<sup>−10</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.7 × 10<sup>−9</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6.3 × 10<sup>−7</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">9.1 × 10<sup>−14</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">-</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.9 × 10<sup>−15</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">QIN FLAIR</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">96.4 ± 0.5</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">93.7 ± 1.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">95.9 ± 0.9</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">93.8 ± 1.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">90.3 ± 4.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">83.4 ± 6.2</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">87.9 ± 3.0</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.7 × 10<sup>−7</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.3 × 10<sup>−2</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.2 × 10<sup>−9</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">8.7 × 10<sup>−5</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.6 × 10<sup>−7</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.5 × 10<sup>−9</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">IXI PDw</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">96.4 ± 1.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">94.6 ± 1.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">95.5 ± 1.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">95.1 ± 1.6</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">78.3 ± 15.8</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">81.3 ± 10.9</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">90.0 ± 3.8</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.3 × 10<sup>−17</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.9 × 10<sup>−7</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.4 × 10<sup>−8</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.8 × 10<sup>−10</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.5 × 10<sup>−12</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.8 × 10<sup>−16</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">FSM PDw</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">97.2 ± 0.5</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">95.8 ± 1.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">95.7 ± 6.5</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">95.5 ± 1.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">88.8 ± 8.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">67.4 ± 9.8</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">79.1 ± 4.4</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.1 × 10<sup>−6</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>2.3 ×
10<sup>−1</sup></bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.7 × 10<sup>−8</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.1 × 10<sup>−6</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.9 × 10<sup>−17</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.4 × 10<sup>−21</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">IXI MRA</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">97.7 ± 0.5</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">82.0 ± 4.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">62.3 ± 14.9</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">95.1 ± 1.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">91.5 ± 5.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">-</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">18.1 ± 19.5</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.5 × 10<sup>−30</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.3 × 10<sup>−21</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.0 × 10<sup>−24</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6.7 × 10<sup>−10</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">-</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.7 × 10<sup>−32</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">FSM qT1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">97.7 ± 0.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">63.1 ± 19.6</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">47.2 ± 13.5</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">44.6 ± 3.8</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">36.3 ± 15.6</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">49.5 ± 10.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.1 ± 7.1</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.1 × 10<sup>−10</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">8.8 × 10<sup>−20</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.9 × 10<sup>−36</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">9.4 × 10<sup>−20</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.7 × 10<sup>−21</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.6 × 10<sup>−36</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">ASL EPI</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">95.2 ± 1.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">88.3 ± 4.6</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">94.2 ± 1.8</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">95.2 ± 1.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">67.4 ± 19.6</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">69.4 ± 11.9</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">92.8 ± 1.5</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.6 × 10<sup>−11</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5.2 × 10<sup>−4</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>5.8 ×
10<sup>−1</sup></bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.0 × 10<sup>−11</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">8.9 × 10<sup>−17</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.1 × 10<sup>−15</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Infant T1w</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">96.1 ± 1.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">87.4 ± 15.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">66.8 ± 20.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">71.6 ± 17.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">63.2 ± 38.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">61.3 ± 31.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">84.4 ± 7.3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.5 × 10<sup>−2</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.0 × 10<sup>−5</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.4 × 10<sup>−5</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.9 × 10<sup>−3</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">7.9 × 10<sup>−4</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.9 × 10<sup>−6</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">IXI DWI</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">95.5 ± 1.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">85.3 ± 5.7</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">93.4 ± 2.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">90.4 ± 2.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">79.5 ± 11.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">75.1 ± 8.8</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">80.5 ± 3.1</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.4 × 10<sup>−10</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.2 × 10<sup>−11</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.6 × 10<sup>−17</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6.8 × 10<sup>−9</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.5 × 10<sup>−13</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.4 × 10<sup>−27</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">CIM PET</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">95.4 ± 1.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">89.5 ± 4.8</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">78.3 ± 8.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">93.7 ± 6.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.6 ± 8.8</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">51.6 ± 10.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">42.3 ± 14.3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.8 × 10<sup>−5</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.9 × 10<sup>−8</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>2.1 ×
10<sup>−1</sup></bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.4 × 10<sup>−18</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.1 × 10<sup>−13</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.6 × 10<sup>−12</sup></td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">CIM CT</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">94.3 ± 0.9</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">73.7 ± 2.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">45.4 ± 4.0</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">60.4 ± 2.9</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.3 ± 3.3</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">48.8 ± 5.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">58.2 ± 6.5</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.8 × 10<sup>−23</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.8 × 10<sup>−22</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4.0 × 10<sup>−22</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3.5 × 10<sup>−26</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.4 × 10<sup>−16</sup></td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.1 × 10<sup>−15</sup></td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T5" orientation="landscape">
    <label>Table 5</label>
    <caption>
      <p id="P71">Average single-threaded CPU runtime (± SD) for T1w images from
the FSM dataset. In addition to BET and FSW, SynthStrip is one of only three
skull-stripping methods that consistently runs in under one minute.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th colspan="6" align="left" valign="middle" rowspan="1">CPU runtime (minutes)<hr/></th>
          <th align="left" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">SynthStrip</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">ROBEX</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">BET</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">3DSS</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">BEaST</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">FSW</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">DMBE</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">0.48 ± 0.01</td>
          <td align="left" valign="top" rowspan="1" colspan="1">2.45 ± 0.11</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.20 ± 0.14</td>
          <td align="left" valign="top" rowspan="1" colspan="1">2.27 ± 1.12</td>
          <td align="left" valign="top" rowspan="1" colspan="1">4.14 ± 0.24</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.19 ± 0.01</td>
          <td align="left" valign="top" rowspan="1" colspan="1">48.89 ± 4.72</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
