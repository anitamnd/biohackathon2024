<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7355289</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btaa430</article-id>
    <article-id pub-id-type="publisher-id">btaa430</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Systems Biology and Networks</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>PEDL: extracting protein–protein associations using deep language models and distant supervision</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Weber</surname>
          <given-names>Leon</given-names>
        </name>
        <xref ref-type="aff" rid="btaa430-aff1">b1</xref>
        <xref ref-type="aff" rid="btaa430-aff2">b2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Thobe</surname>
          <given-names>Kirsten</given-names>
        </name>
        <xref ref-type="aff" rid="btaa430-aff2">b2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Migueles Lozano</surname>
          <given-names>Oscar Arturo</given-names>
        </name>
        <xref ref-type="aff" rid="btaa430-aff2">b2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wolf</surname>
          <given-names>Jana</given-names>
        </name>
        <xref ref-type="aff" rid="btaa430-aff2">b2</xref>
        <xref ref-type="corresp" rid="btaa430-cor1"/>
        <!--<email>jana.wolf@mdc-berlin.de</email>-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Leser</surname>
          <given-names>Ulf</given-names>
        </name>
        <xref ref-type="aff" rid="btaa430-aff1">b1</xref>
        <xref ref-type="corresp" rid="btaa430-cor1"/>
        <!--<email>leser@informatik.hu-berlin.de</email>-->
      </contrib>
    </contrib-group>
    <aff id="btaa430-aff1"><label>b1</label><institution>Computer Science Department, Humboldt-Universität zu Berlin</institution>, Berlin 10099, <country country="DE">Germany</country></aff>
    <aff id="btaa430-aff2"><label>b2</label><institution>Group Mathematical Modelling of Cellular Processes, Max Delbrück Center for Molecular Medicine in the Helmholtz Association</institution>, Berlin 13125, <country country="DE">Germany</country></aff>
    <author-notes>
      <corresp id="btaa430-cor1">To whom correspondence should be addressed. E-mail: <email>leser@informatik.hu-berlin.de</email> or <email>jana.wolf@mdc-berlin.de</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2020-07-13">
      <day>13</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>36</volume>
    <issue>Suppl 1</issue>
    <issue-title>ISMB 2020 Proceedings</issue-title>
    <fpage>i490</fpage>
    <lpage>i498</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btaa430.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>A significant portion of molecular biology investigates signalling pathways and thus depends on an up-to-date and complete resource of functional protein–protein associations (PPAs) that constitute such pathways. Despite extensive curation efforts, major pathway databases are still notoriously incomplete. Relation extraction can help to gather such pathway information from biomedical publications. Current methods for extracting PPAs typically rely exclusively on rare manually labelled data which severely limits their performance.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We propose <bold>P</bold>PA <bold>E</bold>xtraction with <bold>D</bold>eep <bold>L</bold>anguage (PEDL), a method for predicting PPAs from text that combines deep language models and distant supervision. Due to the reliance on distant supervision, PEDL has access to an order of magnitude more training data than methods solely relying on manually labelled annotations. We introduce three different datasets for PPA prediction and evaluate PEDL for the two subtasks of predicting PPAs between two proteins, as well as identifying the text spans stating the PPA. We compared PEDL with a recently published state-of-the-art model and found that on average PEDL performs better in both tasks on all three datasets. An expert evaluation demonstrates that PEDL can be used to predict PPAs that are missing from major pathway databases and that it correctly identifies the text spans supporting the PPA.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>PEDL is freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/leonweber/pedl">https://github.com/leonweber/pedl</ext-link>. The repository also includes scripts to generate the used datasets and to reproduce the experiments from this article.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>HEIBRIDS</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>German Federal Ministry of Education and Research</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>BMBF</institution>
            <institution-id institution-id-type="DOI">10.13039/501100002347</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>031L0189D</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="9"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Molecular biology explores chemical and physical interactions between key intermediates, mostly proteins, in cells. The biological function rarely depends on single interactions but on the complex interplay of many, for example in cellular signalling, metabolism or gene regulation. Techniques from network analysis are widely used to connect interactions between proteins to the functional organization of cells (<xref rid="btaa430-B3" ref-type="bibr">Barabasi and Oltvai, 2004</xref>). A major challenge for building these networks is to gather all relevant information from the literature, since the quality of the model and the model predictions rely on completeness and correctness of the individual proteins and their interactions. It is important to not only have the knowledge that two proteins interact but also to know the exact type of interaction, such as kinase–substrate relation or gene–gene regulation. These functional protein–protein associations (PPAs) (<xref rid="btaa430-B19" ref-type="bibr">Junge and Jensen, 2019</xref>) can be found in manually curated databases such as Reactome (<xref rid="btaa430-B18" ref-type="bibr">Jassal <italic>et al.</italic>, 2019</xref>) or the Protein Interaction Database (PID) (<xref rid="btaa430-B49" ref-type="bibr">Schaefer <italic>et al.</italic>, 2009</xref>). However, these databases are notoriously incomplete despite extensive curation efforts (<xref rid="btaa430-B25" ref-type="bibr">Köksal <italic>et al.</italic>, 2018</xref>). For instance, we found that for a state-of-the-art model of p53 signalling (<xref rid="btaa430-B16" ref-type="bibr">Hat <italic>et al.</italic>, 2016</xref>) 25% of the contained PPAs cannot be found, neither in Reactome nor in PID (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref> S1 for details). Extracting PPAs from the biomedical literature has been a long-standing research goal. Early approaches focused on matching sentences to manually defined templates, usually leading to high-precision but low-recall results (<xref rid="btaa430-B14" ref-type="bibr">Friedman <italic>et al.</italic>, 2001</xref>). Later methods used supervised machine learning to classify whether a sentence expresses a relation between a given pair of proteins, frequently relying on support-vector-machines (SVMs) with graph kernels (<xref rid="btaa430-B31" ref-type="bibr">Miwa <italic>et al.</italic>, 2009</xref>; <xref rid="btaa430-B52" ref-type="bibr">Tikk <italic>et al.</italic>, 2012</xref>). Similar techniques have been applied to biomedical event extraction, which aimed at not only extracting pairwise relations between two proteins but also complex biochemical reactions between proteins (<xref rid="btaa430-B9" ref-type="bibr">Björne <italic>et al.</italic>, 2009</xref>; <xref rid="btaa430-B32" ref-type="bibr">Miwa <italic>et al.</italic>, 2010</xref>). More recently, also approaches based on neural networks have been applied to sentence-wise supervised classification of protein–protein interactions (<xref rid="btaa430-B38" ref-type="bibr">Peng and Lu, 2017</xref>) and to biomedical event extraction (<xref rid="btaa430-B8" ref-type="bibr">Björne and Salakoski, 2018</xref>). None of these methods are capable of detecting relations between proteins mentioned in different sentences or make use of pre-trained language models that recently have led to large gains in other Natural Language Processing (NLP) tasks (<xref rid="btaa430-B13" ref-type="bibr">Devlin <italic>et al.</italic>, 2019</xref>). Additionally, these models rely on manually annotated training data, which for PPA-extraction requires expert knowledge and thus is very costly. Consequently, the available manually labelled PPA datasets are rather small, typically containing at most a few thousand sentences (<xref rid="btaa430-B43" ref-type="bibr">Pyysalo <italic>et al.</italic>, 2008</xref>).</p>
    <p>This data sparsity led to the introduction of distantly supervised approaches (<xref rid="btaa430-B30" ref-type="bibr">Mintz <italic>et al.</italic>, 2009</xref>) for PPA prediction (<xref rid="btaa430-B19" ref-type="bibr">Junge and Jensen, 2019</xref>; <xref rid="btaa430-B41" ref-type="bibr">Poon <italic>et al.</italic>, 2014</xref>; <xref rid="btaa430-B51" ref-type="bibr">Thomas <italic>et al.</italic>, 2011</xref>). However, both <xref rid="btaa430-B51" ref-type="bibr">Thomas <italic>et al.</italic> (2011)</xref> and <xref rid="btaa430-B41" ref-type="bibr">Poon <italic>et al.</italic> (2014)</xref> are based on non-neural models with manually defined features and <xref rid="btaa430-B19" ref-type="bibr">Junge and Jensen (2019)</xref> use averaged word embeddings without leveraging multi-instance learning. Distantly supervised relation extraction methods generate noisy training data by aligning a knowledge base to a large collection of texts. To achieve this, a large knowledge base of relations (in our case PPAs) in the form <inline-formula id="IE1"><mml:math id="IM1"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is connected to a text by first linking the entities from the knowledge base <italic>e</italic><sub>1</sub>, <italic>e</italic><sub>2</sub> to the entities in the text. Initially, the core assumption of distant supervision was that every sentence that contains the entities <italic>e</italic><sub>1</sub>, <italic>e</italic><sub>2</sub> expresses the relation <italic>r</italic>. This assumption can be relaxed through the use of multi-instance learning (<xref rid="btaa430-B17" ref-type="bibr">Hoffmann <italic>et al.</italic>, 2011</xref>; <xref rid="btaa430-B47" ref-type="bibr">Riedel <italic>et al.</italic>, 2010</xref>; <xref rid="btaa430-B50" ref-type="bibr">Surdeanu <italic>et al.</italic>, 2012</xref>). Multi-instance learning explicitly models the assumption that <italic>at least one</italic> sentence expresses the relation between the entity pair in question by selecting only a subset of the sentences to generate the prediction. Originally, probabilistic graphical models were used to achieve this, but recently deep learning-based models in the form of piece-wise convolutional neural networks (<xref rid="btaa430-B61" ref-type="bibr">Zeng <italic>et al.</italic>, 2015</xref>) with selective attention (<xref rid="btaa430-B28" ref-type="bibr">Lin <italic>et al.</italic>, 2016</xref>) were successfully applied. An orthogonal line of work also uses auxiliary directly supervised training examples, achieving significant improvements for graphical models (<xref rid="btaa430-B1" ref-type="bibr">Angeli <italic>et al.</italic>, 2014</xref>; <xref rid="btaa430-B39" ref-type="bibr">Pershina <italic>et al.</italic>, 2014</xref>) and for neural networks (<xref rid="btaa430-B4" ref-type="bibr">Beltagy <italic>et al.</italic>, 2019a</xref>; <xref rid="btaa430-B29" ref-type="bibr">Liu <italic>et al.</italic>, 2017</xref>). However, all of these approaches only consider entity pairs that occur together in the same sentence, which severely limits recall (<xref rid="btaa430-B46" ref-type="bibr">Quirk and Poon, 2017</xref>).</p>
    <p>Accordingly, there is growing interest in using text that spans multiple sentences for distantly supervised biomedical relation extraction. <xref rid="btaa430-B56" ref-type="bibr">Verga <italic>et al.</italic> (2018)</xref> used transformer-based models to predict all relations between chemicals, diseases and genes contained in one abstract but do not consider multiple abstracts simultaneously. <xref rid="btaa430-B46" ref-type="bibr">Quirk and Poon (2017)</xref> used multi-instance learning to predict relations between drugs and genes that can be up to three sentences apart with an SVM-classifier on manually defined dependency graph features.</p>
    <p>Recently, deep language models have seen widespread success in NLP, including the biomedical domain (<xref rid="btaa430-B5" ref-type="bibr">Beltagy <italic>et al.</italic>, 2019b</xref>). The often used two-step process of training these models can be regarded as a type of transfer learning (<xref rid="btaa430-B42" ref-type="bibr">Pratt <italic>et al.</italic>, 1991</xref>): The first step is pre-training, in which a large model, typically with hundreds of million parameters, is trained on a huge corpus of texts with a language modelling task. In the second step, the pre-trained model is applied to the target task, either by fine-tuning the model parameters or using the model to generate contextualized embeddings (<xref rid="btaa430-B40" ref-type="bibr">Peters <italic>et al.</italic>, 2019</xref>). BERT (<xref rid="btaa430-B13" ref-type="bibr">Devlin <italic>et al.</italic>, 2019</xref>) is a highly successful deep language model based on the transformer architecture (<xref rid="btaa430-B55" ref-type="bibr">Vaswani <italic>et al.</italic>, 2017</xref>) which allows to train very large models efficiently by leveraging GPUs. Originally, BERT was trained on a large collection of books and English Wikipedia, but recently two BERT models trained on biomedical abstracts and full texts have been released, BioBERT (<xref rid="btaa430-B27" ref-type="bibr">Lee <italic>et al.</italic>, 2019</xref>) and SciBERT (<xref rid="btaa430-B5" ref-type="bibr">Beltagy <italic>et al.</italic>, 2019b</xref>). As BERT uses WordPiece tokenization (<xref rid="btaa430-B59" ref-type="bibr">Wu <italic>et al.</italic>, 2016</xref>), it learns a domain-dependent vocabulary that allows it to use sub-word information to relate similar words such as <italic>TRAF2</italic> and <italic>TRAF3</italic>. <bold>P</bold>PA <bold>E</bold>xtraction with <bold>D</bold>eep <bold>L</bold>anguage (PEDL) uses SciBERT as its pre-trained language model, because unlike BioBERT, its WordPiece vocabulary is optimized for scientific literature.</p>
    <p>In this work, we propose PEDL models, a model that predicts functional PPAs from biomedical publications. We approach this problem by combining pre-trained language models with distant supervision. Specifically, we source a large number of protein pairs together with their PPAs from the PID database and find texts mentioning these pairs in a collection of roughly 24 million abstracts of biomedical publications and 3 million full texts. The resulting PPA extraction dataset is distantly supervised, i.e. it only contains annotations for relations between the proteins but it is not known whether a text span actually confirms the relation. Given a protein pair, PEDL takes the text spans mentioning both proteins as input and predicts which PPAs hold for this pair, if any. Importantly, in what we call evidence prediction, PEDL predicts not only the PPAs but also which text span expresses it. We augment the training data of PEDL with data which additionally contains annotations for evidence predictions, which we generate from those gold standard datasets (<xref rid="btaa430-B21" ref-type="bibr">Kim <italic>et al.</italic>, 2011b</xref>) that include annotations for all PPA-types considered by us. Following <xref rid="btaa430-B5" ref-type="bibr">Beltagy <italic>et al.</italic> (2019b</xref>), we call this type of data <italic>directly supervised</italic>. We compare the performance of PEDL to state-of-the-art approaches on three different datasets and find that, on average, it performs much better for both PPA and evidence prediction. In a manual evaluation of the top 10 predicted PPAs, conducted by three experts in Systems Biology, we find that PEDL can be used to predict PPAs that cannot be found in major pathway databases. Furthermore, the predicted evidence text spans actually express the relation and thus can be used for easy verification of the predicted PPAs, which is important for expert curation.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <p>In this work, we model PPA extraction following a multi-instance learning framework for relation extraction (<xref rid="btaa430-B17" ref-type="bibr">Hoffmann <italic>et al.</italic>, 2011</xref>; <xref rid="btaa430-B47" ref-type="bibr">Riedel <italic>et al.</italic>, 2010</xref>; <xref rid="btaa430-B50" ref-type="bibr">Surdeanu <italic>et al.</italic>, 2012</xref>). Given two proteins <italic>p</italic><sub>1</sub> and <italic>p</italic><sub>2</sub>, we aim to predict all PPAs <inline-formula id="IE2"><mml:math id="IM2"><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> relating <italic>p</italic><sub>1</sub> to <italic>p</italic><sub>2</sub> by leveraging a corpus of biomedical literature. We focus on a set <italic>R</italic> of five PPAs which is a subset of the Simple Interaction Format relations available in Pathway Commons:
</p>
    <list list-type="bullet">
      <list-item>
        <p><italic>in-complex-with</italic> is true for a protein pair (<italic>A</italic>, <italic>B</italic>), if <italic>A</italic> and <italic>B</italic> occur together in at least one protein complex.</p>
      </list-item>
      <list-item>
        <p><italic>controls-state-change-of</italic> means that <italic>A</italic> regulates some change of <italic>B</italic>. This can be a post-translational modification such as phosphorylation or ubiquitination or a transfer between cellular compartments.</p>
      </list-item>
      <list-item>
        <p><italic>controls-phosphorylation-of</italic> is a subset of <italic>controls-state-change-of</italic> and means that <italic>A</italic> phosphorylates <italic>B</italic>.</p>
      </list-item>
      <list-item>
        <p><italic>controls-transport-of</italic> is a subset of <italic>controls-state-change-of</italic> and denotes that <italic>A</italic> controls the transfer of <italic>B</italic> to a cellular compartment.</p>
      </list-item>
      <list-item>
        <p><italic>controls-expression-of</italic> implies that <italic>A</italic> modulates the expression of <italic>B</italic>.</p>
      </list-item>
    </list>
    <p>Additionally, in what we term <italic>evidence prediction</italic>, we want the model to find the strongest possible evidence for these PPAs in the form of text expressing the relation between the proteins. This section describes how PEDL combines deep language models, distant supervision and auxiliary directly supervised data to approach these two tasks. A detailed graphical description of PEDL can be found in <xref ref-type="fig" rid="btaa430-F1">Figure 1</xref>.
</p>
    <fig id="btaa430-F1" orientation="portrait" position="float">
      <label>Fig. 1.</label>
      <caption>
        <p>(<bold>a</bold>) Overview of PEDL for the two tasks of relation prediction and evidence prediction. In this example, the model predicts relations for the protein pair BTC and ErbB4 given three text spans containing both proteins. First, the BERT component produces a score matrix containing a prediction for each text and relation type. The relation predictions are then generated by applying LSE column-wise to approximate the maximum score for a given PPA type across all spans. The evidence predictions are obtained by taking the row-wise maximum, which is the highest score assigned to this text span regardless of PPA type. (<bold>b</bold>) The generation of one row of the score matrix <italic>s</italic>. In each of BERT’s 12 transformer layers, each token receives a 768 dimensional embedding (<italic>u<sub>i</sub></italic> for the first and <italic>z<sub>i</sub></italic> for the last layer). The embedding of the prepended [CLS] token is used to summarize the text span in the single vector <italic>h</italic>, which is then transformed to one row of the score matrix by the output layer (<italic>W</italic>, <italic>b</italic>)</p>
      </caption>
      <graphic xlink:href="btaa430f1"/>
    </fig>
    <sec>
      <title>2.1 PPA prediction as multi-instance learning</title>
      <p>To predict relations between proteins <italic>p</italic><sub>1</sub> and <italic>p</italic><sub>2</sub>, the first step of PEDL is to collect all text spans <italic>T</italic>, up to a given length, mentioning <italic>p</italic><sub>1</sub> and <italic>p</italic><sub>2</sub> together. This requires the use of named entity recognition (NER) (<xref rid="btaa430-B57" ref-type="bibr">Weber <italic>et al.</italic>, 2020</xref>) and named entity normalization (NEN) (<xref rid="btaa430-B58" ref-type="bibr">Wei <italic>et al.</italic>, 2019</xref>) as a pre-processing step.</p>
      <p>For the two sub-tasks of relation prediction and evidence prediction, the model has to produce two vectors <inline-formula id="IE3"><mml:math id="IM3"><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE4"><mml:math id="IM4"><mml:mrow><mml:mi>e</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>T</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where <italic>R</italic> is the set of considered PPAs and <italic>T</italic> is the set of spans for the pair. The vector <italic>r</italic> contains <inline-formula id="IE5"><mml:math id="IM5"><mml:mrow><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula> scores <inline-formula id="IE6"><mml:math id="IM6"><mml:mrow><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> reflecting the confidence of PEDL in each type of PPA. <italic>e</italic> contains <inline-formula id="IE7"><mml:math id="IM7"><mml:mrow><mml:mo>|</mml:mo><mml:mi>T</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula> scores <inline-formula id="IE8"><mml:math id="IM8"><mml:mrow><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, each modelling PEDL’s confidence that the corresponding text span expresses a relation between <italic>p</italic><sub>1</sub> and <italic>p</italic><sub>2</sub>.</p>
      <p>For this, PEDL predicts a score-matrix <inline-formula id="IE9"><mml:math id="IM9"><mml:mrow><mml:mi>S</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>T</mml:mi><mml:mo>|</mml:mo><mml:mo>×</mml:mo><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for each text span, representing the confidence of the model that a text span supports a given PPA. To achieve this, we first mark the entity pair in each text span by surrounding the first entity with the entity markers <italic>&lt;e1&gt;</italic> and <italic>&lt;/e1&gt;</italic> and the second entity with <italic>&lt;e2&gt;</italic>, <italic>&lt;/e2&gt;</italic>. Then, each text span <italic>T<sub>i</sub></italic> is fed through BERT individually, to obtain the <italic>[CLS]</italic> embedding <inline-formula id="IE10"><mml:math id="IM10"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>768</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> of the 768-dimensional final layer, which can be regarded as a summary of the whole text span. Finally, we use a single hidden layer to transform <italic>h<sub>i</sub></italic> to one row of the score matrix <italic>S<sub>i</sub></italic> containing logits reflecting the confidence of PEDL that the text span expresses a given PPA. See <xref ref-type="fig" rid="btaa430-F1">Figure 1</xref> for a graphical description of this process.</p>
      <p>The relation prediction <italic>r</italic> for each PPA type is generated by aggregating the scores for the PPA over all spans, i.e. column-wise. Correspondingly, the evidence prediction <italic>e</italic> for an individual sentence is produced by aggregating the scores of all PPA predictions for this sentence, i.e. row-wise. Finally, both vectors are normalized by applying the sigmoid function. In preliminary experiments, we used maximum for both aggregations, but found that the resulting sparse gradient flow hampered optimization. Thus, we use the smooth approximation of maximum LogSumExp as aggregation function for PPA predictions, because it allows for gradient flow through all sentences and empirically works well in end-to-end training of transformer models (<xref rid="btaa430-B56" ref-type="bibr">Verga <italic>et al.</italic>, 2018</xref>). Putting everything together, the formulae for predicting PPAs and evidence are the following:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mtext>BERT</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mtext>CLS</mml:mtext><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:munder><mml:mrow><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where log and exp denote element-wise application of logarithm and exponentiation, <inline-formula id="IE11"><mml:math id="IM11"><mml:mrow><mml:mi>W</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>768</mml:mn><mml:mo>×</mml:mo><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE12"><mml:math id="IM12"><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are trainable parameters, and <italic>σ</italic> is the element-wise sigmoid function. Alternatively, <italic>S<sub>ij</sub></italic> can be directly used as an evidence score <italic>per relation</italic>.</p>
      <p>For the training of PEDL, we assume that two types of data are available: <italic>Distantly supervised</italic> data which only has labels for relation prediction and <italic>directly supervised</italic> data which has labels for both relation and evidence prediction. Furthermore, we assume that both types of data share the same label space. The <italic>directly supervised</italic> data is used to give the model additional guidance on how text spans expressing PPAs look like. To achieve this, we combine both types of data using a multi-task learning framework. We introduce one loss term each type of data: <inline-formula id="IE13"><mml:math id="IM13"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">distant</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for the distantly supervised and <inline-formula id="IE14"><mml:math id="IM14"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">direct</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for the directly supervised data. The loss for the directly supervised data is composed of a loss term for relation prediction and another term for evidence prediction: <inline-formula id="IE15"><mml:math id="IM15"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">direct</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">direct</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">relation</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">direct</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">evidence</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>. The loss for the distantly supervised data only consists of the loss term for the relation prediction task, because labels for evidence predictions are not available for this type of data: <inline-formula id="IE16"><mml:math id="IM16"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">distant</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">distant</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">relation</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>. The total loss for the batch is then a weighted average of the direct and distant losses:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi mathvariant="italic">total</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>α</mml:mo><mml:mo>·</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi mathvariant="italic">direct</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo>α</mml:mo><mml:mo>)</mml:mo><mml:mo>·</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi mathvariant="italic">distant</mml:mi></mml:msub></mml:math></disp-formula>where <inline-formula id="IE17"><mml:math id="IM17"><mml:mrow><mml:mo>α</mml:mo><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> is a hyperparameter controlling the relative importance of the direct loss and will be tuned on the development set of each considered dataset separately. At each optimization step, we sample a batch from the distant and one from the directly supervised data.</p>
      <p>Since we model PPA prediction as a multi-label task, all losses are computed with binary cross entropy. Note, that the only parameters of PEDL are those of BERT and one output layer (<italic>W</italic>, <italic>b</italic>). We optimize these parameters with Adam (<xref rid="btaa430-B24" ref-type="bibr">Kingma and Ba, 2015</xref>). The detailed hyperparameter settings can be found in <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref> S2. One training step on one batch (16 protein pairs with up to 100 text spans each) takes ∼9.5 s on four <italic>RTX 2080 Ti</italic> GPUs.</p>
    </sec>
    <sec>
      <title>2.2 Data</title>
      <p>The training of PEDL requires <italic>distantly</italic> and <italic>directly supervised</italic> data. To obtain the distantly supervised data, we follow the standard approach for creating a multi-instance learning dataset (<xref rid="btaa430-B47" ref-type="bibr">Riedel <italic>et al.</italic>, 2010</xref>). First, we collect all protein pairs and the relations between each pair from a large knowledge base, where we opt for the PID data base (<xref rid="btaa430-B49" ref-type="bibr">Schaefer <italic>et al.</italic>, 2009</xref>), due to its very high curation standards. We gather our data from the Simple Interaction Format version of PID provided by PathwayCommons (<ext-link ext-link-type="uri" xlink:href="https://www.pathwaycommons.org/archives/PC2/v11/PathwayCommons11.pid.hgnc.txt.gz">https://www.pathwaycommons.org/archives/PC2/v11/PathwayCommons11.pid.hgnc.txt.gz</ext-link>) (<xref rid="btaa430-B10" ref-type="bibr">Cerami <italic>et al.</italic>, 2011</xref>). Then, for each protein pair <italic>p</italic><sub>1</sub> and <italic>p</italic><sub>2</sub>, we collect all text spans up to the length of 300 characters that mention <italic>p</italic><sub>1</sub> and <italic>p</italic><sub>2</sub> together. To estimate the probability that a protein pair is related by none of the considered PPAs, we also require negative pairs which are not related by any PPA. We generate such negative examples by randomly sampling <inline-formula id="IE18"><mml:math id="IM18"><mml:mrow><mml:mn>10</mml:mn><mml:mo>·</mml:mo><mml:mo>|</mml:mo><mml:mtext>PID</mml:mtext><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula> pairs, where <inline-formula id="IE19"><mml:math id="IM19"><mml:mrow><mml:mo>|</mml:mo><mml:mtext>PID</mml:mtext><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula> is the number of pairs obtained from PID.</p>
      <p>As a text corpus, we use all 24 377 760 <italic>PubMed</italic> abstracts available through PubTator Central (<ext-link ext-link-type="uri" xlink:href="http://ftp://ftp.ncbi.nlm.nih.gov/pub/lu/PubTatorCentral/bioconcepts2pubtatorcentral.offset.gz">ftp://ftp.ncbi.nlm.nih.gov/pub/lu/PubTatorCentral/bioconcepts2pubtatorcentral.offset.gz</ext-link>, Version of 2019/08/19) (<xref rid="btaa430-B58" ref-type="bibr">Wei <italic>et al.</italic>, 2019</xref>) and 2 986 273 full texts available in the PubmedCentral BioC text mining collection (<ext-link ext-link-type="uri" xlink:href="http://ftp://ftp.ncbi.nlm.nih.gov/pub/wilbur/BioC-PMC/">ftp://ftp.ncbi.nlm.nih.gov/pub/wilbur/BioC-PMC/</ext-link>, Version of 2019/05/24) (<xref rid="btaa430-B12" ref-type="bibr">Comeau <italic>et al.</italic>, 2019</xref>). We use the NER and NEN annotations from PubTator Central for both abstracts and full texts. We transform the Entrez ids provided by PubTator Central to Uniprot identifiers with the mapping provided by Uniprot (<ext-link ext-link-type="uri" xlink:href="http://ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/idmapping/by_organism/HUMAN_9606_idmapping.dat.gz">ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/idmapping/by_organism/HUMAN_9606_idmapping.dat.gz</ext-link>) to relate them to the Uniprot identifiers from PID. Additionally, we expand the identified proteins with all homologous proteins obtained from the HomoloGene database (<ext-link ext-link-type="uri" xlink:href="http://ftp://ftp.ncbi.nih.gov/pub/HomoloGene/build68/homologene.data">ftp://ftp.ncbi.nih.gov/pub/HomoloGene/build68/homologene.data</ext-link>), to increase the number of text spans per protein pair, considering only the taxa <italic>Homo Sapiens</italic>, <italic>Rattus norvegicus</italic>, <italic>Mus musculus</italic>, <italic>Oryctolagus cuniculus</italic> and <italic>Cricetulus longicaudatus</italic>. For protein pairs which occur together in more than 100 texts, we randomly sample 100 texts and discard the rest. Finally, we discard all (positive and negative) protein pairs which did not co-occur at least once. Detailed statistics of the resulting dataset can be found in <xref rid="btaa430-T1" ref-type="table">Table 1</xref>.
</p>
      <table-wrap id="btaa430-T1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>Statistics of the datasets BioNLP 2011, BioNLP 2013 and PID</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="2" colspan="1"/>
              <th colspan="6" align="center" rowspan="1">Relations<hr/></th>
              <th colspan="2" align="center" rowspan="1">Pairs<hr/></th>
              <th colspan="2" align="center" rowspan="1">Texts (Avg.)<hr/></th>
            </tr>
            <tr>
              <th align="center" rowspan="1" colspan="1">expr.</th>
              <th align="center" rowspan="1" colspan="1">phosph.</th>
              <th align="center" rowspan="1" colspan="1">State</th>
              <th align="center" rowspan="1" colspan="1">Transport</th>
              <th align="center" rowspan="1" colspan="1">Complex</th>
              <th align="center" rowspan="1" colspan="1">Total</th>
              <th align="center" rowspan="1" colspan="1">pos.</th>
              <th align="center" rowspan="1" colspan="1">neg.</th>
              <th align="center" rowspan="1" colspan="1">pos.</th>
              <th align="center" rowspan="1" colspan="1">neg.</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">BioNLP 2011</td>
              <td rowspan="1" colspan="1">245</td>
              <td rowspan="1" colspan="1">44</td>
              <td rowspan="1" colspan="1">136</td>
              <td rowspan="1" colspan="1">38</td>
              <td rowspan="1" colspan="1">278</td>
              <td rowspan="1" colspan="1">741</td>
              <td rowspan="1" colspan="1">615</td>
              <td rowspan="1" colspan="1">1845</td>
              <td rowspan="1" colspan="1">19.69</td>
              <td rowspan="1" colspan="1">4.97</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BioNLP 2013</td>
              <td rowspan="1" colspan="1">179</td>
              <td rowspan="1" colspan="1">104</td>
              <td rowspan="1" colspan="1">160</td>
              <td rowspan="1" colspan="1">43</td>
              <td rowspan="1" colspan="1">441</td>
              <td rowspan="1" colspan="1">927</td>
              <td rowspan="1" colspan="1">730</td>
              <td rowspan="1" colspan="1">2190</td>
              <td rowspan="1" colspan="1">17.44</td>
              <td rowspan="1" colspan="1">4.85</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PID</td>
              <td rowspan="1" colspan="1">2376</td>
              <td rowspan="1" colspan="1">2714</td>
              <td rowspan="1" colspan="1">8425</td>
              <td rowspan="1" colspan="1">1020</td>
              <td rowspan="1" colspan="1">5799</td>
              <td rowspan="1" colspan="1">20 622</td>
              <td rowspan="1" colspan="1">16 369</td>
              <td rowspan="1" colspan="1">54 261</td>
              <td rowspan="1" colspan="1">53.60</td>
              <td rowspan="1" colspan="1">16.32</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic>Note</italic>: Relations gives the total number of protein pairs for the five considered relations <italic>controls-expression-of</italic> (expr.), <italic>controls-phosphorylation-of</italic> (phosph.), <italic>controls-state-change-of</italic> (state), <italic>controls-transport-of</italic> (transport) and <italic>in-complex-with</italic> (complex). Pairs denote the total number of protein pairs with at least one relation (pos.) and without any relation (neg.). Texts states the average number of text spans per protein pair for pairs with at least one relation (pos.) and without any relation (neg.).</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Next, we describe the generation of the directly supervised data, which we need for two different purposes. First, we use it as additional training data as explained above and second, it allows us to perform experiments with known relations for text spans, which then lets us evaluate the performance for evidence prediction without manual inspection of the predictions. To perform these experiments, we actually need <italic>two</italic> distinct directly supervised datasets, one for evaluation and one as additional training data for PEDL. To generate the directly supervised data, we transform sentence-level event extraction data from the BioNLP-shared tasks (<xref rid="btaa430-B20" ref-type="bibr">Kim <italic>et al.</italic>, 2011a</xref>; <xref rid="btaa430-B33" ref-type="bibr">Nédellec <italic>et al.</italic>, 2013</xref>) into multi-instance learning data. We transform the BioNLP event structures into pairwise relations between proteins with the same five relation types as for the <italic>distant</italic> data. The details of this transformation can be found in <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref> S3. Then, akin to the generation of the <italic>distant</italic> data, we normalize all protein mentions, collect all pairs of co-occurring proteins and sample non-interacting proteins as negative examples. We normalize protein mentions by querying MyGeneInfo (<xref rid="btaa430-B60" ref-type="bibr">Xin <italic>et al.</italic>, 2016</xref>) for the human uniprot id. Tokenization and sentence splitting are performed with the <italic>en_core_sci_sm</italic> model of SciSpacy (<xref rid="btaa430-B34" ref-type="bibr">Neumann <italic>et al.</italic>, 2019</xref>). We perform this transformation for the <italic>Genia</italic> (<xref rid="btaa430-B21" ref-type="bibr">Kim <italic>et al.</italic>, 2011b</xref>) and <italic>epigenetics</italic> (<xref rid="btaa430-B36" ref-type="bibr">Ohta <italic>et al.</italic>, 2011</xref>) datasets from <italic>BioNLP 2011</italic> as well as the <italic>Genia</italic> (<xref rid="btaa430-B22" ref-type="bibr">Kim <italic>et al.</italic>, 2013</xref>) and <italic>Pathway Curation</italic> (<xref rid="btaa430-B37" ref-type="bibr">Ohta <italic>et al.</italic>, 2013</xref>) tasks from <italic>BioNLP 2013</italic>. These BioNLP datasets were specifically selected since they were the only ones containing annotations for all considered PPA types. Finally, we aggregate the protein pairs of both 2011 and 2013 tasks, respectively. This yields two multi-instance learning datasets with the additional information of which text spans express relations between the proteins. Detailed statistics of both datasets can be found in <xref rid="btaa430-T1" ref-type="table">Table 1</xref>.</p>
      <p>In preliminary experiments on the <italic>PID</italic> dataset, we found that the predictions of PEDL seemed to almost exclusively rely on the protein names appearing in the text span. While this led to good performance for relation prediction, this is most likely an artefact of the PID database, because if two proteins are related by a given PPA, then frequently, all members of the respective protein families are related by the same PPA. Ultimately, we are interested in predicting PPAs that are <italic>not</italic> contained in PID, and thus, we performed all further experiments on entity blinded data, which prevents PEDL from inferring family membership. To achieve this, we replaced all protein names recognized by the <italic>en_ner_jnlpba_md</italic> model of SciSpacy with dummy identifiers.</p>
    </sec>
    <sec>
      <title>2.3 Baselines</title>
      <p>We compare PEDL to the two competitor methods <italic>comb-dist</italic> (<xref rid="btaa430-B4" ref-type="bibr">Beltagy <italic>et al.</italic>, 2019a</xref>) and <italic>EVEX</italic> (<xref rid="btaa430-B54" ref-type="bibr">Van Landeghem <italic>et al.</italic>, 2013</xref>), representing the state-of-the-art for distantly supervised relation extraction (<italic>comb-dist</italic>) and for sentence-level relation extraction applied on whole PubMed (<italic>EVEX</italic>).</p>
      <p><italic>comb-dist</italic> is a recently published multi-instance learning method for distantly supervised relation extraction. It set a new state-of-the-art on a standard benchmark for distantly supervised relation extraction (<xref rid="btaa430-B47" ref-type="bibr">Riedel <italic>et al.</italic>, 2010</xref>) strongly outperforming competitor methods by additionally integrating directly supervised data. As a base model, comb-dist uses a piece-wise convolutional neural network with selective attention and pre-trained word embeddings. comb-dist was not developed for biomedical applications and has never been applied to such data as far as we know. In all experiments with comb-dist, we use the (selective) attention distribution over the text spans as evidence predictions. A detailed discussion of the differences between PEDL and comb-dist is provided in <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref> S6. For word embeddings, we equip comb-dist with <italic>wikipedia-pubmed-PMC</italic> (<ext-link ext-link-type="uri" xlink:href="http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.bin">http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.bin</ext-link>) embeddings of <xref rid="btaa430-B44" ref-type="bibr">Pyysalo <italic>et al.</italic> (2013)</xref>, because they performed well in our earlier work (<xref rid="btaa430-B15" ref-type="bibr">Habibi <italic>et al.</italic>, 2017</xref>). The hyperparameter settings of comb-dist for each task are provided in <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref> S2.</p>
      <p><italic>EVEX</italic> is a database of text-mined biological events, accompanied by inferred pairwise PPAs and has annotations for whether an event was deemed speculative or negated. The database was created by applying a state-of-the-art biomedical event extraction tool (<xref rid="btaa430-B7" ref-type="bibr">Björne, 2014</xref>) to a large collection of PubMed abstracts and PMC full texts. Since the EVEX database was last updated in 2013, we compare PEDL with EVEX on a modified test data of PID in which we only use texts published prior to 2013 to ensure a fair comparison. We apply a straight-forward mapping of EVEX’s types of PPAs to the five considered in our work (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref> S4) and remove all relations with a detected negation, but retain speculative relations.</p>
    </sec>
    <sec>
      <title>2.4 Evaluation details</title>
      <p>We use the three datasets <italic>PID</italic>, <italic>BioNLP 2011</italic> and <italic>BioNLP 2013</italic> in three different experimental settings <italic>E1</italic>, <italic>E2</italic> and <italic>E3</italic>.
</p>
      <list list-type="bullet">
        <list-item>
          <p><italic>E1</italic>: <italic>PID</italic> is the distantly supervised data and the union of both BioNLP datasets are the directly supervised auxiliary training data.</p>
        </list-item>
        <list-item>
          <p><italic>E2</italic>: <italic>BioNLP 2011</italic> is the distantly supervised data (disregarding evidence annotations during training) and <italic>BioNLP 2013</italic> is the directly supervised auxiliary training data.</p>
        </list-item>
        <list-item>
          <p><italic>E3</italic>: <italic>BioNLP 2013</italic> is the distantly supervised data and <italic>BioNLP 2011</italic> is the directly supervised auxiliary training data.</p>
        </list-item>
      </list>
      <p>In both <italic>E2</italic> and <italic>E3</italic>, we report the average of five runs with different seeds to compensate for the small dataset sizes. Note that results from the BioNLP shared tasks are not comparable to <italic>E2</italic> and <italic>E3</italic> because we do perform multi-instance learning (and not sentential prediction) and the label spaces are different. We use the directly supervised data only during training and remove all protein pairs occurring in the development and test set from the directly supervised data to prevent knowledge leaks. We split each dataset into train, development and test set by randomly dividing protein pairs with their associated text in a 60:10:30 ratio. For relation prediction, we compare models by plotting their precision–recall (PR) curves. These curves are computed by ranking all PPAs by the predicted confidence score of the model and computing the resulting (micro-averaged) precision and recall for all possible threshold values. We also report the average precision (AP) which is an approximation of the area under the PR-curve. We use mean average precision (mAP) and precision at ten (P@10) to evaluate evidence predictions, both for the automated evaluation in <italic>E2</italic> and <italic>E3</italic>, as well as for the manual evaluation by domain experts in <italic>E1</italic>. mAP averages the individual APs of evidence predictions for each protein pair and P@10 is defined the mean precision of the top ten predictions.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <p>We evaluate PEDL, a method for predicting PPA-relations between proteins and the evidence for these relations, on three different datasets. The results are compared to two competitor methods: comb-dist, a recently published state-of-the-art multi-instance relation extraction method, and EVEX, a large data base of PPAs that was generated by applying biomedical event extraction to a large collection of abstracts and full texts.</p>
    <sec>
      <title>3.1 Prediction of PPAs</title>
      <p>At first, we investigate the results of PEDL for predicting PPAs between pairs of proteins. The results for the BioNLP datasets (E2 and E3) can be found in <xref rid="btaa430-T2" ref-type="table">Table 2</xref> and results for PID (<italic>E1</italic>) in <xref rid="btaa430-T3" ref-type="table">Table 3</xref>. In terms of AP, PEDL performs better than the competitor methods on two of the three considered datasets and comparable on the third. On BioNLP 2013 (<italic>E3</italic>), PEDL achieves an AP score that is 6.07 pp higher than that of comb-dist, while on PID (<italic>E1</italic>, mixing predictions for all PPA types) it is 1.24 pp higher. If one considers predictions for each type of PPA on PID individually, the difference between both models is considerably larger. PEDL performs better than comb-dist on all five types with differences ranging from 1.84 pp for <italic>in-complex-with</italic> to 12.34 for <italic>controls-transport-of</italic>, with an average of 4.66 pp. On BioNLP 2011 (<italic>E2</italic>), the difference in AP of both models is marginal.
</p>
      <table-wrap id="btaa430-T2" orientation="portrait" position="float">
        <label>Table 2.</label>
        <caption>
          <p>Results on the two BioNLP datasets (E2 and E3)</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="2" colspan="1"/>
              <th colspan="2" rowspan="1">BioNLP ’11<hr/></th>
              <th colspan="2" rowspan="1">BioNLP ’13<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">r-AP</th>
              <th rowspan="1" colspan="1">e-mAP</th>
              <th rowspan="1" colspan="1">r-AP</th>
              <th rowspan="1" colspan="1">e-mAP</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">comb-dist</td>
              <td rowspan="1" colspan="1"><inline-formula id="IE20"><mml:math id="IM20"><mml:mrow><mml:mn>65.4</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>2.6</mml:mn></mml:mrow></mml:math></inline-formula>)</td>
              <td rowspan="1" colspan="1"><inline-formula id="IE21"><mml:math id="IM21"><mml:mrow><mml:mn>75.86</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>1.6</mml:mn></mml:mrow></mml:math></inline-formula>)</td>
              <td rowspan="1" colspan="1"><inline-formula id="IE22"><mml:math id="IM22"><mml:mrow><mml:mn>70.68</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>2.6</mml:mn></mml:mrow></mml:math></inline-formula>)</td>
              <td rowspan="1" colspan="1"><inline-formula id="IE23"><mml:math id="IM23"><mml:mrow><mml:mn>79.35</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">− direct</td>
              <td rowspan="1" colspan="1">
                <inline-formula id="IE24">
                  <mml:math id="IM24">
                    <mml:mrow>
                      <mml:mn>62.33</mml:mn>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mn>1.8</mml:mn>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">
                <inline-formula id="IE25">
                  <mml:math id="IM25">
                    <mml:mrow>
                      <mml:mn>54.38</mml:mn>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mn>26.9</mml:mn>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">
                <inline-formula id="IE26">
                  <mml:math id="IM26">
                    <mml:mrow>
                      <mml:mn>70.06</mml:mn>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mn>2.1</mml:mn>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">
                <inline-formula id="IE27">
                  <mml:math id="IM27">
                    <mml:mrow>
                      <mml:mn>54.64</mml:mn>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mn>27.2</mml:mn>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PEDL</td>
              <td rowspan="1" colspan="1">
                <inline-formula id="IE28">
                  <mml:math id="IM28">
                    <mml:mrow>
                      <mml:mn>6</mml:mn>
                      <mml:mi mathvariant="normal">5.59</mml:mi>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mn>4.9</mml:mn>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">
                <inline-formula id="IE29">
                  <mml:math id="IM29">
                    <mml:mrow>
                      <mml:mn>8</mml:mn>
                      <mml:mi mathvariant="normal">2.36</mml:mi>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mn>1.2</mml:mn>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">
                <inline-formula id="IE30">
                  <mml:math id="IM30">
                    <mml:mrow>
                      <mml:mn>7</mml:mn>
                      <mml:mi mathvariant="normal">6.75</mml:mi>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mn>2.0</mml:mn>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">
                <inline-formula id="IE31">
                  <mml:math id="IM31">
                    <mml:mrow>
                      <mml:mn>8</mml:mn>
                      <mml:mi mathvariant="normal">4.67</mml:mi>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mn>1.6</mml:mn>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">− direct</td>
              <td rowspan="1" colspan="1">
                <inline-formula id="IE32">
                  <mml:math id="IM32">
                    <mml:mrow>
                      <mml:mn>60.65</mml:mn>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mn>4.1</mml:mn>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">
                <inline-formula id="IE33">
                  <mml:math id="IM33">
                    <mml:mrow>
                      <mml:mn>64.64</mml:mn>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mn>4.1</mml:mn>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">
                <inline-formula id="IE34">
                  <mml:math id="IM34">
                    <mml:mrow>
                      <mml:mn>71.03</mml:mn>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mn>3.0</mml:mn>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </td>
              <td rowspan="1" colspan="1">
                <inline-formula id="IE35">
                  <mml:math id="IM35">
                    <mml:mrow>
                      <mml:mn>75.14</mml:mn>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mn>2.1</mml:mn>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <p><italic>Note</italic>: r-AP is the AP for relation prediction and e-mAP the mAP for evidence prediction. All results are averages of five runs with different random seeds, with standard deviations given in brackets. ‘- direct’ shows scores without directly supervised data. The best scores are displayed in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <table-wrap id="btaa430-T3" orientation="portrait" position="float">
        <label>Table 3.</label>
        <caption>
          <p>APs for relation prediction on the PID data (E1) for the PPA types <italic>controls-expression-of</italic> (expr.), <italic>controls-phosphorylation-of</italic> (phosph.), <italic>controls-state-change-of</italic> (state), <italic>controls-transport-of</italic> (transport) and <italic>in-complex-with</italic> (complex)</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">expr.</th>
              <th rowspan="1" colspan="1">phosph.</th>
              <th rowspan="1" colspan="1">State</th>
              <th rowspan="1" colspan="1">Transport</th>
              <th rowspan="1" colspan="1">Complex</th>
              <th rowspan="1" colspan="1">Total</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">comb-dist</td>
              <td rowspan="1" colspan="1">42.77</td>
              <td rowspan="1" colspan="1">38.38</td>
              <td rowspan="1" colspan="1">49.14</td>
              <td rowspan="1" colspan="1">5.87</td>
              <td rowspan="1" colspan="1">47.86</td>
              <td rowspan="1" colspan="1">44.78</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PEDL</td>
              <td rowspan="1" colspan="1">
                <bold>46.45</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>40.26</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>52.70</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>18.21</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>49.70</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>46.02</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">count</td>
              <td rowspan="1" colspan="1">694</td>
              <td rowspan="1" colspan="1">817</td>
              <td rowspan="1" colspan="1">2532</td>
              <td rowspan="1" colspan="1">288</td>
              <td rowspan="1" colspan="1">1668</td>
              <td rowspan="1" colspan="1">5999</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn3">
            <p><italic>Note</italic>: Total gives the AP for all PPA types as a micro-average. The best score per relation-type is displayed in bold. Count denotes the number of protein pairs with this type of PPA in the test set. Note that total is computed on a ranking of predictions including all PPA types, which leads to the fact that the difference between both models is smaller than every distance of the individual PPAs. EVEX cannot be compared in this setting, because it does not consider texts published after 2013.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>It is instructive to compare the PR-curves of PEDL, comb-dist and EVEX for relation prediction on the PID data (E1, see <xref ref-type="fig" rid="btaa430-F2">Fig. 2</xref>). We compare with the results of EVEX only on abstracts and full texts published prior to 2013 to account for the fact that EVEX was last updated in 2013. Both models strongly outperform EVEX on the <italic>before 2013</italic> data, both in terms of recall and precision. The difference in recall is especially pronounced, because EVEX only generates positive predictions for fewer than 37% of the PPAs in the PID test set. PEDL performs better than comb-dist in the mid-precision regime but a little worse for low precisions when provided all articles and full texts. On the <italic>before 2013</italic> subset, PEDL performs equal to comb-dist in the high-precision regime but worse for mid-to-low precision values, leading to 40.54% AP for PEDL and 44.24% AP for comb-dist (see Section 4.2 for a discussion of this).
</p>
      <fig id="btaa430-F2" orientation="portrait" position="float">
        <label>Fig. 2.</label>
        <caption>
          <p>(<bold>a</bold>) PR curve for the PID data. The left plot shows results for all available abstracts and full texts. The right plot displays the results using only abstracts and full texts published prior to 2013, which allows a fair comparison with EVEX. These results are based on a ranking that includes all types of PPA. The improvement of PEDL over comb-dist is larger for rankings of only one type of PPA (see <xref rid="btaa430-T3" ref-type="table">Table 3</xref> for numbers and explanation). (<bold>b</bold>) Results from the manual evaluation of evidence prediction on PID</p>
        </caption>
        <graphic xlink:href="btaa430f2"/>
      </fig>
    </sec>
    <sec>
      <title>3.2 Evidence prediction</title>
      <p>In most biomedical applications, extracted PPAs are not accepted per se, but undergo confirmation through experts. The reason is the far from perfect performance of state-of-the-art approaches, and the fact that even a correctly extracted text needs not express biological truth, for instance due to weak experimental evidence. Therefore, it is important that methods not only predict the correct PPA, but also the text spans on which the model’s PPA prediction is based (which we call evidence prediction). <xref rid="btaa430-T2" ref-type="table">Table 2</xref> gives results for evidence prediction on the BioNLP datasets, where both PEDL and comb-dist achieve high mAP scores for evidence prediction. PEDL outperforms comb-dist on both datasets with 6.38 pp for BioNLP ‘11 and 5.32 pp for BioNLP’13.</p>
      <p>In contrast, PID is a distantly supervised dataset and does not have annotations to evaluate evidence predictions. For the predictions of comb-dist and PEDL, two domain experts evaluated the top ten evidence predictions for the top 10 predictions of each PPA-type, amounting to 500 evaluated evidence predictions (the annotation guidelines can be found in <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref> S5). Note, that for this evaluation, we directly use the rows of the score matrix as evidence score per relation for PEDL. This refinement is not possible for comb-dist, because the attention distribution is computed independently of the relation type. This allows PEDL to rank the evidence specifically for one PPA type, while comb-dist only predicts whether there is a relation between the proteins at all. The results of this analysis show that PEDL performs better than comb-dist for predicting evidence for the three PPA-types <italic>controls-transport-of</italic>, <italic>in-complex-with</italic> and <italic>controls-expression-of</italic> (see <xref ref-type="fig" rid="btaa430-F2">Fig. 2</xref>). The results for <italic>controls-state-change-of</italic> are comparable and worse for <italic>controls-phosphorylation-of</italic>. The improvement over comb-dist is especially striking in the case of <italic>controls-transport-of</italic>, for which comb-dist produces almost no correct evidence predictions and PEDL achieves a mAP of 46%. The results in terms of P@10 are similar, with PEDL additionally achieving better results for <italic>controls-state-change-of</italic>. Moreover, the variability in performance across different PPA types is much larger for comb-dist than for PEDL. On average, PEDL achieves a 7.66 pp higher mAP and a 8.14 pp higher P@10 than comb-dist.</p>
    </sec>
    <sec>
      <title>3.3 Analysis of new predictions</title>
      <p>We also evaluated PEDL in a realistic application scenario, where three experts in systems biology manually analyzed the top 10 predictions that are not contained in the aforementioned PathwayCommons versions of neither Reactome nor PID. The results are summarized in <xref rid="btaa430-T4" ref-type="table">Table 4</xref>, where we provide all predictions considered biologically justified by all experts together with the highest ranking true evidence text span. In the evaluation, 6 out of 10 are predicted correctly, while one prediction is wrong due to errors in the protein normalization pre-processing step, and the other three are errors of PEDL. It can be further observed, that for all correct predictions but one, the highest ranking text span (columns <italic>Text span</italic> and <italic>t</italic>) actually expresses the PPA and either states the finding of the PPA or refers to an earlier publication reporting it.
</p>
      <table-wrap id="btaa430-T4" orientation="portrait" position="float">
        <label>Table 4.</label>
        <caption>
          <p>Evaluation results for the top-10 predictions that cannot be found either in Reactome or in PID</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">k</th>
              <th rowspan="1" colspan="1">PPA</th>
              <th rowspan="1" colspan="1">Text span (source PMID)</th>
              <th rowspan="1" colspan="1">t</th>
              <th rowspan="1" colspan="1">Evidence</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">IGF-II <italic>in-complex-with</italic> VN</td>
              <td rowspan="1" colspan="1">‘We have previously reported that IGF-II binds the extracellular matrix protein vitronectin (VN) […] ’ (12746303)</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">
                <xref rid="btaa430-B53" ref-type="bibr">Upton <italic>et al.</italic> (1999)</xref>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">2</td>
              <td rowspan="1" colspan="1">hnRNP-A1 <italic>controls-expression-of</italic> IL10</td>
              <td rowspan="1" colspan="1">‘These results suggest that hnRNP-A1 promotes transcription of human IL10.’ (19349988)</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">
                <xref rid="btaa430-B35" ref-type="bibr">Noguchi <italic>et al.</italic> (2009)</xref>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">4</td>
              <td rowspan="1" colspan="1">NCOR1 <italic>controls-expression-of</italic> PSA</td>
              <td rowspan="1" colspan="1">‘ChIP-reChIP assays revealed that NCOR and […] p300 are present in distinct AR complexes on the promoter of PSA gene […]’ (23518348)</td>
              <td rowspan="1" colspan="1">4</td>
              <td rowspan="1" colspan="1">
                <xref rid="btaa430-B45" ref-type="bibr">Qi <italic>et al.</italic> (2013)</xref>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">5</td>
              <td rowspan="1" colspan="1">ets-2 <italic>controls-expression-of</italic> BRCA1</td>
              <td rowspan="1" colspan="1">‘Conditional overproduction of ets-2 in MCF-7 cells resulted in repression of endogenous BRCA1 mRNA expression.’ (12637547)</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1">
                <xref rid="btaa430-B2" ref-type="bibr">Baker <italic>et al.</italic> (2003)</xref>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">6</td>
              <td rowspan="1" colspan="1">c-Rel <italic>controls-expression-of</italic> Bcl-X</td>
              <td rowspan="1" colspan="1">‘We further demonstrate […] that introduction of two downstream c-Rel target genes, Bcl-X […]’ (15922711)</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1"><xref rid="btaa430-B11" ref-type="bibr">Chen <italic>et al.</italic> (2000)</xref>/ <xref rid="btaa430-B26" ref-type="bibr">Lee <italic>et al.</italic> (1999)</xref></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">8</td>
              <td rowspan="1" colspan="1">C/EBP-beta <italic>controls-expression-of</italic> COX-2</td>
              <td rowspan="1" colspan="1">‘C/EBP-beta is a transcription factor […] capable of inducing COX-2 expression […]’ (19124115)</td>
              <td rowspan="1" colspan="1">1</td>
              <td rowspan="1" colspan="1"><xref rid="btaa430-B23" ref-type="bibr">Kim and Fischer (1998)</xref>/ <xref rid="btaa430-B62" ref-type="bibr">Zhu <italic>et al.</italic> (2002)</xref></td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn4">
            <p><italic>Note</italic>: The rank of the prediction is given by <italic>k</italic>. We provide the highest ranking evidence text span that actually expresses the relation and its rank in PEDL (<italic>t</italic>), as well as manually sourced literature evidence that provides strong biological evidence for the existence of the PPA. Note that this evidence need not be identical to the evidence span predicted by the model.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <sec>
      <title>4.1 Importance of directly supervised data</title>
      <p>The results given in <xref rid="btaa430-T2" ref-type="table">Table 2</xref> allow for interesting observations regarding the importance of directly supervised data. On the BioNLP datasets, the incorporation of directly supervised data improves results for both relation and evidence prediction. The improvement is much more pronounced for the evidence prediction task than for relation prediction. This supports our hypothesis, that we can improve evidence prediction specifically by including directly supervised data. Compared to comb-dist, PEDL has a much larger gain from directly supervised data in the relation prediction task (5.33 pp versus 1.85 pp). For BioNLP 2011, comb-dist even outperforms PEDL in relation prediction when no directly supervised data is available. This might partly be because the inclusion of directly supervised data stabilizes PEDL’s training process. In preliminary experiments on the PID dataset, we observed that without access to directly supervised data the model failed to converge, while setting the whole score matrix to zero. We attribute this to the fact that usually only a few of the (max.) 100 text spans actually express the annotated relation and think that the directly supervised data compensates for the resulting label imbalance for evidence prediction.</p>
      <p>Notably, PEDL achieves strong results for evidence prediction even without access to directly supervised data. This suggests that the constraint of only aggregating (logit-)scores, and not high-dimensional embeddings as in comb-dist’s selective attention, is more appropriate for evidence prediction in absence of directly supervised data. These scores also have a clear interpretation as the confidence of PEDL that the given text span supports a given PPA. The lower (average) performance of comb-dist in this setting can be attributed to strong performance drops for some random seeds (min. 24.03 versus max. 76.61), indicating a notable instability of the model. We furthermore found that running comb-dist with the most recent versions of PyTorch (1.4.0) and AllenNLP (0.9.0) leads to a performance drop of 1 to 5 pp. for both relation prediction and evidence prediction.</p>
    </sec>
    <sec>
      <title>4.2 Comparison to EVEX</title>
      <p>The comparison of the two distantly supervised methods to EVEX (cf. <xref ref-type="fig" rid="btaa430-F2">Fig. 2</xref>) is instructive, because it allows to compare methods trained only on directly supervised data to models with access to both types of data. Especially striking is the difference in recall between EVEX and the distantly supervised models, where EVEX only contains predictions for 36.15% of the positive protein pairs, while PEDL and comb-dist produce predictions for 95.1% and 95.33% of the protein pairs. This might be partially attributed to the advancements in NER and Normalization that were achieved since 2013—which we implicitly incorporate by using PubTator Central—but also stresses the importance of predicting relations for proteins that occur in different sentences. Recall that EVEX only considers single sentences.</p>
      <p>The importance of using multiple sentences will be further discussed in the next section. Notably, the increased recall does not come at the price of reduced precision, as both PEDL and comb-dist strongly outperform EVEX in all precision regimes. Together with the encouraging results of the evidence prediction, this indicates that distant supervision is a promising paradigm to train accurate classifiers for PPA prediction.</p>
      <p>A related interesting observation is that PEDL performs markedly worse on the <italic>before 2013</italic> subset of the data, whereas comb-dist almost retains its performance. We hypothesized that the reason for this behaviour lies in the fact that PEDL does not model the semantic interactions between text spans via attention, making it more susceptible to violations of the at-least-once assumption. To validate this, we inspect the top 10 predictions of PEDL for true PPAs with the largest drop in ranking between the full and the <italic>before 2013</italic> data. We found that for nine of the ten PPAs, none of the texts published prior to 2013 contains any mention of the PPA. Additionally, no text published prior to 2013 contained any mention of the associated protein pair for 5% of all true PPAs, which limits PEDL’s maximum recall to 95% for the <italic>before 2013</italic> data.</p>
    </sec>
    <sec>
      <title>4.3 Importance of using multiple sentences</title>
      <p>We investigate the effect of considering protein mentions across sentences by measuring the fraction of protein-pairs in PID that are at most <italic>d</italic> characters away from each other in at least one text for different values of <italic>d</italic>. Additionally, we report this quantity considering only single sentences, again using the <italic>en_core_sci_sm</italic> model of SciSpacy to split the text into sentences. The results are depicted in <xref ref-type="fig" rid="btaa430-F3">Figure 3</xref>. It can be observed that considering only protein mentions that occur within the same sentence has a strong limiting effect on maximum recall. Using <italic>d </italic>=<italic> </italic>300, PEDL can predict PPAs for 87.9% of the positive pairs in PID, which is a large gain over the 59.24% that would be achievable if we considered only single sentences. This, however, comes at the price of more included negative protein pairs. PEDL predicts PPAs for 50.01% of the considered negative pairs, whereas a sentence-level approach would predict PPAs for only 9.25%. This highlights the importance of using a strong machine learning model to rank the predicted PPAs instead of relying on simple co-occurrence statistics in the high-recall regime.
</p>
      <fig id="btaa430-F3" orientation="portrait" position="float">
        <label>Fig. 3.</label>
        <caption>
          <p>Maximum possible recall for a given maximum character distance between the protein mentions. ‘Positive’ refers to protein pairs with at least one PPA in PID and ‘Negative’ to pairs without any. The dashed lines indicate the maximum recall that is possible for sentence level approaches. The red vertical line indicates our choice for the maximum distance between pairs</p>
        </caption>
        <graphic xlink:href="btaa430f3"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>5 Conclusion</title>
    <p>We propose PEDL, a method for predicting PPAs and their textual evidence by integrating deep language models, distant supervision and auxiliary directly supervised data. We compare PEDL on three different datasets with two state-of-the-art methods and find that, on average, it outperforms them in most cases and performs comparably in the remaining ones. A manual evaluation of the predicted PPAs shows that PEDL can be used to identify PPAs that are missing in major pathway data bases. Furthermore, we demonstrate that the predicted evidence text spans actually express the relation and thus can be used to quickly verify the predicted PPAs.</p>
    <p>Owing to the incorporation of BERT, the method proposed in this article has very high runtime requirements which make it unsuitable for predicting PPAs between all possible pairs. This problem could be solved using recently published model distillation techniques for BERT (<xref rid="btaa430-B48" ref-type="bibr">Sanh <italic>et al.</italic>, 2019</xref>). We only address pairwise PPA prediction in which a relation holds between exactly two proteins. Actual biochemical reactions are much more complex than that, as they can have multiple reactants, products and regulators, which can also be protein complexes or completely different molecules (<xref rid="btaa430-B6" ref-type="bibr">Berg <italic>et al.</italic>, 2019</xref>). It would be worthwhile to study whether biomedical event extraction (<xref rid="btaa430-B37" ref-type="bibr">Ohta <italic>et al.</italic>, 2013</xref>) can be combined with distant supervision to predict such complex biochemical reactions. Finally, PEDL could also be used to predict evidence for known PPAs, for instance those from the distantly supervised training data, which we did not investigate in this work.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>btaa430_Supplementary_Data</label>
      <media xlink:href="btaa430_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>The authors thank Mareike Simon for manual evaluation of the newly predicted PPAs. L.W. acknowledges the support of the Helmholtz Einstein International Berlin Research School in Data Science (HEIBRiDS). The authors acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research.</p>
    <sec>
      <title>Funding</title>
      <p>This work was supported by the Helmholtz Society through the research training group HEIBRIDS. J.W. acknowledges funding by the German Federal Ministry of Education and Research BMBF [e:med project 031L0189D].</p>
      <p><italic>Conflict of Interest</italic>: none declared.</p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btaa430-B1">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Angeli</surname><given-names>G.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) Combining distant and partial supervision for relation extraction. In <italic>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</italic>, Association for Computational Linguistics, Doha, Qatar, pp. <fpage>1556</fpage>–<lpage>1567</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Baker</surname><given-names>K.M.</given-names></name></person-group><etal>et al</etal> (<year>2003</year>) 
<article-title>Ets-2 and components of mammalian SWI/SNF form a repressor complex that negatively regulates the BRCA1 promoter</article-title>. <source>J. Biol. Chem</source>., <volume>278</volume>, <fpage>17876</fpage>–<lpage>17884</lpage>.<pub-id pub-id-type="pmid">12637547</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Barabasi</surname><given-names>A.-L.</given-names></name>, <name name-style="western"><surname>Oltvai</surname><given-names>Z.N.</given-names></name></person-group> (<year>2004</year>) 
<article-title>Network biology: understanding the cell’s functional organization</article-title>. <source>Nat. Rev. Genet</source>., <volume>5</volume>, <fpage>101</fpage>–<lpage>113</lpage>.<pub-id pub-id-type="pmid">14735121</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B4">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Beltagy</surname><given-names>I.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>a) Combining distant and direct supervision for neural relation extraction. In <italic>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</italic>, Association for Computational Linguistics, Minneapolis, Minnesota, pp. <fpage>1858</fpage>–<lpage>1867</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Beltagy</surname><given-names>I.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>b) SciBERT: a pretrained language model for scientific text. In <italic>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</italic>, Association for Computational Linguistics, Hong Kong, China, pp. <fpage>3613</fpage>–<lpage>3618</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B6">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Berg</surname><given-names>J.M.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) <source>Biochemistry</source>, <edition>9th edn</edition>
<publisher-name>WH Freeman, New York</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btaa430-B7">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Björne</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>) <italic>Biomedical Event Extraction with Machine Learning.</italic> Ph.D. thesis, University of Turku.</mixed-citation>
    </ref>
    <ref id="btaa430-B8">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Björne</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Salakoski</surname><given-names>T.</given-names></name></person-group> (<year>2018</year>) Biomedical event extraction using convolutional neural networks and dependency parsing. In <italic>Proceedings of the BioNLP 2018 Workshop</italic>, Association for Computational Linguistics, Melbourne, Australia, pp. <fpage>98</fpage>–<lpage>108</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B9">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Björne</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) Extracting complex biological events with rich graph-based feature sets. In <italic>Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task</italic>, Association for Computational Linguistics, Boulder, Colorado, pp. <fpage>10</fpage>–<lpage>18</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cerami</surname><given-names>E.G.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Pathway Commons, a web resource for biological pathway data</article-title>. <source>Nucleic Acids Res</source>., <volume>39</volume>, <fpage>D685</fpage>–<lpage>D690</lpage>.<pub-id pub-id-type="pmid">21071392</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2000</year>) 
<article-title>The Rel/NF-kappaB family directly activates expression of the apoptosis inhibitor Bcl-x(L)</article-title>. <source>Mol. Cell. Biol</source>., <volume>20</volume>, <fpage>2687</fpage>–<lpage>2695</lpage>.<pub-id pub-id-type="pmid">10733571</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Comeau</surname><given-names>D.C.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>PMC text mining subset in BioC: about three million full-text articles and growing</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>3533</fpage>–<lpage>3535</lpage>.<pub-id pub-id-type="pmid">30715220</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B13">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Devlin</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) BERT: Pre-training of deep bidirectional transformers for language understanding. In <italic>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</italic>, Association for Computational Linguistics, Minneapolis, Minnesota, pp. <fpage>4171</fpage>–<lpage>4186</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Friedman</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2001</year>) 
<article-title>GENIES: a natural-language processing system for the extraction of molecular pathways from journal articles</article-title>. <source>Bioinformatics</source>, <volume>17</volume>, <fpage>S74</fpage>–<lpage>S82</lpage>.<pub-id pub-id-type="pmid">11472995</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Habibi</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Deep learning with word embeddings improves biomedical named entity recognition</article-title>. <source>Bioinformatics</source>, <volume>33</volume>, <fpage>i37</fpage>–<lpage>i48</lpage>.<pub-id pub-id-type="pmid">28881963</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hat</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Feedbacks, bifurcations, and cell fate decision-making in the p53 system</article-title>. <source>PLoS Comput. Biol</source>., <volume>12</volume>, <fpage>e1004787</fpage>.<pub-id pub-id-type="pmid">26928575</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B17">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Hoffmann</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) Knowledge-based weak supervision for information extraction of overlapping relations. In <italic>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</italic>, Association for Computational Linguistics, Portland, Oregon, USA, pp. <fpage>541</fpage>–<lpage>550</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jassal</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>The reactome pathway knowledgebase</article-title>. <source>Nucleic Acids Res</source>., <volume>48</volume>, <fpage>D498</fpage>–<lpage>D503</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Junge</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Jensen</surname><given-names>L.J.</given-names></name></person-group> (<year>2019</year>) 
<article-title>CoCoScore: context-aware co-occurrence scoring for text mining applications using distant supervision</article-title>. <source>Bioinformatics,</source><bold>36</bold>, 264–271.</mixed-citation>
    </ref>
    <ref id="btaa430-B20">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>J.-D.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>a) Overview of BioNLP shared task 2011. In <italic>Proceedings of BioNLP Shared Task 2011 Workshop</italic>, Association for Computational Linguistics, Portland, Oregon, USA, pp. <fpage>1</fpage>–<lpage>6</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B21">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>J.-D.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>b) Overview of Genia event task in BioNLP shared task 2011. In <italic>Proceedings of BioNLP Shared Task 2011 Workshop</italic>, Association for Computational Linguistics, Portland, Oregon, USA, pp. <fpage>7</fpage>–<lpage>15</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B22">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>J.-D.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) The Genia event extraction shared task, 2013 edition – overview. In <italic>Proceedings of the BioNLP Shared Task 2013 Workshop</italic>, Association for Computational Linguistics, Sofia, Bulgaria, pp. <fpage>8</fpage>–<lpage>15</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Fischer</surname><given-names>S.M.</given-names></name></person-group> (<year>1998</year>) 
<article-title>Transcriptional regulation of cyclooxygenase-2 in mouse skin carcinoma cells regulatory role of CCAAT/enhancer-binding proteins in the differential expression of cyclooxygenase-2 in normal and neoplastic tissues</article-title>. <source>J. Biol. Chem</source>., <volume>273</volume>, <fpage>27686</fpage>–<lpage>27694</lpage>.<pub-id pub-id-type="pmid">9765305</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B24">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Kingma</surname><given-names>D.P.</given-names></name>, <name name-style="western"><surname>Ba</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>) Adam: a method for stochastic optimization. In <italic>3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7–9, 2015, Conference Track Proceedings.</italic></mixed-citation>
    </ref>
    <ref id="btaa430-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Köksal</surname><given-names>A.S.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Synthesizing signaling pathways from temporal phosphoproteomic data</article-title>. <source>Cell Rep</source>., <volume>24</volume>, <fpage>3607</fpage>–<lpage>3618</lpage>.<pub-id pub-id-type="pmid">30257219</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>H.H.</given-names></name></person-group><etal>et al</etal> (<year>1999</year>) 
<article-title>NF-kappaB-mediated up-regulation of BCL-x and Bfl-1/A1 is required for CD40 survival signaling in b lymphocytes</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>96</volume>, <fpage>9136</fpage>–<lpage>9141</lpage>.<pub-id pub-id-type="pmid">10430908</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>BioBERT: a pre-trained biomedical language representation model for biomedical text mining</article-title>. <source>Bioinformatics</source>, <bold>36</bold>, 1234–1240.</mixed-citation>
    </ref>
    <ref id="btaa430-B28">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) Neural relation extraction with selective attention over instances. In <italic>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</italic>, Association for Computational Linguistics, Berlin, Germany, pp. <fpage>2124</fpage>–<lpage>2133</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B29">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) A soft-label method for noise-tolerant distantly supervised relation extraction. In <italic>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</italic>, Association for Computational Linguistics, Copenhagen, Denmark, pp. <fpage>1790</fpage>–<lpage>1795</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B30">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Mintz</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) Distant supervision for relation extraction without labeled data. In <italic>Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</italic>, Association for Computational Linguistics, Suntec, Singapore, pp. <fpage>1003</fpage>–<lpage>1011</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B31">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Miwa</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) A rich feature vector for protein–protein interaction extraction from multiple corpora. In <italic>Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</italic>, Association for Computational Linguistics, Singapore, pp. <fpage>121</fpage>–<lpage>130</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Miwa</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>Event extraction with complex event classification using rich features</article-title>. <source>J. Bioinf. Comput. Biol</source>., <volume>08</volume>, <fpage>131</fpage>–<lpage>146</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B33">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Nédellec</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) Overview of BioNLP shared task 2013. In <italic>Proceedings of the BioNLP Shared Task 2013 Workshop</italic>, Association for Computational Linguistics, Sofia, Bulgaria, pp. <fpage>1</fpage>–<lpage>7</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B34">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Neumann</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) ScispaCy: fast and robust models for biomedical natural language processing. In <italic>Proceedings of the 18th BioNLP Workshop and Shared Task</italic>, Association for Computational Linguistics, Florence, Italy, pp. <fpage>319</fpage>–<lpage>327</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Noguchi</surname><given-names>E.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>A Crohn’s disease-associated NOD2 mutation suppresses transcription of human IL10 by inhibiting activity of the nuclear ribonucleoprotein hnRNP-A1</article-title>. <source>Nat. Immunol</source>., <volume>10</volume>, <fpage>471</fpage>–<lpage>479</lpage>.<pub-id pub-id-type="pmid">19349988</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B36">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Ohta</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) Overview of the epigenetics and post-translational modifications (EPI) task of BioNLP shared task 2011. In <italic>Proceedings of BioNLP Shared Task 2011 Workshop</italic>, Association for Computational Linguistics, Portland, Oregon, USA, pp. <fpage>16</fpage>–<lpage>25</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B37">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Ohta</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) Overview of the pathway curation (PC) task of BioNLP shared task 2013. In <italic>Proceedings of the BioNLP Shared Task 2013 Workshop</italic>, Association for Computational Linguistics, Sofia, Bulgaria, pp. <fpage>67</fpage>–<lpage>75</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B38">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Peng</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Lu</surname><given-names>Z.</given-names></name></person-group> (<year>2017</year>) Deep learning for extracting protein–protein interactions from biomedical literature. In: <italic>BioNLP 2017</italic>, Association for Computational Linguistics, Vancouver, Canada, pp. <fpage>29</fpage>–<lpage>38</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B39">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Pershina</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) Infusion of labeled data into distant supervision for relation extraction. In <italic>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</italic>, Association for Computational Linguistics, Baltimore, Maryland, pp. <fpage>732</fpage>–<lpage>738</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B40">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Peters</surname><given-names>M.E.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) To tune or not to tune? adapting pretrained representations to diverse tasks. In <italic>Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</italic>, Association for Computational Linguistics, Florence, Italy, pp. <fpage>7</fpage>–<lpage>14</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B41">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Poon</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) Distant supervision for cancer pathway extraction from text. In: <italic>Proceedings of the Pacific Symposium on Biocomputing, Kohala Coast, Hawaii, USA</italic>, pp. <fpage>120</fpage>–1<lpage>31</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B42">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Pratt</surname><given-names>L.Y.</given-names></name></person-group><etal>et al</etal> (<year>1991</year>) Direct transfer of learned information among neural networks. In <italic>Proceedings of the Ninth National Conference on Artificial Intelligence - Volume 2</italic>, AAAI’91. AAAI Press, Anaheim, USA, pp. <fpage>584</fpage>–<lpage>589</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pyysalo</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>Comparative analysis of five protein–protein interaction corpora</article-title>. <source>BMC Bioinformatics</source>, <volume>9</volume>, <fpage>S6</fpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B44">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Pyysalo</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) Distributional semantics resources for biomedical text processing. In <italic>Proceedings of LBM 2013</italic>, Database Center for Life Science, Tokyo, pp. <fpage>39</fpage>–<lpage>44</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qi</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>The E3 ubiquitin ligase Siah2 contributes to castration–resistant prostate cancer by regulation of androgen receptor transcriptional activity</article-title>. <source>Cancer Cell</source>, <volume>23</volume>, <fpage>332</fpage>–<lpage>346</lpage>.<pub-id pub-id-type="pmid">23518348</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B46">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Quirk</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Poon</surname><given-names>H.</given-names></name></person-group> (<year>2017</year>). Distant supervision for relation extraction beyond the sentence boundary. In <italic>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</italic>, Association for Computational Linguistics, Valencia, Spain, pp. <fpage>1171</fpage>–<lpage>1182</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B47">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Riedel</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) <chapter-title>Modeling relations and their mentions without labeled text</chapter-title> In: <person-group person-group-type="editor"><name name-style="western"><surname>Balcázar</surname><given-names>J.L.</given-names></name></person-group><etal>et al</etal> (eds.) <source>Machine Learning and Knowledge Discovery in Databases</source>. 
<publisher-name>Springer</publisher-name>, 
<publisher-loc>Berlin, Heidelberg</publisher-loc>, pp. <fpage>148</fpage>–<lpage>163</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B48">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Sanh</surname><given-names>V.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) Distilbert, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter. In: <italic>The 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing</italic>, VancouverBC, Canada.</mixed-citation>
    </ref>
    <ref id="btaa430-B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schaefer</surname><given-names>C.F.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>PID: the Pathway Interaction Database</article-title>. <source>Nucleic Acids Res</source>., <volume>37</volume>, <fpage>D674</fpage>–<lpage>679</lpage>.<pub-id pub-id-type="pmid">18832364</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B50">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Surdeanu</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) Multi-instance multi-label learning for relation extraction. In <italic>Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</italic>, Association for Computational Linguistics, Jeju Island, Korea, pp. <fpage>455</fpage>–<lpage>465</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B51">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Thomas</surname><given-names>P.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) Learning protein–protein interaction extraction using distant supervision. In <italic>Proceedings of Workshop on Robust Unsupervised and Semisupervised Methods in Natural Language Processing</italic>, Association for Computational Linguistics, Hissar, Bulgaria, pp. <fpage>25</fpage>–<lpage>32</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tikk</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>A detailed error analysis of 13 kernel methods for protein–protein interaction extraction</article-title>. <source>BMC Bioinformatics, 14, 12</source>.</mixed-citation>
    </ref>
    <ref id="btaa430-B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Upton</surname><given-names>Z.</given-names></name></person-group><etal>et al</etal> (<year>1999</year>) 
<article-title>Identification of vitronectin as a novel insulin-like growth factor-II binding protein</article-title>. <source>Endocrinology</source>, <volume>140</volume>, <fpage>2928</fpage>–<lpage>2931</lpage>.<pub-id pub-id-type="pmid">10342887</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B54">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Van Landeghem</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Large-scale event extraction from literature with multi-level gene normalization</article-title>. <source>PLoS One</source>, <volume>8</volume>, <fpage>e55814</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">23613707</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B55">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) <chapter-title>Attention is all you need</chapter-title> In: <person-group person-group-type="editor"><name name-style="western"><surname>Guyon</surname><given-names>I.</given-names></name></person-group><etal>et al</etal> (eds.) <source>Advances in Neural Information Processing Systems</source>, <italic>Annual Conference on Neural Information Processing Systems 2017</italic>, Long Beach, CA, USA<volume>, pp. </volume><fpage>5998</fpage>–<lpage>6008</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B56">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Verga</surname><given-names>P.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) Simultaneously self-attending to all mentions for full-abstract biological relation extraction. In <italic>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</italic>, Association for Computational Linguistics, New Orleans, Louisiana, pp. <fpage>872</fpage>–<lpage>884</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B57">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Weber</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2020</year>) 
<article-title>HUNER: improving biomedical NER with pretraining</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>295</fpage>–<lpage>302</lpage>.<pub-id pub-id-type="pmid">31243432</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B58">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wei</surname><given-names>C.-H.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>PubTator central: automated concept annotation for biomedical full text articles</article-title>. <source>Nucleic Acids Res</source>., <volume>47</volume>, <fpage>W587</fpage>–<lpage>W593</lpage>.<pub-id pub-id-type="pmid">31114887</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B59">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) Google’s neural machine translation system: Bridging the gap between human and machine translation. <italic>preprint arXiv:1609.08144.</italic></mixed-citation>
    </ref>
    <ref id="btaa430-B60">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xin</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>High-performance web services for querying gene and variant annotation</article-title>. <source>Genome Biol</source>., <volume>17</volume>, <fpage>91</fpage>.<pub-id pub-id-type="pmid">27154141</pub-id></mixed-citation>
    </ref>
    <ref id="btaa430-B61">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Zeng</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>). Distant supervision for relation extraction via piecewise convolutional neural networks. In <italic>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</italic>, Association for Computational Linguistics, Lisbon, Portugal, pp. <fpage>1753</fpage>–<lpage>1762</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa430-B62">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2002</year>) 
<article-title>Dynamic regulation of cyclooxygenase-2 promoter activity by isoforms of CCAAT/enhancer-binding proteins</article-title>. <source>J. Biol. Chem</source>., <volume>277</volume>, <fpage>6923</fpage>–<lpage>6928</lpage>.<pub-id pub-id-type="pmid">11741938</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
