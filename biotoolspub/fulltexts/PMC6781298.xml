<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6781298</article-id>
    <article-id pub-id-type="publisher-id">3049</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-019-3049-1</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title><sc>AIKYATAN</sc>: mapping distal regulatory elements using convolutional learning on GPU</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Fang</surname>
          <given-names>Chih-Hao</given-names>
        </name>
        <address>
          <email>fang150@purdue.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Theera-Ampornpunt</surname>
          <given-names>Nawanol</given-names>
        </name>
        <address>
          <email>nawanol.t@phuket.psu.ac.th</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Roth</surname>
          <given-names>Michael A.</given-names>
        </name>
        <address>
          <email>maroth96@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Grama</surname>
          <given-names>Ananth</given-names>
        </name>
        <address>
          <email>ayg@purdue.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3651-6362</contrib-id>
        <name>
          <surname>Chaterji</surname>
          <given-names>Somali</given-names>
        </name>
        <address>
          <email>schaterji@schaterji.io</email>
        </address>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1937 2197</institution-id><institution-id institution-id-type="GRID">grid.169077.e</institution-id><institution>Department of Ag. and Biological Engineering, </institution><institution>Purdue University, </institution></institution-wrap>West Lafayette, IN USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0470 1162</institution-id><institution-id institution-id-type="GRID">grid.7130.5</institution-id><institution>College of Computing, Prince of Songkla University, </institution></institution-wrap>Bangkok, Thailand </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.420451.6</institution-id><institution>Google Inc., </institution></institution-wrap>Mountain View, California USA </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1937 2197</institution-id><institution-id institution-id-type="GRID">grid.169077.e</institution-id><institution>Department of Ag. and Biological Engineering, Purdue University, </institution></institution-wrap>Purdue University, IN USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>7</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>7</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <elocation-id>488</elocation-id>
    <history>
      <date date-type="received">
        <day>2</day>
        <month>3</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>22</day>
        <month>8</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">The data deluge can leverage sophisticated ML techniques for functionally annotating the regulatory non-coding genome. The challenge lies in selecting the appropriate classifier for the specific functional annotation problem, within the bounds of the hardware constraints and the model’s complexity. In our system <sc>Aikyatan</sc>, we annotate distal epigenomic regulatory sites, e.g., enhancers. Specifically, we develop a binary classifier that classifies genome sequences as distal regulatory regions or not, given their histone modifications’ combinatorial signatures. This problem is challenging because the regulatory regions are distal to the genes, with diverse signatures across classes (e.g., enhancers and insulators) and even within each class (e.g., different enhancer sub-classes).</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We develop a suite of ML models, under the banner <sc>Aikyatan</sc>, including SVM models, random forest variants, and deep learning architectures, for distal regulatory element (DRE) detection. We demonstrate, with strong empirical evidence, deep learning approaches have a computational advantage. Plus, convolutional neural networks (CNN) provide the best-in-class accuracy, superior to the vanilla variant. With the human embryonic cell line H1, CNN achieves an accuracy of 97.9% and an order of magnitude lower runtime than the kernel SVM. Running on a GPU, the training time is sped up 21x and 30x (over CPU) for DNN and CNN, respectively. Finally, our CNN model enjoys superior prediction performance vis-‘a-vis the competition. Specifically, <sc>Aikyatan</sc>-CNN achieved 40% higher validation rate versus CSIANN and the same accuracy as RFECS.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">Our exhaustive experiments using an array of ML tools validate the need for a model that is not only expressive but can scale with increasing data volumes and diversity. In addition, a subset of these datasets have image-like properties and benefit from spatial pooling of features. Our <sc>Aikyatan</sc> suite leverages diverse epigenomic datasets that can then be modeled using CNNs with optimized activation and pooling functions. The goal is to capture the salient features of the integrated epigenomic datasets for deciphering the distal (non-coding) regulatory elements, which have been found to be associated with functional variants. Our source code will be made publicly available at: <ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/cellsandmachines/aikyatan.">https://bitbucket.org/cellsandmachines/aikyatan.</ext-link></p>
      </sec>
      <sec>
        <title>Electronic supplementary material</title>
        <p>The online version of this article (10.1186/s12859-019-3049-1) contains supplementary material, which is available to authorized users.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Support vector machines (SVM)</kwd>
      <kwd>Random forest (RF)</kwd>
      <kwd>Deep neural networks (DNN)</kwd>
      <kwd>ConvNets (CNN)</kwd>
      <kwd>Image processing algorithms</kwd>
      <kwd>Distal regulatory elements</kwd>
      <kwd>Enhancers</kwd>
      <kwd>Silencers</kwd>
      <kwd>Epigenomics</kwd>
      <kwd>ROC curves</kwd>
      <kwd>Graphics processing units (GPU)</kwd>
      <kwd>Hyperparameter tuning</kwd>
      <kwd>NIH roadmap epigenomics</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id>
            <institution>National Institutes of Health</institution>
          </institution-wrap>
        </funding-source>
        <award-id>1R01AI123037-02</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Eukaryotic chromosomes comprise of mosaics of accessible (euchromatin) and inaccessible (heterochromatin) domains whose regulation is controlled by regulatory elements such as promoters, enhancers, and silencers. Further, it is estimated that the human genome contains approximately 20,000 to 25,000 genes representing only 2% of the genomic sequence, while 98% of the genome is non-coding. The non-coding genome includes maintenance elements (e.g., centromeres and telomeres) and origins of replication that control DNA repair and replication processes; regulatory elements such as promoters, enhancers, silencers, insulators; and regulatory RNAs (micro-RNAs), which regulate the spatial, temporal, and cell-type specific expression of genes. Thus, transcriptional regulation of genes is a complex orchestration, subject to DNA folding mechanisms and feedback regulatory controls. The regulatory controls are accomplished not only by proximal promoters, but also by distal regulatory elements, such as, enhancers, superenhancers or stretch enhancers, insulators, and silencers [<xref ref-type="bibr" rid="CR1">1</xref>]. Promoters initiate the transcription process at the transcription start site (TSS), mediated by transcription factors (TFs) and other chromatin-modifying enzymes. Enhancers upregulate gene expression in a distance- and orientation-independent manner. They do so by displaying binding sites for ubiquitous and cell-specific TFs and “looping” to get situated closer to the genes that they target for regulation at that point of space and time [<xref ref-type="bibr" rid="CR2">2</xref>]. Thus, enhancers can be separated from the promoters that they regulate by thousands of base pairs, often situated on different chromosomes, and are drawn close to the transcription factories or active chromatin hubs during gene activation. Further, there are insulators that can restrict the long-range regulation of genomic enhancers and silencers (barriers), conceptualized as specialized derivatives of promoters [<xref ref-type="bibr" rid="CR3">3</xref>], and potentially acting in either capacity, as dictated by the biological process [<xref ref-type="bibr" rid="CR4">4</xref>]. The fact that these distal regulatory elements (DREs) lack common sequence features and often reside far away from their target genes has made them difficult to identify. Further, the annotation of the non-coding genome is an active research area, with discoveries in epigenomic regulatory elements uncovering functional features of DNA (epigenomic marks such as histone modifications, DNA methylation, and genome folding) associated with gene regulatory domains, in myriad cell types and organisms [<xref ref-type="bibr" rid="CR5">5</xref>–<xref ref-type="bibr" rid="CR7">7</xref>]. <disp-quote><p>In <sc>AIKYATAN</sc>, we solve the problem of predicting distal regulatory elements from the DNA sequences, captured by histone modifications, in the vicinity of p300 co-activator binding sites in the DNA.</p></disp-quote></p>
    <p>We wish to annotate distal regulatory elements (DREs)—located distal, in a two-dimensional sense, to the genes that they regulate—comprising of enhancers, insulators, locus-control regions, and silencing elements. While the last decade has seen rapid progress in the development of experimental techniques to identify these regulatory elements on a genome-wide scale, the characterization of the epigenomic features that confer regulatory power to these regions is limited [<xref ref-type="bibr" rid="CR8">8</xref>–<xref ref-type="bibr" rid="CR10">10</xref>]. Of these studies, the focus has primarily been on enhancers, and to some extent, on insulators, which contribute to cell-type specific gene expression in distinct ways. Thus, we wish to increase the scope of predictive algorithms to extensively annotate the varied types of long-range regulatory elements, “learning” their combinatorial histone modification signatures. This superset can then be pipelined into a more specific classifier, such as one for identifying enhancers, e.g., EP-DNN [<xref ref-type="bibr" rid="CR11">11</xref>], to tease out genomic enhancers from this superset of DREs. Further, the residual DREs can then be clustered into other kinds of long-range regulators by unraveling their unique signatures using unsupervised learning or interpretable algorithms, such as [<xref ref-type="bibr" rid="CR12">12</xref>]. Interpretable algorithms, in this problem, can be advantageous because interpretability will result in possible listing of feature-importance scores for different histone modifications and TFs that result in precise and computationally efficient predictions for target DREs. This can enable the identification of newer types of DREs, given that the preprocessing step would decrease some of the noise in the data sets that we started with. Many types of ML techniques have been applied for classification problems in epigenomics, where the data has the characteristics of being both noisy [<xref ref-type="bibr" rid="CR13">13</xref>] and multi-dimensional [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR15">15</xref>]. <bold>We build a fast and accurate classifier for answering the binary question of whether a genomic sequence is a distal regulatory element or not, while taking into consideration the following criteria when building our classifier.</bold>
<list list-type="bullet"><list-item><p><bold>Computational complexity of the ML model:</bold> The chosen ML model should be able to process high data volumes with a large number of training examples (<italic>n</italic>), with the additional constraint of inpterpolating for incompleteness and interpreting high-dimensional features (<italic>d</italic>), the often cited curse of dimensionality, which is ingrained in (epi)genomic data sets. Otherwise, one has to use either feature selection or dimensionality reduction on the original input space in order to reduce <italic>d</italic>, using a method similar to [<xref ref-type="bibr" rid="CR12">12</xref>], or sub-sampling the training set for learning, potentially obfuscating the real data distribution. For example, the distribution of genomic data sets is often found to be skewed normal due to the fact that there may be a small class of genes that demonstrate a high level of connectivity in biological networks forming “network hubs” [<xref ref-type="bibr" rid="CR16">16</xref>], while the more ubiquitous specialized genes control a smaller subset of biological processes, forming smaller networks and participating in fewer of those as well.</p></list-item><list-item><p><bold>Learning the structure of the data:</bold> The chosen ML model should be able to extract knowledge from the structure of the data, which in this domain has a three-dimensional contour offering a complexity similar to that encountered in computer-vision problems. Otherwise, more often than not, a lower-complexity model may introduce unacceptable bias in the learning. We find this empirically for our linear SVM variant of <sc>Aikyatan</sc>, which is mitigated through the use of the kernel variant, as we have seen in other problems in the epigenomic annotation space [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR18">18</xref>]. In the same vein, a simple ANN-based model when converted to a deeper model resulted in a 12% increase in our prediction accuracy in a related epigenomics classification problem that we solved recently, classifying genomic sequences as targets of non-coding regulatory RNA [<xref ref-type="bibr" rid="CR17">17</xref>]. Thus, in most cases, we find that with some loss in interpretability, a non-linear model can handle epigenomic datasets more accurately [<xref ref-type="bibr" rid="CR19">19</xref>–<xref ref-type="bibr" rid="CR21">21</xref>].</p></list-item></list></p>
    <p>Among all types of classifiers, Support Vector Machines (SVM) are robust inferencing machines requiring minimal parameter choices that can be generalized into higher-dimensional spaces using kernel methods. If the data in the input space is linearly separable, then a linear SVM guarantees perfect separation, else a non-linear kernel, such as a Radial Basis Function (RBF) kernel, SVM is recommended. Another approach to increase the prediction performance is to use ensemble methods. Random forest is a popular method in this category and has been proven to be useful for prevent overfitting. [<xref ref-type="bibr" rid="CR22">22</xref>]. However, the memory and the inference time grows as a function of number of training samples [<xref ref-type="bibr" rid="CR23">23</xref>], preventing random forest from being widely used in large-scale data analysis. Looking at the large volumes of data available in our problem domain, plus the additional high-dimensionality attribute [<xref ref-type="bibr" rid="CR20">20</xref>], neural networks coupled with GPU backends, felt like the natural alternative. With this in mind, we consider both vanilla Deep Neural Networks (DNN) and Convolutional Neural Networks (CNN) in this work. In recent years, CNNs [<xref ref-type="bibr" rid="CR21">21</xref>–<xref ref-type="bibr" rid="CR24">24</xref>] have demonstrated success in computer vision, especially in image-classification and recognition tasks. The computer vision applications of CNNs stem from the design of CNNs being highly correlated to the structure of images and their ability to pool the parameters of the image using kernels or filters resulting in data-compression, and to some extent, denoising of the images. Our input can be regarded as snapshots of chromatin signals at specific DNA locations, encoding the spatial notion of the genome into our learning, which enables a CNN to extract distinctive features locally. Thus, a CNN is hypothesized to be effective in extracting image-like input features from the two-dimensional combinatorial histone modification signatures’ data.</p>
    <p>With the above discussion as a guiding principle, we selected a suite of ML protocols under the banner <sc>AIKYATAN</sc><xref ref-type="fn" rid="Fn1">1</xref>, e.g., linear SVM, kernel SVM, random forest, DNN, and CNN, specifically targeted to this problem and using chromatin-based features, namely, 24 histone modifications’ signatures as feature set, for the classification task.</p>
    <p>We train our models on p300 co-activator binding sites; H1-specific, transcription factor binding sites (TFBS): NANOG, OCT4, and SOX2; and uncondensed, cleavage-sensitive, DNase I Hypersensitivity Regions (DHS); which are all distal to TSS; as positive examples. The TSS sites and random genomic regions, which are known to be distal to DHS, are treated as negative samples. Our empirical results show that linear SVM, while being computationally tractable, gives poor accuracy, indicating the bias in the linear model underfitting to the non-linearity embedded in the data. Random forest achieves similar performance compared to linear SVM but still could not outperform kernel SVM and DNN-based models with increasing training set sizes. Kernel SVM significantly improves the accuracy but slows down significantly to the point where it is not feasible to use for a significant-sized genomic dataset. <italic>For example, in our case, with a 1 GB training set size (226k samples), kernel SVM takes ≈ 4.5 days to compute. Consider that by comparison the total dataset for the histone modifications signatures’ feature file alone, after data preprocessing, for the human embryonic stem cell line H1, as available from the NIH Roadmap Epigenomics Mapping Consortium [</italic><xref ref-type="bibr" rid="CR52">52</xref>], is 133 GB (30M samples). Further, the computational cost when performing parameter tuning through cross-validation is multiplied by the number of parameter combinations. A DNN achieves comparable accuracy to a kernel SVM, albeit, with a higher training set size, which is readily available in our problem domain. <italic>As a strong point for deep learning algorithms, their training time grows linearly with training set size while the training time for kernel SVM is between a quadratic and cubic function of the number of training samples. In addition, the testing time for kernel SVM grows linearly with number of support vectors.</italic> Quantitatively, we find that on a CPU backend, the DNN model is 29.x faster than kernel SVM. Finally, in <sc>AIKYATAN</sc>, we find that CNN is best-in-class and we call it <bold>DRP-CNN</bold> (<bold>D</bold>istal <bold>R</bold>egulatory Site <bold>P</bold>rediction using <bold>C</bold>onvolutional <bold>N</bold>eural <bold>N</bold>etworks). We find that CNN performs better than DNN in terms of prediction accuracy. With the largest training set of 16 GB (3.6M samples), CNN achieves 2% higher than DNN for the Validation-Rate experiment, even though for the latter, the DNN model had already reached a 95.8% prediction accuracy. In addition, our empirical results also show that <bold>DRP-CNN</bold> enjoys superior prediction performance vis-à-vis the state-of-the-art methods most pertinent to our problem, e.g., CSIANN [<xref ref-type="bibr" rid="CR30">30</xref>] and RFECS [<xref ref-type="bibr" rid="CR31">31</xref>]. Note that these methods solve a simpler task of enhancer prediction while <sc>AIKYATAN</sc> predicts all distal regulatory elements.</p>
    <p>To further reduce the computational burden of training, we use Graphics Processing Units (GPUs) for our task, as they can significantly reduce the running time required by CPU-based implementations due to their higher memory bandwidth and computational capability. This allows for more elaborate parameter optimization, critical to the success of deep learning models. For our experiments, we use Keras [<xref ref-type="bibr" rid="CR26">26</xref>] as the frontend and Theano [<xref ref-type="bibr" rid="CR27">27</xref>] as the backend. With GPU enabled, the training time is sped up by 21x and 30x over a CPU, for DNN and CNN, respectively.</p>
    <p>Main Contributions: 
<list list-type="order"><list-item><p>We motivate the use of deep learning variants for our problem of predicting which genomic sequences represent DREs and show how to build an ML classifier based on a Convolutional Neural Network (RP-CNN) for this biologically important use case. Specifically, we demonstrate how we formulate histone modification signals as snapshots and demonstrate how local feature extraction and the shift-invariant property of a CNN can apply to histone modification signals and combinatorial functions. This illustrates the applicability of CNNs to biological data that is distinct from typical image datasets. Our empirical results support our hypothesis and show that CNN is the best model to capture these epigenomic patterns, achieving a validation rate of 97.9% or higher. An added benefit is the local connectivity and parameter sharing property of CNNs resulting in dramatic reduction in the number of parameters, while also contributing to the shift invariance property. This is important in the genomics context given the steep rise in the volume and variety of datasets.</p></list-item><list-item><p>We show that a linear SVM and random forest are not expressive enough for the classification task and a kernel SVM, although theoretically powerful, cannot achieve as high a test accuracy as deep learning approaches due to its high training time complexity. This attribute limits the kernel SVM model from learning all the underlying patterns and nuances present in the entire data set, with this complexity increasing with the perpetual increase in data volumes and varieties in genomics, rightly described as a four-headed beast because of the complexity in data acquisition, storage, distribution, and analysis [<xref ref-type="bibr" rid="CR30">30</xref>].</p></list-item><list-item><p>We show how a GPU-backend for the deep learning task speeds up the training process and makes it feasible to deploy our algorithmic variants for high-throughput processing on a large-sized, biologically relevant data set. In addition, we give comprehensive empirical results on the comparison of both training and testing times for <sc>Aikyatan</sc> as well as state-of-the-art methods, such as RFECS [<xref ref-type="bibr" rid="CR31">31</xref>], and CSIANN [<xref ref-type="bibr" rid="CR30">30</xref>], on CPUs.</p></list-item></list></p>
    <sec id="Sec2">
      <title>Related Works</title>
      <p>In this subsection, we discuss recent works on predicting a specific set of distal regulatory elements, enhancers, which is the closest classification task to our problem at hand. We select two leading-edge ML models that are representative of two different classes of ML algorithms, random forest (RFECS) and artificial neural networks with Fisher discriminant analysis (CSIANN), respectively for detailed benchmarking of <sc>AIKYATAN</sc>. However, it is important to note that <sc>AIKYATAN</sc> solves the larger classification problem of whether or not genomic sequences are distal regulatory elements or not, rather than the classification of genomic enhancers that all of these competitors solve. Of these, enhancers are the most versatile in their usage in the genome and by current estimates, there are over a million enhancers in the genome. We believe solving the overall distal regulatory element problem is useful because it is allows us to classify (downstream) different classes of distal regulatory elements rather than annotating genomic enhancers alone.</p>
      <p>RFECS (Random forest-based Enhancer identification from Chromatin States) [<xref ref-type="bibr" rid="CR31">31</xref>] proposed a random forest algorithm to predict p300 enhancers from combinatorial patterns of histone modifications in H1 and IMR90 cell types. CSIANN [<xref ref-type="bibr" rid="CR30">30</xref>] used Fisher Discriminant Analysis for Feature Extraction/Reduction and use 1 hidden layer Time-delay neural network for Enhancer Prediction. DEEP [<xref ref-type="bibr" rid="CR32">32</xref>] utilized a framework of an ensemble of kernel SVMs with an overarching artificial neural network (ANN) to predict enhancers in different cell types using features derived from histone modifications and sequence characteristics. More recently, EP-DNN [<xref ref-type="bibr" rid="CR11">11</xref>] used a DNN, demonstrating superior accuracy of 91.6%, relative to 85.3% for DEEP-ENCODE and 85.5% for RFECS. PEDLA (predicting enhancers with a deep learning-based algorithmic framework) [<xref ref-type="bibr" rid="CR33">33</xref>] implemented a Hidden Markov Model (HMM) with DNN-based probabilities to predict enhancers from distinct categories of heterogeneous data, including histone modifications, TFs’ and co-activators’ binding, DNA methylation, sequence characteristics, etc., and achieved superior performance compared to the previous approaches. PEDLA’s key novelty comes from the integration of diverse datasets rather than the process of extracting maximum signal from a single combinatorial epigenomic dataset (e.g., histone modifications’ signatures for <sc>AIKYATAN</sc>. In our study, we are investigating the learning capability and computational efficiency of machine learning models on a large and diverse epigenomics dataset. For this purpose, we include all distal regulatory elements (e.g., silencers, promoters, and insulators) as positive samples. This is a more challenging problem because of the diversity of the different types of regulatory elements. We are also the first to identify that CNN is the best-of-breed classifier for this problem domain, due to its ability to capture the spatial abstractions from the input. Further, none of the prior approaches quantify the speedup of the various classifiers, relatively on CPU and GPU; ease of speed up by accelerators being an attractive attribute of neural networks. Finally, fast training, while less critical in some domains, is important in genomics because of the need to quickly retrain algorithms, with newer datasets, a product of rapid technology advances.</p>
    </sec>
    <sec id="Sec3">
      <title>ML Background</title>
      <p>The SVM paradigm, originally designed for such binary classification problems, has an impressive geometrical interpretation of discriminating one class from another in a multidimensional input space using a maximum-margin hyperplane [<xref ref-type="bibr" rid="CR34">34</xref>]. It is commonly known that the SVM paradigm is amenable with the regularization framework, where we have a data fit component ensuring the model’s fidelity to the data, on the one hand, and a penalty component enforcing the model’s simplicity on the other [<xref ref-type="bibr" rid="CR35">35</xref><italic>,</italic><xref ref-type="bibr" rid="CR36">36</xref>]. The SVM methodology is based on a solid theoretical foundation, with the core of the algorithm being a quadratic programming problem, separating support vectors (“supporting” the decision boundary) from the rest of the training examples. Specifically, a linear SVM finds a linear decision boundary with the maximum-margin separation between the support vectors of the two classes in the dataset. Further, for overcoming model bias in a complex data landscape, SVMs can be versatile, deploying different kernel functions. While commonly used kernels are polynomial, radial basis function (RBF), and sigmoid, custom kernels can also be defined using a Gram matrix (Gramian matrix or Gramian), which is a Hermitian matrix of inner products. In essence, a kernel SVM projects data from the input space into an higher-dimensional feature space (can be infinite-dimensional). The exact form of the function is determined by the type of kernel used, we use the RBF kernel. Kernel SVM then finds the hyperplane in that space to “linearly” separate the positive and negative training examples. Theoretically, it is always possible to find such a perfect hyperplane in the infinite-dimensional space. However, empirically, finding the best hyper-parameters is time consuming and even a single run of the kernel SVM algorithm can range in complexity from <italic>O</italic>(<italic>n</italic><sup>2</sup>) to <italic>O</italic>(<italic>n</italic><sup>3</sup>) [<xref ref-type="bibr" rid="CR37">37</xref>], which makes it prohibitive for training a large dataset. Thus, while SVMs are powerful tools, their compute and storage requirements increase rapidly with the increase in the number of training vectors.</p>
      <p>Another approach to build a robust classifier is through ensemble methods. Instead of learning a single strong classifier, ensemble methods try to construct multiple weak learners to solve the same problem, random forest being a prominent member of this family. Random forest been proven to be more resilient to overfitting because of its technique of compiling multiple weak learners. However, they are not suitable for large-scale datasets since the memory requirement grows as the function of training set size [<xref ref-type="bibr" rid="CR41">41</xref>]. Recently, deep learning [<xref ref-type="bibr" rid="CR38">38</xref>] has emerged as a powerful tool in the machine learning community, abetted by the volumes and diverse types of datasets. Several theoretical studies [<xref ref-type="bibr" rid="CR39">39</xref><italic>–</italic><xref ref-type="bibr" rid="CR44">44</xref>] have shown that deep learning approaches are able to learn high-level abstractions from data using architectures consisting of multiple layers of non-linear processing units using a variety of activation functions, with the increasing depth of the layers increasing the power of abstraction, but only up to a certain point. Although the success of DNNs is attributed to the hierarchy introduced by the hidden layers making it sort of a hierarchical processing pipeline, there is a tradeoff between the accuracy achieved by this process and the time taken to train this network. Common examples of the areas in which deep learning has been successful include image classification [<xref ref-type="bibr" rid="CR21">21</xref>], automatic speech recognition [<xref ref-type="bibr" rid="CR47">47</xref>], and natural language processing [<xref ref-type="bibr" rid="CR48">48</xref>]. More recently, deep learning has gained traction in several prediction problems in bioinformatics, such as structural predictions of proteins [<xref ref-type="bibr" rid="CR49">49</xref>] and of RNA-binding protein targets [<xref ref-type="bibr" rid="CR50">50</xref>], of RNA splicing predictions [<xref ref-type="bibr" rid="CR51">51</xref>], and of genome annotations with some degree of interpretability [<xref ref-type="bibr" rid="CR54">54</xref>]. We select two variants of DNNs, the vanilla Deep Neural Network (DNN) architecture and Convolutional Neural Networks (CNN), to solve the distal regulatory element prediction problem.</p>
    </sec>
  </sec>
  <sec id="Sec4" sec-type="results">
    <title>Results</title>
    <p>We preface this section with our evaluation metrics and data-preprocessing techniques.</p>
    <sec id="Sec5">
      <title>Performance Metrics and Data Preprocessing</title>
      <p>In this subsection, we start with the benchmarking metrics used for the tools we developed in <sc>AIKYATAN</sc>. We then move on to the data preprocessing techniques that were used to refine the raw data to be used as inputs to our classifiers. <italic>Performance Metrics</italic>: We use Validation Rate (VR) to benchmark <sc>AIKYATAN</sc>. The standard benchmarking metrics, e.g. precision-recall (PR) metrics are not valid here since there are regulatory sites that have not been experimentally mapped, beyond p300, NANOG, SOX2, OCT4 binding sites or TSS. In order to use the more conventional benchmarking metrics, we would have to evaluate performance on all these sites that are unaccounted for, which are not experimentally mapped and hence unknown. This is in line with the observation made by other enhancer prediction methods such as RFECS and EP-DNN [<xref ref-type="bibr" rid="CR11">11</xref>]. Thus, we needed to modify the performance metrics used in <sc>AIKYATAN</sc>.<xref ref-type="fn" rid="Fn2">2</xref> RFECS the validation rate (VR) metric by validating a classification against criteria mentioned below. If a location has histone modification enrichment signatures similar to that of an enhancer (or a distal regulatory site in our case) and a prediction is made at that location, we can say that the classification is valid, given that it is located sufficiently close to a known enhancer marker or to an open chromatin site, which means that the site can be exposed to regulatory factors. Following are the criteria designed for VR-based evaluation of the algorithms. 
<list list-type="bullet"><list-item><p>If a classified regulatory site lies within 2.5 kb of a true positive marker (TPM), then the classification is “validated”, which we also refer to as “gray area”. This is because this site is either a known (experimentally validated) regulatory site, or an unknown regulatory site that overlaps with open chromatin and we can assume to be a regulatory site.</p></list-item><list-item><p>Otherwise, it is deemed “invalid”, which means it is either a TSS or an Unknown and we know for a fact that the prediction is incorrect.</p></list-item></list></p>
      <p><italic>Data Preprocessing</italic>: Figure <xref rid="Fig1" ref-type="fig">1</xref> represents the process of generating the VR dataset. We found that a large fraction of samples (8 percent) have no histone modification signal, which can be seen as all feature values being zero. Such samples provide no meaningful information for training the models, so we remove them from our training set. In addition, the number of positive and negative samples in the dataset is not balanced, with positives samples being in the minority (22% of the total dataset). To avoid modeling bias in favor of the majority class, we balance datasets by subsampling from the much larger number of negative samples. Figure <xref rid="Fig1" ref-type="fig">1</xref> describes the pipeline for generating training and test sets from the raw histone modifications’ input. To reduce the variance, we divide the test set into five non-overlapping test sets and also generate five overlapping training sets for each training set size. The five training sets are paired with the five test sets with a one-on-one correspondence. The final VR numbers are averaged from five train-test pairs for each experiment.
<fig id="Fig1"><label>Fig. 1</label><caption><p>The pipeline for generating Training and Test Sets for VR dataset</p></caption><graphic xlink:href="12859_2019_3049_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec6">
      <title>Empirical Results</title>
      <p>We designed experiments to evaluate the training time and prediction accuracy for the different classifiers in <sc>AIKYATAN</sc>. The machines’ specifications are listed in Table <xref rid="Tab1" ref-type="table">1</xref>. We used Keras [<xref ref-type="bibr" rid="CR26">26</xref>] as the frontend, with Theano [<xref ref-type="bibr" rid="CR27">27</xref>] at the backend, to develop our deep learning models. Thus, our infrastructure runs on a Python backend, which is advantageous for ML algorithms as it can benefit from the rapid progress in Python libraries, compared to the development in Matlab or C/C++.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Computational specifications of machines used for the experiments</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Environment</th><th align="left">CPU</th><th align="left">Number of Cores</th><th align="left">GPU</th></tr></thead><tbody><tr><td align="left">GPU machine</td><td align="left">Intel(R) Core(TM) i7-2600 CPU @ 3.40 GHz</td><td align="left">4</td><td align="left">NVIDIA Telsa K40</td></tr><tr><td align="left">CPU machine</td><td align="left">Intel(R) Xeon(R) X3430 CPU @ 2.40 GHz</td><td align="left">4</td><td align="left">N/A</td></tr><tr><td align="left">GPU machine</td><td align="left">Intel(R) Xeon(R) Platinum 8168 CPU @ 2.70GHz</td><td align="left">24</td><td align="left">NVIDIA P100</td></tr></tbody></table></table-wrap></p>
      <sec id="Sec7">
        <title>
          <bold>Deep learning models demonstrate faster computation time even on CPU</bold>
        </title>
        <p>Without a doubt, it is important that a prediction model should give us superior prediction accuracy. However, we also have to take the computation time into serious consideration when choosing a prediction model. Both training and testing times are important metrics for any ML algorithm though traditionally testing time has been considered the more important of the two. However, in the genomics domain, where volumes of new datasets are becoming available, the model will have to be retrained to update itself on a regular basis and therefore we are also interested in the training times. We measure the training time and testing time as a function of training set size for <sc>AIKYATAN</sc> on the CPU machine. Figure <xref rid="Fig2" ref-type="fig">2</xref>a shows the average training times of the five classifiers with various training set sizes. Random forest exhibits <italic>O</italic>(<italic>n</italic><italic>l</italic><italic>o</italic><italic>g</italic>(<italic>n</italic>)), where <italic>n</italic> denotes the number of training samples, training time complexity. Linear SVM, CNN, and DNN, have training time algorithmic complexity of approximately <italic>O</italic>(<italic>n</italic>), while for kernel SVM with RBF kernel, it is between <italic>O</italic>(<italic>n</italic><sup>2</sup>) and <italic>O</italic>(<italic>n</italic><sup>3</sup>) [<xref ref-type="bibr" rid="CR37">37</xref>]. For our specific parameter for the misclassification penalty, this is found to be <italic>O</italic>(<italic>n</italic><sup>2.2</sup>). We find empirically that the training time follows the relation linear SVM &lt; random forest &lt; DNN &lt; CNN ≪ kernel SVM. With the largest training set size in this experiment, 1,000 MB (226k samples), kernel SVM’s training phase takes around 50.5 hours, which is 255.6x, 161.8x, 9.0x, and 16.1x slower than the linear SVM, random forest, CNN, and DNN, respectively. Figure <xref rid="Fig2" ref-type="fig">2</xref>b shows the average testing times of the 5 classifiers with various training set sizes. For most ML models, training set size does not affect time required for testing. This is evident from the results for the linear SVM, DNN, and CNN models. However, the testing times for the kernel SVM and random forest <italic>do increase</italic> with training set size Figure <xref rid="Fig2" ref-type="fig">2</xref>c. For random forest, the prediction time depends on the depth of trees. In an average case, it is of order <italic>Θ</italic>(<italic>m</italic><italic>n</italic>), where <italic>m</italic> is the number of trees. From Fig <xref rid="Fig2" ref-type="fig">2</xref>b, we notice that as the training set size grows to 1000 MB, the prediction time is larger than DNN, CNN, and linear SVM. For kernel SVM, the prediction time grows linearly with the number of SVs, as we show in Fig <xref rid="Fig2" ref-type="fig">2</xref>b. With the training set size of 1000 MB (226k samples), kernel SVM’s testing phase takes around 57.3 hours, which is 136.9x, 71.4x, 76.7x, and 98.9x slower than a linear SVM, random forest, CNN, and DNN, respectively. Thus, although a kernel SVM has superior prediction performance, the prediction times make it impractical to use, as datasets tend to be very large in our problem domain. To summarize, we have shown that when we use CPU for computation, the training <italic>and</italic> testing times of a kernel SVM are much higher than for the other models and the rate of growth in running time is also higher for a kernel SVM. In the case for random forest, although the time requited to construct model is relatively low, the prediction time is higher than other DNN, CNN, and linear SVMs when the size of training set is large.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Comparison runtime for Aikyatan. Figures 2a and 2b show the training and testing times using CPU for the models, with varying training set sizes. As shown in Figure 2<bold>a</bold>, linear SVMs, DNNs, and CNNs training times scale approximately O(n) while random forests traing time grows at the rate of O(nlog(n)) and kernel SVMs training time grows at the rate of O(n2.2), where n denotes the number of training samples. As in Figure 2<bold>b</bold>, linear SVMs, DNNs, and CNNs testing times remained constant, whereas random forests testing time grows with the rate.(mn), where m denotes the number of trees, and kernel SVMs testing time grows rapidly as training size increases, with corresponding increase in SVs. Figure 2<bold>c</bold> shows the relationship between the number of SVs obtained from the training set and the testing time for the kernel SVM. For the kernel SVM, the testing time grows linearly with SVs</p></caption><graphic xlink:href="12859_2019_3049_Fig2_HTML" id="MO2"/></fig></p>
        <p><italic>Computation Cost Comparison for CNN, RFECS, and CSIANN</italic> Here, we compare the training and testing time for CNN with RFECS and CISANN on 16 GB training set (3643k samples). We could not deploy RFECS and CSIANN on the CPU machine (X3430 processor) that we used for the experiments with <sc>AIKYATAN</sc> (specs in Table <xref rid="Tab1" ref-type="table">1</xref>) because of smaller numbers of cores and lower clock rates of the processor. Instead, we ran RFECS and CSIANN methods on the higher-end Platinum 8168 processor with 24 cores. While utilizing all cores on the higher-end CPU, RFECS still takes 45.6 hours for training and 1.78 hours for testing while <sc>AIKYATAN</sc>-CNN takes 9.13 hours for training and 0.27 hours for testing. Thus, RFECS’ training time is about 5X that of ours<xref ref-type="fn" rid="Fn3">3</xref>. For CSIANN, a bottleneck of the model lies in the high computation cost of the inversion of the large matrix, <italic>O</italic>(<italic>d</italic><sup>3</sup>) where <italic>d</italic> is the dimension of features and usually <italic>d</italic>&gt;&gt;1, during the Fisher Discriminant Analysis. We failed to finish the training of CSIANN within a week using CPU. Thus, we put the matrix inversion computation task into a P100 GPU while other computations remain on CPU for CSIANN. After this modification, CSIANN still takes 31 hours for training and 1.5 hours for testing, 3X times slower than our CNN. In summary, CNN modeling takes less time to train than both RFECS and CSIANN and is also easily amenable to speedup by GPUs. For the next experiment, we investigate how much we can speed up both training and testing through the use of a GPU.</p>
      </sec>
      <sec id="Sec8">
        <title>
          <bold>Deep Learning Models can leverage GPU-based accelerators</bold>
        </title>
        <p>The computation in a neural network can be decomposed into multiple matrix operations, which have the Single Instruction Multiple Data (SIMD) characteristic. These operations are therefore well suited for exploiting the parallelism that is available on GPUs. In this experiment, we quantify how much speedup is possible for <sc>AIKYATAN</sc> DNN and CNN variants by executing them on a GPU. We fixed the model architectures and used the same number of training epochs, which is 50, for both DNN and CNN and trained on different training set sizes. In order to train on a larger dataset, we used the datasets used for VR metrics in this experiment. We first examine the speedup ratio of using GPU over CPU. Figure <xref rid="Fig3" ref-type="fig">3</xref>a and b show the training times for DNN and CNN respectively. For DNN, using GPU is 21x faster than using CPU, while for CNN, it is 30x faster. This can be explained by the fact that CNN training involves a greater number of matrix operations for the convolution phase and thus the CNN operations can more effectively leverage all the GPU cores.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Training and Testing times and GPU speedup of DNN and CNN models. Figures 3<bold>a</bold> and 3<bold>b</bold> show the speed-up ratio for DNN and CNN, respectively. The orange line represents the speed-up ratio, training time using CPU divided by training time using GPU, for training set sizes varying from 1 GB to 8 GB. The speed-up ratio remained constant and the speed up is around 21x for DNN and 30x for CNN, respectively. Figures 3<bold>c</bold> and 3<bold>d</bold> shows how training time and testing time grows as training set size increases for DNN and CNN, when deployed on GPU. We fixed DNN and CNN architectures among all training sets and the number of learning epochs to be 50. Both DNN and CNN training times grow linearly when deployed on GPU</p></caption><graphic xlink:href="12859_2019_3049_Fig3_HTML" id="MO3"/></fig></p>
        <p>Next, we examine the training time and testing time for DNN and CNN on GPUs for different training set sizes.</p>
        <p>Figure <xref rid="Fig3" ref-type="fig">3</xref>c and Fig <xref rid="Fig3" ref-type="fig">3</xref>d shows the training and testing time on GPU for DNN and CNN using varying training set sizes from 500 MB (133k samples) to 16 GB (3643k samples). The training and testing time on GPU behaves similar to the training and testing time on CPU for both DNN and CNN in that the training time grows linearly with the training set size and the testing time remains constant no matter how the size of training set size grows. With the largest training set size of 16 GB, DNN takes around an hour and CNN takes 1.27 hours for training on GPU. Regardless of training set sizes, CNN’s training time relative to DNN’s remains constant, at approximately 1.2. CNN’s testing time relative to DNN’s also remains constant and the two are approximately equal.</p>
      </sec>
      <sec id="Sec9">
        <title>
          <bold>CNN achieves superior performance in prediction capability and time compared to state-of-art methods</bold>
        </title>
        <p>First, we show the prediction performance of our CNN with state-of-art methods, e.g., RFECS [<xref ref-type="bibr" rid="CR31">31</xref>] and CSIANN [<xref ref-type="bibr" rid="CR30">30</xref>]. Because of the high dimensionality of the training data, both RFECS and CSIANN managed to make the computation tractable by using only a subset of histone modifications for learning. Furthermore, CISANN reduces the dimensionality of features using Fisher’s Discriminant Analysis (FDA). In contrast, we aim at demonstrating our computational model is not only able to consume high-dimensional data but also able to learn intricate non-linear features from them resulting in higher expressiveness. Toward achieving a fair comparison, we used our dataset (24 histone modifications instead of a subset) and applied it to RFECS and CSIANN. Again, we selected RFECS and CSIANN as two representative leading edge sophisticated models that use similar epigenomics datasets as <sc>AIKYATAN</sc> (as inputs to the model) and known to be sophisticated classifiers while being distinct. Table <xref rid="Tab2" ref-type="table">2</xref> shows the average VR and the standard deviation of VR on a 16 GB training set for CNN, RFECS, and CSIANN. CNN achieved 1% higher VR than RFECS even though it has already achieved a reasonable VR of 96.65%. CSIANN made two simplifications. First, dimensionality-reduction techniques were used so that coarser features were used for the learning process. Second, only one hidden layer was used for its neural network model. With these two simplifications, CSIANN, performed better than random guessing, but was not able to generalize well on our distal regulatory elements’ prediction problem. Finally, CNN is the most insensitive to the changes in dataset, which is shown in Table <xref rid="Tab2" ref-type="table">2</xref>. The standard deviation of VR derived from the five 16 GB datasets is the smallest, compared to RFECS and CSIANN.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>VR Numbers on 16 GB (3643k samples) training set for CNN, RFECS, and CSIANN</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">CNN</th><th align="left">RFECS</th><th align="left">CSIANN</th><th align="left">Random</th></tr></thead><tbody><tr><td align="left">Avg. VR</td><td align="left">0.9786</td><td align="left">0.9665</td><td align="left">0.5715</td><td align="left">0.2265</td></tr><tr><td align="left">Std. VR</td><td align="left">0.0068</td><td align="left">0.0100</td><td align="left">0.1754</td><td align="left">NA</td></tr></tbody></table></table-wrap></p>
        <p>Next, we are also interested in how the performance grows as a function of training set size. We investigate our <sc>AIKYATAN</sc>’s prediction performance with RFECS [<xref ref-type="bibr" rid="CR31">31</xref>]. We do not do further analysis for CSIANN because not only other methods significantly outperform its inference capability but also its high computation cost due. Figure <xref rid="Fig4" ref-type="fig">4</xref> shows the average VR, benchmarking the predictions of <sc>AIKYATAN</sc> vis-à-vis competition. <italic>Algorithmic Performance</italic>: Both kernel and linear SVM achieve a high VR for small training set sizes, outperforming deep learning variants. However, as the training set size becomes larger, the rate of improvement for both linear and kernel SVM is smaller than for deep learning approaches, especially DNN. Further, the variation of DNN performance on smaller datasets is high, indicating that the DNN model is not stable at this size. This phenomenon occurs because of the large number of learning parameters of a DNN. But as the training set grows, the DNN’s performance becomes stable and outperforms linear SVM. Looking at the trend, one would expect that a kernel SVM can achieve higher VR with larger training set sizes. However, due to a kernel SVM’s high computational cost, we could not train the model for a dataset size larger than 1 GB (230k samples) in an acceptable time.
<fig id="Fig4"><label>Fig. 4</label><caption><p>Average VR performance are shown for Aikyatan. To obtain a larger data set size, unlike in RFECS, where the training set only contains peaks, we include gray area into our training set. By varying the threshold that is used to turn the raw real-valued prediction into one of the two classes, we can generate a VR curve where X-axis is the number of samples predicted as positive and Y-axis is the portion of these predicted positive samples that are validated, i.e., the validation rate (VR). In order to compare the prediction performance across the ML models, we control for the same number of predictions across these models. In order to find the specific number of predictions, we obtained the target number of predictions from RFECS where the best validation in its original data set is for around 100K predictions. Since we took 70% of the original data set as the training set and 30% as the test set and further divided test sets into 5 non-overlapping test sets, our target number of predictions becomes 6000 in each sub-sampled test set</p></caption><graphic xlink:href="12859_2019_3049_Fig4_HTML" id="MO4"/></fig></p>
        <p>On the contrary, the lower computational cost of DNN and CNN allows us to train them using increasingly larger training sets as more and more data becomes available for building the model. We find that the VR performance of deep learning approaches continues to improve with increasing training set sizes. Using 16 GB (3643k samples) training sets, DNN can achieve similar VR to a kernel SVM, while CNN can outperform a kernel SVM, requiring less time for both training and testing phases, which we have already quantified in previous experiments. We also test the performance for random forest. As we can see, although random forest is more stable than other methods, it does not increase much prediction performance as training set size grows. When trained on the largest data set, random forest only achieve 94 % validation rate, 1.7 and 3.8 worse than DNN and CNN respectively. RFECS improves the performance for random forest, at the smallest dataset in this experiments it starts with 95% and reach to 96.65%. However, the VR is still at the same level with kernel SVM and 1.2% worse than CNN. Ranking the Algorithms in <sc>AIKYATAN</sc>: To rank average VR performance among the four ML models in <sc>AIKYATAN</sc>, we perform statistical significance tests to compare (1) linear SVM and kernel SVM on 1 GB training sets, (2) kernel SVM on 1 GB training sets versus DNN on 16 GB training sets, (3) kernel SVM on 1 GB training sets versus RFECS on 16 GB training sets, and (3) DNN and CNN on 16 GB training sets. (4) DNN and random forest on 16 GB training sets. For (1), (3), and (4) we use paired one-tailed t-testing since they are trained using the same group of training sets, whereas for (2) and (3), we use unpaired one-tailed t-testing since they use different groups of training sets. We found that all of the p-values are smaller than 0.05, with the exception of case (2) and (3). We conclude that CNN outperforms the other five models; that kernel SVM, RFECS, and DNN are at the same level; DNN outperforms random forest; and that the linear SVM’s performance is the worst because of the bias (underfitting).</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec10" sec-type="discussion">
    <title>Discussion</title>
    <p>Kernel SVM has emerged as a popular general-purpose ML model and has been used successfully in many domains, especially because of its solid theoretical foundations, based on Vapnik–Chervonenkis theory (VC theory [<xref ref-type="bibr" rid="CR34">34</xref>]). The first results in the field of discrimination, exposed in Vapnik and Chervonenkis (1971), dealt with the computation of dichotomies with binary-valued functions. However, Kernel SVM’s major drawback is its high time complexity to train the model, which is a quadratic to cubic function of the number of training samples. This puts a strain on how much data can be used to train the model, which can lead to situations where the learned model is not discriminating enough to capture all the nuances in the data. In the genomics area, increasing amounts of data are becoming available, and therefore, there is the possibility of using larger and larger amounts of training data to improve a classifier’s accuracy. This led us to consider deep learning models for the problem of predicting distal genomic regulatory sites. However, since long training times are a bottleneck for deep learning algorithms, we use GPU accelerators for faster execution of our neural network models. From other domains, such as computer vision applications of image recognition and classification, it is known that CNN converges faster than DNN if the shift invariant property of the pattern holds. We hypothesized that stemming from the three-dimensional folding abilities of a genome and the image-like properties of the histone modification signals, the translational invariance property also holds for our problem. So, we evaluated CNN architectures alongside DNN and verified this fact. Our results hold promise for the use of deep learning approaches for high-dimensional and diverse genomic datasets. While we have used single-node executions here (for both CPU and GPU implementations), it is possible to use distributed deep learning frameworks, such as TensorFlow [<xref ref-type="bibr" rid="CR63">63</xref>] and PyTorch [<xref ref-type="bibr" rid="CR64">64</xref>] as the data volumes and heterogeneity become more substantial. Given that <sc>AIKYATAN</sc> is the first algorithm of its kind classifying DREs, many of which are yet to be studied in detail, we believe our algorithm can reduce the noise and discover patterns in new types of DREs plus capture the nuances in existing classes of DREs, for example, in enhancers and their sub-types.</p>
  </sec>
  <sec id="Sec11" sec-type="conclusion">
    <title>Conclusions</title>
    <p>In this study, we demonstrate how we formulate histone modification signals as snapshots and demonstrate how local feature extraction and the shift-invariant property of a CNN can apply to histone modification signals and combinatorial epigenomic features. Empirical results demonstrate that CNN has superior generalization performance, achieving a validation rate of 97.9% or higher, compared to standard DNN, linear SVM, kernel SVM as well as the state-of-the-art methods, such as CSIANN and RFECS. Moreover, we give empirical results on training and testing times. With GPU enabled, CNN’s training time is sped up by 30x over a CPU. With the largest training set size in training time comparison of <sc>AIKYATAN</sc>, 1,000 MB (226k samples), kernel SVM’s training phase takes around 50.5 hours, which is 255.6x, 161.8x, 9.0x, and 16.1x slower than the linear SVM, random forest, CNN, and DNN, respectively. Overall, taking into account the expressiveness of the ML models and the computational efficiency, we conclude that Distal Regulatory Element prediction task favors CNN due to its high expressiveness and ease of accelerating its computation.</p>
  </sec>
  <sec id="Sec12">
    <title>Methods</title>
    <sec id="Sec13">
      <title>A. Overview</title>
      <p>Figure <xref rid="Fig5" ref-type="fig">5</xref>a, b, and c represent an overview of <sc>AIKYATAN</sc>’s training and testing phases. Our <sc>AIKYATAN</sc> suite includes a linear SVM model, a radial basis function (RBF) kernel SVM model, random forest, and deep learning variants, DNN and CNN for the task of predicting DREs in the human embryonic cell line (H1), a tier 1 ENCODE project cell type. To obtain the feature vector for each genome position, we use histone modification signatures as input features. Our binary classification task then is as follows: given histone modification signatures at genome location <italic>i</italic>, predict whether genome position <italic>i</italic> is a distal regulatory site or not, <italic>i.e.</italic>, distal to promoters or TSSs.
<fig id="Fig5"><label>Fig. 5</label><caption><p>An overview plot describing five machine learning (ML) models training and testing phases. Figure 5<bold>a</bold> describes the training phase for four ML models. Figure 5<bold>b</bold> describes the prediction phase. After having tuned the hyperparameters for each model, we evaluate its performance using the validation-rate (VR) metric. Figure 5<bold>c</bold> describes the legend we use and the hyperparameters tuned for each model</p></caption><graphic xlink:href="12859_2019_3049_Fig5_HTML" id="MO5"/></fig></p>
    </sec>
    <sec id="Sec14">
      <title>B. Epigenomic datasets</title>
      <p>Histone modification signatures: We use 24 histone modifications for our prediction task. The data was obtained from the NCBI database under NCBI GEO accession number GSE16256. The 24 histone modifications are as follows: H2AK5ac, H2BK120ac, H2BK12ac, H2BK15ac, H2BK20ac, H2BK5ac, H3K14ac, H3K18ac, H3K23ac, H3K27ac, H3K27me3, H3K36me3, H3K4ac, H3K4me1, H3K4me2, H3K4me3, H3K56ac, H3K79me1, H3K79me2, H3K9ac, H3K9me3, H4K20me1, H4K5ac, and H4K91ac, in H1, which were generated as a part of the NIH Epigenome Roadmap Project [<xref ref-type="bibr" rid="CR52">52</xref>]. These histone modifications comprise of a superset of all that are hypothesized to be relevant biologically to the presence (or absence) of regulatory sites [<xref ref-type="bibr" rid="CR31">31</xref>]. The ChIP-seq reads of these histone modifications give us their enhancement level. These were binned into 100 base pair (bp) intervals and normalized against their corresponding inputs by using an RPKM (reads per kilobase per million) measure [<xref ref-type="bibr" rid="CR53">53</xref>]. Multiple replicates of histone modifications were used to minimize batch-related differences and the replicates’ RPKM-levels were averaged to produce a single RPKM measurement per histone modification. This averaged RPKM enrichment level of a histone modification is its signature. For any given location, the histone modification signatures within 1000 bp of that location are used as input to the models. A window of 1000 bp incorporates ten 100 bp bins on each side. With 20 bins for each of the 24 histone modifications, the input comprises 480 features in total. Included locations: For training and testing, the positive set includes all the p300 binding sites, cell type-specific Transcription Factor Binding Sites (TFBS) (NANOG, OCT4, and SOX2), and DNase I Hypersensitivity Sites (DHS), which are at least 1000 bp away from the nearest known Transcription Start Site (TSS). Since p300 co-activators, DNase I, and Transcription Factors (TFs) also bind to TSS, which are not distal regulatory elements, we only considered the binding sites that are distal to known TSS sites as positives. The remaining locations were considered as negatives. Narrow DHS peaks were downloaded from UCSC’s ENCODE site. [<xref ref-type="bibr" rid="CR54">54</xref>] The accession numbers: GSE37858, GSE18292, and GSE17917, contain genome-wide binding data for H1 p300, NANOG, OCT4, and SOX2. p300 and TF peaks were determined using the MACS peak-calling software, with default p-value cutoffs. ChIP-seq input files were used as treatment or background.</p>
    </sec>
    <sec id="Sec15">
      <title>C. Machine learning models</title>
      <p>In this work, we selected a suite of ML protocols under the banner <sc>AIKYATAN</sc><xref ref-type="fn" rid="Fn4">4</xref>, e.g., linear SVM, kernel SVM, random forest, DNN, and CNN, specifically targeted to this problem and using chromatin-based features, namely, 24 histone modifications’ signatures as feature set, for the classification task. The description of SVMs, random forest and the corresponding hyperparameter tuning procedure can be found in the Supplementarty materials. A high-level goal of our work is to optimize individual “algorithmic motifs” or “kernels” recurring in computational genomics algorithms and then stitch together an optimized library of kernels for specific genomics applications, as envisioned in the domain-specific library (DSL)—Sarvavid [<xref ref-type="bibr" rid="CR59">59</xref>]</p>
      <sec id="Sec16">
        <title>Deep neural network model</title>
        <p>The DNN architecture has 480 inputs and and 1 output, applying the <italic>PReLu</italic> (Parametric ReLu [<xref ref-type="bibr" rid="CR55">55</xref>]) activation function for each neuron, which is essentially a Leaky ReLu but with a learnable coefficient to tackle the dying ReLu problem in the vanilla ReLu function. The tuned-DNN architecture has three hidden layers, with 600 neurons in the first layer, 500 in the second, and 400 in the third. To prevent overfitting, <italic>dropout</italic> was applied between each hidden layer, with a dropout rate of 0.3. We use mean squared error as the loss function. We experimented with the following optimizers: <italic>RMSProp</italic> [<xref ref-type="bibr" rid="CR56">56</xref>], <italic>Adadelta</italic> [<xref ref-type="bibr" rid="CR57">57</xref>], <italic>Adagrad</italic> [<xref ref-type="bibr" rid="CR58">58</xref>], and <italic>Adam</italic> [<xref ref-type="bibr" rid="CR59">59</xref>]. We found that the <italic>RMSProp</italic> [<xref ref-type="bibr" rid="CR56">56</xref>] optimizer worked best for this DNN architecture. The DNN architecture is shown in Fig <xref rid="Fig6" ref-type="fig">6</xref>a.
<fig id="Fig6"><label>Fig. 6</label><caption><p>Figure 6<bold>a</bold> shows the DNN architecture. It takes 24 histone modifications (each has 20 features) as input and predicts whether a genomic location is a distal regulatory site or not. There are three hidden layers and one output layer. Between each hidden layer, we used PReLU as activation function and dropout with rate 0.3 between each hidden layer, to prevent overfitting. Figure 6<bold>b</bold> gives an illustrative example of row-wise stacking of histone modifications used as inputs to our CNN model. As shown in Figure 6<bold>b</bold>, each location has various histone modification signals, represented by zigzag lines with di.erent colors in the figure. For illustration purposes, we only represent four histone modification signals. By stacking these signals row-wise, these signals are captured as snapshots of informative features of the genome at each location. Similar to standard RGB images where channels provide di.erent color features, each type of histone modification signal provides unique information to the model. Since the patterns of those signals are quite di.erent across di.erent types of histone modifications, removing any subset of them could result in information loss. With the proper design of the convolution kernel, where the height can cover all signals, the convolution kernel can extract local features to the next layer of the designed CNN. The width of the kernel should not be too large. Too wide a kernel would result in the kernel convolving remote features that are irrelevant to characterizing the local information. Figure 6<bold>c</bold> shows the CNN architecture. The input is in 2D form with each row representing one histone modification feature. After each convolutional layer, it has PReLu layer (due to the space constraint, we skipped showing them in the Figure). After Max-Pooling for down sampling, CNN connects two layers of fully connected neurons, each layer has 300 neurons, and finally connects with output. To prevent overfitting, we also add dropout with rate 0.5 between Max-Pooling and first fully connected layer and between first and second fully connected layer, and dropout with rate 0.3 between the second fully connected layer and output layer</p></caption><graphic xlink:href="12859_2019_3049_Fig6_HTML" id="MO6"/></fig></p>
      </sec>
      <sec id="Sec17">
        <title>Convolutional neural network model</title>
        <p>CNNs have tremendously improved the prediction performance of image-classification tasks. This improvement comes from the following attributes of CNNs. 
<list list-type="bullet"><list-item><p>CNNs are able to perform local feature extraction through the design of specific filters that can pick up target features from the images, and at scale, the parameters such as stride length and filter size can modify the rate at which these target features are detected from the images.</p></list-item><list-item><p>CNNs demonstrate a shift invariant property, which means the exact position of the features does not matter and this comes from the pooling of the features in the pooling step, a useful artefact of which is the dimensionality reduction that occurs in the process.</p></list-item><list-item><p>CNNs perform non-linear transformation of the input through the use of various activation functions. Since the third characteristic is similar to traditional neural networks, we only describe <italic>local feature extraction</italic> and <italic>the shift-invariant property</italic> in greater detail. <bold>Local feature extraction:</bold> Images have structures, with increasing levels of complexity starting with local features of the image and moving on to more abstract, global features. Distinct from the standard fully-connected neural network that treats each pixel position as an independent variable, the kernel of the convolutional layer in a CNN looks at a small region of the input (receptive field) at a time and extracts meaningful features locally from the input (initially). The subsequent convolutional layers hierarchically extract higher-level features from the previous layers’ output and the process carries on with the ability to extract higher-order abstractions with increasing network depths. Now these kernels are essentially an array of numbers (called weights or parameters of the filter) and these “kernel weights” are adjusted throughout the learning process. At the end, these kernels are capable of extracting relevant features for increasing the prediction performance for the task at hand. <bold>Shift invariance:</bold> There are two invariant properties of CNNs: <italic>location invariance</italic> and <italic>translation invariance</italic>. First, since the weights of a specific kernel are shared when scanning through the local region of inputs, no matter where the object that the model is trying to identify, “scanning” the kernel across the image will produce the same output. In other words, the weight sharing characteristic of the kernel of the convolutional layer allows the learned model to be insensitive to the location of the target object in the image. We call this the <italic>location invariant</italic> property of the CNN. Second, when a kernel scans a specific region of input, it computes the dot product between the learned weights and the local inputs. Thus, if the original input is slightly rotated, the dot product does not change much. The pooling layer essentially performs a downsampling operation to the output of the previous layer. Specifically, it distills the most salient features among the nearby ones to capture snapshots in the images. Thus, no matter where the salient features are located within that region, the pooling operator will pick them up. These two factors contribute to the <italic>translation invariance</italic> property of the CNN.</p></list-item></list></p>
        <p><bold>Histone modification signals are snapshots of genome:</bold> Typical images have three channels: R, G, and B. Each channel encodes different values for the same location of the image and these values are essential to represent image. One can also only use gray scale to represent images. However, the gray scale images discard the color information. Similar to images, different histone modification signals characterize distinct properties at each genome location. Therefore, by stacking each histone modification feature row-wise with the proper design of filters or kernels, a location-by-location snapshot of the genome is acquired. We give an illustrative example of how we stack histone modification combinatorial signatures for encoding the information into the CNN in Fig <xref rid="Fig6" ref-type="fig">6</xref>b. We hypothesize that <italic>the information extracted from histone modification snapshots can be well characterized by the CNN model</italic> due to the following reasons. First, the histone signals may be slightly transformed due to the sampling techniques. Those nuances should not affect the output of the learned model. Second, the location of histone modifications signals in the snapshot should not affect the prediction outcome. And third, the permutation of histone modification signals should not change the prediction outcome. We believe that CNN could generalize well from histone modification snapshots since it can perform local feature extraction and can preserve the shift invariant property. Our empirical results support our hypothesis.</p>
        <p><bold>Sensitivity analysis on the hyperparameters’ tuning space:</bold> A valid concern when using deep learning models is that the search space for hyperparameter tuning is too large to generate a specific architecture for a specific problem statement. However, through our analysis for tuning the hyperparameters, we find that the searching is tractable and can be explained by standard learning theory [<xref ref-type="bibr" rid="CR34">34</xref>]. Specifically, we test the size of the kernels of the convolutional layers and the window size of the pooling layer. We find that the higher the number of kernels, the better the validation rate is, up until 128 kernels. This is because the designed CNN requires enough number of kernels to extract distinct features, in order to construct more nuanced outputs for the next layer. However, if the number of kernels exceeds 128, those additional kernels become redundant, resulting in the CNN overfitting to the noise in the features, as is typical in the genomics domain. We leave the details of the sensitivity analysis on these hyperparameters in supplementary Figure S2a, S2b, and S2c.</p>
        <p><bold>Final CNN architecture:</bold> Our final CNN architecture after performing sensitivity analysis is shown in Fig <xref rid="Fig6" ref-type="fig">6</xref><bold>c</bold>. The 480 input features are reshaped into two dimensions, with 24 rows of histone modifications and 20 columns of features for each histone modification. The first convolutional layer uses 64 kernels, with 24 rows and 2 columns, with stride size of 1 to scan through the input, forming the output volume of the first convolutional layer as [64 ×1×19]. The second convolutional layer uses 64 kernels, with 1 rows and 2 column, with a stride size 1, forming the volume [64 ×1×18]. Each convolutional layer connects with <italic>PReLu</italic> layer for thresholding the output from convolutional layer, retaining the same output volume as its previous convolutional layer. The <italic>Max-Pooling</italic> [<xref ref-type="bibr" rid="CR60">60</xref>] uses pool size [1 ×2] for downsampling. After downsampling, it connects with two fully-connected layers, each with 300 neurons. Finally, the second fully-connected layer connects the last layer with one neuron representing the output layer. We use mean-squared error as the loss function. We tried <italic>RMSProp</italic> [<xref ref-type="bibr" rid="CR56">56</xref>], <italic>Adadelta</italic> [<xref ref-type="bibr" rid="CR57">57</xref>], <italic>Adagrad</italic> [<xref ref-type="bibr" rid="CR58">58</xref>], and <italic>Adam</italic> [<xref ref-type="bibr" rid="CR59">59</xref>] optimizers and found <italic>Adagrad</italic> [<xref ref-type="bibr" rid="CR58">58</xref>] to work the best for our model. In order to prevent overfitting, we added dropout at a rate of 0.5 between <italic>Max-Pooling</italic> and the first fully connected layer and between the first and second fully connected layer, and dropout rate of 0.3 between the second fully connected layer and the output layer.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Additional file</title>
    <sec id="Sec18">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2019_3049_MOESM1_ESM.pdf">
            <label>Additional file 1</label>
            <caption>
              <p>This file contains the following: a brief summary of SVM and Random Forest models for distal regulatory site prediction, PR Metric and Data Preprocessing, PR Results, 1 Figure that describes the pipeline for generating the PR dataset, 1 Figure that summarizes the PR results, and 3 Figures that summarize the Sensitivity analysis for tuning our Convolutional Neural Network. (PDF 421 kb)</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>AUC</term>
        <def>
          <p>Area under curve</p>
        </def>
      </def-item>
      <def-item>
        <term>CNN</term>
        <def>
          <p>Convolutional neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>DHS</term>
        <def>
          <p>DNase I hypersensitivity regions</p>
        </def>
      </def-item>
      <def-item>
        <term>DNN</term>
        <def>
          <p>Deep neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>DRE</term>
        <def>
          <p>Distal regulatory element</p>
        </def>
      </def-item>
      <def-item>
        <term>GPU</term>
        <def>
          <p>Graphics processing unit</p>
        </def>
      </def-item>
      <def-item>
        <term>ML</term>
        <def>
          <p>Machine learning</p>
        </def>
      </def-item>
      <def-item>
        <term>PR</term>
        <def>
          <p>Precision recall</p>
        </def>
      </def-item>
      <def-item>
        <term>RBF</term>
        <def>
          <p>Radial basis function</p>
        </def>
      </def-item>
      <def-item>
        <term>RF</term>
        <def>
          <p>Random forest</p>
        </def>
      </def-item>
      <def-item>
        <term>RPKM</term>
        <def>
          <p>Reads per kilobase per million</p>
        </def>
      </def-item>
      <def-item>
        <term>SVM</term>
        <def>
          <p>Support vector machines</p>
        </def>
      </def-item>
      <def-item>
        <term>TF</term>
        <def>
          <p>Transcription factor</p>
        </def>
      </def-item>
      <def-item>
        <term>TFBS</term>
        <def>
          <p>Transcription factor binding site</p>
        </def>
      </def-item>
      <def-item>
        <term>TPM</term>
        <def>
          <p>True positive marker</p>
        </def>
      </def-item>
      <def-item>
        <term>TSS</term>
        <def>
          <p>Transcription start site</p>
        </def>
      </def-item>
      <def-item>
        <term>VR</term>
        <def>
          <p>Validation rate</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p><sc>Aikyatan</sc> (pronounced “Oi-ko-taan”, origin: Sanskrit) meaning a common harmonious chord. This name is an</p>
    </fn>
    <fn id="Fn2">
      <label>2</label>
      <p>We also have the evaluation results using PR metric. We put the results in the supplement for brevity.</p>
    </fn>
    <fn id="Fn3">
      <label>3</label>
      <p>We admit that there exists a parallel version of RFECS. However, we could also speed up the computation of our models using multiple CPUs and even GPUs. In our experiments, we already have shown that the computation time for CNN on single CPU is significantly faster than RFECS. Thus, we do not do further comparisons of multi-CPUs or GPUs for run time comparisons.</p>
    </fn>
    <fn id="Fn4">
      <label>4</label>
      <p><sc>Aikyatan</sc> (pronounced “Oi-ko-taan”, origin: Sanskrit)meaning a common harmonious chord. This name is an</p>
    </fn>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Not Applicable.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>Conceptualization, SC; Formal Analysis: CHF, NT, MR, and SC; Investigation, CHF, NT, and SC; Data Curation, CHF, MR; Writing – Original Draft, CHF and SC; Writing – Review &amp; Editing, CHF, NT, AG, and SC, Visualization, CHF and SC; Project Administration, AG and SC; Funding Acquisition, AG and SC All authors have read and approved the manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Supported by the NIH Grant# 1R01AI123037 (A.G., S.C.) and through a grant from the Lilly Endowment (Wabash Heartland Innovation Network) (S.C.). The funding bodies did not play any role in the design of <sc>Aikyatan</sc>, the interpretation of data, or the writing of this manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>All source code will be made publicly available at <ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/cellsandmachines/aikyatan">https://bitbucket.org/cellsandmachines/aikyatan</ext-link>.</p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p>Not Applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p>Not Applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Heintzman</surname>
            <given-names>ND</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Finding distal regulatory elements in the human genome</article-title>
        <source>Curr Opin Genet Develop</source>
        <year>2009</year>
        <volume>19</volume>
        <issue>6</issue>
        <fpage>541</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1016/j.gde.2009.09.006</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Weintraub</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Zamudio</surname>
            <given-names>AV</given-names>
          </name>
          <name>
            <surname>Sigova</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Hannett</surname>
            <given-names>NM</given-names>
          </name>
          <name>
            <surname>Day</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Abraham</surname>
            <given-names>BJ</given-names>
          </name>
          <name>
            <surname>Cohen</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Nabet</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Buckley</surname>
            <given-names>DL</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Yy1 is a structural regulator of enhancer-promoter loops</article-title>
        <source>Cell</source>
        <year>2017</year>
        <volume>171</volume>
        <issue>7</issue>
        <fpage>1573</fpage>
        <lpage>88</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2017.11.008</pub-id>
        <pub-id pub-id-type="pmid">29224777</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Parker</surname>
            <given-names>SC</given-names>
          </name>
          <name>
            <surname>Stitzel</surname>
            <given-names>ML</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>DL</given-names>
          </name>
          <name>
            <surname>Orozco</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Erdos</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Akiyama</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>van Bueren</surname>
            <given-names>KL</given-names>
          </name>
          <name>
            <surname>Chines</surname>
            <given-names>PS</given-names>
          </name>
          <name>
            <surname>Narisu</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Black</surname>
            <given-names>BL</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Chromatin stretch enhancer states drive cell-specific gene regulation and harbor human disease risk variants</article-title>
        <source>Proc Nat Acad Sci</source>
        <year>2013</year>
        <volume>110</volume>
        <issue>44</issue>
        <fpage>17921</fpage>
        <lpage>6</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1317023110</pub-id>
        <pub-id pub-id-type="pmid">24127591</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Raab</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Kamakaka</surname>
            <given-names>RT</given-names>
          </name>
        </person-group>
        <article-title>Insulators and promoters: closer than we think</article-title>
        <source>Nat Rev Genet</source>
        <year>2010</year>
        <volume>11</volume>
        <issue>6</issue>
        <fpage>439</fpage>
        <pub-id pub-id-type="doi">10.1038/nrg2765</pub-id>
        <pub-id pub-id-type="pmid">20442713</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Consortium</surname>
            <given-names>EP</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>An integrated encyclopedia of dna elements in the human genome</article-title>
        <source>Nature</source>
        <year>2012</year>
        <volume>489</volume>
        <issue>7414</issue>
        <fpage>57</fpage>
        <lpage>74</lpage>
        <pub-id pub-id-type="doi">10.1038/nature11247</pub-id>
        <pub-id pub-id-type="pmid">22955616</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shlyueva</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Stampfel</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Stark</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Transcriptional enhancers: from properties to genome-wide predictions</article-title>
        <source>Nat Rev Genet</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>4</issue>
        <fpage>272</fpage>
        <lpage>86</lpage>
        <pub-id pub-id-type="doi">10.1038/nrg3682</pub-id>
        <pub-id pub-id-type="pmid">24614317</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kellis</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wold</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Snyder</surname>
            <given-names>MP</given-names>
          </name>
          <name>
            <surname>Bernstein</surname>
            <given-names>BE</given-names>
          </name>
          <name>
            <surname>Kundaje</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Marinov</surname>
            <given-names>GK</given-names>
          </name>
          <name>
            <surname>Ward</surname>
            <given-names>LD</given-names>
          </name>
          <name>
            <surname>Birney</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Crawford</surname>
            <given-names>GE</given-names>
          </name>
          <name>
            <surname>Dekker</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Defining functional dna elements in the human genome</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>2014</year>
        <volume>111</volume>
        <issue>17</issue>
        <fpage>6131</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1318948111</pub-id>
        <pub-id pub-id-type="pmid">24753594</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kelsey</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Stegle</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Reik</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Single-cell epigenomics: Recording the past and predicting the future</article-title>
        <source>Science</source>
        <year>2017</year>
        <volume>358</volume>
        <issue>6359</issue>
        <fpage>69</fpage>
        <lpage>75</lpage>
        <pub-id pub-id-type="doi">10.1126/science.aan6826</pub-id>
        <pub-id pub-id-type="pmid">28983045</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hait</surname>
            <given-names>TA</given-names>
          </name>
          <name>
            <surname>Amar</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Shamir</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Elkon</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Focs: a novel method for analyzing enhancer and gene activity patterns infers an extensive enhancer–promoter map</article-title>
        <source>Genome Biol</source>
        <year>2018</year>
        <volume>19</volume>
        <issue>1</issue>
        <fpage>56</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-018-1432-2</pub-id>
        <pub-id pub-id-type="pmid">29716618</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <mixed-citation publication-type="other">Theera-Ampornpunt N, Chaterji S. Prediction of enhancer rna activity levels from chip-seq-derived histone modification combinatorial codes. In: 2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE: 2017. p. 1206–14.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <mixed-citation publication-type="other">Kim SG, Harwani M, Grama A, Chaterji S. EP-DNN: A deep neural network-based global enhancer prediction algorithm. Sci Rep. 2016;6:1–13.</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>SG</given-names>
          </name>
          <name>
            <surname>Theera-Ampornpunt</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Fang</surname>
            <given-names>C. -H.</given-names>
          </name>
          <name>
            <surname>Harwani</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Grama</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Chaterji</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Opening up the blackbox: an interpretable deep neural network-based classifier for cell-type specific enhancer predictions</article-title>
        <source>BMC Syst Biol</source>
        <year>2016</year>
        <volume>10</volume>
        <issue>2</issue>
        <fpage>54</fpage>
        <pub-id pub-id-type="doi">10.1186/s12918-016-0302-3</pub-id>
        <pub-id pub-id-type="pmid">27490187</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ernst</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kellis</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Large-scale imputation of epigenomic datasets for systematic annotation of diverse human tissues</article-title>
        <source>Nat Biotechnol</source>
        <year>2015</year>
        <volume>33</volume>
        <issue>4</issue>
        <fpage>364</fpage>
        <pub-id pub-id-type="doi">10.1038/nbt.3157</pub-id>
        <pub-id pub-id-type="pmid">25690853</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gundem</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Perez-Llamas</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Jene-Sanz</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kedzierska</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Islam</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Deu-Pons</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Furney</surname>
            <given-names>SJ</given-names>
          </name>
          <name>
            <surname>Lopez-Bigas</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Intogen: integration and data mining of multidimensional oncogenomic data</article-title>
        <source>Nat Methods</source>
        <year>2010</year>
        <volume>7</volume>
        <issue>2</issue>
        <fpage>92</fpage>
        <pub-id pub-id-type="doi">10.1038/nmeth0210-92</pub-id>
        <pub-id pub-id-type="pmid">20111033</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Deng</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>Z-X</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Methcna: a database for integrating genomic and epigenomic data in human cancer</article-title>
        <source>BMC genomics</source>
        <year>2018</year>
        <volume>19</volume>
        <issue>1</issue>
        <fpage>138</fpage>
        <pub-id pub-id-type="doi">10.1186/s12864-018-4525-0</pub-id>
        <pub-id pub-id-type="pmid">29433427</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lehner</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Crombie</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Tischler</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Fortunato</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Fraser</surname>
            <given-names>AG</given-names>
          </name>
        </person-group>
        <article-title>Systematic mapping of genetic interactions in caenorhabditis elegans identifies common modifiers of diverse signaling pathways</article-title>
        <source>Nat Genet</source>
        <year>2006</year>
        <volume>38</volume>
        <issue>8</issue>
        <fpage>896</fpage>
        <pub-id pub-id-type="doi">10.1038/ng1844</pub-id>
        <pub-id pub-id-type="pmid">16845399</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koo</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chaterji</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Tiresias: Context-sensitive approach to decipher the presence and strength of microrna regulatory interactions</article-title>
        <source>Theranostics</source>
        <year>2018</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>277</fpage>
        <pub-id pub-id-type="doi">10.7150/thno.22065</pub-id>
        <pub-id pub-id-type="pmid">29290807</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <mixed-citation publication-type="other">Ghoshal A, Grama A, Bagchi S, Chaterji S. An ensemble svm model for the accurate prediction of non-canonical microrna targets. In: Proceedings of the 6th ACM Conference on Bioinformatics, Computational Biology and Health Informatics. ACM: 2015. p. 403–12.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <mixed-citation publication-type="other">Theera-Ampornpunt N, Kim SG, Ghoshal A, Bagchi S, Grama A, Chaterji S. Fast training on large genomics data using distributed support vector machines. In: Communication Systems and Networks (COMSNETS), 2016 8th International Conference On. IEEE: 2016. p. 1–8.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <mixed-citation publication-type="other">Mou L, Li G, Zhang L, Wang T, Jin Z. Convolutional neural networks over tree structures for programming language processing. In: AAAI: 2016. p. 4.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <mixed-citation publication-type="other">Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems: 2012. p. 1097–105.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <mixed-citation publication-type="other">Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V, Rabinovich A, et al.Going deeper with convolutions. CVPR: 2015. p. 1–9.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <mixed-citation publication-type="other">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556. 2014. p1–14.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <mixed-citation publication-type="other">He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition: 2016. p. 770–778.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bernstein</surname>
            <given-names>BE</given-names>
          </name>
          <name>
            <surname>Stamatoyannopoulos</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Costello</surname>
            <given-names>JF</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Milosavljevic</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Meissner</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kellis</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Marra</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Beaudet</surname>
            <given-names>AL</given-names>
          </name>
          <name>
            <surname>Ecker</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Farnham</surname>
            <given-names>PJ</given-names>
          </name>
          <name>
            <surname>Hirst</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lander</surname>
            <given-names>ES</given-names>
          </name>
          <name>
            <surname>Mikkelsen</surname>
            <given-names>TS</given-names>
          </name>
          <name>
            <surname>Thomson</surname>
            <given-names>JA</given-names>
          </name>
        </person-group>
        <article-title>The nih roadmap epigenomics mapping consortium</article-title>
        <source>Nat Biotech</source>
        <year>2010</year>
        <volume>28</volume>
        <issue>10</issue>
        <fpage>1045</fpage>
        <lpage>1048</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt1010-1045</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <mixed-citation publication-type="other">Chollet F. Keras. GitHub. 2015. <ext-link ext-link-type="uri" xlink:href="https://github.com/fchollet/keras">https://github.com/fchollet/keras</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <mixed-citation publication-type="other">Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints. 2016;abs/1605.02688.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dao</surname>
            <given-names>Lan T M</given-names>
          </name>
          <name>
            <surname>Galindo-Albarrán</surname>
            <given-names>Ariel O</given-names>
          </name>
          <name>
            <surname>Castro-Mondragon</surname>
            <given-names>Jaime A</given-names>
          </name>
          <name>
            <surname>Andrieu-Soler</surname>
            <given-names>Charlotte</given-names>
          </name>
          <name>
            <surname>Medina-Rivera</surname>
            <given-names>Alejandra</given-names>
          </name>
          <name>
            <surname>Souaid</surname>
            <given-names>Charbel</given-names>
          </name>
          <name>
            <surname>Charbonnier</surname>
            <given-names>Guillaume</given-names>
          </name>
          <name>
            <surname>Griffon</surname>
            <given-names>Aurélien</given-names>
          </name>
          <name>
            <surname>Vanhille</surname>
            <given-names>Laurent</given-names>
          </name>
          <name>
            <surname>Stephen</surname>
            <given-names>Tharshana</given-names>
          </name>
          <name>
            <surname>Alomairi</surname>
            <given-names>Jaafar</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>David</given-names>
          </name>
          <name>
            <surname>Torres</surname>
            <given-names>Magali</given-names>
          </name>
          <name>
            <surname>Fernandez</surname>
            <given-names>Nicolas</given-names>
          </name>
          <name>
            <surname>Soler</surname>
            <given-names>Eric</given-names>
          </name>
          <name>
            <surname>van Helden</surname>
            <given-names>Jacques</given-names>
          </name>
          <name>
            <surname>Puthier</surname>
            <given-names>Denis</given-names>
          </name>
          <name>
            <surname>Spicuglia</surname>
            <given-names>Salvatore</given-names>
          </name>
        </person-group>
        <article-title>Genome-wide characterization of mammalian promoters with distal enhancer functions</article-title>
        <source>Nature Genetics</source>
        <year>2017</year>
        <volume>49</volume>
        <issue>7</issue>
        <fpage>1073</fpage>
        <lpage>1081</lpage>
        <pub-id pub-id-type="doi">10.1038/ng.3884</pub-id>
        <pub-id pub-id-type="pmid">28581502</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rickels</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Shilatifard</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Enhancer logic and mechanics in development and disease</article-title>
        <source>Trends Cell Biol</source>
        <year>2018</year>
        <volume>28</volume>
        <issue>8</issue>
        <fpage>608</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="doi">10.1016/j.tcb.2018.04.003</pub-id>
        <pub-id pub-id-type="pmid">29759817</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stephens</surname>
            <given-names>ZD</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>SY</given-names>
          </name>
          <name>
            <surname>Faghri</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Campbell</surname>
            <given-names>RH</given-names>
          </name>
          <name>
            <surname>Zhai</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Efron</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Iyer</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Schatz</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Sinha</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Robinson</surname>
            <given-names>GE</given-names>
          </name>
        </person-group>
        <article-title>Big data: astronomical or genomical?</article-title>
        <source>PLoS Biol</source>
        <year>2015</year>
        <volume>13</volume>
        <issue>7</issue>
        <fpage>1002195</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pbio.1002195</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rajagopal</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wagner</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Stamatoyannopoulos</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ernst</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kellis</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Rfecs: a random-forest based algorithm for enhancer identification from chromatin state</article-title>
        <source>PLoS Comput Biol</source>
        <year>2013</year>
        <volume>9</volume>
        <issue>3</issue>
        <fpage>1002968</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1002968</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32</label>
      <mixed-citation publication-type="other">Kleftogiannis D, Kalnis P, Bajic VB. DEEP: a general computational framework for predicting enhancers. Nucleic Acids Res. 2014. p1–14.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33</label>
      <mixed-citation publication-type="other">Liu F, Li H, Ren C, Bo X, Shu W. Pedla: predicting enhancers with a deep learning-based algorithmic framework. bioRxiv. 2016. URL 10.1101/036129. http://biorxiv.org/content/early/2016/05/18/036129.full.pdf. Accessed Aug 2019.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vapnik</surname>
            <given-names>VN</given-names>
          </name>
        </person-group>
        <article-title>An overview of statistical learning theory</article-title>
        <source>IEEE Trans Neural Netw</source>
        <year>1999</year>
        <volume>10</volume>
        <issue>5</issue>
        <fpage>988</fpage>
        <lpage>99</lpage>
        <pub-id pub-id-type="doi">10.1109/72.788640</pub-id>
        <pub-id pub-id-type="pmid">18252602</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35</label>
      <mixed-citation publication-type="other">Wahba G, et al.Support vector machines, reproducing kernel hilbert spaces and the randomized gacv. Adv Kernel Meth-Supp Vect Learn. 1999; 6:69–87.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Evgeniou</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Pontil</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Poggio</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Regularization networks and support vector machines</article-title>
        <source>Adv Comput Math</source>
        <year>2000</year>
        <volume>13</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <pub-id pub-id-type="doi">10.1023/A:1018946025316</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bordes</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ertekin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Weston</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bottou</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Fast kernel classifiers with online and active learning</article-title>
        <source>J Mach Learn Res</source>
        <year>2005</year>
        <volume>6</volume>
        <issue>Sep</issue>
        <fpage>1579</fpage>
        <lpage>619</lpage>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hinton</surname>
            <given-names>GE</given-names>
          </name>
          <name>
            <surname>Salakhutdinov</surname>
            <given-names>RR</given-names>
          </name>
        </person-group>
        <article-title>Reducing the dimensionality of data with neural networks</article-title>
        <source>Science</source>
        <year>2006</year>
        <volume>313</volume>
        <issue>5786</issue>
        <fpage>504</fpage>
        <lpage>07</lpage>
        <pub-id pub-id-type="doi">10.1126/science.1127647</pub-id>
        <pub-id pub-id-type="pmid">16873662</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39</label>
      <mixed-citation publication-type="other">Why does deep and cheap learning work so well?J Stat Phys. 2017; 168(6):1223–47.</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Poggio</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Mhaskar</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Rosasco</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Miranda</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Liao</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review</article-title>
        <source>Int J Autom Comput</source>
        <year>2017</year>
        <volume>14</volume>
        <issue>5</issue>
        <fpage>503</fpage>
        <lpage>19</lpage>
        <pub-id pub-id-type="doi">10.1007/s11633-017-1054-2</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41</label>
      <mixed-citation publication-type="other">Anselmi F, Rosasco L, Tan C, Poggio T. Deep convolutional networks are hierarchical kernel machines. arXiv preprint arXiv:1508.01084. 2015.</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42</label>
      <mixed-citation publication-type="other">Poggio T, Rosasco L, Shashua A, Cohen N, Anselmi F. Notes on hierarchical splines, dclns and i-theory. Tech Rep. 2015.</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43</label>
      <mixed-citation publication-type="other">Poggio T, Anselmi F, Rosasco L. I-theory on depth vs width: hierarchical function composition. Tech Rep. 2015.</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44</label>
      <mixed-citation publication-type="other">Mhaskar H, Liao Q, Poggio T. Learning functions: when is deep better than shallow. arXiv preprint arXiv:1603.00988. 2016.</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mhaskar</surname>
            <given-names>HN</given-names>
          </name>
          <name>
            <surname>Poggio</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Deep vs. shallow networks: An approximation theory perspective</article-title>
        <source>Anal Appl</source>
        <year>2016</year>
        <volume>14</volume>
        <issue>06</issue>
        <fpage>829</fpage>
        <lpage>48</lpage>
        <pub-id pub-id-type="doi">10.1142/S0219530516400042</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46</label>
      <mixed-citation publication-type="other">Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks In: Pereira F, Burges CJC, Bottou L, Weinberger KQ, editors. Advances in Neural Information Processing Systems 25. Curran Associates, Inc: 2012. p. 1097–105. URL <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</ext-link>. Accessed Aug 2019.</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hinton</surname>
            <given-names>Geoffrey</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>Li</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>Dong</given-names>
          </name>
          <name>
            <surname>Dahl</surname>
            <given-names>George</given-names>
          </name>
          <name>
            <surname>Mohamed</surname>
            <given-names>Abdel-rahman</given-names>
          </name>
          <name>
            <surname>Jaitly</surname>
            <given-names>Navdeep</given-names>
          </name>
          <name>
            <surname>Senior</surname>
            <given-names>Andrew</given-names>
          </name>
          <name>
            <surname>Vanhoucke</surname>
            <given-names>Vincent</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>Patrick</given-names>
          </name>
          <name>
            <surname>Sainath</surname>
            <given-names>Tara</given-names>
          </name>
          <name>
            <surname>Kingsbury</surname>
            <given-names>Brian</given-names>
          </name>
        </person-group>
        <article-title>Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups</article-title>
        <source>IEEE Signal Processing Magazine</source>
        <year>2012</year>
        <volume>29</volume>
        <issue>6</issue>
        <fpage>82</fpage>
        <lpage>97</lpage>
        <pub-id pub-id-type="doi">10.1109/MSP.2012.2205597</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Collobert</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Weston</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bottou</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Karlen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kavukcuoglu</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kuksa</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Natural language processing (almost) from scratch</article-title>
        <source>J Mach Learn Res</source>
        <year>2011</year>
        <volume>12</volume>
        <issue>Aug</issue>
        <fpage>2493</fpage>
        <lpage>537</lpage>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49</label>
      <mixed-citation publication-type="other">Lena PD, Nagata K, Baldi PF. Deep spatio-temporal architectures and learning for protein structure prediction. In: Advances in Neural Information Processing Systems: 2012. p. 512–20.</mixed-citation>
    </ref>
    <ref id="CR50">
      <label>50</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Gong</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A deep learning framework for modeling structural features of rna-binding protein targets</article-title>
        <source>Nucleic Acids Res</source>
        <year>2016</year>
        <volume>44</volume>
        <issue>4</issue>
        <fpage>32</fpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkv1025</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leung</surname>
            <given-names>MKK</given-names>
          </name>
          <name>
            <surname>Xiong</surname>
            <given-names>HY</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>LJ</given-names>
          </name>
          <name>
            <surname>Frey</surname>
            <given-names>BJ</given-names>
          </name>
        </person-group>
        <article-title>Deep learning of the tissue-regulated splicing code</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <issue>12</issue>
        <fpage>121</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu277</pub-id>
        <pub-id pub-id-type="pmid">24273246</pub-id>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bernstein</surname>
            <given-names>Bradley E</given-names>
          </name>
          <name>
            <surname>Stamatoyannopoulos</surname>
            <given-names>John A</given-names>
          </name>
          <name>
            <surname>Costello</surname>
            <given-names>Joseph F</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>Bing</given-names>
          </name>
          <name>
            <surname>Milosavljevic</surname>
            <given-names>Aleksandar</given-names>
          </name>
          <name>
            <surname>Meissner</surname>
            <given-names>Alexander</given-names>
          </name>
          <name>
            <surname>Kellis</surname>
            <given-names>Manolis</given-names>
          </name>
          <name>
            <surname>Marra</surname>
            <given-names>Marco A</given-names>
          </name>
          <name>
            <surname>Beaudet</surname>
            <given-names>Arthur L</given-names>
          </name>
          <name>
            <surname>Ecker</surname>
            <given-names>Joseph R</given-names>
          </name>
          <name>
            <surname>Farnham</surname>
            <given-names>Peggy J</given-names>
          </name>
          <name>
            <surname>Hirst</surname>
            <given-names>Martin</given-names>
          </name>
          <name>
            <surname>Lander</surname>
            <given-names>Eric S</given-names>
          </name>
          <name>
            <surname>Mikkelsen</surname>
            <given-names>Tarjei S</given-names>
          </name>
          <name>
            <surname>Thomson</surname>
            <given-names>James A</given-names>
          </name>
        </person-group>
        <article-title>The NIH Roadmap Epigenomics Mapping Consortium</article-title>
        <source>Nature Biotechnology</source>
        <year>2010</year>
        <volume>28</volume>
        <issue>10</issue>
        <fpage>1045</fpage>
        <lpage>1048</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt1010-1045</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hawkins</surname>
            <given-names>R. David</given-names>
          </name>
          <name>
            <surname>Hon</surname>
            <given-names>Gary C.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>Leonard K.</given-names>
          </name>
          <name>
            <surname>Ngo</surname>
            <given-names>QueMinh</given-names>
          </name>
          <name>
            <surname>Lister</surname>
            <given-names>Ryan</given-names>
          </name>
          <name>
            <surname>Pelizzola</surname>
            <given-names>Mattia</given-names>
          </name>
          <name>
            <surname>Edsall</surname>
            <given-names>Lee E.</given-names>
          </name>
          <name>
            <surname>Kuan</surname>
            <given-names>Samantha</given-names>
          </name>
          <name>
            <surname>Luu</surname>
            <given-names>Ying</given-names>
          </name>
          <name>
            <surname>Klugman</surname>
            <given-names>Sarit</given-names>
          </name>
          <name>
            <surname>Antosiewicz-Bourget</surname>
            <given-names>Jessica</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>Zhen</given-names>
          </name>
          <name>
            <surname>Espinoza</surname>
            <given-names>Celso</given-names>
          </name>
          <name>
            <surname>Agarwahl</surname>
            <given-names>Saurabh</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>Li</given-names>
          </name>
          <name>
            <surname>Ruotti</surname>
            <given-names>Victor</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Wei</given-names>
          </name>
          <name>
            <surname>Stewart</surname>
            <given-names>Ron</given-names>
          </name>
          <name>
            <surname>Thomson</surname>
            <given-names>James A.</given-names>
          </name>
          <name>
            <surname>Ecker</surname>
            <given-names>Joseph R.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>Bing</given-names>
          </name>
        </person-group>
        <article-title>Distinct Epigenomic Landscapes of Pluripotent and Lineage-Committed Human Cells</article-title>
        <source>Cell Stem Cell</source>
        <year>2010</year>
        <volume>6</volume>
        <issue>5</issue>
        <fpage>479</fpage>
        <lpage>491</lpage>
        <pub-id pub-id-type="doi">10.1016/j.stem.2010.03.018</pub-id>
        <pub-id pub-id-type="pmid">20452322</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54</label>
      <mixed-citation publication-type="other">UCSC ENCODE DNase. <ext-link ext-link-type="uri" xlink:href="http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeUwDnase/">http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeUwDnase/</ext-link>. Accessed Aug 2019.</mixed-citation>
    </ref>
    <ref id="CR55">
      <label>55</label>
      <mixed-citation publication-type="other">He K, Zhang X, Ren S, Sun J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In: Proceedings of the IEEE International Conference on Computer Vision: 2015. p. 1026–34.</mixed-citation>
    </ref>
    <ref id="CR56">
      <label>56</label>
      <mixed-citation publication-type="other">Tieleman T, Hinton G. Lecture 6.5-rmsprop, coursera: Neural networks for machine learning. University of Toronto, Tech Rep. 2012.</mixed-citation>
    </ref>
    <ref id="CR57">
      <label>57</label>
      <mixed-citation publication-type="other">Zeiler MD. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701. 2012.</mixed-citation>
    </ref>
    <ref id="CR58">
      <label>58</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Duchi</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hazan</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Singer</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Adaptive subgradient methods for online learning and stochastic optimization</article-title>
        <source>J Mach Learn Res</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>2121</fpage>
        <lpage>59</lpage>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59</label>
      <mixed-citation publication-type="other">Kingma D, Ba J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. 2014.</mixed-citation>
    </ref>
    <ref id="CR60">
      <label>60</label>
      <mixed-citation publication-type="other">Scherer D, Müller A, Behnke S. Evaluation of pooling operations in convolutional architectures for object recognition. In: International Conference on Artificial Neural Networks. Springer: 2010. p. 92–101.</mixed-citation>
    </ref>
    <ref id="CR61">
      <label>61</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hofmann</surname>
            <given-names>Thomas</given-names>
          </name>
          <name>
            <surname>Schölkopf</surname>
            <given-names>Bernhard</given-names>
          </name>
          <name>
            <surname>Smola</surname>
            <given-names>Alexander J.</given-names>
          </name>
        </person-group>
        <article-title>Kernel methods in machine learning</article-title>
        <source>The Annals of Statistics</source>
        <year>2008</year>
        <volume>36</volume>
        <issue>3</issue>
        <fpage>1171</fpage>
        <lpage>1220</lpage>
        <pub-id pub-id-type="doi">10.1214/009053607000000677</pub-id>
      </element-citation>
    </ref>
    <ref id="CR62">
      <label>62</label>
      <mixed-citation publication-type="other">Quora. What is the recommended minimum training data set size to train a deep neural network?Quora. 2016. <ext-link ext-link-type="uri" xlink:href="https://www.quora.com/What-is-the-recommended-minimum-training-data-set-size-to-train-a-deep-neural-network">https://www.quora.com/What-is-the-recommended-minimum-training-data-set-size-to-train-a-deep-neural-network</ext-link>. Accessed Aug 2019.</mixed-citation>
    </ref>
    <ref id="CR63">
      <label>63</label>
      <mixed-citation publication-type="other">Abadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, Devin M, Ghemawat S, Irving G, Isard M, et al.Tensorflow: a system for large-scale machine learning. In: OSDI: 2016. p. 265–83.</mixed-citation>
    </ref>
    <ref id="CR64">
      <label>64</label>
      <mixed-citation publication-type="other">Paszke A, Gross S, Chintala S, Chanan G, Yang E, DeVito Z, Lin Z, Desmaison A, Antiga L, Lerer A. Automatic differentiation in pytorch. In: NIPS-W: 2017. p. 1–4.</mixed-citation>
    </ref>
  </ref-list>
</back>
