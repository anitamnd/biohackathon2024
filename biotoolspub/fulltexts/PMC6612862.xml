<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6612862</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btz342</article-id>
    <article-id pub-id-type="publisher-id">btz342</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Ismb/Eccb 2019 Conference Proceedings</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Studies of Phenotypes and Clinical Applications</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Deep learning with multimodal representation for pancancer prognosis prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Cheerla</surname>
          <given-names>Anika</given-names>
        </name>
        <xref ref-type="aff" rid="btz342-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gevaert</surname>
          <given-names>Olivier</given-names>
        </name>
        <xref ref-type="aff" rid="btz342-aff2">2</xref>
        <xref ref-type="corresp" rid="btz342-cor1"/>
        <!--<email>ogevaert@stanford.edu</email>-->
      </contrib>
    </contrib-group>
    <aff id="btz342-aff1"><label>1</label>Monta Vista High School, Cupertino, CA, USA</aff>
    <aff id="btz342-aff2"><label>2</label>Department of Medicine and Biomedical Data Science, Stanford Center for Biomedical Informatics Research, Stanford University, Stanford, CA, USA</aff>
    <author-notes>
      <corresp id="btz342-cor1">To whom correspondence should be addressed. <email>ogevaert@stanford.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2019-07-05">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>35</volume>
    <issue>14</issue>
    <fpage>i446</fpage>
    <lpage>i454</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2019. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btz342.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Estimating the future course of patients with cancer lesions is invaluable to physicians; however, current clinical methods fail to effectively use the vast amount of multimodal data that is available for cancer patients. To tackle this problem, we constructed a multimodal neural network-based model to predict the survival of patients for 20 different cancer types using clinical data, mRNA expression data, microRNA expression data and histopathology whole slide images (WSIs). We developed an unsupervised encoder to compress these four data modalities into a single feature vector for each patient, handling missing data through a resilient, multimodal dropout method. Encoding methods were tailored to each data type—using deep highway networks to extract features from clinical and genomic data, and convolutional neural networks to extract features from WSIs.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We used pancancer data to train these feature encodings and predict single cancer and pancancer overall survival, achieving a C-index of 0.78 overall. This work shows that it is possible to build a pancancer model for prognosis that also predicts prognosis in single cancer sites. Furthermore, our model handles multiple data modalities, efficiently analyzes WSIs and represents patient multimodal data flexibly into an unsupervised, informative representation. We thus present a powerful automated tool to accurately determine prognosis, a key step towards personalized treatment for cancer patients.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>
          <ext-link ext-link-type="uri" xlink:href="https://github.com/gevaertlab/MultimodalPrognosis">https://github.com/gevaertlab/MultimodalPrognosis</ext-link>
        </p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">National Institutes of Health</named-content>
          <named-content content-type="funder-identifier">10.13039/100000002</named-content>
        </funding-source>
        <award-id>R01EB020527</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">National Institute of Dental and Craniofacial Research</named-content>
          <named-content content-type="funder-identifier">10.13039/100000072</named-content>
        </funding-source>
        <award-id>U01DE025188</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">National Cancer Institute</named-content>
          <named-content content-type="funder-identifier">10.13039/100000054</named-content>
        </funding-source>
        <award-id>U01CA199241</award-id>
        <award-id>U01CA217851</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="9"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Estimating tumor progression or predicting prognosis can aid physicians significantly in making decisions about care and treatment of cancer patients. To determine the prognosis of these patients, physicians can leverage several types of data including clinical data, genomic profiling, histology slide images and radiographic images, depending on the tissue site. Yet, the high-dimensional nature of some of these data modalities makes it hard for physicians to manually interpret these multimodal biomedical data to determine treatment and estimate prognosis (<xref rid="btz342-B17" ref-type="bibr">Gevaert <italic>et al.</italic>, 2006</xref><xref rid="btz342-B18" ref-type="bibr">, 2008</xref>). Next, the presence of inter-patient heterogeneity warrants that characterizing tumors individually is essential to improving the treatment process (<xref rid="btz342-B1" ref-type="bibr">Alizadeh <italic>et al.</italic>, 2015</xref>). Previous research has shown how molecular signatures such as gene expression patterns can be mined using machine learning and are predictive of treatment outcomes and prognosis. Similarly, recent work has shown that quantitative analysis of histopathology images using computer vision algorithms can provide additional information on top of what can be discerned by pathologists (<xref rid="btz342-B31" ref-type="bibr">Madabhushi and Lee, 2016</xref>). Thus, automated machine-learning systems, which can discern patterns among high-dimensional data may be the key to better estimate disease aggressiveness and patient outcomes. Another implication of inter-patient heterogeneity is that tumors of different cancer types may share underlying similarities. Thus, pancancer analysis of large-scale data across a broad range of cancers has the potential to improve disease modeling by exploiting these pancancer similarities. Multi-institutional projects such as The Cancer Genome Atlas (TCGA) (<xref rid="btz342-B5" ref-type="bibr">Campbell <italic>et al.</italic>, 2018</xref>; <xref rid="btz342-B32" ref-type="bibr">Malta <italic>et al.</italic>, 2018</xref>; <xref rid="btz342-B41" ref-type="bibr">Weinstein <italic>et al.</italic>, 2013</xref>), which collected standardized clinical, multiomic and imaging data for a wide array of cancers, are crucial to enable this kind of pancancer modeling.</p>
    <p>Automated prognosis prediction, however, remains a difficult task mainly due to the heterogeneity and high dimensionality of the available data. For example each patient in the TCGA database has thousands of genomic features (e.g. microRNA or mRNA) and high resolution histopathology whole slide images (WSIs). Yet, based on previous work, only a subset of the genomic image features are relevant for predicting prognosis. Thus, to successfully develop a multimodal model for prognosis prediction, an approach is required that can efficiently work with clinical, genomic and image data, in essence multimodal data. Here, we tackle this challenging problem by developing a pancancer deep learning architecture drawing from unsupervised and representation learning techniques, and developing a learning architecture that exploits large-scale genomic and image data to the fullest extent. The main goal of this contribution is to harness the vast amount of TCGA data available to develop a robust representation of tumor characteristics that can be used to cluster and compare patients across a variety of different metrics. Using unsupervised representation techniques, we develop pancancer survival models for cancer patients using multimodal data including clinical, genomic and WSI data.</p>
  </sec>
  <sec>
    <title>2 Background</title>
    <p>Prognosis prediction can be formulated as a censored survival analysis problem (<xref rid="btz342-B9" ref-type="bibr">Cox, 2018</xref>; <xref rid="btz342-B29" ref-type="bibr">Luck <italic>et al.</italic>, 2017</xref>), predicting both if and when an event (i.e. patient death) occurs within a given time period. Given the unique statistical distribution of survival times, they are canonically parameterized using the ‘hazard function’, such as in standard Cox regression.</p>
    <p>In recent years, many different approaches have been attempted to predict cancer prognosis using genomic data. For example <xref rid="btz342-B44" ref-type="bibr">Zhang <italic>et al.</italic> (2017)</xref> used an augmented Cox regression on TCGA gene expression data to get a C-index of 0.725 in predicting glioblastoma. MicroRNA data in particular have shown high relevance as a measure for disease modeling and prognosis (<xref rid="btz342-B4" ref-type="bibr">Calin and Croce, 2006</xref>; <xref rid="btz342-B6" ref-type="bibr">Cheerla and Gevaert, 2017</xref>; <xref rid="btz342-B13" ref-type="bibr">Esquela-Kerscher and Slack, 2006</xref>; <xref rid="btz342-B27" ref-type="bibr">Liu <italic>et al.</italic>, 2017</xref>), with <xref rid="btz342-B8" ref-type="bibr">Christinat and Krek (2015</xref>), achieving a C-index of 0.77 on a subset of renal cancer data using random forest classifiers. However, despite the high performance of machine learning models based on molecular data alone, there is still scope for improvement; after all, the tumor environment is a complex, rapidly evolving milieu that is difficult to characterize through molecular profiling alone (<xref rid="btz342-B1" ref-type="bibr">Alizadeh <italic>et al.</italic>, 2015</xref>; <xref rid="btz342-B12" ref-type="bibr">de Bruin <italic>et al.</italic>, 2013</xref>; <xref rid="btz342-B28" ref-type="bibr">Lovly <italic>et al.</italic>, 2016</xref>).</p>
    <p>Recently, the use of WSI data has been shown to improve the performance and generality of prognosis prediction. As WSIs are high resolution images of cellular architecture and environment with potentially only a fraction of the slide relevant to predicting prognosis, much of the literature focuses on hybrid approaches involving pathologist annotation of regions of interest (ROIs). For example <xref rid="btz342-B39" ref-type="bibr">Wang <italic>et al.</italic> (2014</xref>) match the performance of genomic models by using 500 × 500 pixel, physician-selected ROIs and handcrafted slide features to predict prognosis. More recently, deep learning provides a significant boost in predictive power. For example <xref rid="btz342-B42" ref-type="bibr">Yao <italic>et al.</italic> (2016</xref>) are able to significantly outperform all molecular profiling-based methods on two lung cancer datasets using only physician-selected ROIs and convolutional neural networks (CNNs). Other reports, including <xref rid="btz342-B2" ref-type="bibr">Beck <italic>et al.</italic> (2011)</xref> and <xref rid="btz342-B3" ref-type="bibr">Bejnordi <italic>et al.</italic> (2017)</xref>, showing that histopathology image data contains important prognostic information that is complementary to molecular data. Yet, multimodal prognosis models are still highly underexplored (<xref rid="btz342-B33" ref-type="bibr">Momeni <italic>et al.</italic>, 2018a</xref>). To our knowledge, only one paper explores combining genomic and image data for prognosis showing that a lung-cancer genomic model (C-index 0.660) and WSI-based model with hand-annotated ROIs (C-index 0.613) can be combined to get a final classifier with C-index 0.691 (<xref rid="btz342-B46" ref-type="bibr">Zhu <italic>et al.</italic>, 2016</xref>).</p>
    <p>Moreover, the WSI-based methods discussed above require a pathologist to hand-annotate ROIs, a tedious task. Arguably the most difficult part of automated, multimodal prognosis prediction is finding clinically relevant ROIs automatically. In the related field of tumor classification from WSIs, a ‘decision-fusion’ model that randomly samples patches and integrates them into a Gaussian mixture has yielded accurate predictions (<xref rid="btz342-B21" ref-type="bibr">Hou <italic>et al.</italic>, 2016</xref>). Moreover, more recent work has focused on using attention mechanisms to learn what patches are important (<xref rid="btz342-B34" ref-type="bibr">Momeni <italic>et al.</italic>, 2018b</xref>). However, in prognosis prediction, truly automated WSI-based systems have had limited success. One report uses a slide-based approach that relies on unsupervised learning—<xref rid="btz342-B47" ref-type="bibr">Zhu <italic>et al.</italic>’s (2017)</xref> recent paper uses K-means clustering to characterize and adaptively sample patches within slide images, achieving 0.708C-index on lung cancer data, a result that nearly rivals genomic-data approaches.</p>
    <p>Previous research has focused mostly on single-cancer datasets, missing the opportunity to explore commonalities and relationships between tumors in different tissues. And although previous papers explore both genomic and imaging-based approaches, few models have been developed that integrate both data modalities. By exploiting multimodal data, as well as developing better methods to automate WSI scoring and extract useful information from slides, we have the potential to improve upon the state-of-the-art.</p>
    <p>In recent years, CNNs have been used to significantly improve machine learning tasks (<xref rid="btz342-B25" ref-type="bibr">LeCun <italic>et al.</italic>, 2015</xref>) including missing value estimation in genomic data (<xref rid="btz342-B35" ref-type="bibr">Qiu <italic>et al.</italic>, 2018</xref>) and prediction of prognostic factors based on WSI (<xref rid="btz342-B34" ref-type="bibr">Momeni <italic>et al.</italic>, 2018b</xref>). A key component of the success of CNNs is their ability to deal with high-dimensional, unstructured data, in particular image data (<xref rid="btz342-B40" ref-type="bibr">Wang <italic>et al.</italic>, 2017</xref>). For example CNNs can accurately classify scenes from images by learning a set of flexible, hierarchical features (<xref rid="btz342-B45" ref-type="bibr">Zhou <italic>et al.</italic>, 2014</xref>). Even if the majority of pixel inputs are ‘dropped out’ completely for some samples, this model can still be trained to predict accurately and can handle the uncertainty (<xref rid="btz342-B38" ref-type="bibr">Wager <italic>et al.</italic>, 2013</xref>).</p>
    <p>The prognosis prediction task is more unstructured than traditional deep learning tasks; instead of classifying from relatively small images (224 × 224 for ImageNet, e.g.), we must predict survival times from a combination of clinical, genomic and WSI images that are much higher resolution. Furthermore, patients span a wide variety of cancer types, and are often missing some form of imaging, clinical or genomic data, making it difficult to apply standard CNNs. Unsupervised learning has shown significant promise (<xref rid="btz342-B14" ref-type="bibr">Fan <italic>et al.</italic>, 2018</xref>). By learning unsupervised correlations among imaging features and genomic features, it may be possible to overcome the paucity of data labels. Similarly, representation learning techniques might allow us to exploit similarities and relationships between data modalities (<xref rid="btz342-B23" ref-type="bibr">Kaiser <italic>et al.</italic>, 2017</xref>). In prognosis prediction, it is crucial that the model maps similar patients to the same abstract representation in a way that is agnostic to data modality and availability. We propose to use unsupervised and representation learning to tackle many of the challenges that make prognosis prediction using multimodal data difficult.</p>
  </sec>
  <sec>
    <title>3 Materials and methods</title>
    <sec>
      <title>3.1 Datasets and tools</title>
      <p>Our main source of data is preprocessed and batch corrected data from the PanCanAtlas TCGA project (<xref rid="btz342-B5" ref-type="bibr">Campbell <italic>et al.</italic>, 2018</xref>; <xref rid="btz342-B32" ref-type="bibr">Malta <italic>et al.</italic>, 2018</xref>; <xref rid="btz342-B41" ref-type="bibr">Weinstein <italic>et al.</italic>, 2013</xref>). This dataset contains data for 1881 microRNAs, gene expression data for 60 383 genes, a wide range of clinical data, of which we used the race, age, gender and histological grade variables, and WSI data for over 11 000 patients. <xref rid="btz342-T1" ref-type="table">Table 1</xref> describes the data distribution in more detail. Many patients do not have all data available, implying that classifiers and architectures that can deal with missing data are warranted. Each patient has a time of death recorded, right-censored up to a maximum of 11 000 days after diagnosis across all cancer sites. The 20 cancers we examine have significantly different survival patterns, as can be seen in <xref ref-type="fig" rid="btz342-F1">Figure 1</xref>. We rely on the Python package openslide to efficiently read and parse WSIs and the PyTorch framework to enable the creation of neural network models. To train our models, we use an NVIDIA™ GTX 1070 GPU.</p>
      <table-wrap id="btz342-T1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>Data distribution of TCGA data including missing data</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Data type</th>
              <th rowspan="1" colspan="1">Number of cases</th>
              <th rowspan="1" colspan="1">Number of missing cases</th>
              <th rowspan="1" colspan="1">Percentage missing (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Gene expression data</td>
              <td rowspan="1" colspan="1">10 198</td>
              <td rowspan="1" colspan="1">962</td>
              <td rowspan="1" colspan="1">8.62</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MicroRNA expression data</td>
              <td rowspan="1" colspan="1">10 125</td>
              <td rowspan="1" colspan="1">1035</td>
              <td rowspan="1" colspan="1">9.27</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">WSI slide data</td>
              <td rowspan="1" colspan="1">10 914</td>
              <td rowspan="1" colspan="1">246</td>
              <td rowspan="1" colspan="1">2.2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Clinical data</td>
              <td rowspan="1" colspan="1">7512</td>
              <td rowspan="1" colspan="1">3648</td>
              <td rowspan="1" colspan="1">32.69</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Survival target data (time of death)</td>
              <td rowspan="1" colspan="1">11 121</td>
              <td rowspan="1" colspan="1">39</td>
              <td rowspan="1" colspan="1">0.35</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Patients with complete data</td>
              <td rowspan="1" colspan="1">6404</td>
              <td rowspan="1" colspan="1">4756</td>
              <td rowspan="1" colspan="1">42.62</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic>Note:</italic> Survival data are available for the majority of patients, while microRNA and clinical data are missing in a subset of patients. Nearly 43% of patients have at least one type of missing data.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <fig id="btz342-F1" orientation="portrait" position="float">
        <label>Fig. 1.</label>
        <caption>
          <p>Kaplan–Meier survival curves for all cancer sites in TCGA demonstrating that overall survival is tissue specific. The first graph contains the 10 cancers with the highest mean overall survival, the second graph contains the 10 cancers with the lowest mean overall survival</p>
        </caption>
        <graphic xlink:href="btz342f1"/>
      </fig>
      <p>The TCGA dataset of 11 160 patients was split into training and testing datasets in 85/15 ratio, stratifying by cancer type in order to ensure the same distribution of cancers in both the training and test sets.</p>
    </sec>
    <sec>
      <title>3.2 Deep unsupervised representation learning</title>
      <p>In order to train a pancancer model for prognosis prediction, we first attempt to compress multiple data modalities into a single feature vector that represents a patient. Previous work has found significant cross-correlations between different data types (e.g. gene expression, clinical, microRNA and image data) (<xref rid="btz342-B19" ref-type="bibr">Gevaert <italic>et al.</italic>, 2012</xref>; <xref rid="btz342-B33" ref-type="bibr">Momeni <italic>et al.</italic>, 2018a</xref>), and learning these relations in an unsupervised fashion could significantly improve the prognosis prediction process. Thus, we use a representation learning framework to guide our approach. Although approaches such as split-brain autoencoders induce convergence between different multimodal feature representations, they rely on reconstruction error, which may not be a good choice for heterogeneous data sources. Instead, we rely on a method inspired by <xref rid="btz342-B7" ref-type="bibr">Chopra <italic>et al.</italic> (2005</xref>), in which two different views of objects are passed through a Siamese network to create feature representations. For views from the same object, the cosine similarity between these feature representations is maximized, whereas for views from different objects, the cosine similarity is minimized. To ensure stability, a margin-based, hinge-loss formulation is used, such that different-object feature representations are only penalized if they fall within a margin <italic>M</italic> of the same-object representations. This forces different views of a single patient’s information to have similar feature vectors, while avoiding mode collapse where all features predict exactly the same vector for all patients.</p>
      <p>In this work, we use a similar formulation as (<xref rid="btz342-B7" ref-type="bibr">Chopra <italic>et al.</italic>, 2005</xref>), but with some modifications. Because of the different data modalities, instead of using a Siamese network, we use one deep neural network for each data type, with differing architectures described in <xref ref-type="fig" rid="btz342-F2">Figure 2</xref>. We define the feature space to have a length of 512 based on empirical evidence (data not shown). Since we have more than two different modalities, we sum over the similarity loss for each pair of modalities that are present. We can define the loss <inline-formula id="IE1"><mml:math id="IM1"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">sim</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>θ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> as in <xref ref-type="disp-formula" rid="E1 E2 E3">Equations (1)–(3)</xref>:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi mathvariant="italic">si</mml:mi><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mtext>modalities</mml:mtext></mml:mrow></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>θ</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>θ</mml:mo><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>θ</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>θ</mml:mo><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="italic">si</mml:mi><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="italic">si</mml:mi><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">sim</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>θ</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>x<sub>i</sub></italic> is the data for modality <italic>i</italic> and <inline-formula id="IE2"><mml:math id="IM2"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>θ</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the predictive model for modality <italic>i</italic>. Note that the parameter <italic>M</italic> controls the ‘tightness’ of the clustering. If <italic>M</italic> is high, feature vectors for a given patient are permitted to be relatively different, as long as they stay similar to a certain extent. If <italic>M</italic> is low, feature vectors for a patient are forced to be much closer together, which is usually more ideal, but can also cause mode collapse. We settled on <italic>M </italic>= 0.1 as the default value based on our observations that it is the smallest value of <italic>M</italic> that does not cause mode collapse. This loss is computed between every pair of patients in a batch. Thus, the unsupervised model must learn to recognize important, patient-distinguishing patterns in genomic and image data. Moreover, it must learn how patterns in one modality correspond to patterns in a different modality, so it can generate similar encodings for both. As a result, this method naturally generates compact patient representations that are resilient to missing data. The entire process is summarized in <xref ref-type="fig" rid="btz342-F2">Figure 2</xref>.
</p>
      <fig id="btz342-F2" orientation="portrait" position="float">
        <label>Fig. 2.</label>
        <caption>
          <p>Structure of the unsupervised model: the similarity loss can be visualized as projecting representations of different modalities in the same space. Each modality uses a different network architecture. For the clinical data, we use FC layers with sigmoid activations, for the genomic data we use deep highway networks (<xref rid="btz342-B37" ref-type="bibr">Srivastava <italic>et al.</italic>, 2015</xref>) and for the WSI images, we use the SqueezeNet architecture (<xref rid="btz342-B22" ref-type="bibr">Iandola <italic>et al.</italic>, 2016</xref>) (see main text for architecture details). These architectures generate feature vectors that are then aggregated into a single representation and used to predict overall survival</p>
        </caption>
        <graphic xlink:href="btz342f2"/>
      </fig>
    </sec>
    <sec>
      <title>3.3 Prognosis prediction</title>
      <p>In addition to learning the feature representation, the model must also accurately predict prognosis. Because this is a survival data problem, we aim to maximize the concordance score or C-index. Previous research has defined the Cox loss function (<xref rid="btz342-B24" ref-type="bibr">Katzman <italic>et al.</italic>, 2016</xref>), which optimizes the Cox partial likelihood, as the best way to maximize concordance differentiably. Thus, we add a final prediction layer that maps the 512 feature vector to a survival prediction. We use the standard formulation of Cox loss to train the model. Cox loss is defined as
<disp-formula id="E4"><label>(4)</label><mml:math id="M4"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">cox</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>θ</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where the values <italic>T<sub>i</sub></italic>, <italic>E<sub>i</sub></italic> and <italic>x<sub>i</sub></italic> are, respectively, the survival time, the censorship flag and the data for each patient, and <inline-formula id="IE3"><mml:math id="IM3"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> represents the neural network model trained to predict survival times. The loss is computed over all patients whose lack of survival was observed. Combining with the unsupervised model, the overall loss becomes
<disp-formula id="E5"><label>(5)</label><mml:math id="M5"><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>θ</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">sim</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>θ</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">cox</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>θ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
    </sec>
    <sec>
      <title>3.4 Model architectures</title>
      <p>We use a dedicated CNN architecture for each data type. For the clinical data, we use fully connected (FC) layers (<xref ref-type="fig" rid="btz342-F2">Fig. 2</xref>) with sigmoid activations and dropout as encoders. For the gene and microRNA data, we use highway networks as the architecture (<xref rid="btz342-B37" ref-type="bibr">Srivastava <italic>et al.</italic>, 2015</xref>). Because of the complexity and scale of WSI images, we use the CNN architecture to encode the image data. These architectures are now described in more detail.</p>
      <p>The genomic and microRNA patient data sources are represented by dense, large one-dimensional vectors and neural networks are not the traditional choice for such problems, e.g. support vector machines or random forests are more commonly used (<xref rid="btz342-B10" ref-type="bibr">Daemen <italic>et al.</italic>, 2008</xref><xref rid="btz342-B11" ref-type="bibr">, 2009</xref>). However, in order to differentiably optimize the similarity and Cox loss, we must use CNNs to predict these features. Recent improvements to the state-of-the-art have made deep learning approaches competitive with other approaches. Thus, we use deep highway networks to train 10-layer deep feature predictors without compromising gradient flow through a neural gating approach (<xref rid="btz342-B37" ref-type="bibr">Srivastava <italic>et al.</italic>, 2015</xref>). Highway networks use LSTM-style sigmoidal gating to control gradient flow between deep layers, combating the problem of ‘vanishing’ and ‘exploding’ gradient in very deep feed forward neural networks (<xref ref-type="fig" rid="btz342-F2">Fig. 2</xref>).</p>
      <p>In order to represent and encode WSIs, we need to develop machine learning methods that can effectively ‘summarize’ WSIs. However, the high resolution of WSIs makes learning from them in their entirety difficult. Thus, there must be an element of stochastic sampling and filtering involved. In this work, we use a relatively simple approach to sample ROIs. We sample 200 224 × 224 pixel patches at the highest resolution, then compute the ‘color balance’ of each patch; i.e. how far the average (R, G, B) color value deviates from the mean (R, G, B) value of the entire WSI using mean-squared error. Then, we select the top 20% of these 200 patches (or 40 patches) as ROIs; this ensures that ‘non-representative’ patches belonging to white-space and over-staining are ignored. These 40 ROIs represent, on average, 15% of the tissue region within the WSI. Next, we apply a SqueezeNet model (<xref rid="btz342-B22" ref-type="bibr">Iandola <italic>et al.</italic>, 2016</xref>) on these 40 ROIs, with the last layer being replaced by the length-512 feature encoding predictor. The architecture is detailed in <xref ref-type="fig" rid="btz342-F3">Figure 3</xref>. This model is connected to the broader network as shown in <xref ref-type="fig" rid="btz342-F2">Figure 2</xref>, and is trained using the similarity and Cox loss terms. Because the SqueezeNet model is designed to be computationally efficient, we can train on a large percentage of the WSI patches without sacrificing performance. We tuned the hyper parameters of these model architectures on a validation set to find the final model parameters (<xref ref-type="fig" rid="btz342-F2">Figs 2 and 3</xref>). To evaluate the performance of our model, we use the concordance score (C-index) on the test dataset.
</p>
      <fig id="btz342-F3" orientation="portrait" position="float">
        <label>Fig. 3.</label>
        <caption>
          <p>The SqueezeNet model architecture. The SqueezeNet architecture consists of a set of fire modules interspersed with maxpool layers. Each fire module consists of a squeeze layer (with 1 × 1 convolution filters) and expand layer (with a mix of 1 × 1 and 3 × 3 convolution filters). This fire module architecture helps to reduce the parameter space for faster training. We replaced the final softmax layer of the original SqueezeNet model with the 512-length feature encoding predictor</p>
        </caption>
        <graphic xlink:href="btz342f3"/>
      </fig>
    </sec>
    <sec>
      <title>3.5 Multimodal dropout</title>
      <p>Dropout is a commonly used regularization technique in deep neural network architectures in which some randomly selected neurons are dropped out during the training, forcing other neurons to step in to make predictions for missing neurons. This technique results in less overfitting and more generalization (<xref rid="btz342-B36" ref-type="bibr">Srivastava <italic>et al.</italic>, 2014</xref>). We developed a variation of dropout, multimodal dropout, to improve the network’s ability to deal with missing data. In this method, instead of dropping neurons, we drop entire feature vectors corresponding to each modality, and scale up the weights of the other modalities correspondingly similar to our previous work (<xref rid="btz342-B33" ref-type="bibr">Momeni <italic>et al.</italic>, 2018a</xref>). This is applied to each data sample during training with probability <italic>P</italic> for each modality, to force the network to create representations that are robust to missing data modalities. We experimented with a number of different values for <italic>P</italic> before settling on 25% as optimal.</p>
    </sec>
    <sec>
      <title>3.6 Visualization</title>
      <p>T-distributed stochastic neighbor embedding, or T-SNE, is a commonly used visualization technique that maps points in high-dimensional vector spaces into lower-dimensions (<xref rid="btz342-B30" ref-type="bibr">Maaten and Hinton, 2008</xref>). Unlike other dimensionality reduction techniques like Principal Component Analysis (PCA), T-SNE produces more visually interpretable results by converting vector similarities into joint probabilities, generating visually distinct clusters that represent patterns in the data. Here, we use T-SNE to cluster and show the relationships between our length-512 feature vectors representing patients. Because T-SNE is computationally intensive, we first used PCA to project these vectors into a 50-dimensional space, then apply T-SNE to map them into 2D space.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Results and discussion</title>
    <sec>
      <title>4.1 Unsupervised learning representations</title>
      <p>We first evaluated the unsupervised representation learning of our model architecture by visualizing the encodings of the pancancer patient cohort (<xref ref-type="fig" rid="btz342-F4">Fig. 4</xref>). Clusters of patients with similar feature representations tend to have the same traits (race, sex and cancer type), even though the model was not explicitly trained on these variables. The CNN model thus learned, in an unsupervised fashion, relationships between factors such as sex, race and cancer type across different modalities. These results suggest that the unsupervised model can effectively summarize information from multimodal data and our proposed unsupervised encoding could act as a pancancer ‘patient profile’.
</p>
      <fig id="btz342-F4" orientation="portrait" position="float">
        <label>Fig. 4.</label>
        <caption>
          <p>T-SNE-mapped representations of feature vectors T-SNE-mapped representations of feature vectors for 500 patients within the testing set. The 512-length feature vectors were compressed using PCA (50 features) and T-SNE into the 2D space. These representations manage to capture relationships between patients; e.g. patients with the same sex were generally clustered together (left image), and to a lesser extent, patients of the same race and same cancer type tended to be clustered as well (center and right), even when those clinical features were not provided to the model</p>
        </caption>
        <graphic xlink:href="btz342f4"/>
      </fig>
    </sec>
    <sec>
      <title>4.2 Evaluation of multimodal dropout</title>
      <p>Next, we evaluated the use of the multimodal dropout when integrating multimodal clinical, gene expression, microRNA and WSIs across 20 cancer sites to predict the survival of patients. We train the models for 80 epochs and we see model convergence within that span (<xref ref-type="fig" rid="btz342-F5">Fig. 5</xref>). This analysis also showed that the validation C-index improves when using multimodal dropout during training (<xref ref-type="fig" rid="btz342-F5">Fig. 5</xref>), indicating that randomly dropping-out feature vectors during training improves the network’s ability to build accurate representations from missing multimodal data.
</p>
      <fig id="btz342-F5" orientation="portrait" position="float">
        <label>Fig. 5.</label>
        <caption>
          <p>Evaluation of multimodal dropout: learning rate in terms of C-index of the model on the validation dataset for predicting prognosis across 20 cancer sites combining multimodal data. The model converges after 40 epochs and shows that multimodal dropout improves the validation performance</p>
        </caption>
        <graphic xlink:href="btz342f5"/>
      </fig>
    </sec>
    <sec>
      <title>4.3 Pancancer prognosis prediction</title>
      <p>Next, we used our model on the test dataset to predict prognosis in single cancer and pancancer experiments. We compared different combinations of modalities, always including clinical data, and we evaluated the use of multimodal dropout. We observed that only for the integration of clinical and mRNA, multimodal dropout did not improve the results. For the model that is trained with all modalities, many of the cancer types (15 out of 20) have a higher C-index compared to the training without multimodal dropout with an average an improvement of 2.8%. Similar results are observed for integrating less data modalities (<xref rid="btz342-T2" ref-type="table">Table 2</xref>). In addition, the pancancer model integrating clinical, mRNA, miRNA and WSI achieves an overall C-index of 0.78 on all cancers with multimodal dropout versus 0.75 without dropout. Also for the other pancancer models integrating two or three data modalities, an improvement in multimodal dropout was observed except for the integration of clinical and mRNA data (<xref rid="btz342-T2" ref-type="table">Table 2</xref>).</p>
      <table-wrap id="btz342-T2" orientation="portrait" position="float">
        <label>Table 2.</label>
        <caption>
          <p>Model performance using C-index on the 20 studied cancer types, using different combinations of data modalities</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th colspan="3" rowspan="1">Clin+miRNA+mRNA+WSI<hr/></th>
              <th colspan="3" rowspan="1">Clin+miRNA<hr/></th>
              <th colspan="3" rowspan="1">Clin+mRNA<hr/></th>
              <th colspan="3" rowspan="1">Clin+miRNA+mRNA<hr/></th>
              <th colspan="3" rowspan="1">Clin+miRNA+WSI<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Cancer site</th>
              <th rowspan="1" colspan="1">Baseline</th>
              <th rowspan="1" colspan="1">Multimodal dropout</th>
              <th rowspan="1" colspan="1">Delta (%)</th>
              <th rowspan="1" colspan="1">Baseline</th>
              <th rowspan="1" colspan="1">Multimodal dropout</th>
              <th rowspan="1" colspan="1">Delta (%)</th>
              <th rowspan="1" colspan="1">Baseline</th>
              <th rowspan="1" colspan="1">Multimodal dropout</th>
              <th rowspan="1" colspan="1">Delta (%)</th>
              <th rowspan="1" colspan="1">Baseline</th>
              <th rowspan="1" colspan="1">Multimodal dropout</th>
              <th rowspan="1" colspan="1">Delta (%)</th>
              <th rowspan="1" colspan="1">Baseline</th>
              <th rowspan="1" colspan="1">Multimodal dropout</th>
              <th rowspan="1" colspan="1">Delta (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">BLCA</td>
              <td rowspan="1" colspan="1">0.65</td>
              <td rowspan="1" colspan="1">0.73</td>
              <td rowspan="1" colspan="1">12.6</td>
              <td rowspan="1" colspan="1">0.66</td>
              <td rowspan="1" colspan="1">0.69</td>
              <td rowspan="1" colspan="1">4.4</td>
              <td rowspan="1" colspan="1">0.60</td>
              <td rowspan="1" colspan="1">0.58</td>
              <td rowspan="1" colspan="1">−4.4</td>
              <td rowspan="1" colspan="1">0.65</td>
              <td rowspan="1" colspan="1">0.62</td>
              <td rowspan="1" colspan="1">−5.1</td>
              <td rowspan="1" colspan="1">0.65</td>
              <td rowspan="1" colspan="1">0.68</td>
              <td rowspan="1" colspan="1">4.3</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BRCA</td>
              <td rowspan="1" colspan="1">0.77</td>
              <td rowspan="1" colspan="1">0.79</td>
              <td rowspan="1" colspan="1">3.0</td>
              <td rowspan="1" colspan="1">0.80</td>
              <td rowspan="1" colspan="1">0.80</td>
              <td rowspan="1" colspan="1">−0.1</td>
              <td rowspan="1" colspan="1">0.57</td>
              <td rowspan="1" colspan="1">0.56</td>
              <td rowspan="1" colspan="1">−1.9</td>
              <td rowspan="1" colspan="1">0.73</td>
              <td rowspan="1" colspan="1">0.73</td>
              <td rowspan="1" colspan="1">0.3</td>
              <td rowspan="1" colspan="1">0.77</td>
              <td rowspan="1" colspan="1">0.77</td>
              <td rowspan="1" colspan="1">0.0</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CESC</td>
              <td rowspan="1" colspan="1">0.73</td>
              <td rowspan="1" colspan="1">0.76</td>
              <td rowspan="1" colspan="1">4.6</td>
              <td rowspan="1" colspan="1">0.77</td>
              <td rowspan="1" colspan="1">0.76</td>
              <td rowspan="1" colspan="1">−1.2</td>
              <td rowspan="1" colspan="1">0.67</td>
              <td rowspan="1" colspan="1">0.62</td>
              <td rowspan="1" colspan="1">−6.9</td>
              <td rowspan="1" colspan="1">0.74</td>
              <td rowspan="1" colspan="1">0.74</td>
              <td rowspan="1" colspan="1">0.4</td>
              <td rowspan="1" colspan="1">0.78</td>
              <td rowspan="1" colspan="1">0.76</td>
              <td rowspan="1" colspan="1">−2.5</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">COADREAD</td>
              <td rowspan="1" colspan="1">0.72</td>
              <td rowspan="1" colspan="1">0.74</td>
              <td rowspan="1" colspan="1">3.8</td>
              <td rowspan="1" colspan="1">0.78</td>
              <td rowspan="1" colspan="1">0.75</td>
              <td rowspan="1" colspan="1">−4.8</td>
              <td rowspan="1" colspan="1">0.72</td>
              <td rowspan="1" colspan="1">0.58</td>
              <td rowspan="1" colspan="1">−20.0</td>
              <td rowspan="1" colspan="1">0.77</td>
              <td rowspan="1" colspan="1">0.64</td>
              <td rowspan="1" colspan="1">−16.9</td>
              <td rowspan="1" colspan="1">0.70</td>
              <td rowspan="1" colspan="1">0.74</td>
              <td rowspan="1" colspan="1">4.5</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HNSC</td>
              <td rowspan="1" colspan="1">0.61</td>
              <td rowspan="1" colspan="1">0.67</td>
              <td rowspan="1" colspan="1">10.4</td>
              <td rowspan="1" colspan="1">0.64</td>
              <td rowspan="1" colspan="1">0.64</td>
              <td rowspan="1" colspan="1">0.7</td>
              <td rowspan="1" colspan="1">0.58</td>
              <td rowspan="1" colspan="1">0.55</td>
              <td rowspan="1" colspan="1">−5.4</td>
              <td rowspan="1" colspan="1">0.63</td>
              <td rowspan="1" colspan="1">0.66</td>
              <td rowspan="1" colspan="1">4.6</td>
              <td rowspan="1" colspan="1">0.61</td>
              <td rowspan="1" colspan="1">0.65</td>
              <td rowspan="1" colspan="1">6.6</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">KICH</td>
              <td rowspan="1" colspan="1">0.95</td>
              <td rowspan="1" colspan="1">0.93</td>
              <td rowspan="1" colspan="1">−2.0</td>
              <td rowspan="1" colspan="1">0.82</td>
              <td rowspan="1" colspan="1">0.85</td>
              <td rowspan="1" colspan="1">3.0</td>
              <td rowspan="1" colspan="1">0.80</td>
              <td rowspan="1" colspan="1">0.84</td>
              <td rowspan="1" colspan="1">5.5</td>
              <td rowspan="1" colspan="1">0.73</td>
              <td rowspan="1" colspan="1">0.77</td>
              <td rowspan="1" colspan="1">5.9</td>
              <td rowspan="1" colspan="1">0.81</td>
              <td rowspan="1" colspan="1">0.88</td>
              <td rowspan="1" colspan="1">9.7</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">KIRC</td>
              <td rowspan="1" colspan="1">0.73</td>
              <td rowspan="1" colspan="1">0.73</td>
              <td rowspan="1" colspan="1">−0.3</td>
              <td rowspan="1" colspan="1">0.70</td>
              <td rowspan="1" colspan="1">0.72</td>
              <td rowspan="1" colspan="1">3.1</td>
              <td rowspan="1" colspan="1">0.61</td>
              <td rowspan="1" colspan="1">0.65</td>
              <td rowspan="1" colspan="1">5.9</td>
              <td rowspan="1" colspan="1">0.65</td>
              <td rowspan="1" colspan="1">0.66</td>
              <td rowspan="1" colspan="1">2.7</td>
              <td rowspan="1" colspan="1">0.68</td>
              <td rowspan="1" colspan="1">0.61</td>
              <td rowspan="1" colspan="1">−11.1</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">KIRP</td>
              <td rowspan="1" colspan="1">0.84</td>
              <td rowspan="1" colspan="1">0.79</td>
              <td rowspan="1" colspan="1">−6.0</td>
              <td rowspan="1" colspan="1">0.76</td>
              <td rowspan="1" colspan="1">0.79</td>
              <td rowspan="1" colspan="1">4.1</td>
              <td rowspan="1" colspan="1">0.65</td>
              <td rowspan="1" colspan="1">0.64</td>
              <td rowspan="1" colspan="1">−1.0</td>
              <td rowspan="1" colspan="1">0.61</td>
              <td rowspan="1" colspan="1">0.70</td>
              <td rowspan="1" colspan="1">14.5</td>
              <td rowspan="1" colspan="1">0.79</td>
              <td rowspan="1" colspan="1">0.86</td>
              <td rowspan="1" colspan="1">9.2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LAML</td>
              <td rowspan="1" colspan="1">0.66</td>
              <td rowspan="1" colspan="1">0.67</td>
              <td rowspan="1" colspan="1">1.8</td>
              <td rowspan="1" colspan="1">0.69</td>
              <td rowspan="1" colspan="1">0.79</td>
              <td rowspan="1" colspan="1">14.9</td>
              <td rowspan="1" colspan="1">0.57</td>
              <td rowspan="1" colspan="1">0.61</td>
              <td rowspan="1" colspan="1">7.4</td>
              <td rowspan="1" colspan="1">0.66</td>
              <td rowspan="1" colspan="1">0.57</td>
              <td rowspan="1" colspan="1">−12.8</td>
              <td rowspan="1" colspan="1">0.61</td>
              <td rowspan="1" colspan="1">0.59</td>
              <td rowspan="1" colspan="1">−2.8</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LGG</td>
              <td rowspan="1" colspan="1">0.83</td>
              <td rowspan="1" colspan="1">0.85</td>
              <td rowspan="1" colspan="1">3.4</td>
              <td rowspan="1" colspan="1">0.79</td>
              <td rowspan="1" colspan="1">0.81</td>
              <td rowspan="1" colspan="1">2.0</td>
              <td rowspan="1" colspan="1">0.63</td>
              <td rowspan="1" colspan="1">0.67</td>
              <td rowspan="1" colspan="1">6.3</td>
              <td rowspan="1" colspan="1">0.77</td>
              <td rowspan="1" colspan="1">0.78</td>
              <td rowspan="1" colspan="1">1.4</td>
              <td rowspan="1" colspan="1">0.76</td>
              <td rowspan="1" colspan="1">0.82</td>
              <td rowspan="1" colspan="1">8.2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LIHC</td>
              <td rowspan="1" colspan="1">0.72</td>
              <td rowspan="1" colspan="1">0.77</td>
              <td rowspan="1" colspan="1">7.6</td>
              <td rowspan="1" colspan="1">0.73</td>
              <td rowspan="1" colspan="1">0.74</td>
              <td rowspan="1" colspan="1">2.7</td>
              <td rowspan="1" colspan="1">0.64</td>
              <td rowspan="1" colspan="1">0.69</td>
              <td rowspan="1" colspan="1">7.7</td>
              <td rowspan="1" colspan="1">0.68</td>
              <td rowspan="1" colspan="1">0.67</td>
              <td rowspan="1" colspan="1">−1.8</td>
              <td rowspan="1" colspan="1">0.70</td>
              <td rowspan="1" colspan="1">0.77</td>
              <td rowspan="1" colspan="1">11.2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LUAD</td>
              <td rowspan="1" colspan="1">0.72</td>
              <td rowspan="1" colspan="1">0.73</td>
              <td rowspan="1" colspan="1">1.3</td>
              <td rowspan="1" colspan="1">0.72</td>
              <td rowspan="1" colspan="1">0.72</td>
              <td rowspan="1" colspan="1">−0.9</td>
              <td rowspan="1" colspan="1">0.63</td>
              <td rowspan="1" colspan="1">0.58</td>
              <td rowspan="1" colspan="1">−8.9</td>
              <td rowspan="1" colspan="1">0.73</td>
              <td rowspan="1" colspan="1">0.69</td>
              <td rowspan="1" colspan="1">−5.1</td>
              <td rowspan="1" colspan="1">0.69</td>
              <td rowspan="1" colspan="1">0.77</td>
              <td rowspan="1" colspan="1">10.5</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LUSC</td>
              <td rowspan="1" colspan="1">0.67</td>
              <td rowspan="1" colspan="1">0.66</td>
              <td rowspan="1" colspan="1">−0.9</td>
              <td rowspan="1" colspan="1">0.72</td>
              <td rowspan="1" colspan="1">0.67</td>
              <td rowspan="1" colspan="1">−6.5</td>
              <td rowspan="1" colspan="1">0.50</td>
              <td rowspan="1" colspan="1">0.51</td>
              <td rowspan="1" colspan="1">2.1</td>
              <td rowspan="1" colspan="1">0.62</td>
              <td rowspan="1" colspan="1">0.60</td>
              <td rowspan="1" colspan="1">−2.9</td>
              <td rowspan="1" colspan="1">0.67</td>
              <td rowspan="1" colspan="1">0.68</td>
              <td rowspan="1" colspan="1">0.5</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">OV</td>
              <td rowspan="1" colspan="1">0.63</td>
              <td rowspan="1" colspan="1">0.67</td>
              <td rowspan="1" colspan="1">6.4</td>
              <td rowspan="1" colspan="1">0.65</td>
              <td rowspan="1" colspan="1">0.63</td>
              <td rowspan="1" colspan="1">−2.2</td>
              <td rowspan="1" colspan="1">0.47</td>
              <td rowspan="1" colspan="1">0.52</td>
              <td rowspan="1" colspan="1">11.5</td>
              <td rowspan="1" colspan="1">0.59</td>
              <td rowspan="1" colspan="1">0.61</td>
              <td rowspan="1" colspan="1">3.5</td>
              <td rowspan="1" colspan="1">0.62</td>
              <td rowspan="1" colspan="1">0.69</td>
              <td rowspan="1" colspan="1">10.4</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PAAD</td>
              <td rowspan="1" colspan="1">0.71</td>
              <td rowspan="1" colspan="1">0.74</td>
              <td rowspan="1" colspan="1">3.5</td>
              <td rowspan="1" colspan="1">0.68</td>
              <td rowspan="1" colspan="1">0.71</td>
              <td rowspan="1" colspan="1">3.8</td>
              <td rowspan="1" colspan="1">0.57</td>
              <td rowspan="1" colspan="1">0.61</td>
              <td rowspan="1" colspan="1">7.6</td>
              <td rowspan="1" colspan="1">0.59</td>
              <td rowspan="1" colspan="1">0.64</td>
              <td rowspan="1" colspan="1">8.9</td>
              <td rowspan="1" colspan="1">0.69</td>
              <td rowspan="1" colspan="1">0.69</td>
              <td rowspan="1" colspan="1">0.3</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PRAD</td>
              <td rowspan="1" colspan="1">0.77</td>
              <td rowspan="1" colspan="1">0.81</td>
              <td rowspan="1" colspan="1">0.0</td>
              <td rowspan="1" colspan="1">0.64</td>
              <td rowspan="1" colspan="1">0.64</td>
              <td rowspan="1" colspan="1">−0.3</td>
              <td rowspan="1" colspan="1">0.60</td>
              <td rowspan="1" colspan="1">0.58</td>
              <td rowspan="1" colspan="1">−3.5</td>
              <td rowspan="1" colspan="1">0.59</td>
              <td rowspan="1" colspan="1">0.78</td>
              <td rowspan="1" colspan="1">32.8</td>
              <td rowspan="1" colspan="1">0.53</td>
              <td rowspan="1" colspan="1">0.60</td>
              <td rowspan="1" colspan="1">13.4</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SKCM</td>
              <td rowspan="1" colspan="1">0.68</td>
              <td rowspan="1" colspan="1">0.72</td>
              <td rowspan="1" colspan="1">5.2</td>
              <td rowspan="1" colspan="1">0.68</td>
              <td rowspan="1" colspan="1">0.68</td>
              <td rowspan="1" colspan="1">−0.1</td>
              <td rowspan="1" colspan="1">0.56</td>
              <td rowspan="1" colspan="1">0.55</td>
              <td rowspan="1" colspan="1">−0.1</td>
              <td rowspan="1" colspan="1">0.58</td>
              <td rowspan="1" colspan="1">0.72</td>
              <td rowspan="1" colspan="1">24.3</td>
              <td rowspan="1" colspan="1">0.67</td>
              <td rowspan="1" colspan="1">0.72</td>
              <td rowspan="1" colspan="1">6.8</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">STAD</td>
              <td rowspan="1" colspan="1">0.76</td>
              <td rowspan="1" colspan="1">0.78</td>
              <td rowspan="1" colspan="1">2.6</td>
              <td rowspan="1" colspan="1">0.75</td>
              <td rowspan="1" colspan="1">0.76</td>
              <td rowspan="1" colspan="1">1.5</td>
              <td rowspan="1" colspan="1">0.63</td>
              <td rowspan="1" colspan="1">0.54</td>
              <td rowspan="1" colspan="1">−13.9</td>
              <td rowspan="1" colspan="1">0.80</td>
              <td rowspan="1" colspan="1">0.69</td>
              <td rowspan="1" colspan="1">−14.1</td>
              <td rowspan="1" colspan="1">0.72</td>
              <td rowspan="1" colspan="1">0.74</td>
              <td rowspan="1" colspan="1">2.6</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">THCA</td>
              <td rowspan="1" colspan="1">0.95</td>
              <td rowspan="1" colspan="1">0.90</td>
              <td rowspan="1" colspan="1">−4.8</td>
              <td rowspan="1" colspan="1">0.97</td>
              <td rowspan="1" colspan="1">0.95</td>
              <td rowspan="1" colspan="1">−2.6</td>
              <td rowspan="1" colspan="1">0.82</td>
              <td rowspan="1" colspan="1">0.54</td>
              <td rowspan="1" colspan="1">−34.2</td>
              <td rowspan="1" colspan="1">0.70</td>
              <td rowspan="1" colspan="1">0.83</td>
              <td rowspan="1" colspan="1">18.7</td>
              <td rowspan="1" colspan="1">0.93</td>
              <td rowspan="1" colspan="1">0.94</td>
              <td rowspan="1" colspan="1">1.4</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">UCEC</td>
              <td rowspan="1" colspan="1">0.85</td>
              <td rowspan="1" colspan="1">0.85</td>
              <td rowspan="1" colspan="1">0.6</td>
              <td rowspan="1" colspan="1">0.81</td>
              <td rowspan="1" colspan="1">0.85</td>
              <td rowspan="1" colspan="1">4.3</td>
              <td rowspan="1" colspan="1">0.63</td>
              <td rowspan="1" colspan="1">0.63</td>
              <td rowspan="1" colspan="1">0.0</td>
              <td rowspan="1" colspan="1">0.66</td>
              <td rowspan="1" colspan="1">0.78</td>
              <td rowspan="1" colspan="1">18.2</td>
              <td rowspan="1" colspan="1">0.77</td>
              <td rowspan="1" colspan="1">0.80</td>
              <td rowspan="1" colspan="1">3.0</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Average improvement</td>
              <td colspan="3" rowspan="1">2.8%</td>
              <td colspan="3" rowspan="1">1.3%</td>
              <td colspan="3" rowspan="1">−2.3%</td>
              <td colspan="3" rowspan="1">3.9%</td>
              <td colspan="3" rowspan="1">4.3%</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Pancancer</td>
              <td rowspan="1" colspan="1">0.75</td>
              <td rowspan="1" colspan="1">0.78</td>
              <td rowspan="1" colspan="1">4.5</td>
              <td rowspan="1" colspan="1">0.74</td>
              <td rowspan="1" colspan="1">0.78</td>
              <td rowspan="1" colspan="1">4.3</td>
              <td rowspan="1" colspan="1">0.60</td>
              <td rowspan="1" colspan="1">0.60</td>
              <td rowspan="1" colspan="1">−1.2</td>
              <td rowspan="1" colspan="1">0.75</td>
              <td rowspan="1" colspan="1">0.78</td>
              <td rowspan="1" colspan="1">3.6</td>
              <td rowspan="1" colspan="1">0.76</td>
              <td rowspan="1" colspan="1">0.78</td>
              <td rowspan="1" colspan="1">3.2</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <p><italic>Note:</italic> Cancer sites are defined according to TCGA cancer codes. For each cancer, the best result is bold faced. Delta refers to the relative performance improvement of the multimodal dropout model compared to the baseline.</p>
          </fn>
          <fn id="tblfn3">
            <p>Clin, clinical data; miRNA, microRNA expression data; mRNA, mRNA expression data; WSI, whole slide images.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>4.4 Essential data modalities</title>
      <p>Next, we investigated using different combinations of modalities together with clinical data, to examine if the genomic and image modalities are crucial for prognosis prediction. We observed that miRNA is the most informative modality while mRNA is the least informative in a pancancer setting when integrating all modalities (C-index of 0.75 versus 0.60 for the baseline pancancer model, <xref rid="btz342-T2" ref-type="table">Table 2</xref>). For single cancers, different combinations of modalities are important. For eight cancer sites, the integration of all four modalities is the best with the most striking example KICH (C-index 0.95). Next, for six cancer sites, integration of clinical, miRNA and WSI gives the best or equal performance to the model integrating all four modalities, suggesting that mRNA is also not essential in these single cancer models for prognosis prediction (<xref rid="btz342-T2" ref-type="table">Table 2</xref>). For example, the best model for KIRP, OV and LUAD results from integrating clinical, miRNA and WSI with C-index of 0.86, 0.69 and 0.77, respectively, suggesting that these three data modalities are sufficient and necessary for these cancer sites prognosis determination.</p>
    </sec>
    <sec>
      <title>4.5 Pancancer pretraining evaluation</title>
      <p>Next, we tested if training on pancancer data actually improved the prediction of survival across each individual cancer site. To test this, we compared the multimodal pancancer results with the results of models trained on each cancer site using an 85–15 train–test split, separately for the multimodal dropout model using all data modalities (i.e. clin + miRNA + mRNA + WSI), and compared the performance for survival prediction using exactly the same test cases for each cancer site. This showed that for all cancer sites pancancer training improves the results except for KIRC where a drop of 6% was observed (<xref rid="btz342-T3" ref-type="table">Table 3</xref>).</p>
      <table-wrap id="btz342-T3" orientation="portrait" position="float">
        <label>Table 3.</label>
        <caption>
          <p>Comparison of pancancer training with single cancer training using the C-index showing that in the case of integrating clinical, miRNA, mRNA and WSI using multimodal dropout, for all but one cancer site (KIRC), pancancer training performs equally or outperforms training on each cancer individually</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Cancer site</th>
              <th rowspan="1" colspan="1">Single cancer</th>
              <th rowspan="1" colspan="1">Pancancer</th>
              <th rowspan="1" colspan="1">Difference (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">BLCA</td>
              <td rowspan="1" colspan="1">0.60</td>
              <td rowspan="1" colspan="1">0.73</td>
              <td rowspan="1" colspan="1">22</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BRCA</td>
              <td rowspan="1" colspan="1">0.62</td>
              <td rowspan="1" colspan="1">0.79</td>
              <td rowspan="1" colspan="1">28</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CESC</td>
              <td rowspan="1" colspan="1">0.52</td>
              <td rowspan="1" colspan="1">0.76</td>
              <td rowspan="1" colspan="1">48</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">COADREAD</td>
              <td rowspan="1" colspan="1">0.58</td>
              <td rowspan="1" colspan="1">0.74</td>
              <td rowspan="1" colspan="1">28</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HNSC</td>
              <td rowspan="1" colspan="1">0.64</td>
              <td rowspan="1" colspan="1">0.67</td>
              <td rowspan="1" colspan="1">6</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">KICH</td>
              <td rowspan="1" colspan="1">0.69</td>
              <td rowspan="1" colspan="1">0.93</td>
              <td rowspan="1" colspan="1">34</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">KIRC</td>
              <td rowspan="1" colspan="1">0.78</td>
              <td rowspan="1" colspan="1">0.73</td>
              <td rowspan="1" colspan="1">−6</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">KIRP</td>
              <td rowspan="1" colspan="1">0.51</td>
              <td rowspan="1" colspan="1">0.79</td>
              <td rowspan="1" colspan="1">56</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LAML</td>
              <td rowspan="1" colspan="1">0.65</td>
              <td rowspan="1" colspan="1">0.67</td>
              <td rowspan="1" colspan="1">4</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LGG</td>
              <td rowspan="1" colspan="1">0.73</td>
              <td rowspan="1" colspan="1">0.85</td>
              <td rowspan="1" colspan="1">18</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LIHC</td>
              <td rowspan="1" colspan="1">0.78</td>
              <td rowspan="1" colspan="1">0.77</td>
              <td rowspan="1" colspan="1">0</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LUAD</td>
              <td rowspan="1" colspan="1">0.72</td>
              <td rowspan="1" colspan="1">0.73</td>
              <td rowspan="1" colspan="1">1</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LUSC</td>
              <td rowspan="1" colspan="1">0.63</td>
              <td rowspan="1" colspan="1">0.66</td>
              <td rowspan="1" colspan="1">5</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">OV</td>
              <td rowspan="1" colspan="1">0.54</td>
              <td rowspan="1" colspan="1">0.67</td>
              <td rowspan="1" colspan="1">24</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PAAD</td>
              <td rowspan="1" colspan="1">0.57</td>
              <td rowspan="1" colspan="1">0.74</td>
              <td rowspan="1" colspan="1">30</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PRAD</td>
              <td rowspan="1" colspan="1">0.76</td>
              <td rowspan="1" colspan="1">0.81</td>
              <td rowspan="1" colspan="1">7</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SKCM</td>
              <td rowspan="1" colspan="1">0.54</td>
              <td rowspan="1" colspan="1">0.72</td>
              <td rowspan="1" colspan="1">33</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">STAD</td>
              <td rowspan="1" colspan="1">0.60</td>
              <td rowspan="1" colspan="1">0.78</td>
              <td rowspan="1" colspan="1">29</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">THCA</td>
              <td rowspan="1" colspan="1">0.53</td>
              <td rowspan="1" colspan="1">0.90</td>
              <td rowspan="1" colspan="1">69</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">UCEC</td>
              <td rowspan="1" colspan="1">0.67</td>
              <td rowspan="1" colspan="1">0.85</td>
              <td rowspan="1" colspan="1">28</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>4.6 Comparison with previous work</title>
      <p>All previous work on prognosis prediction using genomic and WSI data has focused on specific cancer types and data modalities. For example, <xref rid="btz342-B8" ref-type="bibr">Christinat and Krek (2015</xref>) achieved the highest C-index (0.77) thus far, on renal cancer data (TCGA-KIRC). As can be seen from our results, our method performed slightly worse (0.740) on the same type of data. However, our method outperforms a multimodality classifier on lung adenocarcinoma by <xref rid="btz342-B46" ref-type="bibr">Zhu <italic>et al.</italic> (2016</xref>) (0.726 versus 0.691C-index). In general there is no ‘fair comparison’ that can be made between this method and the previous state-of-the-art, especially because most previous papers discard patients with missing data modalities, while our proposed model is able to train and predict with missing data included. Moreover, our methods achieve comparable or better results from previous research by resiliently handling incomplete data and predicting across 20 different cancer types.</p>
    </sec>
  </sec>
  <sec>
    <title>5 Conclusion</title>
    <p>In this paper, we demonstrate a multimodal approach for predicting prognosis using clinical, genomic and WSI data. First, we developed an unsupervised method to encode multimodal patient data into a common feature representation that is independent of data type or modality. We then illustrated that these unsupervised patient encodings are associated with clinical features, and that patients with similar characteristics tend to cluster together in ‘representation-space’. These feature representations act as an integrated multimodal patient profile, enabling machine learning models to compare and contrast patients in a systematic fashion. Thus, these encodings could be useful in a number of contexts, ranging from prognosis prediction to treatment recommendation.</p>
    <p>We then used these feature representations to predict single cancer and pancancer prognosis. On 20 TCGA cancer sites, our methods achieve the overall C-index of 0.784. Furthermore, on cancer types that have few samples (e.g. KICH), our prognostic prediction model is able to estimate prognosis with relatively high accuracy, leveraging unsupervised features and information from other cancer types to overcome data scarcity.</p>
    <p>Our work distinguishes itself in a number of ways, we demonstrate how to build a pancancer model of prognosis. Next, we show the use of multimodal data, novel representation learning techniques and methods such as multimodal dropout to create models that can generalize well and predict also in the absence of one or more data modalities. More specifically, while learning unsupervised relationships between clinical, genomic and image data, our proposed CNN is forced to develop a unique, consistent representation for each patient. Finally, we propose an efficient automated WSI analysis by sampling ROIs per patient representing on average 15% of patient’s lesions.</p>
    <sec>
      <title>6 Future work</title>
      <p>Although we have created an algorithm to select patches from WSI images, our work for modeling WSI can be further improved. Refining the CNN architecture used for encoding the biopsy slides is crucial to further improve the performance. Future research, likely should focus on learning which image patches are important, rather than randomly sampling patches. Furthermore, we can use more advanced, deeper architectures and advanced data augmentation. Another intriguing possibility is using transfer learning on models designed to detect low-level cellular activity like mitoses (<xref rid="btz342-B43" ref-type="bibr">Zagoruyko and Komodakis, 2016</xref>). Because of the well-established connection between mitotic proliferation and cancer, this could help focus the CNN on important cellular features. Next, integrating more diverse sources of data is another key goal. In this research, resource constraints prevented us from exploring other data genomic modalities in TCGA, such as DNA methylation (<xref rid="btz342-B15" ref-type="bibr">Gevaert, 2015</xref>; <xref rid="btz342-B26" ref-type="bibr">Litovkin <italic>et al.</italic>, 2014</xref>) and DNA copy number data (<xref rid="btz342-B20" ref-type="bibr">Gevaert <italic>et al.</italic>, 2013</xref>; <xref rid="btz342-B16" ref-type="bibr">Gevaert and Plevritis, 2013</xref>), all of which have potentially untapped, prognostically relevant information.</p>
    </sec>
  </sec>
  <sec>
    <title>Funding</title>
    <p>Research reported in this publication was supported by the National Institute of Biomedical Imaging and Bioengineering of the National Institutes of Health under award R01EB020527, the National Institute of Dental and Craniofacial Research (NIDCR) under award U01DE025188, and the National Cancer Institute (NCI) under awards U01CA199241 and U01CA217851. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.</p>
    <p><italic>Conflict of Interest</italic>: none declared.</p>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btz342-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alizadeh</surname><given-names>A.A.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Toward understanding and exploiting tumor heterogeneity</article-title>. <source>Nat. Med</source>., <volume>21,</volume><fpage>846</fpage>–<lpage>853</lpage>.<pub-id pub-id-type="pmid">26248267</pub-id></mixed-citation>
    </ref>
    <ref id="btz342-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Beck</surname><given-names>A.H.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Systematic analysis of breast cancer morphology uncovers stromal features associated with survival</article-title>. <source>Sci. Transl. Med</source>., <volume>3</volume>, <fpage>108ra113</fpage>.</mixed-citation>
    </ref>
    <ref id="btz342-B3">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Bejnordi</surname><given-names>B.E.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) <chapter-title>Deep learning-based assessment of tumor-associated stroma for diagnosing breast cancer in histopathology images</chapter-title> In: <source>IEEE 14th International Symposium on Biomedical Imaging 2017 (ISBI 2017)</source>, pp. <fpage>929</fpage>–<lpage>932</lpage>. 
<publisher-name>IEEE, Melbourne, Australia</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btz342-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Calin</surname><given-names>G.A.</given-names></name>, <name name-style="western"><surname>Croce</surname><given-names>C.M.</given-names></name></person-group> (<year>2006</year>) 
<article-title>MicroRNA signatures in human cancers</article-title>. <source>Nat. Rev. Cancer</source>, <volume>6</volume>, <fpage>857</fpage>.<pub-id pub-id-type="pmid">17060945</pub-id></mixed-citation>
    </ref>
    <ref id="btz342-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Campbell</surname><given-names>J.D.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Genomic, pathway network, and immunologic features distinguishing squamous carcinomas</article-title>. <source>Cell Rep</source>., <volume>23</volume>, <fpage>194</fpage>.<pub-id pub-id-type="pmid">29617660</pub-id></mixed-citation>
    </ref>
    <ref id="btz342-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cheerla</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Gevaert</surname><given-names>O.</given-names></name></person-group> (<year>2017</year>) 
<article-title>Microrna based pan-cancer diagnosis and treatment recommendation</article-title>. <source>BMC Bioinform</source>., <volume>18</volume>, <fpage>32</fpage>.</mixed-citation>
    </ref>
    <ref id="btz342-B7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Chopra</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2005</year>) Learning a similarity metric discriminatively, with application to face verification. In: <italic>Proceedings 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005)</italic>, Vol. <volume>I</volume>, 
<publisher-name>IEEE Computer Society, Los Alamitas, CA</publisher-name>, pp. <fpage>539</fpage>–<lpage>546</lpage>.</mixed-citation>
    </ref>
    <ref id="btz342-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Christinat</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Krek</surname><given-names>W.</given-names></name></person-group> (<year>2015</year>) 
<article-title>Integrated genomic analysis identifies subclasses and prognosis signatures of kidney cancer</article-title>. <source>Oncotarget</source>, <volume>6</volume>, <fpage>10521.</fpage><pub-id pub-id-type="pmid">25826081</pub-id></mixed-citation>
    </ref>
    <ref id="btz342-B9">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Cox</surname><given-names>D.R.</given-names></name></person-group> (<year>2018</year>) <source>Analysis of Survival Data</source>. 
<publisher-name>Routledge, New York</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btz342-B10">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Daemen</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) <chapter-title>Integrating microarray and proteomics data to predict the response on cetuximab in patients with rectal cancer</chapter-title> In: <source>Pacific Symposium on Biocomputing 2008</source>, pp. <fpage>166</fpage>–<lpage>177</lpage>. 
<publisher-name>World Scientific</publisher-name>, 
<publisher-loc>Singapore</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btz342-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Daemen</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>A kernel-based integration of genome-wide data for clinical decision support</article-title>. <source>Genome Med</source>., <volume>1</volume>, <fpage>39</fpage>.<pub-id pub-id-type="pmid">19356222</pub-id></mixed-citation>
    </ref>
    <ref id="btz342-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>de Bruin</surname><given-names>E.C.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Intra-tumor heterogeneity: lessons from microbial evolution and clinical implications</article-title>. <source>Genome Med</source>., <volume>5</volume>, <fpage>101</fpage>.<pub-id pub-id-type="pmid">24267946</pub-id></mixed-citation>
    </ref>
    <ref id="btz342-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Esquela-Kerscher</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Slack</surname><given-names>F.J.</given-names></name></person-group> (<year>2006</year>) 
<article-title>Oncomirs—microRNAs with a role in cancer</article-title>. <source>Nat. Rev. Cancer</source>, <volume>6</volume>, <fpage>259</fpage>–<lpage>269</lpage>.<pub-id pub-id-type="pmid">16557279</pub-id></mixed-citation>
    </ref>
    <ref id="btz342-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fan</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Unsupervised person re-identification: clustering and fine-tuning</article-title>. <source>ACM Trans. Multimedia Comput. Commun. Appl</source>., <volume>14</volume>, <fpage>83</fpage>.</mixed-citation>
    </ref>
    <ref id="btz342-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gevaert</surname><given-names>O.</given-names></name></person-group> (<year>2015</year>) 
<article-title>Methylmix: an r package for identifying DNA methylation-driven genes</article-title>. <source>Bioinformatics</source>, <volume>31</volume>, <fpage>1839</fpage>–<lpage>1841</lpage>.<pub-id pub-id-type="pmid">25609794</pub-id></mixed-citation>
    </ref>
    <ref id="btz342-B16">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Gevaert</surname><given-names>O.</given-names></name>, <name name-style="western"><surname>Plevritis</surname><given-names>S.</given-names></name></person-group> (<year>2013</year>) <chapter-title>Identifying master regulators of cancer and their downstream targets by integrating genomic and epigenomic features.</chapter-title> In: <source>Pacific Symposium on Biocomputing 2013</source>, pp. <fpage>123</fpage>–<lpage>134</lpage>. 
<publisher-name>World Scientific</publisher-name>, 
<publisher-loc>Singapore</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btz342-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gevaert</surname><given-names>O.</given-names></name></person-group><etal>et al</etal> (<year>2006</year>) 
<article-title>Predicting the prognosis of breast cancer by integrating clinical and microarray data with Bayesian networks</article-title>. <source>Bioinformatics</source>, <volume>22</volume>, <fpage>e184</fpage>–<lpage>e190</lpage>.<pub-id pub-id-type="pmid">16873470</pub-id></mixed-citation>
    </ref>
    <ref id="btz342-B18">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Gevaert</surname><given-names>O.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) <chapter-title>Integration of microarray and textual data improves the prognosis prediction of breast, lung and ovarian cancer patients</chapter-title> In: <source>Pacific Symposium on Biocomputing 2008</source>, pp. <fpage>279</fpage>–<lpage>290</lpage>. 
<publisher-name>World Scientific</publisher-name>, 
<publisher-loc>Singapore</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btz342-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gevaert</surname><given-names>O.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Non-small cell lung cancer: identifying prognostic imaging biomarkers by leveraging public gene expression microarray data–methods and preliminary results</article-title>. <source>Radiology</source>, <volume>264</volume>, <fpage>387</fpage>–<lpage>396</lpage>.<pub-id pub-id-type="pmid">22723499</pub-id></mixed-citation>
    </ref>
    <ref id="btz342-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gevaert</surname><given-names>O.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Identification of ovarian cancer driver genes by using module network integration of multi-omics data</article-title>. <source>Interface Focus</source>, <volume>3</volume>, 20130013.</mixed-citation>
    </ref>
    <ref id="btz342-B21">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Hou</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) Patch-based convolutional neural network for whole slide tissue image classification. In: <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>, IEEE Computer Society, Los Alamitas, CA, pp. <fpage>2424</fpage>–<lpage>2433</lpage>.</mixed-citation>
    </ref>
    <ref id="btz342-B22">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Iandola</surname><given-names>F.N.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) Squeezenet: Alexnet-level accuracy with 50x fewer parameters and 0.5 mb model size. arXiv:1602.07360.</mixed-citation>
    </ref>
    <ref id="btz342-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kaiser</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>One model to learn them all</article-title>. <source>Int. J. Comput. Vision</source>.</mixed-citation>
    </ref>
    <ref id="btz342-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Katzman</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network. BMC Medical Research Methodology</article-title>, <volume>1606</volume>, <fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation>
    </ref>
    <ref id="btz342-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Deep learning</article-title>. <source>Nature</source>, <volume>521</volume>, <fpage>436</fpage>.<pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="btz342-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Litovkin</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Methylation of PITX2, HOXD3, RASSF1 and TDRD1 predicts biochemical recurrence in high-risk prostate cancer</article-title>. <source>J. Cancer Res. Clin. Oncol</source>., <volume>140</volume>, <fpage>1849</fpage>–<lpage>1861</lpage>.<pub-id pub-id-type="pmid">24938434</pub-id></mixed-citation>
    </ref>
    <ref id="btz342-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>MiRNAs predict the prognosis of patients with triple negative breast cancer: a meta-analysis</article-title>. <source>PLoS One</source>, <volume>12</volume>, <fpage>e0170088</fpage>.<pub-id pub-id-type="pmid">28085956</pub-id></mixed-citation>
    </ref>
    <ref id="btz342-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lovly</surname><given-names>C.M.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Tumor heterogeneity and therapeutic resistance</article-title>. <source>Am, Soc. Clin. Oncol. Educ. Book</source>, <volume>36</volume>, <fpage>e585</fpage>–<lpage>e593</lpage>.</mixed-citation>
    </ref>
    <ref id="btz342-B29">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Luck</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) Deep learning for patient-specific kidney graft survival analysis. arXiv:1705.10245.</mixed-citation>
    </ref>
    <ref id="btz342-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Maaten</surname><given-names>L. v d.</given-names></name>, <name name-style="western"><surname>Hinton</surname><given-names>G.</given-names></name></person-group> (<year>2008</year>) 
<article-title>Visualizing data using t-sne</article-title>. <source>J. Mach. Learn. Res</source>., <volume>9</volume>, <fpage>2579</fpage>–<lpage>2605</lpage>.</mixed-citation>
    </ref>
    <ref id="btz342-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Madabhushi</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>G.</given-names></name></person-group> (<year>2016</year>) 
<article-title>Image analysis and machine learning in digital pathology: challenges and opportunities</article-title>. <source>Med. Image Anal</source>., <volume>33</volume>, <fpage>170</fpage>–<lpage>175</lpage>.<pub-id pub-id-type="pmid">27423409</pub-id></mixed-citation>
    </ref>
    <ref id="btz342-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Malta</surname><given-names>T.M.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Machine learning identifies stemness features associated with oncogenic dedifferentiation</article-title>. <source>Cell</source>, <volume>173</volume>, <fpage>338</fpage>–<lpage>354</lpage>.<pub-id pub-id-type="pmid">29625051</pub-id></mixed-citation>
    </ref>
    <ref id="btz342-B33">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Momeni</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2018a</year>) Deep recurrent attention models for histopathological image analysis, bioRxiv, <fpage>438341</fpage>
<publisher-name>Springer Nature Switzerland, Cham, Switzerland</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btz342-B34">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Momeni</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2018b</year>) <chapter-title>Dropout-enabled ensemble learning for multi-scale biomedical data.</chapter-title> In: <source>International MICCAI Brainlesion Workshop</source>, pp. <fpage>407</fpage>–<lpage>415</lpage>. 
<publisher-name>Springer</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btz342-B35">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Qiu</surname><given-names>Y.L.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) <italic>A deep learning framework for imputing missing values in genomic data</italic>, bioRxiv, 406066.</mixed-citation>
    </ref>
    <ref id="btz342-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Srivastava</surname><given-names>N.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>. <source>J. Mach. Learn. Res</source>., <volume>15</volume>, <fpage>1929</fpage>–<lpage>1958</lpage>.</mixed-citation>
    </ref>
    <ref id="btz342-B37">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Srivastava</surname><given-names>R.K.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) Highway networks. arXiv:1505.00387.</mixed-citation>
    </ref>
    <ref id="btz342-B38">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Wager</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Dropout training as adaptive regularization</article-title>. In: <source>Advances in Neural Information Processing Systems</source>, 
<publisher-name>Curran Associates, Red Hook, NY</publisher-name>, pp. <fpage>351</fpage>–<lpage>359</lpage>.</mixed-citation>
    </ref>
    <ref id="btz342-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Novel image markers for non-small cell lung cancer classification and survival prediction</article-title>. <source>BMC Bioinform</source>., <volume>15</volume>, <fpage>310</fpage>.</mixed-citation>
    </ref>
    <ref id="btz342-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Central focused convolutional neural networks: developing a data-driven model for lung nodule segmentation</article-title>. <source>Med. Image Anal</source>., <volume>40</volume>, <fpage>172</fpage>–<lpage>183</lpage>.<pub-id pub-id-type="pmid">28688283</pub-id></mixed-citation>
    </ref>
    <ref id="btz342-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Weinstein</surname><given-names>J.N.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>The cancer genome atlas pan-cancer analysis project</article-title>. <source>Nat. Genet</source>., <volume>45</volume>, <fpage>1113</fpage>.<pub-id pub-id-type="pmid">24071849</pub-id></mixed-citation>
    </ref>
    <ref id="btz342-B42">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Yao</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) <chapter-title>Imaging biomarker discovery for lung cancer survival prediction</chapter-title> In: <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source>, pp. <fpage>649</fpage>–<lpage>657</lpage>. 
<publisher-name>Springer Nature Switzerland, Cham, Switzerland</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btz342-B43">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Zagoruyko</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Komodakis</surname><given-names>N.</given-names></name></person-group> (<year>2016</year>) Wide residual networks. arXiv:1605.07146.</mixed-citation>
    </ref>
    <ref id="btz342-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Pathway-structured predictive model for cancer survival prediction: a two-stage approach</article-title>. <source>Genetics</source>, <volume>205</volume>, <fpage>89</fpage>–<lpage>100</lpage>.<pub-id pub-id-type="pmid">28049703</pub-id></mixed-citation>
    </ref>
    <ref id="btz342-B45">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Learning deep features for scene recognition using places database</article-title>. In: <source>Advances in Neural Information Processing Systems</source>, 
<publisher-name>Curran Associates, Red Hook, NY</publisher-name>, pp. <fpage>487</fpage>–<lpage>495</lpage>.</mixed-citation>
    </ref>
    <ref id="btz342-B46">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) <chapter-title>Imaging-genetic data mapping for clinical outcome prediction via supervised conditional Gaussian graphical model</chapter-title> In: <source>2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</source>, pp. <fpage>455</fpage>–<lpage>459</lpage>. 
<publisher-name>IEEE, Danvers, MA</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btz342-B47">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>X.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) WSISA: making survival prediction from whole slide histopathological images. In: <italic>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic>, IEEE Computer Society, Los Alamitas, CA, pp. <fpage>7234</fpage>–<lpage>7242</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
