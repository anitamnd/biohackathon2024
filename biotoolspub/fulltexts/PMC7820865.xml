<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Brief Bioinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Brief Bioinform</journal-id>
    <journal-id journal-id-type="publisher-id">bib</journal-id>
    <journal-title-group>
      <journal-title>Briefings in Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1467-5463</issn>
    <issn pub-type="epub">1477-4054</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7820865</article-id>
    <article-id pub-id-type="pmid">31838514</article-id>
    <article-id pub-id-type="doi">10.1093/bib/bbz130</article-id>
    <article-id pub-id-type="publisher-id">bbz130</article-id>
    <article-categories>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Articles</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>An extensive review of tools for manual annotation of documents</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Neves</surname>
          <given-names>Mariana</given-names>
        </name>
        <xref rid="cor1" ref-type="corresp"/>
        <xref ref-type="aff" rid="aff1"/>
        <!--<email>marianalaraneves@gmail.com</email>-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ševa</surname>
          <given-names>Jurica</given-names>
        </name>
        <xref ref-type="aff" rid="aff1"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><institution>German Centre for the Protection of Laboratory Animals (BfR), German Federal Institute for Risk Assessment (BfR)</institution>, Berlin, Germany</aff>
    <author-notes>
      <corresp id="cor1">Corresponding author: Mariana Neves, German Centre for the Protection of Laboratory Animals (BfR), German Federal Institute for Risk Assessment (BfR), Berlin, Germany. Tel.: ; Fax: ; E-mail: <email>marianalaraneves@gmail.com</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2019-12-15">
      <day>15</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>15</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>22</volume>
    <issue>1</issue>
    <fpage>146</fpage>
    <lpage>163</lpage>
    <history>
      <date date-type="received">
        <day>05</day>
        <month>10</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019. Published by Oxford University Press. All rights reserved.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="cc-by" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="bbz130.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="bbz130s1">
        <title>Motivation</title>
        <p>Annotation tools are applied to build training and test corpora, which are essential for the development and evaluation of new natural language processing algorithms. Further, annotation tools are also used to extract new information for a particular use case. However, owing to the high number of existing annotation tools, finding the one that best fits particular needs is a demanding task that requires searching the scientific literature followed by installing and trying various tools.</p>
      </sec>
      <sec id="bbz130s2">
        <title>Methods</title>
        <p>We searched for annotation tools and selected a subset of them according to five requirements with which they should comply, such as being Web-based or supporting the definition of a schema. We installed the selected tools (when necessary), carried out hands-on experiments and evaluated them using 26 criteria that covered functional and technical aspects. We defined each criterion on three levels of matches and a score for the final evaluation of the tools.</p>
      </sec>
      <sec id="bbz130s3">
        <title>Results</title>
        <p>We evaluated 78 tools and selected the following 15 for a detailed evaluation: BioQRator, brat, Catma, Djangology, ezTag, FLAT, LightTag, MAT, MyMiner, PDFAnno, prodigy, tagtog, TextAE, WAT-SL and WebAnno. Full compliance with our 26 criteria ranged from only 9 up to 20 criteria, which demonstrated that some tools are comprehensive and mature enough to be used on most annotation projects. The highest score of 0.81 was obtained by WebAnno (of a maximum value of 1.0).</p>
      </sec>
    </abstract>
    <kwd-group>
      <kwd>annotation tools</kwd>
      <kwd>corpus construction</kwd>
      <kwd>manual annotation</kwd>
    </kwd-group>
    <counts>
      <page-count count="18"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <title>Introduction</title>
    <p>The use of machine learning algorithms and, more recently, of deep learning algorithms is already a reality in both natural language processing (NLP) and text mining (TM) methods and applications [<xref rid="ref1" ref-type="bibr">1</xref>]. Such methods usually need to rely on suitable annotated data depending on the problem at hand, whether for document classification [<xref rid="ref2" ref-type="bibr">2</xref>], named-entity recognition [<xref rid="ref3" ref-type="bibr">3</xref>] or relation extraction [<xref rid="ref4" ref-type="bibr">4</xref>]. While some machine learning experiments can rely on unsupervised methods that do not require any previously manually annotated data, supervised (or semi-supervised) learning can achieve higher performance if high-quality annotated data are available.</p>
    <p>Manual annotation is the task of reading a particular pre-selected document and providing additional information in the form of the so-called annotations. An annotation can occur at any level of a linguistic component, i.e. document, paragraph, sentence, phrase, word or character. Document-level annotations are useful for supporting document classification tasks, for instance, for identifying documents according to the hallmarks of cancer [<xref rid="ref2" ref-type="bibr">2</xref>]. Previous work has annotated paragraphs and sentences, for instance, for identifying the various sections of a scientific publication, such as background or methods [<xref rid="ref5" ref-type="bibr">5</xref>]. Similarly, phrases can be chosen to be annotated, instead of a sentence, to highlight particular expressions that stand for the level of certainty or polarity of findings described in publications [<xref rid="ref6" ref-type="bibr">6</xref>]. Annotation of words and characters are very common for precisely identifying biomedical entities, such as genes, proteins and diseases [<xref rid="ref7" ref-type="bibr">7</xref>]. Finally, annotations between linguistic components are also a common task, such as for syntactic dependencies [<xref rid="ref7" ref-type="bibr">7</xref>] or interactions between drugs [<xref rid="ref8" ref-type="bibr">8</xref>].</p>
    <p>Annotations can occur in different forms depending on the tools in use and/or the goals of the annotation project. It can vary from an unstructured short piece of text, such as a comment on a text passage, as supported by the Hypothesis tool, to structured annotations by means of highlighting text spans or drawing relations between them. The advantage of structured annotations are various, such as their straightforward use for machine learning purposes, the possibility of computing statistics, as well as direct comparison between the various annotators and their agreement. Further, annotations can be either enforced by (strict) predefined guidelines or performed in a relaxed way, without predefined guidelines. The former is always preferable since it enforces homogenity across the various documents and annotators. Adding metadata to annotations is also possible, e.g. normalizing a named entity (e.g. a protein) by precisely mapping it to a particular identifier in a standard database (e.g. UniProt). Finally, the annotation process can be supported by standard terminologies and ontologies, a feature supported by some of the annotation tools (e.g. Knowtator).</p>
    <p>Manual annotation is still regarded as the bottleneck for many NLP experiments, given that it is a time-consuming manual process. A new annotation project usually involves a variety of activities [<xref rid="ref9" ref-type="bibr">9</xref>], such as defining an annotation schema, writing comprehensive annotation guidelines, gathering an adequate document collection, pre-processing these documents according to the task, training experts for the annotation task and building a final consensus corpus. Consequently, the usability and completeness of a tool have a direct impact on the annotation process and can either accelerate or slow it down.</p>
    <p>Choosing an adequate annotation tool is a demanding task given the huge number of available tools and the lack of an updated list of annotation tools, along with the advantages and disadvantages of each. Indeed, building a comprehensive list of tools is a challenge because new tools are frequently being released while some of the old ones are no longer maintained. Further, choosing the best suitable annotation tool usually involves trying some of them to obtain insight in their usability and features. This process usually fails to find the executable or source code for the many published tools, as well as failing to install the tools owing to various technical issues. Therefore, any previous review of available tools is welcomed to avoid making poor decisions. Indeed, poor decisions can result in unnecessary waste of time of installing and converting documents to a certain format and later having to migrate them to another tool’s format, and even having to re-train the annotators on another tool.</p>
    <p>Because of the reasons stated above, we present a comprehensive review of 78 tools for the manual annotation of documents. We performed a careful search of annotation tools and defined five requirements for the selection of a subset of these for which we performed hands-on experiments based on 26 criteria. As far as we know, this is the most comprehensive review of annotation tools ever performed.</p>
    <p>We are aware of previous publications that presented a review of available annotation tools. A recent book described collaborative annotation and included a review of around 20 tools [<xref rid="ref10" ref-type="bibr">10</xref>].
In addition, we already conducted a comprehensive study on annotation tools that also included hands-on experiments with selected tools [<xref rid="ref11" ref-type="bibr">11</xref>]. In contrast to our previous study, we now extend the number of analyzed tools (to 78) and list all of the surveyed tools, regardless of whether they meet the five requirements for selection. As expected, many tools have been released since our last survey. Meanwhile, some of the tools selected in the previous survey are now either outdated or no longer available. However, we still cite them for the sake of completeness. From the 15 selected tools that we considered here, only three were included in our previous survey. Our single restriction with regard to our previous survey was the consideration of only Web applications. Distinct from the previous survey, our new evaluation, based on a three-level scale, allowed us to assign a score to each selected tool regarding the fulfillment of the criteria.</p>
    <p>Our previous survey was more focused on the biomedical domain, and the selected tools were based on their previous use for the annotation of biomedical corpora or curation, which was rather limiting. We removed this restriction, but we still include criteria related to this field and we address the biomedical domain in a specific subsection in our discussion. Further, whenever possible, our hands-on experiments were based on real use cases using PubMed abstracts.</p>
    <p>The next section presents details about our methods, including the five requirements that we defined for the selection of tools and the 26 criteria (each one a three-level scale) under which we evaluated them. Section <xref ref-type="sec" rid="sec3">3</xref> presents the results of our survey and includes a comprehensive list of the non-selected tools and a detailed evaluation of the selected ones. Finally, we present our discussion regarding various aspects in Section <xref ref-type="sec" rid="sec4">4</xref>.</p>
  </sec>
  <sec id="sec2">
    <title>Methods</title>
    <p>This section describes the methodology used when searching for the annotation tools, the requirements that we defined for selecting a subset of them and the criteria that we considered in the detailed evaluation of the selected ones.</p>
    <sec id="sec2a">
      <title>Initial list of annotation tools</title>
      <p>We collected all tools that we were already aware of, including the ones that we considered in our previous survey [<xref rid="ref11" ref-type="bibr">11</xref>] and those whose publication cites our previous survey.</p>
      <p>We made various queries to Google, Google Scholar and PubMed to check all the tools cited in publications related to annotation tools. We noticed that results from Google and Google Scholar can sometimes be slightly different, with Google giving priority for commercial tools and Google Scholar to research tools.</p>
      <p>We searched the Corpora mailing list (<ext-link ext-link-type="uri" xlink:href="https://mailman.uib.no/listinfo/corpora">https://mailman.uib.no/listinfo/corpora</ext-link>) for the past several years as well as recent publications on text classification and information extraction to check how authors have annotated the corpora for their research.</p>
      <p>Additionally, we searched the last proceedings (since 2014) of conferences in the field of computational linguistics, such as ACL Anthology (<ext-link ext-link-type="uri" xlink:href="https://aclanthology.coli.uni-saarland.de/">https://aclanthology.coli.uni-saarland.de/</ext-link>) and LREC (<ext-link ext-link-type="uri" xlink:href="http://www.lrec-conf.org/">http://www.lrec-conf.org/</ext-link>), as well as the proceedings of the BioNLP workshop (<ext-link ext-link-type="uri" xlink:href="https://aclweb.org/aclwiki/BioNLP_Workshop">https://aclweb.org/aclwiki/BioNLP_Workshop</ext-link>).
Unfortunately, many authors do not cite the tool that they used in the annotation process (e.g. [<xref rid="ref4" ref-type="bibr">4</xref>]) or used a custom (private) tool or an adapted (non-available) version of an existing one [<xref rid="ref12" ref-type="bibr">12</xref>, <xref rid="ref13" ref-type="bibr">13</xref>].</p>
      <p>We did not include tools that we would not consider as an annotations tool since they were developed for a particular purpose of the authors, even though they have been used for corpus construction, such as the so-called ‘games with a purpose’ Zombiling [<xref rid="ref14" ref-type="bibr">14</xref>] and Phrase Detectives [<xref rid="ref15" ref-type="bibr">15</xref>]. These tools do not allow users to define their own documents nor an annotation schema. We also did not evaluate crown-sourcing tools such as the Amazon Mechanical Turk, since these are out of the scope of this survey. However, application of this tool for linguistic annotation purposes has been discussed in previous work [<xref rid="ref16" ref-type="bibr">16</xref>, <xref rid="ref17" ref-type="bibr">17</xref>].</p>
    </sec>
    <sec id="sec2b">
      <title>Selection of tools</title>
      <p>Given the high number of annotation tools that we found, we had to define some requirements with which the tools should comply to be included in our detailed evaluation. These requirements include, among others, features such as being a Web application, supporting a configurable (annotation) schema and whether it is readily available for download (and install) or use. The full set of requirements with which a tool must comply are listed and further explained below:</p>
      <list list-type="order">
        <list-item>
          <p>It should be available. [<italic>Available</italic>]</p>
        </list-item>
        <list-item>
          <p>It should be a Web application, either as an online or downloadable tool. [<italic>Web-based</italic>]</p>
        </list-item>
        <list-item>
          <p>If should be able to be installed in up to 2 h. [<italic>Installable</italic>]</p>
        </list-item>
        <list-item>
          <p>It should work for our hands-on experiments. [<italic>Workable</italic>]</p>
        </list-item>
        <list-item>
          <p>It should allow for the configuration of a schema. [<italic>Schematic</italic>]</p>
        </list-item>
      </list>
      <p>A tool should comply with all the above requirements in order to be selected for further hands-on experiments. The only exception is for tools that are online available, which do not need to comply with the requirement of being installable, since no installation is necessary.</p>
      <fig id="f1" orientation="portrait" position="float">
        <label><sc>Fig.</sc> 1.</label>
        <caption>
          <p>Web-based versus non-Web-based tools: number of annotation tools released in the past 10 years (on the left), and the number of corpora published in the past 5 years (on the right).</p>
        </caption>
        <graphic xlink:href="bbz130f1"/>
      </fig>
      <p><bold>Available.</bold> Our first requirement is that a tool should be readily available, either for direct online use (via a WWW user interface) or for download at the time of writing, without needing to contact the developers. Therefore, we searched for the tool’s URL in its publication (if any) as well as in Google and at the institution’s Web site. This is an indisputable requirement, given that we cannot carry out hands-on experiments on tools that we could not find (e.g. [<xref rid="ref2" ref-type="bibr">2</xref>]). Unfortunately, many tools had their development discontinued owing to various reasons, such as the end of the project, funding or PhD studies. For the sake of completeness, we list the tools that did not comply with this requirement in case others want to contact the developers.</p>
      <p><bold>Web application.</bold> The tool should be a Web application, i.e. it should be either readily available for online use or downloadable for installation as a Web application. Therefore, we excluded stand-alone systems (e.g. MMAX2 [<xref rid="ref18" ref-type="bibr">18</xref>]) or plug-ins that run on other tools (e.g. Knowtator [<xref rid="ref19" ref-type="bibr">19</xref>] runs on Protégé and XConc Suite [<xref rid="ref20" ref-type="bibr">20</xref>] on Eclipse) or in a browser (e.g. Sapient [<xref rid="ref21" ref-type="bibr">21</xref>]). Regarding the latter, even though it runs on a browser, it requires the user (annotator) to locally install the tool, just like a stand-alone tool. The Web-based requirement guarantees that the annotators can concentrate solely on the annotation task and do not need to struggle with the installation of the tool. Additionally, other tasks such as schema configuration and document import and export can be carried out by a project leader (if existent), and the annotators can focus on the annotation process. Manual annotation is a demanding and challenging task in itself, and additional tasks might disturb the annotators and compromise the annotation process.</p>
      <p>This requirement is supported by two analyses that we carried out: (i) the annotation tools that were published in the past 10 years and (ii) the corpora that was published in the past 5 years, annotated using any of the annotation tools that we found. Figure <xref ref-type="fig" rid="f1">1</xref> illustrates the outcomes of these analyses, i.e. the comparison of Web-based on non-Web-based tools that were recently developed or used for corpus construction. Details of the tools included in each of these analyses are presented in the supplementary material. Our analyses clearly show the prevalence of Web applications over stand-alone tools and plug-ins, both when deciding to develop a new tool, as well as when choosing a tool for corpus annotation.</p>
      <p><bold>Installable.</bold> Our third requirement is that a tool, if not available online, should be able to be installed in a maximum of 2 h without needing to contact the developers (Under the assumption of mid-skilled professional.). This is also necessary to enable our hands-on experiments. For the installation, we require that the documentation is comprehensive enough for it to include the tools’ dependencies and instructions on how to start and configure the application. When rejecting a tool under this requirement, we did not consider problems derived from our operating system or other issues related to our servers or environment (e.g. proxy).
This requirement was grounds for dismissal only when both authors had issues during the installation process.</p>
      <p><bold>Workable.</bold> Our fourth requirement expects that the tool works properly during our experiments, as this is a prerequisite for our hands-on experiments. Regardless if the tool is locally installed or available online for use, a minimum set of functionalities, defined by our criteria (as presented in Section <xref ref-type="sec" rid="sec2c">2.3</xref>), should be available. Therefore, the tool should be intuitive or the documentation comprehensive enough that we did not have to contact the developers for support.</p>
      <p><bold>Schematic.</bold> Our fifth and last requirement states that a tool should allow for the configuration of a schema, i.e. to define labels for the annotation. How this configuration is carried out in the tool is irrelevant, e.g. through the graphical user interface (GUI), command line interface (CLI) or by importing a configuration file. This means that we rejected tools that come with a pre-defined set of labels (e.g. [<xref rid="ref22" ref-type="bibr">22</xref>]), which do not support this functionality because it was designed for another purpose (e.g. [<xref rid="ref23" ref-type="bibr">23</xref>]), or that only allow annotation in the form of comments or simple highlighting (e.g. Hypothesis).</p>
    </sec>
    <sec id="sec2c">
      <title>List of evaluation criteria</title>
      <p>To evaluate the selected annotation tools, we defined various criteria. We started our list with the criteria that we used in our previous survey [<xref rid="ref11" ref-type="bibr">11</xref>] and considered which ones were worth keeping and which new ones should not be integrated in this survey. We split our criteria into four categories: (i) publication, (ii) technical, (iii) data and (iv) functional criteria. Each is presented in detail below.</p>
      <p><bold>Publication criteria.</bold> These criteria describe features related to both the tools’ publications and to other publications referencing the use of the tool. These are important aspects to assess the novelty of the tool, as well as its impact on corpus annotation.</p>
      <list list-type="bullet">
        <list-item>
          <p>P1 - Year of the last publication;</p>
        </list-item>
        <list-item>
          <p>P2 - Citations in Google Scholar (as of September 2019);</p>
        </list-item>
        <list-item>
          <p>P3 - Citations for corpus development (as of September 2019).</p>
        </list-item>
      </list>
      <p>Comparing this category to our last survey [<xref rid="ref11" ref-type="bibr">11</xref>], we changed the criterion that evaluates the number of publications (which describe corpus construction) that we found. We now consider any publication that describes corpus from any domain and not only those from the biomedical domain, which was too restricting.</p>
      <p><bold>Technical criteria.</bold> This group of criteria evaluates technical aspects of the software itself, such as source code availability and the necessity and the easiness of installation (if necessary).</p>
      <list list-type="bullet">
        <list-item>
          <p>T1 - Date of the last version (as of August 2019);</p>
        </list-item>
        <list-item>
          <p>T2 - Availability of the source code;</p>
        </list-item>
        <list-item>
          <p>T3 - Online availability for use;</p>
        </list-item>
        <list-item>
          <p>T4 - Easiness of installation;</p>
        </list-item>
        <list-item>
          <p>T5 - Quality of the documentation;</p>
        </list-item>
        <list-item>
          <p>T6 - Type of license;</p>
        </list-item>
        <list-item>
          <p>T7 - Free of charge.</p>
        </list-item>
      </list>
      <p>In this group of criteria, we checked various technical criteria of the tools. A tool for which a recent version is available is an important asset and we assess it with the criterion T1. The availability of the source code (criterion T2) allows researchers to customize it according to their needs. Installing a tool is usually a time-consuming task, therefore we check whether it is readily available online (T3) as well as the easiness of installation (T4). A good documentation is an important feature for either installing or properly using a tool, and we evaluate it with the criterion T5. Finally, the type of the license (criterion T6) indicates for which uses a tool is allowed and whether its redistribution is possible.</p>
      <p>We evaluated whether a tool is freely available for academia in its full functionality. Nowadays, many implemented tools and algorithms are freely available in open-source repositories. As we also considered commercial tools in the survey, we check whether these are freely available or have a free version that includes most of the important functional features. On one hand, commercial tools come with the uncertainty of whether they will remain freely available in the near future. On the other hand, they might have more functionalities or a more appealing GUI than most of the (freely available) tools developed in academia or research institutes.</p>
      <p>We did not consider the criterion ‘type of installation (Web, stand-alone, plug-in)’ from our previous survey [<xref rid="ref11" ref-type="bibr">11</xref>], given that we selected only Web applications. The same applies for ‘supported operating systems’, given that this is no longer relevant when dealing with Web-based tools.</p>
      <p><bold>Data criteria.</bold> These criteria assess the input and output format of documents, schema and annotations. These were roughly the same criteria that we considered in the previous survey [<xref rid="ref11" ref-type="bibr">11</xref>].</p>
      <list list-type="bullet">
        <list-item>
          <p>D1 - Format of the schema;</p>
        </list-item>
        <list-item>
          <p>D2 - Input format for documents;</p>
        </list-item>
        <list-item>
          <p>D3 - Output format for annotations.</p>
        </list-item>
      </list>
      <p>The definition of a schema varies across the tools, but it is usually defined either in the GUI or imported from a file. Further, all tools allow importing the documents on which annotation will be performed and exporting the resulting annotations into a file. Criteria D1, D2 and D3 aim to evaluate whether the tools rely on standard formats for importing and exporting these files. Even though using a certain standard, e.g. XML or JSON, is no guarantee of integration of files between various tools, it indicates that researchers can utilize existing programming libraries for parsing and writing the files.</p>
      <p><bold>Functional criteria.</bold> In this group, we evaluated various criteria related to the functionality of the tools.</p>
      <list list-type="bullet">
        <list-item>
          <p>F1 - Allowance of multi-label annotations;</p>
        </list-item>
        <list-item>
          <p>F2 - Allowance of document-level annotations;</p>
        </list-item>
        <list-item>
          <p>F3 - Support for annotation of relationships;</p>
        </list-item>
        <list-item>
          <p>F4 - Support for ontologies and terminologies;</p>
        </list-item>
        <list-item>
          <p>F5 - Support for pre-annotations;</p>
        </list-item>
        <list-item>
          <p>F6 - Integration with PubMed;</p>
        </list-item>
        <list-item>
          <p>F7 - Suitability for full texts;</p>
        </list-item>
        <list-item>
          <p>F8 - Allowance for saving documents partially;</p>
        </list-item>
        <list-item>
          <p>F9 - Ability to highlight parts of the text;</p>
        </list-item>
        <list-item>
          <p>F10 - Support for users and teams;</p>
        </list-item>
        <list-item>
          <p>F11 - Support for inter-annotator agreement (IAA);</p>
        </list-item>
        <list-item>
          <p>F12 - Data privacy;</p>
        </list-item>
        <list-item>
          <p>F13 - Support for various languages;</p>
        </list-item>
      </list>
      <p>We consider various criteria related to GUI of the tool and how the annotations themselves are carried out in the tool. The ability to support highlighting of text span for the annotations at the levels of sentences, words or characters is evaluated by criterion F9. Further, criterion F1 evaluates whether overlapping text spans are possible, for instance, for assigning more than one label to an annotation. For instance, in the CellFinder corpus [<xref rid="ref24" ref-type="bibr">24</xref>], the text span ‘mesenchymal precursors’ was annotated as a cell type while the word ‘mesenchymal’ also as an anatomical part.</p>
      <p>Annotation of relations between text spans, which is assessed by criterion F3, is an important feature for building corpora for syntactic dependencies or semantic relationships. Further, when annotating long documents, it is essential to choose a tool that is able to display long texts correctly (criterion F7) and that allows saving the annotations partially to later continue the annotation process (criterion F8). We included the new criterion F2 (‘allowance of document-level annotations’), which specifically evaluates the suitability of the tool for this type of annotation, and we included a discussion on this topic (cf. Section <xref ref-type="sec" rid="sec4f">4.6</xref>). This is an interesting feature to support corpora construction for document classification.</p>
      <p>Besides importing the textual documents on which the annotations will be performed, importing additional resources is also important in some situations. When relying on existing terminologies for the annotation of text, a tool that provides ways to import these resources is an important asset, as indicated by criterion F4. Similarly, in some situations, the user would like to perform the annotation over pre-existing annotations, e.g. to keep the right one and remove the wrong ones, a feature that is evaluated by criterion F5. This feature also evaluates built-in features for automatic predictions and active learning for training a model based on the manual annotations.</p>
      <p>Criterion F6 (‘integration with PubMed’) specifically considers an important feature for the biomedical domain, which is the integration with either PubMed <sup>®</sup> or PubMed Central <sup>®</sup> (PMC). Some annotation tools provide this functionality, such as real-time or off-line integration with PubMed for loading titles, abstracts, meta-data or full texts.</p>
      <p>Corpora are usually annotated by more than one expert in order to guarantee the quality of the annotations. Therefore, we evaluate the ability of the tools to support users and teams (criterion F10). After collecting annotations from various experts, a tool that supports a comparison (agreement) between the annotations relieves the researchers of the need to write additional scripts for the post-processing step, as assessed by criterion F11. We include in this criterion various tasks, such as the calculation of simple statistics (inter-annotator agreement) or the support for building a consensus corpus.</p>
      <p>The criterion F12 (‘data privacy’) is new and assesses a topic that is important nowadays in which society has concerns about the use of their data by the online applications they use. Further, many research groups are currently annotating sensitive documents such as clinical reports (e.g. [<xref rid="ref25" ref-type="bibr">25</xref>]). Finally, annotating documents in languages other than English that contain special characters is also an important feature and we assess the suitability of a tool for this task with criterion F13.</p>
      <p>Regarding the functional criteria listed in the previous survey [<xref rid="ref11" ref-type="bibr">11</xref>], we removed those that were no longer relevant, i.e. ‘possibility of using only the keyboard’, ‘automatic selection of a token when clicked’, ‘support for fast annotation’ and ‘allowance of comments on the annotation’. Three other criteria (‘pre-processing the text’, ‘built-in biomedical named-entities recognition’ and ‘easiness of importing pre-annotations’) were somewhat split into the criteria F5 (‘support for pre-annotations’) and D2 (‘input format for documents’).</p>
    </sec>
    <sec id="sec2d">
      <title>Evaluation of tools</title>
      <p>We evaluated each selected tool using all of the criteria listed in Section <xref ref-type="sec" rid="sec2c">2.3</xref>. For the selected tools that we already considered in our previous survey (i.e. brat, Djangology and MyMiner), we provide a new evaluation of the functional criterion based on the previous survey and our previous hands-on experiments (for the new criteria). However, no new versions of these tools were released since the previous survey. Additionally, we present updated values for the criteria related to the publication metrics or the date of the last version. Our detailed evaluation for each tool is available as supplementary material.</p>
      <p>We evaluated each criterion based on a three-level scale: (i) the lowest level means that the criterion was either not covered or only covered in its very basic features, (ii) the medium level means that the criterion was partially covered and (iii) the highest level means that the criterion was fully covered. The application of the three-level scale to each criterion is presented in Table <xref rid="TB1" ref-type="table">1</xref>. We present these three levels in the form of three colors: ‘gray’ (higher level), ‘light gray’ (medium level) and ‘white’ (lower level).</p>
      <table-wrap id="TB1" orientation="portrait" position="float">
        <label><sc>Table</sc> 1</label>
        <caption>
          <p>Definition of the three-level scale for the evaluation for each criterion</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col span="1" align="left"/>
            <col span="2" align="left"/>
            <col span="3" align="left"/>
            <col span="4" align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">Total fulfillment (higher level)</th>
              <th rowspan="1" colspan="1">Partial fulfillment (medium level)</th>
              <th rowspan="1" colspan="1">No fulfillment (lower level)</th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Criteria</th>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">P1</td>
              <td rowspan="1" colspan="1">Last publication since 2009 (past 10 years)</td>
              <td rowspan="1" colspan="1">Last publication until 2009 (more than 10 years ago)</td>
              <td rowspan="1" colspan="1">No publication found</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">P2</td>
              <td rowspan="1" colspan="1">More than 30 citations</td>
              <td rowspan="1" colspan="1">From 11 to 30 citations</td>
              <td rowspan="1" colspan="1">Up to 10 citations</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">P3</td>
              <td rowspan="1" colspan="1">More than 10 citations</td>
              <td rowspan="1" colspan="1">From 6 to 10 citations</td>
              <td rowspan="1" colspan="1">Up to 5 citations</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">T1</td>
              <td rowspan="1" colspan="1">Last version (or commit) since 2014 (past 5 years)</td>
              <td rowspan="1" colspan="1">Last version (or commit) until 2014 (more than 5 years ago)</td>
              <td rowspan="1" colspan="1">Last version is unknown</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">T2</td>
              <td rowspan="1" colspan="1">Source code available in version control platforms (e.g. GitHub)</td>
              <td rowspan="1" colspan="1">Source code available to download</td>
              <td rowspan="1" colspan="1">Source code not available</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">T3</td>
              <td rowspan="1" colspan="1">Online system available for use</td>
              <td rowspan="1" colspan="1">Online system available with restrictions</td>
              <td rowspan="1" colspan="1">Not available online for use</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">T4</td>
              <td rowspan="1" colspan="1">No need to install or easy to install (until half-hour time)</td>
              <td rowspan="1" colspan="1">Moderate installation time (from 1- to 2-h time)</td>
              <td rowspan="1" colspan="1">Difficult to install (more than 2-h time)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">T5</td>
              <td rowspan="1" colspan="1">Good documentation (covers most features)</td>
              <td rowspan="1" colspan="1">Poor documentation (covers few features)</td>
              <td rowspan="1" colspan="1">No documentation</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">T6</td>
              <td rowspan="1" colspan="1">License allows to modify and redistribute the tool</td>
              <td rowspan="1" colspan="1">License allows to modify tool</td>
              <td rowspan="1" colspan="1">No license available or cannot modify tool</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">T7</td>
              <td rowspan="1" colspan="1">Freely available in its full functionalities</td>
              <td rowspan="1" colspan="1">Freely available with limitations</td>
              <td rowspan="1" colspan="1">No freely available version</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">D1</td>
              <td rowspan="1" colspan="1">Schema online configurable or uses standard formats (e.g. XML, JSON)</td>
              <td rowspan="1" colspan="1">Schema uses non-standard formats</td>
              <td rowspan="1" colspan="1">-</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">D2</td>
              <td rowspan="1" colspan="1">Documents based on standard formats (e.g. JSON, XML)</td>
              <td rowspan="1" colspan="1">Documents based on non-standard formats</td>
              <td rowspan="1" colspan="1">-</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">D3</td>
              <td rowspan="1" colspan="1">Annotations downloaded using standard formats (e.g. JSON, XML)</td>
              <td rowspan="1" colspan="1">Annotations based non-standard formats</td>
              <td rowspan="1" colspan="1">-</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">F1</td>
              <td rowspan="1" colspan="1">Support for multi-label annotation</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">No support for multi-label annotation</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">F2</td>
              <td rowspan="1" colspan="1">Support for document-level annotation</td>
              <td rowspan="1" colspan="1">Poor support for document-level annotation</td>
              <td rowspan="1" colspan="1">No support for document-level annotation</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">F3</td>
              <td rowspan="1" colspan="1">Support for relationships</td>
              <td rowspan="1" colspan="1">Support for binary relationships or limited support for relationships</td>
              <td rowspan="1" colspan="1">No support for relationships</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">F4</td>
              <td rowspan="1" colspan="1">Support for ontologies or terminologies</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">No support for ontologies or terminologies</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">F5</td>
              <td rowspan="1" colspan="1">Active learning from annotated documents</td>
              <td rowspan="1" colspan="1">Built-in prediction or upload/import of external annotation</td>
              <td rowspan="1" colspan="1">No support for pre-annotations</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">F6</td>
              <td rowspan="1" colspan="1">Integration with Medline/PubMed</td>
              <td rowspan="1" colspan="1">Partial support with Medline/PubMed</td>
              <td rowspan="1" colspan="1">No integration with Medline/PubMed</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">F7</td>
              <td rowspan="1" colspan="1">Support for full texts</td>
              <td rowspan="1" colspan="1">Partial support to full text</td>
              <td rowspan="1" colspan="1">No support for full texts</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">F8</td>
              <td rowspan="1" colspan="1">Support for partially saving annotations</td>
              <td rowspan="1" colspan="1">Partial support by re-importing annotations</td>
              <td rowspan="1" colspan="1">No support for partially saving annotations</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">F9</td>
              <td rowspan="1" colspan="1">Support for text highlighting</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">No support for text highlighting</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">F10</td>
              <td rowspan="1" colspan="1">Users and teams management</td>
              <td rowspan="1" colspan="1">Individual user login or restricted user management</td>
              <td rowspan="1" colspan="1">No support for users (only anonymous users)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">F11</td>
              <td rowspan="1" colspan="1">IAA and consensus corpus building</td>
              <td rowspan="1" colspan="1">Partial support for either IAA or consensus building</td>
              <td rowspan="1" colspan="1">No IAA</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">F12</td>
              <td rowspan="1" colspan="1">Can be used for private data (e.g. medical text)</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">Cannot be used for private data</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">F13</td>
              <td rowspan="1" colspan="1">Support for various languages</td>
              <td rowspan="1" colspan="1">Partial support for various languages</td>
              <td rowspan="1" colspan="1">No support for various languages</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p> Guidelines for the gray scales when evaluating the tools according to the criteria. The codes for the criteria were defined in Section <xref ref-type="sec" rid="sec2c">2.3</xref>.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>Not all criteria utilize all three levels of the scale. For some criteria, e.g. ‘suitability for full texts’ (F7), we decided that two levels were enough: either the tool supports it or not. By contrast, we did not define the lowest level for the Data criteria, given that all tools (at least partially) fulfill these features by importing and exporting documents, schema or data in some particular format. Some tools do not support the configuration of an annotation schema (not schematic), but none of our selected tools fit this description, given that this is one of our elimination requirements.</p>
      <p>After assessing each criterion, we calculated a final score based on the sum of points obtained by each feature. Fulfilling a criterion (i.e. ‘gray’) corresponds to one point (1.0), partially filling it (i.e. ‘light gray’) to 0.5 point; otherwise, there is no point at all (i.e. ‘white’). Since we did not assign any points to the third level, the latter is ignored for the calculation of the final score. Therefore, the final score is defined by the division of the two following values: (i) the sum of the points obtained from the two highest levels; and (ii) the total number of criteria (e.g. 26 in total).</p>
    </sec>
  </sec>
  <sec id="sec3">
    <title>Results</title>
    <p>We provide the list of all selected and nonselected tools that we found while preparing this survey.</p>
    <p>A complete list of all surveyed tools including links to the publications and their Web site, when available, is presented in our GitHub repository.</p>
    <sec id="sec3a">
      <title>Nonselected tools</title>
      <p>From the 78 tools that we considered, 63 were not selected for a detailed evaluation of the criteria (cf. Section <xref ref-type="sec" rid="sec2c">2.3</xref>). All of the nonselected tools did not comply with at least one of the five requirements that we defined for selecting the tools (cf. Section <xref ref-type="sec" rid="sec2b">2.2</xref>). Nevertheless, in this new survey, we decided to provide a comprehensive list of the tools and their elimination grounds for the sake of transparency.</p>
      <p>Table <xref rid="TB2" ref-type="table">2</xref> summarizes the nonselected tools, the respective publication (whenever available or found) and the reason for exclusion (according to requirements defined in Section <xref ref-type="sec" rid="sec2b">2.2</xref>). We cite at least one requirement with which the tools did not comply. However, we did not evaluate all five requirements for all tools since that would require trying to install and use all of them, which is a very time-consuming task.
While some requirements can be evaluated by checking the publication (e.g. Available, Web-based and Schematic), some are only possible if the tool is available (e.g. Installable and Workable).</p>
      <table-wrap id="TB2" orientation="portrait" position="float">
        <label><sc>Table</sc> 2</label>
        <caption>
          <p>Summary of the nonselected tools and the respective elimination requirement</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col span="1" align="left"/>
            <col span="2" align="left"/>
            <col span="3" align="left"/>
            <col span="4" align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Tools</th>
              <th rowspan="1" colspan="1">Publications</th>
              <th rowspan="1" colspan="1">Elimination requirements</th>
              <th rowspan="1" colspan="1">Comments</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">AGTK</td>
              <td rowspan="1" colspan="1">[<xref rid="ref26" ref-type="bibr">26</xref>, <xref rid="ref27" ref-type="bibr">27</xref>]</td>
              <td rowspan="1" colspan="1">[Web-based]</td>
              <td rowspan="1" colspan="1">Stand-alone tool</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">AlvisAE</td>
              <td rowspan="1" colspan="1">[<xref rid="ref28" ref-type="bibr">28</xref>]</td>
              <td rowspan="1" colspan="1">[Available]</td>
              <td rowspan="1" colspan="1">Not available or not found</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Anafora</td>
              <td rowspan="1" colspan="1">[<xref rid="ref29" ref-type="bibr">29</xref>]</td>
              <td rowspan="1" colspan="1">[Installable]</td>
              <td rowspan="1" colspan="1">Documentation still under construction</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Analec</td>
              <td rowspan="1" colspan="1">[<xref rid="ref30" ref-type="bibr">30</xref>]</td>
              <td rowspan="1" colspan="1">[Web-based]Workable]</td>
              <td rowspan="1" colspan="1">Stand-alone tool and documentation only available in French</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Annotator</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">[Web-Based][Schematic]</td>
              <td rowspan="1" colspan="1">Plug-in, and no configuration of schema</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Anotatornia</td>
              <td rowspan="1" colspan="1">[<xref rid="ref31" ref-type="bibr">31</xref>]</td>
              <td rowspan="1" colspan="1">[Workable]</td>
              <td rowspan="1" colspan="1">Documentation and tool only available in Polish</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">APLenty</td>
              <td rowspan="1" colspan="1">[<xref rid="ref32" ref-type="bibr">32</xref>]</td>
              <td rowspan="1" colspan="1">[Available]</td>
              <td rowspan="1" colspan="1">Not available or not found</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">@Note</td>
              <td rowspan="1" colspan="1">[<xref rid="ref33" ref-type="bibr">33</xref>]</td>
              <td rowspan="1" colspan="1">[Web-Based][Schematic]</td>
              <td rowspan="1" colspan="1">Stand-alone tool, and no configuration of schema</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Argo</td>
              <td rowspan="1" colspan="1">[<xref rid="ref34" ref-type="bibr">34</xref>]</td>
              <td rowspan="1" colspan="1">[Workable]</td>
              <td rowspan="1" colspan="1">Error when running workflow</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Atomic</td>
              <td rowspan="1" colspan="1">[<xref rid="ref35" ref-type="bibr">35</xref>]</td>
              <td rowspan="1" colspan="1">[Web-based]</td>
              <td rowspan="1" colspan="1">Plug-in in Eclipse</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BioAnnotate</td>
              <td rowspan="1" colspan="1">[<xref rid="ref36" ref-type="bibr">36</xref>]</td>
              <td rowspan="1" colspan="1">[Web-based]</td>
              <td rowspan="1" colspan="1">Stand-alone tool</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Bionotate</td>
              <td rowspan="1" colspan="1">[<xref rid="ref37" ref-type="bibr">37</xref>]</td>
              <td rowspan="1" colspan="1">[Installable]</td>
              <td rowspan="1" colspan="1">No documentation on how to start the system</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CCASH</td>
              <td rowspan="1" colspan="1">[<xref rid="ref38" ref-type="bibr">38</xref>]</td>
              <td rowspan="1" colspan="1">[Installable]</td>
              <td rowspan="1" colspan="1">No documentation on how to start the system</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Cadixe</td>
              <td rowspan="1" colspan="1">[<xref rid="ref39" ref-type="bibr">39</xref>]</td>
              <td rowspan="1" colspan="1">[Available][Web-based]</td>
              <td rowspan="1" colspan="1">Not available or not found, and stand-alone tool</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Callisto</td>
              <td rowspan="1" colspan="1">[<xref rid="ref40" ref-type="bibr">40</xref>]</td>
              <td rowspan="1" colspan="1">[Web-based]</td>
              <td rowspan="1" colspan="1">Stand-alone tool</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Cas Editor</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">[Available][Web-based]</td>
              <td rowspan="1" colspan="1">Not available or not found, and plug-in in Eclipse</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CLARIN-EL</td>
              <td rowspan="1" colspan="1">[<xref rid="ref41" ref-type="bibr">41</xref>]</td>
              <td rowspan="1" colspan="1">[Workable]</td>
              <td rowspan="1" colspan="1">Log-in did not work</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Coco</td>
              <td rowspan="1" colspan="1">[<xref rid="ref42" ref-type="bibr">42</xref>]</td>
              <td rowspan="1" colspan="1">[Available][Schematic]</td>
              <td rowspan="1" colspan="1">URL does not exist, and not schematic</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CRAB reader</td>
              <td rowspan="1" colspan="1">[<xref rid="ref43" ref-type="bibr">43</xref>]</td>
              <td rowspan="1" colspan="1">[Available]</td>
              <td rowspan="1" colspan="1">Not available or not found, but used in [<xref rid="ref2" ref-type="bibr">2</xref>]</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DOMEO</td>
              <td rowspan="1" colspan="1">[<xref rid="ref44" ref-type="bibr">44</xref>]</td>
              <td rowspan="1" colspan="1">[Installable]</td>
              <td rowspan="1" colspan="1">No documentation for installation</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">EasyRef</td>
              <td rowspan="1" colspan="1">[<xref rid="ref45" ref-type="bibr">45</xref>]</td>
              <td rowspan="1" colspan="1">[Available][Web-based][Schematic]</td>
              <td rowspan="1" colspan="1">Not available or not found, stand-alone tool, and fixed schema</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Egas</td>
              <td rowspan="1" colspan="1">[<xref rid="ref46" ref-type="bibr">46</xref>]</td>
              <td rowspan="1" colspan="1">[Schematic][Workable]</td>
              <td rowspan="1" colspan="1">Schema is limited to some entities and approval of account takes many days</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">eHost</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">[Available]</td>
              <td rowspan="1" colspan="1">Link to the download file does not work</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Ellogon</td>
              <td rowspan="1" colspan="1">[<xref rid="ref47" ref-type="bibr">47</xref>]</td>
              <td rowspan="1" colspan="1">[Web-based]</td>
              <td rowspan="1" colspan="1">Stand-alone tool</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">EULIA</td>
              <td rowspan="1" colspan="1">[<xref rid="ref48" ref-type="bibr">48</xref>]</td>
              <td rowspan="1" colspan="1">[Available]</td>
              <td rowspan="1" colspan="1">Not available or not found</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GATE Teamware</td>
              <td rowspan="1" colspan="1">[<xref rid="ref49" ref-type="bibr">49</xref>]</td>
              <td rowspan="1" colspan="1">[Installable]</td>
              <td rowspan="1" colspan="1">Problems working with the various components (Tomcat, MySQL)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GitDox</td>
              <td rowspan="1" colspan="1">[<xref rid="ref50" ref-type="bibr">50</xref>]</td>
              <td rowspan="1" colspan="1">[Workable]</td>
              <td rowspan="1" colspan="1">Installation worked, but log-in did not</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Glozz</td>
              <td rowspan="1" colspan="1">[<xref rid="ref51" ref-type="bibr">51</xref>]</td>
              <td rowspan="1" colspan="1">[Web-based]</td>
              <td rowspan="1" colspan="1">Stand-alone tool</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Hypothesis</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">[Schematic]</td>
              <td rowspan="1" colspan="1">No configuration of schema</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Inforex</td>
              <td rowspan="1" colspan="1">[<xref rid="ref52" ref-type="bibr">52</xref>, <xref rid="ref53" ref-type="bibr">53</xref>]</td>
              <td rowspan="1" colspan="1">[Workable]</td>
              <td rowspan="1" colspan="1">Documentation only available in Polish</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">KAFnotator</td>
              <td rowspan="1" colspan="1">[<xref rid="ref54" ref-type="bibr">54</xref>]</td>
              <td rowspan="1" colspan="1">[Available][Schematic]</td>
              <td rowspan="1" colspan="1">Download file not available, and fixed schema</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">KCAT</td>
              <td rowspan="1" colspan="1">[<xref rid="ref55" ref-type="bibr">55</xref>]</td>
              <td rowspan="1" colspan="1">[Schematic][Web-based]</td>
              <td rowspan="1" colspan="1">Entity linking annotation and not Web-based</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Knowtator</td>
              <td rowspan="1" colspan="1">[<xref rid="ref19" ref-type="bibr">19</xref>]</td>
              <td rowspan="1" colspan="1">[Web-based]</td>
              <td rowspan="1" colspan="1">Plug-in in Protégé</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MAE</td>
              <td rowspan="1" colspan="1">[<xref rid="ref56" ref-type="bibr">56</xref>]</td>
              <td rowspan="1" colspan="1">[Web-based]</td>
              <td rowspan="1" colspan="1">Stand-alone tool</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Marky</td>
              <td rowspan="1" colspan="1">[<xref rid="ref57" ref-type="bibr">57</xref>]</td>
              <td rowspan="1" colspan="1">[Installable]</td>
              <td rowspan="1" colspan="1">Documentation is confusing and configuration failed</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MDSWriter</td>
              <td rowspan="1" colspan="1">[<xref rid="ref23" ref-type="bibr">23</xref>]</td>
              <td rowspan="1" colspan="1">[Schematic]</td>
              <td rowspan="1" colspan="1">No configuration of schema</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MMAX2</td>
              <td rowspan="1" colspan="1">[<xref rid="ref18" ref-type="bibr">18</xref>]</td>
              <td rowspan="1" colspan="1">[Web-based]</td>
              <td rowspan="1" colspan="1">Stand-alone tool</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NOMAD</td>
              <td rowspan="1" colspan="1">[<xref rid="ref58" ref-type="bibr">58</xref>]</td>
              <td rowspan="1" colspan="1">[Web-based]</td>
              <td rowspan="1" colspan="1">Stand-alone tool</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ODIN</td>
              <td rowspan="1" colspan="1">[<xref rid="ref59" ref-type="bibr">59</xref>]</td>
              <td rowspan="1" colspan="1">[Available][Schematic]</td>
              <td rowspan="1" colspan="1">Not available or not found, and fixed schema</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">OLLIE</td>
              <td rowspan="1" colspan="1">[<xref rid="ref60" ref-type="bibr">60</xref>]</td>
              <td rowspan="1" colspan="1">[Available]</td>
              <td rowspan="1" colspan="1">Not available or not found</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PALinkA</td>
              <td rowspan="1" colspan="1">[<xref rid="ref61" ref-type="bibr">61</xref>]</td>
              <td rowspan="1" colspan="1">[Available][Web-based]</td>
              <td rowspan="1" colspan="1">URL did not work, and stand-alone tool</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PACTE</td>
              <td rowspan="1" colspan="1">[<xref rid="ref62" ref-type="bibr">62</xref>]</td>
              <td rowspan="1" colspan="1">[Workable]</td>
              <td rowspan="1" colspan="1">Documentation not available</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PubTator</td>
              <td rowspan="1" colspan="1">[<xref rid="ref22" ref-type="bibr">22</xref>]</td>
              <td rowspan="1" colspan="1">[Schematic]</td>
              <td rowspan="1" colspan="1">No configuration of schema</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Pundit</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">[Schematic]</td>
              <td rowspan="1" colspan="1">No configuration of schema</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RAD</td>
              <td rowspan="1" colspan="1">[<xref rid="ref63" ref-type="bibr">63</xref>]</td>
              <td rowspan="1" colspan="1">[Available]</td>
              <td rowspan="1" colspan="1">Not available or not found</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SALTO</td>
              <td rowspan="1" colspan="1">[<xref rid="ref64" ref-type="bibr">64</xref>]</td>
              <td rowspan="1" colspan="1">[Web-based]</td>
              <td rowspan="1" colspan="1">Stand-alone tool</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SANTO</td>
              <td rowspan="1" colspan="1">[<xref rid="ref65" ref-type="bibr">65</xref>]</td>
              <td rowspan="1" colspan="1">[Workable]</td>
              <td rowspan="1" colspan="1">Configuration of schema and document import are confusing</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SAPIENT</td>
              <td rowspan="1" colspan="1">[<xref rid="ref21" ref-type="bibr">21</xref>]</td>
              <td rowspan="1" colspan="1">[Web-based]</td>
              <td rowspan="1" colspan="1">Stand-alone tool</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SAWT</td>
              <td rowspan="1" colspan="1">[<xref rid="ref66" ref-type="bibr">66</xref>]</td>
              <td rowspan="1" colspan="1">[Available]</td>
              <td rowspan="1" colspan="1">Not available or not found</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Semantator</td>
              <td rowspan="1" colspan="1">[<xref rid="ref67" ref-type="bibr">67</xref>]</td>
              <td rowspan="1" colspan="1">[Available][Web-based]</td>
              <td rowspan="1" colspan="1">URL did not work, and plug-in in Protégé</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Serengeti</td>
              <td rowspan="1" colspan="1">[<xref rid="ref68" ref-type="bibr">68</xref>]</td>
              <td rowspan="1" colspan="1">[Available]</td>
              <td rowspan="1" colspan="1">URL does not exist</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Slate</td>
              <td rowspan="1" colspan="1">[<xref rid="ref69" ref-type="bibr">69</xref>]</td>
              <td rowspan="1" colspan="1">[Available]</td>
              <td rowspan="1" colspan="1">Not available or not found</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SLATE</td>
              <td rowspan="1" colspan="1">[<xref rid="ref69" ref-type="bibr">69</xref>]</td>
              <td rowspan="1" colspan="1">[Web-based]</td>
              <td rowspan="1" colspan="1">Run as a terminal</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SYNC3</td>
              <td rowspan="1" colspan="1">[<xref rid="ref70" ref-type="bibr">70</xref>]</td>
              <td rowspan="1" colspan="1">[Available]</td>
              <td rowspan="1" colspan="1">Not available or not found</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Textpresso</td>
              <td rowspan="1" colspan="1">[<xref rid="ref71" ref-type="bibr">71</xref>]</td>
              <td rowspan="1" colspan="1">[Schematic]</td>
              <td rowspan="1" colspan="1">No configuration of schema</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">UAM Corpus</td>
              <td rowspan="1" colspan="1">[<xref rid="ref72" ref-type="bibr">72</xref>]</td>
              <td rowspan="1" colspan="1">[Web-based]</td>
              <td rowspan="1" colspan="1">Stand-alone tool</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Vogon</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">[Web-based]</td>
              <td rowspan="1" colspan="1">Stand-alone tool</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">WARP-Text</td>
              <td rowspan="1" colspan="1">[<xref rid="ref73" ref-type="bibr">73</xref>]</td>
              <td rowspan="1" colspan="1">[Installable]</td>
              <td rowspan="1" colspan="1">No documentation for installation</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">WASA</td>
              <td rowspan="1" colspan="1">[<xref rid="ref74" ref-type="bibr">74</xref>]</td>
              <td rowspan="1" colspan="1">[Available]</td>
              <td rowspan="1" colspan="1">Not available or not found</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">WebAnnotator</td>
              <td rowspan="1" colspan="1">[<xref rid="ref75" ref-type="bibr">75</xref>]</td>
              <td rowspan="1" colspan="1">[Available]</td>
              <td rowspan="1" colspan="1">URL does not exist</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">WordFreak</td>
              <td rowspan="1" colspan="1">[<xref rid="ref76" ref-type="bibr">76</xref>]</td>
              <td rowspan="1" colspan="1">[Web-based]</td>
              <td rowspan="1" colspan="1">Stand-alone tool</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">XConc Suite</td>
              <td rowspan="1" colspan="1">[<xref rid="ref20" ref-type="bibr">20</xref>]</td>
              <td rowspan="1" colspan="1">[Web-based]</td>
              <td rowspan="1" colspan="1">Plug-in in Eclipse</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">YEDDA</td>
              <td rowspan="1" colspan="1">[<xref rid="ref77" ref-type="bibr">77</xref>]</td>
              <td rowspan="1" colspan="1">[Web-based]</td>
              <td rowspan="1" colspan="1">Stand-alone tool</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>We present the tools by alphabetic order. For more details on the requirements, please refer to subsection <xref ref-type="sec" rid="sec2b">2.2</xref>. The URLs of the tools are included in the supplementary material and on-line in our GitHub page.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>The URLs of all nonselected tools (when found) are provided as supplementary material and are also available in the GitHub repository. Eight of the 13 tools that we evaluated in our previous survey [<xref rid="ref11" ref-type="bibr">11</xref>] failed to comply with the Web application requirement and were removed from our new evaluation: @Note, Callisto, GATE, Knowtator, MMAX2, Semantator, WordFreak and XConc Suite. One tool (Bionotate) was not selected because documentation is not detailed enough to instruct the user on how to install and start the application. Further, one tool (Argo) was not selected because an error occurred while executing the workflow and annotating a document was not possible.</p>
      <p>We aimed to evaluate at least three requirements for all tools, namely, whether it is available, is a Web application and is schematic. However, we failed to do so in a few situations. We do not know whether eHost and RAD are Web-based or stand-alone tools. Both tools are not available, the publication for the RAD tool is not open access and we did not find a publication for the eHost tool. As for the schematic requirement, the publications from Serengueti and WASA do not provide enough information to evaluate this feature, while we could not access the publication from RAD.</p>
      <p>From the 63 nonselected tools, we eliminated tools based on all five requirements. The most frequent elimination reason was the tool not being Web-based (27 times), followed by not available (21 times), not schematic (13 times), not workable (9 times) and not installable (7 times).</p>
      <p>The tools that are not a Web application are usually stand-alone tools, but we also found plug-ins to Protégé and Eclipse. We eliminated one tool that runs on a Web browser, namely SAPIENT. It is Web-based but requires installation on the client side and it is not meant to be installed as a Web application and accessed using a browser by various users.</p>
      <p>Regarding availability, most of the tools that were eliminated based on this requirement are described in their publications but without a corresponding URL (e.g. CRAB reader). However, some tools had a URL cited in a publication but that no longer existed (e.g. Serengeti).</p>
      <p>Seven tools (Anafora, Bionotate, CCASH, DOMEO, GATE Teamware, Marky and WARP-Text) did comply with the above requirements, but their installation failed. The documentation for Anafora, Bionotate, CCASH, DOMEO and WARP-Text is poor and does not provide instructions on how to install or start the application. We did our best to install Marky and even checked a forum in the Web for additional information, but the documentation was too confusing, e.g. making reference to files that we could not find in the installation files. A similar problem occurred with the GATE Teamware, which we decided not to install after experiencing many problems since the tool requires a specific version of Tomcat and MySQL, among other dependencies.</p>
      <p>The reasons for a tool being classified as not workable were various, such as not being able to run the workflow that we created owing to an error (Argo), problems related to log-in (CLARIN-EL and Egas), documentation not available in English (Analec and Inforex) or not clear enough to allow for its use (SANTO). The tools that were not schematic were usually those that only allow simple comments and highlighting without a semantic type, such as in the commercial tools Hypothesis and Pundit. Other nonschematic tools include MDSWriter (designed for the annotation of summaries), Textpresso (suitable for data curation) and PubTator (limited to a few biomedical semantic types).</p>
    </sec>
    <sec id="sec3b">
      <title>Selected tools</title>
      <p>We list the 15 selected tools, provide a short summary for each and present a detailed evaluation that we carried out based on the 26 criteria (cf. Section <xref ref-type="sec" rid="sec2c">2.3</xref>) and the derived scores.</p>
      <sec id="sec3b1">
        <title>List of selected tools</title>
        <p>The tools that we selected for a detailed evaluation are listed alphabetically. These tools complied with the five requirements that we defined (cf. Section <xref ref-type="sec" rid="sec2b">2.2</xref>). We provide a short summary for each tool that includes whether it is available online, import/export formats, configuration of schema, available tasks and other relevant features. Further details are presented as supplementary material and the selected tools are also listed in our GitHub repository along with links to the publication, when available, and Web site.</p>
        <p>BioQRator [<xref rid="ref78" ref-type="bibr">78</xref>] (<ext-link ext-link-type="uri" xlink:href="http://www.bioqrator.org">http://www.bioqrator.org</ext-link>)
This is a tool designed for the annotation of biomedical literature. No local installation is possible, but an online version is available for free.
BioQRator supports the BioC format (via file upload) and the retrieval of PubMed articles (via Web services) as input formats, while the export functionality uses the BioC or CSV formats. The annotation schema can be configured in the Web interface by manually adding concepts and assigning them for the annotation of named entities and/or relations. Finally, BioQRator provides pre-annotations based on the Entrez and UniProtKB databases for genes and proteins.</p>
        <p>brat [<xref rid="ref79" ref-type="bibr">79</xref>] (<ext-link ext-link-type="uri" xlink:href="http://brat.nlplab.org">http://brat.nlplab.org</ext-link>)
This is one of the most popular tools for the manual annotation of documents and has been used for the development of various corpora (e.g. [<xref rid="ref80" ref-type="bibr">80</xref>]). brat needs to be locally installed because it is not available online. The schema is configured in a plain text file, and documents are imported in the same format. The annotations can be exported in a similar plain-text format. The highlighting of entities and relations is possible, as well as the normalization to pre-defined terminologies. Although the last version was released in 2012, the tool is still available and is popular in the field. Latest improvements include the embedding of visualizations in HTML pages and integration to external TM tools, among others.</p>
        <p>Catma (<ext-link ext-link-type="uri" xlink:href="http://catma.de">http://catma.de</ext-link>) This is an online tool that allows for the creation of a corpus collection by importing plain-text documents or the retrieval of HTML documents by entering a certain URL. The user can manually create tagsets and annotate the documents based on these. However, the annotation of relationships is not possible. Automatic annotations are allowed based on queries, e.g. by searching for particular words or terms. Finally, the annotations are exported into the XML format and documents can be shared among users.</p>
        <p>Djangology [<xref rid="ref81" ref-type="bibr">81</xref>] (<ext-link ext-link-type="uri" xlink:href="http://sourceforge.net/projects/djangology">http://sourceforge.net/projects/djangology</ext-link>)
This is a Web-based annotation tool for the collaborative annotation of documents.
Users, teams and projects can be configured and managed in the tool as it provides good support for IAA and the construction of a consensus corpus. The annotation schema can be configured in the Web interface, and documents are imported in plain text format. The annotation can be later exported in the plain text format as well.</p>
        <p>ezTag [<xref rid="ref82" ref-type="bibr">82</xref>] (<ext-link ext-link-type="uri" xlink:href="http://eztag.bioqrator.org">http://eztag.bioqrator.org</ext-link>)
This is a tool that allows curators to perform manual annotation and provides training data using the human-in-the-loop process. The tool is available online but can also be locally installed because the source code is available (<ext-link ext-link-type="uri" xlink:href="http://github.com/ncbi-nlp/eztag">http://github.com/ncbi-nlp/eztag</ext-link>). ezTag supports both abstracts from PubMed and full-text articles from PMC. It also provides lexicon-based concept tagging as well as state-of-the-art pre-trained taggers, e.g. TaggerOne, GNormPlus and tmVar.</p>
        <p>FLAT (<ext-link ext-link-type="uri" xlink:href="http://github.com/proycon/flat">http://github.com/proycon/flat</ext-link>)
This allows for semantic and linguistic annotations using the FoLiA format [<xref rid="ref83" ref-type="bibr">83</xref>]. The tool needs to be installed locally as it is not available online. Using the system was not intuitive and the FoLiA XML format, although comprehensive, is not straightforward to use. This is the same format to define new annotation types. We were only able to upload an example document (in Dutch) that was available from the tool but did not succeed in uploading our own biomedical document. FLAT is probably more suitable for linguistic annotations given its support for annotating dependencies, lemmas, chunks, etc.</p>
        <p>LightTag (<ext-link ext-link-type="uri" xlink:href="https://www.lighttag.io/">https://www.lighttag.io/</ext-link>)
LightTag is an online commercial annotation tool, which is rich in features. No local installation is possible. It supports working in various languages (Arabic, Hebrew and CJK among others), document level, multiword, nested and relationship annotations, among other. Additionally, it uses machine learning to learn from active annotators and suggests possible annotations on unseen text. Annotators can also be split up in teams. Finally, various metrics regarding the entities and annotators are made available, either via the Web interface or via the LightTag API (<ext-link ext-link-type="uri" xlink:href="https://guide.lighttag.io/in-depth/api.html">https://guide.lighttag.io/in-depth/api.html</ext-link>).</p>
        <p>MAT (<ext-link ext-link-type="uri" xlink:href="http://mat-annotation.sourceforge.net">http://mat-annotation.sourceforge.net</ext-link>)
This is a tool that includes an active learning functionality. The tool needs to be installed locally because it is not available online. It was not difficult to install, but its use was not intuitive. Although the documentation is comprehensive, it is rather long and some topics are difficult to find, such as the format of the input files. It is possible to configure the schema as an XML import file, including attributes and visualization properties. The annotations can be exported in either JSON or XML formats.</p>
        <p>MyMiner [<xref rid="ref84" ref-type="bibr">84</xref>] (<ext-link ext-link-type="uri" xlink:href="http://myminer.armi.monash.edu.au">http://myminer.armi.monash.edu.au</ext-link>) This is only available online and cannot be locally installed. Through an integration with PubMed, it is possible to retrieve abstracts and create datasets for later annotation. However, this procedure is rather slow. MyMiner provides support for document labeling, tagging of entity and binary relationships, entity linking and for comparing two annotated files. Pre-annotations can be provided for some entity types based on the Abner and LINNAEUS tools. Documents can be imported in plain-text format and annotations can be exported in a similar format, but the annotation schema can only be configured on-line. No log-in to the system is needed, thus, there is no support for users and teams.</p>
        <p>PDFAnno [<xref rid="ref85" ref-type="bibr">85</xref>] (<ext-link ext-link-type="uri" xlink:href="http://github.com/paperai/pdfanno">http://github.com/paperai/pdfanno</ext-link>)
This is an open-source tool for the annotation of PDF documents. The documents can only be uploaded in the PDF format, and annotations can be carried out for entities and relationships. PDFAnno also allows for the annotation of nontextual regions in the PDF such as figures and tables. The annotation schema can be configured in the Web interface or imported in the plain text format. Partially saved annotations can be exported in the plain text format and later re-imported for further annotation. Teams and users can only work with this tool if all users have access to the particular folder in the server where all PDFs are stored and where the exported annotations are stored. Annotations from various users can be uploaded simultaneously for the creation of a consensus corpus.</p>
        <p>prodigy (<ext-link ext-link-type="uri" xlink:href="https://prodi.gy/">https://prodi.gy/</ext-link>) This is a commercial tool, which is freely available only as a demo (showcasing the Web application GUI annotation interface). The developers offer the tool either as a virtual machine (for less tech savvy users) or as local installation. Although the entire setup (installation, starting of the service, setting up initial terminologies for individual labels among other) is done via CLI, the tool is easy to use and made for novice users. It works with multiple file types, which need to be structured appropriately, and it also allows the use of several storage types. prodigy is made with the goal of enabling active learning. As such, it allows for a quick setup of the needed environment and bootstraping the first machine learning models. Currently, it supports the tasks of document classification and named entity recognition. Tasks such as nested/discontinuous spans, relation extraction and automatic linking to knowledge bases/ontologies of interest are currently not supported out of the box. The Web application GUI offers a clean and usable annotation interface and annotations are exported, again via CLI.</p>
        <p>tagtog [<xref rid="ref86" ref-type="bibr">86</xref>] (<ext-link ext-link-type="uri" xlink:href="http://www.tagtog.net">http://www.tagtog.net</ext-link>) This is an online commercial tool for the annotation of documents on both entity and document levels, as well as of relationships. Because it is a commercial tool, it has some limitations in its free version (e.g. it can only be used online and cannot be installed locally). It provides an easy integration with PubMed for abstracts and full text retrieval and for the definition of a schema in the Web interface. Finally, a machine learning functionality is available for active learning, but we did not evaluate it. The annotations can be linked to some databases (e.g. Entrez and UniProtKB).</p>
        <p>TextAE (<ext-link ext-link-type="uri" xlink:href="http://github.com/pubannotation/textae">http://github.com/pubannotation/textae</ext-link>) This allows for the annotation of entities and relationships in documents.
It is open source, and it is possible to use the fully functional online version or install it locally. The documents and pre-annotations can be imported in the PubAnnotation JSON format, and the tool provides different views for the annotation of entities and relations. The annotations and documents can be exported in the same JSON file. As an additional feature, it allows for the embedding of annotations in HTML documents for visualization. Finally, it can be used with PubAnnotation [<xref rid="ref87" ref-type="bibr">87</xref>] for the remote storage of document collections and for integration with PubMed and PMC.</p>
        <p>WAT-SL [<xref rid="ref88" ref-type="bibr">88</xref>] (<ext-link ext-link-type="uri" xlink:href="http://github.com/webis-de/wat">http://github.com/webis-de/wat</ext-link>) This is a tool developed for segmentation labeling. WAT-SL is not available on-line, and it needs to be locally installed. It is not possible to highlight an arbitrary span of text, and thus the documents must be previously split into the units that will be annotated whether they are paragraphs, sentences, clauses, chunks or tokens. It is simple to install and use, even though the documentation is poor. The schema (segments and labels), input documents and annotations are defined in simple plain-text files. It is possible to define users and assign projects to each of them. A ‘curator interface’ supports the creation of a consensus corpus based on majority voting.</p>
        <p>WebAnno (<ext-link ext-link-type="uri" xlink:href="http://webanno.github.io/webanno">http://webanno.github.io/webanno</ext-link>) [<xref rid="ref89" ref-type="bibr">89</xref>, <xref rid="ref90" ref-type="bibr">90</xref>] This is also a very popular annotation tool that provides full functionality for both semantic and syntax annotations. WebAnno allows for the import of schema and documents and the export of annotations in a variety of formats. The tool works with labels for various syntactic and semantic annotations.
WebAnno provides support for users as well as for IAA. It is one of the more comprehensive tools, but its use is not very intuitive (owing to its various annotation layers). However, the documentation is good and covers most of the important topics.</p>
      </sec>
      <sec id="sec3b2">
        <title>Evaluation of selected tools</title>
        <p>We present a detailed evaluation of the 15 selected tools based on our 26 criteria (cf. Section <xref ref-type="sec" rid="sec2c">2.3</xref>). Each criterion was defined using a three-level scale (cf. Table <xref rid="TB1" ref-type="table">1</xref>), and the results are depicted in Table <xref rid="TB3" ref-type="table">3</xref>.</p>
        <table-wrap id="TB3" orientation="portrait" position="float">
          <label><sc>Table</sc> 3</label>
          <caption>
            <p>Visualization of the three-values evaluation of the criteria for the selected tools</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col span="1" align="left"/>
            </colgroup>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">
                  <inline-graphic xlink:href="bbz130t3.jpg" mimetype="image"/>
                </td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <p>List of criteria: year of last publication (P1), citations in Google Scholar (P2), citations for corpus development (P3), date of last version (T1), availability of source code (T2), online availability for use (T3), easiness of installation (T4), quality of documentation (T5), type of license (T6), free of charge (T7), format of schema (D1), input format for documents (D2), output format for annotations (D3), allowance of multi-label annotations (F1), allowance of document-level annotations (F2), support for relationships (F3), support for ontologies (F4), support for pre-annotations (F5), integration with Medline or PubMed (F6), suitability for full texts (F7), allowance of partial saving (F8), ability to highlight (F9), support for users and teams (F10), support for IAA (F11), data privacy (F12) and support for various languages (F13). For more details on the criteria, please refer to subsection <xref ref-type="sec" rid="sec2c">2.3</xref>. The last three columns show the number of occurrences for each value in the three-values scale.</p>
          </table-wrap-foot>
        </table-wrap>
        <p>The three tools that fulfilled most of the criteria (gray color) were WebAnno, FLAT and ezTag: 20, 18 and 17 criteria, respectively.
One tool fulfilled 16 criteria (brat), one tool 15 criteria (PDFAnno), six tools 14 criteria (Catma, LightTag, MAT, prodigy, tagtog and TextAE) and one tool 13 criteria (BioQRator). Finally, Djangology and MyMiner fulfilled 10 criteria, and one tool (WAT-SL) only nine. Meanwhile, there are three tools (prodigy, LightTag and WAT-SL) that missed the most criteria (11, 10 and 10, respectively), followed by four tools (BioQRator, MAT, MyMiner and tagtog) that missed nine criteria each. The tools that less criteria missed were WebAnno (four criteria) and brat (only three criteria).</p>
        <p>WebAnno and brat are the most comprehensive tools and also the most popular tools regarding number of citations.</p>
        <p>brat and WebAnno were the only tools that fulfilled all three criteria for publication, while Catma, FLAT, LightTag, MAT, prodigy and TextAE could not even partially fulfill any of these, given that they do not have a corresponding publication. The technical criteria were completely fulfilled by three tools (Catma, FLAT and TextAE), given that they are available online and for download. brat and WebAnno did not score well here because these tools are not available online and are difficult to install. Ten of the 15 tools fulfilled the data criteria, while no tool fulfilled all of the functional criteria, probably owing to the high number of these (13 in total).</p>
        <p>The visualization using the three-level scale in Table <xref rid="TB3" ref-type="table">3</xref> provides a good overview of the fulfillment. Further, we provide an analysis of the top missing and fulfilled criteria in our discussion section. Based on the results from Table <xref rid="TB3" ref-type="table">3</xref>, we present in Table <xref rid="TB4" ref-type="table">4</xref> the number of points received by each tool per group of criteria and in total, along with the score. We show a visualization of the percentage of fulfillment in Figure <xref ref-type="fig" rid="f2">2</xref>.</p>
        <table-wrap id="TB4" orientation="portrait" position="float">
          <label><sc>Table</sc> 4</label>
          <caption>
            <p>Summary of the evaluation of all criteria for the selected tools</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col span="1" align="left"/>
              <col span="2" align="center"/>
              <col span="3" align="center"/>
              <col span="4" align="center"/>
              <col span="5" align="center"/>
              <col span="6" align="center"/>
              <col span="7" align="center"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Tools</th>
                <th rowspan="1" colspan="1">P</th>
                <th rowspan="1" colspan="1">T</th>
                <th rowspan="1" colspan="1">D</th>
                <th rowspan="1" colspan="1">F</th>
                <th rowspan="1" colspan="1">Total</th>
                <th rowspan="1" colspan="1">Scores</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">BioQRator</td>
                <td rowspan="1" colspan="1">1.5</td>
                <td rowspan="1" colspan="1">4.5</td>
                <td rowspan="1" colspan="1">3.0</td>
                <td rowspan="1" colspan="1">6.0</td>
                <td rowspan="1" colspan="1">15</td>
                <td rowspan="1" colspan="1">0.58</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">brat</td>
                <td rowspan="1" colspan="1">3.0</td>
                <td rowspan="1" colspan="1">5.0</td>
                <td rowspan="1" colspan="1">2.0</td>
                <td rowspan="1" colspan="1">9.5</td>
                <td rowspan="1" colspan="1">19.5</td>
                <td rowspan="1" colspan="1">0.75</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Catma</td>
                <td rowspan="1" colspan="1">0.0</td>
                <td rowspan="1" colspan="1">7.0</td>
                <td rowspan="1" colspan="1">2.5</td>
                <td rowspan="1" colspan="1">6.5</td>
                <td rowspan="1" colspan="1">16</td>
                <td rowspan="1" colspan="1">0.61</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Djangology</td>
                <td rowspan="1" colspan="1">1.5</td>
                <td rowspan="1" colspan="1">3.0</td>
                <td rowspan="1" colspan="1">2.0</td>
                <td rowspan="1" colspan="1">7.5</td>
                <td rowspan="1" colspan="1">14</td>
                <td rowspan="1" colspan="1">0.54</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ezTag</td>
                <td rowspan="1" colspan="1">1.0</td>
                <td rowspan="1" colspan="1">6.0</td>
                <td rowspan="1" colspan="1">3.0</td>
                <td rowspan="1" colspan="1">7.5</td>
                <td rowspan="1" colspan="1">17.5</td>
                <td rowspan="1" colspan="1">0.67</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">FLAT</td>
                <td rowspan="1" colspan="1">0.0</td>
                <td rowspan="1" colspan="1">7.0</td>
                <td rowspan="1" colspan="1">3.0</td>
                <td rowspan="1" colspan="1">8.5</td>
                <td rowspan="1" colspan="1">18.5</td>
                <td rowspan="1" colspan="1">0.71</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">LightTag</td>
                <td rowspan="1" colspan="1">0.0</td>
                <td rowspan="1" colspan="1">3.5</td>
                <td rowspan="1" colspan="1">3.0</td>
                <td rowspan="1" colspan="1">8.5</td>
                <td rowspan="1" colspan="1">15</td>
                <td rowspan="1" colspan="1">0.58</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">MAT</td>
                <td rowspan="1" colspan="1">0.0</td>
                <td rowspan="1" colspan="1">4.5</td>
                <td rowspan="1" colspan="1">3.0</td>
                <td rowspan="1" colspan="1">8.0</td>
                <td rowspan="1" colspan="1">15.5</td>
                <td rowspan="1" colspan="1">0.60</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">MyMiner</td>
                <td rowspan="1" colspan="1">1.5</td>
                <td rowspan="1" colspan="1">4.0</td>
                <td rowspan="1" colspan="1">2.0</td>
                <td rowspan="1" colspan="1">6.0</td>
                <td rowspan="1" colspan="1">13.5</td>
                <td rowspan="1" colspan="1">0.52</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">PDFAnno</td>
                <td rowspan="1" colspan="1">1.0</td>
                <td rowspan="1" colspan="1">6.5</td>
                <td rowspan="1" colspan="1">3.0</td>
                <td rowspan="1" colspan="1">6.5</td>
                <td rowspan="1" colspan="1">17</td>
                <td rowspan="1" colspan="1">0.65</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">prodigy</td>
                <td rowspan="1" colspan="1">0.0</td>
                <td rowspan="1" colspan="1">3.0</td>
                <td rowspan="1" colspan="1">3.0</td>
                <td rowspan="1" colspan="1">8.5</td>
                <td rowspan="1" colspan="1">14.5</td>
                <td rowspan="1" colspan="1">0.56</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">tagtog</td>
                <td rowspan="1" colspan="1">1.5</td>
                <td rowspan="1" colspan="1">3.0</td>
                <td rowspan="1" colspan="1">3.0</td>
                <td rowspan="1" colspan="1">8.0</td>
                <td rowspan="1" colspan="1">15.5</td>
                <td rowspan="1" colspan="1">0.60</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">TextAE</td>
                <td rowspan="1" colspan="1">0.0</td>
                <td rowspan="1" colspan="1">7.0</td>
                <td rowspan="1" colspan="1">3.0</td>
                <td rowspan="1" colspan="1">6.0</td>
                <td rowspan="1" colspan="1">16</td>
                <td rowspan="1" colspan="1">0.61</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">WAT-SL</td>
                <td rowspan="1" colspan="1">1.0</td>
                <td rowspan="1" colspan="1">5.5</td>
                <td rowspan="1" colspan="1">1.5</td>
                <td rowspan="1" colspan="1">4.5</td>
                <td rowspan="1" colspan="1">12.5</td>
                <td rowspan="1" colspan="1">0.48</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">WebAnno</td>
                <td rowspan="1" colspan="1">3.0</td>
                <td rowspan="1" colspan="1">5.0</td>
                <td rowspan="1" colspan="1">3.0</td>
                <td rowspan="1" colspan="1">10.0</td>
                <td rowspan="1" colspan="1">21</td>
                <td rowspan="1" colspan="1">0.81</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Average</td>
                <td rowspan="1" colspan="1">1.0</td>
                <td rowspan="1" colspan="1">4.9</td>
                <td rowspan="1" colspan="1">2.7</td>
                <td rowspan="1" colspan="1">7.4</td>
                <td rowspan="1" colspan="1">16.1</td>
                <td rowspan="1" colspan="1">0.62</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Possible max</td>
                <td rowspan="1" colspan="1">3.0</td>
                <td rowspan="1" colspan="1">7.0</td>
                <td rowspan="1" colspan="1">3.0</td>
                <td rowspan="1" colspan="1">13.0</td>
                <td rowspan="1" colspan="1">26</td>
                <td rowspan="1" colspan="1">1.0</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <p>Summary of the points received by each tool for each group of criteria (publication, technical, data and functional), as well as the final total of points and score. The points are calculated according the number of number of gray, light gray and white features as shown in Table <xref rid="TB3" ref-type="table">3</xref>. In the last two lines, we show the average for each criterion category and the possible maximum number of points and score.</p>
            <p>For more details on the criteria, please refer to subsection <xref ref-type="sec" rid="sec2c">2.3</xref>.</p>
          </table-wrap-foot>
        </table-wrap>
        <fig id="f2" orientation="portrait" position="float">
          <label><sc>Fig.</sc> 2.</label>
          <caption>
            <p>Visualization of the total percentage of fulfillment and the contribution of each criteria category.</p>
          </caption>
          <graphic xlink:href="bbz130f2"/>
        </fig>
        <p>The scores varied from 0.48 (WAT-SL) to 0.81 (WebAnno), with an average value of 0.62. This shows that many of the tools fulfilled at least half of our criteria. Two tools scored under or slightly over the 0.50 value, namely, WAT-SL (0.48) and MyMiner (0.52). The tools WebAnno (0.81), brat (0.75) and FLAT (0.71) obtained scores over 0.70. Indeed, the average score of all tools is 0.62, which shows an overall reasonable coverage of many criteria by most of the tools. We also calculated a score-per-criteria category to check which categories were less fulfilled by the tools. The group with the most coverage is data (score of 0.88), followed by technical (0.71), functional (0.57) and publication (0.33). Certainly, the reason for the high score of the data category was due to these covering just the two highest levels of fulfillment.</p>
        <p>We performed an ablation study to investigate how the scores (and corresponding ranks) of the annotation tools changed by removing each of the groups of criteria separately. On one hand, the ranks for WebAnno and brat varied very little since their positions were always among the top three. On the other hand, the ranks for some tools (Catma, LighTag, PDFAnno, prodigy, tagtog, TextAE) varied substantially depending of the group of criteria that was removed, which indicates that their coverage across the various categories is still unbalanced. Details for the various scores and ranks are presented as supplementary material. In spite of this study, the main goal of the scores was not to decide winners among the tools but to identify those that cover the most criteria that we defined.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec4">
    <title>Discussion</title>
    <p>This discussion is based on the current state of the art of annotation tools and considers interesting aspects that we observed during this study, such as tools more suitable for the biomedical domain, for the annotation for relationships and the scarce number of tools for document-level annotations.</p>
    <sec id="sec4a">
      <title>State of the art of annotation tools</title>
      <p>As we described in the previous section, the average score of 0.62 and minimum score of 0.48 show that all selected tools cover the determined criteria to a great amount. Further, these 15 tools also comply with our five requirements to be in the selected group, especially when considering that 63 other tools could not comply with all requirements.</p>
      <p>While the highest scores (up to 0.81) indicate that many of our criteria have indeed been covered by many of the tools, this still leaves almost 20% of the points un-fulfilled (cf. Figure <xref ref-type="fig" rid="f2">2</xref>). However, WebAnno, brat and FLAT can probably comply with the needs of most users and should be the first choices for those looking for a complete and general-purpose annotation tool. Further, ezTag (fourth best score of 0.67) offers integration with PubMed and PMC, which is particularly important for those working in the field of biomedicine.</p>
      <p>Meanwhile, the tools with the lowest scores can also be useful for many researchers, but they are probably more suitable for particular tasks. For instance, WAT-SL, which obtained the lowest score, was designed for segmentation labeling, and thus did not support many of the features that we evaluated. Indeed, this is one of its advantages, as described by the authors [<xref rid="ref88" ref-type="bibr">88</xref>], over general-purpose annotation tools.
The nonselected tools can also be useful for particular tasks, such as Textpresso, which is suitable for the customization of curation tasks [<xref rid="ref71" ref-type="bibr">71</xref>], Knowtator for annotation based on ontologies [<xref rid="ref19" ref-type="bibr">19</xref>] and MDSWriter for developing summaries [<xref rid="ref23" ref-type="bibr">23</xref>].</p>
      <p>While many tools have been developed in the past several decades, new tools are constantly being released. Indeed, some of the tools evaluated here were released in 2018 or 2019, e.g. APLenty [<xref rid="ref32" ref-type="bibr">32</xref>], KCAT [<xref rid="ref55" ref-type="bibr">55</xref>], PDFAnno [<xref rid="ref85" ref-type="bibr">85</xref>], SANTO [<xref rid="ref65" ref-type="bibr">65</xref>], SLATE [<xref rid="ref69" ref-type="bibr">69</xref>], WARP-Text [<xref rid="ref64" ref-type="bibr">64</xref>] and WASA [<xref rid="ref74" ref-type="bibr">74</xref>]. Further, by observing criteria P1 (year of the last publication) and T1 (date of the last version), we conclude that nine of the selected tools were published since 2009, and eight tools released their latest version (or source code’s commit) since 2014.</p>
      <p>We hypothesize that the reasons for creating a new tool are various. A particular (but important) feature might be missing; indeed, fulfillment of our criteria is still under 80% for most tools. For instance, PDFAnno was recently created aiming to support the annotation of PDF files, a feature not supported by some of the most popular tools, such as brat. Sometimes, making changes to an existing tool might not be an option because source code is not available for many tools (criterion only fulfilled by eight tools). Further, some tools are developed with a very particular task in mind, as confirmed by the many tools that were eliminated for not being schematic. For instance, MDSWriter is an interesting tool for creating corpora for multi-document summarization.</p>
    </sec>
    <sec id="sec4b">
      <title>Top fulfilled features</title>
      <p>When looking at the results in Table <xref rid="TB3" ref-type="table">3</xref>, no white cells are visible for the data-related criteria. The main reason for this is that our own definition of the levels did not consider the lowest level (white) for this category (cf. Table <xref rid="TB1" ref-type="table">1</xref>), as already explained in Section <xref ref-type="sec" rid="sec2">2</xref>. Moreover, only 10 (around 22%) of 42 cells (15 tools <inline-formula><tex-math id="M1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\times $\end{document}</tex-math></inline-formula> 3 criteria) obtained a partial score. This demonstrates that most of the tools utilize standard input and output formats, such as XML and JSON. In spite of the above, a non-standard format does not necessarily stop users from using a certain tool. For instance, brat is one of the most popular tools, and its publication has more than 400 citations. We found at least 10 citations for corpus construction. However, brat supports no standard formats and uses a plain stand-off text format.</p>
      <p>Among the technical criteria, we observe just one white space for the easiness of installation (T4) and for free of charge (T7), and none for the quality of the documentation (T5). The latter indicates that the quality of the documentation of individual tools is either high or adequate. Indeed, tools without a tutorial or documentation were not selected because they were either not installable or not workable. Regarding the easiness of installation (T4), only one tool (WebAnno) was assessed as being difficult to install.
Actually, many of the tools that fulfilled this criterion had an online version available (as shown by criterion T3). Finally, only one tool (prodigy) does not have a free-of-charge version of their system.</p>
      <p>The top functional criteria fulfilled by most of the tools were support for full text (F7, 11 times, once partially), partially annotating documents (F8, 11 times, 4 times partially), highlighting (F9, 14 tools) and data privacy (F12, 11 times). Indeed, the ability to highlight, save partial annotations and support full text are features that most tools currently provide. Meanwhile, the possibility of data privacy (F12), an important topic nowadays, given the high number of annotations of sensitive documents from patients, is fulfilled by all of the tools except those that are only available online and cannot be locally installed, namely, BioQRator, LightTag, MyMiner and tagtog.</p>
    </sec>
    <sec id="sec4c">
      <title>Top missing features</title>
      <p>The criteria related to the publication had the highest percentage of white cells, namely, 28 (around 62%) of 45 cells (15 tools <inline-formula><tex-math id="M2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\times $\end{document}</tex-math></inline-formula> 3 criteria). Fifteen of the white cells refer to tools (Catma, FLAT, LighTag, MAT and TextAE) that do not have a publication. Nevertheless, we find scientific publications an important resource for disseminating the tool, describing its main features in a summarized way and presenting a use case. Further, if criterion P1 (year of the last publication) is not met, this has a direct impact on the other two criteria that are associated with the citations even though we found references to some tools’ names in publications. Criterion P1 is also important for tracking future citations to the tool, and thus for enforcing its credibility. The tools that obtained the highest evaluation for all three publication criteria were brat and WebAnno, which are very popular and has already been used for a variety of corpus constructions, also in the biomedical domain [<xref rid="ref91" ref-type="bibr">91</xref>].</p>
      <p>Regarding technical criteria, the top missing features are the type of license (T6, 7 times), online availability of the tool (T3, 6 times), availability of the source code (T2, 6 times, plus 1 partially) and the year of the last version (T1, 3 times, plus 4 partially). More than half of the tools offer a free online version of the tool, thus relieving users from the burden of installation. Further, in spite of the increasing open-source movement, six tools do not provide their code for free, even though three of them are not commercial.</p>
      <p>Finally, the most missing functional features include document-level annotation (F2, 11 times), integration with PubMed/PMC (F6, 10 times), the use of ontologies or terminologies (F4, 8 times), IAA (F11, 8 times, plus 4 partially), support for multi-label annotation (F1, 7 times, plus 1 partially), relationship annotation (F3, 6 times, 1 partially) and support for teams and users (F10, 4 times, 8 partially).</p>
      <p>We discuss in more detail the support for integration with tools and resources for biomedicine in the next subsection and for document-level annotation in Subsection <xref ref-type="sec" rid="sec4f">4.6</xref>.</p>
    </sec>
    <sec id="sec4d">
      <title>Tools suitable for biomedicine</title>
      <p>Our survey evaluated whether the tools support the integration of PubMed or PMC (criterion F6), as this facilitates the retrieval, parsing and even pre-processing of documents for further annotations.</p>
      <p>Only four of the 15 selected tools fulfilled this functionality, either fully or partially: BioQRator, ezTag, MyMiner and tagtog. Indeed, these are tools that were developed by researchers working on biomedical natural language processing (BioNLP) or TM. ezTag and tagtog are probably the more suitable tools for this purpose because they provide integration with both PubMed and PMC, while BioQRator and MyMiner only provide integration with PubMed.
However, the functionality included in tagtog did not always work properly during our experiments (e.g. for the PMIDs 24167564, 24025585 and 23082216). Additionally, two tools (BioQRator and ezTag) support the BioC XML format [<xref rid="ref92" ref-type="bibr">92</xref>], which is a standard in the BioNLP community. Alternatively, the TextAE tool also allows for integration with PubMed using the PubAnnotation repository.</p>
      <p>In spite of the above, the limitations of PubMed and PMC are well known in the biomedical TM community. While Pubmed currently includes more than 29 millions citations (as of August/2019), many of these only contain titles, which are of little use for the researchers. Further, the PMC Open Access subset, which contain full text articles whose license allows automatic processing, currently only contains less than 2.5 million articles (as of August 2019).</p>
      <p>Further, some of the tools include pre-trained models or tools for the automatic extraction of semantic entities. We provide a summary of the entity types below:</p>
      <list list-type="bullet">
        <list-item>
          <p>BioQRator [<xref rid="ref78" ref-type="bibr">78</xref>]: genes/proteins;</p>
        </list-item>
        <list-item>
          <p>ezTag [<xref rid="ref82" ref-type="bibr">82</xref>]: chemicals, diseases, gene/proteins, species and variations;</p>
        </list-item>
        <list-item>
          <p>MyMiner [<xref rid="ref84" ref-type="bibr">84</xref>]: protein, DNA, RNA, cell line, cell type and organism from ABNER [<xref rid="ref93" ref-type="bibr">93</xref>];</p>
        </list-item>
        <list-item>
          <p>tagtog [<xref rid="ref86" ref-type="bibr">86</xref>]: gene/protein.</p>
        </list-item>
      </list>
      <p>Regarding the use of general-purpose tools such as brat and WebAnno for the annotation of biomedical documents, these might be more suitable for computational linguists than for biomedical researchers. It is desirable that an annotation tool should be as simple as possible and should not contain any additional functionalities that might disturb its main goal of performing the annotations themselves. In spite of this, many projects have used brat for the annotation of biomedical documents [<xref rid="ref94" ref-type="bibr">94</xref>, <xref rid="ref95" ref-type="bibr">95</xref>]. Finally, we identified some nonselected tools that support integration with either PubMed or PMC, namely, @Note, Argo, Egas, Marky, ODIN, PubTator and Textpresso. Further, Knowtator has also been used for the annotation of biomedical corpora given its good support for ontologies.</p>
    </sec>
    <sec id="sec4e">
      <title>Annotation of relationships</title>
      <p>Annotating textual documents with relationships between span texts is a common task when annotating corpora. Relations have been used for the annotation of linguistics elements, e.g. co-references [<xref rid="ref96" ref-type="bibr">96</xref>] and dependencies [<xref rid="ref7" ref-type="bibr">7</xref>], as well as semantic relationships between entities, either binaries relations [<xref rid="ref8" ref-type="bibr">8</xref>] or more complex biological events [<xref rid="ref95" ref-type="bibr">95</xref>].</p>
      <p>From all annotation tools that we reviewed, we found almost 30 available tools that support the annotation of relationships in some way. Among the selected ones, this criterion (F3) is fully supported by BioQRator, brat, MAT, PDFAnno, tagtog, TextAE and WebAnno, while partially supported by MyMiner. We consider that optimal annotation of relationships is performed in a drag-and-drop way, i.e. by graphically drawing arrows (or lines) between entities with the mouse while visualizing the whole text. Indeed, this is already supported by various tools, such as brat, LightTag, PDFAnno, TextAE, XConc Suite and WebAnno. Other tools support this feature using fields in a form or with a table (or matrix), with which the user defines which entities are related, usually only for binary relation but also sometimes by defining a predicate. This is supported by some tools, namely, Anafora, BioQRator, Callisto, Egas, Glozz, Inforex, Knowtator, MAE, MAT, MMAX2, PubTator, SANTO, tagtog, UAM Corpus and WARP-Text. Further, the Vogon tool provides support for annotating the text both on a text-based (in a form) or on graphical way by drawing arrow between boxes (entities). Interestingly, WordFreak present the text in a tree structure and the annotation of relations in a tabular way.</p>
      <p>Some tools provide only limited support for this feature. For instance, MyMiner presents a matrix of all annotated entities in which it is possible to annotate binary relations. Being a tool developed for the curation task, Bionotate allows the definition of a curation task that can also be used for annotating a single relationship per snippet of text, as carried out in [<xref rid="ref97" ref-type="bibr">97</xref>].</p>
    </sec>
    <sec id="sec4f">
      <title>Annotation tools for document classification</title>
      <p>The initial goal of this survey was to evaluate tools for annotation on the document level. Owing to the few tools that support this feature (F2), we had to widen the focus of the survey, and thus consider more annotation tools. From Table <xref rid="TB3" ref-type="table">3</xref>, only four tools address this functionality with span-less annotations: prodigy, MAT, MyMiner and tagtog. Further, we also identified some nonselected tools that seem to support document-level annotation: CRAB reader, LightTag, MAE, Textpresso, UAM Corpus, Vogon and WARP-Text.</p>
      <p>MyMiner seems to be the only selected tool that explicitly provides a specific task for this purpose. The user can specify the labels and annotate the documents based on the latter. Given the impossibility of configuring projects and users, the annotators have to define the labels every time a new set of documents is to be annotated. Additionally, the users need to be careful when configuring the labels (use the same names or the same order) in order to be able to later normalize annotations from various annotators or sessions.</p>
      <p>Most of the other tools only support this feature by highlighting the text span in the documents. One example involves zero-width annotations or a workaround by highlighting any fixed pre-defined token (e.g. ‘DOCUMENTLABEL’) at the beginning of a document, as suggested by the WebAnno developers (<ext-link ext-link-type="uri" xlink:href="https://github.com/webanno/webanno/issues/923">https://github.com/webanno/webanno/issues/923</ext-link>). Using this workaround, tools designed for the annotation of semantic entities and relations can also be used for document-level annotations, but this makes the annotation process more complex and time consuming.</p>
      <p>In summary, most tools are far from being suitable for a multi-class, multi-label annotation project. Further, if the use of ontologies is needed, finding an appropriate tool is a challenge given that large terminologies are supported by few tools. Therefore, there is much room for improvement of tools with such features, or even the development of a tool specifically for this purpose.</p>
    </sec>
    <sec id="sec4g">
      <title>Web applications versus stand-alone tools and plug-ins</title>
      <p>A total of 18 tools were not selected only based on the requirement that the annotation tools should be a Web application. Figure <xref ref-type="fig" rid="f1">1</xref> illustrates the prevalence of the Web applications over the non-Web-based ones well. More specifically, since 2015, only two stand-alone tools have been released, namely, YEDDA [<xref rid="ref77" ref-type="bibr">77</xref>] and SLATE [<xref rid="ref69" ref-type="bibr">69</xref>]. The developers of YEDDA highlight the advantages of their tool over the Web applications, e.g. being able to be installed in Windows operating systems. However, this only applies in some particular situations when only Windows computers are available. Further, the developer of SLATE clearly stated that his goal was to build a terminal-based annotation tool that was light, easy-to-install and keyboard-based (private conversation during poster session). However, such a tool is not suitable for annotators who are not familiar with terminals, including many biomedical researchers.</p>
      <p>In spite of that, many researchers continue to use annotation tools that are not a Web application (cf. Figure <xref ref-type="fig" rid="f1">1</xref>). Our analysis showed that the most popular stand-alone (or plug-in) annotation tools are Knowtator, MMAX2 and UAM Corpus. On one hand, Knowtator, which is a plug-in for the Protégé tool, is still popular in the biomedical domain given its good support for ontologies, and was recently used for an extension of the CRAFT corpus [<xref rid="ref98" ref-type="bibr">98</xref>], as well as in a couple of clinical corpora [<xref rid="ref99" ref-type="bibr">99</xref>, <xref rid="ref100" ref-type="bibr">100</xref>]. On the other hand, MMAX2 was usually used for the annotation of linguistic elements, especially for coreferential and anaphorical relations [<xref rid="ref101" ref-type="bibr">101</xref>, <xref rid="ref102" ref-type="bibr">102</xref>]. Finally, the UAM Corpus tool is usually used for multimodal corpora, given its support for text, images and videos, e.g. in [<xref rid="ref103" ref-type="bibr">103</xref>].</p>
    </sec>
    <sec id="sec4h">
      <title>Limitations of survey</title>
      <p>Even though we surveyed 78 tools, making this the largest analysis of annotation tools to the best knowledge of the authors, our work has some limitations.</p>
      <p>Our search of annotation tools included a variety of resources and publications and lasted for many months. We included all tools that we found during this time. However, we are aware that we might have missed some tools, given their seemingly large number. Further, we did not explore tools (apps) developed for mobile devices (if any is available), which can indeed be a desirable feature for annotation while commuting, for instance.</p>
      <p>We had to specify some requirements to limit the number of tools with which we could carry out a detailed manual evaluation. Even though we set only five requirements, these resulted in the elimination of 63 tools. However, while some of the requirements might be disputable for some researchers (e.g. Web application, schematic), some of them are mandatory for hands-on experiments (e.g. available, installable, workable).</p>
      <p>We defined 26 criteria for the evaluation of the selected tools. While this is an adequate number of features to evaluate the 15 selected tools, we probably missed some other interesting criteria. For instance, some researchers might be interested in features that are specific for domains other than biomedicine or for linguistic tasks (e.g. semantic role labeling or grammatical parsing). We had to dismiss some criteria that, even though important, would require an extremely large amount of time. For instance, we could have evaluated in which browser (e.g. Firefox, Safari or Chrome) or operating system (e.g. Mac OS, Windows or Linux) the tools worked well.</p>
      <p>We considered all criteria as being equally important. However, some criteria are certainly more important than others, e.g. support for annotation of relationships (F2) over the year of the publication (P1). Ranking the criteria and assigning higher weights to the more important ones would affect the score (and maybe the rank) of an annotation tool, depending on whether more or less important criteria were fulfilled. Further, we did not consider the inter-dependence among criteria, as it is the case of the publication criteria.</p>
      <p>We certainly missed some citations to corpus construction. When searching for corpora that used a particular tool for annotation purposes, we dismissed publications that were not freely available to us (i.e. publications behind a paywall), as we could not check their full text. However, we believe that the main goal of this criterion is to provide an estimate of whether the tool has been (successfully) previously used for corpus construction, rather than a precise number of corpora.</p>
      <p>We admit that some of our criteria are rather subjective and that other researchers might have a different opinion than ours for many of them, for example, regarding the quality of the documentation or the easiness of installation. The latter is indeed very dependent on the technical skills of the authors. Nevertheless, we hope that our three-level scale helps users to identify adequate tools for their needs.</p>
      <p>Finally, manually reviewing many annotation tools is an exhausting manual process for which few tools are currently available to automatize this task. We did not develop any tool to tackle this problem, but, theoretically, TM is indeed suitable for this task, and is already being used to support construction of systematic reviews [<xref rid="ref104" ref-type="bibr">104</xref>]. In this new version of the survey, we provide a list of all tools with links to their publications and software, whenever available. We hope that this resource could be the starting point for the development of tools aiming at producing semi-automatic (or even live) surveys.</p>
    </sec>
  </sec>
  <sec id="sec5">
    <title>Conclusion</title>
    <p>We presented the most comprehensive survey available of tools for the manual annotation of textual documents. We reviewed 78 tools, which were evaluated under five requirements that resulted in the selection of 15 tools for detailed hands-on experiments. The evaluation of the selected tools involved 26 criteria defined based on a three-level scale and a final score calculated from the obtained points.</p>
    <p>Our results showed that two tools already offer coverage for around 80% of our criteria, while the lowest-scoring and even some unselected tools are still useful for particular curation or annotation tasks. We provided a discussion of the top most frequent and missing features, as well as the suitability of the tools for the biomedical domain and for text classification.</p>
    <p>Our review aims to support both users (researchers and annotators) in finding the most suitable tool for their annotation purposes, as well as for developers to identify weaknesses in their (selected or nonselected) tools. In spite of the high number of already available tools, we believe that this study is also a valuable resource for those planning to develop new annotation tools.</p>
  </sec>
  <sec id="sec10">
    <title> </title>
    <boxed-text id="box01" position="float" orientation="portrait">
      <sec>
        <title>Key points</title>
        <list list-type="bullet">
          <list-item>
            <p>We provided the most recent and comprehensive list of annotation tools for textual documents by reviewing 78 tools.</p>
          </list-item>
          <list-item>
            <p>We selected 15 tools for hands-on experiments and our results showed that, on average, these tools cover around 60% of the 26 criteria that we defined.</p>
          </list-item>
          <list-item>
            <p>The top fulfilled features were, among others, the use of standard data formats, easiness of installation or no requirement for installation, good-quality documentation, text span highlighting and support for data privacy.</p>
          </list-item>
          <list-item>
            <p>The top missing features were, among others, absence of a publication (and consequently, citations), online and source code availability, calculation of an inter-annotation agreement, support for users and teams and integration with PubMed or PMC.</p>
          </list-item>
          <list-item>
            <p>Four selected annotation tools provide out-of-the box functionality for either named-entity recognition for the biomedical domain or document retrieval from PubMed or PMC.</p>
          </list-item>
        </list>
      </sec>
    </boxed-text>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>Supplementary_data_for_the_Survey_annotation_tool_bbz130</label>
      <media xlink:href="supplementary_data_for_the_survey_annotation_tool_bbz130.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgments</title>
    <p>We would like to thank Madeleine Kittner for initial discussion and suggestions for criteria, and Bettina Bert and Barbara Grune for their helpful comments and corrections. We thank prodigy for providing a free online version of their tool for our hands-on experiments. We would also like to thank the four anonymous reviewers for the helpful comments and suggestions.</p>
  </ack>
  <ref-list id="bib1">
    <title>References</title>
    <ref id="ref1">
      <label>1.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Goldberg</surname><given-names>Y</given-names></name></person-group><source>Neural Network Methods for Natural Language Processing. Synthesis Lectures on Human Language Technologies</source>.
<publisher-name>Morgan &amp; Claypool Publishers</publisher-name>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="ref2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Baker</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Silins</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Guo</surname><given-names>Y</given-names></name></person-group><article-title>et al. Automatic semantic classification of scientific literature according to the hallmarks of cancer</article-title>. <source>Bioinformatics</source>  <year>2016</year>; <volume>32</volume>(<issue>3</issue>): <fpage>432</fpage>–<lpage>40</lpage>.<pub-id pub-id-type="pmid">26454282</pub-id></mixed-citation>
    </ref>
    <ref id="ref3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Habibi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Weber</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Neves</surname><given-names>M</given-names></name></person-group><article-title>et al. Deep learning with word embeddings improves biomedical named entity recognition</article-title>. <source>Bioinformatics</source>  <year>2017</year>; <volume>33</volume>(<issue>14</issue>): <fpage>i37</fpage>–<lpage>48</lpage>.<pub-id pub-id-type="pmid">28881963</pub-id></mixed-citation>
    </ref>
    <ref id="ref4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Choi</surname><given-names>Y</given-names></name></person-group><article-title>et al. Deep learning of mutation-gene-drug relations from the literature</article-title>. <source>BMC Bioinform</source>  <year>2018</year>; <volume>19</volume>(<issue>1</issue>): <fpage>21</fpage>.</mixed-citation>
    </ref>
    <ref id="ref5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liakata</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Teufel</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Siddharthan</surname><given-names>A</given-names></name>, et al.</person-group>  <article-title>Corpora for the conceptualisation and zoning of scientific papers. In: Calzolari N (Conference Chair), Choukri K, Maegaard B, Mariani J, Odijk J, Piperidis S, Rosner M and Tapias D (eds). Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC'10), Valletta, Malta, May 2010</article-title>. <source>European Language Resources Association (ELRA)</source>.</mixed-citation>
    </ref>
    <ref id="ref6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>John Wilbur</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Rzhetsky</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Shatkay</surname><given-names>H</given-names></name></person-group><article-title>New directions in biomedical text annotation: definitions</article-title>, <source>guidelines and corpus construction. BMC Bioinform</source>  <year>2006</year>; <volume>7</volume>(<issue>1</issue>): <fpage>356</fpage>.</mixed-citation>
    </ref>
    <ref id="ref7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bada</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Eckert</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Evans</surname><given-names>D</given-names></name>, et al.</person-group>  <article-title>Concept annotation in the craft corpus</article-title>. <source>BMC Bioinform</source>  <year>2012</year>; <volume>13</volume>(<issue>1</issue>): <fpage>161</fpage>.</mixed-citation>
    </ref>
    <ref id="ref8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Herrero-Zazo</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Segura-Bedmar</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Martínez</surname><given-names>P</given-names></name>, et al.</person-group>  <article-title>The ddi corpus: an annotated corpus with pharmacological substances and drug-drug interactions</article-title>. <source>J Biomed Inform</source>  <year>2013</year>; <volume>46</volume>(<issue>5</issue>): <fpage>914</fpage>–<lpage>20</lpage>.<pub-id pub-id-type="pmid">23906817</pub-id></mixed-citation>
    </ref>
    <ref id="ref9">
      <label>9.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Pustejovsky</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Stubbs</surname><given-names>A</given-names></name></person-group><source>Natural Language Annotation for Machine Learning—A Guide to Corpus-Building for Applications</source>.
<publisher-name>O’Reilly</publisher-name>, <year>2012</year>.</mixed-citation>
    </ref>
    <ref id="ref10">
      <label>10.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Fort</surname><given-names>K</given-names></name></person-group><article-title>Collaborative annotation for reliable natural language processing</article-title>. In: <source>Technical and Sociological Aspects</source>, <source>1st edn</source>.
<publisher-name>Wiley-IEEE Press</publisher-name>, <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="ref11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Neves</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Leser</surname><given-names>U</given-names></name></person-group><article-title>A survey on annotation tools for the biomedical literature</article-title>. <source>Brief Bioinform</source>  <year>2014</year>; <volume>15</volume>(<issue>2</issue>): <fpage>327</fpage>–<lpage>40</lpage>.<pub-id pub-id-type="pmid">23255168</pub-id></mixed-citation>
    </ref>
    <ref id="ref12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wallace</surname><given-names>BC</given-names></name>, <name name-style="western"><surname>Choe</surname><given-names>DK</given-names></name>, <name name-style="western"><surname>Kertz</surname><given-names>L</given-names></name>, et al</person-group><article-title>Humans require context to infer ironic intent (so computers probably do, too). In: ACL (2). The Association for Computer Linguistics</article-title>  <year>2014</year>, 512–16.</mixed-citation>
    </ref>
    <ref id="ref13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Krallinger</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Rabal</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Leitner</surname><given-names>F</given-names></name>, et al.</person-group>  <article-title>The chemdner corpus of chemicals and drugs and its annotation principles</article-title>. <source>J Cheminform</source>  <year>2015</year>; <volume>7</volume>(<issue>1</issue>): <comment>S2</comment>.</mixed-citation>
    </ref>
    <ref id="ref14">
      <label>14.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Fort</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Guillaume</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Chastant</surname><given-names>H</given-names></name></person-group><article-title>Creating Zombilingo, a game with a purpose for dependency syntax annotation</article-title>. In: <source>Proceedings of the First International Workshop on Gamification for Information Retrieval, GamifIR@ECIR’14, Amsterdam, The Netherlands, April 13, 2014, 2–6</source>, <year>2014</year>.</mixed-citation>
    </ref>
    <ref id="ref15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Poesio</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Chamberlain</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Kruschwitz</surname><given-names>U</given-names></name>, et al.</person-group>  <article-title>Phrase detectives: utilizing collective intelligence for internet-scale language resource creation</article-title>. <source>ACM Trans Interact Intell Syst</source>  <year>2013</year>; <volume>3</volume>(<issue>1</issue>): <fpage>3:1</fpage>–<lpage>3:44</lpage>.</mixed-citation>
    </ref>
    <ref id="ref16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fort</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Adda</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Cohen</surname><given-names>KB</given-names></name></person-group><article-title>Amazon mechanical turk: gold mine or coal mine?</article-title>  <source>Comput Linguist</source>  <year>2011</year>; <volume>37</volume>(<issue>2</issue>): <fpage>413</fpage>–<lpage>20</lpage>.</mixed-citation>
    </ref>
    <ref id="ref17">
      <label>17.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Adda</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Sagot</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Fort</surname><given-names>K</given-names></name>, and <name name-style="western"><surname>Mariani</surname><given-names>J</given-names></name></person-group> Crowdsourcing for Language Resource Development: Critical Analysis of Amazon Mechanical Turk Overpowering Use. In: 5th Language and Technology Conference,, Poznan, Poland,<year>2011</year>.</mixed-citation>
    </ref>
    <ref id="ref18">
      <label>18.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Müller</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Strube</surname><given-names>M</given-names></name></person-group><article-title>Multi-level annotation of linguistic data with MMAX2</article-title>. In: <source>Corpus Technology and Language Pedagogy: New Resources</source>. <publisher-loc>New Methods</publisher-loc>:
<publisher-name>New Tools</publisher-name>, <fpage>2006</fpage>.</mixed-citation>
    </ref>
    <ref id="ref19">
      <label>19.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Ogren</surname><given-names>PV</given-names></name></person-group> Knowtator: A Protégé plug-in for annotated corpus construction. In: Proceedings of the 2006 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: Companion Volume: Demonstrations, NAACL-Demonstrations’06, 273–75, Stroudsburg, PA, USA, <year>2006</year>. Association for Computational Linguistics.</mixed-citation>
    </ref>
    <ref id="ref20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>J-D</given-names></name>, <name name-style="western"><surname>Ohta</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Tateisi</surname><given-names>Y</given-names></name>, et al.</person-group>  <article-title>Genia corpus—a semantically annotated corpus for bio-textmining</article-title>. <source>Bioinformatics</source>  <year>2003</year>; <volume>19</volume>(<issue>suppl_1</issue>): <fpage>i180</fpage>–<lpage>2</lpage>.<pub-id pub-id-type="pmid">12855455</pub-id></mixed-citation>
    </ref>
    <ref id="ref21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Maria</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Claire</surname><given-names>Q</given-names></name>, and <name name-style="western"><surname>Soldatova</surname><given-names>LN</given-names></name></person-group><article-title>Semantic annotation of papers: interface &amp; enrichment tool (SAPIENT)</article-title>. In: <source>Proceedings of the BioNLP 2009 Workshop</source>, <fpage>193</fpage>–<lpage>200</lpage>, <comment>Boulder, Colorado</comment>, <year>2009</year>. Association for Computational Linguistics.</mixed-citation>
    </ref>
    <ref id="ref22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wei</surname><given-names>C-H</given-names></name>, <name name-style="western"><surname>Kao</surname><given-names>H-Y</given-names></name>, <name name-style="western"><surname>Lu</surname><given-names>Z</given-names></name></person-group><article-title>PubTator: a web-based text mining tool for assisting biocuration</article-title>. <source>Nucleic Acids Res</source>  <year>2013</year>; <volume>41</volume>(<issue>W1</issue>): <fpage>W518</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">23703206</pub-id></mixed-citation>
    </ref>
    <ref id="ref23">
      <label>23.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Meyer</surname><given-names>CM</given-names></name>, <name name-style="western"><surname>Benikova</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Mieskes</surname><given-names>M</given-names></name>, et al</person-group><article-title>MDSWriter: annotation tool for creating high-quality multi-document summarization corpora</article-title>. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL): System Demonstrations, <fpage>97</fpage>–<lpage>102</lpage>, <publisher-loc>Berlin, Germany</publisher-loc>, <year>2016</year><person-group person-group-type="editor"><collab>Association for Computational Linguistics</collab></person-group>.</mixed-citation>
    </ref>
    <ref id="ref24">
      <label>24.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Neves</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Damaschun</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Kurtz</surname><given-names>A</given-names></name>, and <name name-style="western"><surname>Leser</surname><given-names>U</given-names></name></person-group><article-title>Annotating and evaluating text for stem cell research. In: Third Workshop on Building and Evaluation Resources for Biomedical Text Mining (BioTxtM 2012) at Language Resources and Evaluation (LREC) 2012 (to appear)</article-title>, <publisher-loc>Istanbul, Turkey</publisher-loc>, <year>2012</year><person-group person-group-type="editor"><collab>European Language Resources Association (ELRA)</collab></person-group>.</mixed-citation>
    </ref>
    <ref id="ref25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pham</surname><given-names>A-D</given-names></name>, <name name-style="western"><surname>Névéol</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Lavergne</surname><given-names>T</given-names></name>, et al.</person-group>  <article-title>Natural language processing of radiology reports for the detection of thromboembolic diseases and clinically relevant incidental findings</article-title>. <source>BMC Bioinform</source>  <year>2014</year>; <volume>15</volume>(<issue>1</issue>): <fpage>266</fpage>.</mixed-citation>
    </ref>
    <ref id="ref26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Maeda</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Strassel</surname><given-names>S</given-names></name></person-group><article-title>Annotation tools for large-scale corpus development: using AGTK at the linguistic data consortium. In: Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC'04), Lisbon, Portugal, 2004</article-title>. <source>European Language Resources Association (ELRA)</source>.</mixed-citation>
    </ref>
    <ref id="ref27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Maeda</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Medero</surname><given-names>J</given-names></name>, et al.</person-group>  <article-title>A new phase in annotation tool development at the linguistic data consortium: the evolution of the annotation graph toolkit. In: Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC'06), Genoa, Italy, 2006</article-title>. <source>European Language Resources Association (ELRA)</source>.</mixed-citation>
    </ref>
    <ref id="ref28">
      <label>28.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Papazian</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Bossy</surname><given-names>R</given-names></name>, and <name name-style="western"><surname>Nédellec</surname><given-names>C</given-names></name></person-group><article-title>AlvisAE: a collaborative web text annotation editor for knowledge acquisition</article-title>. In: <source>Proceedings of the Sixth Linguistic Annotation Workshop</source>, <comment>LAW VI’12</comment>, <fpage>149</fpage>–<lpage>52</lpage>, <publisher-loc>Stroudsburg, PA, USA</publisher-loc>, <year>2012</year><comment>Association for Computational Linguistics</comment>.</mixed-citation>
    </ref>
    <ref id="ref29">
      <label>29.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>W-T</given-names></name> and <name name-style="western"><surname>Styler</surname><given-names>W</given-names></name></person-group> Anafora: a web-based general purpose annotation tool. In: Proceedings of the 2013 NAACL HLT Demonstration Session, 14–19, Atlanta, Georgia, <year>2013</year>
<person-group person-group-type="editor"><collab>Association for Computational Linguistics</collab></person-group>.</mixed-citation>
    </ref>
    <ref id="ref30">
      <label>30.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Landragin</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Poibeau</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Victorri</surname><given-names>B</given-names></name></person-group><article-title>ANALEC: a new tool for the dynamic annotation of textual data</article-title>. In: <source>European Language Resources Association (ELRA), editor, International Conference on Language Resources and Evaluation (LREC 2012)</source>. <publisher-loc>Istanbul</publisher-loc>:
<publisher-name>Turkey</publisher-name>, <year>2012</year>, <fpage>357</fpage>–<lpage>62</lpage>.</mixed-citation>
    </ref>
    <ref id="ref31">
      <label>31.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Przepiórkowski</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Murzynowski</surname><given-names>G</given-names></name></person-group><article-title>Manual annotation of the National Corpus of Polish with Anotatornia</article-title>. In: <person-group person-group-type="editor"><name name-style="western"><surname>Goźdź-Roszkowski</surname><given-names>S</given-names></name></person-group> (ed). <source>The Proceedings of Practical Applications in Language and Computers PALC 2009</source>. <publisher-loc>Frankfurt am Main</publisher-loc>:
<publisher-name>Peter Lang. Forthcoming</publisher-name>, <year>2009</year>.</mixed-citation>
    </ref>
    <ref id="ref32">
      <label>32.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Nghiem</surname><given-names>M-Q</given-names></name>, <name name-style="western"><surname>Ananiadou</surname><given-names>S</given-names></name></person-group><article-title>APLenty: annotation tool for creating high-quality datasets using active and proactive learning</article-title>. In: <source>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 108–13. Association for Computational Linguistics</source>, <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="ref33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Louren</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Carreira</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Carneiro</surname><given-names>S</given-names></name></person-group><article-title>et al. @note: a workbench for biomedical text mining</article-title>. <source>J Biomed Inform</source>  <year>2009</year>; <volume>42</volume>(<issue>4</issue>): <fpage>710</fpage>–<lpage>20</lpage>.<pub-id pub-id-type="pmid">19393341</pub-id></mixed-citation>
    </ref>
    <ref id="ref34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rak</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Rowley</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Black</surname><given-names>W</given-names></name>, et al.</person-group>  <article-title>Argo: an integrative, interactive, text mining-based workbench supporting curation</article-title>. <source>Database</source>, <year>2012</year>.</mixed-citation>
    </ref>
    <ref id="ref35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Druskat</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Bierkandt</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Gast</surname><given-names>V</given-names></name>, et al</person-group><article-title>Atomic: an open-source software platform for multi-layer corpus annotation</article-title>. In: <source>Proceedings of the 12th Konferenz zur Verarbeitung Natürlicher Sprache (KONVENS 2014)</source>, <comment>Hildesheim,</comment><year>2014</year>, <fpage>228</fpage>–<lpage>34</lpage>. In <person-group person-group-type="editor"><name name-style="western"><surname>Ruppert</surname><given-names>Josef</given-names></name> and <name name-style="western"><surname>Faaß</surname><given-names>Gertrud</given-names></name></person-group> (eds), <year>2014</year>.</mixed-citation>
    </ref>
    <ref id="ref36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>López-Fernández</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Reboiro-Jato</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Glez-Peña</surname><given-names>D</given-names></name>, et al.</person-group>  <article-title>BioAnnote: a software platform for annotating biomedical documents with application in medical learning environments</article-title>. <source>Comput Methods Programs Biomed</source>  <year>2013</year>; <volume>111</volume>(<issue>1</issue>): <fpage>139</fpage>–<lpage>47</lpage>.<pub-id pub-id-type="pmid">23562645</pub-id></mixed-citation>
    </ref>
    <ref id="ref37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cano</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Monaghan</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Blanco</surname><given-names>A</given-names></name>, et al.</person-group>  <article-title>Collaborative text-annotation resource for disease-centered relation extraction from biomedical text</article-title>. <source>J Biomed Inform</source>  <year>2009</year>; <volume>42</volume>(<issue>5</issue>): <fpage>967</fpage>–<lpage>77</lpage>. <comment>Biomedical Natural Language Processing</comment>.<pub-id pub-id-type="pmid">19232400</pub-id></mixed-citation>
    </ref>
    <ref id="ref38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Felt</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Merkling</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Carmen</surname><given-names>M</given-names></name>, et al.</person-group>  <article-title>CCASH: a web application framework for efficient, distributed language resource development. In: Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta, 2010</article-title>. <source>European Languages Resources Association (ELRA)</source>.</mixed-citation>
    </ref>
    <ref id="ref39">
      <label>39.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Alphonse</surname><given-names>Erick</given-names></name>, <name name-style="western"><surname>Aubin</surname><given-names>Sophie</given-names></name>, <name name-style="western"><surname>Bessières</surname><given-names>Philippe</given-names></name>, et al</person-group><article-title>Event-based information extraction for the biomedical domain: the caderige project</article-title>. In: <source>COLING 2004 International Joint workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP) 2004</source>, <fpage>43</fpage>–<lpage>9</lpage>, <publisher-loc>Geneva, Switzerland</publisher-loc>, <year>August 28th and 29th 2004</year><comment>COLING</comment>.</mixed-citation>
    </ref>
    <ref id="ref40">
      <label>40.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Day</surname><given-names>D</given-names></name>, <name name-style="western"><surname>McHenry</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Kozierok</surname><given-names>R</given-names></name>, et al.</person-group>  <article-title>Callisto: A Configurable Annotation Workbench</article-title>. <source>Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC'04)</source>. <publisher-name>European Language Resources Association (ELRA)</publisher-name>: <publisher-loc>Lispon, Portugal</publisher-loc>, <year>2004</year>.</mixed-citation>
    </ref>
    <ref id="ref41">
      <label>41.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Katakis</surname><given-names>IM</given-names></name>, <name name-style="western"><surname>Petasis</surname><given-names>G</given-names></name>, and <name name-style="western"><surname>Karkaletsis</surname><given-names>V</given-names></name></person-group><article-title>CLARIN-EL web-based annotation tool</article-title>. In: <person-group person-group-type="editor"><name name-style="western"><surname>Calzolari (Conference Chair)</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Choukri</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Declerck</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Goggi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Grobelnik</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Maegaard</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Mariani</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Mazo</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Moreno</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Odijk</surname><given-names>J</given-names></name> and <name name-style="western"><surname>Piperidis</surname><given-names>S</given-names></name></person-group>, (eds). <source>Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</source>, <publisher-loc>Paris, France</publisher-loc>, <year>2016</year><comment>European Language Resources Association(ELRA)</comment>.</mixed-citation>
    </ref>
    <ref id="ref42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bonet</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Vila</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Rodríguez</surname><given-names>H</given-names></name>, et al.</person-group>  <article-title>Coco, a web interface for corpora compilation</article-title>. <source>Proceedings of the Procesamiento del Lenguaje Natural (PLN)</source>  <year>2009</year>; <volume>43</volume>:<fpage>367</fpage>–<lpage>8</lpage>. <comment>Sociedad Espaõla para el Procesamiento del Lenguaje Natural</comment>.</mixed-citation>
    </ref>
    <ref id="ref43">
      <label>43.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Ciccarese</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Ocana</surname><given-names>M</given-names></name>, and <name name-style="western"><surname>Clark</surname><given-names>T</given-names></name></person-group><article-title>Domeo: a web-based tool for semantic annotation of online documents</article-title>. <ext-link ext-link-type="uri" xlink:href="http://bio-ontologies.knowledgeblog.org/297">http://bio-ontologies.knowledgeblog.org/297</ext-link>, <year>2012</year>.</mixed-citation>
    </ref>
    <ref id="ref44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ciccarese</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Ocana</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Clark</surname><given-names>T</given-names></name></person-group><article-title>Open semantic annotation of scientific publications using domeo</article-title>. <source>J Biomed Semant</source>  <year>2012</year>; <volume>3</volume>(<issue>1</issue>): <fpage>S1</fpage>.</mixed-citation>
    </ref>
    <ref id="ref45">
      <label>45.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>de La Clergerie</surname><given-names>VÉ</given-names></name></person-group><article-title>A collaborative infrastructure for handling syntactic annotations</article-title>. In: <source>Proceedings of The First Workshop on Automated Syntactic Annotations for Interoperable Language Resources</source>,
<publisher-name>Hong-Kong</publisher-name>, <publisher-loc>Hong Kong SAR China</publisher-loc>, <fpage>2008</fpage>.</mixed-citation>
    </ref>
    <ref id="ref46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Campos</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Louren</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Matos</surname><given-names>S</given-names></name>, et al.</person-group>  <article-title>Egas: a collaborative and interactive document curation platform</article-title>. <source>Database</source>  <year>2014</year>; <comment>bau048, 2014.</comment></mixed-citation>
    </ref>
    <ref id="ref47">
      <label>47.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Petasis</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Karkaletsis</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Paliouras</surname><given-names>G</given-names></name>, et al.</person-group>  <article-title>Ellogon: a new text engineering platform</article-title>. In: <source>Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC 2002)</source>, <fpage>72</fpage>–<lpage>8</lpage>,
<publisher-name>Las Palmas</publisher-name>, <publisher-loc>Canary Islands, Spain</publisher-loc>, <year>May 29–31 2002</year>
<comment>European Language Resources Association</comment>.</mixed-citation>
    </ref>
    <ref id="ref48">
      <label>48.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Artola</surname><given-names>X</given-names></name>, <name name-style="western"><surname>de Ilarraza</surname><given-names>AD</given-names></name>, <name name-style="western"><surname>Ezeiza</surname><given-names>N</given-names></name>, et al.</person-group>  <article-title>Eulia: a graphical web interface for creating, browsing and editing linguistically annotated corpora</article-title>. In: <source>LREC 2004. ISBN 2-9517408-1-6</source>, <year>2004</year>.</mixed-citation>
    </ref>
    <ref id="ref49">
      <label>49.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bontcheva</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Cunningham</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Roberts</surname><given-names>I</given-names></name>, et al.</person-group>  <article-title>GATE teamware: a web-based, collaborative text annotation framework</article-title>. <source>Lang Resour Eval</source>  <year>2013</year>; <volume>47</volume>(<issue>4</issue>): <fpage>1007</fpage>–<lpage>29</lpage>.</mixed-citation>
    </ref>
    <ref id="ref50">
      <label>50.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>S</given-names></name> and <name name-style="western"><surname>Zeldes</surname><given-names>A</given-names></name></person-group><source>GitDOX: A linked version controlled online xml editor for manuscript transcription</source>. <comment>In: Florida Artificial Intelligence Research Society Conference</comment>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="ref51">
      <label>51.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Widlöcher</surname><given-names>A</given-names></name> and <name name-style="western"><surname>Mathet</surname><given-names>Y</given-names></name></person-group><article-title>The Glozz platform: a corpus annotation and mining tool</article-title>. In: <source>Proceedings of the 2012 ACM Symposium on Document Engineering</source>, <comment>DocEng’12</comment>, <fpage>171</fpage>–<lpage>80</lpage>, <publisher-loc>New York, NY, USA</publisher-loc>, <year>2012</year><comment>ACM</comment>.</mixed-citation>
    </ref>
    <ref id="ref52">
      <label>52.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Marcinczuk</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Oleksy</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kocon</surname><given-names>J</given-names></name></person-group><article-title>Inforex—a collaborative system for text corpora annotation and analysis</article-title>. In: <comment>R Mitkov R and Angelova G (eds). Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, Varna, Bulgaria, September 2–8, 2017, 473–82. INCOMA Ltd.</comment>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="ref53">
      <label>53.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Marcinczuk</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kocon</surname><given-names>J</given-names></name>, and <name name-style="western"><surname>Broda</surname><given-names>B</given-names></name></person-group><article-title>Inforex—a web-based tool for text corpus management and semantic annotation</article-title>. In: <person-group person-group-type="editor"><name name-style="western"><surname>Calzolari (Conference Chair)</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Choukri</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Declerck</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Doğan</surname><given-names>MU</given-names></name>, <name name-style="western"><surname>Maegaard</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Mariani</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Moreno</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Odijk</surname><given-names>J</given-names></name> and <name name-style="western"><surname>Piperidis</surname><given-names>S</given-names></name></person-group> (eds). <source>Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)</source>, <publisher-loc>Istanbul, Turkey</publisher-loc>, <year>2012</year>
<source>European Language Resources Association (ELRA).</source></mixed-citation>
    </ref>
    <ref id="ref54">
      <label>54.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tesconi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ronzano</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Minutoli</surname><given-names>S</given-names></name>, et al.</person-group>  <article-title>KAFnotator: a multilingual semantic text annotation tool</article-title>. <source>The Second International Conference on Global Interoperability for Language Resources</source>  <year>2010</year>; <volume>01</volume>.</mixed-citation>
    </ref>
    <ref id="ref55">
      <label>55.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Zheng</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>B</given-names></name>, et al</person-group><article-title>KCAT: A knowledge-constraint typing annotation tool</article-title>. In: <source>Proceedings of the 57th Conference of the Association for Computational Linguistics: System Demonstrations</source>, <fpage>99</fpage>–<lpage>104</lpage>, <publisher-loc>Florence, Italy</publisher-loc>, <year>2019</year><comment>Association for Computational Linguistics</comment>.</mixed-citation>
    </ref>
    <ref id="ref56">
      <label>56.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Stubbs</surname><given-names>A</given-names></name></person-group><article-title>MAE and MAI: lightweight annotation and adjudication tools</article-title>. In: <source>Proceedings of the 5th Linguistic Annotation Workshop</source>, <comment>LAW V’11</comment>, <fpage>129</fpage>–<lpage>33</lpage>, <publisher-loc>Stroudsburg, PA, USA</publisher-loc>, <year>2011</year><comment>Association for Computational Linguistics</comment>.</mixed-citation>
    </ref>
    <ref id="ref57">
      <label>57.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pérez-Pérez</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Glez-Peña</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Fdez-Riverola</surname><given-names>F</given-names></name>, et al.</person-group>  <article-title>Marky: a tool supporting annotation consistency in multi-user and iterative document annotation projects</article-title>. <source>Comput Methods Programs Biomed</source>  <year>2015</year>; <volume>118</volume>(<issue>2</issue>): <fpage>242</fpage>–<lpage>51</lpage>.<pub-id pub-id-type="pmid">25480679</pub-id></mixed-citation>
    </ref>
    <ref id="ref58">
      <label>58.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Petasis</surname><given-names>G</given-names></name></person-group><article-title>Annotating arguments: the NOMAD collaborative annotation tool. In: Calzolari N (Conference Chair), Choukri K, Declerck T, Loftsson H, Maegaard B, Mariani J, Moreno A, Odijk J and Piperidis S (eds). Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14), Reykjavik, Iceland, 2014</article-title>. <source>European Language Resources Association (ELRA)</source>.</mixed-citation>
    </ref>
    <ref id="ref59">
      <label>59.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rinaldi</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Clematide</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Marques</surname><given-names>H</given-names></name>, et al</person-group><article-title>OntoGene web services for biomedical text mining</article-title>. <source>BMC Bioinform</source>, <volume>15</volume>(<issue>14</issue>): <comment>S6</comment>, <year>2014</year>.</mixed-citation>
    </ref>
    <ref id="ref60">
      <label>60.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Cunningham</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Tablan</surname><given-names>V.</given-names></name>, <name name-style="western"><surname>Bontcheva</surname><given-names>K.</given-names></name>, et al</person-group><article-title>Language engineering tools for collaborative corpus annotation</article-title>. In: <source>Proceedings of Corpus Linguistics 2003</source>, <fpage>80</fpage>–<lpage>7</lpage>.
<publisher-name>Wiley</publisher-name>, <year>2003</year>.</mixed-citation>
    </ref>
    <ref id="ref61">
      <label>61.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Orăsan</surname><given-names>C</given-names></name></person-group><article-title>PALinkA: a highly customisable tool for discourse annotation</article-title>. <source>In: Proceedings of the Fourth SIGdial Workshop on Discourse and Dialogue</source>  <year>2003</year>;<fpage>39</fpage>–<lpage>43</lpage>.</mixed-citation>
    </ref>
    <ref id="ref62">
      <label>62.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Menard</surname><given-names>PA</given-names></name>, <name name-style="western"><surname>Caroline</surname><given-names>BPACTE</given-names></name></person-group><article-title>A collaborative platform for textual annotation</article-title>. In: <source>Proceedings of the 13th Joint ISO-ACL Workshop on Interoperable Semantic Annotation (ISA-13)</source>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="ref63">
      <label>63.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Khaitan</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Ramakrishnan</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Joshi</surname><given-names>S</given-names></name></person-group><article-title>et al. RAD: a scalable framework for annotator development</article-title>. In: <source>2008 IEEE 24th International Conference on Data Engineering</source>, <year>2008</year>,<fpage>1624</fpage>–<lpage>7</lpage>.</mixed-citation>
    </ref>
    <ref id="ref64">
      <label>64.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Burchardt</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Erk</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Frank</surname><given-names>A</given-names></name>, et al</person-group><article-title>SALTO: a versatile multi-level annotation tool</article-title>. In: <source>Proceedings of LREC-2006</source>, <publisher-loc>Genoa, Italy</publisher-loc>, <year>2006</year>.</mixed-citation>
    </ref>
    <ref id="ref65">
      <label>65.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hartung</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Horst</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Grimm</surname><given-names>F</given-names></name>, et al.</person-group>  <article-title>SANTO: a web-based annotation tool for ontology-driven slot filling</article-title>. <source>Proceedings of ACL 2018, System Demonstrations</source>  <year>2018</year>;<fpage>68</fpage>–<lpage>73</lpage>. <comment>Association for Computational Linguistics</comment>.</mixed-citation>
    </ref>
    <ref id="ref66">
      <label>66.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Samih</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Maier</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Laura Kallmeyer</surname><given-names>SAWT</given-names></name></person-group><article-title>Sequence annotation web tool. In: Proceedings of the Second Workshop on Computational Approaches to Code Switching</article-title>. <source>Association for Computational Linguistics</source>  <year>2016</year>;<fpage>65</fpage>–<lpage>70</lpage>.</mixed-citation>
    </ref>
    <ref id="ref67">
      <label>67.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Chute</surname><given-names>CG</given-names></name>, <name name-style="western"><surname>Tao</surname><given-names>C</given-names></name></person-group><article-title>Semantator: annotating clinical narratives with semantic web ontologies</article-title>. <source>AMIA Jt Summits Transl Sci Proc</source>  <year>2012</year>; <volume>2012</volume>:<fpage>20</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">22779043</pub-id></mixed-citation>
    </ref>
    <ref id="ref68">
      <label>68.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Stührenberg</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Goecke</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Diewald</surname><given-names>N</given-names></name>, et al</person-group><article-title>Web-based annotation of anaphoric relations and lexical chains</article-title>. <source>In: Proceedings of the Linguistic Annotation Workshop</source>, <comment>LAW’07</comment>, <fpage>140</fpage>–<lpage>47</lpage>, <publisher-loc>Stroudsburg, PA, USA</publisher-loc>, <year>2007</year><comment>Association for Computational Linguistics</comment>.</mixed-citation>
    </ref>
    <ref id="ref69">
      <label>69.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Jonathan</surname><given-names>K.</given-names></name></person-group> Kummerfeld. <article-title>SLATE: a super-lightweight annotation tool for experts</article-title>. <source>In: Proceedings of the 57th Conference of the Association for Computational Linguistics: System Demonstrations</source>, <fpage>7</fpage>–<lpage>12</lpage>, <publisher-loc>Florence, Italy</publisher-loc>, <year>2019</year>
<comment>Association for Computational Linguistics</comment>.</mixed-citation>
    </ref>
    <ref id="ref70">
      <label>70.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Petasis</surname><given-names>G</given-names></name></person-group><article-title>The SYNC3 collaborative annotation tool</article-title>. In: <source>Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012)</source>, <fpage>363</fpage>–<lpage>70</lpage>, <publisher-loc>Istanbul, Turkey</publisher-loc>, <year>2012</year><comment>European Language Resources Association (ELRA)</comment>.</mixed-citation>
    </ref>
    <ref id="ref71">
      <label>71.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Müller</surname><given-names>H-M</given-names></name>, <name name-style="western"><surname>Kenny</surname><given-names>EE</given-names></name>, <name name-style="western"><surname>Sternberg</surname><given-names>PW</given-names></name></person-group><article-title>Textpresso: an ontology-based information retrieval and extraction system for biological literature</article-title>. <source>PLOS Biol</source>  <year>2004</year>; <volume>2</volume>(<issue>11</issue>): <fpage>09</fpage>.</mixed-citation>
    </ref>
    <ref id="ref72">
      <label>72.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>O’Donnell</surname><given-names>M</given-names></name></person-group><article-title>Demonstration of the uam corpustool for text and image annotation</article-title>. In: <source>Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Demo Session</source>, <comment>HLT-Demonstrations’08</comment>, <fpage>13</fpage>–<lpage>6</lpage>, <publisher-loc>Stroudsburg, PA, USA</publisher-loc>, <year>2008</year><comment>Association for Computational Linguistics</comment>.</mixed-citation>
    </ref>
    <ref id="ref73">
      <label>73.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Kovatchev</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Marti</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Salamo</surname><given-names>M</given-names></name></person-group><article-title>WARP-Text: a web-based tool for annotating relationships between pairs of texts</article-title>. In: <source>Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations, 132–36. Association for Computational Linguistics</source>, <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="ref74">
      <label>74.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>AlGhamdi</surname><given-names>F</given-names></name> and <name name-style="western"><surname>Diab</surname><given-names>M</given-names></name></person-group><article-title>WASA: a web application for sequence annotation</article-title>. In: <person-group person-group-type="editor"><name name-style="western"><surname>Calzolari (Conference chair)</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Choukri</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Cieri</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Declerck</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Goggi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Hasida</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Isahara</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Maegaard</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Mariani</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Mazo</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Moreno</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Odijk</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Piperidis</surname><given-names>S</given-names></name> and <name name-style="western"><surname>Tokunaga</surname><given-names>T</given-names></name></person-group> (eds). <source>Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</source>, <publisher-loc>Miyazaki, Japan</publisher-loc>, <year>May 7–12, 2018</year>
<comment>European Language Resources Association (ELRA)</comment>.</mixed-citation>
    </ref>
    <ref id="ref75">
      <label>75.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tannier</surname><given-names>X</given-names></name></person-group><article-title>WebAnnotator, an annotation tool for web pages. In: Calzolari N (Conference Chair), Choukri K, Declerck T, Doğan MU, Maegaard B, Mariani J, Moreno A, Odijk J and Piperidis S (eds) Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12), Istanbul, Turkey, 2012</article-title>. <source>European Language Resources Association (ELRA)</source>.</mixed-citation>
    </ref>
    <ref id="ref76">
      <label>76.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Morton</surname><given-names>TS</given-names></name>, <name name-style="western"><surname>LaCivita</surname><given-names>J</given-names></name></person-group><article-title>Wordfreak: an open tool for linguistic annotation</article-title>. <source>In: HLT-NAACL</source>  <year>2003</year>.</mixed-citation>
    </ref>
    <ref id="ref77">
      <label>77.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Yue</surname><given-names>ZYEDDA</given-names></name></person-group><article-title>A lightweight collaborative text span annotation tool. arXiv preprint arXiv</article-title>. <source>Proceedings of ACL 2018, System Demonstrations</source>. <publisher-name>Association for Computational Linguistics</publisher-name>: <publisher-loc>Melbourne, Australia</publisher-loc>, <year>2017</year>; <volume>1711</volume>: <fpage>31</fpage>–<lpage>36</lpage>.</mixed-citation>
    </ref>
    <ref id="ref78">
      <label>78.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kwon</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Shin</surname><given-names>S-Y</given-names></name>, et al.</person-group>  <article-title>Assisting manual literature curation for protein-protein interactions using BioQRator</article-title>. <source>Database</source>  <year>2014</year>; <comment>bau067, 2014</comment>.</mixed-citation>
    </ref>
    <ref id="ref79">
      <label>79.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Stenetorp</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Pyysalo</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Topić</surname><given-names>G</given-names></name>, et al</person-group><article-title>brat: a web-based tool for nlp-assisted text annotation</article-title>. <source>In: Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics</source>, <comment>EACL’12</comment>, <fpage>102</fpage>–<lpage>7</lpage>, <publisher-loc>Stroudsburg, PA, USA</publisher-loc>, <year>2012</year><comment>Association for Computational Linguistics.</comment></mixed-citation>
    </ref>
    <ref id="ref80">
      <label>80.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pontiki</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Galanis</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Papageorgiou</surname><given-names>H</given-names></name>, et al.</person-group>  <article-title>Semeval-2016 task 5: aspect based sentiment analysis</article-title>. <source>In: Proceedings of the 10th International Workshop on Semantic Evaluation SemEval-2016</source>, <year>2016</year>;<fpage>19</fpage>–<lpage>30</lpage>. <comment>Association for Computational Linguistics</comment>.</mixed-citation>
    </ref>
    <ref id="ref81">
      <label>81.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Apostolova</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Neilan</surname><given-names>S</given-names></name>, <name name-style="western"><surname>An</surname><given-names>G</given-names></name>, et al</person-group><article-title>Djangology: a light-weight web-based tool for distributed collaborative text annotation</article-title>. In: <person-group person-group-type="editor"><name name-style="western"><surname>Calzolari (Conference Chair)</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Choukri</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Maegaard</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Mariani</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Odijk</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Piperidis</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Rosner</surname><given-names>M</given-names></name> and <name name-style="western"><surname>Tapias</surname><given-names>D</given-names></name></person-group> (eds). <source>Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10)</source>, <publisher-loc>Valletta, Malta</publisher-loc>, <year>2010</year>
<comment>European Language Resources Association (ELRA)</comment>.</mixed-citation>
    </ref>
    <ref id="ref82">
      <label>82.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kwon</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Wei</surname><given-names>C-H</given-names></name>, et al.</person-group>  <article-title>eztag: tagging biomedical concepts via interactive learning</article-title>. <source>Nucleic Acids Res</source>  <year>2018</year>; <volume>46</volume>(<issue>W1</issue>): <fpage>W523</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">29788413</pub-id></mixed-citation>
    </ref>
    <ref id="ref83">
      <label>83.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>van
Gompel</surname><given-names>M</given-names></name> and <name name-style="western"><surname>Martin</surname><given-names>R</given-names></name></person-group><article-title>FoLiA: a practical xml format for linguistic annotation—a descriptive and comparative study</article-title>. <source>Comput Linguist Netherlands J</source>, <volume>3</volume>:<fpage>63</fpage>–<lpage>81</lpage>, <volume>12</volume>  <issue>2013</issue>  <year>2013</year>.</mixed-citation>
    </ref>
    <ref id="ref84">
      <label>84.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Salgado</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Krallinger</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Depaule</surname><given-names>M</given-names></name></person-group><article-title>et al. MyMiner: a web application for computer-assisted biocuration and text annotation</article-title>. <source>Bioinformatics</source>  <year>2012</year>; <volume>28</volume>(<issue>17</issue>): <fpage>2285</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">22789588</pub-id></mixed-citation>
    </ref>
    <ref id="ref85">
      <label>85.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Shindo</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Munesada</surname><given-names>Y</given-names></name>, and <name name-style="western"><surname>Matsumoto</surname><given-names>Y</given-names></name></person-group><article-title>PDFAnno: a web-based linguistic annotation tool for pdf documents</article-title>. In: <person-group person-group-type="editor"><name name-style="western"><surname>Calzolari (Conference chair)</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Choukri</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Cieri</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Declerck</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Goggi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Hasida</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Isahara</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Maegaard</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Mariani</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Mazo</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Moreno</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Odijk</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Piperidis</surname><given-names>S</given-names></name> and <name name-style="western"><surname>Tokunaga</surname><given-names>T</given-names></name></person-group> (eds). <source>Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</source>, <publisher-loc>Paris, France</publisher-loc>, <year>2018</year>
<comment>European Language Resources Association (ELRA)</comment>.</mixed-citation>
    </ref>
    <ref id="ref86">
      <label>86.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cejuela</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>McQuilton</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Ponting</surname><given-names>L</given-names></name>, et al.</person-group>  <article-title>tagtog: interactive and text-mining-assisted annotation of gene mentions in plos full-text articles</article-title>. <source>Database</source>, <comment>2014:bau033</comment><year>2014</year>.</mixed-citation>
    </ref>
    <ref id="ref87">
      <label>87.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>J-D</given-names></name> and <name name-style="western"><surname>Wang</surname><given-names>Y</given-names></name></person-group> PubAnnotation: a persistent and sharable corpus and annotation repository. In: Proceedings of the 2012 Workshop on Biomedical Natural Language Processing, BioNLP’12, 202–05, Stroudsburg, PA, USA, <year>2012</year>
<comment>Association for Computational Linguistics</comment>.</mixed-citation>
    </ref>
    <ref id="ref88">
      <label>88.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Kiesel</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Wachsmuth</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Al Khatib</surname><given-names>K</given-names></name>, et al</person-group> WAT-SL: a customizable web annotation tool for segment labeling. In: Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017, Valencia, Spain, April 3–7, 2017, Software Demonstrations, <fpage>13</fpage>–<lpage>6</lpage>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="ref89">
      <label>89.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>de Castilho</surname><given-names>RE</given-names></name>, <name name-style="western"><surname>Mújdricza-Maydt</surname><given-names>É</given-names></name>, <name name-style="western"><surname>Yimam</surname><given-names>SM</given-names></name>, et al</person-group> A web-based tool for the integrated annotation of semantic and syntactic structures. In: Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities (LT4DH), 76–84. The COLING 2016 Organizing Committee, <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="ref90">
      <label>90.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Yimam</surname><given-names>SM</given-names></name>, <name name-style="western"><surname>Gurevych</surname><given-names>I</given-names></name>, <name name-style="western"><surname>de Castilho</surname><given-names>RE</given-names></name>, et al</person-group> WebAnno: a flexible, web-based and visually supported system for distributed annotations. In: Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, <fpage>1</fpage>–<lpage>6</lpage>, <publisher-loc>Sofia, Bulgaria</publisher-loc>, <year>2013</year>
<comment>Association for Computational Linguistics</comment>.</mixed-citation>
    </ref>
    <ref id="ref91">
      <label>91.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Yimam</surname><given-names>SM</given-names></name>, <name name-style="western"><surname>Remus</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Panchenko</surname><given-names>A</given-names></name>, et al</person-group> Entity-centric information access with human in the loop for the biomedical domain. In: Proceedings of the Biomedical NLP Workshop associated with RANLP 2017, 42–8. INCOMA Ltd., <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="ref92">
      <label>92.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Comeau</surname><given-names>DC</given-names></name>, <name name-style="western"><surname>Doğan</surname><given-names>RI</given-names></name>, <name name-style="western"><surname>Ciccarese</surname><given-names>P</given-names></name>, et al.</person-group>  <article-title>BioC: a minimalist approach to interoperability for biomedical text processing</article-title>. <source>Database</source>  <year>2013</year>; <comment>bat064, 2013.</comment></mixed-citation>
    </ref>
    <ref id="ref93">
      <label>93.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Settles</surname><given-names>B</given-names></name></person-group><article-title>ABNER: an open source tool for automatically tagging genes, proteins and other entity names in text</article-title>. <source>Bioinformatics</source>  <year>2005</year>; <volume>21</volume>(<issue>14</issue>): <fpage>3191</fpage>–<lpage>2</lpage>.<pub-id pub-id-type="pmid">15860559</pub-id></mixed-citation>
    </ref>
    <ref id="ref94">
      <label>94.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pyysalo</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Ohta</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Miwa</surname><given-names>M</given-names></name></person-group><article-title>et al. Event extraction across multiple levels of biological organization</article-title>. <source>Bioinformatics</source>  <year>2012</year>; <volume>28</volume>(<issue>18</issue>): <fpage>i575</fpage>–<lpage>81</lpage>.<pub-id pub-id-type="pmid">22962484</pub-id></mixed-citation>
    </ref>
    <ref id="ref95">
      <label>95.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Verspoor</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Yepes</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Cavedon</surname><given-names>L</given-names></name>, et al</person-group><article-title>Annotating the biomedical literature for the human variome</article-title>. <source>Database</source>, <year>2013</year>:bat019, 2013.</mixed-citation>
    </ref>
    <ref id="ref96">
      <label>96.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>J-D</given-names></name> and <name name-style="western"><surname>Tsujii</surname><given-names>J</given-names></name></person-group><comment>Overview of BioNLP 2011 protein coreference shared task. In: Proceedings of BioNLP Shared Task 2011 Workshop</comment>, <fpage>74</fpage>–<lpage>82</lpage>, <publisher-loc>Portland, Oregon, USA</publisher-loc>, <year>2011</year><comment>Association for Computational Linguistics</comment>.</mixed-citation>
    </ref>
    <ref id="ref97">
      <label>97.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Neves</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Damaschun</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Mah</surname><given-names>N</given-names></name>, et al</person-group><article-title>Preliminary evaluation of the cellfinder literature curation pipeline for gene expression in kidney cells and anatomical parts</article-title>. <source>Database</source>, <year>2013</year>:bat020, 2013.</mixed-citation>
    </ref>
    <ref id="ref98">
      <label>98.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bada</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Vasilevsky</surname><given-names>N</given-names></name>, <name name-style="given-only"><given-names>Jr Baumgartner</given-names></name>, et al</person-group><article-title>Gold-standard ontology-based anatomical annotation in the CRAFT Corpus</article-title>. <source>Database</source>, <volume>2017</volume>, <issue>12</issue>  <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="ref99">
      <label>99.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Garvin</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Gobbel</surname><given-names>GT</given-names></name>, et al</person-group> Automating quality measures for heart failure using natural language processing: a descriptive study in the department of veterans affairs. JMIR Med Inform, 6(1):e5, 2018.</mixed-citation>
    </ref>
    <ref id="ref100">
      <label>100.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Blackley</surname><given-names>SV</given-names></name>, <name name-style="western"><surname>Kowalski</surname><given-names>L</given-names></name></person-group>, et al. <article-title>Analysis of errors in dictated clinical documents assisted by speech recognition software and professional transcriptionists errors in clinical documents created using speech recognition software errors in clinical documents created using speech recognition software</article-title>. <source>JAMA Netw Open</source>, <volume>1</volume>(<issue>3</issue>): <fpage>e180530</fpage>–<lpage>e180530</lpage>,<year>2018</year>.<pub-id pub-id-type="pmid">30370424</pub-id></mixed-citation>
    </ref>
    <ref id="ref101">
      <label>101.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Poesio</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Grishina</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Kolhatkar</surname><given-names>V</given-names></name>, et al</person-group><article-title>Anaphora resolution with the ARRAU corpus</article-title>. In: <source>Proceedings of the First Workshop on Computational Models of Reference, Anaphora and Coreference</source>, <fpage>11</fpage>–<lpage>22</lpage>, <publisher-loc>New Orleans, Louisiana</publisher-loc>, <year>2018</year><comment>Association for Computational Linguistics</comment>.</mixed-citation>
    </ref>
    <ref id="ref102">
      <label>102.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ceberio</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Aduriz</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Díaz de Ilarraza</surname><given-names>A</given-names></name>, et al.</person-group>  <article-title>Coreferential relations in basque: the annotation process</article-title>. <source>J Psycholinguist Res</source>  <year>2018</year>; <volume>47</volume>(<issue>2</issue>): <fpage>325</fpage>–<lpage>42</lpage>.<pub-id pub-id-type="pmid">29399705</pub-id></mixed-citation>
    </ref>
    <ref id="ref103">
      <label>103.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Page</surname><given-names>R</given-names></name></person-group><article-title>Group selfies and snapchat: from sociality to synthetic collectivisation</article-title>. <source>Discourse, Context &amp; Media</source>  <year>2019</year>; <volume>28</volume>:<fpage>79</fpage>–<lpage>92</lpage>.</mixed-citation>
    </ref>
    <ref id="ref104">
      <label>104.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>O’Mara-Eves</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Thomas</surname><given-names>J</given-names></name>, <name name-style="western"><surname>McNaught</surname><given-names>J</given-names></name>, et al.</person-group>  <article-title>Using text mining for study identification in systematic reviews: a systematic review of current approaches</article-title>. <source>Syst Rev</source>  <year>2015</year>; <volume>4</volume>(<issue>1</issue>): <fpage>5</fpage>.<pub-id pub-id-type="pmid">25588314</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
