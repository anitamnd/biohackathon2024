<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Hortic Res</journal-id>
    <journal-id journal-id-type="iso-abbrev">Hortic Res</journal-id>
    <journal-title-group>
      <journal-title>Horticulture Research</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">2662-6810</issn>
    <issn pub-type="epub">2052-7276</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8325680</article-id>
    <article-id pub-id-type="publisher-id">608</article-id>
    <article-id pub-id-type="doi">10.1038/s41438-021-00608-w</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MFCIS: an automatic leaf-based identification pipeline for plant cultivars using deep learning and persistent homology</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Yanping</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Peng</surname>
          <given-names>Jing</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yuan</surname>
          <given-names>Xiaohui</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Lisi</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhu</surname>
          <given-names>Dongzi</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6238-4554</contrib-id>
        <name>
          <surname>Hong</surname>
          <given-names>Po</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5598-2115</contrib-id>
        <name>
          <surname>Wang</surname>
          <given-names>Jiawei</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Qingzhong</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0780-0284</contrib-id>
        <name>
          <surname>Liu</surname>
          <given-names>Weizhen</given-names>
        </name>
        <address>
          <email>liuweizhen@whut.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.162110.5</institution-id><institution-id institution-id-type="ISNI">0000 0000 9291 3229</institution-id><institution>School of Computer Science and Technology, </institution><institution>Wuhan University of Technology, </institution></institution-wrap>Wuhan, Hubei China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.162110.5</institution-id><institution-id institution-id-type="ISNI">0000 0000 9291 3229</institution-id><institution>Chongqing Research Institute, Wuhan University of Technology, </institution></institution-wrap>Chongqing, China </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.452757.6</institution-id><institution-id institution-id-type="ISNI">0000 0004 0644 6150</institution-id><institution>Shandong Key Laboratory of Fruit Biotechnology Breeding, </institution><institution>Shandong Institute of Pomology, </institution></institution-wrap>Taian, Shandong China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>1</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>1</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>8</volume>
    <elocation-id>172</elocation-id>
    <history>
      <date date-type="received">
        <day>12</day>
        <month>10</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>5</day>
        <month>5</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>5</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Recognizing plant cultivars reliably and efficiently can benefit plant breeders in terms of property rights protection and innovation of germplasm resources. Although leaf image-based methods have been widely adopted in plant species identification, they seldom have been applied in cultivar identification due to the high similarity of leaves among cultivars. Here, we propose an automatic leaf image-based cultivar identification pipeline called MFCIS (<underline>M</underline>ulti-<underline>f</underline>eature Combined <underline>C</underline>ultivar <underline>I</underline>dentification <underline>S</underline>ystem), which combines multiple leaf morphological features collected by persistent homology and a convolutional neural network (CNN). Persistent homology, a multiscale and robust method, was employed to extract the topological signatures of leaf shape, texture, and venation details. A CNN-based algorithm, the Xception network, was fine-tuned for extracting high-level leaf image features. For fruit species, we benchmarked the MFCIS pipeline on a sweet cherry (<italic>Prunus avium</italic> L.) leaf dataset with &gt;5000 leaf images from 88 varieties or unreleased selections and achieved a mean accuracy of 83.52%. For annual crop species, we applied the MFCIS pipeline to a soybean (Glycine max L. Merr.) leaf dataset with 5000 leaf images of 100 cultivars or elite breeding lines collected at five growth periods. The identification models for each growth period were trained independently, and their results were combined using a score-level fusion strategy. The classification accuracy after score-level fusion was 91.4%, which is much higher than the accuracy when utilizing each growth period independently or mixing all growth periods. To facilitate the adoption of the proposed pipelines, we constructed a user-friendly web service, which is freely available at <ext-link ext-link-type="uri" xlink:href="http://www.mfcis.online">http://www.mfcis.online</ext-link>.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Plant breeding</kwd>
      <kwd>Field trials</kwd>
      <kwd>Biodiversity</kwd>
      <kwd>Bioinformatics</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100007724</institution-id>
            <institution>Wuhan University of Technology (WUT)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>104-40120526</award-id>
        <award-id>WUT: 2020IVA026</award-id>
        <principal-award-recipient>
          <name>
            <surname>Liu</surname>
            <given-names>Weizhen</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par2">Plant cultivar identification is a fundamental field of interest for plant breeding in terms of cultivar rights protection and the continuous breeding of new cultivars. The conventional method for cultivar identification usually relies on the expertize and empirical knowledge of crop breeders and has unpredictable accuracy and low efficiency. In recent decades, diverse molecular markers have been employed for plant variety identification<sup><xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR4">4</xref></sup>. Nevertheless, the accuracies of these methods mainly depend on the amount of genetic differentiation across the cultivars evaluated and the number and type of molecular markers selected in the analysis system<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. These methods involve complicated operating procedures in a laboratory that are time-consuming and are not well suited for use by ordinary farmers. Therefore, rapid, stable, and accurate methods for plant cultivar identification are required.</p>
    <p id="Par3">A computer vision pipeline based on morphological features extracted from leaf images is an ideal option. Compared with the other organs, plant leaves have more general and lasting characteristics in terms of shape, texture, and venation and therefore have been successfully used for plant species identification/plant taxonomy<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>. To date, many image processing-based algorithms and software, such as Leaf GUI<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, Rosette Tracker<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, Leaf-GP<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, MorphoLeaf<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, and Phenotiki<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, that measure leaf morphological phenotypes have been reported. However, a majority of the phenotypes extracted using these algorithms are not descriptive enough to discriminate leaves from different cultivars. Moreover, there are some leaf image-based plant species identification pipelines that use features of leaf shape, texture, and venation independently<sup><xref ref-type="bibr" rid="CR12">12</xref>–<xref ref-type="bibr" rid="CR15">15</xref></sup> or combine them<sup><xref ref-type="bibr" rid="CR16">16</xref>–<xref ref-type="bibr" rid="CR19">19</xref></sup>. Unfortunately, when transferring these methods from species identification to cultivar identification, most of them failed due to the high similarity of leaves among different cultivars. Recently, a deep convolutional neural network (DCNN)-based identification method for apple cultivars was reported and tested on an apple leaf image dataset consisting of more than 10,000 images from 14 cultivars<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. The unprecedented accuracy demonstrated the fantastic effectiveness of CNNs in cultivar recognition. However, the DCNN-based method required a large number of leaf images for each cultivar to train the model. A backpropagation neural network-based cultivar identification method was proposed for oleander identification. Using 18 global leaf characters (morphology and color) extracted from 22 cultivars, it only achieved an average accuracy of approximately 54.55%<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. Its accuracy might be improved by adding more local leaf features into the model, but the computation speed would be sacrificed. The multi-orientation region transform method was developed to characterize leaf contour and vein structure features simultaneously for soybean cultivar identification<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. They achieved an accuracy of ~54.2% in the recognition of 100 soybean cultivars. This study was the first to investigate large-scale cultivar classification with leaf images and proved the effectiveness of leaf venation features in cultivar classification. However, the drawback is that it requires vein images with primary and secondary vein annotations, which is quite labor intensive.</p>
    <p id="Par4">These challenges of leaf image-based cultivar identification models motivated us to develop a cultivar identification pipeline with the following characteristics: (1) Automation: the leaf images can be directly fitted as the input into the pipeline, and the cultivar identification results are the output. (2) Multi-feature combination: the leaf morphological features are comprehensively measured both locally and globally to improve the identification accuracy. (3) High efficiency: the method does not require a large sample size for model training and has an acceptable computation time. (4) Generalization capacity: the model can be easily adapted to other plants. Therefore, we incorporated persistent homology (PH), a topological method, to capture the morphological signatures of leaf shape, texture, and venation both locally and globally. Topology is a branch of mathematics that concerns spatial properties under deformation while disallows tearing apart or reattaching<sup><xref ref-type="bibr" rid="CR23">23</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup>. The core idea of PH is to track the appearance and disappearance of holes in topological space at multiple scales. The obtained features are a set of points in the plane, called the persistence diagram<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> (PD), which can characterize structural properties and reflect spatial information in an invariant way<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. Compared to the traditional feature extractors and CNN-based methods, PH is more resilient to local perturbations and scale variance<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>. This is because although the shapes of connected components and holes may change under geometric transformations, the number of components and holes remains the same<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Therefore, it is recognized that the incorporation of morphological features extracted by PH into CNN-based models can provide a significant performance improvement. These ideas have been validated in a wide range of computer vision and biomedical image processing studies, such as automated tumor segmentation<sup><xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>, 3D surface texture analysis<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, and image classification<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>.</p>
    <p id="Par5">In this study, we propose a leaf image-based and automatic plant cultivar classification pipeline, called MFCIS (<underline>M</underline>ulti-feature Combined <underline>C</underline>ultivar <underline>I</underline>dentification <underline>S</underline>ystem), that combines morphological features of the leaf shape, texture, and venation extracted by PH and the high-level image features extracted by the fine-tuned Xception<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> model. We primarily tested the proposed pipeline on a sweet cherry leaf cultivar dataset with &gt;5000 leaf images at the same growth stage derived from 88 commercial cultivars or unreleased selections. To assess the generalization ability of the proposed pipeline, we also applied the MFCIS to a soybean leaf cultivar dataset consisting of 5000 leaf images of 100 cultivars. Unlike the sweet cherry dataset, leaves in the soybean dataset were collected from five different growth periods. Therefore, we attempted cultivar identification using leaves from each period independently, mixed leaves from all growth periods without taking the soybean growth periods into account, and leaves from each period independently with a fusion of the prediction results.</p>
  </sec>
  <sec id="Sec2" sec-type="materials|methods">
    <title>Materials and methods</title>
    <sec id="Sec3">
      <title>Plant materials</title>
      <sec id="Sec4">
        <title>Sweet cherry cultivar dataset</title>
        <p id="Par6">Sweet cherry leaves were collected from 88 sweet cherry cultivars (64 commercial cultivars and 24 unreleased selections) at the research station of the Shandong Institute of Pomology in Tai’an, Shandong Province, China in May 2020. For each cultivar, we randomly sampled 50 to 90 mature leaves (Fig. S<xref rid="MOESM1" ref-type="media">1</xref>) in the middle parts of the branches from 3 to 5 trees to ensure that these leaves were at the same growth stage. These leaves were imaged by transmission scanning (Fig. <xref rid="Fig1" ref-type="fig">1a</xref>) with a resolution of 600 DPI and color depth of 48 bits with an EPSON Perfection V850 Pro scanner (Epson Co., Ltd, Shanghai, China).<fig id="Fig1"><label>Fig. 1</label><caption><title>Examples of transmitted light leaf images.</title><p><bold>a</bold> Sweet cherry leaves of two cultivars. <bold>b</bold> Soybean leaves of two cultivars at five growth stages. The first and second rows contain two different cultivars. Columns one to five represent the growth periods of R1, R3, R4, R5, and R6, respectively. Images in the last column show the local details of the augmented leaf regions</p></caption><graphic xlink:href="41438_2021_608_Fig1_HTML" id="d32e502"/></fig></p>
      </sec>
      <sec id="Sec5">
        <title>Soybean cultivar dataset</title>
        <p id="Par7">Leaves of 100 soybean cultivars were collected at the Soybean Experimental Station of Heilongjiang Academy of Agricultural Sciences in Mudanjiang, Heilongjiang Province, China, in 2017. For each cultivar, a sample of 50 fully expanded leaves was randomly picked from several plants at five reproductive stages: the beginning flowering, beginning pod, full pod, beginning seed, and full seed stages (denoted as R1, R3, R4, R5, and R6, respectively). These leaves were immediately scanned after they were detached from the plants using a backside transmission-type scanner (EPSON Perfection V850 Pro) under the same configuration as the sweet cherry leaf image dataset. Therefore, 5,000 leaf images were collected for our soybean cultivar dataset (Fig. <xref rid="Fig1" ref-type="fig">1b</xref>).</p>
      </sec>
    </sec>
    <sec id="Sec6">
      <title>Image analysis pipeline for cultivar identification</title>
      <p id="Par8">We developed a robust pipeline for automatically identifying cultivars using leaf images as input. A flowchart of the main algorithm is presented in Fig. <xref rid="Fig2" ref-type="fig">2</xref> and is described in detail as follows: (I) Data preprocessing. A leaf RGB image was converted to a grayscale image, and the image was enhanced by extending the dynamic range of the grayscale value. (II) Feature extraction. Shape features were extracted by filtrating the leaf from different directions. Global leaf texture and venation features were captured by computing the PDs of the enhanced grayscale images and leaf venation images. PDs of the leaf shape, texture, and venation were further processed by a 1-D CNN model. Additional image features were extracted by a CNN model. (III) Feature combination and cultivar classification. The ‘early fusion’ technique<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> was used to concatenate the leaf morphological features and the high-level CNN features. Crop cultivars were classified by a fully connected network using the combined features.<fig id="Fig2"><label>Fig. 2</label><caption><title>Flowchart for the proposed MFCIS method.</title><p><bold>I</bold> Data preprocessing. <bold>II</bold> Feature extraction using the persistent homology and CNN. <bold>III</bold> Multi-feature combination and cultivar identification</p></caption><graphic xlink:href="41438_2021_608_Fig2_HTML" id="d32e541"/></fig></p>
      <sec id="Sec7">
        <title>Image preprocessing</title>
        <p id="Par9">The image preprocessing procedure consists of background removal and contrast enhancement. Owing to the high quality and clean background of our leaf images, a typical global thresholding method was adopted to remove the image background, and linear stretching was applied to enlarge the dynamic grayscale value range.</p>
      </sec>
      <sec id="Sec8">
        <title>Leaf morphological feature extraction using PH</title>
        <p id="Par10">As shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, leaves of different cultivars exhibit distinct leaf morphological patterns in terms of their shapes, textures, and venations. Leaves of the same cultivar at different growth stages also display subtle differences. Hence, we employed PH to capture leaf shape, texture, and vein features from a topological perspective and described the extracted features with PDs. The leaf shape PDs were calculated with the Pershombox package<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, and the leaf texture and venation PDs were both computed with the Python Homcloud package [Ippei Obayashi, Hiraoka lab (AIMR, Tohoku University)].</p>
        <p id="Par11">The detailed feature extraction procedure is described in supplementary materials section 1 and Fig. S<xref rid="MOESM1" ref-type="media">2</xref>. Here, we briefly introduce the design philosophy. Inspired by Hofer et al.<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> and Turner et al.<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, the leaf shape features were analyzed by multi-directional height function filtration. We constructed the height functions for the leaf binary mask image from 30 directions, filtered these functions and stacked the PDs of the three consecutive directions to preserve the morphology context information. The light transmittance difference in leaf venation and mesophyll caused the unique textures in the transmission scanned images is indicated by the variety of grayscale values in the different leaf regions. Therefore, we formulated the leaf grayscale values as real value functions and filtered them by increasing the threshold from −∞ to +∞. In the filtration procedure, the changes in the components and holes were captured and recorded. For venation feature extraction, we applied the distance transform on binary leaf venation images and tracked the areola and venation structure changes under the different scales by filtrating the obtained distance map of the leaf venation network.</p>
      </sec>
      <sec id="Sec9">
        <title>High-level image feature extraction by a CNN</title>
        <p id="Par12">The deep CNN model, Xception<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, was implemented as the backbone network to extract high-level features from leaf images. It was difficult to train such a deep neural network from scratch on our relatively small dataset. Additionally, the knowledge learned from other image classification tasks may be helpful in our study. Therefore, we applied transfer learning to the cultivar classification task by adopting the weights of Xception on the ImageNet dataset<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> as the initial settings. The output of the global average pooling layer is a 2048-dimensional vector and was used as the high-level image features for the following multifeature combination process. High-level image features are more abstract and discriminative than traditional hand-crafted morphological features (the leaf area, perimeter, etc.) and the image processing-based descriptors.</p>
      </sec>
      <sec id="Sec10">
        <title>PD feature vectorization and multifeature combination</title>
        <p id="Par13">Since a PD is a set of points in a plane with unusual multiset structures, it is a considerable challenge to use it for machine learning purposes<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. Here, our innovation is to vectorize the PDs of the leaf shape, texture, and venation by inputting them into a CNN. Compared to the other vectorization methods (such as persistence image<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> and persistence landscape<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>), the input layer maps the PD features to a task-optimal representation with trainable weights<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. Owing to the high definition and resolution of the transmitted scanned leaf images, the texture and venation PDs contain thousands of points with different distances from the diagonal, representing their ‘lifetime’. Empirically, features with a longer lifetime are usually more essential<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, and noise generates points close to the diagonal. To balance the model complexity and accuracy, we sorted these points by their lifetime and selected the first 700 points for shape features and 1000 points for texture and venation features as input.</p>
        <p id="Par14">The topological features were further processed by a CNN model based on the previously mentioned input layer. The model structure was similar to the network for 2D object shape classification<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, except for the filters and dropout layers. The network structure details are given in Fig. S<xref rid="MOESM1" ref-type="media">3</xref>. There were ten layers in the topological feature model. The first layer was the input layer. For shape features, the PDs from three consecutive filtration directions were stacked as a 3-channel input. The number of filtration directions determined the branch number. The texture and venation features occupied four input branches. Both the 0th and 1st PD were used as input. A total of 16 filters were used in the following convolutional layers, and each of them learned different feature types. Each filter was slid over the input, and convolution was applied. The max-pooling layer reduced the feature dimension. It should be noted that max-pooling operated along the filter dimension. Dropout layers were used to avoid overfitting. The batch normalization layer was used to normalize the output of the previous layers. The ReLU layer performed a nonlinear transformation on the output of the previous layers.</p>
        <p id="Par15">To enhance the accuracy, we combined the outputs of the topological feature processing model and the Xception model. The concatenated feature was a 10,752-dimensional vector. The output dimension of the Xception model was 2048, and the dimension of the processed topological features was 8704; these processed features consisted of 30 shape feature streams, two texture feature streams, and two venation feature streams. A dropout layer followed the concatenation layer and had a dropout rate of 0.5, a dense layer with 2048 neurons, a batch normalization layer, and an output layer whose neuron number was the same as that of the image class.</p>
      </sec>
      <sec id="Sec11">
        <title>Score-level fusion of the results derived from different growth periods</title>
        <p id="Par16">Leaves from different growth periods of the same crop cultivar may have different shapes, textures, and venation patterns, as shown in Fig. <xref rid="Fig1" ref-type="fig">1a</xref>. Leaves from different growth periods of the same cultivar may provide different but complementary clues for cultivar classification<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. Therefore, we adopted a score-level fusion method to combine the prediction results of each growth period. The results were fused with (1).<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G(X) = arg \,max \frac{1}{T}\mathop {\sum}\limits_{t = 1}^T {g_t(x_t)} ,\,X = \left\{ {x_1,x_2,...,x_T} \right\}$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mspace width="0.25em"/><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="}" open="{"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41438_2021_608_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <italic>G(X)</italic> is the final prediction result of the sample set <italic>X</italic>, <italic>T</italic> is the total model number, and <italic>g</italic><sub><italic>t</italic></sub>(<italic>x</italic><sub><italic>t</italic></sub>) is the prediction result of sample <italic>x</italic><sub><italic>t</italic></sub> by model <italic>t</italic>.</p>
      </sec>
    </sec>
    <sec id="Sec12">
      <title>Model evaluation</title>
      <p id="Par17">We evaluated our model performance with ten different train-and-test splits to ensure that the proposed pipeline is reliable and stable. In each iteration, the dataset was randomly shuffled and divided into training and test sets. There were ten iterations for each model, and the mean result was used to evaluate the model performance.</p>
    </sec>
    <sec id="Sec13">
      <title>Statistical analysis</title>
      <p id="Par18">We employed the one-way statistical analysis of variance (ANOVA)<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> test to compare the performance of different models and investigate whether the population mean vectors are the same. The null hypothesis <italic>H</italic><sub>0</sub> indicates that there is no significant difference among the group means, which can be formulated as <italic>H</italic><sub>0</sub>: <italic>µ</italic><sub>1</sub> = <italic>µ</italic><sub>2</sub> = <italic>· · · = µ</italic><sub>K</sub>. We fail to accept <italic>H</italic><sub>0</sub> if the <italic>p</italic>-value for an experiment is less than the selected significance level (<italic>α</italic>) of 0.05, which implies that at least one group mean is significantly different from the others. The <italic>F</italic>-test rejects <italic>H</italic><sub>0</sub> at level <italic>α</italic> if<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F = \frac{{S_1^2}}{{S_2^2}} &gt; F_{K - 1,\,N - K}(\alpha )$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41438_2021_608_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_1^2 = \mathop {\sum}\limits_{i = 1}^K {n_i(\bar Y_{i \cdot } - \bar Y)^2} /(K - 1)$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>Ȳ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⋅</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>Ȳ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41438_2021_608_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_2^2 = \mathop {\sum}\limits_{i = 1}^K {\mathop {\sum}\limits_{j = 1}^{n_i} {(Y_{ij} - \bar Y_{i \cdot })^2} } /(N - K)$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>Ȳ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⋅</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41438_2021_608_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq1"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_1^2$$\end{document}</tex-math><mml:math id="M10"><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41438_2021_608_Article_IEq1.gif"/></alternatives></inline-formula> is the between-group variance and <inline-formula id="IEq2"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_2^2$$\end{document}</tex-math><mml:math id="M12"><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41438_2021_608_Article_IEq2.gif"/></alternatives></inline-formula> is the within-group variance. <inline-formula id="IEq3"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bar{Y}_{i}$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mrow><mml:mi>Ȳ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41438_2021_608_Article_IEq3.gif"/></alternatives></inline-formula> denotes the sample mean in the <italic>i</italic>th group, <italic>n</italic><sub><italic>i</italic></sub> is the number of observations in the <italic>i</italic>th group, <inline-formula id="IEq4"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bar Y$$\end{document}</tex-math><mml:math id="M16"><mml:mi>Ȳ</mml:mi></mml:math><inline-graphic xlink:href="41438_2021_608_Article_IEq4.gif"/></alternatives></inline-formula> is the overall mean of the data, and <italic>K</italic> denotes the number of groups. <italic>Y</italic><sub><italic>ij</italic></sub> is the <italic>j</italic>th observation in the <italic>i</italic>th group of <italic>K</italic> groups, and <italic>N</italic> is the overall sample size.</p>
    </sec>
    <sec id="Sec14">
      <title>Online cultivar recognition system</title>
      <p id="Par19">To facilitate the adoption of the proposed pipeline, we developed an online cultivar recognition system (Fig. <xref rid="Fig3" ref-type="fig">3</xref>) on a publicly available website at <ext-link ext-link-type="uri" xlink:href="http://www.mfcis.online">http://www.mfcis.online</ext-link>. The website was developed based on the web framework Flask and is accessible via the internet. It provides an easy-to-use cultivar identification service that consists of the following three steps: (1) Model and dataset selection. In this step, users need to choose a preferred model and the target dataset and then wait for model loading and configuration. (2) Image uploading and cultivar recognition. In this step, users should select and upload a local image file via a pop-up. The chosen image can be enlarged to view the image details. Users should wait for several minutes until the prediction is made. (3) View the prediction result. The top 3 cultivar prediction results are displayed as a pie chart. Users can click the pie chart to view the prediction score of each cultivar. A detailed user manual can be found in the Supplementary Manual.<fig id="Fig3"><label>Fig. 3</label><caption><title>The main parts of the online cultivar recognition system.</title><p><bold>a</bold> Model and dataset selection panel. <bold>b</bold> Image preview and upload pop-up. <bold>c</bold> Main interface. <bold>d</bold> Results panel. The number in the blue circle shows the work step of the pipeline</p></caption><graphic xlink:href="41438_2021_608_Fig3_HTML" id="d32e1228"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec15" sec-type="results">
    <title>Results</title>
    <sec id="Sec16">
      <title>Leaf morphological feature extraction by PH</title>
      <p id="Par20">The leaf shape, texture, and venation PD samples are listed in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. Owing to the high resolution of leaf images, the texture and venation PDs contained a large number of points. To intuitively compare the differences among PDs, we visualized all the PDs by their 2D histograms. The plane was discretized into a mesh of 32 × 32, and the point number of each grid was indicated by its color. As shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>, most topological features in the texture and venation PDs were close to the diagonal, meaning that they had a similar distributions among leaves from different cultivars. However, the points far from the diagonal had different distributions. Even though it was difficult to describe in words, the neural network could excavate the clues behind it. The leaf shape PDs in each direction had fewer points than the texture and venation PDs, and most of them were close to diagonal. To capture the local shape details while retaining the shape context, we considered all the directions and stacked three consecutive directions as a set of shape features.<fig id="Fig4"><label>Fig. 4</label><caption><title>Leaf texture, venation, and shape PDs.</title><p>Each column represents a sweet cherry cultivar. PD<sub>0</sub> of the leaf texture and venation and PD<sub>0</sub> of the leaf shape near 0°, 45°, 90°, and 135° are listed. All the diagrams are 2D histograms of points that discretize the plane into a grid of 32 × 32, and the point number of each grid is counted. The color of each grid represents the number of points</p></caption><graphic xlink:href="41438_2021_608_Fig4_HTML" id="d32e1259"/></fig></p>
    </sec>
    <sec id="Sec17">
      <title>Performance evaluation of MFCIS on the sweet cherry cultivar dataset</title>
      <p id="Par21">To assess the effectiveness of our MFCIS pipeline, we tested its performance on the sweet cherry cultivar dataset, which contains leaf images that were collected at one growth stage. The model was trained with 200 epochs, and the learning rate halved if the validation loss did not decrease in five consecutive epochs. The model with the smallest validation loss was selected as the final model. The ratios of the test and validation data were set to 0.3 and 0.1, respectively. RMSProp was chosen as the optimizer with an initial learning rate of 0.001. All the RGB leaf images were resized to 256 × 256 pixels. The leaf morphological and CNN-extracted image features were concatenated as a joint descriptor. The identification accuracies were examined using ten different train-and-test splits.</p>
      <p id="Par22">The MFCIS pipeline achieved a mean accuracy of 83.52% on the sweet cherry dataset (Table <xref rid="Tab1" ref-type="table">1</xref>). To highlight the higher performance of the MFCIS pipeline, we tested two classic CNN models on the sweet cherry dataset, and the details of the parameter settings are presented in supplementary materials section 2. The fine-tuned Xception, which achieved the highest performance on several species classification datasets (Table <xref rid="MOESM1" ref-type="media">S1</xref>), was trained and obtained an accuracy that was 17% lower than that of the MFCIS pipeline (Table <xref rid="Tab1" ref-type="table">1</xref>). A DCNN model customized for an apple cultivar recognition task<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> was also implemented and achieved an accuracy that was 50.97% lower than that of the MFCIS pipeline (Table <xref rid="Tab1" ref-type="table">1</xref>). Collectively, these results show the superiority of the MFCIS pipeline for leaf image classification tasks on a relatively small dataset.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Comparison of the classification accuracies of the different models on the sweet cherry dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>Accuracy (%)</th></tr></thead><tbody><tr><td>IDSC + DP</td><td>10.91</td></tr><tr><td>HSC</td><td>16.47</td></tr><tr><td>PH</td><td>42.08</td></tr><tr><td>Customized DCNN for apple cultivar classification</td><td>32.55</td></tr><tr><td>Fine-tuned Xception</td><td>66.52</td></tr><tr><td>MFCIS (Our model)</td><td><bold>83.52</bold></td></tr></tbody></table><table-wrap-foot><p>Only the Top-1 accuracy is shown. The accuracies in bold are the results of the proposed method.</p></table-wrap-foot></table-wrap></p>
      <p id="Par23">We also evaluated the accuracies of the cultivar identification models with leaf features extracted by some image processing-based methods to exhibit the advantages of high-level image features (Table <xref rid="Tab1" ref-type="table">1</xref>). Their parameter settings are presented in supplementary materials section 2. Image processing-based methods, shape classification using the inner-distance (IDSC)<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> and hierarchical string cuts (HSC)<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>, were also tested on the sweet cherry leaf image dataset and achieved much lower accuracies than the fine-tuned Xception and MFCIS methods. The IDSC method was integrated with dynamic programming (DP) to match the contour points. The accuracy of the IDSC+DP algorithm was 10.91%, and the accuracy of the HSC algorithm was 16.47%. The classification model using only leaf morphological features extracted by PH (referred to as the PH method in Tables <xref rid="Tab1" ref-type="table">1</xref> and <xref rid="Tab2" ref-type="table">2</xref>) was tested and obtained an accuracy of 42.08%. Even though it achieved much higher performance than IDSC + DP and HSC, it was less accurate than both the MFCIS pipeline and the fine-tuned Xception pipeline.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Mean accuracies of the different cultivar classification models using leaves from each period in the soybean dataset independently</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Method</th><th colspan="5">Accuracy (%)</th></tr><tr><th>R1</th><th>R3</th><th>R4</th><th>R5</th><th>R6</th></tr></thead><tbody><tr><td>IDSC + DP</td><td>19.67</td><td>23.67</td><td>20.33</td><td>20.00</td><td>16.00</td></tr><tr><td>HSC</td><td>27.02</td><td>31.07</td><td>31.20</td><td>30.60</td><td>27.71</td></tr><tr><td>PH</td><td>16.70</td><td>17.15</td><td>18.20</td><td>17.25</td><td>12.70</td></tr><tr><td>DF-VGG16/LDA</td><td>18.87</td><td>22.50</td><td>19.90</td><td>15.90</td><td>13.37</td></tr><tr><td>Fine-tuned Xception</td><td>30.60</td><td>35.97</td><td>33.37</td><td>29.73</td><td>20.40</td></tr><tr><td>MFCIS (Our model)</td><td><bold>55.90</bold><sup><bold>*</bold></sup></td><td><bold>61.40</bold><sup><bold>*</bold></sup></td><td><bold>61.37</bold><sup><bold>*</bold></sup></td><td><bold>59.80</bold><sup><bold>*</bold></sup></td><td><bold>44.87</bold><sup><bold>*</bold></sup></td></tr></tbody></table><table-wrap-foot><p>All the models were tested with ten different train-and-test splits. Only the Top-1 accuracy is listed. The accuracies in bold are the results of the proposed method. The superscript “*” denotes a significant difference (<italic>P</italic> &lt; 0.05) between the proposed method and the other models using one-way ANOVA.</p></table-wrap-foot></table-wrap></p>
      <p id="Par24">To assess the confidence of the prediction results, we analyzed the final output of the softmax layer. The probability (the maximum value of the output of the softmax layer) of the correctly classified cultivars ranged from 0.26 to 1, and the mean probability was 0.96 with a standard deviation of 0.10. Furthermore, we explored the classification results of each cultivar by adopting the F<sub>1</sub>-score, which provided a more comprehensive evaluation of the predictions. The distribution of the F<sub>1</sub>-score in sweet cherry cultivar identification is shown in Fig. <xref rid="Fig5" ref-type="fig">5(a)</xref>. Seventy-three cultivars had F<sub>1</sub>-scores higher than 0.7. The top-10 sweet cherry cultivars in terms of the F<sub>1</sub>-score were five commercial cultivars (“Luyu2”, “Windser”, “Regina”, “Luying 5”, and “Zayadka”) and five unreleased selections (“Zhao-B-4-5”, “Zhao-B-4-1”, “Zou-1”, “Zhao-B”, and “16-4”). All of them achieved F<sub>1</sub>-scores higher than 0.9, and more details are listed in Table <xref rid="MOESM1" ref-type="media">S2</xref>. By observing the leaf images listed in Fig. S<xref rid="MOESM1" ref-type="media">4</xref>, we found that these ten cultivars possess distinctive leaf shapes and margins. For example, the cultivar “Zayadka” has an ovoid shape with a sharper margin, while the cultivar “Luyu 2” has a smoother margin. Although these visible morphological differences could not be quantitatively measured, they could be easily captured by the CNN and summarized by high-level image features. Most of the poorly recognized cultivars have neither distinctive shape features nor distinctive leaf margin features. The large inner cultivar difference also played an important role in poor recognition performance.<fig id="Fig5"><label>Fig. 5</label><caption><title>Cultivar identification results of different datasets.</title><p><bold>a</bold> The distribution of F<sub>1</sub>-scores on the sweet cherry dataset. <bold>b</bold> Boxplot of the accuracy of cultivar identification using leaves from each period in the soybean dataset independently. The triangle represents the mean accuracy of the corresponding period. The different letters on the top of each box indicate significant differences in accuracies among different growth stages (<italic>P</italic> &lt; 0.05) using one-way ANOVA</p></caption><graphic xlink:href="41438_2021_608_Fig5_HTML" id="d32e1596"/></fig></p>
    </sec>
    <sec id="Sec18">
      <title>Performance evaluation of MFCIS on the soybean cultivar dataset</title>
      <p id="Par25">To evaluate the generalization performance of the proposed MFCIS pipeline, we applied it to a soybean cultivar dataset consisting of 5000 leaf images with 100 cultivars collected at five growth stages.</p>
      <sec id="Sec19">
        <title>Cultivar Identification using leaves from each period independently</title>
        <p id="Par26">As shown in Fig. <xref rid="Fig1" ref-type="fig">1b</xref>, leaves of the same cultivar from different growth periods exhibit distinct shape, texture, and venation characteristics. To investigate the prediction ability of each growth period, we conducted the classification task using leaves of each period independently. In soybean cultivar identification, our pipeline was equipped with the same hyperparameters as those in sweet cherry cultivar identification.</p>
        <p id="Par27">Using MFCIS, the mean accuracies for the five growth stages ranged from 44.8 to 61.4% (Fig. <xref rid="Fig5" ref-type="fig">5b</xref> and Table <xref rid="Tab2" ref-type="table">2</xref>). The R3 stage achieved a significantly higher accuracy than the other periods, whereas R6 achieved a substantially lower accuracy. The R1, R4, and R5 stages exhibited similar performances.</p>
        <p id="Par28">To highlight the superior performance of the proposed MFCIS pipeline, we tested the image processing-based methods IDSC+DP<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>, HSC<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>, and PH, as well as other deep-learning models, such as fine-tuned Xception<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> and DF-VGG16/LDA<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. Their parameter settings are presented in supplementary materials section 2. As shown in Table <xref rid="Tab2" ref-type="table">2</xref>, the proposed MFCIS pipeline significantly outperformed all the other models across all the growth periods.</p>
      </sec>
      <sec id="Sec20">
        <title>Cultivar identification using leaf images from all growth periods</title>
        <p id="Par29">To evaluate the impact of the intra-cultivar difference on cultivar identification, we mixed leaves from different periods within the same cultivar and conducted the classification task. All the parameter settings were the same as in the classification task using leaves from each growth period independently. The accuracies of using mixed leaves ranged from 52.6 to 56.5% in ten iterations with a mean accuracy of 54.45%, which was lower than that obtained when using each period independently, except for in the R6 stage.</p>
      </sec>
      <sec id="Sec21">
        <title>Cultivar classification by score fusion</title>
        <p id="Par30">Although leaves exhibit considerable intra-cultivar differences across the five growth periods, the inter-cultivar discrepancies are substantial. In this experiment, we trained the model for each period independently and then applied a score-level fusion of the classification results from all five growth periods. Here, we fused the prediction scores of all periods with the same weights and evaluated the score fusion with ten different train-and-test splits.</p>
        <p id="Par31">The mean accuracy of the proposed MFCIS method was 91.4% with a standard deviation of 1.64 × 10<sup>−2</sup>, which is a considerable improvement compared to cultivar identification using leaves from each growth period independently. In addition, the proposed pipeline also significantly outperformed the fine-tuned Xception model without leaf morphological features, whose mean accuracy was 64% and standard deviation was 2.19 × 10<sup>−2</sup>.</p>
      </sec>
    </sec>
    <sec id="Sec22">
      <title>The online recognition platform</title>
      <p id="Par32">To date, the website can recognize sweet cherry and soybean cultivars. We also incorporated the Swedish and Flavia leaf image datasets for species classification since our model also achieved notable performance in plant species classification on these datasets (Table <xref rid="MOESM1" ref-type="media">S1</xref>). We will continue to update the model to support more recognition tasks in the future. The website provides an intuitional process for cultivar identification. A detailed introduction is given in the supplementary video. However, to run the pipeline more efficiently and accurately, we highly recommend downloading the source code<sup><xref ref-type="bibr" rid="CR43">43</xref></sup> and running it on a high-performance server.</p>
    </sec>
  </sec>
  <sec id="Sec23" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par33">In crop breeding communities, cultivar identification has long been dependent on visual recognition by experts, which highly relies on personnel skills. Therefore, the accuracy cannot be guaranteed<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. It is of great significance to speed up the recognition process and make it accessible for nonexperts<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. With the development of deep learning and imaging techniques, leaf image-based cultivar identification studies have attracted attention from horticultural researchers and the computer vision society. Nevertheless, the accuracies of previous models remain a vital problem that has yet to be solved. This study proposed an effective strategy for leaf image acquisition, generated a highly accurate image analysis pipeline for cultivar identification, and constructed a web-based and user-friendly cultivar identification platform.</p>
    <sec id="Sec24">
      <title>Leaf-image acquisition strategy</title>
      <p id="Par34">We used the transmission scan technique, which is a low-cost and readily available method, to collect high-quality leaf images with subtle texture and clear leaf venation features. Similar to human fingerprints<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>, leaf texture and venation details provide vital information to discriminate cultivars<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR22">22</xref></sup>. However, they have not been widely applied thus far because venation and texture details in images obtained with general RGB cameras are usually not clear enough<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>.</p>
      <p id="Par35">The traditional way to improve the clarity of venation details was to treat the leaf with some chemical solutions before imaging, which destroys the leaf texture and is time consuming<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. X-ray imaging techniques can obtain perfect venation details<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>, but they are expensive. Therefore, transmission scans are a better choice than snapshots by mobile devices and reflection scans to record leaf texture and venation details. In our experiments, we used a 16-bit depth grayscale image to extract the delicate texture and venation details of a leaf. Although we captured subtler texture changes, the computation of the pipeline was expensive. Hence, we recommend using grayscale-level compression to reduce the computational cost if fine texture features are unnecessary for image analysis tasks.</p>
    </sec>
    <sec id="Sec25">
      <title>Contributions of MFCIS to cultivar identification accuracy improvement</title>
      <p id="Par36">Compared to other image processing<sup><xref ref-type="bibr" rid="CR12">12</xref>–<xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR40">40</xref>,<xref ref-type="bibr" rid="CR41">41</xref></sup> and classic machine learning-based algorithms<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR44">44</xref></sup>, the MFCIS pipeline significantly improved cultivar identification accuracy on both the soybean and sweet cherry leaf image datasets. One of the main reasons is the adoption of a feature fusion strategy that combines leaf morphological features extracted by PH and high-level image features extracted by deep learning. The combination of a CNN and PH might strengthen the generalization of the model when handling small datasets, which was reflected in the large improvement in accuracy obtained by MFCIS compared to the accuracy obtained by PH or a CNN independently in soybean cultivar recognition.</p>
      <p id="Par37">Another important reason is the adoption of a score-level fusion strategy that improved the utilization of information derived from leaves of the same cultivars collected from different growth periods. Considering that leaf morphology may be deformed during plant growth stages, we generated a soybean cultivar dataset with leaf images from five growth periods. Soybean leaves from each period were used independently to obtain identification accuracies varying from 44.87% (R6 period) to 61.37% (R3 period) in the MFCIS pipeline. Similar trends were also observed in other cultivar identification algorithms (Table <xref rid="Tab2" ref-type="table">2</xref>). The accuracy difference across growth periods suggests that the morphological and venation characteristics inconstantly change during growth, which indicates the necessity and rationality for using leaves collected from multiple growth periods in cultivar identification models. On the other hand, the lower identification accuracy of the R6 growth period was probably due to the lower quality of the leaf images compared to the leaf images from the other growth periods. Wormholes and disease spots were observed in leaves from the R6 period, indicating that environmental conditions could be an important factor affecting leaf data quality and the inconsistent accuracy of plant variety identification across growth periods.</p>
      <p id="Par38">Moreover, two strategies that incorporated leaves of many growth periods were evaluated. (1) We trained the model using leaf images from all growth periods. (2) We trained the model for each period independently and then applied a score-level fusion of the identification results from all five growth periods. As expected, the multigrowth-period score fusion method brought a significant performance improvement compared to using mixed leaves from different periods within the same cultivar. This is mainly due to score-level fusion, which provides moderately rich information<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> while avoiding dimension explosion and complicated dimension reduction<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, which are difficult challenges encountered when using small datasets. The high accuracy of the multigrowth-period score fusion method proved that leaves from different growth periods of the same cultivar render complementary information for cultivar classification<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>.</p>
    </sec>
    <sec id="Sec26">
      <title>User-friendly online platform</title>
      <p id="Par39">Software is crucial for biological research<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> and has a significant impact on research efficiency. Many well-designed software programs, such as Leaf GUI<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, Rosette Tracker<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, Leaf-GP<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, MorphoLeaf<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, and Phenotiki<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, have been developed and have benefited biological researchers. However, since most of the software programs are offline clients, they usually require a complicated setup and a high-performance computer. We constructed a web-based cultivar recognition platform that is accessible to researchers and farmers through the internet. Compared to the existing software, the only requirement of our system is a browser and the internet, which is much more convenient.</p>
      <p id="Par40">Another user-friendly property of our online platform is full automation. Some leaf image-based cultivar identification pipelines, such as the multi-orientation region transform<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, require the manual annotation of leaf images before fitting into the models. However, for our online platform, users can directly input the leaf images into the pipeline and output cultivar identification results without any manual processing of the input images.</p>
    </sec>
    <sec id="Sec27">
      <title>Comparison with molecular marker-based technologies</title>
      <p id="Par41">Our proposed system, including leaf-image acquisition, the MSCIF algorithm, and the online platform, provides an easy-to-use, simple and nondestructive method for plant variety identification based on the morphological differences in plant leaves. It is a rapid workflow with a total time cost of &lt;15 min from scanning the leaf to obtaining the identification result. It can be arranged for outdoor-environment applications since the leaf image scanner and the online image processing algorithms are portable. The highly accurate results on the sweet cheery and soybean leaf datasets have proven the effectiveness of the pipeline. However, the morphological features of plant leaves are vulnerable to environmental and climatic conditions. Phenotypic plasticity is an inherent limitation in plant variety identification for all morphological descriptor-based methods. Identification accuracy depends on the similarity of the growth conditions between the training and testing leaf datasets. In addition to environmental factors, the growth period is another factor that may influence the generalization ability of morphological descriptor-based methods since leaves at different growth periods may exhibit various morphological features. If a model was trained on leaves from a certain period but used to predict leaves from the other growth periods, the accuracy would probably decrease significantly. Considering this growth period factor, we incorporated leaves across growth periods to train the MFCIS model on our soybean dataset.</p>
      <p id="Par42">In contrast, molecular marker-based technologies are known as powerful tools for the identification of plant varieties. Unlike leaf image-based methods that use morphological characteristics, they solely rely on variety-specific DNA fingerprints for discrimination. They, therefore, are more reliable and robust to different environmental conditions and the growth periods of plants. Furthermore, these methods do not need fully expanded leaves but can be applied in any growth stage using any plant tissues (such as seeds, fruits, and young leaves), making plant growth redundant<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. With these advantages, molecular marker-based methods are being used in many breeding institutes and companies for routine identification in a wide range of ornamental, vegetable, fruit, and cereal crops<sup><xref ref-type="bibr" rid="CR52">52</xref>–<xref ref-type="bibr" rid="CR57">57</xref></sup>. However, their disadvantages are also apparent. First, molecular marker-based technologies are not universally usable since they lack specific primers, polymorphic molecular markers and referable results for many plant varieties<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Second, these methods are destructive for sample preparation, have low efficiency and may cause pollution due to the after-treatment of the chemicals<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. Moreover, the cost is a major issue. Molecular markers are expensive to develop and require a well-equipped laboratory and skilled operational staff to use. The cost has greatly hindered the development and adoption of molecular marker-based techniques in many countries<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>. In conclusion, both leaf image- and molecular-based approaches have applicable scenarios and mutual complements for plant variety identification.</p>
    </sec>
    <sec id="Sec28">
      <title>Machine learning in horticulture research</title>
      <p id="Par43">Another aim of this work is to further machine learning-based plant phenotyping solutions in horticultural research, which is a fast-growing and multidisciplinary research area covering plant biology, advanced sensors, automation technology and big data analysis. The emergence of plant phenotyping in recent years has brought new perspectives to horticulture research. By combining advanced sensors and automation technology, more physiological and morphological traits across plant growth and development can be assessed, e.g., daily evapotranspiration<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>, crop biomass and leaf area indices<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>, seed germination<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>, and biophysical and biochemical traits<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>. In particular, it has been shown that many machine learning- and computer vision-based analytic methods improve phenotyping accuracy, reliability, and speed. However, there are some challenges that were encountered when applying machine learning-based algorithms in our study: (1) the performance of the learning algorithm could be poor if the phenotyping data is insufficient or has low quality; (2) many machine learning models, especially deep neural networks, are difficult to interpret and cannot further improve model performance. Hence, one of the major bottlenecks for plant phenotyping currently is the lack of powerful machining learning- and computer vision-based algorithms to make sense of phenotyping data effectively and accurately.</p>
    </sec>
    <sec id="Sec29">
      <title>Limitations and further work</title>
      <p id="Par44">In the MFCIS algorithm, leaf shape and margin features usually significantly impact cultivar identification<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. For example, all the top-10 sweet cherry cultivars with the highest F<sub>1</sub>-scores had distinctive leaf shape and margin features. However, some poorly identified cultivars with lower F<sub>1</sub>-scores might be distinct in neither leaf shape nor margin features. Although the venation and texture features might produce a marked effect, the inner cultivar difference still made the proposed method fail to recognize some of them. Visualization of feature maps in the MFCIS model may help to detect more discriminative features<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. The most widely used visualization techniques include deconvolution<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>, activation maximization<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>, and saliency maps<sup><xref ref-type="bibr" rid="CR66">66</xref></sup>. For our study, each layer of the CNN had multiple convolutional kernels, and the sizes of the feature maps of deeper convolutional layers were small. These factors made it difficult to detect the potential classification clues by visualizing each filter. A saliency map can be used to identify the relatively important image pixels by computing the gradients of the output against the input image. To locate the vital leaf regions and detect discriminative features from these regions, we generated saliency maps of the misclassified and correctly classified leaves from the same cultivars, as shown in Fig. S<xref rid="MOESM1" ref-type="media">5</xref>. However, we failed to locate the important leaf areas by comparing their saliency maps because the leaf regions with high saliency values varied in correctly classified and misclassified cases. It is essential to point out that the saliency maps shown in Fig. S<xref rid="MOESM1" ref-type="media">5</xref> were generated only from CNN-based feature extraction. We were unable to visualize the topological features with a saliency map. However, the accuracy improvement of the proposed MFCIS method was mainly due to the integration of topological features. Therefore, our future work for interpreting the results and improving the model accuracy will explain the impact of topological features on leaf image classification and propose a quantitative method to assess the contributions of each leaf feature.</p>
      <p id="Par45">When analyzing the soybean leaf image dataset, we developed a multigrowth-period score fusion strategy to utilize the time-series information incorporated in leaves of the same cultivars across growth periods. Although exciting accuracy was achieved, it is worthwhile to explore other data fusion techniques<sup><xref ref-type="bibr" rid="CR67">67</xref>,<xref ref-type="bibr" rid="CR68">68</xref></sup>. Chitwood and colleagues<sup><xref ref-type="bibr" rid="CR69">69</xref></sup> recently reported a composite leaf-modeling method for formulating leaf homologous universal landmarks by their relative positions in shoots over 4 years. It generated composite leaves to capture the spectrum of possible leaf morphologies for cultivar identification. This feature-level fusion strategy may achieve better performance than our score-level fusion approach because the weights of leaf features derived from different growth periods can be optimized during the training process.</p>
    </sec>
  </sec>
  <sec id="Sec30" sec-type="conclusion">
    <title>Conclusions</title>
    <p id="Par46">In this study, we proposed an automatic leaf image-based plant cultivar classification pipeline called MFCIS. Persistent homology was adopted to extract the morphological features of leaf shape, texture, and venation details. A fine-tuned deep CNN, Xception, was employed to extract features from leaf images. The task-optimal weighted leaf morphological features and high-level image features were automatically captured and concatenated in the training process. We obtained promising results for both sweet cherry and soybean cultivar recognition, illustrating the superiority and generalization capacity of the PH and CNN combination strategy for leaf image-based classification applications. A large performance improvement was observed when applying score fusion of different soybean growth periods, indicating that cultivar identification based on leaf morphological features requires the use of leaves collected at different growth periods to improve the identification accuracy.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec31">
      <supplementary-material content-type="local-data" id="MOESM1">
        <media xlink:href="41438_2021_608_MOESM1_ESM.docx">
          <caption>
            <p>Supplementary Materials</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM2">
        <media xlink:href="41438_2021_608_MOESM2_ESM.docx">
          <caption>
            <p>Supplementary User Manual</p>
          </caption>
        </media>
      </supplementary-material>
    </sec>
  </sec>
</body>
<back>
  <sec>
    <title>Supplementary information</title>
    <p>The online version contains supplementary material available at 10.1038/s41438-021-00608-w.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>This work was supported by the National Key Research and Development Program of China (grant no. 2016YFD0101900), Sanya Science and Education Innovation Park of Wuhan University of Technology of China (grant no. 2020KF0053), and the start-up grant from Wuhan University of Technology of China (grant no. 104-40120526). We thank the Soybean Experimental Station of Heilongjiang Academy of Agricultural Sciences for maintaining the soybean field.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>Y.Z., X.Y., Q.L., and W.L. designed and managed the project. Y.Z., J.P., L.Z., J.W., D.Z., P.H., and Q.L. collected the leaf images. Y.Z. generated the image analysis pipeline and the web-based leaf identification platform. J.P., X.Y., and W.L. provided constructive suggestions on the image analysis pipeline. Y.Z., J.P., L.Z., J.W., D.Z., P.H., and Q.L. helped prepare the tables in the manuscript and interpret the plant identification results. Y.Z. and W.L. wrote the manuscript. All authors revised the manuscript and approved the final version.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>The cherry and soybean image datasets are available at <ext-link ext-link-type="uri" xlink:href="http://mfcis.online/">http://mfcis.online/</ext-link>.</p>
    <p>The environment is packaged as a Docker image to facilitate deployment and remove obstacles among different operating systems. All the code and Docker files are available at the source code repository.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>Project name: Multifeature combined plant cultivar identification system</p>
    <p>Project home page: <ext-link ext-link-type="uri" xlink:href="https://github.com/WeizhenLiuBioinform/mfcis">https://github.com/WeizhenLiuBioinform/mfcis</ext-link></p>
    <p>Web-based platform: <ext-link ext-link-type="uri" xlink:href="http://mfcis.online/">http://mfcis.online/</ext-link></p>
    <p>Source code: <ext-link ext-link-type="uri" xlink:href="https://github.com/WeizhenLiuBioinform/mfcis">https://github.com/WeizhenLiuBioinform/mfcis</ext-link></p>
    <p>Operating system(s): Platform-independent</p>
    <p>Programming language: Python 3.6</p>
    <p>Requirements: Keras, TensorFlow, Scikit-image, Homcloud, etc. Detailed requirements are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/WeizhenLiuBioinform/mfcis/blob/master/requirements.txt">https://github.com/WeizhenLiuBioinform/mfcis/blob/master/requirements.txt</ext-link></p>
    <p>License: BSD-3-Clause available at <ext-link ext-link-type="uri" xlink:href="https://opensource.org/licenses/BSD-3-Clause">https://opensource.org/licenses/BSD-3-Clause</ext-link></p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Conflict of interest</title>
    <p id="Par47">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sohn</surname>
            <given-names>HB</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Barcode system for genetic identification of soybean [Glycine max (L.) Merrill] cultivars using InDel markers specific to dense variation blocks</article-title>
        <source>Front. Plant Sci.</source>
        <year>2017</year>
        <volume>8</volume>
        <fpage>520</fpage>
        <pub-id pub-id-type="doi">10.3389/fpls.2017.00520</pub-id>
        <?supplied-pmid 28443113?>
        <pub-id pub-id-type="pmid">28443113</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Korir</surname>
            <given-names>NK</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Plant variety and cultivar identification: advances and prospects</article-title>
        <source>Crit. Rev. Biotechnol.</source>
        <year>2013</year>
        <volume>33</volume>
        <fpage>111</fpage>
        <lpage>125</lpage>
        <pub-id pub-id-type="doi">10.3109/07388551.2012.675314</pub-id>
        <?supplied-pmid 22698516?>
        <pub-id pub-id-type="pmid">22698516</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jamali</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Identification and distinction of soybean commercial cultivars using morphological and microsatellite markers., Iranian</article-title>
        <source>J. Crop Sci.</source>
        <year>2011</year>
        <volume>13</volume>
        <fpage>131</fpage>
        <lpage>145</lpage>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Genetic analysis and molecular characterization of Chinese sesame (Sesamum indicum L.) cultivars using Insertion-Deletion (InDel) and Simple Sequence Repeat (SSR) markers</article-title>
        <source>BMC Genet.</source>
        <year>2014</year>
        <volume>15</volume>
        <fpage>35</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2156-15-35</pub-id>
        <?supplied-pmid 24641723?>
        <pub-id pub-id-type="pmid">24641723</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>SH</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>How deep learning extracts and learns leaf features for plant classification</article-title>
        <source>Pattern Recognit.</source>
        <year>2017</year>
        <volume>71</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patcog.2017.05.015</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>SSF</given-names>
          </name>
          <name>
            <surname>Cham</surname>
            <given-names>W-K</given-names>
          </name>
          <name>
            <surname>Chu</surname>
            <given-names>LM</given-names>
          </name>
        </person-group>
        <article-title>Plant identification using leaf shapes: a pattern counting approach</article-title>
        <source>Pattern Recognit.</source>
        <year>2015</year>
        <volume>48</volume>
        <fpage>3203</fpage>
        <lpage>3215</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patcog.2015.04.004</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Price</surname>
            <given-names>CA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Leaf extraction and analysis framework graphical user interface: segmenting and analyzing the structure of leaf veins and areoles</article-title>
        <source>Plant Physiol.</source>
        <year>2011</year>
        <volume>155</volume>
        <fpage>236</fpage>
        <lpage>245</lpage>
        <pub-id pub-id-type="doi">10.1104/pp.110.162834</pub-id>
        <?supplied-pmid 21057114?>
        <pub-id pub-id-type="pmid">21057114</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>De Vylder</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Vandenbussche</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Rosette tracker: an open source image analysis tool for automatic quantification of genotype effects[J]</article-title>
        <source>Plant physiology</source>
        <year>2012</year>
        <volume>160</volume>
        <fpage>1149</fpage>
        <lpage>1159</lpage>
        <pub-id pub-id-type="doi">10.1104/pp.112.202762</pub-id>
        <?supplied-pmid 22942389?>
        <pub-id pub-id-type="pmid">22942389</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Leaf-GP: an open and automated software application for measuring growth phenotypes for arabidopsis and wheat</article-title>
        <source>Plant Methods</source>
        <year>2017</year>
        <volume>13</volume>
        <fpage>117</fpage>
        <pub-id pub-id-type="doi">10.1186/s13007-017-0266-3</pub-id>
        <?supplied-pmid 29299051?>
        <pub-id pub-id-type="pmid">29299051</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Biot</surname>
            <given-names>E</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Multi-scale quantification of morphodynamics: MorphoLeaf software for 2D shape analysis</article-title>
        <source>Development</source>
        <year>2016</year>
        <volume>143</volume>
        <fpage>3417</fpage>
        <lpage>3428</lpage>
        <?supplied-pmid 27387872?>
        <pub-id pub-id-type="pmid">27387872</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Minervini</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Phenotiki: an open software and hardware platform for affordable and easy image-based phenotyping of rosette-shaped plants</article-title>
        <source>Plant J.</source>
        <year>2017</year>
        <volume>90</volume>
        <fpage>204</fpage>
        <lpage>216</lpage>
        <pub-id pub-id-type="doi">10.1111/tpj.13472</pub-id>
        <?supplied-pmid 28066963?>
        <pub-id pub-id-type="pmid">28066963</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Neto</surname>
            <given-names>JC</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Plant species identification using Elliptic Fourier leaf shape analysis</article-title>
        <source>Computers Electron. Agriculture</source>
        <year>2006</year>
        <volume>50</volume>
        <fpage>121</fpage>
        <lpage>134</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2005.09.004</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Cope, J. S. et al. in <italic>International Symposium on Visual Computing</italic> (eds Bebis, G. et al.) 669–677 (Springer, 2010).</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Chaki, J. &amp; Parekh, R. Plant leaf recognition using shape based features and neural network classifiers, <italic>Int. J. Adv. Comp. Sci. Appl</italic>. <bold>2</bold>, 41–47 (2011).</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Naresh</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Nagendraswamy</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Classification of medicinal plants: an approach using modified LBP with symbolic representation</article-title>
        <source>Neurocomputing</source>
        <year>2016</year>
        <volume>173</volume>
        <fpage>1789</fpage>
        <lpage>1797</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neucom.2015.08.090</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Pradeep Kumar, T., Veera Prasad Reddy, M. &amp; Bora, P. K. Leaf identification using shape and texture features. <italic>Proceedings of International Conference on Computer Vision and Image Processing</italic> (eds Raman B., Kumar S., Roy P. P., Sen D.) 531–541 (Springer Singapore, 2017).</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Tharwat, A., Gaber, T., Awad, Y. M., Dey, N. &amp; Hassanien, A. E. Plants identification using feature fusion technique and bagging classifier. (eds Gaber T., Hassanien A. E., El-Bendary N., Dey N.). <italic>The 1st International Conference on Advanced Intelligent System and Informatics (AISI2015), November 28–30, 2015, Beni Suef, Egypt</italic>. 461–471 (Springer International Publishing, 2016).</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Codizar, A. L. &amp; Solano, G. Plant leaf recognition by venation and shape using artificial neural networks. In: <italic>2016 7th International Conference on Information,Intelligence, Systems &amp; Applications (IISA)</italic>. 1–4 (IEEE, 2016).</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Plant leaf recognition by integrating shape and texture features</article-title>
        <source>Pattern Recognit.</source>
        <year>2021</year>
        <volume>112</volume>
        <fpage>107809</fpage>
        <pub-id pub-id-type="doi">10.1016/j.patcog.2020.107809</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A novel identification method for apple (Malus domestica Borkh.) cultivars based on a deep convolutional neural network with leaf image input</article-title>
        <source>Symmetry</source>
        <year>2020</year>
        <volume>12</volume>
        <fpage>217</fpage>
        <pub-id pub-id-type="doi">10.3390/sym12020217</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Baldi</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A leaf-based back propagation neural network for oleander (Nerium oleander L.) cultivar identification</article-title>
        <source>Computers Electron. Agriculture</source>
        <year>2017</year>
        <volume>142</volume>
        <fpage>515</fpage>
        <lpage>520</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2017.11.021</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">X. Yu, et al. Patchy image structure classification using multi-orientation region transform. in <italic>Proceedings of the AAAI Conference on Artificial Intelligence</italic>. 12741–12748 (AAAI, 2020).</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Edelsbrunner, H &amp; Harer, J. in <italic>Persistent Homology—a Survey</italic> (eds Goodman, J. E., Pach, J., Pollack, R.). 257–282 (Contemporary Mathematics American Mathematical Society, 2008).</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Topological data analysis as a morphometric method: using persistent homology to demarcate a leaf morphospace</article-title>
        <source>Front. Plant Sci.</source>
        <year>2018</year>
        <volume>9</volume>
        <fpage>553</fpage>
        <pub-id pub-id-type="doi">10.3389/fpls.2018.00553</pub-id>
        <?supplied-pmid 29922307?>
        <pub-id pub-id-type="pmid">29922307</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Reininghaus, J. et al. A stable multi-scale kernel for topological machine learning, in: <italic>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 4741–4748 (IEEE, Boston, MA, USA, 2015).</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Li, C., Ovsjanikov, M. &amp; Chazal, F. Persistence-based structural recognition. in <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>. 1995–2002 (IEEE Computer Society, 2014).</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Dey, T., Mandal, S. &amp; Varcho, W. Improved image classification using topological persistence. in <italic>Proceedings of the Conference on Vision, Modeling and Visualization</italic>. 161–168 (Eurographics Association, 2017).</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>MacLane</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Homology</article-title>
        <source>Bull. Am. Math. Soc.</source>
        <year>1964</year>
        <volume>70</volume>
        <fpage>329</fpage>
        <lpage>331</lpage>
        <pub-id pub-id-type="doi">10.1090/S0002-9904-1964-11082-X</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Qaiser, T. et al. Tumor segmentation in whole slide images using persistent homology and deep convolutional features. in <italic>Annual Conference on Medical Image Understanding and Analysis</italic>. 320–329 (Springer, 2017).</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qaiser</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Fast and accurate tumor segmentation of histology images using persistent homology and deep convolutional features</article-title>
        <source>Med. Image Anal.</source>
        <year>2019</year>
        <volume>55</volume>
        <fpage>1</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2019.03.014</pub-id>
        <?supplied-pmid 30991188?>
        <pub-id pub-id-type="pmid">30991188</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zeppelzauer</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A study on topological descriptors for the analysis of 3d surface texture</article-title>
        <source>Computer Vis. Image Underst.</source>
        <year>2018</year>
        <volume>167</volume>
        <fpage>74</fpage>
        <lpage>88</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cviu.2017.10.012</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Chollet, F. Xception: Deep learning with depthwise separable convolutions. in <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>. 1251–1258 (IEEE Computer Society, 2017).</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Hofer, C. et al. Deep learning with topological signatures. In: <italic>Advances in Neural Information Processing Systems</italic>. 1634–1644 (Curran Associates Inc., 2017).</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Turner</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Mukherjee</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Boyer</surname>
            <given-names>DM</given-names>
          </name>
        </person-group>
        <article-title>Persistent homology transform for modeling shapes and surfaces</article-title>
        <source>Inf. Inference.: A J. IMA</source>
        <year>2014</year>
        <volume>3</volume>
        <fpage>310</fpage>
        <lpage>344</lpage>
        <pub-id pub-id-type="doi">10.1093/imaiai/iau011</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Deng, J. et al. Imagenet: a large-scale hierarchical image database. in <italic>2009 IEEE Conference on Computer Vision and Pattern Recognition</italic>. 248–255 (IEEE, 2009).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Adams</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Persistence images: A stable vector representation of persistent homology</article-title>
        <source>J. Mach. Learn. Res.</source>
        <year>2017</year>
        <volume>18</volume>
        <fpage>218</fpage>
        <lpage>252</lpage>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bubenik</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Statistical topological data analysis using persistence landscapes</article-title>
        <source>J. Mach. Learn. Res.</source>
        <year>2015</year>
        <volume>16</volume>
        <fpage>77</fpage>
        <lpage>102</lpage>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>From species to cultivar: Soybean cultivar recognition using joint leaf image patterns by multi-scale sliding chord matching</article-title>
        <source>Biosyst. Eng.</source>
        <year>2020</year>
        <volume>194</volume>
        <fpage>99</fpage>
        <lpage>111</lpage>
        <pub-id pub-id-type="doi">10.1016/j.biosystemseng.2020.03.019</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Heiberger, R. M., &amp; Neuwirth E. One-way ANOVA<italic>. In: R through Excel</italic>. 165–191 (Springer, 2009).</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ling</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Jacobs</surname>
            <given-names>DW</given-names>
          </name>
        </person-group>
        <article-title>Shape classification using the inner-distance</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2007</year>
        <volume>29</volume>
        <fpage>286</fpage>
        <lpage>299</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2007.41</pub-id>
        <?supplied-pmid 17170481?>
        <pub-id pub-id-type="pmid">17170481</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Wang, B. &amp; Gao, Y. Hierarchical string cuts: a translation, rotation, scale, and mirror invariant descriptor for fast shape retrieval. <italic>IEEE Trans. Image Process</italic><bold>23</bold>, 4101–4111 (2014).</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kaya</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Analysis of transfer learning for deep neural network-based plant classification models</article-title>
        <source>Computers Electron. Agriculture</source>
        <year>2019</year>
        <volume>158</volume>
        <fpage>20</fpage>
        <lpage>29</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2019.01.041</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Yanping, Z. &amp; Liu, W. WeizhenLiuBioinform/mfcis: source code of mfcis. (Version 1.0.2). <italic>Zenodo</italic>10.5281/zenodo.4739746 (2021).</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Barré</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>LeafNet: a computer vision system for automatic plant species identification</article-title>
        <source>Ecol. Inform.</source>
        <year>2017</year>
        <volume>40</volume>
        <fpage>50</fpage>
        <lpage>56</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ecoinf.2017.05.005</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Beghin, T. et al. Shape and texture-based plant leaf classification. in <italic>International Conference on Advanced Concepts for Intelligent Vision Systems</italic>, 345–353 (Springer, 2010).</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blonder</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>X-ray imaging of leaf venation networks</article-title>
        <source>N. Phytologist</source>
        <year>2012</year>
        <volume>196</volume>
        <fpage>1274</fpage>
        <lpage>1282</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1469-8137.2012.04355.x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gan</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automatic hierarchy classification in venation networks using directional morphological filtering for hierarchical structure traits extraction</article-title>
        <source>Computational Biol. Chem.</source>
        <year>2019</year>
        <volume>80</volume>
        <fpage>187</fpage>
        <lpage>194</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiolchem.2019.03.012</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cui</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Score level fusion of fingerprint and finger vein recognition</article-title>
        <source>J. Computational Inf. Syst.</source>
        <year>2011</year>
        <volume>7</volume>
        <fpage>5723</fpage>
        <lpage>5731</lpage>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Park</surname>
            <given-names>H-A</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>KR</given-names>
          </name>
        </person-group>
        <article-title>Iris recognition based on score level fusion by using SVM</article-title>
        <source>Pattern Recognit. Lett.</source>
        <year>2007</year>
        <volume>28</volume>
        <fpage>2019</fpage>
        <lpage>2028</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patrec.2007.05.017</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ghosh</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Software for systems biology: from tools to integrated platforms</article-title>
        <source>Nat. Rev. Genet.</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>821</fpage>
        <lpage>832</lpage>
        <pub-id pub-id-type="doi">10.1038/nrg3096</pub-id>
        <?supplied-pmid 22048662?>
        <pub-id pub-id-type="pmid">22048662</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">Smulders, M., Booy, I. &amp; Vosman, B. Use of molecular and biochemical methods for identification of plant varieties throughout the agri-chain. (eds Trienekens, J. H. &amp; Zuurbier, P. J. P.) In <italic>Proceedings of the 2nd International Conference on Chain Management in Agri-and Food Business</italic>. 591–600 (Department of Management studies Wageningen Agricultural University, May 1996).</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Park</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Molecular identification of sweet potato accessions using ARMS-PCR based on SNPs</article-title>
        <source>J. Plant Biotechnol.</source>
        <year>2020</year>
        <volume>47</volume>
        <fpage>124</fpage>
        <lpage>130</lpage>
        <pub-id pub-id-type="doi">10.5010/JPB.2020.47.2.124</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fufa</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Comparison of phenotypic and molecular marker-based classifications of hard red winter wheat cultivars</article-title>
        <source>Euphytica</source>
        <year>2005</year>
        <volume>145</volume>
        <fpage>133</fpage>
        <lpage>146</lpage>
        <pub-id pub-id-type="doi">10.1007/s10681-005-0626-3</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Genome-wide SNP discovery and core marker sets for DNA barcoding and variety identification in commercial tomato cultivars</article-title>
        <source>Sci. Horticulturae</source>
        <year>2021</year>
        <volume>276</volume>
        <fpage>109734</fpage>
        <pub-id pub-id-type="doi">10.1016/j.scienta.2020.109734</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Patzak</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Henychová</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Paprštein</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Sedlák</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Evaluation of S-incompatibility locus, genetic diversity and structure of sweet cherry (Prunus avium L.) genetic resources by molecular methods and phenotypic characteristics</article-title>
        <source>J. Horticultural Sci. Biotechnol.</source>
        <year>2020</year>
        <volume>95</volume>
        <fpage>84</fpage>
        <lpage>92</lpage>
        <pub-id pub-id-type="doi">10.1080/14620316.2019.1647798</pub-id>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pourkhaloee</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Molecular analysis of genetic diversity, population structure, and phylogeny of wild and cultivated tulips (Tulipa L.) by genic microsatellites</article-title>
        <source>Horticulture Environ. Biotechnol.</source>
        <year>2018</year>
        <volume>59</volume>
        <fpage>875</fpage>
        <lpage>888</lpage>
        <pub-id pub-id-type="doi">10.1007/s13580-018-0055-6</pub-id>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cho</surname>
            <given-names>KH</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Sequence-characterized amplified region markers and multiplex-polymerase chain reaction assays for kiwifruit cultivar identification</article-title>
        <source>Horticulture Environ., Biotechnol.</source>
        <year>2020</year>
        <volume>61</volume>
        <fpage>395</fpage>
        <lpage>406</lpage>
        <pub-id pub-id-type="doi">10.1007/s13580-020-00227-9</pub-id>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Agarwal</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Shrivastava</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Padh</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Advances in molecular marker techniques and their applications in plant sciences</article-title>
        <source>Plant Cell Rep.</source>
        <year>2008</year>
        <volume>27</volume>
        <fpage>617</fpage>
        <lpage>631</lpage>
        <pub-id pub-id-type="doi">10.1007/s00299-008-0507-z</pub-id>
        <?supplied-pmid 18246355?>
        <pub-id pub-id-type="pmid">18246355</pub-id>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nadeem</surname>
            <given-names>MA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DNA molecular markers in plant breeding: current status and recent advancements in genomic selection and genome editing</article-title>
        <source>Biotechnol. Biotechnological Equip.</source>
        <year>2018</year>
        <volume>32</volume>
        <fpage>261</fpage>
        <lpage>285</lpage>
        <pub-id pub-id-type="doi">10.1080/13102818.2017.1400401</pub-id>
      </element-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yamaç</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Todorovic</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Estimation of daily potato crop evapotranspiration using three different machine learning algorithms and four scenarios of available meteorological data</article-title>
        <source>Agric. Water Manag.</source>
        <year>2020</year>
        <volume>228</volume>
        <fpage>105875</fpage>
        <pub-id pub-id-type="doi">10.1016/j.agwat.2019.105875</pub-id>
      </element-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reisi Gahrouei</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>McNairn</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Hosseini</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Homayouni</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Estimation of crop biomass and leaf area index from multitemporal and multispectral imagery using machine learning approaches</article-title>
        <source>Can. J. Remote Sens.</source>
        <year>2020</year>
        <volume>46</volume>
        <fpage>84</fpage>
        <lpage>99</lpage>
        <pub-id pub-id-type="doi">10.1080/07038992.2020.1740584</pub-id>
      </element-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Colmer</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SeedGerm: a cost-effective phenotyping platform for automated seed imaging and machine-learning based phenotypic analysis of crop seed germination</article-title>
        <source>N. Phytologist</source>
        <year>2020</year>
        <volume>228</volume>
        <fpage>778</fpage>
        <lpage>793</lpage>
        <pub-id pub-id-type="doi">10.1111/nph.16736</pub-id>
      </element-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Danner</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Berger</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Wocher</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mauser</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Hank</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Efficient RTM-based training of machine learning regression algorithms to quantify biophysical &amp; biochemical traits of agricultural crops</article-title>
        <source>ISPRS J. Photogramm. Remote Sens.</source>
        <year>2021</year>
        <volume>173</volume>
        <fpage>278</fpage>
        <lpage>296</lpage>
        <pub-id pub-id-type="doi">10.1016/j.isprsjprs.2021.01.017</pub-id>
      </element-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <mixed-citation publication-type="other">Zeiler, M. D. &amp; Fergus R. in <italic>Visualizing and Understanding Convolutional Networks</italic> (eds Fleet D., Pajdla T., Schiele B., Tuytelaars T.). <italic>Computer Vision–ECCV 2014</italic>. 818–833 (Springer International Publishing, 2014).</mixed-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Erhan</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Courville</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Vincent</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Visualizing higher-layer features of a deep network</article-title>
        <source>Univ. Montr.</source>
        <year>2009</year>
        <volume>1341</volume>
        <fpage>1</fpage>
      </element-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <mixed-citation publication-type="other">Simonyan, K., Vedaldi, A. &amp; Zisserman, A. Deep inside convolutional networks: Visualising image classification models and saliency maps[C]//InWorkshop at International Conference on Learning Representations. (2014).</mixed-citation>
    </ref>
    <ref id="CR67">
      <label>67.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Islam</surname>
            <given-names>MR</given-names>
          </name>
        </person-group>
        <article-title>Feature and score fusion based multiple classifier selection for iris recognition</article-title>
        <source>Computational Intell. Neurosci.</source>
        <year>2014</year>
        <volume>2014</volume>
        <fpage>e380585</fpage>
        <pub-id pub-id-type="doi">10.1155/2014/380585</pub-id>
      </element-citation>
    </ref>
    <ref id="CR68">
      <label>68.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Feature fusion: parallel strategy vs. serial strategy</article-title>
        <source>Pattern Recognit.</source>
        <year>2003</year>
        <volume>36</volume>
        <fpage>1369</fpage>
        <lpage>1381</lpage>
        <pub-id pub-id-type="doi">10.1016/S0031-3203(02)00262-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR69">
      <label>69.</label>
      <mixed-citation publication-type="other">Bryson, A. E. et al. Composite modeling of leaf shape across shoots discriminates Vitis species better than individual leaves. Preprint at <italic>bioRxiv</italic>10.1101/2020.06.22.163899 (2020).</mixed-citation>
    </ref>
  </ref-list>
</back>
