<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6886168</article-id>
    <article-id pub-id-type="publisher-id">3076</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-019-3076-y</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepEP: a deep learning framework for identifying essential proteins</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Zeng</surname>
          <given-names>Min</given-names>
        </name>
        <address>
          <email>zengmin@csu.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Li</surname>
          <given-names>Min</given-names>
        </name>
        <address>
          <email>limin@mail.csu.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wu</surname>
          <given-names>Fang-Xiang</given-names>
        </name>
        <address>
          <email>faw341@mail.usask.ca</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Yaohang</given-names>
        </name>
        <address>
          <email>yaohang@cs.odu.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pan</surname>
          <given-names>Yi</given-names>
        </name>
        <address>
          <email>yipan@gsu.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0379 7164</institution-id><institution-id institution-id-type="GRID">grid.216417.7</institution-id><institution>School of Computer Science and Engineering, Central South University, </institution></institution-wrap>Changsha, 410083 People’s Republic of China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2154 235X</institution-id><institution-id institution-id-type="GRID">grid.25152.31</institution-id><institution>Division of Biomedical Engineering and Department of Mechanical Engineering, </institution><institution>University of Saskatchewan, </institution></institution-wrap>Saskatoon, SKS7N5A9 Canada </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2164 3177</institution-id><institution-id institution-id-type="GRID">grid.261368.8</institution-id><institution>Department of Computer Science, </institution><institution>Old Dominion University, </institution></institution-wrap>Norfolk, VA23529 USA </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 7400</institution-id><institution-id institution-id-type="GRID">grid.256304.6</institution-id><institution>Department of Computer Science, </institution><institution>Georgia State University, </institution></institution-wrap>Atlanta, GA30302 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>2</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>2</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <issue>Suppl 16</issue>
    <issue-sponsor>Publication of this supplement has not been supported by sponsorship. Information about the source of funding for publication charges can be found in the individual articles. The articles have undergone the journal's standard peer review process for supplements. XH was an author of one of the papers in this supplement but was not involved in the process of its selection and review. No other competing interests were declared.</issue-sponsor>
    <elocation-id>506</elocation-id>
    <permissions>
      <copyright-statement>© The Author(s). 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Essential proteins are crucial for cellular life and thus, identification of essential proteins is an important topic and a challenging problem for researchers. Recently lots of computational approaches have been proposed to handle this problem. However, traditional centrality methods cannot fully represent the topological features of biological networks. In addition, identifying essential proteins is an imbalanced learning problem; but few current shallow machine learning-based methods are designed to handle the imbalanced characteristics.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We develop DeepEP based on a deep learning framework that uses the node2vec technique, multi-scale convolutional neural networks and a sampling technique to identify essential proteins. In DeepEP, the node2vec technique is applied to automatically learn topological and semantic features for each protein in protein-protein interaction (PPI) network. Gene expression profiles are treated as images and multi-scale convolutional neural networks are applied to extract their patterns. In addition, DeepEP uses a sampling method to alleviate the imbalanced characteristics. The sampling method samples the same number of the majority and minority samples in a training epoch, which is not biased to any class in training process. The experimental results show that DeepEP outperforms traditional centrality methods. Moreover, DeepEP is better than shallow machine learning-based methods. Detailed analyses show that the dense vectors which are generated by node2vec technique contribute a lot to the improved performance. It is clear that the node2vec technique effectively captures the topological and semantic properties of PPI network. The sampling method also improves the performance of identifying essential proteins.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par3">We demonstrate that DeepEP improves the prediction performance by integrating multiple deep learning techniques and a sampling method. DeepEP is more effective than existing methods.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Deep learning</kwd>
      <kwd>Identifying essential proteins</kwd>
      <kwd>node2vec</kwd>
      <kwd>Imbalanced learning</kwd>
      <kwd>Protein-protein interaction network</kwd>
      <kwd>Multi-scale convolutional neural networks</kwd>
    </kwd-group>
    <conference xlink:href="http://orienta.ugr.es/bibm2018/">
      <conf-name>IEEE International Conference on Bioinformatics and Biomedicine 2018</conf-name>
      <conf-loc>Madrid, Spain</conf-loc>
      <conf-date>3-6 December 2018</conf-date>
    </conference>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par11">Essential proteins are indispensable for organisms and play a very important role in maintaining cellular life [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>]. Determination of essential proteins not only helps us understand the basic requirements of a cell at a molecular level, but also helps identifying essential genes and finding potential drug targets. Thus identifying essential proteins is very important for researchers. There are several biological experimental methods to identify essential proteins, such as RNA interference [<xref ref-type="bibr" rid="CR3">3</xref>], conditional knockout [<xref ref-type="bibr" rid="CR4">4</xref>], and single gene knockout [<xref ref-type="bibr" rid="CR5">5</xref>]. But these methods require lots of resources and time. Moreover, in some complex organisms, these methods are not always applicable. Considering these experimental constraints, it is appealing to develop an accurate and effective computational approach for identifying essential proteins.</p>
    <p id="Par12">Existing computational approaches can be roughly divided into two categories: centrality methods and shallow machine learning-based methods. Jeong et al. [<xref ref-type="bibr" rid="CR6">6</xref>] proposed centrality-lethality rule which point out that the highly connected proteins in a PPI network tend to be essential. Based on this rule, a lot of centrality methods have been proposed [<xref ref-type="bibr" rid="CR7">7</xref>–<xref ref-type="bibr" rid="CR12">12</xref>]. Meanwhile, researchers began to integrate more different useful biological information to identify essential proteins. A lot of different types of biological information, such as gene expression profiles [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR14">14</xref>], subcellular localization information [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>], protein domains [<xref ref-type="bibr" rid="CR17">17</xref>], orthologous information [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR19">19</xref>], GO annotation and RNA-Seq data [<xref ref-type="bibr" rid="CR20">20</xref>], have been used in various studies.</p>
    <p id="Par13">With the rapid development of high-throughput sequencing technique, we can easily get a lot of biological data which provide a solid foundation of using machine learning methods [<xref ref-type="bibr" rid="CR21">21</xref>]. Generally, researchers develop a machine learning method for prediction according to the following steps: select some useful features (in this case, topological features of a PPI network), construct training and testing datasets, select an appropriate machine learning algorithm, and evaluate the performance of the algorithm. A number of shallow machine learning-based methods including support vector machine (SVM) [<xref ref-type="bibr" rid="CR22">22</xref>], ensemble learning-based model [<xref ref-type="bibr" rid="CR23">23</xref>], Naïve Bayes [<xref ref-type="bibr" rid="CR24">24</xref>], decision tree [<xref ref-type="bibr" rid="CR25">25</xref>] and genetic algorithm [<xref ref-type="bibr" rid="CR26">26</xref>], are wildly used in identification of essential proteins.</p>
    <p id="Par14">Both centrality methods and shallow machine learning-based methods perform well, but each has some limitations. For centrality methods, current methods predict essential proteins by using a function to characterize the topological features of PPI networks according to their prior domain knowledge. But when the PPI network is very complicated (such as thousands of proteins and tens of thousands of protein-protein interactions), the function cannot characterize the topological features of such a complicated PPI network due to the output of the function is just a scalar [<xref ref-type="bibr" rid="CR27">27</xref>, <xref ref-type="bibr" rid="CR28">28</xref>]. For shallow machine learning-based methods, the first step is selecting features. They usually select features by manual feature selection, which may pose a theoretical limitation to explain why these topological features are chosen in this study and depend heavily on the prior knowledge of researchers. In addition, identifying essential proteins is an imbalanced learning problem due to the number of non-essential proteins is much larger than the number of essential proteins. Data imbalance usually hinders the performance of machine learning methods, but few current shallow machine learning-based methods are designed to handle the imbalanced learning in essential proteins prediction.</p>
    <p id="Par15">To tackle the above limitations and further improve machine learning methods for identifying essential proteins, we propose DeepEP, a deep learning framework for identifying essential proteins. Recently, deep learning methods have been applied to represent network information and learn network topological features. They achieve the state-of-the-art performance in lots of applications [<xref ref-type="bibr" rid="CR29">29</xref>, <xref ref-type="bibr" rid="CR30">30</xref>]. Inspired by their success, we aim to investigate whether deep learning methods could achieve notable improvements in the field of identifying essential proteins as well. We believe that deep learning techniques can be used to obtain better representation and thus improve performance. In particular, we employ the node2vec technique to encode a PPI network into a low-dimensional space, and then learn a low-dimensional dense vector for each protein in the PPI network. The low-dimensional dense vector represents the topological features of the corresponding protein. Using the node2vec technique has two advantages: (i) it provides a vector representation for a protein, this vector has a richer representation for topological features of a PPI network than a scalar; (ii) the node2vec technique can automatically learn vector representations from a PPI network and thus not require to choose some topological features. In addition, we use a sampling method to alleviate the imbalanced learning problem. The sampling method samples the same number of the negative samples (non-essential proteins) and positive samples (essential proteins) in a training epoch, and thus ensures the results are not biased to any class in training process. We use this strategy in many training epochs and can make full use of all non-essential proteins to train DeepEP with a high probability. In addition to overcoming the above limitations, DeepEP also uses other deep learning techniques to improve prediction performance. In this study, we use a PPI network dataset and gene expression profiles for training. For gene expression profiles, we transform them to images and thus we can use some deep learning techniques to better extract their patterns. Multi-scale convolutional neural network (CNN) is a newly developed deep learning architecture and is powerful for pattern extraction. We utilize it to extract more effective patterns of gene expression profiles.</p>
    <p id="Par16">To demonstrate the effectiveness of DeepEP, we perform extensive experiments on <italic>S. cerevisiae</italic> dataset. The experimental results show that DeepEP achieves better performance than traditional centrality methods and outperforms the shallow machine learning-based methods. To discover the vital element of DeepEP, we compare the results obtained by node2vec technique with those of 6 central methods. Detailed ablation study shows that the dense vectors which are generated by node2vec technique contribute a lot to the improved performance. Additionally, the sampling method also helps to improve the performance of identifying essential proteins.</p>
  </sec>
  <sec id="Sec2">
    <title>Materials and methods</title>
    <sec id="Sec3">
      <title>Overview: DeepEP</title>
      <p id="Par17">We propose a novel deep learning framework, DeepEP, for identifying essential proteins. Figure <xref rid="Fig1" ref-type="fig">1</xref> illustrates the architecture of DeepEP. It consists of two major modules: a feature extraction module and a classification module. DeepEP accepts two kinds of biological datasets (PPI network dataset and gene expression profiles) as inputs. In the feature extraction module, the node2vec technique is applied to automatically learn a dense vector for each protein in a PPI network to capture the semantic and topological features of the biological network. Gene expression profiles are treated as images, and thus multi-scale CNN is applied to extract patterns. After multi-scale convolution layer, the pooling layer is used to perform dimension reduction. Then, the outputs of each component (node2vec technique, multi-scale CNN and pooling layer) are concatenated together as the inputs for classification module. The classification module consists of a fully connected layer and an output layer. A rectified linear unit (ReLU) function is applied to the fully connected layer as the activation function. After the fully connected layer, another fully connected layer with softmax activation function as output layer predicts the final label of a protein. In addition to using deep learning techniques, we also use a sampling method to alleviate the imbalanced learning problem. The details of the sampling method will be discussed in sampling method section.
<fig id="Fig1"><label>Fig. 1</label><caption><p>The architecture of our deep learning framework for identifying essential proteins</p></caption><graphic xlink:href="12859_2019_3076_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec4">
      <title>Network representation learning</title>
      <p id="Par18">As mentioned in the previous section, researchers need to select some useful features to accomplish the development of machine learning approach. Selecting PPI topological features is a very critical step in the study. Over the past 10 years, researchers proposed many effective computational methods to predict essential proteins based on network topological features such as DC, BC, CC, EC and so on. However, it is still difficult to select some centrality indexes from them. Traditional feature selection method used in identifying essential proteins is manual feature selection. There are two disadvantages in manual feature selection. The first one is that we have to must lots of prior knowledge about essential proteins. The second one is the selected topological feature is a scalar which cannot represent the complex topological features of a PPI network. To address the two issues, we use network representation learning technique to obtain biological features from a PPI network. Different from manual feature selection, network representation learning can automatically learn a low-dimensional dense vector for each protein in the biological network to represent the semantic and topological features. By using this technique, a dense vector which has more powerful representation than a scalar can be obtained and thus, it can improve the performance [<xref ref-type="bibr" rid="CR31">31</xref>].</p>
      <p id="Par19">Various network representation learning techniques have been proposed in recent years [<xref ref-type="bibr" rid="CR32">32</xref>]. Specifically, we used the node2vec technique [<xref ref-type="bibr" rid="CR33">33</xref>] which can learn dense vector representations of vertexes in network based on deep learning methods. It uses biased random walk algorithm to generate a corpus which consists of every vertex’s sequence for training, and aims to predict the context of the given center node by maximizing the co-occurrence likelihood function. The node2vec technique can explore different types of networks and obtain richer topological representation of the network than traditional methods.</p>
    </sec>
    <sec id="Sec5">
      <title>Sampling method</title>
      <p id="Par20">Data imbalance is a very common phenomenon in real-world and we must take it into consideration in machine learning field. The imbalance problem is encountered in prediction of essential proteins. The classes that have more data instances are defined as the majority class, while the ones with fewer instances are the minority class. In the essential proteins dataset we used, the essential proteins belong to the minority class and non-essential proteins belong to the majority class. The imbalanced nature of data poses a challenge for identifying essential proteins. Most traditional machine learning methods usually bias towards the majority class and hence lead to loss of predictive performance for the minority class. Here our focus is to identify the essential proteins out of many non-essential ones, which requires us to tackle the problem of data imbalance effectively.</p>
      <p id="Par21">Previous studies have made great efforts to alleviate the imbalanced data learning problem. Sampling methods are the most wildly used and very effective methods [<xref ref-type="bibr" rid="CR34">34</xref>–<xref ref-type="bibr" rid="CR36">36</xref>]. However, we cannot direct use traditional sampling methods (random oversampling and SMOTE) in DeepEP due to the high consumption of computer resources. The vector which is fed to the classification module is a high-dimensional vector, and we do not want to synthesize any new samples for training based on the raw high-dimensional vector.</p>
      <p id="Par22">To alleviate the imbalanced learning problem, we use a low-computational cost sampling method. M and N denote the number of minority class samples (essential proteins) and the number of majority class samples (non-essential proteins), respectively. In each epoch, we sample M instances from the majority class, and then combine the M instances in the majority class and all instances in the minority class as a new subset to train DeepEP. We carry out this process k times to train DeepEP. The main advantage of using this sampling method is that it can ensure the results are not biased to any class in training process. Figure <xref rid="Fig2" ref-type="fig">2</xref> gives the illustration of the sampling method.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Illustration of the used sampling method</p></caption><graphic xlink:href="12859_2019_3076_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par23">In addition to the above advantage, the sampling method can make full use of all instances in the majority class of the raw dataset to train the deep learning model. In the above sampling process, at each epoch, the probability that a non-essential protein instance is picked is M/N. Therefore, for a specific non-essential protein, the probability that a non-essential protein is not picked at least once after k draws is:
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{p}={\left(1-\mathrm{M}/\mathrm{N}\right)}^{\mathrm{k}} $$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mi mathvariant="normal">p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">M</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="normal">k</mml:mi></mml:msup></mml:math><graphic xlink:href="12859_2019_3076_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par24">In order to make this probability as small as possible, we can specify a threshold α to control it. If α is as small as possible, we believe that we have sampled all majority class instances of the raw dataset.
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\left(1-\mathrm{M}/\mathrm{N}\right)}^{\mathrm{k}}&lt;\upalpha $$\end{document}</tex-math><mml:math id="M4" display="block"><mml:msup><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">M</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="normal">k</mml:mi></mml:msup><mml:mo>&lt;</mml:mo><mml:mi mathvariant="normal">α</mml:mi></mml:math><graphic xlink:href="12859_2019_3076_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par25">In this study, we set α =0.001, the training times k can be determined by Eq. (<xref rid="Equ2" ref-type="">2</xref>).</p>
    </sec>
    <sec id="Sec6">
      <title>Multi-scale architecture</title>
      <p id="Par26">In order to better capture the patterns of gene expression profiles, we treat them as images. A gene expression profile has three successive metabolic cycles and each cycle has 12 time points. It is natural to regard one gene expression profile as an image with 1 channel * 3 rows * 12 columns, and thus some related techniques in computer vision can be applied in feature extraction for essential proteins prediction. Deep learning techniques have been successfully applied in computer vision and CNN is the most wildly used network architecture. CNN uses convolutional filters to extract local features [<xref ref-type="bibr" rid="CR37">37</xref>] from raw images and multi-scale CNN uses different kernels to extract local contextual features [<xref ref-type="bibr" rid="CR38">38</xref>]. By using different kernels, we obtain different information of different spatial scales. The combination of the information from the different scales can help to improve the prediction task. Figure <xref rid="Fig1" ref-type="fig">1</xref> shows the illustration of how a gene expression profile is treated as an image.</p>
    </sec>
    <sec id="Sec7">
      <title>Assessment metrics</title>
      <p id="Par27">In order to evaluate the performance of DeepEP and other methods, in this study, we used six measures: accuracy, precision, recall, F-measure, area under the curve (AUC), and average precision (AP) score. Accuracy, precision, recall and F-measure are the most frequently used metrics in machine learning classification, they are defined as:
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ Accuracy=\left( TP+ TN\right)/\left( TP+ TN+ FP+ FN\right) $$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mtext mathvariant="italic">Accuracy</mml:mtext><mml:mo>=</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">TN</mml:mi></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2019_3076_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ precision= TP/\left( TP+ FP\right) $$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mtext mathvariant="italic">precision</mml:mtext><mml:mo>=</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>/</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2019_3076_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ recall= TP/\left( TP+ FN\right) $$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mtext mathvariant="italic">recall</mml:mtext><mml:mo>=</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>/</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2019_3076_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ F- measure=\frac{2\ast precision\ast recall}{precision+ recall} $$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mi>F</mml:mi><mml:mo>−</mml:mo><mml:mtext mathvariant="italic">measure</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>∗</mml:mo><mml:mtext mathvariant="italic">precision</mml:mtext><mml:mo>∗</mml:mo><mml:mtext mathvariant="italic">recall</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">precision</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">recall</mml:mtext></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="12859_2019_3076_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par28">AUC is defined as the area under the Receiver Operating Characteristic (ROC) curve and ROC curve is a commonly used tool of visualizing performance of a classifier. AP score is defined as the area under the precision-recall (PR) curve and this assessment metric is widely used for evaluating identification of essential proteins. Note that F-measure, AUC, and AP score are more important than accuracy, precision and recall in an imbalanced learning problem due to they can offer a comprehensive assessment of a machine learning classifier.</p>
    </sec>
    <sec id="Sec8">
      <title>Datasets</title>
      <p id="Par29">We use three kinds of biological datasets in our experiments: PPI network dataset, essential proteins dataset, and gene expression profiles. The PPI network dataset is collected from BioGRID database [<xref ref-type="bibr" rid="CR39">39</xref>]. To eliminate the noise of the dataset, we removed self-interactions and repeated interactions. There are 5616 proteins and 52,833 protein-protein interactions in the preprocessed PPI network dataset. The essential proteins dataset is collected from the four databases: MIPS [<xref ref-type="bibr" rid="CR40">40</xref>], SGD [<xref ref-type="bibr" rid="CR41">41</xref>], DEG [<xref ref-type="bibr" rid="CR42">42</xref>], and SGDP. We removed some overlap proteins and integrated the information of the four databases. The preprocessed dataset of essential proteins contains 1199 essential proteins. The gene expression profiles dataset is collected from GEO database (accession number: GSE3431). It consists of 6776 gene products (proteins) and 36 samples. There are three successive metabolic cycles and each cycle has 12 time points.</p>
    </sec>
  </sec>
  <sec id="Sec9">
    <title>Results and discussion</title>
    <sec id="Sec10">
      <title>Implementation details</title>
      <p id="Par30">In our experiments, we first employ the node2vec technique to generate network representation vectors. Each protein in PPI network is represented by a 64-dimensional vector. Our deep learning framework is implemented by Tensorflow which is a wildly used deep learning system [<xref ref-type="bibr" rid="CR43">43</xref>, <xref ref-type="bibr" rid="CR44">44</xref>]. Multi-scale CNN layers with kernel size 1, 3, and 5 are utilized to extract contextual features of gene expression profiles. By using multi-scale CNN layer we obtain 3 feature maps, each having 8 channels. These feature maps are concatenated together as the extracted contextual feature vector. Then the output of multi-scale CNN layer is fed to the maxpooling layer. After maxpooling layer, the output vectors and network representation vectors generated by node2vec are concatenated, and then the concatenated vector is fed to a fully connected layer which has 312 nodes with ReLU activation function. To avoid overfitting, a dropout rate of 0.1 is applied in DeepEP on fully connected layer. Finally, we train our deep learning framework using the Adam optimizer. The batch size is set to 32 and initial learning rate is set to 0.001.</p>
    </sec>
    <sec id="Sec11">
      <title>Comparison with other centrality methods</title>
      <p id="Par31">To demonstrate the effectiveness of DeepEP, we compared it with several popular centrality methods for essential proteins prediction. Eight centrality methods are used for the comparison. These centrality methods are used in following way. First, we compute the values of proteins in PPI network using each centrality method. Second, we rank their scores in descending order. Third, the top 1185 proteins are selected as candidate essential proteins. Last, we calculate precision, recall, F-measure and accuracy according to the true labels of proteins. The results of predicting essential proteins for each compared methods are shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. As shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>, the results of DeepEP outperform the other centrality methods. For instance, the F-measure of DeepEP achieves the highest value. Similarity, other assessment metrics of DeepEP significantly are higher than those of other centrality methods. These results demonstrate the effectiveness of DeepEP for identifying essential proteins.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Performance of DeepEP, DC, BC, CC, EC, NC, LAC, PeC, and WDC</p></caption><graphic xlink:href="12859_2019_3076_Fig3_HTML" id="MO3"/></fig></p>
    </sec>
    <sec id="Sec12">
      <title>Comparison with shallow machine learning-based methods</title>
      <p id="Par32">Machine learning-based methods are widely used in predicting essential proteins. SVM and ensemble learning-based model are the two most commonly used shallow machine learning-based methods. Besides, decision tree and Naïve Bayes are very popular methods. Thus these shallow machine learning methods (SVM, ensemble learning-based model, decision tree, Naïve Bayes) are compared to DeepEP. All of these shallow machine learning methods are implemented by scikit-learn python library with default parameters. We shuffle all samples in raw dataset and then split raw dataset into training dataset and testing dataset. Training dataset is composed of 80% samples of raw dataset and the rest samples constitute testing dataset. In both the training and the testing datasets, the ratio of positive samples (essential proteins) and negative samples (non-essential proteins) remains the same. We use two different ways to compare the machine learning-based methods. First, we directly utilize raw training dataset for training and testing on testing dataset. Second, we first apply the random undersampling technique to draw M (number of essential protein samples) samples from non-essential protein set of training dataset. Then we combine the selected non-essential proteins and all essential proteins together as input data to train machine learning models. The overall performance of all machine learning and deep learning algorithms are evaluated using testing dataset. To ensure a fair comparison, the input features are the same.</p>
      <p id="Par33">Table <xref rid="Tab1" ref-type="table">1</xref> gives a comparison of the experimental results of DeepEP with other shallow machine learning-based methods using different ratios for training. As shown in Table <xref rid="Tab1" ref-type="table">1</xref>, we can see that the imbalanced nature of dataset hampers the mining of machine learning methods. F-measure and AUC increase from 0.21 and 0.72 (raw dataset) to 0.23 and 0.75 (1:1) by using random undersampling technique for SVM, from 0.35 and 0.58 (raw dataset) to 0.50 and 0.69 (1:1) for decision tree, from 0.27 and 0.70 (raw dataset) to 0.43 and 0.78 (1:1) for random forest, from 0.42 and 0.73 (raw dataset) to 0.43 and 0.75 (1:1) for Adaboost, and from 0.42 and 0.70 (raw dataset) to 0.44 and 0.71 (1:1) for Naïve Bayes. Other metrics of accuracy, precision and recall obtained in this work are also improved by using random undersampling technique except for the accuracy and precision of Adaboost (raw dataset). Our results show that it is necessary to consider the imbalanced nature of dataset. In addition, from Table <xref rid="Tab1" ref-type="table">1</xref>, we conclude that DeepEP outperforms other machine learning-based methods. For instance, the F-measure and AUC of DeepEP are 0.55 and 0.82, respectively. They are higher than those of SVM (best performance: 0.23 and 0.75), decision tree (best performance: 0.50 and 0.69), random forest (best performance: 0.43 and 0.78), Adaboost (best performance: 0.43 and 0.75) and Naïve Bayes (best performance: 0.44 and 0.71).
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Performance of DeepEP and other shallow machine learning–based methods with different ratios</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Machine learning algorithms</th><th>Accuracy</th><th>Precision</th><th>Recall</th><th>F-measure</th><th>AUC</th></tr></thead><tbody><tr><td>SVM (raw dataset)</td><td>0.809</td><td>0.71</td><td>0.12</td><td>0.21</td><td>0.72</td></tr><tr><td>SVM (1:1)</td><td>0.813</td><td>0.75</td><td>0.14</td><td>0.23</td><td>0.75</td></tr><tr><td>Decision tree (raw dataset)</td><td>0.698</td><td>0.31</td><td>0.39</td><td>0.35</td><td>0.58</td></tr><tr><td>Decision tree (1:1)</td><td>0.781</td><td>0.47</td><td>0.54</td><td>0.50</td><td>0.69</td></tr><tr><td>Random forest (raw dataset)</td><td>0.809</td><td>0.63</td><td>0.17</td><td>0.27</td><td>0.70</td></tr><tr><td>Random forest (1:1)</td><td>0.843</td><td>0.74</td><td>0.31</td><td>0.43</td><td>0.78</td></tr><tr><td>Adaboost (raw dataset)</td><td>0.805</td><td>0.54</td><td>0.34</td><td>0.42</td><td>0.73</td></tr><tr><td>Adaboost (1:1)</td><td>0.785</td><td>0.47</td><td>0.39</td><td>0.43</td><td>0.75</td></tr><tr><td>Naïve Bayes (raw dataset)</td><td>0.750</td><td>0.40</td><td>0.44</td><td>0.42</td><td>0.70</td></tr><tr><td>Naïve Bayes (1:1)</td><td>0.773</td><td>0.41</td><td>0.46</td><td>0.44</td><td>0.71</td></tr><tr><td>DeepEP</td><td>0.826</td><td>0.58</td><td>0.52</td><td>0.55</td><td>0.82</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec13">
      <title>Ablation study</title>
      <p id="Par34">Our experimental results show that DeepEP improves the performances of identifying essential proteins and outperforms other existing methods. To discover the vital element of DeepEP, we perform experiments by substituting node2vec technique with 6 common used central indexes and the proposed sampling method with different ratios of the positive samples to negative samples in our deep learning framework. In Table <xref rid="Tab2" ref-type="table">2</xref> we compare the performances obtained by using node2vec technique with the results of traditional central indexes (DC, CC, EC, BC, NC, and LAC). We use a python library called networkx to calculate the six central indexes of PPI network as the network representation of PPI. The rest part of deep learning framework stays the same settings. From Table <xref rid="Tab2" ref-type="table">2</xref>, we can clearly see that node2vec technique is the most effective component and therefore is a crucial element in our deep learning framework. By using node2vec technique, F-measure and AUC of DeepEP are 0.552 and 0.816, respectively, which are better than gene expression data with DC (0.315 and 0.701), CC (0.318 and 0.667), EC (0.348 and 0.690), BC (0.296 and 0.657), NC (0.311 and 0.670), and LAC (0.302 and 0.672). Other metrics of accuracy, precision and recall obtained by node2vec technique are 0.826, 0.584 and 0.524, respectively, which are the highest among all methods. Figure <xref rid="Fig4" ref-type="fig">4</xref> plots the ROC and PR curves of DeepEP and comparing models which use gene expression profiles combined with different central indexes (DC, CC, EC, BC, NC, and LAC). It is evident that DeepEP has the best ROC curve and highest AUC value. Moreover, the AP score of DeepEP is 0.61, which outperforms DC (0.42), CC (0.37), EC (0.39), BC (0.36), NC (0.37), and LAC (0.38). These results indicate that the node2vec technique captures better network features than traditional central indexes. A single central index of PPI network makes use of a single scalar to represent the complex topological features of a protein. Instead, node2vec technique projects a PPI network to a low-dimensional space and generates a dense vector for a protein, and hence it can have richer representation of network topology. In the node2vec technique, vertices are mapped to a low-dimensional space of features which maximizes the likelihood of network neighborhoods of vertices. It makes use of biased random walk technique to efficiently explore diverse neighborhoods and thus the diversity of connectivity patterns in networks are captured, which is the key step to learning richer representations.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Performances of DeepEP and comparing models (using gene expression profiles combined with different central indexes (DC, CC, EC, BC, NC, and LAC))</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>Accuracy</th><th>Precision</th><th>Recall</th><th>F-measure</th><th>AUC</th></tr></thead><tbody><tr><td>Gene expression + DC</td><td>0.803</td><td>0.558</td><td>0.220</td><td>0.315</td><td>0.701</td></tr><tr><td>Gene expression + CC</td><td>0.782</td><td>0.446</td><td>0.247</td><td>0.318</td><td>0.667</td></tr><tr><td>Gene expression + EC</td><td>0.774</td><td>0.429</td><td>0.293</td><td>0.348</td><td>0.690</td></tr><tr><td>Gene expression + BC</td><td>0.789</td><td>0.474</td><td>0.215</td><td>0.296</td><td>0.657</td></tr><tr><td>Gene expression + NC</td><td>0.779</td><td>0.432</td><td>0.243</td><td>0.311</td><td>0.670</td></tr><tr><td>Gene expression + LAC</td><td>0.796</td><td>0.533</td><td>0.211</td><td>0.302</td><td>0.672</td></tr><tr><td>Gene expression + node2vec</td><td>0.826</td><td>0.584</td><td>0.524</td><td>0.552</td><td>0.816</td></tr></tbody></table></table-wrap>
<fig id="Fig4"><label>Fig. 4</label><caption><p>ROC and PR curves of DeepEP and models which use gene expression data combined with different central indexes (DC, CC, EC, BC, NC and LAC)</p></caption><graphic xlink:href="12859_2019_3076_Fig4_HTML" id="MO4"/></fig></p>
      <p id="Par35">We tested the performance of models by using random undersampling technique with different ratios. Random undersampling technique is employed to obtain different datasets which have different ratios of essential proteins to non-essential proteins from raw training dataset. Then different datasets are applied to train different deep learning framework. Specifically, we train our models with different ratios (1:1, 1:1.5, 1:2, 1:2.5 and 1:3) and raw dataset and their performances are given in Table <xref rid="Tab3" ref-type="table">3</xref>. It can be seen that the sampling method is a crucial element in DeepEP. By using the sampling method, F-measure and AUC values obtained by DeepEP are 0.552 and 0.816, respectively, which are better than the ratio of 1:1 (0.508 and 0.783), ratio of 1:1.5 (0.507 and 0.785), ratio of 1:2 (0.510 and 0.791), ratio of 1:2.5 (0.511 and 0.783), ratio of 1:3 (0.482 and 0.788) and using raw dataset (0.463 and 0.803). The ROC and PR curves of comparing methods are shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. We can see that the ROC curve of DeepEP is slightly higher than those of different ratios. In addition, we can see that the AP score obtained by DeepEP is 0.61, which is obviously higher than 1:1 (0.54), 1:1.5 (0.53), 1:2 (0.58), 1:2.5 (0.55), 1:3 (0.54) and raw dataset (0.58). These two figures also demonstrate that DeepEP works better than random undersampling sampling method with different ratios due to the sampling method. Our analysis shows that two main factors contribute to the better performance of the sampling method. First, we utilize a balanced subset for training in each training epoch, thus our classifier does not bias to any class in each training batch. Second, we make use of all non-essential protein samples in high probability and hence, we do not lose any information of raw dataset.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Performance of DeepEP and comparing methods (models with different ratios (1:1, 1:1.5, 1:2, 1:2.5 and 1:3) and a model which uses raw dataset for training)</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Ratios (Essential VS non-essential)</th><th>Accuracy</th><th>Precision</th><th>Recall</th><th>F-measure</th><th>AUC</th></tr></thead><tbody><tr><td>1: 1</td><td>0.732</td><td>0.408</td><td>0.674</td><td>0.508</td><td>0.783</td></tr><tr><td>1: 1.5</td><td>0.758</td><td>0.437</td><td>0.605</td><td>0.507</td><td>0.785</td></tr><tr><td>1: 2</td><td>0.784</td><td>0.479</td><td>0.545</td><td>0.510</td><td>0.791</td></tr><tr><td>1: 2.5</td><td>0.796</td><td>0.504</td><td>0.518</td><td>0.511</td><td>0.783</td></tr><tr><td>1: 3</td><td>0.801</td><td>0.521</td><td>0.449</td><td>0.482</td><td>0.788</td></tr><tr><td>Raw dataset</td><td>0.832</td><td>0.675</td><td>0.353</td><td>0.463</td><td>0.803</td></tr><tr><td>Our method</td><td>0.826</td><td>0.584</td><td>0.524</td><td>0.552</td><td>0.816</td></tr></tbody></table></table-wrap>
<fig id="Fig5"><label>Fig. 5</label><caption><p>ROC and PR curves of DeepEP, our deep learning framework using different ratios of essential proteins to non-essential proteins (1: 1, 1: 1.5, 1: 2, 1: 2.5 and 1: 3), and using raw dataset. Note: RU refers to random undersampling</p></caption><graphic xlink:href="12859_2019_3076_Fig5_HTML" id="MO5"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Conclusions</title>
    <p id="Par36">We propose a new deep learning framework, DeepEP, which is used for identifying essential proteins. DeepEP aims to investigate whether deep learning and sampling methods could achieve notable improvements for identifying essential proteins. The topological features of PPI networks are difficult captured by traditional methods. DeepEP utilizes the node2vec technique to automatically learn complex topological features from PPI network. The node2vec can project the PPI network to low-dimensional space and obtain the representation of proteins with low-dimensional vectors, which allow DeepEP to address the limitations of the traditional methods. In addition, the essential proteins prediction is an imbalanced learning problem; a sampling method is applied in DeepEP to handle this issue. The experimental results obtained by DeepEP show that the proposed approach is able to achieve the state-of-the-art performances that are higher than those obtained by other centrality methods and shallow machine learning-based methods. To understand why DeepEP works well for identifying essential proteins, we conduct studies by substituting node2vec technique with 6 common used central indexes and the proposed sampling method with different ratios. Experimental results show that the dense vectors which are generated by node2vec technique contribute a lot to the improved performance. In addition, the sampling method also helps to improve the performance of deep learning framework.</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>AUC</term>
        <def>
          <p id="Par4">Area Under receiver operating characteristic Curve</p>
        </def>
      </def-item>
      <def-item>
        <term>CNN</term>
        <def>
          <p id="Par5">Convolutional neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>PPI</term>
        <def>
          <p id="Par6">Protein-protein interaction</p>
        </def>
      </def-item>
      <def-item>
        <term>PR</term>
        <def>
          <p id="Par7">Precision-recall</p>
        </def>
      </def-item>
      <def-item>
        <term>RF</term>
        <def>
          <p id="Par8">Random forest</p>
        </def>
      </def-item>
      <def-item>
        <term>ROC</term>
        <def>
          <p id="Par9">Receiver Operating Characteristic</p>
        </def>
      </def-item>
      <def-item>
        <term>SVM</term>
        <def>
          <p id="Par10">support vector machine</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Not applicable.</p>
    <sec id="FPar1">
      <title>About this supplement</title>
      <p id="Par37">This article has been published as part of <italic>BMC Bioinformatics Volume 20 Supplement 16, 2019: Selected articles from the IEEE BIBM International Conference on Bioinformatics &amp; Biomedicine (BIBM) 2018: bioinformatics and systems biology.</italic> The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-20-supplement-16">https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-20-supplement-16</ext-link>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>MZ and ML conceived and designed the experiments. MZ performed the experiments. MZ and ML drafted the manuscript. FXW, YL, and YP revised the manuscript. All authors approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was supported in part by the National Natural Science Foundation of China under Grants (No. 61832019, No. 61622213 and No. 61728211), Hunan Provincial Science and Technology Program (No. 2018WK4001), the Fundamental Research Funds for the Central Universities of Central South University (No. 502221903), and the 111 Project (No.B18059 and No.G20190018001). The publication costs of this article were funded by the National Natural Science Foundation of China under Grant No. 61832019.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The DeepEP source code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/CSUBioGroup/DeepEP">https://github.com/CSUBioGroup/DeepEP</ext-link>.</p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p id="Par38">Not applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p id="Par39">Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par40">The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Glass</surname>
            <given-names>JI</given-names>
          </name>
          <name>
            <surname>Hutchison</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>HO</given-names>
          </name>
          <name>
            <surname>Venter</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>A systems biology tour de force for a near-minimal bacterium</article-title>
        <source>Mol Syst Biol</source>
        <year>2009</year>
        <volume>5</volume>
        <issue>1</issue>
        <fpage>330</fpage>
        <pub-id pub-id-type="doi">10.1038/msb.2009.89</pub-id>
        <pub-id pub-id-type="pmid">19953084</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Clatworthy</surname>
            <given-names>AE</given-names>
          </name>
          <name>
            <surname>Pierson</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Hung</surname>
            <given-names>DT</given-names>
          </name>
        </person-group>
        <article-title>Targeting virulence: a new paradigm for antimicrobial therapy</article-title>
        <source>Nat Chem Biol</source>
        <year>2007</year>
        <volume>3</volume>
        <issue>9</issue>
        <fpage>541</fpage>
        <pub-id pub-id-type="doi">10.1038/nchembio.2007.24</pub-id>
        <pub-id pub-id-type="pmid">17710100</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Roemer</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Davison</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ketela</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Veillette</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Breton</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tandia</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Linteau</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sillaots</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Marta</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Large-scale essential gene identification in Candida albicans and applications to antifungal drug discovery</article-title>
        <source>Mol Microbiol</source>
        <year>2003</year>
        <volume>50</volume>
        <issue>1</issue>
        <fpage>167</fpage>
        <lpage>181</lpage>
        <pub-id pub-id-type="doi">10.1046/j.1365-2958.2003.03697.x</pub-id>
        <pub-id pub-id-type="pmid">14507372</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cullen</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Arndt</surname>
            <given-names>GM</given-names>
          </name>
        </person-group>
        <article-title>Genome-wide screening for gene function using RNAi in mammalian cells</article-title>
        <source>Immunol Cell Biol</source>
        <year>2005</year>
        <volume>83</volume>
        <issue>3</issue>
        <fpage>217</fpage>
        <lpage>223</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1440-1711.2005.01332.x</pub-id>
        <pub-id pub-id-type="pmid">15877598</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Giaever</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Chu</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Ni</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Connelly</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Riles</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Veronneau</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Dow</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lucau-Danila</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Anderson</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Andre</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Functional profiling of the Saccharomyces cerevisiae genome</article-title>
        <source>Nature</source>
        <year>2002</year>
        <volume>418</volume>
        <issue>6896</issue>
        <fpage>387</fpage>
        <pub-id pub-id-type="doi">10.1038/nature00935</pub-id>
        <pub-id pub-id-type="pmid">12140549</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jeong</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Mason</surname>
            <given-names>SP</given-names>
          </name>
          <name>
            <surname>Barabási</surname>
            <given-names>A-L</given-names>
          </name>
          <name>
            <surname>Oltvai</surname>
            <given-names>ZN</given-names>
          </name>
        </person-group>
        <article-title>Lethality and centrality in protein networks</article-title>
        <source>Nature</source>
        <year>2001</year>
        <volume>411</volume>
        <issue>6833</issue>
        <fpage>41</fpage>
        <pub-id pub-id-type="doi">10.1038/35075138</pub-id>
        <pub-id pub-id-type="pmid">11333967</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hahn</surname>
            <given-names>MW</given-names>
          </name>
          <name>
            <surname>Kern</surname>
            <given-names>AD</given-names>
          </name>
        </person-group>
        <article-title>Comparative genomics of centrality and essentiality in three eukaryotic protein-interaction networks</article-title>
        <source>Mol Biol Evol</source>
        <year>2004</year>
        <volume>22</volume>
        <issue>4</issue>
        <fpage>803</fpage>
        <lpage>806</lpage>
        <pub-id pub-id-type="doi">10.1093/molbev/msi072</pub-id>
        <pub-id pub-id-type="pmid">15616139</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Joy</surname>
            <given-names>MP</given-names>
          </name>
          <name>
            <surname>Brock</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ingber</surname>
            <given-names>DE</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>High-betweenness proteins in the yeast protein interaction network</article-title>
        <source>Biomed Res Int</source>
        <year>2005</year>
        <volume>2005</volume>
        <issue>2</issue>
        <fpage>96</fpage>
        <lpage>103</lpage>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wuchty</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Stadler</surname>
            <given-names>PF</given-names>
          </name>
        </person-group>
        <article-title>Centers of complex networks</article-title>
        <source>J Theor Biol</source>
        <year>2003</year>
        <volume>223</volume>
        <issue>1</issue>
        <fpage>45</fpage>
        <lpage>53</lpage>
        <pub-id pub-id-type="doi">10.1016/S0022-5193(03)00071-7</pub-id>
        <pub-id pub-id-type="pmid">12782116</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Estrada</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Rodriguez-Velazquez</surname>
            <given-names>JA</given-names>
          </name>
        </person-group>
        <article-title>Subgraph centrality in complex networks</article-title>
        <source>Phys Rev E</source>
        <year>2005</year>
        <volume>71</volume>
        <issue>5</issue>
        <fpage>056103</fpage>
        <pub-id pub-id-type="doi">10.1103/PhysRevE.71.056103</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Identification of essential proteins based on edge clustering coefficient</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinform</source>
        <year>2012</year>
        <volume>9</volume>
        <issue>4</issue>
        <fpage>1070</fpage>
        <lpage>1080</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2011.147</pub-id>
        <pub-id pub-id-type="pmid">22084147</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Li G, Li M, Wang J, Li Y, Pan Y. United neighborhood closeness centrality and orthology for predicting essential proteins. IEEE/ACM Trans Comput Biol Bioinform. 2018. 10.1109/TCBB.2018.2889978.</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>J-x</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>A new essential protein discovery method based on the integration of protein-protein interaction and gene expression data</article-title>
        <source>BMC Syst Biol</source>
        <year>2012</year>
        <volume>6</volume>
        <issue>1</issue>
        <fpage>15</fpage>
        <pub-id pub-id-type="doi">10.1186/1752-0509-6-15</pub-id>
        <pub-id pub-id-type="pmid">22405054</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhong</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Predicting essential proteins based on weighted degree centrality</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinform</source>
        <year>2014</year>
        <volume>11</volume>
        <issue>2</issue>
        <fpage>407</fpage>
        <lpage>418</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2013.2295318</pub-id>
        <pub-id pub-id-type="pmid">26355787</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Zhang J, Li W, Zeng M, Meng X, Kurgan L, Wu F, Li M. NetEPD: a network-based essential protein discovery platform. Tsinghua Sci Technol. 2019. 10.26599/TST.2019.9010056.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Zeng M, Li M, Fei Z, Wu F, Li Y, Pan Y, Wang J. A deep learning framework for identifying essential proteins by integrating multiple types of biological information. IEEE/ACM Trans Comput Biol Bioinform. 2019. 10.1109/TCBB.2019.2897679.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>UDoNC: an algorithm for identifying essential proteins based on protein domains and protein-protein interaction networks</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinform</source>
        <year>2015</year>
        <volume>12</volume>
        <issue>2</issue>
        <fpage>276</fpage>
        <lpage>288</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2014.2338317</pub-id>
        <pub-id pub-id-type="pmid">26357216</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Niu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhong</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>A reliable neighbor-based method for identifying essential proteins by integrating gene expressions, orthology, and subcellular localization information</article-title>
        <source>Tsinghua Sci Technol</source>
        <year>2016</year>
        <volume>21</volume>
        <issue>6</issue>
        <fpage>668</fpage>
        <lpage>677</lpage>
        <pub-id pub-id-type="doi">10.1109/TST.2016.7787009</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>F-X</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Predicting essential proteins based on subcellular localization, orthology and PPI networks</article-title>
        <source>BMC Bioinf</source>
        <year>2016</year>
        <volume>17</volume>
        <issue>8</issue>
        <fpage>279</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-016-1115-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lei</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Fujita</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Predicting essential proteins based on RNA-Seq, subcellular localization and GO annotation datasets</article-title>
        <source>Knowl-Based Syst</source>
        <year>2018</year>
        <volume>151</volume>
        <fpage>136</fpage>
        <lpage>148</lpage>
        <pub-id pub-id-type="doi">10.1016/j.knosys.2018.03.027</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Li X, Li W, Zeng M, Zheng R, Li M. Network-based methods for predicting essential genes or proteins: a survey. Brief Bioinform. 2019. 10.1093/bib/bbz017.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hwang</surname>
            <given-names>Y-C</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>C-C</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>J-Y</given-names>
          </name>
          <name>
            <surname>Mori</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Juan</surname>
            <given-names>H-F</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>H-C</given-names>
          </name>
        </person-group>
        <article-title>Predicting essential genes based on network and sequence analysis</article-title>
        <source>Mol BioSyst</source>
        <year>2009</year>
        <volume>5</volume>
        <issue>12</issue>
        <fpage>1672</fpage>
        <lpage>1678</lpage>
        <pub-id pub-id-type="doi">10.1039/b900611g</pub-id>
        <pub-id pub-id-type="pmid">19452048</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Rhodes</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>LJ</given-names>
          </name>
        </person-group>
        <article-title>Predicting essential genes for identifying potential drug targets in Aspergillus fumigatus</article-title>
        <source>Comput Biol Chem</source>
        <year>2014</year>
        <volume>50</volume>
        <fpage>29</fpage>
        <lpage>40</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiolchem.2014.01.011</pub-id>
        <pub-id pub-id-type="pmid">24569026</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cheng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Tao</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Training set selection for the prediction of essential genes</article-title>
        <source>PLoS One</source>
        <year>2014</year>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>e86805</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0086805</pub-id>
        <pub-id pub-id-type="pmid">24466248</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Acencio</surname>
            <given-names>ML</given-names>
          </name>
          <name>
            <surname>Lemke</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Towards the prediction of essential genes by integration of network topology, cellular localization and biological process information</article-title>
        <source>BMC Bioinf</source>
        <year>2009</year>
        <volume>10</volume>
        <issue>1</issue>
        <fpage>290</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-10-290</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhong</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Prediction of essential proteins based on gene expression programming</article-title>
        <source>BMC Genomics</source>
        <year>2013</year>
        <volume>14</volume>
        <issue>4</issue>
        <fpage>S7</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2164-14-S8-S7</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Li M, Gao H, Wang J, Wu F. Control principles for complex biological networks. Brief Bioinform. 2018. 10.1093/bib/bby088.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Kurgan</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>DeepFunc: a deep learning framework for accurate prediction of protein functions from protein sequences and interactions</article-title>
        <source>Proteomics</source>
        <year>2019</year>
        <volume>19</volume>
        <fpage>1900019</fpage>
        <pub-id pub-id-type="doi">10.1002/pmic.201900019</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zeng</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Fei</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Automatic ICD-9 coding via deep transfer learning</article-title>
        <source>Neurocomputing</source>
        <year>2019</year>
        <volume>324</volume>
        <fpage>43</fpage>
        <lpage>50</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neucom.2018.04.081</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Min</given-names>
          </name>
          <name>
            <surname>Fei</surname>
            <given-names>Zhihui</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>Min</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Fang-Xiang</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Yaohang</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>Yi</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Jianxin</given-names>
          </name>
        </person-group>
        <article-title>Automated ICD-9 Coding via A Deep Learning Approach</article-title>
        <source>IEEE/ACM Transactions on Computational Biology and Bioinformatics</source>
        <year>2019</year>
        <volume>16</volume>
        <issue>4</issue>
        <fpage>1193</fpage>
        <lpage>1202</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2018.2817488</pub-id>
        <pub-id pub-id-type="pmid">29994157</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Tu</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Max-margin DeepWalk: discriminative learning of network representation</article-title>
        <source>IJCAI</source>
        <year>2016</year>
        <fpage>3889</fpage>
        <lpage>3895</lpage>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Mikolov</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Corrado</surname>
            <given-names>GS</given-names>
          </name>
          <name>
            <surname>Dean</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Distributed representations of words and phrases and their compositionality</article-title>
        <source>Advances in neural information processing systems</source>
        <year>2013</year>
        <fpage>3111</fpage>
        <lpage>3119</lpage>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Grover A, Leskovec J. node2vec: scalable feature learning for networks. In: Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. New York: ACM; 2016. p. 855–64. 10.1145/2939672.2939754.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">He H, Garcia EA. Learning from imbalanced data. IEEE Trans Knowl Data Eng. 2009;21(9):1263–84.</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Zeng M, Zou B, Wei F, Liu X, Wang L. Effective prediction of three common diseases by combining SMOTE with Tomek links technique for imbalanced medical data. In: 2016 IEEE International Conference of Online Analysis and Computing Science (ICOACS). Chongqing: IEEE; 2016. p. 225–8. 10.1109/ICOACS.2016.7563084.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chawla</surname>
            <given-names>NV</given-names>
          </name>
          <name>
            <surname>Bowyer</surname>
            <given-names>KW</given-names>
          </name>
          <name>
            <surname>Hall</surname>
            <given-names>LO</given-names>
          </name>
          <name>
            <surname>Kegelmeyer</surname>
            <given-names>WP</given-names>
          </name>
        </person-group>
        <article-title>SMOTE: synthetic minority over-sampling technique</article-title>
        <source>J Artif Intell Res</source>
        <year>2002</year>
        <volume>16</volume>
        <fpage>321</fpage>
        <lpage>357</lpage>
        <pub-id pub-id-type="doi">10.1613/jair.953</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Zeng M, Zhang F, Wu F, Li Y, Wang J, Li M. Protein-protein interaction site prediction through combining local and global features with deep neural networks. Bioinformatics. 10.1093/bioinformatics/btz699.</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kamnitsas</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Ledig</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Newcombe</surname>
            <given-names>VF</given-names>
          </name>
          <name>
            <surname>Simpson</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Kane</surname>
            <given-names>AD</given-names>
          </name>
          <name>
            <surname>Menon</surname>
            <given-names>DK</given-names>
          </name>
          <name>
            <surname>Rueckert</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Glocker</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation</article-title>
        <source>Med Image Anal</source>
        <year>2017</year>
        <volume>36</volume>
        <fpage>61</fpage>
        <lpage>78</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2016.10.004</pub-id>
        <pub-id pub-id-type="pmid">27865153</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stark</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Breitkreutz</surname>
            <given-names>B-J</given-names>
          </name>
          <name>
            <surname>Reguly</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Boucher</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Breitkreutz</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tyers</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>BioGRID: a general repository for interaction datasets</article-title>
        <source>Nucleic Acids Res</source>
        <year>2006</year>
        <volume>34</volume>
        <issue>suppl_1</issue>
        <fpage>D535</fpage>
        <lpage>D539</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkj109</pub-id>
        <pub-id pub-id-type="pmid">16381927</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mewes</surname>
            <given-names>H-W</given-names>
          </name>
          <name>
            <surname>Frishman</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Güldener</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Mannhaupt</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Mayer</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Mokrejs</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Morgenstern</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Münsterkötter</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rudd</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Weil</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>MIPS: a database for genomes and protein sequences</article-title>
        <source>Nucleic Acids Res</source>
        <year>2002</year>
        <volume>30</volume>
        <issue>1</issue>
        <fpage>31</fpage>
        <lpage>34</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/30.1.31</pub-id>
        <pub-id pub-id-type="pmid">11752246</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cherry</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Adler</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ball</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Chervitz</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Dwight</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Hester</surname>
            <given-names>ET</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Juvik</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Roe</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Schroeder</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>SGD: Saccharomyces genome database</article-title>
        <source>Nucleic Acids Res</source>
        <year>1998</year>
        <volume>26</volume>
        <issue>1</issue>
        <fpage>73</fpage>
        <lpage>79</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/26.1.73</pub-id>
        <pub-id pub-id-type="pmid">9399804</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>DEG 5.0, a database of essential genes in both prokaryotes and eukaryotes</article-title>
        <source>Nucleic Acids Res</source>
        <year>2008</year>
        <volume>37</volume>
        <issue>suppl_1</issue>
        <fpage>D455</fpage>
        <lpage>D458</lpage>
        <?supplied-pmid 18974178?>
        <pub-id pub-id-type="pmid">18974178</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Abadi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Barham</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Davis</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Dean</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Devin</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ghemawat</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Irving</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Isard</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Tensorflow: a system for large-scale machine learning</article-title>
        <source>OSDI</source>
        <year>2016</year>
        <fpage>265</fpage>
        <lpage>283</lpage>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>Ying</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Min</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Liangliang</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Yaohang</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Jianxin</given-names>
          </name>
        </person-group>
        <article-title>Clinical big data and deep learning: Applications, challenges, and future outlooks</article-title>
        <source>Big Data Mining and Analytics</source>
        <year>2019</year>
        <volume>2</volume>
        <issue>4</issue>
        <fpage>288</fpage>
        <lpage>305</lpage>
        <pub-id pub-id-type="doi">10.26599/BDMA.2019.9020007</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
