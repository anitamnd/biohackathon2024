<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10941452</article-id>
    <article-id pub-id-type="pmid">38486137</article-id>
    <article-id pub-id-type="publisher-id">5730</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-024-05730-9</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MetaTron: advancing biomedical annotation empowering relation annotation and collaboration</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Irrera</surname>
          <given-names>Ornella</given-names>
        </name>
        <address>
          <email>ornella.irrera@unipd.it</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Marchesin</surname>
          <given-names>Stefano</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Silvello</surname>
          <given-names>Gianmaria</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00240q980</institution-id><institution-id institution-id-type="GRID">grid.5608.b</institution-id><institution-id institution-id-type="ISNI">0000 0004 1757 3470</institution-id><institution>Department of Information Engineering, </institution><institution>University of Padova, </institution></institution-wrap>Padua, Italy </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>14</day>
      <month>3</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>14</day>
      <month>3</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2024</year>
    </pub-date>
    <volume>25</volume>
    <elocation-id>112</elocation-id>
    <history>
      <date date-type="received">
        <day>26</day>
        <month>5</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>4</day>
        <month>3</month>
        <year>2024</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2024</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">The constant growth of biomedical data is accompanied by the need for new methodologies to effectively and efficiently extract machine-readable knowledge for training and testing purposes. A crucial aspect in this regard is creating large, often manually or semi-manually, annotated corpora vital for developing effective and efficient methods for tasks like relation extraction, topic recognition, and entity linking. However, manual annotation is expensive and time-consuming especially if not assisted by interactive, intuitive, and collaborative computer-aided tools. To support healthcare experts in the annotation process and foster annotated corpora creation, we present <italic>MetaTron</italic>. <italic>MetaTron</italic> is an open-source and free-to-use web-based annotation tool to annotate biomedical data interactively and collaboratively; it supports both mention-level and document-level annotations also integrating automatic built-in predictions. Moreover, <italic>MetaTron</italic> enables relation annotation with the support of ontologies, functionalities often overlooked by off-the-shelf annotation tools.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We conducted a qualitative analysis to compare <italic>MetaTron</italic> with a set of manual annotation tools including <italic>TeamTat</italic>, <italic>INCEpTION</italic>, <italic>LightTag</italic>, <italic>MedTAG</italic>, and <italic>brat</italic>, on three sets of criteria: technical, data, and functional. A quantitative evaluation allowed us to assess <italic>MetaTron</italic> performances in terms of time and number of clicks to annotate a set of documents. The results indicated that <italic>MetaTron</italic> fulfills almost all the selected criteria and achieves the best performances.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3"><italic>MetaTron</italic> stands out as one of the few annotation tools targeting the biomedical domain supporting the annotation of relations, and fully customizable with documents in several formats—PDF included, as well as abstracts retrieved from PubMed, Semantic Scholar, and OpenAIRE. To meet any user need, we released <italic>MetaTron</italic> both as an online instance and as a Docker image locally deployable.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Digital health</kwd>
      <kwd>Biomedical annotation tool</kwd>
      <kwd>Relation extraction</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id>
            <institution>Horizon 2020 Framework Programme</institution>
          </institution-wrap>
        </funding-source>
        <award-id>825292</award-id>
        <award-id>825292</award-id>
        <award-id>825292</award-id>
        <principal-award-recipient>
          <name>
            <surname>Irrera</surname>
            <given-names>Ornella</given-names>
          </name>
          <name>
            <surname>Marchesin</surname>
            <given-names>Stefano</given-names>
          </name>
          <name>
            <surname>Silvello</surname>
            <given-names>Gianmaria</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Università degli Studi di Padova</institution>
        </funding-source>
      </award-group>
      <open-access>
        <p>Open access funding provided by Università degli Studi di Padova.</p>
      </open-access>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© BioMed Central Ltd., part of Springer Nature 2024</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par4">In recent years, the exponential growth of biomedical data such as medical reports, Electronic Health Records (EHR) and physician notes posed relevant challenges in effectively and efficiently organizing, curating, managing, and reusing this data both for clinical and research purposes [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR5">5</xref>]. Given the textual nature of biomedical data (according to [<xref ref-type="bibr" rid="CR6">6</xref>], the 70–80% of clinical data is text-based), extracting and reusing the knowledge in the biomedical literature can drive advances in biomedical research, enhance decision-making processes, and accelerate the discovery of drugs and diseases [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR7">7</xref>–<xref ref-type="bibr" rid="CR9">9</xref>]. Consequently, Natural Language Processing (NLP) techniques have gained substantial importance as they can automate retrieval, biomedical data processing, and knowledge extraction. The research area specialized in applying NLP technique to the biomedical data is defined <italic>BioNLP</italic> [<xref ref-type="bibr" rid="CR3">3</xref>]. Developing efficient and effective NLP methods is challenging as it requires the availability of large manually annotated corpora. In [<xref ref-type="bibr" rid="CR10">10</xref>], authors address this problem and present a comparison of the most commonly employed manual annotation tools that can be used to create manually annotated corpora.</p>
    <p id="Par5">Several works studied the application of NLP technique to process biomedical data on different domains: in [<xref ref-type="bibr" rid="CR11">11</xref>], for example, authors discuss the use of NLP techniques to extract symptoms from EHR, while [<xref ref-type="bibr" rid="CR12">12</xref>] discuss the use of NLP to process oncology medical records. In [<xref ref-type="bibr" rid="CR13">13</xref>], a large language model for EHR is proposed, and in [<xref ref-type="bibr" rid="CR6">6</xref>], authors analyze NLP methods to identify advance care planning documentation in patient clinical notes.</p>
    <p id="Par6">In this context, Relation Extraction (RE) task captured considerable interest in the biomedical community as the knowledge stored in the biomedical data may contain valuable insights about the relationships between entities—e.g., protein–protein, gene–disease, drug–drug, and drug–target interactions. Furthermore, Entity Linking (EL), the task of identifying and disambiguating entity occurrences in unstructured text [<xref ref-type="bibr" rid="CR14">14</xref>], is key to detecting the various textual representations of an entity and capturing its underlying meaning. In [<xref ref-type="bibr" rid="CR4">4</xref>], for example, authors show how EL has numerous benefits, including better use of EHR, improved search and retrieval of biomedical resources, abbreviation disambiguation. In addition to RE and EL, another important theme in the biomedical community is Topic Recognition (TR). Detecting the topics discussed in biomedical text plays a vital role in organizing and classifying the vast amount of information available [<xref ref-type="bibr" rid="CR15">15</xref>]. The most recent proposed techniques to perform RE [<xref ref-type="bibr" rid="CR16">16</xref>–<xref ref-type="bibr" rid="CR22">22</xref>], EL [<xref ref-type="bibr" rid="CR23">23</xref>–<xref ref-type="bibr" rid="CR28">28</xref>] and TR [<xref ref-type="bibr" rid="CR29">29</xref>–<xref ref-type="bibr" rid="CR35">35</xref>] on biomedical texts rely on Machine Learning (ML) models whose performances depend on the availability of large annotated corpora used in training, validation, and test. Creating sizeable and trustworthy manual annotated datasets for the biomedical domain and sub-domains is a time-consuming task requiring people with a high level of expertise [<xref ref-type="bibr" rid="CR6">6</xref>]. In this respect, the creation of these corpora is made even more challenging by the intrinsic diversity in topics and concepts within the biomedical field; the UMLS Metathesaurus [<xref ref-type="bibr" rid="CR36">36</xref>], for example, contains over 3.5 million unique concepts belonging to 127 different semantic types [<xref ref-type="bibr" rid="CR27">27</xref>]. In addition, biomedical terminology is complex, and some terms may have different meanings depending on the context where they are used [<xref ref-type="bibr" rid="CR37">37</xref>]. Some notable examples of large annotated corpora are [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR38">38</xref>–<xref ref-type="bibr" rid="CR45">45</xref>]. Furthermore, some annotated corpora targets the annotation of relationships such as [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR46">46</xref>, <xref ref-type="bibr" rid="CR47">47</xref>]. Given the importance of the annotation task and the significant effort required by experts, several manual annotation tools have been developed explicitly for the biomedical domain [<xref ref-type="bibr" rid="CR48">48</xref>–<xref ref-type="bibr" rid="CR53">53</xref>]. Other tools, on the other hand, are general-purpose [<xref ref-type="bibr" rid="CR54">54</xref>–<xref ref-type="bibr" rid="CR58">58</xref>] and offer different features that are not aligned with the biomedical experts’ needs. The coexistence of multiple annotation tools arises from other tools offering diverse functionalities and targeting various domains of interest.<fig id="Fig1"><label>Fig. 1</label><caption><p>Annotation tools features overview. Each of the selected twenty annotation tools is evaluated based on 24 criteria: 7 technical (T) criteria, three criteria about input and output data formats (D), and 15 criteria concerning the functionalities (F) provided by each tool. The first 22 criteria are taken from [<xref ref-type="bibr" rid="CR10">10</xref>], while we added the last two. Each criterion is marked in blue for each tool if the feature is fully satisfied, light blue if partially satisfied, and white if not satisfied</p></caption><graphic xlink:href="12859_2024_5730_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par7">In [<xref ref-type="bibr" rid="CR10">10</xref>], a review of the widest-used annotation tools is proposed, highlighting the points of strength and weakness of each. They selected 15 annotation tools according to the following criteria:<list list-type="bullet"><list-item><p id="Par8"><italic>Availability</italic>: the tool must be readily available. This criterion ensures that the tool is accessible via a public URL or is downloadable, independently of the user’s expertise. In this respect, the availability of source code is important not only in terms of transparency and accountability but also to guarantee further development and customization according to the user’s needs;</p></list-item><list-item><p id="Par9"><italic>Web-based</italic>: the tool must be a web application. With the term “web application,” we refer to a software application following the most frequently used 3-tiers architecture: a presentation layer handles user interface and interaction, an application layer implements the business logic, and a data layer permanently stores data resulting from or useful to the user interaction [<xref ref-type="bibr" rid="CR59">59</xref>]. Web-based tools can run online. Hence, they can be accessed via a public URL or offline and, in this case, can be installed as web applications. Offline web-based tools require an installation procedure to run on the user’s personal computers or distributed servers. Web-based tools are usually more flexible and solid with the passing of time, and there is a growing emphasis on software not requiring local installations [<xref ref-type="bibr" rid="CR60">60</xref>]; they allow multiple annotators to work collaboratively on the same document in different environments and are platform independent—e.g., other operative systems and different machines.. Conversely, according to [<xref ref-type="bibr" rid="CR10">10</xref>], non-web-based tools are stand-alone systems or plugins running on other tools and platforms. According to two analyses carried out in [<xref ref-type="bibr" rid="CR10">10</xref>], in the past ten years, web-based tools have been more prevalent than stand-alone tools and plugins and are the most frequently used in the annotation of biomedical corpora;</p></list-item><list-item><p id="Par10"><italic>Installability</italic>: offline web-based tools are needed when documents and annotations must be kept private. These tools must be installed on personal computers or distributed servers, and their installation procedure must be finished in less than two hours. The ease and speed of installation and setup are crucial factors that influence the usability of a tool for a diverse range of users with varying backgrounds. Installability is a crucial feature also highlighted by another recent survey of image annotation tools [<xref ref-type="bibr" rid="CR61">61</xref>]; indeed, installation issues are one of the main reasons annotation tools cannot be reused in the field;</p></list-item><list-item><p id="Par11"><italic>Workable</italic>: the tool must be intuitive, and all the features must be comprehensive enough to be used independently of the level of expertise, relying on a well-documented set of instructions and without the help of the developers. This is a pivotal factor that directly impacts the tool’s utilization. Features that are challenging to comprehend render the examined tool impractical; see also [<xref ref-type="bibr" rid="CR61">61</xref>] for further supporting analysis on the usability of annotation tools. Therefore, it is crucial to take into account aspects related to usability and implementation;</p></list-item><list-item><p id="Par12"><italic>Schematic</italic>: the tool allows for schema configuration. In this context, the tool defines elements such as labels, documents, and concepts according to the user’s needs. The tool should not be developed for a specific use-case and should not provide a fixed set of labels, concepts and rules for the annotation. Concerning this point, a tool that does not allow the user to define a schema or is designed for a specific use case will likely face challenges in being reused.</p></list-item></list>Once the tools were chosen according to the aforementioned selection criteria, the authors defined a set of 26 evaluation criteria to compare them. The evaluation criteria can be subdivided into four macro-areas: (i) <italic>Publication</italic>: these features concern the tools’ publications and citations; (ii) <italic>Technical</italic>: these features concern technical aspects of the tool, and are useful to determine the availability of the tool, and the ease of installation; (iii) <italic>Data</italic>: these features describe what formats the tool requires in input and output; (iv) <italic>Functional</italic>: criteria concerning the functionalities provided by the tools. Functional criteria describe all the feature a tool provides—i.e., document-level annotations, availability of overlapping mentions, active learning, collaborative features.</p>
    <p id="Par13">While <italic>Functional</italic> criteria are significant in identifying the primary distinctions among tools and deciding which tool best suits the user’s needs, <italic>Data</italic> criteria, on the other hand, enable users to comprehend the required data formats for each tool. Consequently, users can assess whether their data needs preprocessing. A subset of these criteria has been used also for evaluation purposes in [<xref ref-type="bibr" rid="CR62">62</xref>] that describes an ecosystem for knowledge discovery. We revised the assessment conducted by [<xref ref-type="bibr" rid="CR10">10</xref>], updating <italic>LightTag</italic>, <italic>INCEpTION</italic>, and <italic>MedTAG</italic> according to our experience with each of these tools, adding some tools released after the publication of the paper and including new features we deemed as important for the current trend in bioinformatics.</p>
    <p id="Par14">The heat map we obtained is reported in Fig. <xref rid="Fig1" ref-type="fig">1</xref> with the evaluated annotation tools as rows and the tested criteria as columns. The color intensity of the cells indicates the level of adherence of a tool to each criterion. We evaluated 24 criteria, including 22 criteria (from T1 to F13 in Fig. <xref rid="Fig1" ref-type="fig">1</xref>) from the set of 26 criteria defined in [<xref ref-type="bibr" rid="CR10">10</xref>] and two new criteria (F14 and F15) defined here for the first time. The first six criteria are <italic>Technical (T)</italic>: (T1) date of last version or commit—whether the last version has been released within the past five years; (T2) availability of source code; (T3) online availability; (T4) easiness of installation; (T6) license allowing modification and redistribution; (T7) free of charge. Three criteria concern <italic>Data (D)</italic>: (D1) schema format—whether the schema is configurable; (D2) input format—whether the input documents follow a standard format; (D3) output annotations format—whether the annotations are based on standard formats. Finally, fifteen criteria are <italic>Functional (F)</italic>: (F1) support for overlapping mentions; (F2) support for document-level annotation; (F3) support for relationship annotation; (F4) support for ontologies; (F5) support for built-in prediction and active learning; (F6) integration with PubMed; (F7) suitability for full texts; (F8) support for the partial saving of an annotation (allowing the user to continue the annotation process later); (F9) support for text highlighting; (F10) support for users and teams; (F11) support for Inter Annotator Agreement (IAA); (F12) data privacy; (F13) multilingual support; (F14) connection to ORCID; (F15) retrieval of abstracts from external repositories or services. The use of most of the evaluation criteria from [<xref ref-type="bibr" rid="CR10">10</xref>], ensures that the evaluation analysis we conducted is as objective as possible, avoiding to bias the study towards <italic>MetaTron</italic> strong points. Moreover, we further validated <italic>MetaTron</italic> against these evaluation criteria by the means of two expert-based case studies.</p>
    <p id="Par15">As remarked in [<xref ref-type="bibr" rid="CR10">10</xref>], we confirm that no currently available off-the-shelf tool comprehensively meets all the requirements. This is also evident from Fig. <xref rid="Fig1" ref-type="fig">1</xref>, where the missing features in the majority of selected annotation tools are: support for relationship annotation (F3), support for overlapping mentions (F1), support for document-level annotations (F2), connection to ORCID (F14) and the integration with external repositories and services such as PubMed to retrieve publications’ abstracts (F6, F15).</p>
    <p id="Par16">In this paper, we introduce <italic>MetaTron</italic>, an innovative web-based annotation tool for the biomedical literature which fulfills all the selected evaluation criteria. <italic>MetaTron</italic> is released both as an online instance and as a Docker image deployable on a local server relying on a quick and easy installation procedure. It is fully customizable, as users can upload documents in JSON, PDF, CSV, and TXT or retrieve and upload abstracts from PubMed, Semantic Scholar, and OpenAIRE. The support for both mention-level and document-level annotation types makes <italic>MetaTron</italic> suitable for several use cases. Additionally, <italic>MetaTron</italic> supports automatic built-in predictions.</p>
    <p id="Par17">The rest of the paper is organized as follows: in “<xref rid="Sec2" ref-type="sec">Implementation</xref>” Section we describe <italic>MetaTron</italic> and its features, focusing on the annotation types <italic>MetaTron</italic> provides and <italic>AutoTron</italic> for automatic built-in predictions; in “<xref rid="Sec12" ref-type="sec">Results</xref>” Section we describe the qualitative and quantitative analyses we conducted to evaluate <italic>MetaTron</italic> and compare to other annotation tools; in  “<xref rid="Sec17" ref-type="sec">Conclusions</xref>” Section we draw some final remarks.</p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <sec id="Sec3">
      <title>System overview</title>
      <p id="Par18"><italic>MetaTron</italic> is an annotation tool designed to annotate biomedical documents. One of the key features of <italic>MetaTron</italic> is its support for multiple annotation types. The annotation types can be classified into <italic>document-level annotations</italic> and <italic>mention-level annotation</italic>. <italic>Mention-level annotations</italic> concern the annotation of specific portions of the textual document and comprize <italic>mention, concept linking</italic>, and <italic>relationship</italic> annotations. Mention annotation detects the mentions in a textual document, and each mention can be linked to one or more <italic>concepts</italic> from an ontology—i.e., concepts linking. In this work, we use the terms <italic>entities</italic> and <italic>concepts</italic> interchangeably; in particular, we consider a <italic>concept</italic> as an atomic, identifiable object that has a distinct and independent existence [<xref ref-type="bibr" rid="CR63">63</xref>]. In <italic>MetaTron</italic>, as in the Semantic Web realm, a concept is identified by a URI and described by a name and a type—e.g., gene or disease—making the concept human-understandable. Relationship annotation involves identifying and marking “statements” or “facts” within a text. A statement typically consists of three components: a subject, a predicate, and an object, collectively conveying a specific meaning. It is important to note that the constituents of a statement may be explicitly mentioned in the text, or they can be implicitly understood by considering the surrounding context and their association with ontological concepts.</p>
      <p id="Par19"><italic>Document-level annotations</italic> pertain to considering an entire textual document as a unit. In the <italic>MetaTron</italic> framework, there are two types of document-level annotations: <italic>label</italic> and <italic>assertion</italic> annotations. The former involves assigning one or more labels (representing individual concepts) to the document to classify its content. The latter enables the annotation of a document with a collection of assertions, subject-predicate-object triples linked to ontological concepts. These assertions provide a high-level description of the document’s content. Treating the assertions as machine-readable triples can be incorporated into a Resource Description Framework (RDF) graph, facilitating inference and knowledge representation.</p>
      <p id="Par20"><italic>MetaTron</italic> offers support for ontologies by enabling users to define a collection of ontological concepts identified by an identifier, a name, a type (e.g., <italic>gene</italic> or <italic>disease</italic>) and a description. Concepts are not necessarily tied to a specific ontology: this guarantees more customizability and flexibility, allowing the user to add concepts belonging not only to widely recognized and publicly accessible ontologies but also to user-designed or not yet published ontologies or vocabularies. Concepts can be uploaded in batch or added at the moment of annotation, allowing the user to enrich the set of concepts when needed.</p>
      <p id="Par21">These features of <italic>MetaTron</italic> enable users to annotate a diverse range of biomedical entities and relationships, including genes, proteins, diseases, drug treatments, and their associations. By allowing this customization and flexibility, <italic>MetaTron</italic> can be adapted to suit various use cases and user needs.</p>
      <p id="Par22">Collaborative annotation is a significant aspect of the <italic>MetaTron</italic> system, particularly in the context of collectively annotating a group of documents. This collaborative feature is crucial as it facilitates users to annotate documents together, improving annotation quality and accuracy. By working collaboratively, annotators can save time and enhance the overall annotation process. Through this feature, users can view annotations made by their colleagues and identify annotations that exhibit the highest agreement. Furthermore, the members of the document collection have access to comprehensive statistics about the entire collection or specific documents. This functionality enables them to monitor the progress of annotations and visually analyze the complete set of annotations, considering various annotation types and levels of agreement.</p>
      <p id="Par23">To enhance the user experience and expedite the annotation process, <italic>MetaTron</italic> incorporates <italic>AutoTron</italic>, a feature that offers automated predictions within the system. <italic>AutoTron</italic> is a framework designed to automatically annotate relationships and assertions, allowing users to implement their methods as desired. Leveraging automatic annotations is pivotal in enhancing and accelerating the overall annotation workflow by offering users an initial set of annotations that can be modified. The <italic>AutoTron</italic> system operates as a plug-and-play mechanism, enabling users to integrate their custom automatic annotation methods seamlessly. Additionally, <italic>MetaTron</italic> includes two pre-built methods specifically designed for automatically annotating gene-disease associations and gene expression-cancer associations, further augmenting the automated annotation capabilities.</p>
      <p id="Par24"><italic>MetaTron</italic> is designed to be highly adaptable and can be customized to suit any area of interest. It offers an easy-to-use customization process, where documents can be uploaded in various formats, such as JSON, CSV, TXT, and PDF, using the integration with GROBID (GeneRation Of BIbliographic Data) [<xref ref-type="bibr" rid="CR64">64</xref>]. GROBID is open-source software that uses machine learning techniques to extract structured data such as author names, affiliations, abstracts, publication dates, and references from scientific articles. This feature helps detect the various sections of a publication. Additionally, <italic>MetaTron</italic> incorporates REST APIs such as PubMed, Semantic Scholar, and OpenAIRE to enable users to upload one or more PMIDs (for PubMed) or DOIs (for Semantic Scholar and OpenAIRE) and annotate related information such as the title, abstract, authors, venue, and date of publication.</p>
      <p id="Par25">Additional features of <italic>MetaTron</italic> are the following: (i) an easy-to-use user interface that supports the automatic saving of the annotations; (ii) integration of keyboard shortcuts to navigate between documents and perform new annotations; (iii) support for the download of annotations in JSON, CSV, BioC/XML formats; (iv) support for the upload new annotations from JSON or CSV file; (v) support for user-defined style properties such as the colors of the mentions, the size or the line height of the textual content; (vi) multilingual support; (vii) support for annotation suggestions; (viii) IAA support, implemented through Fliess’ kappa, Cohen’s kappa and majority voting; (ix) dockerized and online instance available, (x) support for multiple ontologies, (xi) connection to ORCID, (xii) support for multiple annotation rounds.</p>
      <sec id="Sec4">
        <title>Architecture</title>
        <p id="Par26">
          <fig id="Fig2">
            <label>Fig. 2</label>
            <caption>
              <p>MetaTron architecture. <italic>MetaTron</italic> has three layers: the <italic>Data layer</italic> (a PostgreSQL database for data and annotations), the <italic>Business layer</italic> (with a REST API and services for automatic annotation and PDF parsing), and the <italic>Presentation layer</italic> (the user interface)</p>
            </caption>
            <graphic xlink:href="12859_2024_5730_Fig2_HTML" id="MO2"/>
          </fig>
        </p>
        <p id="Par27">The system architecture is illustrated in Fig. <xref rid="Fig2" ref-type="fig">2</xref> and can be divided into three layers: a data layer, a business layer, and a presentation layer.</p>
        <p id="Par28">The data layer is based on a PostgreSQL database that stores the annotations data, information about collections, concepts, and documents. This layer is responsible for managing the persistence and retrieval of data and ensuring data integrity.</p>
        <p id="Par29">The business layer comprises a REST API and a business logic implemented using the Django Python web framework.<xref ref-type="fn" rid="Fn1">1</xref> The REST API acts as an intermediary between the presentation layer and the data layer, while the business logic handles and processes data retrieved from the database based on the application’s needs. This layer also utilizes additional services such as <italic>AutoTron</italic> (presented in Sect. <xref rid="Sec8" ref-type="sec">2.2.2</xref>) and GROBID [<xref ref-type="bibr" rid="CR64">64</xref>].</p>
        <p id="Par30">The presentation layer is responsible for displaying the data to the users and receiving their input. It is developed using ReactJS, HTML, CSS, and JavaScript. This layer interacts with the business logic layer through the REST API to retrieve and display the data to the user.</p>
        <p id="Par31">Overall, this architecture provides a clear separation of concerns between the different layers, improving the system’s maintainability, scalability, and modularity. Using a database, a REST API, and additional services enhances the system’s data management and processing capabilities.</p>
      </sec>
      <sec id="Sec5">
        <title>Availability</title>
        <p id="Par32"><italic>MetaTron</italic> is released as an online and dockerized instance. The online instance is available at <ext-link ext-link-type="uri" xlink:href="https://metatron.dei.unipd.it">https://metatron.dei.unipd.it</ext-link>.<xref ref-type="fn" rid="Fn2">2</xref> An online demo and tutorial is available at: <ext-link ext-link-type="uri" xlink:href="https://metatron.dei.unipd.it/demo">https://metatron.dei.unipd.it/demo</ext-link>.</p>
        <p id="Par33">It is intended to be used with scientific publications—e.g., scientific articles or publications in PubMed. To use <italic>MetaTron</italic>, it is necessary to sign up by providing a username, a password, and a profile that identifies the level of expertise. Once signed up, the user will be asked to specify their level of expertise. They can create new document collections and invite other collaborators to join the project.</p>
        <p id="Par34"><italic>MetaTron</italic> is also released as a <italic>Docker container</italic> which guarantees cross-platform portability, scalability, and isolation. Furthermore, the dockerized version can be utilized by users who want to upload collections of documents whose content must be kept private, in all the cases where the network is not fully operational or when users want to introduce new features (i.e., new methods for automatic relation extraction). A local installation is also required if the user works with confidential documents or in a privacy-preserving setting. The dockerized version also eases the installation of <italic>MetaTron</italic> in a private network setting when the documents are unavailable on the Web, but distributed collaboration amongst the annotators is required. The installation procedure is detailed in the <italic>MetaTron</italic> repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/GDAMining/metatron/">https://github.com/GDAMining/metatron/</ext-link>), where the source code is publicly available.</p>
      </sec>
    </sec>
    <sec id="Sec6">
      <title>Annotation interface</title>
      <p id="Par35">
        <fig id="Fig3">
          <label>Fig. 3</label>
          <caption>
            <p>MetaTron user interface. The main annotation interface consists of five distinct blocks: in the main header (<bold>1</bold>), the user can navigate to other web pages and logout; in the document header (<bold>2</bold>), there is the main information about a document; in annotation panel (<bold>3</bold>) it is possible to check the annotation status; the document takes the largest part of the page to annotate (<bold>4</bold>); and in the vertical toolbar (<bold>5</bold>) it is possible to access several functions a user can perform during the annotation process</p>
          </caption>
          <graphic xlink:href="12859_2024_5730_Fig3_HTML" id="MO3"/>
        </fig>
      </p>
      <p id="Par36">The <italic>MetaTron</italic> annotation interface and its features have been designed to be intuitive and facilitate and speed up the entire annotation process. <italic>MetaTron</italic> annotation interface is illustrated in Fig. <xref rid="Fig3" ref-type="fig">3</xref> that we use as a guide to illustrate the tool’s main features. Upon successful login, the system presents the user with the most recently annotated document. At the top of the annotation interface, there is the <italic>main header</italic> (<bold>1</bold>). The <italic>Home</italic>, <italic>Collections</italic>, <italic>Statistics</italic> buttons can be used to navigate to home—i.e., the page illustrated in Fig. <xref rid="Fig3" ref-type="fig">3</xref>, collections, and statistics web pages. The user name is displayed at the top right of the page and can be used to log out. The <italic>document header</italic> (<bold>2</bold>), placed below the main header, includes the information about the current document identifier—e.g., pubmed_27839516—and the related collection name—e.g., pubmed_collection. Two arrow buttons allow the user to navigate between the documents of the collection; it is worth noting that it is also possible to navigate from one document to another through a custom shortcut designed to allow the user to change documents relying only on the keyboard. The <italic>Delete</italic> button deletes the annotations for the current document, while the <italic>Assertion</italic> button creates a new assertion as a document-level annotation.</p>
      <p id="Par37">By clicking on the <italic>Annotation</italic> button, a drop-down menu appears, enabling the user to choose one or more annotation types. For each selected annotation type, the list of annotations is then displayed on the right-hand side of the annotation interface, in the <italic>annotation panel</italic> (3). The user can view, add, modify, or delete their annotations using this panel. The largest portion of the page is taken by the textual document (4) the user annotates. The <italic>vertical toolbar</italic> (5) provides a set of functionalities illustrated in Fig. <xref rid="Fig3" ref-type="fig">3</xref> from (A) to (K), that can be accessed directly from the main annotation interface improving, and speeding up the annotation procedure, and minimizing the number of actions to be performed. In (A), the list of documents of the collections is shown. The user can filter documents based on whether they contain at least one annotation and search for a specific document to annotate using its ID. In (B), the user can view a list of document collections available for annotation. Each collection is represented by a button displaying the collection name and the percentage of annotated documents. The button color varies based on the percentage of documents that have been annotated—e.g., green color is used when the user annotated more than 80% of documents, while red color is used when the user annotated less than 20% of documents. By clicking on a collection, it is possible to start annotating its documents. In (C), the users who annotated the current document are listed. It is possible to load the annotation of one of the users in the list by clicking on the associated username. (D) allows the user to open two tables containing <italic>personal</italic> and <italic>global</italic> annotations statistics overview for the current document; the former concerns the annotations of the current user, while the latter those of all the annotators. Each table contains the number of mentions, associations, mentions-concepts, relationships, assertions, and labels annotated. In the <italic>global</italic> overview, the agreement computed with the Fleiss’ kappa measure is provided. (E) allows the user to customize <italic>MetaTron</italic>. It is possible to: hide or display specific sections of the document, increase or decrease the font size and the line height, and set the color associated with each concept type. These settings have been defined to facilitate and speed up the annotation workflow and improve document readability. (F) enables to download of the annotations. The user has to specify the file format—e.g., JSON, CSV, BioC, the annotation type—e.g., mentions, concepts, relationships, and the annotator username, choosing between all the users who annotated that document. (G) opens an upload panel where only the user who created the collection can upload new lists of documents and ontological concepts. Documents can be uploaded in several formats: CSV, TXT, JSON, and PDF. The user can search for a specific publication in PubMed (by providing the related PMID), in Semantic Scholar, and OpenAIRE (by providing the related DOI): <italic>MetaTron</italic> takes advantage of their REST APIs to retrieve the title, the abstract, the authors, the date of publication, and the venue information. The user can upload a new set of annotations in CSV or JSON format that will be automatically loaded in <italic>MetaTron</italic>. In (H), the user can hide the ontological concepts associated with the mentions and visualize only the annotated mentions increasing the document’s readability. In (I), it is possible to rely on <italic>AutoTron</italic> to annotate the current document automatically for specific cases. (J) and (K) open new tabs with the <italic>MetaTron</italic> instructions, and the credits respectively.</p>
      <sec id="Sec7">
        <title>Manual annotation</title>
        <p id="Par38"><italic>Mention annotation</italic> Mentions are textual spans that can be linked to one or more ontological concepts. In <italic>MetaTron</italic>, a mention can either consist of one or more consecutive tokens (or words), where a token is a sequence of characters between two spaces or a substring of one or more contiguous tokens. In this case, the first or last character of the mention does not necessarily coincide with the character that follows or precedes a space. It is possible to select a mention that consists of two or more consecutive tokens by clicking on the first and the last words, respectively: all the words comprised between the two selected will be part of the new mention. To create a mention containing a single token, it is possible to double-click on the desired token. To annotate a substring, hence selecting a part of the token (or two or more consecutive tokens), it is possible to drag and drop the mouse from the first to the last characters of the substring. <italic>MetaTron</italic> allows for the selection of overlapping mentions, meaning that a piece of text already included in a mention can be selected.</p>
        <p id="Par39">This implementation is based on our direct experience with other annotation tools, which has allowed us to assess the pros and cons of various possible implementations. We have decided to implement the annotation of mentions both through drag and drop and by clicking on individual words to allow the user to annotate both specific textual substrings and mentions of two or more words quickly, streamlining and expediting the workflow.</p>
        <p id="Par40">When the textual content of a mention occurs more than once in the textual document, it is possible to annotate all the mentions simultaneously. When a mention is annotated, a new modal will appear if its content occurs more than once, and the user can decide to annotate the mentions altogether.</p>
        <p id="Par41">It is possible to access the <italic>mention panel</italic> illustrated in Fig. <xref rid="Fig4" ref-type="fig">4</xref> by performing a right-click on a mention. This panel includes all possible actions and annotations related to the selected mention. From this panel, users can get information about the mention—e.g., the date of annotation and the number of annotators who annotated that mention for that document, receive some suggestions about the concepts to link to the mention, perform new annotations—i.e., add a new concept or a new relationship, and delete the mention. The option <italic>Annotate all</italic> finds all the occurrences of a mention in the document and annotates them simultaneously.<fig id="Fig4"><label>Fig. 4</label><caption><p>MetaTron mention panel. The mention panel opens when the user right-clicks on the desired mention. It allows the user to get more information about the mention, receive suggestions about the concepts to link, add new concepts or relationships, annotate similar mentions, and delete the mention</p></caption><graphic xlink:href="12859_2024_5730_Fig4_HTML" id="MO4"/></fig></p>
        <p id="Par42"><italic>Concepts linking</italic> To link a new ontological concept to a mention, the user can open the mention panel of a mention and select the <italic>Add Concept</italic> option. A new modal will appear, allowing the user to select a concept. Each collection of documents has a list of user-defined ontological concepts, not necessarily tied to a single ontology. From the modal, the user can explore this set of concepts, filter them according to their type, name, and identifier, and view the associated description. If the list of ontological concepts of the collection is large, to aid the user in selecting a concept, <italic>MetaTron</italic> provides auto-completion facilities to find the desired concept easily.</p>
        <p id="Par43">The incorporation of concept annotations in our system mirrors the approach employed by various tools, such as <italic>brat</italic> and <italic>INCEpTION</italic>. Nevertheless, we observed that our implementation is characterized by its intuitiveness. By positioning the concept above the corresponding mention and employing distinct colors based on the type, users can readily discern between different concepts, thereby augmenting the immediacy of the annotation experience.</p>
        <p id="Par44">Alternatively, if the concept is unavailable in the provided list, the user can define a new concept that will be automatically added to the collection’s concepts list. In this case, the user must define the concept’s type, name, URI (or ID), and an optional description.<fig id="Fig5"><label>Fig. 5</label><caption><p>MetaTron concept selection modal. The concept selection modal allows the user to select a concept to link to a mention. The user can filter the concepts according to the concept type and use auto-completion facilities to filter further the list of concepts (<bold>A</bold>). Once a concept is selected, its description will automatically appear in the modal (<bold>B</bold>)</p></caption><graphic xlink:href="12859_2024_5730_Fig5_HTML" id="MO5"/></fig></p>
        <p id="Par45">In Fig. <xref rid="Fig5" ref-type="fig">5</xref>, we can see how to link a concept to a mention. The user can filter the concepts to choose from, specifying the concept type—i.e., <italic>disease</italic> in the figure. By typing the first letters of the desired concepts, the available options are automatically shown (A in Fig. <xref rid="Fig5" ref-type="fig">5</xref>). The required information typically consists of the concept type and the name to select a concept. The user is required to select the ID of the concept only if two or more concepts share the same name but with different URIs. Once a concept is selected, the related description will automatically appear (B in Fig. <xref rid="Fig5" ref-type="fig">5</xref>). If no option is shown, the concept is not on the list, and it is possible to add a new concept.</p>
        <p id="Par46">If a mention is linked to one or more concepts, by clicking on the <italic>Annotate all</italic> option of the mention panel, it is possible to locate all the instances of that mention in the document and associate them with the same set of concepts simultaneously.</p>
        <p id="Par47">Each mention can have more than one linked concept. The concepts linked to a mention are displayed above the mention; they are clickable so that the user can be provided with the information about the concept—i.e., the URI, the name, the description, and the type. In the annotation interface, each concept can have a different color depending on its type: in Fig. <xref rid="Fig3" ref-type="fig">3</xref>, for example, concepts of type <italic>Disease</italic> and the associated mentions are highlighted in pink, while <italic>Gene</italic> type concepts in dark blue.<fig id="Fig6"><label>Fig. 6</label><caption><p>Relationship annotation. When the user annotates a new relationship, the document’s content is blurred except for the mentions highlighted with different colors, which can be selected as subject, predicate, and object, respectively. The three steps to create a relationship are shown in the example examined. First, the object mention is selected from the menu (<bold>1</bold>); then, it is selected a predicate (<bold>2</bold>): by clicking on <bold>A</bold>, it is possible to manually type the predicate of the relationship, while by clicking on <bold>B</bold> it is possible to select a concept. In (<bold>3</bold>), the final relationship is represented: it comprises two mentions—the subject and the object and an ontological concept—the predicate. Each relationship element has a different color to make each element easily recognizable. The panel on the right allows the user to update and save the relationship</p></caption><graphic xlink:href="12859_2024_5730_Fig6_HTML" id="MO6"/></fig></p>
        <p id="Par48"><italic>Relationship annotation</italic> A relationship comprizes three primary components: a <italic>subject</italic>, a <italic>predicate</italic>, and an <italic>object</italic>; the relationship always starts from the subject and ends with the object. Each relationship element can be represented by either an ontological concept or a textual mention (with or without linked concepts). At least one of the three components of a relationship must be a mention.<xref ref-type="fn" rid="Fn3">3</xref> A new relationship can be added through the <italic>Add Relationship</italic> option of the mention panel of a mention. By default, this mention will be the subject of the relationship. This action automatically shows a <italic>relationship panel</italic> that provides a comprehensive overview of the relationship and its components. All the other mentions composing the relationship (if any) can be added by clicking on each mention, or by right-clicking on the desired mention, it is possible to select its role. It is worth noting that it is always possible to change the role of a mention—e.g., a mention which was the subject can become the object. From the relationship panel, it is possible to select the concepts of the relationship by clicking on the <italic>Add predicate</italic>, <italic>Add subject</italic>, or <italic>Add object</italic> buttons. In Fig. <xref rid="Fig6" ref-type="fig">6</xref>, the creation of a new relationship is illustrated. The relationship comprises two mentions (the subject BRCA1 and the object breast cancer) and an ontological concept (the predicate Oncogenes). In <bold>1</bold>, the object mention is selected by declaring its role from the menu. In <bold>2</bold>, the predicate is selected. There are two ways to select a predicate: from <bold>A</bold>, it is possible to input a string manually that represents the predicate, while from <bold>B</bold>, it is possible to select a predicate concept. In <bold>3</bold>, the created relationship is shown. In the textual document, each relationship component has a different color depending on its role: the subject is highlighted in red, the predicate in green, and the object in orange. The three components are linked via arrows whose positions can be changed by the user to facilitate the document readability—i.e., each mention is surrounded by four points that determine the points where the arrows can start or end.</p>
        <p id="Par49">On the right, the <italic>relationship panel</italic>, for each relationship component, shows the type of the component—e.g., mention or concept, and its role—e.g., subject, predicate, object. The annotations panel provides an overview of all the relationships the user annotates; each relationship can be viewed, edited, and deleted directly from the interface.</p>
        <p id="Par50">In this implementation, each relationship component is manually chosen, whether a mention selected in the text or a concept chosen relying on the right panel. This approach, even though it may require more steps than previous annotations, allows the users to designate which element within the relationship should be identified as a mention and which as a concept. Unlike <italic>brat</italic> and <italic>INCEpTION</italic> that enable the annotation of relationships, <italic>MetaTron</italic> does not necessitate the subject and object to be mentions within the textual document. Relationships may span multiple sentences; at least one among subject, predicate and object must be a mention, while the remaining components can be concepts.<fig id="Fig7"><label>Fig. 7</label><caption><p>Relationships list. Overview of the relationships annotated by the user. The list is subdivided into subject, predicate, and object, each into concept types. It is also possible to filter the concepts composing the relationships according to the type and the name, taking advantage of auto-completion facilities</p></caption><graphic xlink:href="12859_2024_5730_Fig7_HTML" id="MO7"/></fig></p>
        <p id="Par51">In Fig. <xref rid="Fig7" ref-type="fig">7</xref>, we can see the relationships list. It is subdivided into three categories: subject, predicate, and object; each category is further subdivided into concept types. This layout provides the user with an overview of the different concept types characterizing subjects, predicates, and objects of the annotated relationships.</p>
        <p id="Par52"><italic>Assertions annotation</italic> Like relationships, assertions consist of a subject, predicate, and object, each represented by an ontological concept unlinked to any mention in the document. To add a new assertion (see Fig. <xref rid="Fig8" ref-type="fig">8</xref>), a dedicated button in the document header opens an <italic>assertion panel</italic> similar to the one provided for relationships. Users can create a new assertion by specifying the types, names, and URIs of the subject, predicate, and object concepts. The annotated assertions can be viewed, edited, and removed via the annotation panel by selecting the assertion annotation type; the annotation panel contains the assertions the user created. For each assertion, the subject, the predicate, and the object concepts are provided with information, including the date and number of annotators. Furthermore, each assertion can be viewed, edited, or deleted. As far as our current knowledge extends, <italic>MetaTron</italic> stands out as the initial tool to introduce the creation of document-level assertions. Therefore, we have opted to maintain a close resemblance between assertion annotations and relationship annotations. This decision aims to facilitate a seamless user experience, enabling users to bypass the need to acquaint themselves with a novel annotation methodology and expedite the overall annotation workflow.<fig id="Fig8"><label>Fig. 8</label><caption><p>Assertion details. The annotation panel contains an overview of the annotated assertions. For each assertion, the subject, the predicate, and the object concepts are provided; each assertion can be edited or deleted via the two buttons placed near the <italic>Assertion 1</italic> title. The date of annotation and the number of annotators are shown below the assertion</p></caption><graphic xlink:href="12859_2024_5730_Fig8_HTML" id="MO8"/></fig></p>
        <p id="Par53"><italic>Labels annotation</italic> Labels allow the user to classify the document. Each collection has its own set of labels specified at its creation. To add one or more labels to the document, the user must open the annotation panel and select the labels annotation type. Each label is a button that can be selected or selected by a click. In Fig. <xref rid="Fig9" ref-type="fig">9</xref>, the annotation panel shows an example of the labels that can be associated with the displayed document for the selected collection. The selected labels have a light blue background.<fig id="Fig9"><label>Fig. 9</label><caption><p>Labels annotation. The annotation panel contains the labels to assign to the document. Each label is a button that can be selected or deselected. All the selected labels have a colored background; the others have a transparent background</p></caption><graphic xlink:href="12859_2024_5730_Fig9_HTML" id="MO9"/></fig></p>
      </sec>
      <sec id="Sec8">
        <title>AutoTron: automatic annotations</title>
        <p id="Par54"><italic>AutoTron</italic> represents the automatic annotation component of <italic>MetaTron</italic>. As an automatic component, <italic>AutoTron</italic> can be implemented by the user at will. The only requirement lies on the I/O structure, where specific I/O data are required to integrate the component within <italic>MetaTron</italic>. Below, we first describe the general architecture of the <italic>AutoTron</italic> component and then present two different implementations used in two annotation tasks: document-level Gene-Disease Association (GDA) extraction and sentence-level Gene expression-Cancer Association (GCA) extraction. Both implementations are currently available in <italic>MetaTron</italic> and can be used by the user on the corresponding annotation tasks. Figure <xref rid="Fig10" ref-type="fig">10</xref> shows the <italic>AutoTron</italic> workflow to extract GCAs from a sample PubMed article (i.e., PubMed id: 24662820). The workflow involves three steps. First, users select the desired task (1), which in this case is GCA. Once the user clicks the <italic>annotate</italic> button, a loading icon indicates that the automatic annotation process is underway (2). Finally, the annotations generated by <italic>AutoTron</italic> are displayed to the user (3).<fig id="Fig10"><label>Fig. 10</label><caption><p>AutoTron workflow. Overview of the workflow to extract GCAs from PubMed article 24662820. In (1), users select the task (i.e., GCA). Then, once the annotation process starts, a loading icon indicates the process is underway (2). Finally, the generated annotations are displayed to users in (3)</p></caption><graphic xlink:href="12859_2024_5730_Fig10_HTML" id="MO10"/></fig></p>
        <p id="Par55"><italic>Architecture</italic><italic>AutoTron</italic> consists of two main components and specific I/O data requirements. The main components are EL and RE. EL assigns unique meanings to entities mentioned within text [<xref ref-type="bibr" rid="CR65">65</xref>, <xref ref-type="bibr" rid="CR66">66</xref>], whereas RE identifies and extract relations between linked entities mentioned in text [<xref ref-type="bibr" rid="CR67">67</xref>, <xref ref-type="bibr" rid="CR68">68</xref>]. The EL and RE modules are containers where different methods can be plugged in/out, but which must adhere to specific I/O data. In this regard, the I/O data consists of specific fields. In input, <italic>AutoTron</italic> requires a field containing the text to annotate. In the output, <italic>AutoTron</italic> provides, for each extracted relationship or assertion, the subject, predicate, and object concept IDs, names, and types, as well as the corresponding mention positions within the text (if any). Together, these fields allow <italic>MetaTron</italic> to seamlessly integrate any implementation of the <italic>AutoTron</italic> component. In other words, <italic>AutoTron</italic> represents a framework for automatically annotating relationships and assertions that users can implement at will.</p>
        <p id="Par56"><italic>Entity linking</italic> We consider different EL systems depending on the input text. When the input text comes from PubMed, we use the PubTator system [<xref ref-type="bibr" rid="CR69">69</xref>–<xref ref-type="bibr" rid="CR71">71</xref>]. PubTator provides automated annotations from state-of-the-art text mining systems for genes/proteins, genetic variants, diseases, chemicals, species, and cell lines. In particular, PubTator normalizes annotated genes to NCBI Gene [<xref ref-type="bibr" rid="CR72">72</xref>] identifiers and annotated diseases to MeSH [<xref ref-type="bibr" rid="CR73">73</xref>] identifiers. When the input text comes from sources different than PubMed, we use the MetaMapLite system [<xref ref-type="bibr" rid="CR74">74</xref>], a near real-time EL tool that identifies UMLS [<xref ref-type="bibr" rid="CR36">36</xref>] concepts within the biomedical text. MetaMapLite returns, among other information, the CUI, the preferred term, and the location in the text of the identified UMLS concepts.</p>
        <p id="Par57">The text annotated by EL systems is then passed to RE methods to perform GDA/GCA extraction.</p>
        <p id="Par58"><italic>GDA extraction</italic> The discovery of GDAs is one of the most pressing challenges to advance precision medicine and drug discovery [<xref ref-type="bibr" rid="CR75">75</xref>]. Therefore, the automatic extraction and curation of GDAs is pivotal to advancing precision medicine and providing knowledge to assist disease diagnostics, drug discovery, and therapeutic decision-making. To this end, we use a document-level RE method that adopts Multi-Instance Learning (MIL) to extract GDA assertions from text [<xref ref-type="bibr" rid="CR76">76</xref>, <xref ref-type="bibr" rid="CR77">77</xref>]. Under MIL, text sentences are divided into bags based on pairs of concepts, and the prediction of relations (i.e., predicates) occurs at the bag level. The use of MIL well suits the assertion annotation task, where subject, predicate, and object are not associated with mentions. As the underlying ML model, the RE method exploits Piecewise Convolutional Neural Network (PCNN) model [<xref ref-type="bibr" rid="CR21">21</xref>]. PCNN first encodes sentences using a CNN and then applies a piecewise max pooling operation. This operation divides each sentence into three segments based on the positions of the two given entities and returns the maximum value in each segment instead of a single maximum value over the entire sentence. For MIL, the RE method performs average-based aggregation. This aggregation strategy assumes that all sentences within the same bag contribute equally to the bag-level representation. In other words, the bag representation is the average of all its sentence representations.</p>
        <p id="Par59">We trained the method on the TBGA dataset [<xref ref-type="bibr" rid="CR78">78</xref>], a large-scale, semi-automatically annotated dataset for GDA extraction. TBGA contains over 200,000 instances and 100,000 bags, divided into four GDA types: Therapeutic, Biomarker, Genomic Alterations, and NA. Once trained, the RE method is deployed within <italic>AutoTron</italic>.</p>
        <p id="Par60"><italic>GCA extraction</italic> Cancer prevention is one of the century’s most pressing challenges that public health needs to face. In the last few years, the rise of microarray and next-generation sequencing technologies triggered the generation of large amounts of raw experimental data about gene expression-cancer interactions. These raw data require investigation, processing, and validation by experts to be used to guide diagnosis, assess prognosis, or predict therapy response [<xref ref-type="bibr" rid="CR75">75</xref>, <xref ref-type="bibr" rid="CR79">79</xref>]. The outcomes of experts’ analyses are (often) described in scientific publications in the form of GCAs. However, manual knowledge extraction requires high economic and time costs [<xref ref-type="bibr" rid="CR80">80</xref>–<xref ref-type="bibr" rid="CR82">82</xref>]. Thus, it is of paramount importance to assist manual GCA extraction through the use of automated methods. In this regard, we use a sentence-level RE method that combines the outcomes of different models to obtain the corresponding GCA. Specifically, the method combines three ML models, each of which predicts a specific aspect associated with GCAs. The considered aspects are the Change of Gene Expression (CGE), the Change of Cancer Status (CCS), and the Gene-Cancer Interaction (GCI). Once predicted, the different aspects are combined—following a set of inference rules defined in [<xref ref-type="bibr" rid="CR47">47</xref>, <xref ref-type="bibr" rid="CR83">83</xref>]—to infer the role the given gene has on the specific cancer disease. All the ML models adopt SciBERT [<xref ref-type="bibr" rid="CR84">84</xref>], a pre-trained language model based on BERT [<xref ref-type="bibr" rid="CR85">85</xref>]. SciBERT addresses the lack of high-quality, large-scale labeled scientific data by pretraining on scientific papers from Semantic Scholar [<xref ref-type="bibr" rid="CR86">86</xref>]. On top of it, a linear layer takes SciBERT pooled output. Predictions are scores in [0, 1]; the higher the score for an aspect value, the more the model believes the sentence expresses that particular (aspect) value.</p>
        <p id="Par61">The method has been used to build a large-scale Knowledge Base (KB) on GCAs [<xref ref-type="bibr" rid="CR87">87</xref>, <xref ref-type="bibr" rid="CR88">88</xref>]. In this work, we deploy the method as is within <italic>AutoTron</italic>.</p>
      </sec>
    </sec>
    <sec id="Sec9">
      <title>Collections and customization</title>
      <p id="Par62">
        <fig id="Fig11">
          <label>Fig. 11</label>
          <caption>
            <p>MetaTron collections interface. Overview of the main components of the collection page. The user can search for a specific collection or filter the collections list at the top-right. The <italic>Add collection</italic> button allows the user to create a new collection. The collections list takes up the largest part of the page. Each collection includes information that the collection’s creator can edit</p>
          </caption>
          <graphic xlink:href="12859_2024_5730_Fig11_HTML" id="MO11"/>
        </fig>
      </p>
      <p id="Par63">Collections are sets of documents that one or more users can annotate. The collection web page is illustrated in Fig. <xref rid="Fig11" ref-type="fig">11</xref>, and is accessible through the annotation interface by clicking the <italic>Collections</italic> button.</p>
      <p id="Par64">At the top of the collection page is a text field that allows users to search for a specific collection by its name. Additionally, the four buttons below enable users to filter the collections. The default filter is <italic>All</italic>, which provides users with a list of all the collections they can annotate. <italic>Created</italic> button shows collections created by the user, while <italic>Shared</italic> displays collections that the user can annotate and be created by another team member. Lastly, the <italic>Invited</italic> button shows collections the user has been asked to join as an annotator. A new collection can be created via the <italic>Add Collections</italic> button. Finally, in the remaining part of the page, the user’s collections (either the entire or filtered list) are listed. For each collection, the following information is provided in this order: the creator, the collection’s name, the date of creation, the number of documents, the number of documents annotated by at least one user, and the description; by clicking on <italic>Learn More</italic> button, located below the collection, the information about the annotators of the collection, and the list of labels are loaded. If the user is also the collection’s creator, they can add or remove one or more annotators and add new labels. The <italic>New Round</italic> button allows the user to create a new annotation round. Depending on the annotation task, the users can perform one or more rounds of annotation; hence, they annotate the collection documents several times. The collection’s creator can also decide on the annotators of each round so that different sets of annotators can contribute to different rounds. This option automatically duplicates the annotations of the last round and makes them available in a new collection. The annotators who access this new collection are provided with all the annotations they performed in the last annotations round for that collection. Encapsulating each round on a separate collection of documents and annotations allows each round to be independent of all the other rounds and facilitates the annotators’ work. The <italic>Documents</italic> button redirects the user to the collection’s documents web page containing the list of documents of the collection together with its annotations—a more detailed description of this table is provided in Sect. <xref rid="Sec10" ref-type="sec">2.4</xref>. The <italic>Annotate</italic> button allows the user to annotate that collection: the user will be automatically redirected to the annotation interface and provided with the last annotated document of the collection (if any, otherwise with the first document available). Only the creator is provided with the <italic>Delete</italic> button, which allows for the deletion of the entire collection and the related annotations.<fig id="Fig12"><label>Fig. 12</label><caption><p>MetaTron new collection form. This figure shows the information a user must provide to create a new collection of documents. The form must provide the collection’s name and description, a list of concepts, labels, and documents</p></caption><graphic xlink:href="12859_2024_5730_Fig12_HTML" id="MO12"/></fig></p>
      <p id="Par65">In Fig. <xref rid="Fig12" ref-type="fig">12</xref>, the form to add a new collection is illustrated. A user must provide the collection’s name and description to create a new collection. Then, the user is asked to provide a list of members authorized to annotate the new collection. This is not mandatory since a user may decide to work independently on a collection. The user is asked to add a list of labels necessary to perform label annotation. Also, in this case, adding a list of labels is not mandatory at the moment of collection creation. The creator can edit the collection’s annotators and labels at any moment. Uploading a set of ontological concepts is highly recommended to perform concept linking, relationship annotation, and assertion annotation. Ontological concepts must be uploaded in CSV or JSON files, and for each concept, it is mandatory to provide the URI (or ID), the name, and the type; a description is not mandatory but recommended. Since the files introducing new concepts must follow predefined structures, we provided two downloadable templates (one for the CSV and one for the JSON formats). It is worth noting that the set of provided ontological concepts is not tied to a specific ontology, allowing the user to fully customize the collection’s configuration with concepts belonging to different ontologies, which may also not be publicly available. The insertion of new concepts is always possible at any moment.</p>
      <p id="Par66">A collection must contain at least one document. The creator can upload one or more files in the following formats: JSON, CSV, TXT. The keys in JSON files and the CSVs’ headers will also be available for annotation. <italic>MetaTron</italic> supports uploading PDF files automatically parsed by GROBID.</p>
      <p id="Par67">To retrieve information about articles having a PMID, <italic>MetaTron</italic> relies on the PubMed REST API to obtain the article’s title, abstract, date of publication, authors, and venue. For publications with a DOI, <italic>MetaTron</italic> utilizes the REST APIs of OpenAIRE and Semantic Scholar to extract the same information. The information obtained from the REST APIs is extracted, processed, and presented to the collection users as documents that can be annotated.</p>
      <p id="Par68">By default, <italic>MetaTron</italic> provides the user with the entire document to annotate. However, the user can select specific parts of the document using the <italic>settings</italic> option in the vertical toolbar of the annotation interface.</p>
    </sec>
    <sec id="Sec10">
      <title>Collaborative features</title>
      <p id="Par69">
        <fig id="Fig13">
          <label>Fig. 13</label>
          <caption>
            <p>MetaTron documents collection. Overview of the documents web page related to a collection of interest. Each document is the row of a table; each row contains the information about a document, the annotators, and the total count of annotations performed for each annotation type. For each annotation type, it is possible to see an overview of the related annotations. The last column contains buttons allowing the user to download, visualize and delete the document</p>
          </caption>
          <graphic xlink:href="12859_2024_5730_Fig13_HTML" id="MO13"/>
        </fig>
        <fig id="Fig14">
          <label>Fig. 14</label>
          <caption>
            <p>MetaTron relationships overview modal. The modal shows the subject, predicate, and object elements for each relationship annotated in the document. The mention text and the location in the text are reported; for each concept, the type, name, and URI are reported. Finally, the modal shows the number of annotators and their usernames</p>
          </caption>
          <graphic xlink:href="12859_2024_5730_Fig14_HTML" id="MO14"/>
        </fig>
        <fig id="Fig15">
          <label>Fig. 15</label>
          <caption>
            <p>Linked concepts suggestions. The suggestion modal provides a list of concepts linked to a specific mention by the other annotators of the document. For each concept, the related information is provided; the user can link the suggested concept(s) by clicking the <italic>Accept</italic> button associated with the desired concepts</p>
          </caption>
          <graphic xlink:href="12859_2024_5730_Fig15_HTML" id="MO15"/>
        </fig>
      </p>
      <p id="Par70"><italic>The documents web page. </italic> The user can keep track of the collection’s annotation state via the documents web page, accessible by clicking on the <italic>Documents</italic> button under each collection’s set of information. The documents web page, illustrated in Fig. <xref rid="Fig13" ref-type="fig">13</xref>, contains a dynamic table where, for each document, general information is provided—e.g., the ID, the batch number, and the number of annotations categorized by annotation type. Finally, the last column is the same for all the documents and allows one to visualize the text of the document, download its annotations, and, if the user is the creator of the collection, delete the document and the related annotations. In addition, it is possible to view all the annotations performed for each type, together with the related annotators. An example is provided in Fig. <xref rid="Fig14" ref-type="fig">14</xref>, where the overview of the relationships annotated for a document in the list is shown. Each relationship is subdivided into subject, predicate, and object components. If one of the components is a mention, the related text is reported along with the section in the text where it has been found—e.g., the abstract, the starting, and ending indices (this information is under the <italic>location</italic> field). If one of the components is an ontological concept—unlinked to any mention, instead, it is reported its type, its name (named as <italic>concept</italic>), and its URI (or ID). Finally, the number of annotators and their usernames are reported. This feature allows the user to have a complete overview of the relationship. Furthermore, the information about the annotators is an important indicator of the annotator’s expertise and the reliability of the annotation.</p>
      <p id="Par71"><italic>Load the teammates’ annotations</italic> The user can load a particular teammate’s annotation for a document directly from the vertical toolbar (C in Fig. <xref rid="Fig3" ref-type="fig">3</xref>). Once loaded, the user can copy one or more annotations from the teammate, resulting in both the user and the teammate having the same (or partial) set of annotations.</p>
      <p id="Par72">This feature has been implemented to allow the user to visualize and interact with other members’ annotations. The possibility to copy other members’ annotations facilitates and speeds up the annotation process as the user does not have to create new annotations from scratch.</p>
      <p id="Par73"><italic>Receiving suggestions</italic> By accessing the <italic>Suggestion</italic> option in the panel of a mention, it is possible to visualize the list of ontological concepts that the other annotators linked to that mention. Figure <xref rid="Fig15" ref-type="fig">15</xref> shows an example of a suggestion modal. The concept type, name, URI (or ID), and the number of annotators are provided for each concept. The <italic>Accept</italic> button under placed a concept allows the user to assign that ontological concept to the mention; the <italic>Close</italic> button discards the suggestion. <italic>MetaTron</italic> allows users to link more than one suggested ontological concept to the same mention.</p>
    </sec>
    <sec id="Sec11">
      <title>Inter annotator agreement (IAA) and statistics</title>
      <p id="Par74"><italic>Inter annotator agreement (IAA)</italic><italic>MetaTron</italic> implements two IAA methods: majority voting, Fleiss’ kappa and Cohen’s kappa. The first method selects all annotated annotations by more than half of the document’s annotators. In <italic>MetaTron</italic>, viewing and editing the annotations selected through majority voting is possible. The majority voting-based annotations have two goals: (i) providing the user with information about the most frequent annotations, and (ii) facilitate and speed up the annotation process. The user can copy all the annotations selected via majority voting and edit them if needed; this allows the annotator to receive an initial set of annotations, consequently saving time. It is possible to load these annotations directly from the vertical toolbar (C in Fig. <xref rid="Fig3" ref-type="fig">3</xref>): they can be loaded by clicking on the user called <italic>IAA - Inter Annotator Agreement</italic>.</p>
      <p id="Par75">Fleiss’ kappa is a statistical measure that assesses the level of agreement between two or more annotators [<xref ref-type="bibr" rid="CR89">89</xref>]. In <italic>MetaTron</italic>, two Fleiss’ kappa agreement values are computed for each annotation type: one concerns the entire collection of documents, and one concerns each single document. It is possible to check Fleiss’ kappa agreement values on the <italic>Statistics</italic> web page, whose details are provided in the paragraph below.</p>
      <p id="Par76">Cohen’s kappa is a statistical measure ranging between <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-1$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2024_5730_Article_IEq1.gif"/></alternatives></inline-formula> and 1 that assesses the level of agreement between two raters rating the same set of elements [<xref ref-type="bibr" rid="CR90">90</xref>]. Similarly to Fleiss’ kappa, we provide the Cohen’s kappa for each annotation type; it is computed for the entire collection or each document, according to the users’ needs.</p>
      <p id="Par77">It is always possible to introduce new agreement functions according to the users’ needs and requirements: <italic>MetaTron</italic> is indeed flexible, and its modular design allows for integrating new functions and measures.</p>
      <p id="Par78">Having Fleiss’ kappa and Cohen’s kappa values provides a more complete view of annotation quality. Fleiss’ kappa can show how agreement varies with a different number of annotators, while Cohen’s kappa offers a more specific assessment between pairs of annotators. Using both coefficients ensures a more accurate evaluation of annotation reliability, considering the variety of annotators involved and the specific agreement between pairs of annotators.<fig id="Fig16"><label>Fig. 16</label><caption><p>Statistics tool bar. The statistics tool bar allows to select different types of statistics. In 1 and 2 it is possible to switch between personal and global statistics. In 3 it is possible to select two annotators and get the Cohen’s kappa agreement between them; in 4 the agreement amongo multiple annotation rounds is provided; in 5 it is possible to select one document and check its statistics</p></caption><graphic xlink:href="12859_2024_5730_Fig16_HTML" id="MO16"/></fig><fig id="Fig17"><label>Fig. 17</label><caption><p>Cohen’s kappa agreement. The Cohen’s kappa agreement is provided for a pair of annotators selected by the user. The user can decide to compute the Cohen’s kappa basing on a single document or on the entire collection</p></caption><graphic xlink:href="12859_2024_5730_Fig17_HTML" id="MO17"/></fig><fig id="Fig18"><label>Fig. 18</label><caption><p>Fleiss’ kappa rounds agreement. This modal provides a table where, for each round, it is provided the Fleiss’ kappa for each type. The user can select the document or, alternatively, can check the agreement computed on the entire collection</p></caption><graphic xlink:href="12859_2024_5730_Fig18_HTML" id="MO18"/></fig><fig id="Fig19"><label>Fig. 19</label><caption><p>General statistics. The table at the left shows each annotation type, the total count of annotations, and the inter-annotator agreement. The second table concerns the concepts linking annotation type and for each concept type, it is possible to see how many concepts have been linked and the related name. Finally, the last table shows, for each concept type, the number of concepts (both unlinked and linked to a mention) taking part in a relationship (or an assertion) and whose role is subject, predicate, and object, respectively</p></caption><graphic xlink:href="12859_2024_5730_Fig19_HTML" id="MO19"/></fig><fig id="Fig20"><label>Fig. 20</label><caption><p>Annotations and annotators statistics. The first histogram illustrates for each document of the collection, how many annotations have been performed for each annotation type. The second instead, shows the number of annotators for each document</p></caption><graphic xlink:href="12859_2024_5730_Fig20_HTML" id="MO20"/></fig><fig id="Fig21"><label>Fig. 21</label><caption><p>Concept types distribution. The <italic>Global</italic> pie chart illustrates the concepts type distribution of the concepts assigned in all the annotation types. The other pie charts illustrate the concept types distributions of the concepts (unlinked and linked to the mentions) taking part in relationships and assertions and whose role was subject, predicate, and object, respectively</p></caption><graphic xlink:href="12859_2024_5730_Fig21_HTML" id="MO21"/></fig></p>
      <p id="Par79"><italic>Collections’ statistics</italic> Detailed statistics are provided on the statistics web page. <italic>MetaTron</italic> provides two types of statistics: (i) <italic>personal</italic> statistics concern the single user, and (ii) <italic>global</italic> statistics concern the entire set of annotators of the collection. The two buttons at the top of the page allow the user to switch between personal ((1) in Fig. <xref rid="Fig16" ref-type="fig">16</xref>) and global ((2) in Fig. <xref rid="Fig16" ref-type="fig">16</xref>) statistics. (3) in Fig. <xref rid="Fig16" ref-type="fig">16</xref> allows the user to be provided with the Cohen’s kappa agreement between two users they selected and the agreement is computed for each annotation type. An example is provided in Fig. <xref rid="Fig17" ref-type="fig">17</xref>. (4) in Fig. <xref rid="Fig16" ref-type="fig">16</xref> allows the user to be provided with the Fleiss’ kappa agreement for each annotation type on each round of annotation. An example is provided in Fig. <xref rid="Fig18" ref-type="fig">18</xref>: for each round it is possible to see how the agreement evolves. The text area (5) in Fig. <xref rid="Fig16" ref-type="fig">16</xref>) allows the user to select a document to check the statistics of—by default, the statistics concern the entire set of collection documents An example of global statistics is illustrated in Figs. <xref rid="Fig19" ref-type="fig">19</xref>, <xref rid="Fig20" ref-type="fig">20</xref>, <xref rid="Fig21" ref-type="fig">21</xref>. <italic>General</italic> statistics, (A) in Fig. <xref rid="Fig19" ref-type="fig">19</xref> include the number of annotated documents, the number of annotators, the number of annotations for each annotation type, and the inter-annotator agreement computed basing on the Fleiss’ kappa measure. The <italic>Linked concepts overview</italic> section (B) displays the count of how many times an ontological concept has been linked to a mention. The list is subdivided into concept types. Additionally, in section (<bold>C</bold>), there are three lists: subject, predicate, and object lists. Similarly to the previous case, each list contains the concepts, grouped by concept type, that were annotated as a subject, predicate, or object, respectively. The <italic>Documents annotations overview</italic> section ((D) in Fig. <xref rid="Fig20" ref-type="fig">20</xref>) provides for each document of the collection the total number of annotations for each annotation type. In <italic>Annotators per document count</italic> ((E) in Fig. <xref rid="Fig20" ref-type="fig">20</xref>) the number of annotators is provided for each document. Finally, the pie chart in (F) in Fig. <xref rid="Fig21" ref-type="fig">21</xref> provides a global overview of the distribution of the concept types annotated in concepts linking, relationships, and assertions annotations. The three pie charts in (G) instead exclusively concern relationships and assertions and provide an overview of the concept types assigned to subjects, predicates, and objects, respectively. <italic>Personal</italic> statistics show the same set of statistics, except for the <italic>Annotators overview</italic>—which is not considered since personal statistics exclusively concern a single user. If a document is selected instead, the <italic>Documents annotations overview</italic> and the <italic>Annotators overview</italic> are not shown since these statistics concern the entire collection of documents. It is worth noting that it is always possible to access the statistics of the documents a user annotates via the <italic>statistics</italic> button in the vertical toolbar, which shows both the personal and the global statistics.</p>
    </sec>
  </sec>
  <sec id="Sec12">
    <title>Results</title>
    <p id="Par80">This section compares a subset of the annotation tools illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. The online tools selected for comparison are <italic>TeamTat</italic>, <italic>MedTAG</italic>, <italic>LightTag</italic>, and <italic>MetaTron</italic>. The offline tools instead are <italic>MetaTron (dockerized)</italic>, <italic>INCEpTION</italic>, and <italic>brat</italic>. While <italic>TeamTat</italic> and <italic>MetaTron</italic> target the biomedical domain, the other annotation tools are general purpose.</p>
    <p id="Par81">We provide a <italic>qualitative comparison</italic> where we outline the core functionalities of each tool. Furthermore, we provide a <italic>quantitative analysis</italic> by conducting experiments to assess the performance of each tool in tasks including mentions annotations and concepts linking, and relationships annotation. In the quantitative analysis, we did not consider <italic>MedTAG</italic>—as it does not support relationship annotation, and <italic>brat</italic>—as it has not been possible to automatically test it with a web agent.</p>
    <p id="Par82">We planned to include in our comparison also <italic>tagtog</italic> [<xref ref-type="bibr" rid="CR50">50</xref>], however, as of May 2023, the online version of <italic>tagtog</italic> did not allow us to add new documents to a collection; as a consequence, it has been impossible to qualitatively and quantitatively assess its performances. Furthermore, <italic>LightTag</italic> does not support entity linking, hence we cannot link a concept to a mention, we can only tag the mention with a concept type. However, we included this tool in the quantitative comparison as it is one of the newest tools of the past years and allows for relationships annotation.</p>
    <p id="Par83">In the last section, we describe a user study conducted on two tasks, namely GCA and GDA. GCA focuses on the annotation of relationships where subject and object are mentions and the predicate is an ontological concept. GDA focuses on the annotation of assertions. The user study involved 10 PubMed abstracts per task annotated by three experts in the biomedical domain. All the users were initially provided with the automatic annotations performed by AutoTron. We analyzed the results <italic>quantitatively</italic>, measuring the agreement among the annotators, and <italic>qualitatively</italic> via a questionnaire involving the annotators’ experience. Finally, we studied how AutoTron impacted on the annotations analyzing how many annotations generated automatically have been updated, removed, added or confirmed.</p>
    <sec id="Sec13">
      <title>Qualitative analysis</title>
      <p id="Par84">Figure <xref rid="Fig1" ref-type="fig">1</xref> illustrates an overview of the features characterizing a set of annotation tools. In the qualitative analysis we compare: <italic>MetaTron</italic>, <italic>MedTAG</italic>, <italic>TeamTat</italic>, <italic>brat</italic>, <italic>LightTag</italic>, and <italic>INCEpTION</italic>. The qualitative comparison is based on our direct experience with all these tools.</p>
      <p id="Par85"><italic>MetaTron</italic> is the unique tool that fully satisfies 23 of 24 criteria—active learning, and built-in prediction is only partially satisfied; specifically, it is the unique tool that satisfies the connection to ORCID (for login purposes) (F14) external libraries integration (F15). We see that <italic>TeamTat</italic>, and <italic>INCEpTION</italic> are the most complete tools: they fully satisfy 17 and 19 criteria, respectively, 5 criteria and 1 criterion, respectively, are only partially satisfied, and the remaining are not satisfied at all. Conversely, <italic>LightTag</italic>, is the least complete: 13 criteria are fully satisfied, 3 are partially satisfied, and 8 are not. Among the tools we compared <italic>MetaTron</italic> with, <italic>LightTag</italic> is the unique one without source code available (T2), does not show the date of the last version (or commit) (T1), does not allow for modification and redistribution (T6), is not free of charge (T7), and does not support ontologies (F4). <italic>brat</italic>, instead, is the unique tool that does not support document-level annotation; hence, it is impossible to associate one or more classes to a document. Finally, only <italic>TeamTat</italic> fully supports active learning (F5), and only <italic>TeamTat</italic> and <italic>MedTAG</italic> support the annotation of PubMed abstracts. Online availability (T3) is satisfied by <italic>TeamTat</italic> and <italic>LightTag</italic>; <italic>INCEpTION</italic> and <italic>brat</italic> are available offline—the online demo is available for <italic>INCEpTION</italic>, and <italic>MetaTron</italic> and <italic>MedTAG</italic> are available online and can be locally installed. Online tools are usually easier and faster to configure than locally installable ones: online tools usually require the user to upload a set of documents in predefined formats and a schema for the annotation, which usually includes the definition of the labels to classify the documents, or the definition of the entity types to associate. However, online tools might suffer from network delays that may occur when a large amount of data is uploaded/downloaded. Offline tools guarantee isolation and preserve data privacy; simultaneously, their installation might be difficult, and the configuration is time-demanding for someone with little technical expertise.</p>
      <p id="Par86">As of our experience with the compared tools, the offline tools—<italic>brat</italic> and <italic>INCEpTION</italic>, were the least intuitive and required the most time to be configured. In particular, <italic>INCEpTION</italic> required a deep study of the documentation as the notion of annotation layers that characterize the tool is not intuitive. On the other hand, <italic>TeamTat</italic> has the fastest and easiest configuration: it requires the definition of a collection and the upload of a set of documents. <italic>LightTag</italic>, together with the set of documents to annotate, required the user to define one or more annotation schemas and relation schemas: the former allows to define the tags to associate to the entities, and the classes to classify the documents with, while the latter allows user to specify the relation types—i.e., the labels to be assigned to the edge between two entities of the relationship. <italic>MedTAG</italic> instead requires uploading a set of documents, labels for the document-level annotation, and a set of ontological concepts.</p>
      <p id="Par87">The compared tools report substantial differences in the supported document formats: <italic>MetaTron</italic>, <italic>INCEpTION</italic>, and <italic>TeamTat</italic> are the unique tools supporting the upload of PDFs and TXT files. <italic>MetaTron</italic>, <italic>MedTAG</italic>, <italic>LigthTag</italic> support JSON and CSV. <italic>TeamTat</italic>, <italic>MetaTron</italic>, and <italic>MedTAG</italic> allow the user to upload PubMed abstract. Only <italic>MetaTron</italic> allows the user to specify a DOI and annotate the related abstract extracted from Semantic Scholar or OpenAIRE. In this respect <italic>MetaTron</italic> is the only tool that supports all the aforementioned formats and is integrated with three different APIs to abstracts upload.</p>
      <p id="Par88">We analyzed the annotation procedure for what concerns: labels annotation, mentions annotation, concepts linking, and relationship annotation. All the analyzed tools implement mention annotation via drag and drop, except for <italic>MedTAG</italic> where mentions are selected by clicking on each token composing the mention. <italic>LightTag</italic> is the only tool that allows users to perform entity tagging and does not support entity linking. <italic>MetaTron</italic> is the unique tool providing three modalities to select mentions, enhancing the annotation experience. In all the other tools, the concepts linked to a mention are always selected, specifying the related type and URI or name. Also, label annotation is similar in all the examined tools and can be achieved by clicking on the labels to be associated with the document. Relationships annotation may vary depending on the examined tool: <italic>LightTag</italic> and <italic>TeamTat</italic> for example, support n-ary relationships; in <italic>MetaTron</italic> instead, a relationship always has three components, and only one of them must be a mention annotated in the document. In <italic>INCEpTION</italic> and <italic>brat</italic>, instead, the source and the target in the relationships must be mentions. <italic>MetaTron</italic> is the most versatile tool among those described, as in a relationship, subject and object are not required to be mentions in the text. Additionally, <italic>MetaTron</italic> is the only tool to propose assertions annotation, not necessarily tied to sentences in the text. In this respect, the availability of document-level annotations is a relevant feature for <italic>MetaTron</italic> as, according to [<xref ref-type="bibr" rid="CR10">10</xref>], the most adopted biomedical annotation tool does not implement this feature.</p>
      <p id="Par89">We compared <italic>MetaTron</italic> and the tools in terms of collaborative features and agreement; <italic>MetaTron, MedTAG, TeamTat, INCEpTION and LightTag</italic> support the collaboration between multiple annotators. Specifically, <italic>TeamTat</italic> supports multiple rounds of annotation and provides the annotators with agreement and disagreement between the annotators, as well as disagreement resolution. <italic>LightTag</italic> implements task management features, assigning tasks to different groups of annotators based on specific needs—e.g., language, and allowing project managers to keep track of annotations and agreement. <italic>tagtog</italic> implements user roles and allows for the definition of a set of custom annotation guidelines. In this respect, <italic>MetaTron</italic> implements different annotation rounds and provides some additional collaborative features to facilitate and speed up the annotators’ work. Specifically, it allows annotators to copy other members’ annotations, and the annotation with the highest agreement is computed via majority voting. In addition, <italic>MetaTron</italic> implements annotation suggestions: given a mention, users can see what the concepts assigned by the other annotators are and select one of them accordingly, depending, for example, on how many users have linked a specific ontological concept. For each annotation performed, the user can keep track of the number of users who performed the same annotation and change it accordingly. In <italic>MetaTron</italic>, the collection’s creator can keep track of the annotation progress and is responsible for selecting the annotators of each round. All the users can see the entire sets of annotations of each collection document and the related annotators. <italic>MetaTron</italic> is the unique tool providing different agreement measures (it implements both Fleiss’ kappa and Cohen’s kappa) computed on the entire collection or on single documents.</p>
      <p id="Par90"><italic>brat</italic> and <italic>INCEpTION</italic> are included in our comparison even if they do not target the biomedical domain.</p>
    </sec>
    <sec id="Sec14">
      <title>Quantitative analysis</title>
      <p id="Par91">To compare the performance of the selected manual annotation tools, we conducted a series of experiments on two different tasks: concepts linking, and relationships annotation. We did not treat mention annotation as a separate task because the annotation method was the same across all the annotation tools. Our experiments concerned the time elapsed and the number of clicks required to annotate a collection of the same 15 documents.</p>
      <p id="Par92">To evaluate the performances of the selected tools we relied on Selenium,<xref ref-type="fn" rid="Fn4">4</xref> an open-source testing framework used to automate web browsers. We designed four web agents, one for each annotation tool—the same web agent has been used for the two instances of <italic>MetaTron</italic>, and we used them to simulate the concepts liking and relationships annotations task on a collection of 15 abstracts extracted from PubMed. In order to simulate the annotator activity, we selected abstracts of various lengths; the mentions, the linked concepts, and the relationships have been extracted using <italic>AutoTron</italic>. Overall, <italic>AutoTron</italic> extracted 94 mentions and 71 relationships; specifically, for each abstract, a minimum of 2 mentions and 1 relationship, and a maximum of 13 mentions and 9 relationships were found. Each mention has been linked to exactly one concept. Variable delays were introduced based on the examined tool to prevent errors caused by server overload and to allow sufficient time for request processing. To provide a fair analysis, we treated online tools and offline tools separately, as the performance of online tools is influenced by the server hosting the application, while offline tools rely on the individual machine running the test.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Overview of the time spent to perform mentions annotation (MA), concepts linking (CL), and relationship annotation (RA) on a set of 15 documents. The reported value of average (AVG), standard deviation (STD), median (MED), and 5<italic>th</italic> and 95<italic>th</italic> percentiles refer to the time spent annotating 15 documents 50 times</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left"/><th align="left" colspan="5">MA + CL + RE</th></tr><tr><th align="left"/><th align="left"/><th align="left">AVG</th><th align="left">STD</th><th align="left">5th</th><th align="left">MED</th><th align="left">95th</th></tr></thead><tbody><tr><td align="left" rowspan="3">Online</td><td align="left">MetaTron</td><td align="left"><bold>638</bold>.<bold>73</bold></td><td align="left">1.67</td><td align="left">636.97</td><td align="left"><bold>638</bold>.<bold>22</bold></td><td align="left">642.49</td></tr><tr><td align="left">TeamTat</td><td align="left">842.93</td><td align="left">0.76</td><td align="left">841.67</td><td align="left">843.03</td><td align="left">844.18</td></tr><tr><td align="left">LightTag</td><td align="left">661.13</td><td align="left">1.23</td><td align="left">659.35</td><td align="left">660.85</td><td align="left">663.07</td></tr><tr><td align="left" rowspan="2">Offline</td><td align="left">MetaTron</td><td align="left"><bold>642</bold>.<bold>39</bold></td><td align="left">1.30</td><td align="left">640.35</td><td align="left"><bold>642</bold>.<bold>28</bold></td><td align="left">644.54</td></tr><tr><td align="left">INCEpTION</td><td align="left">704.85</td><td align="left">2.14</td><td align="left">701.53</td><td align="left">705.12</td><td align="left">707.33</td></tr></tbody></table><table-wrap-foot><p>The boldface values represent avg and median results of the tools with the best performances, i.e., the lowest time taken to annotate 15 documents 50 times</p></table-wrap-foot></table-wrap></p>
      <p id="Par93">We conducted an analysis concerning the time spent annotating the entire collection. The results are shown in Tables <xref rid="Tab1" ref-type="table">1</xref> and <xref rid="Tab2" ref-type="table">2</xref>. We compared the annotation tools on the average (AVG), the median (MED), standard deviation (STD), the 5<italic>th</italic> and the 95<italic>th</italic> percentiles of the time required to annotate the entire collection 50 times. We studied the time taken to perform two tasks: (i) performing an entire annotation, which comprehends mentions annotation (MA), concepts linking (CL), and relationships annotation (RA); and (ii) annotating the mentions (MA) and linking the concepts (CL). We remark that <italic>LightTag</italic> does not support concept linking, it allows only to tag the mentions with concept types.</p>
      <p id="Par94">Looking at the results achieved by the online tools in the entire annotation (Table <xref rid="Tab1" ref-type="table">1</xref>), we see that <italic>MetaTron</italic> is the most efficient tool in terms of average time, as <italic>MetaTron</italic> required 638.73 s on average (while <italic>TeamTat</italic> requires 842.93 s). <italic>LightTag</italic>’s performance falls between <italic>TeamTat</italic> and <italic>MetaTron</italic>. However, <italic>TeamTat</italic> does not reveal significant fluctuations during the 50 annotation rounds; this means that it is a tool with good stability that performs well during long annotation sessions. The substantial difference between the performance of <italic>TeamTat</italic> and those of <italic>MetaTron</italic> should be attributed to how the annotation is performed: we noticed <italic>TeamTat</italic> needed longer delays to save each annotation correctly. These aspects might not be visible to a human annotator who performs slower than the web agent; as such, a human annotator will take more time than an automatic one to annotate the selected set of documents. In the comparison involving the offline tools, the dockerized instance of <italic>MetaTron</italic> achieved better results than <italic>INCEpTION</italic>, as <italic>MetaTron</italic> required 642.39 s on average and <italic>INCEpTION</italic> 704.85.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Overview of the time spent to perform mentions annotation (MA) and concepts linking (CL) on a set of 15 documents. The reported value of average (AVG), standard deviation (STD), median (MED), and 5<italic>th</italic> and 95<italic>th</italic> percentiles refer to the time spent annotating 15 documents 50 times</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left"/><th align="left" colspan="5">MA + CL</th></tr><tr><th align="left"/><th align="left"/><th align="left">AVG</th><th align="left">STD</th><th align="left">5th</th><th align="left">MED</th><th align="left">95th</th></tr></thead><tbody><tr><td align="left" rowspan="3">Online</td><td align="left">MetaTron</td><td align="left"><bold>329</bold>.<bold>25</bold></td><td align="left">1.53</td><td align="left">327.13</td><td align="left"><bold>328</bold>.<bold>96</bold></td><td align="left">331.93</td></tr><tr><td align="left">TeamTat</td><td align="left">446.33</td><td align="left">1.34</td><td align="left">444.28</td><td align="left">446.53</td><td align="left">448.53</td></tr><tr><td align="left">LightTag</td><td align="left">412.61</td><td align="left">1.92</td><td align="left">409.48</td><td align="left">412.68</td><td align="left">415.65</td></tr><tr><td align="left" rowspan="2">Offline</td><td align="left">MetaTron</td><td align="left">330.35</td><td align="left">1.37</td><td align="left">327.98</td><td align="left">330.28</td><td align="left">331.96</td></tr><tr><td align="left">INCEpTION</td><td align="left"><bold>326</bold>.<bold>85</bold></td><td align="left">1.91</td><td align="left">323.60</td><td align="left"><bold>327</bold>.<bold>00</bold></td><td align="left">329.92</td></tr></tbody></table><table-wrap-foot><p>The boldface values represent avg and median results of the tools with the best performances, i.e., the lowest time taken to annotate 15 documents 50 times</p></table-wrap-foot></table-wrap></p>
      <p id="Par95">For what concerns the second task, mentions annotation and concepts linking, whose results are shown in Table <xref rid="Tab2" ref-type="table">2</xref>, similarly to the previous case, the best online tool is <italic>MetaTron</italic>, which required 329.25 s on average while the one that requires the highest average time is <italic>TeamTat</italic>, that required 446.33 s. Also in this case the performances of <italic>LightTag</italic>, 412.61 s, fall between those of <italic>MetaTron</italic> and <italic>TeamTat</italic>. <italic>INCEpTION</italic> and <italic>MetaTron (offline)</italic> achieved comparable performances. In both the analyzed task, all the tools had a standard deviation lower than 2.5. The 5<italic>th</italic> and 95<italic>th</italic> percentiles indicate that in all the examined tools all the computed times are uniformly distributed around the median.</p>
      <p id="Par96">The online and offline instances of <italic>MetaTron</italic> achieved similar performances in both tasks. This can be attributed to the absence of any differences between the code running locally and the code of the online instance. Furthermore, the server hosting <italic>MetaTron</italic> was underutilized when we ran the automatic agents, leading to performances comparable to the docker-based instance. It is notable the case of the <italic>INCEpTION</italic>: it achieved the worst performances in the first task (MA + CL + RE), and the best in the second one (MA + CL). This aspect points out that the annotation of relationships is the most expensive in terms of time, while <italic>INCEpTION</italic> is the most efficient tool in mentions annotation and linking. <italic>MetaTron</italic> and <italic>TeamTat</italic> present similar behaviors: the average time taken in the second task is half the time taken in the first: this indicates that annotating the relationships takes the same amount of time required to annotate the mentions and link them to the concepts. Conversely, in <italic>LightTag</italic>, mentions annotation and linking require more than half of the time: this is partially related to the implementation of the web agent in the relationship part, and partially depends on how mentions selection and linking have been implemented in the tool.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Overview of the number of clicks required to perform the annotation of 15 documents in the two selected tasks: mentions annotation and concepts linking (MA + CL) and an entire annotation—mentions annotation, concepts linking, relationship annotation (MA + CL + RA)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">MA + CL</th><th align="left">MA + CL + RA</th></tr></thead><tbody><tr><td align="left">MetaTron</td><td align="left">485</td><td char="." align="char">1028</td></tr><tr><td align="left">TeamTat</td><td align="left">500</td><td char="." align="char">1423</td></tr><tr><td align="left">LightTag</td><td align="left">327</td><td char="." align="char">611</td></tr><tr><td align="left">INCePTION</td><td align="left">391</td><td char="." align="char">746</td></tr></tbody></table></table-wrap></p>
      <p id="Par97">We analyzed the number of clicks performed to annotate the collection. The results include the count of clicks required to annotate every single document of the collection and the clicks necessary to change the document. The results are reported in Table <xref rid="Tab3" ref-type="table">3</xref>. We see that <italic>MetaTron</italic> and <italic>TeamTat</italic> are the tools that require the highest number of clicks to annotate the collection of 15 documents. This aspect is motivated by how the tools implement relationship annotation. In both the aforementioned tools, to annotate a new relationship it is required to set the two mentions and select the predicate and the associated ontological concept: all these actions makes the total number of clicks increase. <italic>LightTag</italic> instead, is the most efficient, however, it does not support entity linking, and this aspect motivates the lower number of clicks with respect to the other tools, since only the concept type—i.e., gene or disease, is required. The annotation of relationships in <italic>LightTag</italic> is the fastest compared to the other tools; the first reason is that <italic>LightTag</italic> requires the user to provide a schema configuration for relationship annotation and this allows the user to save clicks and time; in addition, in <italic>LightTag</italic> each mention/concept composing the relationship is selected by dragging and dropping it into a box hosting the relationship components, and this allows the user to save clicks—a drag and drop action is performed in a single click. <italic>INCEpTION</italic> is most efficient after <italic>LightTag</italic>, and requires a half of the clicks compared to <italic>TeamTat</italic>.</p>
    </sec>
    <sec id="Sec15">
      <title>Discussion</title>
      <p id="Par98">The qualitative and quantitative analyses conducted to assess the performances of <italic>MetaTron</italic>, <italic>MedTAG</italic>, <italic>TeamTat</italic>, <italic>LightTag</italic>, <italic>brat</italic>, and <italic>INCEpTION</italic> allowed us to draw some conclusion.</p>
      <p id="Par99">The results deriving from the analyses showed that <italic>MetaTron</italic> emerges as a competitive annotation tool in the biomedical domain, as it is the only one that fulfills all the analyzed features and achieves the best results in terms of time spent in the annotation of a set of documents. <italic>MetaTron</italic> provides an environment where one or more annotators can collaborate in annotating documents in five different ways, both at the document level and mention level and can leverage automatic annotations to expedite the annotation process. Additionally, <italic>MetaTron</italic> is open-source, free of charge, and supports a wide range of input and output formats. The tool is released as an online and offline instance, making <italic>MetaTron</italic> a versatile tool that can be adapted to different needs and use cases. The online instance of <italic>MetaTron</italic> is valuable to test its features, take advantage of <italic>AutoTron</italic>’s automatic annotations, and collaboratively annotate PubMed, Semantic Scholar, and OpenAIRE abstracts; the offline release guarantees data privacy and allows users to locally deploy <italic>MetaTron</italic> and share the tool with a controlled number of users. In addition, the offline tool is useful when dealing with large volumes of data that would require a significant amount of time for uploading.</p>
    </sec>
    <sec id="Sec16">
      <title>User study</title>
      <p id="Par100">The user study consisted of sentence-level tasks for Gene-Cancer Associations (GCA) and document-level tasks for Gene-Disease Associations (GDA). The GCA task required annotating relationships where the subject and object, representing gene and cancer mentions, respectively, are involved. The predicate corresponds to one of the following ontological concepts: (i) biomarker, indicating whether the gene associated with the disease is altered in conjunction with the disease; (ii) tumor suppressor, indicating whether the gene plays a role in preventing the disease; and (iii) oncogene, indicating whether the gene promotes the progression of the disease.</p>
      <p id="Par101">The GDA task encompassed annotating assertions where the subject, predicate, and object are ontological concepts unrelated to specific textual mentions. The subject and object represent gene and disease, respectively. At the same time, the predicate is categorized into one of the following concepts: (i) biomarker, (ii) genomic alteration, indicating a connection between a genomic alteration and the gene associated with the disease phenotype, and (iii) therapeutic, signifying the gene’s therapeutic role in ameliorating the disease.</p>
      <p id="Par102">We chose ten pertinent PubMed abstracts for each task, which three experts in the biomedical domain annotated. To streamline and expedite the annotation process for the annotators, we furnished them with automatic annotations generated by running <italic>AutoTron</italic> on each document.</p>
      <p id="Par103">The annotators performed two annotation rounds for each task. In round 1, the annotators had to annotate each document relying exclusively on the auxiliary information provided by the automatic annotations. The users were allowed to add new mentions, link concepts to them, and add new relationships. However, they could not rely on collaborative features to annotate the documents—i.e., the annotators could not check other members’ annotations or documents’ statistics. In round 2, instead, the annotators were asked to rely on the collaborative features of <italic>MetaTron</italic> to update their annotations: as a consequence, they had access to other members’ annotations and the annotation obtained via majority voting. At the end of the annotation rounds, we provided all the annotators with a questionnaire with 15 questions about their annotation experience.</p>
      <p id="Par104">In the following sections, we analyze how the annotations change after each round and how the collaborative features impact the agreement among annotators, we summarize the answers to the questionnaire, and finally, we study the quality of the annotations of <italic>AutoTron</italic> as a means to speed up and facilitate annotators’ work.</p>
      <p id="Par105"><italic>Quantitative results</italic> In Table <xref rid="Tab4" ref-type="table">4</xref>, we report the Fleiss’ kappa agreement among the annotators for each task after each round. Specifically, we provide the agreement computed on concepts and relationship annotations for the GCA task. Instead, we provide the agreement computed on assertions annotation for the GDA task. Our goal is to investigate the extent to which the presence of annotations from other annotators impacts the work of the annotators. The highest agreement has been achieved by concept annotations in all the rounds. At the end of round 1, the agreement obtained in relationships (GCA) and assertions (GDA) annotation is negative, indicating that there is no agreement among the annotators. At the end of round 2, the agreement on concept annotations did not change from round 1, while the agreement on relationships increased from <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-$$\end{document}</tex-math><mml:math id="M4"><mml:mo>-</mml:mo></mml:math><inline-graphic xlink:href="12859_2024_5730_Article_IEq2.gif"/></alternatives></inline-formula>0.2179 to <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-$$\end{document}</tex-math><mml:math id="M6"><mml:mo>-</mml:mo></mml:math><inline-graphic xlink:href="12859_2024_5730_Article_IEq3.gif"/></alternatives></inline-formula>0.0872 and on assertions from <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-$$\end{document}</tex-math><mml:math id="M8"><mml:mo>-</mml:mo></mml:math><inline-graphic xlink:href="12859_2024_5730_Article_IEq4.gif"/></alternatives></inline-formula>0.0364 to 0.2490. These results confirmed that the collaborative features provided by <italic>MetaTron</italic> play a key role in improving the results of the annotation process.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Overview of Fleiss’s kappa agreement at the end of each round. The agreement has been computed for concepts, relationships, and assertion annotations for each task’s entire set of documents</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left" colspan="2">GCA</th><th align="left">GDA</th></tr><tr><th align="left"/><th align="left">Concepts annotation</th><th align="left">Relationships annotation</th><th align="left">Assertions annotation</th></tr></thead><tbody><tr><td align="left">Round 1</td><td align="left">0.3312</td><td align="left">− 0.2179</td><td char="." align="char">− 0.0364</td></tr><tr><td align="left">Round 2</td><td align="left">0.3312</td><td align="left">− 0.0872</td><td char="." align="char">0.2490</td></tr></tbody></table></table-wrap></p>
      <p id="Par106"><italic>Qualitative results</italic> At the end of round 2, each annotator compiled a questionnaire concerning their experience using <italic>MetaTron</italic>. The questionnaire consisted of 15 questions about annotation experience, GCA and GDA tasks, and collaborative experience, with responses ranging from 1 to 5.</p>
      <p id="Par107">About the annotation experience, the annotators identified the annotation of mentions, concepts, relationships, and assertions as a straightforward process (all the questions about the complexity of annotations received scores equal to 1 and 2). Only one annotator needed to contact the developers to clarify how to annotate. All the annotators agreed that the automatic annotations generated via <italic>AutoTron</italic> are a useful starting point and contributed to speeding up and facilitating the annotation process. All the annotators considered <italic>MetaTron</italic> had a positive annotation experience and will use <italic>MetaTron</italic> in the future.</p>
      <p id="Par108">Considering task complexity, all the annotators found the GCA task more complex than the GDA one. Two over three annotators assigned a score equal to 3 in the complexity of the GCA task, while only one assigned 4. In the GDA task, instead, the annotators assigned 2, 3, and 4.</p>
      <p id="Par109">One relevant aspect is the annotators’ collaboration. All the annotators found <italic>MetaTron</italic> effectively supports collaboration among annotators. Specifically, all the annotators agreed that the possibility of copying one or more annotations from another annotator is an important feature that speeds up the annotation process. One annotator found useful the annotation generated via majority voting (score equal to 4), while the other two annotators assigned a score equal to 3 hence this annotation did not play a key role in their annotations. Finally, two out of three annotators admitted that those of the other annotators did not significantly influence their annotations. The remaining annotator, however, made numerous changes to the performed annotations in round 2. Two out of three annotators found it necessary to discuss their annotations and determine which relationship’s predicate to apply. This highlights the difficulty of the proposed tasks.</p>
      <p id="Par110">The annotators were required to point out the most useful features of <italic>MetaTron</italic> according to their experience. In Fig. <xref rid="Fig22" ref-type="fig">22</xref> we provide the results of this analysis. According to our results, three features have not been selected, specifically ontology support, statistics and collection’s agreement availability. These three features had minimal impact on annotators’ work as they do not offer direct support in the annotations; instead, they prove their key role in providing insights into overall agreements. One annotator considers five features relevant features, while six features by two. The possibility to copy the annotations of another annotator has been considered the most valuable feature, as all the annotators agreed on its importance.<fig id="Fig22"><label>Fig. 22</label><caption><p><italic>MetaTron</italic> features qualitative analysis. The overview concerns the features the annotators considered important to perform the two annotation tasks. The most significant features concern the collaboration among multiple annotators, while the least used concern the ontology support and the availability statistics</p></caption><graphic xlink:href="12859_2024_5730_Fig22_HTML" id="MO22"/></fig></p>
      <p id="Par111"><italic>AutoTron results</italic> In Table <xref rid="Tab5" ref-type="table">5</xref>, we report the total number of annotations at the end of each round. The first row, <italic>AutoTron</italic> refers to the total number of automatic annotations generated via <italic>AutoTron</italic> each annotator has been provided with at the beginning of round 1. Each annotator started with 228 concepts, and 57 relationships for the GCA task, and 17 assertions for the GDA task. At the end of round 1, the number of concepts increased to 235, the number of relationships increased to 115, and the number of assertions increased to 32. At the end of round 2, the number of concepts increased to 237, the number of relationships decreased to 111, and the number of assertions decreased to 24. The most significant change was identified in the GCA task for the doubled relationships at the end of round 1.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Overview of the total number of linked concepts, relationships, assertions annotations identified at each round. The first row, <italic>AutoTron</italic>, represents the starting point for each annotator, indicating the set of automatic annotations provided</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left" colspan="2">GCA</th><th align="left">GDA</th></tr><tr><th align="left"/><th align="left">Concepts annotation</th><th align="left">Relationships annotation</th><th align="left">Assertions annotation</th></tr></thead><tbody><tr><td align="left">AutoTron</td><td align="left">228</td><td char="." align="char">57</td><td align="left">17</td></tr><tr><td align="left">Round 1</td><td align="left">235</td><td char="." align="char">115</td><td align="left">32</td></tr><tr><td align="left">Round 2</td><td align="left">237</td><td char="." align="char">111</td><td align="left">24</td></tr></tbody></table></table-wrap></p>
      <p id="Par112">We considered the set of distinct annotations obtained at the end of round 2, and we counted how many concepts, relationships, and assertions have been updated, added to, and deleted from the automatically generated set of annotations provided for round 1. We considered an update when, in a relationship or assertion, the predicate assigned by the annotators is different from the one automatically assigned. We have a concept update, instead, when the linked concept changes.</p>
      <p id="Par113">In the GCA task, we detected that 9 concepts had been added, 2 were updated, 0 were removed, and 226 were confirmed. For what concerns relationships instead, 29 relationships have been added, 27 updated, 0 deleted, and 30 confirmed. In the GDA task, we detected that four assertions have been added, three updated, 0 deleted, and 14 confirmed.</p>
      <p id="Par114">The absence of deleted annotations confirms that <italic>AutoTron</italic> overall generates accurate annotations. Only in the GCA task in the relationships annotation the number of relationships added, updated, and confirmed remains the same.</p>
      <p id="Par115">Our results indicate that relying on <italic>AutoTron</italic> to generate a set of annotations used as a starting point is useful to facilitate and speed up the entire annotation process of the annotators. However, especially for relationships, the intervention of a human annotator is crucial for identifying and updating all existing relationships in a document. In this respect, according to the results obtained in the qualitative analyses, we see that the annotators found the GDA task easier than the GCA one: this results not only in a higher agreement but also in a lower number of updates and additions with respect to the automatic annotations.</p>
    </sec>
  </sec>
  <sec id="Sec17">
    <title>Conclusions</title>
    <p id="Par116">This paper presents <italic>MetaTron</italic>, a collaborative web-based annotation tool designed specifically for the biomedical domain. The tool facilitates the annotation of mentions, relationships, and document-level labels. It supports various document formats, including PDF, TXT, JSON, and CSV. Additionally, users can utilize the PubMed, Semantic Scholar, and OpenAIRE REST APIs to upload PMIDs or DOIs and annotate corresponding abstracts. Furthermore, <italic>MetaTron</italic> allows users to leverage their teammates’ annotations and incorporate annotations generated by <italic>AutoTron</italic> for fast annotation creation.</p>
    <p id="Par117">To ensure data privacy and limit tool usage to specific research groups, the <italic>MetaTron</italic> docker image enables local deployment on personal servers. Conversely, the online instance of <italic>MetaTron</italic> is designed for online annotation of PubMed, Semantic Scholar, and OpenAIRE abstracts, with the added advantage of utilizing AutoTron’s automatic prediction capabilities.</p>
    <p id="Par118">Noteworthy features of <italic>MetaTron</italic> include support for multiple ontologies, multilingual capabilities, login via ORCID ID, and the option to download annotations in JSON, CSV, and BioC/XML formats.</p>
    <p id="Par119">In our evaluation, we compared <italic>MetaTron</italic> to five other annotation tools, both general purpose and specifically tailored to the biomedical domain. We assessed them against 24 criteria classified into three categories: <italic>Data</italic>, <italic>Technical</italic>, and <italic>Functionalities</italic>. The qualitative analysis revealed that <italic>MetaTron</italic> fulfills almost all of the selected criteria. From a quantitative perspective, the online instance of <italic>MetaTron</italic> outperformed <italic>TeamTat</italic> and <italic>LightTag</italic> in terms of time elapsed and the number of clicks required. Additionally, the dockerized version of <italic>MetaTron</italic> achieved better results than <italic>INCEpTION</italic> in the task of mentions annotation, concept linking, and relationship annotation (MA + CL + RA). We conducted a user study which involved three human annotators and two tasks: relationships annotation and assertions annotation. The user study pointed out that <italic>MetaTron</italic> is an intuitive and easy to use tool. The collaborative features have been of great assistance, enabling annotators to enhance the accuracy of their annotations and improve agreement. The possibility of using <italic>AutoTron</italic> to automatically annotate documents, and to copy other members’ annotations has proven to be one of <italic>MetaTron</italic>’s most valued features, streamlining and facilitating the annotation process. In summary, <italic>MetaTron</italic> presents itself as a compelling annotation tool for the biomedical and bioinformatics community, providing collaborative and interactive features that can effectively streamline the annotation process. With a commitment to ongoing maintenance and a notable emphasis on relation annotation, often overlooked by other annotation tools, we think that <italic>MetaTron</italic> represents one of the highly recommended options for researchers in these domains.</p>
    <p id="Par120">As future work, we plan to integrate more use cases for built-in automatic predictions to allow <italic>MetaTron</italic> to widen to other domains of applications for automatic annotation of text. Moreover, two functionalities deserve to be implemented: the first one is to introduce a new annotation type, which is entity tagging, and let the user associate to a mention a concept type instead of the concept itself; then, we plan to implement discontinuous mentions allowing the user to be more accurate letting them decide which tokens compose the mention. About the support for the ontologies, we plan to introduce the possibility of (i) uploading the full ontologies directly from the related files and (ii) automatically suggesting the concept to associate to a mention relying on the textual and semantic similarity between the mention and the concepts. These features would support the user in speeding up the upload and the selection of the concepts. Finally, we plan to introduce in <italic>MetaTron</italic> some large corpora of documents that one or more members can annotate: this would provide the community with a tool already configured and ready-to-use and would promote the analyses of annotators’ behavior on well-known datasets.</p>
  </sec>
  <sec id="Sec18">
    <title>Availability and requirements</title>
    <p id="Par121">
      <list list-type="bullet">
        <list-item>
          <p id="Par122">Project name: MetaTron</p>
        </list-item>
        <list-item>
          <p id="Par123">Project home page: <ext-link ext-link-type="uri" xlink:href="https://github.com/GDAMining/metatron/">https://github.com/GDAMining/metatron/</ext-link></p>
        </list-item>
        <list-item>
          <p id="Par124">Online instance: <ext-link ext-link-type="uri" xlink:href="https://metatron.dei.unipd.it">https://metatron.dei.unipd.it</ext-link></p>
        </list-item>
        <list-item>
          <p id="Par125">Archived version: not applicable</p>
        </list-item>
        <list-item>
          <p id="Par126">Operating system(s): Platform independent</p>
        </list-item>
        <list-item>
          <p id="Par127">Programming language: Python, JavaScript, HTML, CSS</p>
        </list-item>
        <list-item>
          <p id="Par128">Other requirements: Docker and docker-compose (for the dockerized version) for the offline version</p>
        </list-item>
        <list-item>
          <p id="Par129">License: MIT License</p>
        </list-item>
        <list-item>
          <p id="Par130">Any restrictions to use by non-academics: No</p>
        </list-item>
      </list>
    </p>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p id="Par134"><ext-link ext-link-type="uri" xlink:href="https://www.djangoproject.com">https://www.djangoproject.com</ext-link>.</p>
    </fn>
    <fn id="Fn2">
      <label>2</label>
      <p id="Par135">By accessing with the username <italic>demo</italic> and password <italic>demo</italic>, it is possible to annotate a test collection.</p>
    </fn>
    <fn id="Fn3">
      <label>3</label>
      <p id="Par136">A relationship that solely consists of ontological concepts is treated as a document-level assertion.</p>
    </fn>
    <fn id="Fn4">
      <label>4</label>
      <p id="Par137"><ext-link ext-link-type="uri" xlink:href="https://www.selenium.dev">https://www.selenium.dev</ext-link>.</p>
    </fn>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank Fabio Giachelle, Niccoló Marini, and Laura Menotti for contributing to useful discussions and tool improvements. We appreciate their time, effort, and valuable input, significantly enriching our research. We also thank the anonymous reviewers for their insightful suggestions, which improved the paper and the described tool.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>O.I. was the designer and developer of MetaTron, wrote the code, designed the user study, wrote the main parts of the manuscript, and led the work. S.M. was the designer and developer of AutoTron and the relation extraction methods, contributed to the design of the user study, suggested some functionalities, wrote the parts describing AutoTron and revised the whole paper. G.S. coordinated the work, contributed to the design of MetaTron, suggested some functionalities, and contributed to the writing and revision of the paper.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Open access funding provided by Università degli Studi di Padova. This work is partially supported by the HEREDITARY Project, as part of the European Union’s Horizon Europe research and innovation programme under Grant Agreement No GA 101137074. The work of O.I. was partially funded by the EC H2020 project OpenAIRE-Nexus (Grant Agreement No. 101017452).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The code used in this study is publicly available on GitHub <ext-link ext-link-type="uri" xlink:href="https://github.com/GDAMining/metatron/">https://github.com/GDAMining/metatron/</ext-link>. All the underlying libraries used in this work are open-source. The complete list of libraries and their versions are reported in the GitHub repository.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1">
      <title>Ethics approval and consent to participate</title>
      <p id="Par131">Not applicable.</p>
    </notes>
    <notes id="FPar2">
      <title>Consent for publication</title>
      <p id="Par132">Not applicable.</p>
    </notes>
    <notes id="FPar3" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par133">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Costa</surname>
            <given-names>FF</given-names>
          </name>
        </person-group>
        <article-title>Big data in biomedicine</article-title>
        <source>Drug Discov Today</source>
        <year>2014</year>
        <volume>19</volume>
        <issue>4</issue>
        <fpage>433</fpage>
        <lpage>440</lpage>
        <pub-id pub-id-type="doi">10.1016/j.drudis.2013.10.012</pub-id>
        <?supplied-pmid 24183925?>
        <pub-id pub-id-type="pmid">24183925</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Murdoch</surname>
            <given-names>TB</given-names>
          </name>
          <name>
            <surname>Detsky</surname>
            <given-names>AS</given-names>
          </name>
        </person-group>
        <article-title>The inevitable application of big data to health care</article-title>
        <source>JAMA</source>
        <year>2013</year>
        <volume>309</volume>
        <issue>13</issue>
        <fpage>1351</fpage>
        <lpage>1352</lpage>
        <pub-id pub-id-type="doi">10.1001/jama.2013.393</pub-id>
        <?supplied-pmid 23549579?>
        <pub-id pub-id-type="pmid">23549579</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>CC</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Community challenges in biomedical text mining over 10 years: success, failure and the future</article-title>
        <source>Brief Bioinform</source>
        <year>2016</year>
        <volume>17</volume>
        <issue>1</issue>
        <fpage>132</fpage>
        <lpage>144</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbv024</pub-id>
        <?supplied-pmid 25935162?>
        <pub-id pub-id-type="pmid">25935162</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jovanović</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bagheri</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Semantic annotation in biomedicine: the current landscape</article-title>
        <source>J Biomed Semant</source>
        <year>2017</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>18</lpage>
        <pub-id pub-id-type="doi">10.1186/s13326-017-0153-x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Patumcharoenpol</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Meechai</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Vongsangnak</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Biomedical text mining and its applications in cancer research</article-title>
        <source>J Biomed Inform</source>
        <year>2013</year>
        <volume>46</volume>
        <issue>2</issue>
        <fpage>200</fpage>
        <lpage>211</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2012.10.007</pub-id>
        <?supplied-pmid 23159498?>
        <pub-id pub-id-type="pmid">23159498</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lindvall</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>C-Y</given-names>
          </name>
          <name>
            <surname>Moseley</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Agaronnik</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>El-Jawahri</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Paasche-Orlow</surname>
            <given-names>MK</given-names>
          </name>
          <name>
            <surname>Lakin</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Volandes</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tulsky</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Investigators</surname>
            <given-names>A-P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Natural language processing to identify advance care planning documentation in a multisite pragmatic clinical trial</article-title>
        <source>J Pain Symptom Manag</source>
        <year>2022</year>
        <volume>63</volume>
        <issue>1</issue>
        <fpage>29</fpage>
        <lpage>36</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jpainsymman.2021.06.025</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cohen</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Hersh</surname>
            <given-names>WR</given-names>
          </name>
        </person-group>
        <article-title>A survey of current work in biomedical text mining</article-title>
        <source>Brief Bioinform</source>
        <year>2005</year>
        <volume>6</volume>
        <issue>1</issue>
        <fpage>57</fpage>
        <lpage>71</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/6.1.57</pub-id>
        <?supplied-pmid 15826357?>
        <pub-id pub-id-type="pmid">15826357</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kersloot</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>van Putten</surname>
            <given-names>FJ</given-names>
          </name>
          <name>
            <surname>Abu-Hanna</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cornet</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Arts</surname>
            <given-names>DL</given-names>
          </name>
        </person-group>
        <article-title>Natural language processing algorithms for mapping clinical text fragments onto ontology concepts: a systematic review and recommendations for future studies</article-title>
        <source>J Biomed Semant</source>
        <year>2020</year>
        <volume>11</volume>
        <fpage>1</fpage>
        <lpage>21</lpage>
        <pub-id pub-id-type="doi">10.1186/s13326-020-00231-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lacson</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Pitzer</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Hinske</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Galante</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Ohno-Machado</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Evaluation of a large-scale biomedical data annotation initiative</article-title>
        <source>BMC Bioinform</source>
        <year>2009</year>
        <volume>10</volume>
        <fpage>1</fpage>
        <lpage>6</lpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-10-S9-S10</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Neves</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ševa</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>An extensive review of tools for manual annotation of documents</article-title>
        <source>Brief Bioinform</source>
        <year>2021</year>
        <volume>22</volume>
        <issue>1</issue>
        <fpage>146</fpage>
        <lpage>163</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbz130</pub-id>
        <?supplied-pmid 31838514?>
        <pub-id pub-id-type="pmid">31838514</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koleck</surname>
            <given-names>TA</given-names>
          </name>
          <name>
            <surname>Dreisbach</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Bourne</surname>
            <given-names>PE</given-names>
          </name>
          <name>
            <surname>Bakken</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Natural language processing of symptoms documented in free-text narratives of electronic health records: a systematic review</article-title>
        <source>J Am Med Inform Assoc</source>
        <year>2019</year>
        <volume>26</volume>
        <issue>4</issue>
        <fpage>364</fpage>
        <lpage>379</lpage>
        <pub-id pub-id-type="doi">10.1093/jamia/ocy173</pub-id>
        <?supplied-pmid 30726935?>
        <pub-id pub-id-type="pmid">30726935</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yim</surname>
            <given-names>W-W</given-names>
          </name>
          <name>
            <surname>Yetisgen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Harris</surname>
            <given-names>WP</given-names>
          </name>
          <name>
            <surname>Kwan</surname>
            <given-names>SW</given-names>
          </name>
        </person-group>
        <article-title>Natural language processing in oncology: a review</article-title>
        <source>JAMA Oncol</source>
        <year>2016</year>
        <volume>2</volume>
        <issue>6</issue>
        <fpage>797</fpage>
        <lpage>804</lpage>
        <pub-id pub-id-type="doi">10.1001/jamaoncol.2016.0213</pub-id>
        <?supplied-pmid 27124593?>
        <pub-id pub-id-type="pmid">27124593</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>PourNejatian</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Shin</surname>
            <given-names>HC</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>KE</given-names>
          </name>
          <name>
            <surname>Parisien</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Compas</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Costa</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Flores</surname>
            <given-names>MG</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A large language model for electronic health records</article-title>
        <source>NPJ Digit Med</source>
        <year>2022</year>
        <volume>5</volume>
        <issue>1</issue>
        <fpage>194</fpage>
        <pub-id pub-id-type="doi">10.1038/s41746-022-00742-2</pub-id>
        <?supplied-pmid 36572766?>
        <pub-id pub-id-type="pmid">36572766</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Meij E, Balog K, Odijk D. Entity linking and retrieval. In: Proceedings of the 36th international ACM SIGIR conference on research and development in information retrieval; 2013. p. 1127.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Recent advances in biomedical literature mining</article-title>
        <source>Brief Bioinform</source>
        <year>2021</year>
        <volume>22</volume>
        <issue>3</issue>
        <fpage>057</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbaa057</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hong</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wan</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A novel machine learning framework for automated biomedical relation extraction from large-scale literature repositories</article-title>
        <source>Nat Mach Intell</source>
        <year>2020</year>
        <volume>2</volume>
        <issue>6</issue>
        <fpage>347</fpage>
        <lpage>355</lpage>
        <pub-id pub-id-type="doi">10.1038/s42256-020-0189-y</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Biomedical relation extraction via knowledge-enhanced reading comprehension</article-title>
        <source>BMC Bioinform</source>
        <year>2022</year>
        <volume>23</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>19</lpage>
        <pub-id pub-id-type="doi">10.1186/s12859-021-04534-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xing</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>BioRel: towards large-scale biomedical relation extraction</article-title>
        <source>BMC Bioinform</source>
        <year>2020</year>
        <volume>21</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1186/s12859-020-03889-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Zhou P, Shi W, Tian J, Qi Z, Li B, Hao H, Xu B. Attention-based bidirectional long short-term memory networks for relation classification. In: Proceedings of the 54th annual meeting of the Association for Computational Linguistics (volume 2: short papers); 2016. p. 207–12.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Zhang D, Wang D. Relation classification via recurrent neural network. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1508.01006">arXiv:1508.01006</ext-link> (2015).</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Zeng D, Liu K, Chen Y, Zhao J. Distant supervision for relation extraction via piecewise convolutional neural networks. In: Proceedings of the 2015 conference on empirical methods in natural language processing, EMNLP 2015, Lisbon, Portugal, September 17–21, 2015. p. 1753–62.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Exploring semi-supervised variational autoencoders for biomedical relation extraction</article-title>
        <source>Methods</source>
        <year>2019</year>
        <volume>166</volume>
        <fpage>112</fpage>
        <lpage>119</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ymeth.2019.02.021</pub-id>
        <?supplied-pmid 30822516?>
        <pub-id pub-id-type="pmid">30822516</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Krauthammer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nenadic</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Term identification in the biomedical literature</article-title>
        <source>J Biomed Inform</source>
        <year>2004</year>
        <volume>37</volume>
        <issue>6</issue>
        <fpage>512</fpage>
        <lpage>526</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2004.08.004</pub-id>
        <?supplied-pmid 15542023?>
        <pub-id pub-id-type="pmid">15542023</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Matthews</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Distinguishing the species of biomedical named entities for term identification</article-title>
        <source>BMC Bioinform</source>
        <year>2008</year>
        <volume>9</volume>
        <issue>11</issue>
        <fpage>1</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Elhadad</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Unsupervised biomedical named entity recognition: experiments with clinical and biological texts</article-title>
        <source>J Biomed Inform</source>
        <year>2013</year>
        <volume>46</volume>
        <issue>6</issue>
        <fpage>1088</fpage>
        <lpage>1098</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2013.08.004</pub-id>
        <?supplied-pmid 23954592?>
        <pub-id pub-id-type="pmid">23954592</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Gorrell G, Song X, Roberts A. Bio-yodie: a named entity linking system for biomedical text. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1811.04860">arXiv:1811.04860</ext-link> (2018)</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vashishth</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Newman-Griffis</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Joshi</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Dutt</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Rosé</surname>
            <given-names>CP</given-names>
          </name>
        </person-group>
        <article-title>Improving broad-coverage medical entity linking with semantic type prediction and large-scale datasets</article-title>
        <source>J Biomed Inform</source>
        <year>2021</year>
        <volume>121</volume>
        <fpage>103880</fpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2021.103880</pub-id>
        <?supplied-pmid 34390853?>
        <pub-id pub-id-type="pmid">34390853</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">D’Souza J, Ng V. Sieve-based entity linking for the biomedical domain. In: Proceedings of the 53rd annual meeting of the Association for Computational Linguistics and the 7th international joint conference on natural language processing (volume 2: short papers); 2015. p. 297–302.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Jiang X, Ringwald M, Blake J, Shatkay H. Effective biomedical document classification for identifying publications relevant to the mouse Gene Expression Database (GXD). Database 2017;2017.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pérez-Pérez</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ferreira</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Lourenço</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Igrejas</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Fdez-Riverola</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Boosting biomedical document classification through the use of domain entity recognizers and semantic ontologies for document representation: the case of gluten bibliome</article-title>
        <source>Neurocomputing</source>
        <year>2022</year>
        <volume>484</volume>
        <fpage>223</fpage>
        <lpage>237</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neucom.2021.10.100</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Trabucco</surname>
            <given-names>JT</given-names>
          </name>
          <name>
            <surname>Raciti</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ringwald</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Marai</surname>
            <given-names>GE</given-names>
          </name>
          <name>
            <surname>Arighi</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Shatkay</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Utilizing image and caption information for biomedical document classification</article-title>
        <source>Bioinformatics</source>
        <year>2021</year>
        <volume>37</volume>
        <issue>Supplement-1</issue>
        <fpage>468</fpage>
        <lpage>476</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btab331</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Burns GA, Li X, Peng N. Building deep learning models for evidence classification from the open access biomedical literature. Database 2019; 2019.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dramé</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Mougin</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Diallo</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Large scale biomedical texts classification: a kNN and an ESA-based approaches</article-title>
        <source>Journal of biomedical semantics</source>
        <year>2016</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1186/s13326-016-0073-1</pub-id>
        <pub-id pub-id-type="pmid">26759709</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Simon</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Davidsen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Hansen</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Seymour</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Barnkob</surname>
            <given-names>MB</given-names>
          </name>
          <name>
            <surname>Olsen</surname>
            <given-names>LR</given-names>
          </name>
        </person-group>
        <article-title>BioReader: a text mining tool for performing classification of biomedical literature</article-title>
        <source>BMC Bioinform</source>
        <year>2019</year>
        <volume>19</volume>
        <fpage>165</fpage>
        <lpage>170</lpage>
        <pub-id pub-id-type="doi">10.1186/s12859-019-2607-x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Jiang X, M, Blake JA, Arighi C, Zhang G, Shatkay H. An effective biomedical document classification scheme in support of biocuration: addressing class imbalance. Database 2019; 2019.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bodenreider</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>The unified medical language system (UMLS): integrating biomedical terminology</article-title>
        <source>Nucleic Acids Res</source>
        <year>2004</year>
        <volume>32</volume>
        <issue>suppl–1</issue>
        <fpage>267</fpage>
        <lpage>270</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkh061</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>J-D</given-names>
          </name>
          <name>
            <surname>Ohta</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tsujii</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Corpus annotation for mining biomedical events from literature</article-title>
        <source>BMC Bioinform</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>1</fpage>
        <lpage>25</lpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-9-10</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bada</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Eckert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Evans</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Garcia</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Shipley</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Sitnikov</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Baumgartner</surname>
            <given-names>WA</given-names>
          </name>
          <name>
            <surname>Cohen</surname>
            <given-names>KB</given-names>
          </name>
          <name>
            <surname>Verspoor</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Blake</surname>
            <given-names>JA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Concept annotation in the CRAFT corpus</article-title>
        <source>BMC Bioinform</source>
        <year>2012</year>
        <volume>13</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>20</lpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-13-161</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Doğan</surname>
            <given-names>RI</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>NCBI disease corpus: a resource for disease name recognition and concept normalization</article-title>
        <source>J Biomed Inform</source>
        <year>2014</year>
        <volume>47</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2013.12.006</pub-id>
        <?supplied-pmid 24393765?>
        <pub-id pub-id-type="pmid">24393765</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Krallinger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rabal</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Leitner</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Vazquez</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Salgado</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lowe</surname>
            <given-names>DM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The CHEMDNER corpus of chemicals and drugs and its annotation principles</article-title>
        <source>J Cheminform</source>
        <year>2015</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="doi">10.1186/1758-2946-7-S1-S1</pub-id>
        <pub-id pub-id-type="pmid">25705261</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Ohta</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tateisi</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Tsujii</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>GENIA corpus—a semantically annotated corpus for bio-textmining</article-title>
        <source>Bioinformatics</source>
        <year>2003</year>
        <volume>19</volume>
        <issue>suppl-1</issue>
        <fpage>180</fpage>
        <lpage>182</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btg1023</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Li J, Sun Y, Johnson RJ, Sciaky D, Wei CH, Leaman R, Davis AP, Mattingly CJ, Wiegers TC, Lu Z. Biocreative V CDR task corpus: a resource for chemical disease relation extraction. Database 2016; 2016.</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Mohan S, Li D. Medmentions: a large biomedical corpus annotated with UMLS concepts. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1902.09476">arXiv:1902.09476</ext-link> (2019)</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Uzuner</surname>
            <given-names>Ö</given-names>
          </name>
          <name>
            <surname>South</surname>
            <given-names>BR</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>DuVall</surname>
            <given-names>SL</given-names>
          </name>
        </person-group>
        <article-title>2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text</article-title>
        <source>J Am Med Inform Assoc</source>
        <year>2011</year>
        <volume>18</volume>
        <issue>5</issue>
        <fpage>552</fpage>
        <lpage>556</lpage>
        <pub-id pub-id-type="doi">10.1136/amiajnl-2011-000203</pub-id>
        <?supplied-pmid 21685143?>
        <pub-id pub-id-type="pmid">21685143</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Johnson</surname>
            <given-names>AE</given-names>
          </name>
          <name>
            <surname>Pollard</surname>
            <given-names>TJ</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Lehman</surname>
            <given-names>LH</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ghassemi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Moody</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Szolovits</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Celi</surname>
            <given-names>LA</given-names>
          </name>
          <name>
            <surname>Mark</surname>
            <given-names>RG</given-names>
          </name>
        </person-group>
        <article-title>MIMIC-III, a freely accessible critical care database</article-title>
        <source>Sci Data</source>
        <year>2016</year>
        <volume>3</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1038/sdata.2016.35</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van Mulligen</surname>
            <given-names>EM</given-names>
          </name>
          <name>
            <surname>Fourrier-Reglat</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gurwitz</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Molokhia</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nieto</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Trifiro</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Kors</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Furlong</surname>
            <given-names>LI</given-names>
          </name>
        </person-group>
        <article-title>The EU-ADR corpus: annotated drugs, diseases, targets, and their relationships</article-title>
        <source>J Biomed Inform</source>
        <year>2012</year>
        <volume>45</volume>
        <issue>5</issue>
        <fpage>879</fpage>
        <lpage>884</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2012.04.004</pub-id>
        <?supplied-pmid 22554700?>
        <pub-id pub-id-type="pmid">22554700</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>HJ</given-names>
          </name>
          <name>
            <surname>Shim</surname>
            <given-names>SH</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>CoMAGC: a corpus with multi-faceted annotations of gene-cancer relations</article-title>
        <source>BMC Bioinform</source>
        <year>2013</year>
        <volume>14</volume>
        <fpage>323</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-14-323</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Giachelle</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Irrera</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Silvello</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>MedTAG: a portable and customizable annotation tool for biomedical documents</article-title>
        <source>BMC Med Inform Decis Mak</source>
        <year>2021</year>
        <volume>21</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>19</lpage>
        <pub-id pub-id-type="doi">10.1186/s12911-021-01706-4</pub-id>
        <pub-id pub-id-type="pmid">33388057</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Islamaj</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Kwon</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>TeamTat: a collaborative text annotation tool</article-title>
        <source>Nucleic Acids Res</source>
        <year>2020</year>
        <volume>48</volume>
        <issue>W1</issue>
        <fpage>5</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkaa333</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Cejuela JM, McQuilton P, Ponting L, Marygold SJ, Stefancsik R, Millburn GH, Rost B, Consortium F, et al. tagtog: interactive and text-mining-assisted annotation of gene mentions in PLOS full-text articles. Database 2014; 2014.</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Salgado</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Krallinger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Depaule</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Drula</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Tendulkar</surname>
            <given-names>AV</given-names>
          </name>
          <name>
            <surname>Leitner</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Valencia</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Marcelle</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>MyMiner: a web application for computer-assisted biocuration and text annotation</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>28</volume>
        <issue>17</issue>
        <fpage>2285</fpage>
        <lpage>2287</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts435</pub-id>
        <?supplied-pmid 22789588?>
        <pub-id pub-id-type="pmid">22789588</pub-id>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other">Kwon D, Kim S, Shin S, Wilbur WJ. BioQRator: a web-based interactive biomedical literature curating system. In: Proceedings of the fourth biocreative challenge evaluation workshop, vol 1; 2013. p. 241–46.</mixed-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kwon</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>ezTag: tagging biomedical concepts via interactive learning</article-title>
        <source>Nucleic Acids Res</source>
        <year>2018</year>
        <volume>46</volume>
        <issue>W1</issue>
        <fpage>523</fpage>
        <lpage>529</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gky428</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <mixed-citation publication-type="other">Stenetorp P, Pyysalo S, Topić G, Ohta T, Ananiadou S, Tsujii J. BRAT: a web-based tool for NLP-assisted text annotation. In: Proceedings of the demonstrations at the 13th conference of the European chapter of the Association for Computational Linguistics; 2012. p. 102–7.</mixed-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <mixed-citation publication-type="other">Giachelle F, Irrera O, Silvello G. DocTAG: a customizable annotation tool for ground truth creation. In: Advances in information retrieval: 44th European conference on IR Research, ECIR 2022, Stavanger, Norway, April 10–14, 2022, proceedings, part II. Springer; 2022. p. 288–93.</mixed-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <mixed-citation publication-type="other">Klie J-C, Bugert M, Boullosa B, de Castilho RE, Gurevych I. The inception platform: machine-assisted and knowledge-oriented interactive annotation. In: Proceedings of the 27th international conference on computational linguistics: system demonstrations; 2018. p. 5–9.</mixed-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <mixed-citation publication-type="other">Perry T. Lighttag: text annotation platform. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2109.02320">arXiv:2109.02320</ext-link> (2021)</mixed-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <mixed-citation publication-type="other">Muhie SY, Gurevych I, de Castilho RE, Biemann C. Webanno: a flexible, web-based and visually supported system for distributed annotations. In: Proceedings of the 51st annual meeting of the Association for Computational Linguistics: system demonstrations; 2013. p. 1–6.</mixed-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <mixed-citation publication-type="other">Jazayeri M. Some trends in web application development. In: Future of software engineering (FOSE’07). IEEE; 2007. p. 199–213.</mixed-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dobbie</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Strafford</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Pickrell</surname>
            <given-names>WO</given-names>
          </name>
          <name>
            <surname>Fonferko-Shadrach</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Akbari</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Thompson</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lacey</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Markup: a web-based annotation tool powered by active learning</article-title>
        <source>Front Digit Health</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.3389/fdgth.2021.598916</pub-id>
        <?supplied-pmid 34713086?>
        <pub-id pub-id-type="pmid">34713086</pub-id>
      </element-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Wen</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Moon</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Towards a better understanding of annotation tools for medical imaging: a survey</article-title>
        <source>Multimed Tools Appl</source>
        <year>2022</year>
        <volume>81</volume>
        <issue>18</issue>
        <fpage>25877</fpage>
        <lpage>25911</lpage>
        <pub-id pub-id-type="doi">10.1007/s11042-022-12100-1</pub-id>
        <pub-id pub-id-type="pmid">35350630</pub-id>
      </element-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Piad-Morffis</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gutiérrez</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Almeida-Cruz</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Munoz</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>A computational ecosystem to support ehealth knowledge discovery technologies in Spanish</article-title>
        <source>J Biomed Inform</source>
        <year>2020</year>
        <volume>109</volume>
        <fpage>103517</fpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2020.103517</pub-id>
        <?supplied-pmid 32712157?>
        <pub-id pub-id-type="pmid">32712157</pub-id>
      </element-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reinanda</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Meij</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>de Rijke</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Knowledge graphs: an information retrieval perspective</article-title>
        <source>Found Trends® Inf Retr</source>
        <year>2020</year>
        <volume>14</volume>
        <issue>4</issue>
        <fpage>289</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="doi">10.1561/1500000063</pub-id>
      </element-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <mixed-citation publication-type="other">Lopez P. GROBID: combining automatic bibliographic data recognition and term extraction for scholarship publications. In: Research and advanced technology for digital libraries: 13th European conference, ECDL 2009, Corfu, Greece, September 27–October 2, 2009. Proceedings 2009, vol 13. Springer. p. 473–4.</mixed-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>French</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>McInnes</surname>
            <given-names>BT</given-names>
          </name>
        </person-group>
        <article-title>An overview of biomedical entity linking throughout the years</article-title>
        <source>J Biomed Inform</source>
        <year>2023</year>
        <volume>137</volume>
        <fpage>104252</fpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2022.104252</pub-id>
        <?supplied-pmid 36464228?>
        <pub-id pub-id-type="pmid">36464228</pub-id>
      </element-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sevgili</surname>
            <given-names>Ö</given-names>
          </name>
          <name>
            <surname>Shelmanov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Arkhipov</surname>
            <given-names>MY</given-names>
          </name>
          <name>
            <surname>Panchenko</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Biemann</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Neural entity linking: a survey of models based on deep learning</article-title>
        <source>Semant Web</source>
        <year>2022</year>
        <volume>13</volume>
        <issue>3</issue>
        <fpage>527</fpage>
        <lpage>570</lpage>
        <pub-id pub-id-type="doi">10.3233/SW-222986</pub-id>
      </element-citation>
    </ref>
    <ref id="CR67">
      <label>67.</label>
      <mixed-citation publication-type="other">Aydar M, Bozal O, Özbay F. Neural relation extraction: a survey. CoRR <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2007.04247">arXiv: 2007.04247</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR68">
      <label>68.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Smirnova</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cudré-Mauroux</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Relation extraction using distant supervision: a survey</article-title>
        <source>ACM Comput Surv</source>
        <year>2019</year>
        <volume>51</volume>
        <issue>5</issue>
        <fpage>106</fpage>
        <lpage>110635</lpage>
        <pub-id pub-id-type="doi">10.1145/3241741</pub-id>
      </element-citation>
    </ref>
    <ref id="CR69">
      <label>69.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Kao</surname>
            <given-names>HY</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>PubTator: a web-based text mining tool for assisting biocuration</article-title>
        <source>Nucleic Acids Res</source>
        <year>2013</year>
        <volume>41</volume>
        <issue>Webserver-Issue</issue>
        <fpage>518</fpage>
        <lpage>522</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkt441</pub-id>
        <pub-id pub-id-type="pmid">23125361</pub-id>
      </element-citation>
    </ref>
    <ref id="CR70">
      <label>70.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Beyond accuracy: creating interoperable and scalable text-mining web services</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <issue>12</issue>
        <fpage>1907</fpage>
        <lpage>1910</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btv760</pub-id>
        <?supplied-pmid 26883486?>
        <pub-id pub-id-type="pmid">26883486</pub-id>
      </element-citation>
    </ref>
    <ref id="CR71">
      <label>71.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Allot</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>PubTator central: automated concept annotation for biomedical full text articles</article-title>
        <source>Nucleic Acids Res</source>
        <year>2019</year>
        <volume>47</volume>
        <issue>Webserver-Issue</issue>
        <fpage>587</fpage>
        <lpage>593</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkz389</pub-id>
      </element-citation>
    </ref>
    <ref id="CR72">
      <label>72.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maglott</surname>
            <given-names>DR</given-names>
          </name>
          <name>
            <surname>Ostell</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Pruitt</surname>
            <given-names>KD</given-names>
          </name>
          <name>
            <surname>Tatusova</surname>
            <given-names>TA</given-names>
          </name>
        </person-group>
        <article-title>Entrez gene: gene-centered information at NCBI</article-title>
        <source>Nucleic Acids Res</source>
        <year>2011</year>
        <volume>39</volume>
        <issue>Database-Issue</issue>
        <fpage>52</fpage>
        <lpage>57</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkq1237</pub-id>
      </element-citation>
    </ref>
    <ref id="CR73">
      <label>73.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lipscomb</surname>
            <given-names>CE</given-names>
          </name>
        </person-group>
        <article-title>Medical subject headings (MeSH)</article-title>
        <source>Bull Med Libr Assoc</source>
        <year>2000</year>
        <volume>88</volume>
        <issue>3</issue>
        <fpage>265</fpage>
        <?supplied-pmid 10928714?>
        <pub-id pub-id-type="pmid">10928714</pub-id>
      </element-citation>
    </ref>
    <ref id="CR74">
      <label>74.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Demner-Fushman</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Rogers</surname>
            <given-names>WJ</given-names>
          </name>
          <name>
            <surname>Aronson</surname>
            <given-names>AR</given-names>
          </name>
        </person-group>
        <article-title>MetaMap lite: an evaluation of a new Java implementation of MetaMap</article-title>
        <source>J Am Med Inform Assoc</source>
        <year>2017</year>
        <volume>24</volume>
        <issue>4</issue>
        <fpage>841</fpage>
        <lpage>844</lpage>
        <pub-id pub-id-type="doi">10.1093/jamia/ocw177</pub-id>
        <?supplied-pmid 28130331?>
        <pub-id pub-id-type="pmid">28130331</pub-id>
      </element-citation>
    </ref>
    <ref id="CR75">
      <label>75.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dugger</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Platt</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Goldstein</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Drug development in the era of precision medicine</article-title>
        <source>Nat Rev Drug Discov</source>
        <year>2018</year>
        <volume>17</volume>
        <fpage>183</fpage>
        <lpage>196</lpage>
        <pub-id pub-id-type="doi">10.1038/nrd.2017.226</pub-id>
        <?supplied-pmid 29217837?>
        <pub-id pub-id-type="pmid">29217837</pub-id>
      </element-citation>
    </ref>
    <ref id="CR76">
      <label>76.</label>
      <mixed-citation publication-type="other">Surdeanu M, Tibshirani J, Nallapati R, Manning CD. Multi-instance multi-label learning for relation extraction. In: Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning, EMNLP-CoNLL 2012, July 12–14, 2012, Jeju Island, Korea; 2012. p. 455–65.</mixed-citation>
    </ref>
    <ref id="CR77">
      <label>77.</label>
      <mixed-citation publication-type="other">Riedel S, Yao L, McCallum A. Modeling relations and their mentions without labeled text. In: Proceedings of machine learning and knowledge discovery in databases, European conference, ECML PKDD 2010, Barcelona, Spain, September 20–24, 2010. LNCS, vol 6323; 2010. p. 148–63.</mixed-citation>
    </ref>
    <ref id="CR78">
      <label>78.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marchesin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Silvello</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>TBGA: a large-scale gene-disease association dataset for biomedical relation extraction</article-title>
        <source>BMC Bioinform</source>
        <year>2022</year>
        <volume>23</volume>
        <issue>1</issue>
        <fpage>111</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-022-04646-6</pub-id>
      </element-citation>
    </ref>
    <ref id="CR79">
      <label>79.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Neary</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Identifying gene expression patterns associated with drug-specific survival in cancer patients</article-title>
        <source>Sci Rep</source>
        <year>2021</year>
        <volume>11</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1038/s41598-021-84211-y</pub-id>
        <pub-id pub-id-type="pmid">33414495</pub-id>
      </element-citation>
    </ref>
    <ref id="CR80">
      <label>80.</label>
      <mixed-citation publication-type="other">Liu F, Chen J, Jagannatha A, Yu H. Learning for biomedical information extraction: methodological review of recent advances. CoRR abs/1606.07993 (2016)</mixed-citation>
    </ref>
    <ref id="CR81">
      <label>81.</label>
      <mixed-citation publication-type="other">Krallinger M, Rabal O, Akhondi SA, Pérez MP, Santamaría J, Rodríguez GP, Tsatsaronis G, Intxaurrondo A, Lopez JA, Nandal UK, van Buel EM, Chandrasekhar A, Rodenburg M, Lægreid A, Doornenbal MA, Oyarzábal J, Lourenço A, Valencia A. Overview of the BioCreative VI chemical-protein interaction Track. In: Proceedings of the sixth biocreative challenge evaluation workshop; 2017.</mixed-citation>
    </ref>
    <ref id="CR82">
      <label>82.</label>
      <mixed-citation publication-type="other">Miranda A, Mehryary F, Luoma J, Pyysalo S, Valencia A, Krallinger M. Overview of DrugProt BioCreative VII track: quality evaluation and large scale text mining of drug-gene/protein relations. In: Proceedings of the seventh biocreative challenge evaluation workshop; 2021.</mixed-citation>
    </ref>
    <ref id="CR83">
      <label>83.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>HJ</given-names>
          </name>
          <name>
            <surname>Dang</surname>
            <given-names>TC</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>OncoSearch: cancer gene search engine with literature evidence</article-title>
        <source>Nucleic Acids Res</source>
        <year>2014</year>
        <volume>42</volume>
        <issue>Webserver-Issue</issue>
        <fpage>416</fpage>
        <lpage>421</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gku368</pub-id>
      </element-citation>
    </ref>
    <ref id="CR84">
      <label>84.</label>
      <mixed-citation publication-type="other">Beltagy I, Lo K, Cohan A. SciBERT: a pretrained language model for scientific text. In: Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3–7, 2019; 2019. p. 3613–18.</mixed-citation>
    </ref>
    <ref id="CR85">
      <label>85.</label>
      <mixed-citation publication-type="other">Devlin J, Chang MW, Lee K, Toutanova K. BERT: pre-training of deep bidirectional transformers for language understanding. In: Proceedings of the 2019 conference of the North American Chapter of the Association for Computational Linguistics: human language technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2–7, 2019, volume 1 (long and short papers); 2019. p. 4171–86.</mixed-citation>
    </ref>
    <ref id="CR86">
      <label>86.</label>
      <mixed-citation publication-type="other">Ammar W, Groeneveld D, Bhagavatula C, Beltagy I, Crawford M, Downey D, Dunkelberger J, Elgohary A, Feldman S, Ha V, Kinney R, Kohlmeier S, Lo K, Murray T, Ooi HH, Peters ME, Power J, Skjonsberg S, Wang LL, Wilhelm C, Yuan Z, van Zuylen M, Etzioni O. Construction of the literature graph in semantic scholar. In: Proceedings of the 2018 conference of the North American Chapter of the Association for Computational Linguistics: human language technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1–6, 2018, volume 3 (industry papers); 2018. p. 84–91.</mixed-citation>
    </ref>
    <ref id="CR87">
      <label>87.</label>
      <mixed-citation publication-type="other">Giachelle F, Marchesin S, Silvello G, Alonso O. Searching for reliable facts over a medical knowledge base. In: Proceedings of the 46th international ACM SIGIR conference on research and development in information retrieval, SIGIR 2023, Taipei, Taiwan, July 23–27, 2023; 2023. p. 23–7. 10.1145/3539618.3591822.</mixed-citation>
    </ref>
    <ref id="CR88">
      <label>88.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marchesin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Menotti</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Giachelle</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Silvello</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Alonso</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>Building a large gene expression-cancer knowledge base with limited human annotations</article-title>
        <source>Database J Biol Databases Curation</source>
        <year>2023</year>
        <pub-id pub-id-type="doi">10.1093/DATABASE/BAAD061</pub-id>
      </element-citation>
    </ref>
    <ref id="CR89">
      <label>89.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fleiss</surname>
            <given-names>JL</given-names>
          </name>
        </person-group>
        <article-title>Measuring nominal scale agreement among many raters</article-title>
        <source>Psychol Bull</source>
        <year>1971</year>
        <volume>76</volume>
        <issue>5</issue>
        <fpage>378</fpage>
        <pub-id pub-id-type="doi">10.1037/h0031619</pub-id>
      </element-citation>
    </ref>
    <ref id="CR90">
      <label>90.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McHugh</surname>
            <given-names>ML</given-names>
          </name>
        </person-group>
        <article-title>Interrater reliability: the kappa statistic</article-title>
        <source>Biochem Med</source>
        <year>2012</year>
        <volume>22</volume>
        <issue>3</issue>
        <fpage>276</fpage>
        <lpage>282</lpage>
        <pub-id pub-id-type="doi">10.11613/BM.2012.031</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10941452</article-id>
    <article-id pub-id-type="pmid">38486137</article-id>
    <article-id pub-id-type="publisher-id">5730</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-024-05730-9</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MetaTron: advancing biomedical annotation empowering relation annotation and collaboration</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Irrera</surname>
          <given-names>Ornella</given-names>
        </name>
        <address>
          <email>ornella.irrera@unipd.it</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Marchesin</surname>
          <given-names>Stefano</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Silvello</surname>
          <given-names>Gianmaria</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00240q980</institution-id><institution-id institution-id-type="GRID">grid.5608.b</institution-id><institution-id institution-id-type="ISNI">0000 0004 1757 3470</institution-id><institution>Department of Information Engineering, </institution><institution>University of Padova, </institution></institution-wrap>Padua, Italy </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>14</day>
      <month>3</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>14</day>
      <month>3</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2024</year>
    </pub-date>
    <volume>25</volume>
    <elocation-id>112</elocation-id>
    <history>
      <date date-type="received">
        <day>26</day>
        <month>5</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>4</day>
        <month>3</month>
        <year>2024</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2024</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">The constant growth of biomedical data is accompanied by the need for new methodologies to effectively and efficiently extract machine-readable knowledge for training and testing purposes. A crucial aspect in this regard is creating large, often manually or semi-manually, annotated corpora vital for developing effective and efficient methods for tasks like relation extraction, topic recognition, and entity linking. However, manual annotation is expensive and time-consuming especially if not assisted by interactive, intuitive, and collaborative computer-aided tools. To support healthcare experts in the annotation process and foster annotated corpora creation, we present <italic>MetaTron</italic>. <italic>MetaTron</italic> is an open-source and free-to-use web-based annotation tool to annotate biomedical data interactively and collaboratively; it supports both mention-level and document-level annotations also integrating automatic built-in predictions. Moreover, <italic>MetaTron</italic> enables relation annotation with the support of ontologies, functionalities often overlooked by off-the-shelf annotation tools.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We conducted a qualitative analysis to compare <italic>MetaTron</italic> with a set of manual annotation tools including <italic>TeamTat</italic>, <italic>INCEpTION</italic>, <italic>LightTag</italic>, <italic>MedTAG</italic>, and <italic>brat</italic>, on three sets of criteria: technical, data, and functional. A quantitative evaluation allowed us to assess <italic>MetaTron</italic> performances in terms of time and number of clicks to annotate a set of documents. The results indicated that <italic>MetaTron</italic> fulfills almost all the selected criteria and achieves the best performances.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3"><italic>MetaTron</italic> stands out as one of the few annotation tools targeting the biomedical domain supporting the annotation of relations, and fully customizable with documents in several formats—PDF included, as well as abstracts retrieved from PubMed, Semantic Scholar, and OpenAIRE. To meet any user need, we released <italic>MetaTron</italic> both as an online instance and as a Docker image locally deployable.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Digital health</kwd>
      <kwd>Biomedical annotation tool</kwd>
      <kwd>Relation extraction</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id>
            <institution>Horizon 2020 Framework Programme</institution>
          </institution-wrap>
        </funding-source>
        <award-id>825292</award-id>
        <award-id>825292</award-id>
        <award-id>825292</award-id>
        <principal-award-recipient>
          <name>
            <surname>Irrera</surname>
            <given-names>Ornella</given-names>
          </name>
          <name>
            <surname>Marchesin</surname>
            <given-names>Stefano</given-names>
          </name>
          <name>
            <surname>Silvello</surname>
            <given-names>Gianmaria</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Università degli Studi di Padova</institution>
        </funding-source>
      </award-group>
      <open-access>
        <p>Open access funding provided by Università degli Studi di Padova.</p>
      </open-access>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© BioMed Central Ltd., part of Springer Nature 2024</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par4">In recent years, the exponential growth of biomedical data such as medical reports, Electronic Health Records (EHR) and physician notes posed relevant challenges in effectively and efficiently organizing, curating, managing, and reusing this data both for clinical and research purposes [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR5">5</xref>]. Given the textual nature of biomedical data (according to [<xref ref-type="bibr" rid="CR6">6</xref>], the 70–80% of clinical data is text-based), extracting and reusing the knowledge in the biomedical literature can drive advances in biomedical research, enhance decision-making processes, and accelerate the discovery of drugs and diseases [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR7">7</xref>–<xref ref-type="bibr" rid="CR9">9</xref>]. Consequently, Natural Language Processing (NLP) techniques have gained substantial importance as they can automate retrieval, biomedical data processing, and knowledge extraction. The research area specialized in applying NLP technique to the biomedical data is defined <italic>BioNLP</italic> [<xref ref-type="bibr" rid="CR3">3</xref>]. Developing efficient and effective NLP methods is challenging as it requires the availability of large manually annotated corpora. In [<xref ref-type="bibr" rid="CR10">10</xref>], authors address this problem and present a comparison of the most commonly employed manual annotation tools that can be used to create manually annotated corpora.</p>
    <p id="Par5">Several works studied the application of NLP technique to process biomedical data on different domains: in [<xref ref-type="bibr" rid="CR11">11</xref>], for example, authors discuss the use of NLP techniques to extract symptoms from EHR, while [<xref ref-type="bibr" rid="CR12">12</xref>] discuss the use of NLP to process oncology medical records. In [<xref ref-type="bibr" rid="CR13">13</xref>], a large language model for EHR is proposed, and in [<xref ref-type="bibr" rid="CR6">6</xref>], authors analyze NLP methods to identify advance care planning documentation in patient clinical notes.</p>
    <p id="Par6">In this context, Relation Extraction (RE) task captured considerable interest in the biomedical community as the knowledge stored in the biomedical data may contain valuable insights about the relationships between entities—e.g., protein–protein, gene–disease, drug–drug, and drug–target interactions. Furthermore, Entity Linking (EL), the task of identifying and disambiguating entity occurrences in unstructured text [<xref ref-type="bibr" rid="CR14">14</xref>], is key to detecting the various textual representations of an entity and capturing its underlying meaning. In [<xref ref-type="bibr" rid="CR4">4</xref>], for example, authors show how EL has numerous benefits, including better use of EHR, improved search and retrieval of biomedical resources, abbreviation disambiguation. In addition to RE and EL, another important theme in the biomedical community is Topic Recognition (TR). Detecting the topics discussed in biomedical text plays a vital role in organizing and classifying the vast amount of information available [<xref ref-type="bibr" rid="CR15">15</xref>]. The most recent proposed techniques to perform RE [<xref ref-type="bibr" rid="CR16">16</xref>–<xref ref-type="bibr" rid="CR22">22</xref>], EL [<xref ref-type="bibr" rid="CR23">23</xref>–<xref ref-type="bibr" rid="CR28">28</xref>] and TR [<xref ref-type="bibr" rid="CR29">29</xref>–<xref ref-type="bibr" rid="CR35">35</xref>] on biomedical texts rely on Machine Learning (ML) models whose performances depend on the availability of large annotated corpora used in training, validation, and test. Creating sizeable and trustworthy manual annotated datasets for the biomedical domain and sub-domains is a time-consuming task requiring people with a high level of expertise [<xref ref-type="bibr" rid="CR6">6</xref>]. In this respect, the creation of these corpora is made even more challenging by the intrinsic diversity in topics and concepts within the biomedical field; the UMLS Metathesaurus [<xref ref-type="bibr" rid="CR36">36</xref>], for example, contains over 3.5 million unique concepts belonging to 127 different semantic types [<xref ref-type="bibr" rid="CR27">27</xref>]. In addition, biomedical terminology is complex, and some terms may have different meanings depending on the context where they are used [<xref ref-type="bibr" rid="CR37">37</xref>]. Some notable examples of large annotated corpora are [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR38">38</xref>–<xref ref-type="bibr" rid="CR45">45</xref>]. Furthermore, some annotated corpora targets the annotation of relationships such as [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR46">46</xref>, <xref ref-type="bibr" rid="CR47">47</xref>]. Given the importance of the annotation task and the significant effort required by experts, several manual annotation tools have been developed explicitly for the biomedical domain [<xref ref-type="bibr" rid="CR48">48</xref>–<xref ref-type="bibr" rid="CR53">53</xref>]. Other tools, on the other hand, are general-purpose [<xref ref-type="bibr" rid="CR54">54</xref>–<xref ref-type="bibr" rid="CR58">58</xref>] and offer different features that are not aligned with the biomedical experts’ needs. The coexistence of multiple annotation tools arises from other tools offering diverse functionalities and targeting various domains of interest.<fig id="Fig1"><label>Fig. 1</label><caption><p>Annotation tools features overview. Each of the selected twenty annotation tools is evaluated based on 24 criteria: 7 technical (T) criteria, three criteria about input and output data formats (D), and 15 criteria concerning the functionalities (F) provided by each tool. The first 22 criteria are taken from [<xref ref-type="bibr" rid="CR10">10</xref>], while we added the last two. Each criterion is marked in blue for each tool if the feature is fully satisfied, light blue if partially satisfied, and white if not satisfied</p></caption><graphic xlink:href="12859_2024_5730_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par7">In [<xref ref-type="bibr" rid="CR10">10</xref>], a review of the widest-used annotation tools is proposed, highlighting the points of strength and weakness of each. They selected 15 annotation tools according to the following criteria:<list list-type="bullet"><list-item><p id="Par8"><italic>Availability</italic>: the tool must be readily available. This criterion ensures that the tool is accessible via a public URL or is downloadable, independently of the user’s expertise. In this respect, the availability of source code is important not only in terms of transparency and accountability but also to guarantee further development and customization according to the user’s needs;</p></list-item><list-item><p id="Par9"><italic>Web-based</italic>: the tool must be a web application. With the term “web application,” we refer to a software application following the most frequently used 3-tiers architecture: a presentation layer handles user interface and interaction, an application layer implements the business logic, and a data layer permanently stores data resulting from or useful to the user interaction [<xref ref-type="bibr" rid="CR59">59</xref>]. Web-based tools can run online. Hence, they can be accessed via a public URL or offline and, in this case, can be installed as web applications. Offline web-based tools require an installation procedure to run on the user’s personal computers or distributed servers. Web-based tools are usually more flexible and solid with the passing of time, and there is a growing emphasis on software not requiring local installations [<xref ref-type="bibr" rid="CR60">60</xref>]; they allow multiple annotators to work collaboratively on the same document in different environments and are platform independent—e.g., other operative systems and different machines.. Conversely, according to [<xref ref-type="bibr" rid="CR10">10</xref>], non-web-based tools are stand-alone systems or plugins running on other tools and platforms. According to two analyses carried out in [<xref ref-type="bibr" rid="CR10">10</xref>], in the past ten years, web-based tools have been more prevalent than stand-alone tools and plugins and are the most frequently used in the annotation of biomedical corpora;</p></list-item><list-item><p id="Par10"><italic>Installability</italic>: offline web-based tools are needed when documents and annotations must be kept private. These tools must be installed on personal computers or distributed servers, and their installation procedure must be finished in less than two hours. The ease and speed of installation and setup are crucial factors that influence the usability of a tool for a diverse range of users with varying backgrounds. Installability is a crucial feature also highlighted by another recent survey of image annotation tools [<xref ref-type="bibr" rid="CR61">61</xref>]; indeed, installation issues are one of the main reasons annotation tools cannot be reused in the field;</p></list-item><list-item><p id="Par11"><italic>Workable</italic>: the tool must be intuitive, and all the features must be comprehensive enough to be used independently of the level of expertise, relying on a well-documented set of instructions and without the help of the developers. This is a pivotal factor that directly impacts the tool’s utilization. Features that are challenging to comprehend render the examined tool impractical; see also [<xref ref-type="bibr" rid="CR61">61</xref>] for further supporting analysis on the usability of annotation tools. Therefore, it is crucial to take into account aspects related to usability and implementation;</p></list-item><list-item><p id="Par12"><italic>Schematic</italic>: the tool allows for schema configuration. In this context, the tool defines elements such as labels, documents, and concepts according to the user’s needs. The tool should not be developed for a specific use-case and should not provide a fixed set of labels, concepts and rules for the annotation. Concerning this point, a tool that does not allow the user to define a schema or is designed for a specific use case will likely face challenges in being reused.</p></list-item></list>Once the tools were chosen according to the aforementioned selection criteria, the authors defined a set of 26 evaluation criteria to compare them. The evaluation criteria can be subdivided into four macro-areas: (i) <italic>Publication</italic>: these features concern the tools’ publications and citations; (ii) <italic>Technical</italic>: these features concern technical aspects of the tool, and are useful to determine the availability of the tool, and the ease of installation; (iii) <italic>Data</italic>: these features describe what formats the tool requires in input and output; (iv) <italic>Functional</italic>: criteria concerning the functionalities provided by the tools. Functional criteria describe all the feature a tool provides—i.e., document-level annotations, availability of overlapping mentions, active learning, collaborative features.</p>
    <p id="Par13">While <italic>Functional</italic> criteria are significant in identifying the primary distinctions among tools and deciding which tool best suits the user’s needs, <italic>Data</italic> criteria, on the other hand, enable users to comprehend the required data formats for each tool. Consequently, users can assess whether their data needs preprocessing. A subset of these criteria has been used also for evaluation purposes in [<xref ref-type="bibr" rid="CR62">62</xref>] that describes an ecosystem for knowledge discovery. We revised the assessment conducted by [<xref ref-type="bibr" rid="CR10">10</xref>], updating <italic>LightTag</italic>, <italic>INCEpTION</italic>, and <italic>MedTAG</italic> according to our experience with each of these tools, adding some tools released after the publication of the paper and including new features we deemed as important for the current trend in bioinformatics.</p>
    <p id="Par14">The heat map we obtained is reported in Fig. <xref rid="Fig1" ref-type="fig">1</xref> with the evaluated annotation tools as rows and the tested criteria as columns. The color intensity of the cells indicates the level of adherence of a tool to each criterion. We evaluated 24 criteria, including 22 criteria (from T1 to F13 in Fig. <xref rid="Fig1" ref-type="fig">1</xref>) from the set of 26 criteria defined in [<xref ref-type="bibr" rid="CR10">10</xref>] and two new criteria (F14 and F15) defined here for the first time. The first six criteria are <italic>Technical (T)</italic>: (T1) date of last version or commit—whether the last version has been released within the past five years; (T2) availability of source code; (T3) online availability; (T4) easiness of installation; (T6) license allowing modification and redistribution; (T7) free of charge. Three criteria concern <italic>Data (D)</italic>: (D1) schema format—whether the schema is configurable; (D2) input format—whether the input documents follow a standard format; (D3) output annotations format—whether the annotations are based on standard formats. Finally, fifteen criteria are <italic>Functional (F)</italic>: (F1) support for overlapping mentions; (F2) support for document-level annotation; (F3) support for relationship annotation; (F4) support for ontologies; (F5) support for built-in prediction and active learning; (F6) integration with PubMed; (F7) suitability for full texts; (F8) support for the partial saving of an annotation (allowing the user to continue the annotation process later); (F9) support for text highlighting; (F10) support for users and teams; (F11) support for Inter Annotator Agreement (IAA); (F12) data privacy; (F13) multilingual support; (F14) connection to ORCID; (F15) retrieval of abstracts from external repositories or services. The use of most of the evaluation criteria from [<xref ref-type="bibr" rid="CR10">10</xref>], ensures that the evaluation analysis we conducted is as objective as possible, avoiding to bias the study towards <italic>MetaTron</italic> strong points. Moreover, we further validated <italic>MetaTron</italic> against these evaluation criteria by the means of two expert-based case studies.</p>
    <p id="Par15">As remarked in [<xref ref-type="bibr" rid="CR10">10</xref>], we confirm that no currently available off-the-shelf tool comprehensively meets all the requirements. This is also evident from Fig. <xref rid="Fig1" ref-type="fig">1</xref>, where the missing features in the majority of selected annotation tools are: support for relationship annotation (F3), support for overlapping mentions (F1), support for document-level annotations (F2), connection to ORCID (F14) and the integration with external repositories and services such as PubMed to retrieve publications’ abstracts (F6, F15).</p>
    <p id="Par16">In this paper, we introduce <italic>MetaTron</italic>, an innovative web-based annotation tool for the biomedical literature which fulfills all the selected evaluation criteria. <italic>MetaTron</italic> is released both as an online instance and as a Docker image deployable on a local server relying on a quick and easy installation procedure. It is fully customizable, as users can upload documents in JSON, PDF, CSV, and TXT or retrieve and upload abstracts from PubMed, Semantic Scholar, and OpenAIRE. The support for both mention-level and document-level annotation types makes <italic>MetaTron</italic> suitable for several use cases. Additionally, <italic>MetaTron</italic> supports automatic built-in predictions.</p>
    <p id="Par17">The rest of the paper is organized as follows: in “<xref rid="Sec2" ref-type="sec">Implementation</xref>” Section we describe <italic>MetaTron</italic> and its features, focusing on the annotation types <italic>MetaTron</italic> provides and <italic>AutoTron</italic> for automatic built-in predictions; in “<xref rid="Sec12" ref-type="sec">Results</xref>” Section we describe the qualitative and quantitative analyses we conducted to evaluate <italic>MetaTron</italic> and compare to other annotation tools; in  “<xref rid="Sec17" ref-type="sec">Conclusions</xref>” Section we draw some final remarks.</p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <sec id="Sec3">
      <title>System overview</title>
      <p id="Par18"><italic>MetaTron</italic> is an annotation tool designed to annotate biomedical documents. One of the key features of <italic>MetaTron</italic> is its support for multiple annotation types. The annotation types can be classified into <italic>document-level annotations</italic> and <italic>mention-level annotation</italic>. <italic>Mention-level annotations</italic> concern the annotation of specific portions of the textual document and comprize <italic>mention, concept linking</italic>, and <italic>relationship</italic> annotations. Mention annotation detects the mentions in a textual document, and each mention can be linked to one or more <italic>concepts</italic> from an ontology—i.e., concepts linking. In this work, we use the terms <italic>entities</italic> and <italic>concepts</italic> interchangeably; in particular, we consider a <italic>concept</italic> as an atomic, identifiable object that has a distinct and independent existence [<xref ref-type="bibr" rid="CR63">63</xref>]. In <italic>MetaTron</italic>, as in the Semantic Web realm, a concept is identified by a URI and described by a name and a type—e.g., gene or disease—making the concept human-understandable. Relationship annotation involves identifying and marking “statements” or “facts” within a text. A statement typically consists of three components: a subject, a predicate, and an object, collectively conveying a specific meaning. It is important to note that the constituents of a statement may be explicitly mentioned in the text, or they can be implicitly understood by considering the surrounding context and their association with ontological concepts.</p>
      <p id="Par19"><italic>Document-level annotations</italic> pertain to considering an entire textual document as a unit. In the <italic>MetaTron</italic> framework, there are two types of document-level annotations: <italic>label</italic> and <italic>assertion</italic> annotations. The former involves assigning one or more labels (representing individual concepts) to the document to classify its content. The latter enables the annotation of a document with a collection of assertions, subject-predicate-object triples linked to ontological concepts. These assertions provide a high-level description of the document’s content. Treating the assertions as machine-readable triples can be incorporated into a Resource Description Framework (RDF) graph, facilitating inference and knowledge representation.</p>
      <p id="Par20"><italic>MetaTron</italic> offers support for ontologies by enabling users to define a collection of ontological concepts identified by an identifier, a name, a type (e.g., <italic>gene</italic> or <italic>disease</italic>) and a description. Concepts are not necessarily tied to a specific ontology: this guarantees more customizability and flexibility, allowing the user to add concepts belonging not only to widely recognized and publicly accessible ontologies but also to user-designed or not yet published ontologies or vocabularies. Concepts can be uploaded in batch or added at the moment of annotation, allowing the user to enrich the set of concepts when needed.</p>
      <p id="Par21">These features of <italic>MetaTron</italic> enable users to annotate a diverse range of biomedical entities and relationships, including genes, proteins, diseases, drug treatments, and their associations. By allowing this customization and flexibility, <italic>MetaTron</italic> can be adapted to suit various use cases and user needs.</p>
      <p id="Par22">Collaborative annotation is a significant aspect of the <italic>MetaTron</italic> system, particularly in the context of collectively annotating a group of documents. This collaborative feature is crucial as it facilitates users to annotate documents together, improving annotation quality and accuracy. By working collaboratively, annotators can save time and enhance the overall annotation process. Through this feature, users can view annotations made by their colleagues and identify annotations that exhibit the highest agreement. Furthermore, the members of the document collection have access to comprehensive statistics about the entire collection or specific documents. This functionality enables them to monitor the progress of annotations and visually analyze the complete set of annotations, considering various annotation types and levels of agreement.</p>
      <p id="Par23">To enhance the user experience and expedite the annotation process, <italic>MetaTron</italic> incorporates <italic>AutoTron</italic>, a feature that offers automated predictions within the system. <italic>AutoTron</italic> is a framework designed to automatically annotate relationships and assertions, allowing users to implement their methods as desired. Leveraging automatic annotations is pivotal in enhancing and accelerating the overall annotation workflow by offering users an initial set of annotations that can be modified. The <italic>AutoTron</italic> system operates as a plug-and-play mechanism, enabling users to integrate their custom automatic annotation methods seamlessly. Additionally, <italic>MetaTron</italic> includes two pre-built methods specifically designed for automatically annotating gene-disease associations and gene expression-cancer associations, further augmenting the automated annotation capabilities.</p>
      <p id="Par24"><italic>MetaTron</italic> is designed to be highly adaptable and can be customized to suit any area of interest. It offers an easy-to-use customization process, where documents can be uploaded in various formats, such as JSON, CSV, TXT, and PDF, using the integration with GROBID (GeneRation Of BIbliographic Data) [<xref ref-type="bibr" rid="CR64">64</xref>]. GROBID is open-source software that uses machine learning techniques to extract structured data such as author names, affiliations, abstracts, publication dates, and references from scientific articles. This feature helps detect the various sections of a publication. Additionally, <italic>MetaTron</italic> incorporates REST APIs such as PubMed, Semantic Scholar, and OpenAIRE to enable users to upload one or more PMIDs (for PubMed) or DOIs (for Semantic Scholar and OpenAIRE) and annotate related information such as the title, abstract, authors, venue, and date of publication.</p>
      <p id="Par25">Additional features of <italic>MetaTron</italic> are the following: (i) an easy-to-use user interface that supports the automatic saving of the annotations; (ii) integration of keyboard shortcuts to navigate between documents and perform new annotations; (iii) support for the download of annotations in JSON, CSV, BioC/XML formats; (iv) support for the upload new annotations from JSON or CSV file; (v) support for user-defined style properties such as the colors of the mentions, the size or the line height of the textual content; (vi) multilingual support; (vii) support for annotation suggestions; (viii) IAA support, implemented through Fliess’ kappa, Cohen’s kappa and majority voting; (ix) dockerized and online instance available, (x) support for multiple ontologies, (xi) connection to ORCID, (xii) support for multiple annotation rounds.</p>
      <sec id="Sec4">
        <title>Architecture</title>
        <p id="Par26">
          <fig id="Fig2">
            <label>Fig. 2</label>
            <caption>
              <p>MetaTron architecture. <italic>MetaTron</italic> has three layers: the <italic>Data layer</italic> (a PostgreSQL database for data and annotations), the <italic>Business layer</italic> (with a REST API and services for automatic annotation and PDF parsing), and the <italic>Presentation layer</italic> (the user interface)</p>
            </caption>
            <graphic xlink:href="12859_2024_5730_Fig2_HTML" id="MO2"/>
          </fig>
        </p>
        <p id="Par27">The system architecture is illustrated in Fig. <xref rid="Fig2" ref-type="fig">2</xref> and can be divided into three layers: a data layer, a business layer, and a presentation layer.</p>
        <p id="Par28">The data layer is based on a PostgreSQL database that stores the annotations data, information about collections, concepts, and documents. This layer is responsible for managing the persistence and retrieval of data and ensuring data integrity.</p>
        <p id="Par29">The business layer comprises a REST API and a business logic implemented using the Django Python web framework.<xref ref-type="fn" rid="Fn1">1</xref> The REST API acts as an intermediary between the presentation layer and the data layer, while the business logic handles and processes data retrieved from the database based on the application’s needs. This layer also utilizes additional services such as <italic>AutoTron</italic> (presented in Sect. <xref rid="Sec8" ref-type="sec">2.2.2</xref>) and GROBID [<xref ref-type="bibr" rid="CR64">64</xref>].</p>
        <p id="Par30">The presentation layer is responsible for displaying the data to the users and receiving their input. It is developed using ReactJS, HTML, CSS, and JavaScript. This layer interacts with the business logic layer through the REST API to retrieve and display the data to the user.</p>
        <p id="Par31">Overall, this architecture provides a clear separation of concerns between the different layers, improving the system’s maintainability, scalability, and modularity. Using a database, a REST API, and additional services enhances the system’s data management and processing capabilities.</p>
      </sec>
      <sec id="Sec5">
        <title>Availability</title>
        <p id="Par32"><italic>MetaTron</italic> is released as an online and dockerized instance. The online instance is available at <ext-link ext-link-type="uri" xlink:href="https://metatron.dei.unipd.it">https://metatron.dei.unipd.it</ext-link>.<xref ref-type="fn" rid="Fn2">2</xref> An online demo and tutorial is available at: <ext-link ext-link-type="uri" xlink:href="https://metatron.dei.unipd.it/demo">https://metatron.dei.unipd.it/demo</ext-link>.</p>
        <p id="Par33">It is intended to be used with scientific publications—e.g., scientific articles or publications in PubMed. To use <italic>MetaTron</italic>, it is necessary to sign up by providing a username, a password, and a profile that identifies the level of expertise. Once signed up, the user will be asked to specify their level of expertise. They can create new document collections and invite other collaborators to join the project.</p>
        <p id="Par34"><italic>MetaTron</italic> is also released as a <italic>Docker container</italic> which guarantees cross-platform portability, scalability, and isolation. Furthermore, the dockerized version can be utilized by users who want to upload collections of documents whose content must be kept private, in all the cases where the network is not fully operational or when users want to introduce new features (i.e., new methods for automatic relation extraction). A local installation is also required if the user works with confidential documents or in a privacy-preserving setting. The dockerized version also eases the installation of <italic>MetaTron</italic> in a private network setting when the documents are unavailable on the Web, but distributed collaboration amongst the annotators is required. The installation procedure is detailed in the <italic>MetaTron</italic> repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/GDAMining/metatron/">https://github.com/GDAMining/metatron/</ext-link>), where the source code is publicly available.</p>
      </sec>
    </sec>
    <sec id="Sec6">
      <title>Annotation interface</title>
      <p id="Par35">
        <fig id="Fig3">
          <label>Fig. 3</label>
          <caption>
            <p>MetaTron user interface. The main annotation interface consists of five distinct blocks: in the main header (<bold>1</bold>), the user can navigate to other web pages and logout; in the document header (<bold>2</bold>), there is the main information about a document; in annotation panel (<bold>3</bold>) it is possible to check the annotation status; the document takes the largest part of the page to annotate (<bold>4</bold>); and in the vertical toolbar (<bold>5</bold>) it is possible to access several functions a user can perform during the annotation process</p>
          </caption>
          <graphic xlink:href="12859_2024_5730_Fig3_HTML" id="MO3"/>
        </fig>
      </p>
      <p id="Par36">The <italic>MetaTron</italic> annotation interface and its features have been designed to be intuitive and facilitate and speed up the entire annotation process. <italic>MetaTron</italic> annotation interface is illustrated in Fig. <xref rid="Fig3" ref-type="fig">3</xref> that we use as a guide to illustrate the tool’s main features. Upon successful login, the system presents the user with the most recently annotated document. At the top of the annotation interface, there is the <italic>main header</italic> (<bold>1</bold>). The <italic>Home</italic>, <italic>Collections</italic>, <italic>Statistics</italic> buttons can be used to navigate to home—i.e., the page illustrated in Fig. <xref rid="Fig3" ref-type="fig">3</xref>, collections, and statistics web pages. The user name is displayed at the top right of the page and can be used to log out. The <italic>document header</italic> (<bold>2</bold>), placed below the main header, includes the information about the current document identifier—e.g., pubmed_27839516—and the related collection name—e.g., pubmed_collection. Two arrow buttons allow the user to navigate between the documents of the collection; it is worth noting that it is also possible to navigate from one document to another through a custom shortcut designed to allow the user to change documents relying only on the keyboard. The <italic>Delete</italic> button deletes the annotations for the current document, while the <italic>Assertion</italic> button creates a new assertion as a document-level annotation.</p>
      <p id="Par37">By clicking on the <italic>Annotation</italic> button, a drop-down menu appears, enabling the user to choose one or more annotation types. For each selected annotation type, the list of annotations is then displayed on the right-hand side of the annotation interface, in the <italic>annotation panel</italic> (3). The user can view, add, modify, or delete their annotations using this panel. The largest portion of the page is taken by the textual document (4) the user annotates. The <italic>vertical toolbar</italic> (5) provides a set of functionalities illustrated in Fig. <xref rid="Fig3" ref-type="fig">3</xref> from (A) to (K), that can be accessed directly from the main annotation interface improving, and speeding up the annotation procedure, and minimizing the number of actions to be performed. In (A), the list of documents of the collections is shown. The user can filter documents based on whether they contain at least one annotation and search for a specific document to annotate using its ID. In (B), the user can view a list of document collections available for annotation. Each collection is represented by a button displaying the collection name and the percentage of annotated documents. The button color varies based on the percentage of documents that have been annotated—e.g., green color is used when the user annotated more than 80% of documents, while red color is used when the user annotated less than 20% of documents. By clicking on a collection, it is possible to start annotating its documents. In (C), the users who annotated the current document are listed. It is possible to load the annotation of one of the users in the list by clicking on the associated username. (D) allows the user to open two tables containing <italic>personal</italic> and <italic>global</italic> annotations statistics overview for the current document; the former concerns the annotations of the current user, while the latter those of all the annotators. Each table contains the number of mentions, associations, mentions-concepts, relationships, assertions, and labels annotated. In the <italic>global</italic> overview, the agreement computed with the Fleiss’ kappa measure is provided. (E) allows the user to customize <italic>MetaTron</italic>. It is possible to: hide or display specific sections of the document, increase or decrease the font size and the line height, and set the color associated with each concept type. These settings have been defined to facilitate and speed up the annotation workflow and improve document readability. (F) enables to download of the annotations. The user has to specify the file format—e.g., JSON, CSV, BioC, the annotation type—e.g., mentions, concepts, relationships, and the annotator username, choosing between all the users who annotated that document. (G) opens an upload panel where only the user who created the collection can upload new lists of documents and ontological concepts. Documents can be uploaded in several formats: CSV, TXT, JSON, and PDF. The user can search for a specific publication in PubMed (by providing the related PMID), in Semantic Scholar, and OpenAIRE (by providing the related DOI): <italic>MetaTron</italic> takes advantage of their REST APIs to retrieve the title, the abstract, the authors, the date of publication, and the venue information. The user can upload a new set of annotations in CSV or JSON format that will be automatically loaded in <italic>MetaTron</italic>. In (H), the user can hide the ontological concepts associated with the mentions and visualize only the annotated mentions increasing the document’s readability. In (I), it is possible to rely on <italic>AutoTron</italic> to annotate the current document automatically for specific cases. (J) and (K) open new tabs with the <italic>MetaTron</italic> instructions, and the credits respectively.</p>
      <sec id="Sec7">
        <title>Manual annotation</title>
        <p id="Par38"><italic>Mention annotation</italic> Mentions are textual spans that can be linked to one or more ontological concepts. In <italic>MetaTron</italic>, a mention can either consist of one or more consecutive tokens (or words), where a token is a sequence of characters between two spaces or a substring of one or more contiguous tokens. In this case, the first or last character of the mention does not necessarily coincide with the character that follows or precedes a space. It is possible to select a mention that consists of two or more consecutive tokens by clicking on the first and the last words, respectively: all the words comprised between the two selected will be part of the new mention. To create a mention containing a single token, it is possible to double-click on the desired token. To annotate a substring, hence selecting a part of the token (or two or more consecutive tokens), it is possible to drag and drop the mouse from the first to the last characters of the substring. <italic>MetaTron</italic> allows for the selection of overlapping mentions, meaning that a piece of text already included in a mention can be selected.</p>
        <p id="Par39">This implementation is based on our direct experience with other annotation tools, which has allowed us to assess the pros and cons of various possible implementations. We have decided to implement the annotation of mentions both through drag and drop and by clicking on individual words to allow the user to annotate both specific textual substrings and mentions of two or more words quickly, streamlining and expediting the workflow.</p>
        <p id="Par40">When the textual content of a mention occurs more than once in the textual document, it is possible to annotate all the mentions simultaneously. When a mention is annotated, a new modal will appear if its content occurs more than once, and the user can decide to annotate the mentions altogether.</p>
        <p id="Par41">It is possible to access the <italic>mention panel</italic> illustrated in Fig. <xref rid="Fig4" ref-type="fig">4</xref> by performing a right-click on a mention. This panel includes all possible actions and annotations related to the selected mention. From this panel, users can get information about the mention—e.g., the date of annotation and the number of annotators who annotated that mention for that document, receive some suggestions about the concepts to link to the mention, perform new annotations—i.e., add a new concept or a new relationship, and delete the mention. The option <italic>Annotate all</italic> finds all the occurrences of a mention in the document and annotates them simultaneously.<fig id="Fig4"><label>Fig. 4</label><caption><p>MetaTron mention panel. The mention panel opens when the user right-clicks on the desired mention. It allows the user to get more information about the mention, receive suggestions about the concepts to link, add new concepts or relationships, annotate similar mentions, and delete the mention</p></caption><graphic xlink:href="12859_2024_5730_Fig4_HTML" id="MO4"/></fig></p>
        <p id="Par42"><italic>Concepts linking</italic> To link a new ontological concept to a mention, the user can open the mention panel of a mention and select the <italic>Add Concept</italic> option. A new modal will appear, allowing the user to select a concept. Each collection of documents has a list of user-defined ontological concepts, not necessarily tied to a single ontology. From the modal, the user can explore this set of concepts, filter them according to their type, name, and identifier, and view the associated description. If the list of ontological concepts of the collection is large, to aid the user in selecting a concept, <italic>MetaTron</italic> provides auto-completion facilities to find the desired concept easily.</p>
        <p id="Par43">The incorporation of concept annotations in our system mirrors the approach employed by various tools, such as <italic>brat</italic> and <italic>INCEpTION</italic>. Nevertheless, we observed that our implementation is characterized by its intuitiveness. By positioning the concept above the corresponding mention and employing distinct colors based on the type, users can readily discern between different concepts, thereby augmenting the immediacy of the annotation experience.</p>
        <p id="Par44">Alternatively, if the concept is unavailable in the provided list, the user can define a new concept that will be automatically added to the collection’s concepts list. In this case, the user must define the concept’s type, name, URI (or ID), and an optional description.<fig id="Fig5"><label>Fig. 5</label><caption><p>MetaTron concept selection modal. The concept selection modal allows the user to select a concept to link to a mention. The user can filter the concepts according to the concept type and use auto-completion facilities to filter further the list of concepts (<bold>A</bold>). Once a concept is selected, its description will automatically appear in the modal (<bold>B</bold>)</p></caption><graphic xlink:href="12859_2024_5730_Fig5_HTML" id="MO5"/></fig></p>
        <p id="Par45">In Fig. <xref rid="Fig5" ref-type="fig">5</xref>, we can see how to link a concept to a mention. The user can filter the concepts to choose from, specifying the concept type—i.e., <italic>disease</italic> in the figure. By typing the first letters of the desired concepts, the available options are automatically shown (A in Fig. <xref rid="Fig5" ref-type="fig">5</xref>). The required information typically consists of the concept type and the name to select a concept. The user is required to select the ID of the concept only if two or more concepts share the same name but with different URIs. Once a concept is selected, the related description will automatically appear (B in Fig. <xref rid="Fig5" ref-type="fig">5</xref>). If no option is shown, the concept is not on the list, and it is possible to add a new concept.</p>
        <p id="Par46">If a mention is linked to one or more concepts, by clicking on the <italic>Annotate all</italic> option of the mention panel, it is possible to locate all the instances of that mention in the document and associate them with the same set of concepts simultaneously.</p>
        <p id="Par47">Each mention can have more than one linked concept. The concepts linked to a mention are displayed above the mention; they are clickable so that the user can be provided with the information about the concept—i.e., the URI, the name, the description, and the type. In the annotation interface, each concept can have a different color depending on its type: in Fig. <xref rid="Fig3" ref-type="fig">3</xref>, for example, concepts of type <italic>Disease</italic> and the associated mentions are highlighted in pink, while <italic>Gene</italic> type concepts in dark blue.<fig id="Fig6"><label>Fig. 6</label><caption><p>Relationship annotation. When the user annotates a new relationship, the document’s content is blurred except for the mentions highlighted with different colors, which can be selected as subject, predicate, and object, respectively. The three steps to create a relationship are shown in the example examined. First, the object mention is selected from the menu (<bold>1</bold>); then, it is selected a predicate (<bold>2</bold>): by clicking on <bold>A</bold>, it is possible to manually type the predicate of the relationship, while by clicking on <bold>B</bold> it is possible to select a concept. In (<bold>3</bold>), the final relationship is represented: it comprises two mentions—the subject and the object and an ontological concept—the predicate. Each relationship element has a different color to make each element easily recognizable. The panel on the right allows the user to update and save the relationship</p></caption><graphic xlink:href="12859_2024_5730_Fig6_HTML" id="MO6"/></fig></p>
        <p id="Par48"><italic>Relationship annotation</italic> A relationship comprizes three primary components: a <italic>subject</italic>, a <italic>predicate</italic>, and an <italic>object</italic>; the relationship always starts from the subject and ends with the object. Each relationship element can be represented by either an ontological concept or a textual mention (with or without linked concepts). At least one of the three components of a relationship must be a mention.<xref ref-type="fn" rid="Fn3">3</xref> A new relationship can be added through the <italic>Add Relationship</italic> option of the mention panel of a mention. By default, this mention will be the subject of the relationship. This action automatically shows a <italic>relationship panel</italic> that provides a comprehensive overview of the relationship and its components. All the other mentions composing the relationship (if any) can be added by clicking on each mention, or by right-clicking on the desired mention, it is possible to select its role. It is worth noting that it is always possible to change the role of a mention—e.g., a mention which was the subject can become the object. From the relationship panel, it is possible to select the concepts of the relationship by clicking on the <italic>Add predicate</italic>, <italic>Add subject</italic>, or <italic>Add object</italic> buttons. In Fig. <xref rid="Fig6" ref-type="fig">6</xref>, the creation of a new relationship is illustrated. The relationship comprises two mentions (the subject BRCA1 and the object breast cancer) and an ontological concept (the predicate Oncogenes). In <bold>1</bold>, the object mention is selected by declaring its role from the menu. In <bold>2</bold>, the predicate is selected. There are two ways to select a predicate: from <bold>A</bold>, it is possible to input a string manually that represents the predicate, while from <bold>B</bold>, it is possible to select a predicate concept. In <bold>3</bold>, the created relationship is shown. In the textual document, each relationship component has a different color depending on its role: the subject is highlighted in red, the predicate in green, and the object in orange. The three components are linked via arrows whose positions can be changed by the user to facilitate the document readability—i.e., each mention is surrounded by four points that determine the points where the arrows can start or end.</p>
        <p id="Par49">On the right, the <italic>relationship panel</italic>, for each relationship component, shows the type of the component—e.g., mention or concept, and its role—e.g., subject, predicate, object. The annotations panel provides an overview of all the relationships the user annotates; each relationship can be viewed, edited, and deleted directly from the interface.</p>
        <p id="Par50">In this implementation, each relationship component is manually chosen, whether a mention selected in the text or a concept chosen relying on the right panel. This approach, even though it may require more steps than previous annotations, allows the users to designate which element within the relationship should be identified as a mention and which as a concept. Unlike <italic>brat</italic> and <italic>INCEpTION</italic> that enable the annotation of relationships, <italic>MetaTron</italic> does not necessitate the subject and object to be mentions within the textual document. Relationships may span multiple sentences; at least one among subject, predicate and object must be a mention, while the remaining components can be concepts.<fig id="Fig7"><label>Fig. 7</label><caption><p>Relationships list. Overview of the relationships annotated by the user. The list is subdivided into subject, predicate, and object, each into concept types. It is also possible to filter the concepts composing the relationships according to the type and the name, taking advantage of auto-completion facilities</p></caption><graphic xlink:href="12859_2024_5730_Fig7_HTML" id="MO7"/></fig></p>
        <p id="Par51">In Fig. <xref rid="Fig7" ref-type="fig">7</xref>, we can see the relationships list. It is subdivided into three categories: subject, predicate, and object; each category is further subdivided into concept types. This layout provides the user with an overview of the different concept types characterizing subjects, predicates, and objects of the annotated relationships.</p>
        <p id="Par52"><italic>Assertions annotation</italic> Like relationships, assertions consist of a subject, predicate, and object, each represented by an ontological concept unlinked to any mention in the document. To add a new assertion (see Fig. <xref rid="Fig8" ref-type="fig">8</xref>), a dedicated button in the document header opens an <italic>assertion panel</italic> similar to the one provided for relationships. Users can create a new assertion by specifying the types, names, and URIs of the subject, predicate, and object concepts. The annotated assertions can be viewed, edited, and removed via the annotation panel by selecting the assertion annotation type; the annotation panel contains the assertions the user created. For each assertion, the subject, the predicate, and the object concepts are provided with information, including the date and number of annotators. Furthermore, each assertion can be viewed, edited, or deleted. As far as our current knowledge extends, <italic>MetaTron</italic> stands out as the initial tool to introduce the creation of document-level assertions. Therefore, we have opted to maintain a close resemblance between assertion annotations and relationship annotations. This decision aims to facilitate a seamless user experience, enabling users to bypass the need to acquaint themselves with a novel annotation methodology and expedite the overall annotation workflow.<fig id="Fig8"><label>Fig. 8</label><caption><p>Assertion details. The annotation panel contains an overview of the annotated assertions. For each assertion, the subject, the predicate, and the object concepts are provided; each assertion can be edited or deleted via the two buttons placed near the <italic>Assertion 1</italic> title. The date of annotation and the number of annotators are shown below the assertion</p></caption><graphic xlink:href="12859_2024_5730_Fig8_HTML" id="MO8"/></fig></p>
        <p id="Par53"><italic>Labels annotation</italic> Labels allow the user to classify the document. Each collection has its own set of labels specified at its creation. To add one or more labels to the document, the user must open the annotation panel and select the labels annotation type. Each label is a button that can be selected or selected by a click. In Fig. <xref rid="Fig9" ref-type="fig">9</xref>, the annotation panel shows an example of the labels that can be associated with the displayed document for the selected collection. The selected labels have a light blue background.<fig id="Fig9"><label>Fig. 9</label><caption><p>Labels annotation. The annotation panel contains the labels to assign to the document. Each label is a button that can be selected or deselected. All the selected labels have a colored background; the others have a transparent background</p></caption><graphic xlink:href="12859_2024_5730_Fig9_HTML" id="MO9"/></fig></p>
      </sec>
      <sec id="Sec8">
        <title>AutoTron: automatic annotations</title>
        <p id="Par54"><italic>AutoTron</italic> represents the automatic annotation component of <italic>MetaTron</italic>. As an automatic component, <italic>AutoTron</italic> can be implemented by the user at will. The only requirement lies on the I/O structure, where specific I/O data are required to integrate the component within <italic>MetaTron</italic>. Below, we first describe the general architecture of the <italic>AutoTron</italic> component and then present two different implementations used in two annotation tasks: document-level Gene-Disease Association (GDA) extraction and sentence-level Gene expression-Cancer Association (GCA) extraction. Both implementations are currently available in <italic>MetaTron</italic> and can be used by the user on the corresponding annotation tasks. Figure <xref rid="Fig10" ref-type="fig">10</xref> shows the <italic>AutoTron</italic> workflow to extract GCAs from a sample PubMed article (i.e., PubMed id: 24662820). The workflow involves three steps. First, users select the desired task (1), which in this case is GCA. Once the user clicks the <italic>annotate</italic> button, a loading icon indicates that the automatic annotation process is underway (2). Finally, the annotations generated by <italic>AutoTron</italic> are displayed to the user (3).<fig id="Fig10"><label>Fig. 10</label><caption><p>AutoTron workflow. Overview of the workflow to extract GCAs from PubMed article 24662820. In (1), users select the task (i.e., GCA). Then, once the annotation process starts, a loading icon indicates the process is underway (2). Finally, the generated annotations are displayed to users in (3)</p></caption><graphic xlink:href="12859_2024_5730_Fig10_HTML" id="MO10"/></fig></p>
        <p id="Par55"><italic>Architecture</italic><italic>AutoTron</italic> consists of two main components and specific I/O data requirements. The main components are EL and RE. EL assigns unique meanings to entities mentioned within text [<xref ref-type="bibr" rid="CR65">65</xref>, <xref ref-type="bibr" rid="CR66">66</xref>], whereas RE identifies and extract relations between linked entities mentioned in text [<xref ref-type="bibr" rid="CR67">67</xref>, <xref ref-type="bibr" rid="CR68">68</xref>]. The EL and RE modules are containers where different methods can be plugged in/out, but which must adhere to specific I/O data. In this regard, the I/O data consists of specific fields. In input, <italic>AutoTron</italic> requires a field containing the text to annotate. In the output, <italic>AutoTron</italic> provides, for each extracted relationship or assertion, the subject, predicate, and object concept IDs, names, and types, as well as the corresponding mention positions within the text (if any). Together, these fields allow <italic>MetaTron</italic> to seamlessly integrate any implementation of the <italic>AutoTron</italic> component. In other words, <italic>AutoTron</italic> represents a framework for automatically annotating relationships and assertions that users can implement at will.</p>
        <p id="Par56"><italic>Entity linking</italic> We consider different EL systems depending on the input text. When the input text comes from PubMed, we use the PubTator system [<xref ref-type="bibr" rid="CR69">69</xref>–<xref ref-type="bibr" rid="CR71">71</xref>]. PubTator provides automated annotations from state-of-the-art text mining systems for genes/proteins, genetic variants, diseases, chemicals, species, and cell lines. In particular, PubTator normalizes annotated genes to NCBI Gene [<xref ref-type="bibr" rid="CR72">72</xref>] identifiers and annotated diseases to MeSH [<xref ref-type="bibr" rid="CR73">73</xref>] identifiers. When the input text comes from sources different than PubMed, we use the MetaMapLite system [<xref ref-type="bibr" rid="CR74">74</xref>], a near real-time EL tool that identifies UMLS [<xref ref-type="bibr" rid="CR36">36</xref>] concepts within the biomedical text. MetaMapLite returns, among other information, the CUI, the preferred term, and the location in the text of the identified UMLS concepts.</p>
        <p id="Par57">The text annotated by EL systems is then passed to RE methods to perform GDA/GCA extraction.</p>
        <p id="Par58"><italic>GDA extraction</italic> The discovery of GDAs is one of the most pressing challenges to advance precision medicine and drug discovery [<xref ref-type="bibr" rid="CR75">75</xref>]. Therefore, the automatic extraction and curation of GDAs is pivotal to advancing precision medicine and providing knowledge to assist disease diagnostics, drug discovery, and therapeutic decision-making. To this end, we use a document-level RE method that adopts Multi-Instance Learning (MIL) to extract GDA assertions from text [<xref ref-type="bibr" rid="CR76">76</xref>, <xref ref-type="bibr" rid="CR77">77</xref>]. Under MIL, text sentences are divided into bags based on pairs of concepts, and the prediction of relations (i.e., predicates) occurs at the bag level. The use of MIL well suits the assertion annotation task, where subject, predicate, and object are not associated with mentions. As the underlying ML model, the RE method exploits Piecewise Convolutional Neural Network (PCNN) model [<xref ref-type="bibr" rid="CR21">21</xref>]. PCNN first encodes sentences using a CNN and then applies a piecewise max pooling operation. This operation divides each sentence into three segments based on the positions of the two given entities and returns the maximum value in each segment instead of a single maximum value over the entire sentence. For MIL, the RE method performs average-based aggregation. This aggregation strategy assumes that all sentences within the same bag contribute equally to the bag-level representation. In other words, the bag representation is the average of all its sentence representations.</p>
        <p id="Par59">We trained the method on the TBGA dataset [<xref ref-type="bibr" rid="CR78">78</xref>], a large-scale, semi-automatically annotated dataset for GDA extraction. TBGA contains over 200,000 instances and 100,000 bags, divided into four GDA types: Therapeutic, Biomarker, Genomic Alterations, and NA. Once trained, the RE method is deployed within <italic>AutoTron</italic>.</p>
        <p id="Par60"><italic>GCA extraction</italic> Cancer prevention is one of the century’s most pressing challenges that public health needs to face. In the last few years, the rise of microarray and next-generation sequencing technologies triggered the generation of large amounts of raw experimental data about gene expression-cancer interactions. These raw data require investigation, processing, and validation by experts to be used to guide diagnosis, assess prognosis, or predict therapy response [<xref ref-type="bibr" rid="CR75">75</xref>, <xref ref-type="bibr" rid="CR79">79</xref>]. The outcomes of experts’ analyses are (often) described in scientific publications in the form of GCAs. However, manual knowledge extraction requires high economic and time costs [<xref ref-type="bibr" rid="CR80">80</xref>–<xref ref-type="bibr" rid="CR82">82</xref>]. Thus, it is of paramount importance to assist manual GCA extraction through the use of automated methods. In this regard, we use a sentence-level RE method that combines the outcomes of different models to obtain the corresponding GCA. Specifically, the method combines three ML models, each of which predicts a specific aspect associated with GCAs. The considered aspects are the Change of Gene Expression (CGE), the Change of Cancer Status (CCS), and the Gene-Cancer Interaction (GCI). Once predicted, the different aspects are combined—following a set of inference rules defined in [<xref ref-type="bibr" rid="CR47">47</xref>, <xref ref-type="bibr" rid="CR83">83</xref>]—to infer the role the given gene has on the specific cancer disease. All the ML models adopt SciBERT [<xref ref-type="bibr" rid="CR84">84</xref>], a pre-trained language model based on BERT [<xref ref-type="bibr" rid="CR85">85</xref>]. SciBERT addresses the lack of high-quality, large-scale labeled scientific data by pretraining on scientific papers from Semantic Scholar [<xref ref-type="bibr" rid="CR86">86</xref>]. On top of it, a linear layer takes SciBERT pooled output. Predictions are scores in [0, 1]; the higher the score for an aspect value, the more the model believes the sentence expresses that particular (aspect) value.</p>
        <p id="Par61">The method has been used to build a large-scale Knowledge Base (KB) on GCAs [<xref ref-type="bibr" rid="CR87">87</xref>, <xref ref-type="bibr" rid="CR88">88</xref>]. In this work, we deploy the method as is within <italic>AutoTron</italic>.</p>
      </sec>
    </sec>
    <sec id="Sec9">
      <title>Collections and customization</title>
      <p id="Par62">
        <fig id="Fig11">
          <label>Fig. 11</label>
          <caption>
            <p>MetaTron collections interface. Overview of the main components of the collection page. The user can search for a specific collection or filter the collections list at the top-right. The <italic>Add collection</italic> button allows the user to create a new collection. The collections list takes up the largest part of the page. Each collection includes information that the collection’s creator can edit</p>
          </caption>
          <graphic xlink:href="12859_2024_5730_Fig11_HTML" id="MO11"/>
        </fig>
      </p>
      <p id="Par63">Collections are sets of documents that one or more users can annotate. The collection web page is illustrated in Fig. <xref rid="Fig11" ref-type="fig">11</xref>, and is accessible through the annotation interface by clicking the <italic>Collections</italic> button.</p>
      <p id="Par64">At the top of the collection page is a text field that allows users to search for a specific collection by its name. Additionally, the four buttons below enable users to filter the collections. The default filter is <italic>All</italic>, which provides users with a list of all the collections they can annotate. <italic>Created</italic> button shows collections created by the user, while <italic>Shared</italic> displays collections that the user can annotate and be created by another team member. Lastly, the <italic>Invited</italic> button shows collections the user has been asked to join as an annotator. A new collection can be created via the <italic>Add Collections</italic> button. Finally, in the remaining part of the page, the user’s collections (either the entire or filtered list) are listed. For each collection, the following information is provided in this order: the creator, the collection’s name, the date of creation, the number of documents, the number of documents annotated by at least one user, and the description; by clicking on <italic>Learn More</italic> button, located below the collection, the information about the annotators of the collection, and the list of labels are loaded. If the user is also the collection’s creator, they can add or remove one or more annotators and add new labels. The <italic>New Round</italic> button allows the user to create a new annotation round. Depending on the annotation task, the users can perform one or more rounds of annotation; hence, they annotate the collection documents several times. The collection’s creator can also decide on the annotators of each round so that different sets of annotators can contribute to different rounds. This option automatically duplicates the annotations of the last round and makes them available in a new collection. The annotators who access this new collection are provided with all the annotations they performed in the last annotations round for that collection. Encapsulating each round on a separate collection of documents and annotations allows each round to be independent of all the other rounds and facilitates the annotators’ work. The <italic>Documents</italic> button redirects the user to the collection’s documents web page containing the list of documents of the collection together with its annotations—a more detailed description of this table is provided in Sect. <xref rid="Sec10" ref-type="sec">2.4</xref>. The <italic>Annotate</italic> button allows the user to annotate that collection: the user will be automatically redirected to the annotation interface and provided with the last annotated document of the collection (if any, otherwise with the first document available). Only the creator is provided with the <italic>Delete</italic> button, which allows for the deletion of the entire collection and the related annotations.<fig id="Fig12"><label>Fig. 12</label><caption><p>MetaTron new collection form. This figure shows the information a user must provide to create a new collection of documents. The form must provide the collection’s name and description, a list of concepts, labels, and documents</p></caption><graphic xlink:href="12859_2024_5730_Fig12_HTML" id="MO12"/></fig></p>
      <p id="Par65">In Fig. <xref rid="Fig12" ref-type="fig">12</xref>, the form to add a new collection is illustrated. A user must provide the collection’s name and description to create a new collection. Then, the user is asked to provide a list of members authorized to annotate the new collection. This is not mandatory since a user may decide to work independently on a collection. The user is asked to add a list of labels necessary to perform label annotation. Also, in this case, adding a list of labels is not mandatory at the moment of collection creation. The creator can edit the collection’s annotators and labels at any moment. Uploading a set of ontological concepts is highly recommended to perform concept linking, relationship annotation, and assertion annotation. Ontological concepts must be uploaded in CSV or JSON files, and for each concept, it is mandatory to provide the URI (or ID), the name, and the type; a description is not mandatory but recommended. Since the files introducing new concepts must follow predefined structures, we provided two downloadable templates (one for the CSV and one for the JSON formats). It is worth noting that the set of provided ontological concepts is not tied to a specific ontology, allowing the user to fully customize the collection’s configuration with concepts belonging to different ontologies, which may also not be publicly available. The insertion of new concepts is always possible at any moment.</p>
      <p id="Par66">A collection must contain at least one document. The creator can upload one or more files in the following formats: JSON, CSV, TXT. The keys in JSON files and the CSVs’ headers will also be available for annotation. <italic>MetaTron</italic> supports uploading PDF files automatically parsed by GROBID.</p>
      <p id="Par67">To retrieve information about articles having a PMID, <italic>MetaTron</italic> relies on the PubMed REST API to obtain the article’s title, abstract, date of publication, authors, and venue. For publications with a DOI, <italic>MetaTron</italic> utilizes the REST APIs of OpenAIRE and Semantic Scholar to extract the same information. The information obtained from the REST APIs is extracted, processed, and presented to the collection users as documents that can be annotated.</p>
      <p id="Par68">By default, <italic>MetaTron</italic> provides the user with the entire document to annotate. However, the user can select specific parts of the document using the <italic>settings</italic> option in the vertical toolbar of the annotation interface.</p>
    </sec>
    <sec id="Sec10">
      <title>Collaborative features</title>
      <p id="Par69">
        <fig id="Fig13">
          <label>Fig. 13</label>
          <caption>
            <p>MetaTron documents collection. Overview of the documents web page related to a collection of interest. Each document is the row of a table; each row contains the information about a document, the annotators, and the total count of annotations performed for each annotation type. For each annotation type, it is possible to see an overview of the related annotations. The last column contains buttons allowing the user to download, visualize and delete the document</p>
          </caption>
          <graphic xlink:href="12859_2024_5730_Fig13_HTML" id="MO13"/>
        </fig>
        <fig id="Fig14">
          <label>Fig. 14</label>
          <caption>
            <p>MetaTron relationships overview modal. The modal shows the subject, predicate, and object elements for each relationship annotated in the document. The mention text and the location in the text are reported; for each concept, the type, name, and URI are reported. Finally, the modal shows the number of annotators and their usernames</p>
          </caption>
          <graphic xlink:href="12859_2024_5730_Fig14_HTML" id="MO14"/>
        </fig>
        <fig id="Fig15">
          <label>Fig. 15</label>
          <caption>
            <p>Linked concepts suggestions. The suggestion modal provides a list of concepts linked to a specific mention by the other annotators of the document. For each concept, the related information is provided; the user can link the suggested concept(s) by clicking the <italic>Accept</italic> button associated with the desired concepts</p>
          </caption>
          <graphic xlink:href="12859_2024_5730_Fig15_HTML" id="MO15"/>
        </fig>
      </p>
      <p id="Par70"><italic>The documents web page. </italic> The user can keep track of the collection’s annotation state via the documents web page, accessible by clicking on the <italic>Documents</italic> button under each collection’s set of information. The documents web page, illustrated in Fig. <xref rid="Fig13" ref-type="fig">13</xref>, contains a dynamic table where, for each document, general information is provided—e.g., the ID, the batch number, and the number of annotations categorized by annotation type. Finally, the last column is the same for all the documents and allows one to visualize the text of the document, download its annotations, and, if the user is the creator of the collection, delete the document and the related annotations. In addition, it is possible to view all the annotations performed for each type, together with the related annotators. An example is provided in Fig. <xref rid="Fig14" ref-type="fig">14</xref>, where the overview of the relationships annotated for a document in the list is shown. Each relationship is subdivided into subject, predicate, and object components. If one of the components is a mention, the related text is reported along with the section in the text where it has been found—e.g., the abstract, the starting, and ending indices (this information is under the <italic>location</italic> field). If one of the components is an ontological concept—unlinked to any mention, instead, it is reported its type, its name (named as <italic>concept</italic>), and its URI (or ID). Finally, the number of annotators and their usernames are reported. This feature allows the user to have a complete overview of the relationship. Furthermore, the information about the annotators is an important indicator of the annotator’s expertise and the reliability of the annotation.</p>
      <p id="Par71"><italic>Load the teammates’ annotations</italic> The user can load a particular teammate’s annotation for a document directly from the vertical toolbar (C in Fig. <xref rid="Fig3" ref-type="fig">3</xref>). Once loaded, the user can copy one or more annotations from the teammate, resulting in both the user and the teammate having the same (or partial) set of annotations.</p>
      <p id="Par72">This feature has been implemented to allow the user to visualize and interact with other members’ annotations. The possibility to copy other members’ annotations facilitates and speeds up the annotation process as the user does not have to create new annotations from scratch.</p>
      <p id="Par73"><italic>Receiving suggestions</italic> By accessing the <italic>Suggestion</italic> option in the panel of a mention, it is possible to visualize the list of ontological concepts that the other annotators linked to that mention. Figure <xref rid="Fig15" ref-type="fig">15</xref> shows an example of a suggestion modal. The concept type, name, URI (or ID), and the number of annotators are provided for each concept. The <italic>Accept</italic> button under placed a concept allows the user to assign that ontological concept to the mention; the <italic>Close</italic> button discards the suggestion. <italic>MetaTron</italic> allows users to link more than one suggested ontological concept to the same mention.</p>
    </sec>
    <sec id="Sec11">
      <title>Inter annotator agreement (IAA) and statistics</title>
      <p id="Par74"><italic>Inter annotator agreement (IAA)</italic><italic>MetaTron</italic> implements two IAA methods: majority voting, Fleiss’ kappa and Cohen’s kappa. The first method selects all annotated annotations by more than half of the document’s annotators. In <italic>MetaTron</italic>, viewing and editing the annotations selected through majority voting is possible. The majority voting-based annotations have two goals: (i) providing the user with information about the most frequent annotations, and (ii) facilitate and speed up the annotation process. The user can copy all the annotations selected via majority voting and edit them if needed; this allows the annotator to receive an initial set of annotations, consequently saving time. It is possible to load these annotations directly from the vertical toolbar (C in Fig. <xref rid="Fig3" ref-type="fig">3</xref>): they can be loaded by clicking on the user called <italic>IAA - Inter Annotator Agreement</italic>.</p>
      <p id="Par75">Fleiss’ kappa is a statistical measure that assesses the level of agreement between two or more annotators [<xref ref-type="bibr" rid="CR89">89</xref>]. In <italic>MetaTron</italic>, two Fleiss’ kappa agreement values are computed for each annotation type: one concerns the entire collection of documents, and one concerns each single document. It is possible to check Fleiss’ kappa agreement values on the <italic>Statistics</italic> web page, whose details are provided in the paragraph below.</p>
      <p id="Par76">Cohen’s kappa is a statistical measure ranging between <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-1$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2024_5730_Article_IEq1.gif"/></alternatives></inline-formula> and 1 that assesses the level of agreement between two raters rating the same set of elements [<xref ref-type="bibr" rid="CR90">90</xref>]. Similarly to Fleiss’ kappa, we provide the Cohen’s kappa for each annotation type; it is computed for the entire collection or each document, according to the users’ needs.</p>
      <p id="Par77">It is always possible to introduce new agreement functions according to the users’ needs and requirements: <italic>MetaTron</italic> is indeed flexible, and its modular design allows for integrating new functions and measures.</p>
      <p id="Par78">Having Fleiss’ kappa and Cohen’s kappa values provides a more complete view of annotation quality. Fleiss’ kappa can show how agreement varies with a different number of annotators, while Cohen’s kappa offers a more specific assessment between pairs of annotators. Using both coefficients ensures a more accurate evaluation of annotation reliability, considering the variety of annotators involved and the specific agreement between pairs of annotators.<fig id="Fig16"><label>Fig. 16</label><caption><p>Statistics tool bar. The statistics tool bar allows to select different types of statistics. In 1 and 2 it is possible to switch between personal and global statistics. In 3 it is possible to select two annotators and get the Cohen’s kappa agreement between them; in 4 the agreement amongo multiple annotation rounds is provided; in 5 it is possible to select one document and check its statistics</p></caption><graphic xlink:href="12859_2024_5730_Fig16_HTML" id="MO16"/></fig><fig id="Fig17"><label>Fig. 17</label><caption><p>Cohen’s kappa agreement. The Cohen’s kappa agreement is provided for a pair of annotators selected by the user. The user can decide to compute the Cohen’s kappa basing on a single document or on the entire collection</p></caption><graphic xlink:href="12859_2024_5730_Fig17_HTML" id="MO17"/></fig><fig id="Fig18"><label>Fig. 18</label><caption><p>Fleiss’ kappa rounds agreement. This modal provides a table where, for each round, it is provided the Fleiss’ kappa for each type. The user can select the document or, alternatively, can check the agreement computed on the entire collection</p></caption><graphic xlink:href="12859_2024_5730_Fig18_HTML" id="MO18"/></fig><fig id="Fig19"><label>Fig. 19</label><caption><p>General statistics. The table at the left shows each annotation type, the total count of annotations, and the inter-annotator agreement. The second table concerns the concepts linking annotation type and for each concept type, it is possible to see how many concepts have been linked and the related name. Finally, the last table shows, for each concept type, the number of concepts (both unlinked and linked to a mention) taking part in a relationship (or an assertion) and whose role is subject, predicate, and object, respectively</p></caption><graphic xlink:href="12859_2024_5730_Fig19_HTML" id="MO19"/></fig><fig id="Fig20"><label>Fig. 20</label><caption><p>Annotations and annotators statistics. The first histogram illustrates for each document of the collection, how many annotations have been performed for each annotation type. The second instead, shows the number of annotators for each document</p></caption><graphic xlink:href="12859_2024_5730_Fig20_HTML" id="MO20"/></fig><fig id="Fig21"><label>Fig. 21</label><caption><p>Concept types distribution. The <italic>Global</italic> pie chart illustrates the concepts type distribution of the concepts assigned in all the annotation types. The other pie charts illustrate the concept types distributions of the concepts (unlinked and linked to the mentions) taking part in relationships and assertions and whose role was subject, predicate, and object, respectively</p></caption><graphic xlink:href="12859_2024_5730_Fig21_HTML" id="MO21"/></fig></p>
      <p id="Par79"><italic>Collections’ statistics</italic> Detailed statistics are provided on the statistics web page. <italic>MetaTron</italic> provides two types of statistics: (i) <italic>personal</italic> statistics concern the single user, and (ii) <italic>global</italic> statistics concern the entire set of annotators of the collection. The two buttons at the top of the page allow the user to switch between personal ((1) in Fig. <xref rid="Fig16" ref-type="fig">16</xref>) and global ((2) in Fig. <xref rid="Fig16" ref-type="fig">16</xref>) statistics. (3) in Fig. <xref rid="Fig16" ref-type="fig">16</xref> allows the user to be provided with the Cohen’s kappa agreement between two users they selected and the agreement is computed for each annotation type. An example is provided in Fig. <xref rid="Fig17" ref-type="fig">17</xref>. (4) in Fig. <xref rid="Fig16" ref-type="fig">16</xref> allows the user to be provided with the Fleiss’ kappa agreement for each annotation type on each round of annotation. An example is provided in Fig. <xref rid="Fig18" ref-type="fig">18</xref>: for each round it is possible to see how the agreement evolves. The text area (5) in Fig. <xref rid="Fig16" ref-type="fig">16</xref>) allows the user to select a document to check the statistics of—by default, the statistics concern the entire set of collection documents An example of global statistics is illustrated in Figs. <xref rid="Fig19" ref-type="fig">19</xref>, <xref rid="Fig20" ref-type="fig">20</xref>, <xref rid="Fig21" ref-type="fig">21</xref>. <italic>General</italic> statistics, (A) in Fig. <xref rid="Fig19" ref-type="fig">19</xref> include the number of annotated documents, the number of annotators, the number of annotations for each annotation type, and the inter-annotator agreement computed basing on the Fleiss’ kappa measure. The <italic>Linked concepts overview</italic> section (B) displays the count of how many times an ontological concept has been linked to a mention. The list is subdivided into concept types. Additionally, in section (<bold>C</bold>), there are three lists: subject, predicate, and object lists. Similarly to the previous case, each list contains the concepts, grouped by concept type, that were annotated as a subject, predicate, or object, respectively. The <italic>Documents annotations overview</italic> section ((D) in Fig. <xref rid="Fig20" ref-type="fig">20</xref>) provides for each document of the collection the total number of annotations for each annotation type. In <italic>Annotators per document count</italic> ((E) in Fig. <xref rid="Fig20" ref-type="fig">20</xref>) the number of annotators is provided for each document. Finally, the pie chart in (F) in Fig. <xref rid="Fig21" ref-type="fig">21</xref> provides a global overview of the distribution of the concept types annotated in concepts linking, relationships, and assertions annotations. The three pie charts in (G) instead exclusively concern relationships and assertions and provide an overview of the concept types assigned to subjects, predicates, and objects, respectively. <italic>Personal</italic> statistics show the same set of statistics, except for the <italic>Annotators overview</italic>—which is not considered since personal statistics exclusively concern a single user. If a document is selected instead, the <italic>Documents annotations overview</italic> and the <italic>Annotators overview</italic> are not shown since these statistics concern the entire collection of documents. It is worth noting that it is always possible to access the statistics of the documents a user annotates via the <italic>statistics</italic> button in the vertical toolbar, which shows both the personal and the global statistics.</p>
    </sec>
  </sec>
  <sec id="Sec12">
    <title>Results</title>
    <p id="Par80">This section compares a subset of the annotation tools illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. The online tools selected for comparison are <italic>TeamTat</italic>, <italic>MedTAG</italic>, <italic>LightTag</italic>, and <italic>MetaTron</italic>. The offline tools instead are <italic>MetaTron (dockerized)</italic>, <italic>INCEpTION</italic>, and <italic>brat</italic>. While <italic>TeamTat</italic> and <italic>MetaTron</italic> target the biomedical domain, the other annotation tools are general purpose.</p>
    <p id="Par81">We provide a <italic>qualitative comparison</italic> where we outline the core functionalities of each tool. Furthermore, we provide a <italic>quantitative analysis</italic> by conducting experiments to assess the performance of each tool in tasks including mentions annotations and concepts linking, and relationships annotation. In the quantitative analysis, we did not consider <italic>MedTAG</italic>—as it does not support relationship annotation, and <italic>brat</italic>—as it has not been possible to automatically test it with a web agent.</p>
    <p id="Par82">We planned to include in our comparison also <italic>tagtog</italic> [<xref ref-type="bibr" rid="CR50">50</xref>], however, as of May 2023, the online version of <italic>tagtog</italic> did not allow us to add new documents to a collection; as a consequence, it has been impossible to qualitatively and quantitatively assess its performances. Furthermore, <italic>LightTag</italic> does not support entity linking, hence we cannot link a concept to a mention, we can only tag the mention with a concept type. However, we included this tool in the quantitative comparison as it is one of the newest tools of the past years and allows for relationships annotation.</p>
    <p id="Par83">In the last section, we describe a user study conducted on two tasks, namely GCA and GDA. GCA focuses on the annotation of relationships where subject and object are mentions and the predicate is an ontological concept. GDA focuses on the annotation of assertions. The user study involved 10 PubMed abstracts per task annotated by three experts in the biomedical domain. All the users were initially provided with the automatic annotations performed by AutoTron. We analyzed the results <italic>quantitatively</italic>, measuring the agreement among the annotators, and <italic>qualitatively</italic> via a questionnaire involving the annotators’ experience. Finally, we studied how AutoTron impacted on the annotations analyzing how many annotations generated automatically have been updated, removed, added or confirmed.</p>
    <sec id="Sec13">
      <title>Qualitative analysis</title>
      <p id="Par84">Figure <xref rid="Fig1" ref-type="fig">1</xref> illustrates an overview of the features characterizing a set of annotation tools. In the qualitative analysis we compare: <italic>MetaTron</italic>, <italic>MedTAG</italic>, <italic>TeamTat</italic>, <italic>brat</italic>, <italic>LightTag</italic>, and <italic>INCEpTION</italic>. The qualitative comparison is based on our direct experience with all these tools.</p>
      <p id="Par85"><italic>MetaTron</italic> is the unique tool that fully satisfies 23 of 24 criteria—active learning, and built-in prediction is only partially satisfied; specifically, it is the unique tool that satisfies the connection to ORCID (for login purposes) (F14) external libraries integration (F15). We see that <italic>TeamTat</italic>, and <italic>INCEpTION</italic> are the most complete tools: they fully satisfy 17 and 19 criteria, respectively, 5 criteria and 1 criterion, respectively, are only partially satisfied, and the remaining are not satisfied at all. Conversely, <italic>LightTag</italic>, is the least complete: 13 criteria are fully satisfied, 3 are partially satisfied, and 8 are not. Among the tools we compared <italic>MetaTron</italic> with, <italic>LightTag</italic> is the unique one without source code available (T2), does not show the date of the last version (or commit) (T1), does not allow for modification and redistribution (T6), is not free of charge (T7), and does not support ontologies (F4). <italic>brat</italic>, instead, is the unique tool that does not support document-level annotation; hence, it is impossible to associate one or more classes to a document. Finally, only <italic>TeamTat</italic> fully supports active learning (F5), and only <italic>TeamTat</italic> and <italic>MedTAG</italic> support the annotation of PubMed abstracts. Online availability (T3) is satisfied by <italic>TeamTat</italic> and <italic>LightTag</italic>; <italic>INCEpTION</italic> and <italic>brat</italic> are available offline—the online demo is available for <italic>INCEpTION</italic>, and <italic>MetaTron</italic> and <italic>MedTAG</italic> are available online and can be locally installed. Online tools are usually easier and faster to configure than locally installable ones: online tools usually require the user to upload a set of documents in predefined formats and a schema for the annotation, which usually includes the definition of the labels to classify the documents, or the definition of the entity types to associate. However, online tools might suffer from network delays that may occur when a large amount of data is uploaded/downloaded. Offline tools guarantee isolation and preserve data privacy; simultaneously, their installation might be difficult, and the configuration is time-demanding for someone with little technical expertise.</p>
      <p id="Par86">As of our experience with the compared tools, the offline tools—<italic>brat</italic> and <italic>INCEpTION</italic>, were the least intuitive and required the most time to be configured. In particular, <italic>INCEpTION</italic> required a deep study of the documentation as the notion of annotation layers that characterize the tool is not intuitive. On the other hand, <italic>TeamTat</italic> has the fastest and easiest configuration: it requires the definition of a collection and the upload of a set of documents. <italic>LightTag</italic>, together with the set of documents to annotate, required the user to define one or more annotation schemas and relation schemas: the former allows to define the tags to associate to the entities, and the classes to classify the documents with, while the latter allows user to specify the relation types—i.e., the labels to be assigned to the edge between two entities of the relationship. <italic>MedTAG</italic> instead requires uploading a set of documents, labels for the document-level annotation, and a set of ontological concepts.</p>
      <p id="Par87">The compared tools report substantial differences in the supported document formats: <italic>MetaTron</italic>, <italic>INCEpTION</italic>, and <italic>TeamTat</italic> are the unique tools supporting the upload of PDFs and TXT files. <italic>MetaTron</italic>, <italic>MedTAG</italic>, <italic>LigthTag</italic> support JSON and CSV. <italic>TeamTat</italic>, <italic>MetaTron</italic>, and <italic>MedTAG</italic> allow the user to upload PubMed abstract. Only <italic>MetaTron</italic> allows the user to specify a DOI and annotate the related abstract extracted from Semantic Scholar or OpenAIRE. In this respect <italic>MetaTron</italic> is the only tool that supports all the aforementioned formats and is integrated with three different APIs to abstracts upload.</p>
      <p id="Par88">We analyzed the annotation procedure for what concerns: labels annotation, mentions annotation, concepts linking, and relationship annotation. All the analyzed tools implement mention annotation via drag and drop, except for <italic>MedTAG</italic> where mentions are selected by clicking on each token composing the mention. <italic>LightTag</italic> is the only tool that allows users to perform entity tagging and does not support entity linking. <italic>MetaTron</italic> is the unique tool providing three modalities to select mentions, enhancing the annotation experience. In all the other tools, the concepts linked to a mention are always selected, specifying the related type and URI or name. Also, label annotation is similar in all the examined tools and can be achieved by clicking on the labels to be associated with the document. Relationships annotation may vary depending on the examined tool: <italic>LightTag</italic> and <italic>TeamTat</italic> for example, support n-ary relationships; in <italic>MetaTron</italic> instead, a relationship always has three components, and only one of them must be a mention annotated in the document. In <italic>INCEpTION</italic> and <italic>brat</italic>, instead, the source and the target in the relationships must be mentions. <italic>MetaTron</italic> is the most versatile tool among those described, as in a relationship, subject and object are not required to be mentions in the text. Additionally, <italic>MetaTron</italic> is the only tool to propose assertions annotation, not necessarily tied to sentences in the text. In this respect, the availability of document-level annotations is a relevant feature for <italic>MetaTron</italic> as, according to [<xref ref-type="bibr" rid="CR10">10</xref>], the most adopted biomedical annotation tool does not implement this feature.</p>
      <p id="Par89">We compared <italic>MetaTron</italic> and the tools in terms of collaborative features and agreement; <italic>MetaTron, MedTAG, TeamTat, INCEpTION and LightTag</italic> support the collaboration between multiple annotators. Specifically, <italic>TeamTat</italic> supports multiple rounds of annotation and provides the annotators with agreement and disagreement between the annotators, as well as disagreement resolution. <italic>LightTag</italic> implements task management features, assigning tasks to different groups of annotators based on specific needs—e.g., language, and allowing project managers to keep track of annotations and agreement. <italic>tagtog</italic> implements user roles and allows for the definition of a set of custom annotation guidelines. In this respect, <italic>MetaTron</italic> implements different annotation rounds and provides some additional collaborative features to facilitate and speed up the annotators’ work. Specifically, it allows annotators to copy other members’ annotations, and the annotation with the highest agreement is computed via majority voting. In addition, <italic>MetaTron</italic> implements annotation suggestions: given a mention, users can see what the concepts assigned by the other annotators are and select one of them accordingly, depending, for example, on how many users have linked a specific ontological concept. For each annotation performed, the user can keep track of the number of users who performed the same annotation and change it accordingly. In <italic>MetaTron</italic>, the collection’s creator can keep track of the annotation progress and is responsible for selecting the annotators of each round. All the users can see the entire sets of annotations of each collection document and the related annotators. <italic>MetaTron</italic> is the unique tool providing different agreement measures (it implements both Fleiss’ kappa and Cohen’s kappa) computed on the entire collection or on single documents.</p>
      <p id="Par90"><italic>brat</italic> and <italic>INCEpTION</italic> are included in our comparison even if they do not target the biomedical domain.</p>
    </sec>
    <sec id="Sec14">
      <title>Quantitative analysis</title>
      <p id="Par91">To compare the performance of the selected manual annotation tools, we conducted a series of experiments on two different tasks: concepts linking, and relationships annotation. We did not treat mention annotation as a separate task because the annotation method was the same across all the annotation tools. Our experiments concerned the time elapsed and the number of clicks required to annotate a collection of the same 15 documents.</p>
      <p id="Par92">To evaluate the performances of the selected tools we relied on Selenium,<xref ref-type="fn" rid="Fn4">4</xref> an open-source testing framework used to automate web browsers. We designed four web agents, one for each annotation tool—the same web agent has been used for the two instances of <italic>MetaTron</italic>, and we used them to simulate the concepts liking and relationships annotations task on a collection of 15 abstracts extracted from PubMed. In order to simulate the annotator activity, we selected abstracts of various lengths; the mentions, the linked concepts, and the relationships have been extracted using <italic>AutoTron</italic>. Overall, <italic>AutoTron</italic> extracted 94 mentions and 71 relationships; specifically, for each abstract, a minimum of 2 mentions and 1 relationship, and a maximum of 13 mentions and 9 relationships were found. Each mention has been linked to exactly one concept. Variable delays were introduced based on the examined tool to prevent errors caused by server overload and to allow sufficient time for request processing. To provide a fair analysis, we treated online tools and offline tools separately, as the performance of online tools is influenced by the server hosting the application, while offline tools rely on the individual machine running the test.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Overview of the time spent to perform mentions annotation (MA), concepts linking (CL), and relationship annotation (RA) on a set of 15 documents. The reported value of average (AVG), standard deviation (STD), median (MED), and 5<italic>th</italic> and 95<italic>th</italic> percentiles refer to the time spent annotating 15 documents 50 times</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left"/><th align="left" colspan="5">MA + CL + RE</th></tr><tr><th align="left"/><th align="left"/><th align="left">AVG</th><th align="left">STD</th><th align="left">5th</th><th align="left">MED</th><th align="left">95th</th></tr></thead><tbody><tr><td align="left" rowspan="3">Online</td><td align="left">MetaTron</td><td align="left"><bold>638</bold>.<bold>73</bold></td><td align="left">1.67</td><td align="left">636.97</td><td align="left"><bold>638</bold>.<bold>22</bold></td><td align="left">642.49</td></tr><tr><td align="left">TeamTat</td><td align="left">842.93</td><td align="left">0.76</td><td align="left">841.67</td><td align="left">843.03</td><td align="left">844.18</td></tr><tr><td align="left">LightTag</td><td align="left">661.13</td><td align="left">1.23</td><td align="left">659.35</td><td align="left">660.85</td><td align="left">663.07</td></tr><tr><td align="left" rowspan="2">Offline</td><td align="left">MetaTron</td><td align="left"><bold>642</bold>.<bold>39</bold></td><td align="left">1.30</td><td align="left">640.35</td><td align="left"><bold>642</bold>.<bold>28</bold></td><td align="left">644.54</td></tr><tr><td align="left">INCEpTION</td><td align="left">704.85</td><td align="left">2.14</td><td align="left">701.53</td><td align="left">705.12</td><td align="left">707.33</td></tr></tbody></table><table-wrap-foot><p>The boldface values represent avg and median results of the tools with the best performances, i.e., the lowest time taken to annotate 15 documents 50 times</p></table-wrap-foot></table-wrap></p>
      <p id="Par93">We conducted an analysis concerning the time spent annotating the entire collection. The results are shown in Tables <xref rid="Tab1" ref-type="table">1</xref> and <xref rid="Tab2" ref-type="table">2</xref>. We compared the annotation tools on the average (AVG), the median (MED), standard deviation (STD), the 5<italic>th</italic> and the 95<italic>th</italic> percentiles of the time required to annotate the entire collection 50 times. We studied the time taken to perform two tasks: (i) performing an entire annotation, which comprehends mentions annotation (MA), concepts linking (CL), and relationships annotation (RA); and (ii) annotating the mentions (MA) and linking the concepts (CL). We remark that <italic>LightTag</italic> does not support concept linking, it allows only to tag the mentions with concept types.</p>
      <p id="Par94">Looking at the results achieved by the online tools in the entire annotation (Table <xref rid="Tab1" ref-type="table">1</xref>), we see that <italic>MetaTron</italic> is the most efficient tool in terms of average time, as <italic>MetaTron</italic> required 638.73 s on average (while <italic>TeamTat</italic> requires 842.93 s). <italic>LightTag</italic>’s performance falls between <italic>TeamTat</italic> and <italic>MetaTron</italic>. However, <italic>TeamTat</italic> does not reveal significant fluctuations during the 50 annotation rounds; this means that it is a tool with good stability that performs well during long annotation sessions. The substantial difference between the performance of <italic>TeamTat</italic> and those of <italic>MetaTron</italic> should be attributed to how the annotation is performed: we noticed <italic>TeamTat</italic> needed longer delays to save each annotation correctly. These aspects might not be visible to a human annotator who performs slower than the web agent; as such, a human annotator will take more time than an automatic one to annotate the selected set of documents. In the comparison involving the offline tools, the dockerized instance of <italic>MetaTron</italic> achieved better results than <italic>INCEpTION</italic>, as <italic>MetaTron</italic> required 642.39 s on average and <italic>INCEpTION</italic> 704.85.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Overview of the time spent to perform mentions annotation (MA) and concepts linking (CL) on a set of 15 documents. The reported value of average (AVG), standard deviation (STD), median (MED), and 5<italic>th</italic> and 95<italic>th</italic> percentiles refer to the time spent annotating 15 documents 50 times</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left"/><th align="left" colspan="5">MA + CL</th></tr><tr><th align="left"/><th align="left"/><th align="left">AVG</th><th align="left">STD</th><th align="left">5th</th><th align="left">MED</th><th align="left">95th</th></tr></thead><tbody><tr><td align="left" rowspan="3">Online</td><td align="left">MetaTron</td><td align="left"><bold>329</bold>.<bold>25</bold></td><td align="left">1.53</td><td align="left">327.13</td><td align="left"><bold>328</bold>.<bold>96</bold></td><td align="left">331.93</td></tr><tr><td align="left">TeamTat</td><td align="left">446.33</td><td align="left">1.34</td><td align="left">444.28</td><td align="left">446.53</td><td align="left">448.53</td></tr><tr><td align="left">LightTag</td><td align="left">412.61</td><td align="left">1.92</td><td align="left">409.48</td><td align="left">412.68</td><td align="left">415.65</td></tr><tr><td align="left" rowspan="2">Offline</td><td align="left">MetaTron</td><td align="left">330.35</td><td align="left">1.37</td><td align="left">327.98</td><td align="left">330.28</td><td align="left">331.96</td></tr><tr><td align="left">INCEpTION</td><td align="left"><bold>326</bold>.<bold>85</bold></td><td align="left">1.91</td><td align="left">323.60</td><td align="left"><bold>327</bold>.<bold>00</bold></td><td align="left">329.92</td></tr></tbody></table><table-wrap-foot><p>The boldface values represent avg and median results of the tools with the best performances, i.e., the lowest time taken to annotate 15 documents 50 times</p></table-wrap-foot></table-wrap></p>
      <p id="Par95">For what concerns the second task, mentions annotation and concepts linking, whose results are shown in Table <xref rid="Tab2" ref-type="table">2</xref>, similarly to the previous case, the best online tool is <italic>MetaTron</italic>, which required 329.25 s on average while the one that requires the highest average time is <italic>TeamTat</italic>, that required 446.33 s. Also in this case the performances of <italic>LightTag</italic>, 412.61 s, fall between those of <italic>MetaTron</italic> and <italic>TeamTat</italic>. <italic>INCEpTION</italic> and <italic>MetaTron (offline)</italic> achieved comparable performances. In both the analyzed task, all the tools had a standard deviation lower than 2.5. The 5<italic>th</italic> and 95<italic>th</italic> percentiles indicate that in all the examined tools all the computed times are uniformly distributed around the median.</p>
      <p id="Par96">The online and offline instances of <italic>MetaTron</italic> achieved similar performances in both tasks. This can be attributed to the absence of any differences between the code running locally and the code of the online instance. Furthermore, the server hosting <italic>MetaTron</italic> was underutilized when we ran the automatic agents, leading to performances comparable to the docker-based instance. It is notable the case of the <italic>INCEpTION</italic>: it achieved the worst performances in the first task (MA + CL + RE), and the best in the second one (MA + CL). This aspect points out that the annotation of relationships is the most expensive in terms of time, while <italic>INCEpTION</italic> is the most efficient tool in mentions annotation and linking. <italic>MetaTron</italic> and <italic>TeamTat</italic> present similar behaviors: the average time taken in the second task is half the time taken in the first: this indicates that annotating the relationships takes the same amount of time required to annotate the mentions and link them to the concepts. Conversely, in <italic>LightTag</italic>, mentions annotation and linking require more than half of the time: this is partially related to the implementation of the web agent in the relationship part, and partially depends on how mentions selection and linking have been implemented in the tool.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Overview of the number of clicks required to perform the annotation of 15 documents in the two selected tasks: mentions annotation and concepts linking (MA + CL) and an entire annotation—mentions annotation, concepts linking, relationship annotation (MA + CL + RA)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">MA + CL</th><th align="left">MA + CL + RA</th></tr></thead><tbody><tr><td align="left">MetaTron</td><td align="left">485</td><td char="." align="char">1028</td></tr><tr><td align="left">TeamTat</td><td align="left">500</td><td char="." align="char">1423</td></tr><tr><td align="left">LightTag</td><td align="left">327</td><td char="." align="char">611</td></tr><tr><td align="left">INCePTION</td><td align="left">391</td><td char="." align="char">746</td></tr></tbody></table></table-wrap></p>
      <p id="Par97">We analyzed the number of clicks performed to annotate the collection. The results include the count of clicks required to annotate every single document of the collection and the clicks necessary to change the document. The results are reported in Table <xref rid="Tab3" ref-type="table">3</xref>. We see that <italic>MetaTron</italic> and <italic>TeamTat</italic> are the tools that require the highest number of clicks to annotate the collection of 15 documents. This aspect is motivated by how the tools implement relationship annotation. In both the aforementioned tools, to annotate a new relationship it is required to set the two mentions and select the predicate and the associated ontological concept: all these actions makes the total number of clicks increase. <italic>LightTag</italic> instead, is the most efficient, however, it does not support entity linking, and this aspect motivates the lower number of clicks with respect to the other tools, since only the concept type—i.e., gene or disease, is required. The annotation of relationships in <italic>LightTag</italic> is the fastest compared to the other tools; the first reason is that <italic>LightTag</italic> requires the user to provide a schema configuration for relationship annotation and this allows the user to save clicks and time; in addition, in <italic>LightTag</italic> each mention/concept composing the relationship is selected by dragging and dropping it into a box hosting the relationship components, and this allows the user to save clicks—a drag and drop action is performed in a single click. <italic>INCEpTION</italic> is most efficient after <italic>LightTag</italic>, and requires a half of the clicks compared to <italic>TeamTat</italic>.</p>
    </sec>
    <sec id="Sec15">
      <title>Discussion</title>
      <p id="Par98">The qualitative and quantitative analyses conducted to assess the performances of <italic>MetaTron</italic>, <italic>MedTAG</italic>, <italic>TeamTat</italic>, <italic>LightTag</italic>, <italic>brat</italic>, and <italic>INCEpTION</italic> allowed us to draw some conclusion.</p>
      <p id="Par99">The results deriving from the analyses showed that <italic>MetaTron</italic> emerges as a competitive annotation tool in the biomedical domain, as it is the only one that fulfills all the analyzed features and achieves the best results in terms of time spent in the annotation of a set of documents. <italic>MetaTron</italic> provides an environment where one or more annotators can collaborate in annotating documents in five different ways, both at the document level and mention level and can leverage automatic annotations to expedite the annotation process. Additionally, <italic>MetaTron</italic> is open-source, free of charge, and supports a wide range of input and output formats. The tool is released as an online and offline instance, making <italic>MetaTron</italic> a versatile tool that can be adapted to different needs and use cases. The online instance of <italic>MetaTron</italic> is valuable to test its features, take advantage of <italic>AutoTron</italic>’s automatic annotations, and collaboratively annotate PubMed, Semantic Scholar, and OpenAIRE abstracts; the offline release guarantees data privacy and allows users to locally deploy <italic>MetaTron</italic> and share the tool with a controlled number of users. In addition, the offline tool is useful when dealing with large volumes of data that would require a significant amount of time for uploading.</p>
    </sec>
    <sec id="Sec16">
      <title>User study</title>
      <p id="Par100">The user study consisted of sentence-level tasks for Gene-Cancer Associations (GCA) and document-level tasks for Gene-Disease Associations (GDA). The GCA task required annotating relationships where the subject and object, representing gene and cancer mentions, respectively, are involved. The predicate corresponds to one of the following ontological concepts: (i) biomarker, indicating whether the gene associated with the disease is altered in conjunction with the disease; (ii) tumor suppressor, indicating whether the gene plays a role in preventing the disease; and (iii) oncogene, indicating whether the gene promotes the progression of the disease.</p>
      <p id="Par101">The GDA task encompassed annotating assertions where the subject, predicate, and object are ontological concepts unrelated to specific textual mentions. The subject and object represent gene and disease, respectively. At the same time, the predicate is categorized into one of the following concepts: (i) biomarker, (ii) genomic alteration, indicating a connection between a genomic alteration and the gene associated with the disease phenotype, and (iii) therapeutic, signifying the gene’s therapeutic role in ameliorating the disease.</p>
      <p id="Par102">We chose ten pertinent PubMed abstracts for each task, which three experts in the biomedical domain annotated. To streamline and expedite the annotation process for the annotators, we furnished them with automatic annotations generated by running <italic>AutoTron</italic> on each document.</p>
      <p id="Par103">The annotators performed two annotation rounds for each task. In round 1, the annotators had to annotate each document relying exclusively on the auxiliary information provided by the automatic annotations. The users were allowed to add new mentions, link concepts to them, and add new relationships. However, they could not rely on collaborative features to annotate the documents—i.e., the annotators could not check other members’ annotations or documents’ statistics. In round 2, instead, the annotators were asked to rely on the collaborative features of <italic>MetaTron</italic> to update their annotations: as a consequence, they had access to other members’ annotations and the annotation obtained via majority voting. At the end of the annotation rounds, we provided all the annotators with a questionnaire with 15 questions about their annotation experience.</p>
      <p id="Par104">In the following sections, we analyze how the annotations change after each round and how the collaborative features impact the agreement among annotators, we summarize the answers to the questionnaire, and finally, we study the quality of the annotations of <italic>AutoTron</italic> as a means to speed up and facilitate annotators’ work.</p>
      <p id="Par105"><italic>Quantitative results</italic> In Table <xref rid="Tab4" ref-type="table">4</xref>, we report the Fleiss’ kappa agreement among the annotators for each task after each round. Specifically, we provide the agreement computed on concepts and relationship annotations for the GCA task. Instead, we provide the agreement computed on assertions annotation for the GDA task. Our goal is to investigate the extent to which the presence of annotations from other annotators impacts the work of the annotators. The highest agreement has been achieved by concept annotations in all the rounds. At the end of round 1, the agreement obtained in relationships (GCA) and assertions (GDA) annotation is negative, indicating that there is no agreement among the annotators. At the end of round 2, the agreement on concept annotations did not change from round 1, while the agreement on relationships increased from <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-$$\end{document}</tex-math><mml:math id="M4"><mml:mo>-</mml:mo></mml:math><inline-graphic xlink:href="12859_2024_5730_Article_IEq2.gif"/></alternatives></inline-formula>0.2179 to <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-$$\end{document}</tex-math><mml:math id="M6"><mml:mo>-</mml:mo></mml:math><inline-graphic xlink:href="12859_2024_5730_Article_IEq3.gif"/></alternatives></inline-formula>0.0872 and on assertions from <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-$$\end{document}</tex-math><mml:math id="M8"><mml:mo>-</mml:mo></mml:math><inline-graphic xlink:href="12859_2024_5730_Article_IEq4.gif"/></alternatives></inline-formula>0.0364 to 0.2490. These results confirmed that the collaborative features provided by <italic>MetaTron</italic> play a key role in improving the results of the annotation process.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Overview of Fleiss’s kappa agreement at the end of each round. The agreement has been computed for concepts, relationships, and assertion annotations for each task’s entire set of documents</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left" colspan="2">GCA</th><th align="left">GDA</th></tr><tr><th align="left"/><th align="left">Concepts annotation</th><th align="left">Relationships annotation</th><th align="left">Assertions annotation</th></tr></thead><tbody><tr><td align="left">Round 1</td><td align="left">0.3312</td><td align="left">− 0.2179</td><td char="." align="char">− 0.0364</td></tr><tr><td align="left">Round 2</td><td align="left">0.3312</td><td align="left">− 0.0872</td><td char="." align="char">0.2490</td></tr></tbody></table></table-wrap></p>
      <p id="Par106"><italic>Qualitative results</italic> At the end of round 2, each annotator compiled a questionnaire concerning their experience using <italic>MetaTron</italic>. The questionnaire consisted of 15 questions about annotation experience, GCA and GDA tasks, and collaborative experience, with responses ranging from 1 to 5.</p>
      <p id="Par107">About the annotation experience, the annotators identified the annotation of mentions, concepts, relationships, and assertions as a straightforward process (all the questions about the complexity of annotations received scores equal to 1 and 2). Only one annotator needed to contact the developers to clarify how to annotate. All the annotators agreed that the automatic annotations generated via <italic>AutoTron</italic> are a useful starting point and contributed to speeding up and facilitating the annotation process. All the annotators considered <italic>MetaTron</italic> had a positive annotation experience and will use <italic>MetaTron</italic> in the future.</p>
      <p id="Par108">Considering task complexity, all the annotators found the GCA task more complex than the GDA one. Two over three annotators assigned a score equal to 3 in the complexity of the GCA task, while only one assigned 4. In the GDA task, instead, the annotators assigned 2, 3, and 4.</p>
      <p id="Par109">One relevant aspect is the annotators’ collaboration. All the annotators found <italic>MetaTron</italic> effectively supports collaboration among annotators. Specifically, all the annotators agreed that the possibility of copying one or more annotations from another annotator is an important feature that speeds up the annotation process. One annotator found useful the annotation generated via majority voting (score equal to 4), while the other two annotators assigned a score equal to 3 hence this annotation did not play a key role in their annotations. Finally, two out of three annotators admitted that those of the other annotators did not significantly influence their annotations. The remaining annotator, however, made numerous changes to the performed annotations in round 2. Two out of three annotators found it necessary to discuss their annotations and determine which relationship’s predicate to apply. This highlights the difficulty of the proposed tasks.</p>
      <p id="Par110">The annotators were required to point out the most useful features of <italic>MetaTron</italic> according to their experience. In Fig. <xref rid="Fig22" ref-type="fig">22</xref> we provide the results of this analysis. According to our results, three features have not been selected, specifically ontology support, statistics and collection’s agreement availability. These three features had minimal impact on annotators’ work as they do not offer direct support in the annotations; instead, they prove their key role in providing insights into overall agreements. One annotator considers five features relevant features, while six features by two. The possibility to copy the annotations of another annotator has been considered the most valuable feature, as all the annotators agreed on its importance.<fig id="Fig22"><label>Fig. 22</label><caption><p><italic>MetaTron</italic> features qualitative analysis. The overview concerns the features the annotators considered important to perform the two annotation tasks. The most significant features concern the collaboration among multiple annotators, while the least used concern the ontology support and the availability statistics</p></caption><graphic xlink:href="12859_2024_5730_Fig22_HTML" id="MO22"/></fig></p>
      <p id="Par111"><italic>AutoTron results</italic> In Table <xref rid="Tab5" ref-type="table">5</xref>, we report the total number of annotations at the end of each round. The first row, <italic>AutoTron</italic> refers to the total number of automatic annotations generated via <italic>AutoTron</italic> each annotator has been provided with at the beginning of round 1. Each annotator started with 228 concepts, and 57 relationships for the GCA task, and 17 assertions for the GDA task. At the end of round 1, the number of concepts increased to 235, the number of relationships increased to 115, and the number of assertions increased to 32. At the end of round 2, the number of concepts increased to 237, the number of relationships decreased to 111, and the number of assertions decreased to 24. The most significant change was identified in the GCA task for the doubled relationships at the end of round 1.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Overview of the total number of linked concepts, relationships, assertions annotations identified at each round. The first row, <italic>AutoTron</italic>, represents the starting point for each annotator, indicating the set of automatic annotations provided</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left" colspan="2">GCA</th><th align="left">GDA</th></tr><tr><th align="left"/><th align="left">Concepts annotation</th><th align="left">Relationships annotation</th><th align="left">Assertions annotation</th></tr></thead><tbody><tr><td align="left">AutoTron</td><td align="left">228</td><td char="." align="char">57</td><td align="left">17</td></tr><tr><td align="left">Round 1</td><td align="left">235</td><td char="." align="char">115</td><td align="left">32</td></tr><tr><td align="left">Round 2</td><td align="left">237</td><td char="." align="char">111</td><td align="left">24</td></tr></tbody></table></table-wrap></p>
      <p id="Par112">We considered the set of distinct annotations obtained at the end of round 2, and we counted how many concepts, relationships, and assertions have been updated, added to, and deleted from the automatically generated set of annotations provided for round 1. We considered an update when, in a relationship or assertion, the predicate assigned by the annotators is different from the one automatically assigned. We have a concept update, instead, when the linked concept changes.</p>
      <p id="Par113">In the GCA task, we detected that 9 concepts had been added, 2 were updated, 0 were removed, and 226 were confirmed. For what concerns relationships instead, 29 relationships have been added, 27 updated, 0 deleted, and 30 confirmed. In the GDA task, we detected that four assertions have been added, three updated, 0 deleted, and 14 confirmed.</p>
      <p id="Par114">The absence of deleted annotations confirms that <italic>AutoTron</italic> overall generates accurate annotations. Only in the GCA task in the relationships annotation the number of relationships added, updated, and confirmed remains the same.</p>
      <p id="Par115">Our results indicate that relying on <italic>AutoTron</italic> to generate a set of annotations used as a starting point is useful to facilitate and speed up the entire annotation process of the annotators. However, especially for relationships, the intervention of a human annotator is crucial for identifying and updating all existing relationships in a document. In this respect, according to the results obtained in the qualitative analyses, we see that the annotators found the GDA task easier than the GCA one: this results not only in a higher agreement but also in a lower number of updates and additions with respect to the automatic annotations.</p>
    </sec>
  </sec>
  <sec id="Sec17">
    <title>Conclusions</title>
    <p id="Par116">This paper presents <italic>MetaTron</italic>, a collaborative web-based annotation tool designed specifically for the biomedical domain. The tool facilitates the annotation of mentions, relationships, and document-level labels. It supports various document formats, including PDF, TXT, JSON, and CSV. Additionally, users can utilize the PubMed, Semantic Scholar, and OpenAIRE REST APIs to upload PMIDs or DOIs and annotate corresponding abstracts. Furthermore, <italic>MetaTron</italic> allows users to leverage their teammates’ annotations and incorporate annotations generated by <italic>AutoTron</italic> for fast annotation creation.</p>
    <p id="Par117">To ensure data privacy and limit tool usage to specific research groups, the <italic>MetaTron</italic> docker image enables local deployment on personal servers. Conversely, the online instance of <italic>MetaTron</italic> is designed for online annotation of PubMed, Semantic Scholar, and OpenAIRE abstracts, with the added advantage of utilizing AutoTron’s automatic prediction capabilities.</p>
    <p id="Par118">Noteworthy features of <italic>MetaTron</italic> include support for multiple ontologies, multilingual capabilities, login via ORCID ID, and the option to download annotations in JSON, CSV, and BioC/XML formats.</p>
    <p id="Par119">In our evaluation, we compared <italic>MetaTron</italic> to five other annotation tools, both general purpose and specifically tailored to the biomedical domain. We assessed them against 24 criteria classified into three categories: <italic>Data</italic>, <italic>Technical</italic>, and <italic>Functionalities</italic>. The qualitative analysis revealed that <italic>MetaTron</italic> fulfills almost all of the selected criteria. From a quantitative perspective, the online instance of <italic>MetaTron</italic> outperformed <italic>TeamTat</italic> and <italic>LightTag</italic> in terms of time elapsed and the number of clicks required. Additionally, the dockerized version of <italic>MetaTron</italic> achieved better results than <italic>INCEpTION</italic> in the task of mentions annotation, concept linking, and relationship annotation (MA + CL + RA). We conducted a user study which involved three human annotators and two tasks: relationships annotation and assertions annotation. The user study pointed out that <italic>MetaTron</italic> is an intuitive and easy to use tool. The collaborative features have been of great assistance, enabling annotators to enhance the accuracy of their annotations and improve agreement. The possibility of using <italic>AutoTron</italic> to automatically annotate documents, and to copy other members’ annotations has proven to be one of <italic>MetaTron</italic>’s most valued features, streamlining and facilitating the annotation process. In summary, <italic>MetaTron</italic> presents itself as a compelling annotation tool for the biomedical and bioinformatics community, providing collaborative and interactive features that can effectively streamline the annotation process. With a commitment to ongoing maintenance and a notable emphasis on relation annotation, often overlooked by other annotation tools, we think that <italic>MetaTron</italic> represents one of the highly recommended options for researchers in these domains.</p>
    <p id="Par120">As future work, we plan to integrate more use cases for built-in automatic predictions to allow <italic>MetaTron</italic> to widen to other domains of applications for automatic annotation of text. Moreover, two functionalities deserve to be implemented: the first one is to introduce a new annotation type, which is entity tagging, and let the user associate to a mention a concept type instead of the concept itself; then, we plan to implement discontinuous mentions allowing the user to be more accurate letting them decide which tokens compose the mention. About the support for the ontologies, we plan to introduce the possibility of (i) uploading the full ontologies directly from the related files and (ii) automatically suggesting the concept to associate to a mention relying on the textual and semantic similarity between the mention and the concepts. These features would support the user in speeding up the upload and the selection of the concepts. Finally, we plan to introduce in <italic>MetaTron</italic> some large corpora of documents that one or more members can annotate: this would provide the community with a tool already configured and ready-to-use and would promote the analyses of annotators’ behavior on well-known datasets.</p>
  </sec>
  <sec id="Sec18">
    <title>Availability and requirements</title>
    <p id="Par121">
      <list list-type="bullet">
        <list-item>
          <p id="Par122">Project name: MetaTron</p>
        </list-item>
        <list-item>
          <p id="Par123">Project home page: <ext-link ext-link-type="uri" xlink:href="https://github.com/GDAMining/metatron/">https://github.com/GDAMining/metatron/</ext-link></p>
        </list-item>
        <list-item>
          <p id="Par124">Online instance: <ext-link ext-link-type="uri" xlink:href="https://metatron.dei.unipd.it">https://metatron.dei.unipd.it</ext-link></p>
        </list-item>
        <list-item>
          <p id="Par125">Archived version: not applicable</p>
        </list-item>
        <list-item>
          <p id="Par126">Operating system(s): Platform independent</p>
        </list-item>
        <list-item>
          <p id="Par127">Programming language: Python, JavaScript, HTML, CSS</p>
        </list-item>
        <list-item>
          <p id="Par128">Other requirements: Docker and docker-compose (for the dockerized version) for the offline version</p>
        </list-item>
        <list-item>
          <p id="Par129">License: MIT License</p>
        </list-item>
        <list-item>
          <p id="Par130">Any restrictions to use by non-academics: No</p>
        </list-item>
      </list>
    </p>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p id="Par134"><ext-link ext-link-type="uri" xlink:href="https://www.djangoproject.com">https://www.djangoproject.com</ext-link>.</p>
    </fn>
    <fn id="Fn2">
      <label>2</label>
      <p id="Par135">By accessing with the username <italic>demo</italic> and password <italic>demo</italic>, it is possible to annotate a test collection.</p>
    </fn>
    <fn id="Fn3">
      <label>3</label>
      <p id="Par136">A relationship that solely consists of ontological concepts is treated as a document-level assertion.</p>
    </fn>
    <fn id="Fn4">
      <label>4</label>
      <p id="Par137"><ext-link ext-link-type="uri" xlink:href="https://www.selenium.dev">https://www.selenium.dev</ext-link>.</p>
    </fn>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank Fabio Giachelle, Niccoló Marini, and Laura Menotti for contributing to useful discussions and tool improvements. We appreciate their time, effort, and valuable input, significantly enriching our research. We also thank the anonymous reviewers for their insightful suggestions, which improved the paper and the described tool.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>O.I. was the designer and developer of MetaTron, wrote the code, designed the user study, wrote the main parts of the manuscript, and led the work. S.M. was the designer and developer of AutoTron and the relation extraction methods, contributed to the design of the user study, suggested some functionalities, wrote the parts describing AutoTron and revised the whole paper. G.S. coordinated the work, contributed to the design of MetaTron, suggested some functionalities, and contributed to the writing and revision of the paper.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Open access funding provided by Università degli Studi di Padova. This work is partially supported by the HEREDITARY Project, as part of the European Union’s Horizon Europe research and innovation programme under Grant Agreement No GA 101137074. The work of O.I. was partially funded by the EC H2020 project OpenAIRE-Nexus (Grant Agreement No. 101017452).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The code used in this study is publicly available on GitHub <ext-link ext-link-type="uri" xlink:href="https://github.com/GDAMining/metatron/">https://github.com/GDAMining/metatron/</ext-link>. All the underlying libraries used in this work are open-source. The complete list of libraries and their versions are reported in the GitHub repository.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1">
      <title>Ethics approval and consent to participate</title>
      <p id="Par131">Not applicable.</p>
    </notes>
    <notes id="FPar2">
      <title>Consent for publication</title>
      <p id="Par132">Not applicable.</p>
    </notes>
    <notes id="FPar3" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par133">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Costa</surname>
            <given-names>FF</given-names>
          </name>
        </person-group>
        <article-title>Big data in biomedicine</article-title>
        <source>Drug Discov Today</source>
        <year>2014</year>
        <volume>19</volume>
        <issue>4</issue>
        <fpage>433</fpage>
        <lpage>440</lpage>
        <pub-id pub-id-type="doi">10.1016/j.drudis.2013.10.012</pub-id>
        <?supplied-pmid 24183925?>
        <pub-id pub-id-type="pmid">24183925</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Murdoch</surname>
            <given-names>TB</given-names>
          </name>
          <name>
            <surname>Detsky</surname>
            <given-names>AS</given-names>
          </name>
        </person-group>
        <article-title>The inevitable application of big data to health care</article-title>
        <source>JAMA</source>
        <year>2013</year>
        <volume>309</volume>
        <issue>13</issue>
        <fpage>1351</fpage>
        <lpage>1352</lpage>
        <pub-id pub-id-type="doi">10.1001/jama.2013.393</pub-id>
        <?supplied-pmid 23549579?>
        <pub-id pub-id-type="pmid">23549579</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>CC</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Community challenges in biomedical text mining over 10 years: success, failure and the future</article-title>
        <source>Brief Bioinform</source>
        <year>2016</year>
        <volume>17</volume>
        <issue>1</issue>
        <fpage>132</fpage>
        <lpage>144</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbv024</pub-id>
        <?supplied-pmid 25935162?>
        <pub-id pub-id-type="pmid">25935162</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jovanović</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bagheri</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Semantic annotation in biomedicine: the current landscape</article-title>
        <source>J Biomed Semant</source>
        <year>2017</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>18</lpage>
        <pub-id pub-id-type="doi">10.1186/s13326-017-0153-x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Patumcharoenpol</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Meechai</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Vongsangnak</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Biomedical text mining and its applications in cancer research</article-title>
        <source>J Biomed Inform</source>
        <year>2013</year>
        <volume>46</volume>
        <issue>2</issue>
        <fpage>200</fpage>
        <lpage>211</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2012.10.007</pub-id>
        <?supplied-pmid 23159498?>
        <pub-id pub-id-type="pmid">23159498</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lindvall</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>C-Y</given-names>
          </name>
          <name>
            <surname>Moseley</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Agaronnik</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>El-Jawahri</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Paasche-Orlow</surname>
            <given-names>MK</given-names>
          </name>
          <name>
            <surname>Lakin</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Volandes</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tulsky</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Investigators</surname>
            <given-names>A-P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Natural language processing to identify advance care planning documentation in a multisite pragmatic clinical trial</article-title>
        <source>J Pain Symptom Manag</source>
        <year>2022</year>
        <volume>63</volume>
        <issue>1</issue>
        <fpage>29</fpage>
        <lpage>36</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jpainsymman.2021.06.025</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cohen</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Hersh</surname>
            <given-names>WR</given-names>
          </name>
        </person-group>
        <article-title>A survey of current work in biomedical text mining</article-title>
        <source>Brief Bioinform</source>
        <year>2005</year>
        <volume>6</volume>
        <issue>1</issue>
        <fpage>57</fpage>
        <lpage>71</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/6.1.57</pub-id>
        <?supplied-pmid 15826357?>
        <pub-id pub-id-type="pmid">15826357</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kersloot</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>van Putten</surname>
            <given-names>FJ</given-names>
          </name>
          <name>
            <surname>Abu-Hanna</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cornet</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Arts</surname>
            <given-names>DL</given-names>
          </name>
        </person-group>
        <article-title>Natural language processing algorithms for mapping clinical text fragments onto ontology concepts: a systematic review and recommendations for future studies</article-title>
        <source>J Biomed Semant</source>
        <year>2020</year>
        <volume>11</volume>
        <fpage>1</fpage>
        <lpage>21</lpage>
        <pub-id pub-id-type="doi">10.1186/s13326-020-00231-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lacson</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Pitzer</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Hinske</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Galante</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Ohno-Machado</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Evaluation of a large-scale biomedical data annotation initiative</article-title>
        <source>BMC Bioinform</source>
        <year>2009</year>
        <volume>10</volume>
        <fpage>1</fpage>
        <lpage>6</lpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-10-S9-S10</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Neves</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ševa</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>An extensive review of tools for manual annotation of documents</article-title>
        <source>Brief Bioinform</source>
        <year>2021</year>
        <volume>22</volume>
        <issue>1</issue>
        <fpage>146</fpage>
        <lpage>163</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbz130</pub-id>
        <?supplied-pmid 31838514?>
        <pub-id pub-id-type="pmid">31838514</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koleck</surname>
            <given-names>TA</given-names>
          </name>
          <name>
            <surname>Dreisbach</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Bourne</surname>
            <given-names>PE</given-names>
          </name>
          <name>
            <surname>Bakken</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Natural language processing of symptoms documented in free-text narratives of electronic health records: a systematic review</article-title>
        <source>J Am Med Inform Assoc</source>
        <year>2019</year>
        <volume>26</volume>
        <issue>4</issue>
        <fpage>364</fpage>
        <lpage>379</lpage>
        <pub-id pub-id-type="doi">10.1093/jamia/ocy173</pub-id>
        <?supplied-pmid 30726935?>
        <pub-id pub-id-type="pmid">30726935</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yim</surname>
            <given-names>W-W</given-names>
          </name>
          <name>
            <surname>Yetisgen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Harris</surname>
            <given-names>WP</given-names>
          </name>
          <name>
            <surname>Kwan</surname>
            <given-names>SW</given-names>
          </name>
        </person-group>
        <article-title>Natural language processing in oncology: a review</article-title>
        <source>JAMA Oncol</source>
        <year>2016</year>
        <volume>2</volume>
        <issue>6</issue>
        <fpage>797</fpage>
        <lpage>804</lpage>
        <pub-id pub-id-type="doi">10.1001/jamaoncol.2016.0213</pub-id>
        <?supplied-pmid 27124593?>
        <pub-id pub-id-type="pmid">27124593</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>PourNejatian</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Shin</surname>
            <given-names>HC</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>KE</given-names>
          </name>
          <name>
            <surname>Parisien</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Compas</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Costa</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Flores</surname>
            <given-names>MG</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A large language model for electronic health records</article-title>
        <source>NPJ Digit Med</source>
        <year>2022</year>
        <volume>5</volume>
        <issue>1</issue>
        <fpage>194</fpage>
        <pub-id pub-id-type="doi">10.1038/s41746-022-00742-2</pub-id>
        <?supplied-pmid 36572766?>
        <pub-id pub-id-type="pmid">36572766</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Meij E, Balog K, Odijk D. Entity linking and retrieval. In: Proceedings of the 36th international ACM SIGIR conference on research and development in information retrieval; 2013. p. 1127.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Recent advances in biomedical literature mining</article-title>
        <source>Brief Bioinform</source>
        <year>2021</year>
        <volume>22</volume>
        <issue>3</issue>
        <fpage>057</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbaa057</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hong</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wan</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A novel machine learning framework for automated biomedical relation extraction from large-scale literature repositories</article-title>
        <source>Nat Mach Intell</source>
        <year>2020</year>
        <volume>2</volume>
        <issue>6</issue>
        <fpage>347</fpage>
        <lpage>355</lpage>
        <pub-id pub-id-type="doi">10.1038/s42256-020-0189-y</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Biomedical relation extraction via knowledge-enhanced reading comprehension</article-title>
        <source>BMC Bioinform</source>
        <year>2022</year>
        <volume>23</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>19</lpage>
        <pub-id pub-id-type="doi">10.1186/s12859-021-04534-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xing</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>BioRel: towards large-scale biomedical relation extraction</article-title>
        <source>BMC Bioinform</source>
        <year>2020</year>
        <volume>21</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1186/s12859-020-03889-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Zhou P, Shi W, Tian J, Qi Z, Li B, Hao H, Xu B. Attention-based bidirectional long short-term memory networks for relation classification. In: Proceedings of the 54th annual meeting of the Association for Computational Linguistics (volume 2: short papers); 2016. p. 207–12.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Zhang D, Wang D. Relation classification via recurrent neural network. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1508.01006">arXiv:1508.01006</ext-link> (2015).</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Zeng D, Liu K, Chen Y, Zhao J. Distant supervision for relation extraction via piecewise convolutional neural networks. In: Proceedings of the 2015 conference on empirical methods in natural language processing, EMNLP 2015, Lisbon, Portugal, September 17–21, 2015. p. 1753–62.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Exploring semi-supervised variational autoencoders for biomedical relation extraction</article-title>
        <source>Methods</source>
        <year>2019</year>
        <volume>166</volume>
        <fpage>112</fpage>
        <lpage>119</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ymeth.2019.02.021</pub-id>
        <?supplied-pmid 30822516?>
        <pub-id pub-id-type="pmid">30822516</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Krauthammer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nenadic</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Term identification in the biomedical literature</article-title>
        <source>J Biomed Inform</source>
        <year>2004</year>
        <volume>37</volume>
        <issue>6</issue>
        <fpage>512</fpage>
        <lpage>526</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2004.08.004</pub-id>
        <?supplied-pmid 15542023?>
        <pub-id pub-id-type="pmid">15542023</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Matthews</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Distinguishing the species of biomedical named entities for term identification</article-title>
        <source>BMC Bioinform</source>
        <year>2008</year>
        <volume>9</volume>
        <issue>11</issue>
        <fpage>1</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Elhadad</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Unsupervised biomedical named entity recognition: experiments with clinical and biological texts</article-title>
        <source>J Biomed Inform</source>
        <year>2013</year>
        <volume>46</volume>
        <issue>6</issue>
        <fpage>1088</fpage>
        <lpage>1098</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2013.08.004</pub-id>
        <?supplied-pmid 23954592?>
        <pub-id pub-id-type="pmid">23954592</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Gorrell G, Song X, Roberts A. Bio-yodie: a named entity linking system for biomedical text. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1811.04860">arXiv:1811.04860</ext-link> (2018)</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vashishth</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Newman-Griffis</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Joshi</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Dutt</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Rosé</surname>
            <given-names>CP</given-names>
          </name>
        </person-group>
        <article-title>Improving broad-coverage medical entity linking with semantic type prediction and large-scale datasets</article-title>
        <source>J Biomed Inform</source>
        <year>2021</year>
        <volume>121</volume>
        <fpage>103880</fpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2021.103880</pub-id>
        <?supplied-pmid 34390853?>
        <pub-id pub-id-type="pmid">34390853</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">D’Souza J, Ng V. Sieve-based entity linking for the biomedical domain. In: Proceedings of the 53rd annual meeting of the Association for Computational Linguistics and the 7th international joint conference on natural language processing (volume 2: short papers); 2015. p. 297–302.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Jiang X, Ringwald M, Blake J, Shatkay H. Effective biomedical document classification for identifying publications relevant to the mouse Gene Expression Database (GXD). Database 2017;2017.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pérez-Pérez</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ferreira</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Lourenço</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Igrejas</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Fdez-Riverola</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Boosting biomedical document classification through the use of domain entity recognizers and semantic ontologies for document representation: the case of gluten bibliome</article-title>
        <source>Neurocomputing</source>
        <year>2022</year>
        <volume>484</volume>
        <fpage>223</fpage>
        <lpage>237</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neucom.2021.10.100</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Trabucco</surname>
            <given-names>JT</given-names>
          </name>
          <name>
            <surname>Raciti</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ringwald</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Marai</surname>
            <given-names>GE</given-names>
          </name>
          <name>
            <surname>Arighi</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Shatkay</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Utilizing image and caption information for biomedical document classification</article-title>
        <source>Bioinformatics</source>
        <year>2021</year>
        <volume>37</volume>
        <issue>Supplement-1</issue>
        <fpage>468</fpage>
        <lpage>476</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btab331</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Burns GA, Li X, Peng N. Building deep learning models for evidence classification from the open access biomedical literature. Database 2019; 2019.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dramé</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Mougin</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Diallo</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Large scale biomedical texts classification: a kNN and an ESA-based approaches</article-title>
        <source>Journal of biomedical semantics</source>
        <year>2016</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1186/s13326-016-0073-1</pub-id>
        <pub-id pub-id-type="pmid">26759709</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Simon</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Davidsen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Hansen</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Seymour</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Barnkob</surname>
            <given-names>MB</given-names>
          </name>
          <name>
            <surname>Olsen</surname>
            <given-names>LR</given-names>
          </name>
        </person-group>
        <article-title>BioReader: a text mining tool for performing classification of biomedical literature</article-title>
        <source>BMC Bioinform</source>
        <year>2019</year>
        <volume>19</volume>
        <fpage>165</fpage>
        <lpage>170</lpage>
        <pub-id pub-id-type="doi">10.1186/s12859-019-2607-x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Jiang X, M, Blake JA, Arighi C, Zhang G, Shatkay H. An effective biomedical document classification scheme in support of biocuration: addressing class imbalance. Database 2019; 2019.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bodenreider</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>The unified medical language system (UMLS): integrating biomedical terminology</article-title>
        <source>Nucleic Acids Res</source>
        <year>2004</year>
        <volume>32</volume>
        <issue>suppl–1</issue>
        <fpage>267</fpage>
        <lpage>270</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkh061</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>J-D</given-names>
          </name>
          <name>
            <surname>Ohta</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tsujii</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Corpus annotation for mining biomedical events from literature</article-title>
        <source>BMC Bioinform</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>1</fpage>
        <lpage>25</lpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-9-10</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bada</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Eckert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Evans</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Garcia</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Shipley</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Sitnikov</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Baumgartner</surname>
            <given-names>WA</given-names>
          </name>
          <name>
            <surname>Cohen</surname>
            <given-names>KB</given-names>
          </name>
          <name>
            <surname>Verspoor</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Blake</surname>
            <given-names>JA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Concept annotation in the CRAFT corpus</article-title>
        <source>BMC Bioinform</source>
        <year>2012</year>
        <volume>13</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>20</lpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-13-161</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Doğan</surname>
            <given-names>RI</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>NCBI disease corpus: a resource for disease name recognition and concept normalization</article-title>
        <source>J Biomed Inform</source>
        <year>2014</year>
        <volume>47</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2013.12.006</pub-id>
        <?supplied-pmid 24393765?>
        <pub-id pub-id-type="pmid">24393765</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Krallinger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rabal</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Leitner</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Vazquez</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Salgado</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lowe</surname>
            <given-names>DM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The CHEMDNER corpus of chemicals and drugs and its annotation principles</article-title>
        <source>J Cheminform</source>
        <year>2015</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="doi">10.1186/1758-2946-7-S1-S1</pub-id>
        <pub-id pub-id-type="pmid">25705261</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Ohta</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tateisi</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Tsujii</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>GENIA corpus—a semantically annotated corpus for bio-textmining</article-title>
        <source>Bioinformatics</source>
        <year>2003</year>
        <volume>19</volume>
        <issue>suppl-1</issue>
        <fpage>180</fpage>
        <lpage>182</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btg1023</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Li J, Sun Y, Johnson RJ, Sciaky D, Wei CH, Leaman R, Davis AP, Mattingly CJ, Wiegers TC, Lu Z. Biocreative V CDR task corpus: a resource for chemical disease relation extraction. Database 2016; 2016.</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Mohan S, Li D. Medmentions: a large biomedical corpus annotated with UMLS concepts. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1902.09476">arXiv:1902.09476</ext-link> (2019)</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Uzuner</surname>
            <given-names>Ö</given-names>
          </name>
          <name>
            <surname>South</surname>
            <given-names>BR</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>DuVall</surname>
            <given-names>SL</given-names>
          </name>
        </person-group>
        <article-title>2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text</article-title>
        <source>J Am Med Inform Assoc</source>
        <year>2011</year>
        <volume>18</volume>
        <issue>5</issue>
        <fpage>552</fpage>
        <lpage>556</lpage>
        <pub-id pub-id-type="doi">10.1136/amiajnl-2011-000203</pub-id>
        <?supplied-pmid 21685143?>
        <pub-id pub-id-type="pmid">21685143</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Johnson</surname>
            <given-names>AE</given-names>
          </name>
          <name>
            <surname>Pollard</surname>
            <given-names>TJ</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Lehman</surname>
            <given-names>LH</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ghassemi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Moody</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Szolovits</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Celi</surname>
            <given-names>LA</given-names>
          </name>
          <name>
            <surname>Mark</surname>
            <given-names>RG</given-names>
          </name>
        </person-group>
        <article-title>MIMIC-III, a freely accessible critical care database</article-title>
        <source>Sci Data</source>
        <year>2016</year>
        <volume>3</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1038/sdata.2016.35</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van Mulligen</surname>
            <given-names>EM</given-names>
          </name>
          <name>
            <surname>Fourrier-Reglat</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gurwitz</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Molokhia</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nieto</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Trifiro</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Kors</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Furlong</surname>
            <given-names>LI</given-names>
          </name>
        </person-group>
        <article-title>The EU-ADR corpus: annotated drugs, diseases, targets, and their relationships</article-title>
        <source>J Biomed Inform</source>
        <year>2012</year>
        <volume>45</volume>
        <issue>5</issue>
        <fpage>879</fpage>
        <lpage>884</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2012.04.004</pub-id>
        <?supplied-pmid 22554700?>
        <pub-id pub-id-type="pmid">22554700</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>HJ</given-names>
          </name>
          <name>
            <surname>Shim</surname>
            <given-names>SH</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>CoMAGC: a corpus with multi-faceted annotations of gene-cancer relations</article-title>
        <source>BMC Bioinform</source>
        <year>2013</year>
        <volume>14</volume>
        <fpage>323</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-14-323</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Giachelle</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Irrera</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Silvello</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>MedTAG: a portable and customizable annotation tool for biomedical documents</article-title>
        <source>BMC Med Inform Decis Mak</source>
        <year>2021</year>
        <volume>21</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>19</lpage>
        <pub-id pub-id-type="doi">10.1186/s12911-021-01706-4</pub-id>
        <pub-id pub-id-type="pmid">33388057</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Islamaj</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Kwon</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>TeamTat: a collaborative text annotation tool</article-title>
        <source>Nucleic Acids Res</source>
        <year>2020</year>
        <volume>48</volume>
        <issue>W1</issue>
        <fpage>5</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkaa333</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Cejuela JM, McQuilton P, Ponting L, Marygold SJ, Stefancsik R, Millburn GH, Rost B, Consortium F, et al. tagtog: interactive and text-mining-assisted annotation of gene mentions in PLOS full-text articles. Database 2014; 2014.</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Salgado</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Krallinger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Depaule</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Drula</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Tendulkar</surname>
            <given-names>AV</given-names>
          </name>
          <name>
            <surname>Leitner</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Valencia</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Marcelle</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>MyMiner: a web application for computer-assisted biocuration and text annotation</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>28</volume>
        <issue>17</issue>
        <fpage>2285</fpage>
        <lpage>2287</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts435</pub-id>
        <?supplied-pmid 22789588?>
        <pub-id pub-id-type="pmid">22789588</pub-id>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other">Kwon D, Kim S, Shin S, Wilbur WJ. BioQRator: a web-based interactive biomedical literature curating system. In: Proceedings of the fourth biocreative challenge evaluation workshop, vol 1; 2013. p. 241–46.</mixed-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kwon</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>ezTag: tagging biomedical concepts via interactive learning</article-title>
        <source>Nucleic Acids Res</source>
        <year>2018</year>
        <volume>46</volume>
        <issue>W1</issue>
        <fpage>523</fpage>
        <lpage>529</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gky428</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <mixed-citation publication-type="other">Stenetorp P, Pyysalo S, Topić G, Ohta T, Ananiadou S, Tsujii J. BRAT: a web-based tool for NLP-assisted text annotation. In: Proceedings of the demonstrations at the 13th conference of the European chapter of the Association for Computational Linguistics; 2012. p. 102–7.</mixed-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <mixed-citation publication-type="other">Giachelle F, Irrera O, Silvello G. DocTAG: a customizable annotation tool for ground truth creation. In: Advances in information retrieval: 44th European conference on IR Research, ECIR 2022, Stavanger, Norway, April 10–14, 2022, proceedings, part II. Springer; 2022. p. 288–93.</mixed-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <mixed-citation publication-type="other">Klie J-C, Bugert M, Boullosa B, de Castilho RE, Gurevych I. The inception platform: machine-assisted and knowledge-oriented interactive annotation. In: Proceedings of the 27th international conference on computational linguistics: system demonstrations; 2018. p. 5–9.</mixed-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <mixed-citation publication-type="other">Perry T. Lighttag: text annotation platform. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2109.02320">arXiv:2109.02320</ext-link> (2021)</mixed-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <mixed-citation publication-type="other">Muhie SY, Gurevych I, de Castilho RE, Biemann C. Webanno: a flexible, web-based and visually supported system for distributed annotations. In: Proceedings of the 51st annual meeting of the Association for Computational Linguistics: system demonstrations; 2013. p. 1–6.</mixed-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <mixed-citation publication-type="other">Jazayeri M. Some trends in web application development. In: Future of software engineering (FOSE’07). IEEE; 2007. p. 199–213.</mixed-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dobbie</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Strafford</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Pickrell</surname>
            <given-names>WO</given-names>
          </name>
          <name>
            <surname>Fonferko-Shadrach</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Akbari</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Thompson</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lacey</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Markup: a web-based annotation tool powered by active learning</article-title>
        <source>Front Digit Health</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.3389/fdgth.2021.598916</pub-id>
        <?supplied-pmid 34713086?>
        <pub-id pub-id-type="pmid">34713086</pub-id>
      </element-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Wen</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Moon</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Towards a better understanding of annotation tools for medical imaging: a survey</article-title>
        <source>Multimed Tools Appl</source>
        <year>2022</year>
        <volume>81</volume>
        <issue>18</issue>
        <fpage>25877</fpage>
        <lpage>25911</lpage>
        <pub-id pub-id-type="doi">10.1007/s11042-022-12100-1</pub-id>
        <pub-id pub-id-type="pmid">35350630</pub-id>
      </element-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Piad-Morffis</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gutiérrez</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Almeida-Cruz</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Munoz</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>A computational ecosystem to support ehealth knowledge discovery technologies in Spanish</article-title>
        <source>J Biomed Inform</source>
        <year>2020</year>
        <volume>109</volume>
        <fpage>103517</fpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2020.103517</pub-id>
        <?supplied-pmid 32712157?>
        <pub-id pub-id-type="pmid">32712157</pub-id>
      </element-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reinanda</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Meij</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>de Rijke</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Knowledge graphs: an information retrieval perspective</article-title>
        <source>Found Trends® Inf Retr</source>
        <year>2020</year>
        <volume>14</volume>
        <issue>4</issue>
        <fpage>289</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="doi">10.1561/1500000063</pub-id>
      </element-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <mixed-citation publication-type="other">Lopez P. GROBID: combining automatic bibliographic data recognition and term extraction for scholarship publications. In: Research and advanced technology for digital libraries: 13th European conference, ECDL 2009, Corfu, Greece, September 27–October 2, 2009. Proceedings 2009, vol 13. Springer. p. 473–4.</mixed-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>French</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>McInnes</surname>
            <given-names>BT</given-names>
          </name>
        </person-group>
        <article-title>An overview of biomedical entity linking throughout the years</article-title>
        <source>J Biomed Inform</source>
        <year>2023</year>
        <volume>137</volume>
        <fpage>104252</fpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2022.104252</pub-id>
        <?supplied-pmid 36464228?>
        <pub-id pub-id-type="pmid">36464228</pub-id>
      </element-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sevgili</surname>
            <given-names>Ö</given-names>
          </name>
          <name>
            <surname>Shelmanov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Arkhipov</surname>
            <given-names>MY</given-names>
          </name>
          <name>
            <surname>Panchenko</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Biemann</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Neural entity linking: a survey of models based on deep learning</article-title>
        <source>Semant Web</source>
        <year>2022</year>
        <volume>13</volume>
        <issue>3</issue>
        <fpage>527</fpage>
        <lpage>570</lpage>
        <pub-id pub-id-type="doi">10.3233/SW-222986</pub-id>
      </element-citation>
    </ref>
    <ref id="CR67">
      <label>67.</label>
      <mixed-citation publication-type="other">Aydar M, Bozal O, Özbay F. Neural relation extraction: a survey. CoRR <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2007.04247">arXiv: 2007.04247</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR68">
      <label>68.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Smirnova</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cudré-Mauroux</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Relation extraction using distant supervision: a survey</article-title>
        <source>ACM Comput Surv</source>
        <year>2019</year>
        <volume>51</volume>
        <issue>5</issue>
        <fpage>106</fpage>
        <lpage>110635</lpage>
        <pub-id pub-id-type="doi">10.1145/3241741</pub-id>
      </element-citation>
    </ref>
    <ref id="CR69">
      <label>69.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Kao</surname>
            <given-names>HY</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>PubTator: a web-based text mining tool for assisting biocuration</article-title>
        <source>Nucleic Acids Res</source>
        <year>2013</year>
        <volume>41</volume>
        <issue>Webserver-Issue</issue>
        <fpage>518</fpage>
        <lpage>522</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkt441</pub-id>
        <pub-id pub-id-type="pmid">23125361</pub-id>
      </element-citation>
    </ref>
    <ref id="CR70">
      <label>70.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Beyond accuracy: creating interoperable and scalable text-mining web services</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <issue>12</issue>
        <fpage>1907</fpage>
        <lpage>1910</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btv760</pub-id>
        <?supplied-pmid 26883486?>
        <pub-id pub-id-type="pmid">26883486</pub-id>
      </element-citation>
    </ref>
    <ref id="CR71">
      <label>71.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Allot</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>PubTator central: automated concept annotation for biomedical full text articles</article-title>
        <source>Nucleic Acids Res</source>
        <year>2019</year>
        <volume>47</volume>
        <issue>Webserver-Issue</issue>
        <fpage>587</fpage>
        <lpage>593</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkz389</pub-id>
      </element-citation>
    </ref>
    <ref id="CR72">
      <label>72.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maglott</surname>
            <given-names>DR</given-names>
          </name>
          <name>
            <surname>Ostell</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Pruitt</surname>
            <given-names>KD</given-names>
          </name>
          <name>
            <surname>Tatusova</surname>
            <given-names>TA</given-names>
          </name>
        </person-group>
        <article-title>Entrez gene: gene-centered information at NCBI</article-title>
        <source>Nucleic Acids Res</source>
        <year>2011</year>
        <volume>39</volume>
        <issue>Database-Issue</issue>
        <fpage>52</fpage>
        <lpage>57</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkq1237</pub-id>
      </element-citation>
    </ref>
    <ref id="CR73">
      <label>73.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lipscomb</surname>
            <given-names>CE</given-names>
          </name>
        </person-group>
        <article-title>Medical subject headings (MeSH)</article-title>
        <source>Bull Med Libr Assoc</source>
        <year>2000</year>
        <volume>88</volume>
        <issue>3</issue>
        <fpage>265</fpage>
        <?supplied-pmid 10928714?>
        <pub-id pub-id-type="pmid">10928714</pub-id>
      </element-citation>
    </ref>
    <ref id="CR74">
      <label>74.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Demner-Fushman</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Rogers</surname>
            <given-names>WJ</given-names>
          </name>
          <name>
            <surname>Aronson</surname>
            <given-names>AR</given-names>
          </name>
        </person-group>
        <article-title>MetaMap lite: an evaluation of a new Java implementation of MetaMap</article-title>
        <source>J Am Med Inform Assoc</source>
        <year>2017</year>
        <volume>24</volume>
        <issue>4</issue>
        <fpage>841</fpage>
        <lpage>844</lpage>
        <pub-id pub-id-type="doi">10.1093/jamia/ocw177</pub-id>
        <?supplied-pmid 28130331?>
        <pub-id pub-id-type="pmid">28130331</pub-id>
      </element-citation>
    </ref>
    <ref id="CR75">
      <label>75.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dugger</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Platt</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Goldstein</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Drug development in the era of precision medicine</article-title>
        <source>Nat Rev Drug Discov</source>
        <year>2018</year>
        <volume>17</volume>
        <fpage>183</fpage>
        <lpage>196</lpage>
        <pub-id pub-id-type="doi">10.1038/nrd.2017.226</pub-id>
        <?supplied-pmid 29217837?>
        <pub-id pub-id-type="pmid">29217837</pub-id>
      </element-citation>
    </ref>
    <ref id="CR76">
      <label>76.</label>
      <mixed-citation publication-type="other">Surdeanu M, Tibshirani J, Nallapati R, Manning CD. Multi-instance multi-label learning for relation extraction. In: Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning, EMNLP-CoNLL 2012, July 12–14, 2012, Jeju Island, Korea; 2012. p. 455–65.</mixed-citation>
    </ref>
    <ref id="CR77">
      <label>77.</label>
      <mixed-citation publication-type="other">Riedel S, Yao L, McCallum A. Modeling relations and their mentions without labeled text. In: Proceedings of machine learning and knowledge discovery in databases, European conference, ECML PKDD 2010, Barcelona, Spain, September 20–24, 2010. LNCS, vol 6323; 2010. p. 148–63.</mixed-citation>
    </ref>
    <ref id="CR78">
      <label>78.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marchesin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Silvello</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>TBGA: a large-scale gene-disease association dataset for biomedical relation extraction</article-title>
        <source>BMC Bioinform</source>
        <year>2022</year>
        <volume>23</volume>
        <issue>1</issue>
        <fpage>111</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-022-04646-6</pub-id>
      </element-citation>
    </ref>
    <ref id="CR79">
      <label>79.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Neary</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Identifying gene expression patterns associated with drug-specific survival in cancer patients</article-title>
        <source>Sci Rep</source>
        <year>2021</year>
        <volume>11</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1038/s41598-021-84211-y</pub-id>
        <pub-id pub-id-type="pmid">33414495</pub-id>
      </element-citation>
    </ref>
    <ref id="CR80">
      <label>80.</label>
      <mixed-citation publication-type="other">Liu F, Chen J, Jagannatha A, Yu H. Learning for biomedical information extraction: methodological review of recent advances. CoRR abs/1606.07993 (2016)</mixed-citation>
    </ref>
    <ref id="CR81">
      <label>81.</label>
      <mixed-citation publication-type="other">Krallinger M, Rabal O, Akhondi SA, Pérez MP, Santamaría J, Rodríguez GP, Tsatsaronis G, Intxaurrondo A, Lopez JA, Nandal UK, van Buel EM, Chandrasekhar A, Rodenburg M, Lægreid A, Doornenbal MA, Oyarzábal J, Lourenço A, Valencia A. Overview of the BioCreative VI chemical-protein interaction Track. In: Proceedings of the sixth biocreative challenge evaluation workshop; 2017.</mixed-citation>
    </ref>
    <ref id="CR82">
      <label>82.</label>
      <mixed-citation publication-type="other">Miranda A, Mehryary F, Luoma J, Pyysalo S, Valencia A, Krallinger M. Overview of DrugProt BioCreative VII track: quality evaluation and large scale text mining of drug-gene/protein relations. In: Proceedings of the seventh biocreative challenge evaluation workshop; 2021.</mixed-citation>
    </ref>
    <ref id="CR83">
      <label>83.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>HJ</given-names>
          </name>
          <name>
            <surname>Dang</surname>
            <given-names>TC</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>OncoSearch: cancer gene search engine with literature evidence</article-title>
        <source>Nucleic Acids Res</source>
        <year>2014</year>
        <volume>42</volume>
        <issue>Webserver-Issue</issue>
        <fpage>416</fpage>
        <lpage>421</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gku368</pub-id>
      </element-citation>
    </ref>
    <ref id="CR84">
      <label>84.</label>
      <mixed-citation publication-type="other">Beltagy I, Lo K, Cohan A. SciBERT: a pretrained language model for scientific text. In: Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3–7, 2019; 2019. p. 3613–18.</mixed-citation>
    </ref>
    <ref id="CR85">
      <label>85.</label>
      <mixed-citation publication-type="other">Devlin J, Chang MW, Lee K, Toutanova K. BERT: pre-training of deep bidirectional transformers for language understanding. In: Proceedings of the 2019 conference of the North American Chapter of the Association for Computational Linguistics: human language technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2–7, 2019, volume 1 (long and short papers); 2019. p. 4171–86.</mixed-citation>
    </ref>
    <ref id="CR86">
      <label>86.</label>
      <mixed-citation publication-type="other">Ammar W, Groeneveld D, Bhagavatula C, Beltagy I, Crawford M, Downey D, Dunkelberger J, Elgohary A, Feldman S, Ha V, Kinney R, Kohlmeier S, Lo K, Murray T, Ooi HH, Peters ME, Power J, Skjonsberg S, Wang LL, Wilhelm C, Yuan Z, van Zuylen M, Etzioni O. Construction of the literature graph in semantic scholar. In: Proceedings of the 2018 conference of the North American Chapter of the Association for Computational Linguistics: human language technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1–6, 2018, volume 3 (industry papers); 2018. p. 84–91.</mixed-citation>
    </ref>
    <ref id="CR87">
      <label>87.</label>
      <mixed-citation publication-type="other">Giachelle F, Marchesin S, Silvello G, Alonso O. Searching for reliable facts over a medical knowledge base. In: Proceedings of the 46th international ACM SIGIR conference on research and development in information retrieval, SIGIR 2023, Taipei, Taiwan, July 23–27, 2023; 2023. p. 23–7. 10.1145/3539618.3591822.</mixed-citation>
    </ref>
    <ref id="CR88">
      <label>88.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marchesin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Menotti</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Giachelle</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Silvello</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Alonso</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>Building a large gene expression-cancer knowledge base with limited human annotations</article-title>
        <source>Database J Biol Databases Curation</source>
        <year>2023</year>
        <pub-id pub-id-type="doi">10.1093/DATABASE/BAAD061</pub-id>
      </element-citation>
    </ref>
    <ref id="CR89">
      <label>89.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fleiss</surname>
            <given-names>JL</given-names>
          </name>
        </person-group>
        <article-title>Measuring nominal scale agreement among many raters</article-title>
        <source>Psychol Bull</source>
        <year>1971</year>
        <volume>76</volume>
        <issue>5</issue>
        <fpage>378</fpage>
        <pub-id pub-id-type="doi">10.1037/h0031619</pub-id>
      </element-citation>
    </ref>
    <ref id="CR90">
      <label>90.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McHugh</surname>
            <given-names>ML</given-names>
          </name>
        </person-group>
        <article-title>Interrater reliability: the kappa statistic</article-title>
        <source>Biochem Med</source>
        <year>2012</year>
        <volume>22</volume>
        <issue>3</issue>
        <fpage>276</fpage>
        <lpage>282</lpage>
        <pub-id pub-id-type="doi">10.11613/BM.2012.031</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10941452</article-id>
    <article-id pub-id-type="pmid">38486137</article-id>
    <article-id pub-id-type="publisher-id">5730</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-024-05730-9</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MetaTron: advancing biomedical annotation empowering relation annotation and collaboration</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Irrera</surname>
          <given-names>Ornella</given-names>
        </name>
        <address>
          <email>ornella.irrera@unipd.it</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Marchesin</surname>
          <given-names>Stefano</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Silvello</surname>
          <given-names>Gianmaria</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00240q980</institution-id><institution-id institution-id-type="GRID">grid.5608.b</institution-id><institution-id institution-id-type="ISNI">0000 0004 1757 3470</institution-id><institution>Department of Information Engineering, </institution><institution>University of Padova, </institution></institution-wrap>Padua, Italy </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>14</day>
      <month>3</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>14</day>
      <month>3</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2024</year>
    </pub-date>
    <volume>25</volume>
    <elocation-id>112</elocation-id>
    <history>
      <date date-type="received">
        <day>26</day>
        <month>5</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>4</day>
        <month>3</month>
        <year>2024</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2024</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">The constant growth of biomedical data is accompanied by the need for new methodologies to effectively and efficiently extract machine-readable knowledge for training and testing purposes. A crucial aspect in this regard is creating large, often manually or semi-manually, annotated corpora vital for developing effective and efficient methods for tasks like relation extraction, topic recognition, and entity linking. However, manual annotation is expensive and time-consuming especially if not assisted by interactive, intuitive, and collaborative computer-aided tools. To support healthcare experts in the annotation process and foster annotated corpora creation, we present <italic>MetaTron</italic>. <italic>MetaTron</italic> is an open-source and free-to-use web-based annotation tool to annotate biomedical data interactively and collaboratively; it supports both mention-level and document-level annotations also integrating automatic built-in predictions. Moreover, <italic>MetaTron</italic> enables relation annotation with the support of ontologies, functionalities often overlooked by off-the-shelf annotation tools.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We conducted a qualitative analysis to compare <italic>MetaTron</italic> with a set of manual annotation tools including <italic>TeamTat</italic>, <italic>INCEpTION</italic>, <italic>LightTag</italic>, <italic>MedTAG</italic>, and <italic>brat</italic>, on three sets of criteria: technical, data, and functional. A quantitative evaluation allowed us to assess <italic>MetaTron</italic> performances in terms of time and number of clicks to annotate a set of documents. The results indicated that <italic>MetaTron</italic> fulfills almost all the selected criteria and achieves the best performances.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3"><italic>MetaTron</italic> stands out as one of the few annotation tools targeting the biomedical domain supporting the annotation of relations, and fully customizable with documents in several formats—PDF included, as well as abstracts retrieved from PubMed, Semantic Scholar, and OpenAIRE. To meet any user need, we released <italic>MetaTron</italic> both as an online instance and as a Docker image locally deployable.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Digital health</kwd>
      <kwd>Biomedical annotation tool</kwd>
      <kwd>Relation extraction</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id>
            <institution>Horizon 2020 Framework Programme</institution>
          </institution-wrap>
        </funding-source>
        <award-id>825292</award-id>
        <award-id>825292</award-id>
        <award-id>825292</award-id>
        <principal-award-recipient>
          <name>
            <surname>Irrera</surname>
            <given-names>Ornella</given-names>
          </name>
          <name>
            <surname>Marchesin</surname>
            <given-names>Stefano</given-names>
          </name>
          <name>
            <surname>Silvello</surname>
            <given-names>Gianmaria</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Università degli Studi di Padova</institution>
        </funding-source>
      </award-group>
      <open-access>
        <p>Open access funding provided by Università degli Studi di Padova.</p>
      </open-access>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© BioMed Central Ltd., part of Springer Nature 2024</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par4">In recent years, the exponential growth of biomedical data such as medical reports, Electronic Health Records (EHR) and physician notes posed relevant challenges in effectively and efficiently organizing, curating, managing, and reusing this data both for clinical and research purposes [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR5">5</xref>]. Given the textual nature of biomedical data (according to [<xref ref-type="bibr" rid="CR6">6</xref>], the 70–80% of clinical data is text-based), extracting and reusing the knowledge in the biomedical literature can drive advances in biomedical research, enhance decision-making processes, and accelerate the discovery of drugs and diseases [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR7">7</xref>–<xref ref-type="bibr" rid="CR9">9</xref>]. Consequently, Natural Language Processing (NLP) techniques have gained substantial importance as they can automate retrieval, biomedical data processing, and knowledge extraction. The research area specialized in applying NLP technique to the biomedical data is defined <italic>BioNLP</italic> [<xref ref-type="bibr" rid="CR3">3</xref>]. Developing efficient and effective NLP methods is challenging as it requires the availability of large manually annotated corpora. In [<xref ref-type="bibr" rid="CR10">10</xref>], authors address this problem and present a comparison of the most commonly employed manual annotation tools that can be used to create manually annotated corpora.</p>
    <p id="Par5">Several works studied the application of NLP technique to process biomedical data on different domains: in [<xref ref-type="bibr" rid="CR11">11</xref>], for example, authors discuss the use of NLP techniques to extract symptoms from EHR, while [<xref ref-type="bibr" rid="CR12">12</xref>] discuss the use of NLP to process oncology medical records. In [<xref ref-type="bibr" rid="CR13">13</xref>], a large language model for EHR is proposed, and in [<xref ref-type="bibr" rid="CR6">6</xref>], authors analyze NLP methods to identify advance care planning documentation in patient clinical notes.</p>
    <p id="Par6">In this context, Relation Extraction (RE) task captured considerable interest in the biomedical community as the knowledge stored in the biomedical data may contain valuable insights about the relationships between entities—e.g., protein–protein, gene–disease, drug–drug, and drug–target interactions. Furthermore, Entity Linking (EL), the task of identifying and disambiguating entity occurrences in unstructured text [<xref ref-type="bibr" rid="CR14">14</xref>], is key to detecting the various textual representations of an entity and capturing its underlying meaning. In [<xref ref-type="bibr" rid="CR4">4</xref>], for example, authors show how EL has numerous benefits, including better use of EHR, improved search and retrieval of biomedical resources, abbreviation disambiguation. In addition to RE and EL, another important theme in the biomedical community is Topic Recognition (TR). Detecting the topics discussed in biomedical text plays a vital role in organizing and classifying the vast amount of information available [<xref ref-type="bibr" rid="CR15">15</xref>]. The most recent proposed techniques to perform RE [<xref ref-type="bibr" rid="CR16">16</xref>–<xref ref-type="bibr" rid="CR22">22</xref>], EL [<xref ref-type="bibr" rid="CR23">23</xref>–<xref ref-type="bibr" rid="CR28">28</xref>] and TR [<xref ref-type="bibr" rid="CR29">29</xref>–<xref ref-type="bibr" rid="CR35">35</xref>] on biomedical texts rely on Machine Learning (ML) models whose performances depend on the availability of large annotated corpora used in training, validation, and test. Creating sizeable and trustworthy manual annotated datasets for the biomedical domain and sub-domains is a time-consuming task requiring people with a high level of expertise [<xref ref-type="bibr" rid="CR6">6</xref>]. In this respect, the creation of these corpora is made even more challenging by the intrinsic diversity in topics and concepts within the biomedical field; the UMLS Metathesaurus [<xref ref-type="bibr" rid="CR36">36</xref>], for example, contains over 3.5 million unique concepts belonging to 127 different semantic types [<xref ref-type="bibr" rid="CR27">27</xref>]. In addition, biomedical terminology is complex, and some terms may have different meanings depending on the context where they are used [<xref ref-type="bibr" rid="CR37">37</xref>]. Some notable examples of large annotated corpora are [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR38">38</xref>–<xref ref-type="bibr" rid="CR45">45</xref>]. Furthermore, some annotated corpora targets the annotation of relationships such as [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR46">46</xref>, <xref ref-type="bibr" rid="CR47">47</xref>]. Given the importance of the annotation task and the significant effort required by experts, several manual annotation tools have been developed explicitly for the biomedical domain [<xref ref-type="bibr" rid="CR48">48</xref>–<xref ref-type="bibr" rid="CR53">53</xref>]. Other tools, on the other hand, are general-purpose [<xref ref-type="bibr" rid="CR54">54</xref>–<xref ref-type="bibr" rid="CR58">58</xref>] and offer different features that are not aligned with the biomedical experts’ needs. The coexistence of multiple annotation tools arises from other tools offering diverse functionalities and targeting various domains of interest.<fig id="Fig1"><label>Fig. 1</label><caption><p>Annotation tools features overview. Each of the selected twenty annotation tools is evaluated based on 24 criteria: 7 technical (T) criteria, three criteria about input and output data formats (D), and 15 criteria concerning the functionalities (F) provided by each tool. The first 22 criteria are taken from [<xref ref-type="bibr" rid="CR10">10</xref>], while we added the last two. Each criterion is marked in blue for each tool if the feature is fully satisfied, light blue if partially satisfied, and white if not satisfied</p></caption><graphic xlink:href="12859_2024_5730_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par7">In [<xref ref-type="bibr" rid="CR10">10</xref>], a review of the widest-used annotation tools is proposed, highlighting the points of strength and weakness of each. They selected 15 annotation tools according to the following criteria:<list list-type="bullet"><list-item><p id="Par8"><italic>Availability</italic>: the tool must be readily available. This criterion ensures that the tool is accessible via a public URL or is downloadable, independently of the user’s expertise. In this respect, the availability of source code is important not only in terms of transparency and accountability but also to guarantee further development and customization according to the user’s needs;</p></list-item><list-item><p id="Par9"><italic>Web-based</italic>: the tool must be a web application. With the term “web application,” we refer to a software application following the most frequently used 3-tiers architecture: a presentation layer handles user interface and interaction, an application layer implements the business logic, and a data layer permanently stores data resulting from or useful to the user interaction [<xref ref-type="bibr" rid="CR59">59</xref>]. Web-based tools can run online. Hence, they can be accessed via a public URL or offline and, in this case, can be installed as web applications. Offline web-based tools require an installation procedure to run on the user’s personal computers or distributed servers. Web-based tools are usually more flexible and solid with the passing of time, and there is a growing emphasis on software not requiring local installations [<xref ref-type="bibr" rid="CR60">60</xref>]; they allow multiple annotators to work collaboratively on the same document in different environments and are platform independent—e.g., other operative systems and different machines.. Conversely, according to [<xref ref-type="bibr" rid="CR10">10</xref>], non-web-based tools are stand-alone systems or plugins running on other tools and platforms. According to two analyses carried out in [<xref ref-type="bibr" rid="CR10">10</xref>], in the past ten years, web-based tools have been more prevalent than stand-alone tools and plugins and are the most frequently used in the annotation of biomedical corpora;</p></list-item><list-item><p id="Par10"><italic>Installability</italic>: offline web-based tools are needed when documents and annotations must be kept private. These tools must be installed on personal computers or distributed servers, and their installation procedure must be finished in less than two hours. The ease and speed of installation and setup are crucial factors that influence the usability of a tool for a diverse range of users with varying backgrounds. Installability is a crucial feature also highlighted by another recent survey of image annotation tools [<xref ref-type="bibr" rid="CR61">61</xref>]; indeed, installation issues are one of the main reasons annotation tools cannot be reused in the field;</p></list-item><list-item><p id="Par11"><italic>Workable</italic>: the tool must be intuitive, and all the features must be comprehensive enough to be used independently of the level of expertise, relying on a well-documented set of instructions and without the help of the developers. This is a pivotal factor that directly impacts the tool’s utilization. Features that are challenging to comprehend render the examined tool impractical; see also [<xref ref-type="bibr" rid="CR61">61</xref>] for further supporting analysis on the usability of annotation tools. Therefore, it is crucial to take into account aspects related to usability and implementation;</p></list-item><list-item><p id="Par12"><italic>Schematic</italic>: the tool allows for schema configuration. In this context, the tool defines elements such as labels, documents, and concepts according to the user’s needs. The tool should not be developed for a specific use-case and should not provide a fixed set of labels, concepts and rules for the annotation. Concerning this point, a tool that does not allow the user to define a schema or is designed for a specific use case will likely face challenges in being reused.</p></list-item></list>Once the tools were chosen according to the aforementioned selection criteria, the authors defined a set of 26 evaluation criteria to compare them. The evaluation criteria can be subdivided into four macro-areas: (i) <italic>Publication</italic>: these features concern the tools’ publications and citations; (ii) <italic>Technical</italic>: these features concern technical aspects of the tool, and are useful to determine the availability of the tool, and the ease of installation; (iii) <italic>Data</italic>: these features describe what formats the tool requires in input and output; (iv) <italic>Functional</italic>: criteria concerning the functionalities provided by the tools. Functional criteria describe all the feature a tool provides—i.e., document-level annotations, availability of overlapping mentions, active learning, collaborative features.</p>
    <p id="Par13">While <italic>Functional</italic> criteria are significant in identifying the primary distinctions among tools and deciding which tool best suits the user’s needs, <italic>Data</italic> criteria, on the other hand, enable users to comprehend the required data formats for each tool. Consequently, users can assess whether their data needs preprocessing. A subset of these criteria has been used also for evaluation purposes in [<xref ref-type="bibr" rid="CR62">62</xref>] that describes an ecosystem for knowledge discovery. We revised the assessment conducted by [<xref ref-type="bibr" rid="CR10">10</xref>], updating <italic>LightTag</italic>, <italic>INCEpTION</italic>, and <italic>MedTAG</italic> according to our experience with each of these tools, adding some tools released after the publication of the paper and including new features we deemed as important for the current trend in bioinformatics.</p>
    <p id="Par14">The heat map we obtained is reported in Fig. <xref rid="Fig1" ref-type="fig">1</xref> with the evaluated annotation tools as rows and the tested criteria as columns. The color intensity of the cells indicates the level of adherence of a tool to each criterion. We evaluated 24 criteria, including 22 criteria (from T1 to F13 in Fig. <xref rid="Fig1" ref-type="fig">1</xref>) from the set of 26 criteria defined in [<xref ref-type="bibr" rid="CR10">10</xref>] and two new criteria (F14 and F15) defined here for the first time. The first six criteria are <italic>Technical (T)</italic>: (T1) date of last version or commit—whether the last version has been released within the past five years; (T2) availability of source code; (T3) online availability; (T4) easiness of installation; (T6) license allowing modification and redistribution; (T7) free of charge. Three criteria concern <italic>Data (D)</italic>: (D1) schema format—whether the schema is configurable; (D2) input format—whether the input documents follow a standard format; (D3) output annotations format—whether the annotations are based on standard formats. Finally, fifteen criteria are <italic>Functional (F)</italic>: (F1) support for overlapping mentions; (F2) support for document-level annotation; (F3) support for relationship annotation; (F4) support for ontologies; (F5) support for built-in prediction and active learning; (F6) integration with PubMed; (F7) suitability for full texts; (F8) support for the partial saving of an annotation (allowing the user to continue the annotation process later); (F9) support for text highlighting; (F10) support for users and teams; (F11) support for Inter Annotator Agreement (IAA); (F12) data privacy; (F13) multilingual support; (F14) connection to ORCID; (F15) retrieval of abstracts from external repositories or services. The use of most of the evaluation criteria from [<xref ref-type="bibr" rid="CR10">10</xref>], ensures that the evaluation analysis we conducted is as objective as possible, avoiding to bias the study towards <italic>MetaTron</italic> strong points. Moreover, we further validated <italic>MetaTron</italic> against these evaluation criteria by the means of two expert-based case studies.</p>
    <p id="Par15">As remarked in [<xref ref-type="bibr" rid="CR10">10</xref>], we confirm that no currently available off-the-shelf tool comprehensively meets all the requirements. This is also evident from Fig. <xref rid="Fig1" ref-type="fig">1</xref>, where the missing features in the majority of selected annotation tools are: support for relationship annotation (F3), support for overlapping mentions (F1), support for document-level annotations (F2), connection to ORCID (F14) and the integration with external repositories and services such as PubMed to retrieve publications’ abstracts (F6, F15).</p>
    <p id="Par16">In this paper, we introduce <italic>MetaTron</italic>, an innovative web-based annotation tool for the biomedical literature which fulfills all the selected evaluation criteria. <italic>MetaTron</italic> is released both as an online instance and as a Docker image deployable on a local server relying on a quick and easy installation procedure. It is fully customizable, as users can upload documents in JSON, PDF, CSV, and TXT or retrieve and upload abstracts from PubMed, Semantic Scholar, and OpenAIRE. The support for both mention-level and document-level annotation types makes <italic>MetaTron</italic> suitable for several use cases. Additionally, <italic>MetaTron</italic> supports automatic built-in predictions.</p>
    <p id="Par17">The rest of the paper is organized as follows: in “<xref rid="Sec2" ref-type="sec">Implementation</xref>” Section we describe <italic>MetaTron</italic> and its features, focusing on the annotation types <italic>MetaTron</italic> provides and <italic>AutoTron</italic> for automatic built-in predictions; in “<xref rid="Sec12" ref-type="sec">Results</xref>” Section we describe the qualitative and quantitative analyses we conducted to evaluate <italic>MetaTron</italic> and compare to other annotation tools; in  “<xref rid="Sec17" ref-type="sec">Conclusions</xref>” Section we draw some final remarks.</p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <sec id="Sec3">
      <title>System overview</title>
      <p id="Par18"><italic>MetaTron</italic> is an annotation tool designed to annotate biomedical documents. One of the key features of <italic>MetaTron</italic> is its support for multiple annotation types. The annotation types can be classified into <italic>document-level annotations</italic> and <italic>mention-level annotation</italic>. <italic>Mention-level annotations</italic> concern the annotation of specific portions of the textual document and comprize <italic>mention, concept linking</italic>, and <italic>relationship</italic> annotations. Mention annotation detects the mentions in a textual document, and each mention can be linked to one or more <italic>concepts</italic> from an ontology—i.e., concepts linking. In this work, we use the terms <italic>entities</italic> and <italic>concepts</italic> interchangeably; in particular, we consider a <italic>concept</italic> as an atomic, identifiable object that has a distinct and independent existence [<xref ref-type="bibr" rid="CR63">63</xref>]. In <italic>MetaTron</italic>, as in the Semantic Web realm, a concept is identified by a URI and described by a name and a type—e.g., gene or disease—making the concept human-understandable. Relationship annotation involves identifying and marking “statements” or “facts” within a text. A statement typically consists of three components: a subject, a predicate, and an object, collectively conveying a specific meaning. It is important to note that the constituents of a statement may be explicitly mentioned in the text, or they can be implicitly understood by considering the surrounding context and their association with ontological concepts.</p>
      <p id="Par19"><italic>Document-level annotations</italic> pertain to considering an entire textual document as a unit. In the <italic>MetaTron</italic> framework, there are two types of document-level annotations: <italic>label</italic> and <italic>assertion</italic> annotations. The former involves assigning one or more labels (representing individual concepts) to the document to classify its content. The latter enables the annotation of a document with a collection of assertions, subject-predicate-object triples linked to ontological concepts. These assertions provide a high-level description of the document’s content. Treating the assertions as machine-readable triples can be incorporated into a Resource Description Framework (RDF) graph, facilitating inference and knowledge representation.</p>
      <p id="Par20"><italic>MetaTron</italic> offers support for ontologies by enabling users to define a collection of ontological concepts identified by an identifier, a name, a type (e.g., <italic>gene</italic> or <italic>disease</italic>) and a description. Concepts are not necessarily tied to a specific ontology: this guarantees more customizability and flexibility, allowing the user to add concepts belonging not only to widely recognized and publicly accessible ontologies but also to user-designed or not yet published ontologies or vocabularies. Concepts can be uploaded in batch or added at the moment of annotation, allowing the user to enrich the set of concepts when needed.</p>
      <p id="Par21">These features of <italic>MetaTron</italic> enable users to annotate a diverse range of biomedical entities and relationships, including genes, proteins, diseases, drug treatments, and their associations. By allowing this customization and flexibility, <italic>MetaTron</italic> can be adapted to suit various use cases and user needs.</p>
      <p id="Par22">Collaborative annotation is a significant aspect of the <italic>MetaTron</italic> system, particularly in the context of collectively annotating a group of documents. This collaborative feature is crucial as it facilitates users to annotate documents together, improving annotation quality and accuracy. By working collaboratively, annotators can save time and enhance the overall annotation process. Through this feature, users can view annotations made by their colleagues and identify annotations that exhibit the highest agreement. Furthermore, the members of the document collection have access to comprehensive statistics about the entire collection or specific documents. This functionality enables them to monitor the progress of annotations and visually analyze the complete set of annotations, considering various annotation types and levels of agreement.</p>
      <p id="Par23">To enhance the user experience and expedite the annotation process, <italic>MetaTron</italic> incorporates <italic>AutoTron</italic>, a feature that offers automated predictions within the system. <italic>AutoTron</italic> is a framework designed to automatically annotate relationships and assertions, allowing users to implement their methods as desired. Leveraging automatic annotations is pivotal in enhancing and accelerating the overall annotation workflow by offering users an initial set of annotations that can be modified. The <italic>AutoTron</italic> system operates as a plug-and-play mechanism, enabling users to integrate their custom automatic annotation methods seamlessly. Additionally, <italic>MetaTron</italic> includes two pre-built methods specifically designed for automatically annotating gene-disease associations and gene expression-cancer associations, further augmenting the automated annotation capabilities.</p>
      <p id="Par24"><italic>MetaTron</italic> is designed to be highly adaptable and can be customized to suit any area of interest. It offers an easy-to-use customization process, where documents can be uploaded in various formats, such as JSON, CSV, TXT, and PDF, using the integration with GROBID (GeneRation Of BIbliographic Data) [<xref ref-type="bibr" rid="CR64">64</xref>]. GROBID is open-source software that uses machine learning techniques to extract structured data such as author names, affiliations, abstracts, publication dates, and references from scientific articles. This feature helps detect the various sections of a publication. Additionally, <italic>MetaTron</italic> incorporates REST APIs such as PubMed, Semantic Scholar, and OpenAIRE to enable users to upload one or more PMIDs (for PubMed) or DOIs (for Semantic Scholar and OpenAIRE) and annotate related information such as the title, abstract, authors, venue, and date of publication.</p>
      <p id="Par25">Additional features of <italic>MetaTron</italic> are the following: (i) an easy-to-use user interface that supports the automatic saving of the annotations; (ii) integration of keyboard shortcuts to navigate between documents and perform new annotations; (iii) support for the download of annotations in JSON, CSV, BioC/XML formats; (iv) support for the upload new annotations from JSON or CSV file; (v) support for user-defined style properties such as the colors of the mentions, the size or the line height of the textual content; (vi) multilingual support; (vii) support for annotation suggestions; (viii) IAA support, implemented through Fliess’ kappa, Cohen’s kappa and majority voting; (ix) dockerized and online instance available, (x) support for multiple ontologies, (xi) connection to ORCID, (xii) support for multiple annotation rounds.</p>
      <sec id="Sec4">
        <title>Architecture</title>
        <p id="Par26">
          <fig id="Fig2">
            <label>Fig. 2</label>
            <caption>
              <p>MetaTron architecture. <italic>MetaTron</italic> has three layers: the <italic>Data layer</italic> (a PostgreSQL database for data and annotations), the <italic>Business layer</italic> (with a REST API and services for automatic annotation and PDF parsing), and the <italic>Presentation layer</italic> (the user interface)</p>
            </caption>
            <graphic xlink:href="12859_2024_5730_Fig2_HTML" id="MO2"/>
          </fig>
        </p>
        <p id="Par27">The system architecture is illustrated in Fig. <xref rid="Fig2" ref-type="fig">2</xref> and can be divided into three layers: a data layer, a business layer, and a presentation layer.</p>
        <p id="Par28">The data layer is based on a PostgreSQL database that stores the annotations data, information about collections, concepts, and documents. This layer is responsible for managing the persistence and retrieval of data and ensuring data integrity.</p>
        <p id="Par29">The business layer comprises a REST API and a business logic implemented using the Django Python web framework.<xref ref-type="fn" rid="Fn1">1</xref> The REST API acts as an intermediary between the presentation layer and the data layer, while the business logic handles and processes data retrieved from the database based on the application’s needs. This layer also utilizes additional services such as <italic>AutoTron</italic> (presented in Sect. <xref rid="Sec8" ref-type="sec">2.2.2</xref>) and GROBID [<xref ref-type="bibr" rid="CR64">64</xref>].</p>
        <p id="Par30">The presentation layer is responsible for displaying the data to the users and receiving their input. It is developed using ReactJS, HTML, CSS, and JavaScript. This layer interacts with the business logic layer through the REST API to retrieve and display the data to the user.</p>
        <p id="Par31">Overall, this architecture provides a clear separation of concerns between the different layers, improving the system’s maintainability, scalability, and modularity. Using a database, a REST API, and additional services enhances the system’s data management and processing capabilities.</p>
      </sec>
      <sec id="Sec5">
        <title>Availability</title>
        <p id="Par32"><italic>MetaTron</italic> is released as an online and dockerized instance. The online instance is available at <ext-link ext-link-type="uri" xlink:href="https://metatron.dei.unipd.it">https://metatron.dei.unipd.it</ext-link>.<xref ref-type="fn" rid="Fn2">2</xref> An online demo and tutorial is available at: <ext-link ext-link-type="uri" xlink:href="https://metatron.dei.unipd.it/demo">https://metatron.dei.unipd.it/demo</ext-link>.</p>
        <p id="Par33">It is intended to be used with scientific publications—e.g., scientific articles or publications in PubMed. To use <italic>MetaTron</italic>, it is necessary to sign up by providing a username, a password, and a profile that identifies the level of expertise. Once signed up, the user will be asked to specify their level of expertise. They can create new document collections and invite other collaborators to join the project.</p>
        <p id="Par34"><italic>MetaTron</italic> is also released as a <italic>Docker container</italic> which guarantees cross-platform portability, scalability, and isolation. Furthermore, the dockerized version can be utilized by users who want to upload collections of documents whose content must be kept private, in all the cases where the network is not fully operational or when users want to introduce new features (i.e., new methods for automatic relation extraction). A local installation is also required if the user works with confidential documents or in a privacy-preserving setting. The dockerized version also eases the installation of <italic>MetaTron</italic> in a private network setting when the documents are unavailable on the Web, but distributed collaboration amongst the annotators is required. The installation procedure is detailed in the <italic>MetaTron</italic> repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/GDAMining/metatron/">https://github.com/GDAMining/metatron/</ext-link>), where the source code is publicly available.</p>
      </sec>
    </sec>
    <sec id="Sec6">
      <title>Annotation interface</title>
      <p id="Par35">
        <fig id="Fig3">
          <label>Fig. 3</label>
          <caption>
            <p>MetaTron user interface. The main annotation interface consists of five distinct blocks: in the main header (<bold>1</bold>), the user can navigate to other web pages and logout; in the document header (<bold>2</bold>), there is the main information about a document; in annotation panel (<bold>3</bold>) it is possible to check the annotation status; the document takes the largest part of the page to annotate (<bold>4</bold>); and in the vertical toolbar (<bold>5</bold>) it is possible to access several functions a user can perform during the annotation process</p>
          </caption>
          <graphic xlink:href="12859_2024_5730_Fig3_HTML" id="MO3"/>
        </fig>
      </p>
      <p id="Par36">The <italic>MetaTron</italic> annotation interface and its features have been designed to be intuitive and facilitate and speed up the entire annotation process. <italic>MetaTron</italic> annotation interface is illustrated in Fig. <xref rid="Fig3" ref-type="fig">3</xref> that we use as a guide to illustrate the tool’s main features. Upon successful login, the system presents the user with the most recently annotated document. At the top of the annotation interface, there is the <italic>main header</italic> (<bold>1</bold>). The <italic>Home</italic>, <italic>Collections</italic>, <italic>Statistics</italic> buttons can be used to navigate to home—i.e., the page illustrated in Fig. <xref rid="Fig3" ref-type="fig">3</xref>, collections, and statistics web pages. The user name is displayed at the top right of the page and can be used to log out. The <italic>document header</italic> (<bold>2</bold>), placed below the main header, includes the information about the current document identifier—e.g., pubmed_27839516—and the related collection name—e.g., pubmed_collection. Two arrow buttons allow the user to navigate between the documents of the collection; it is worth noting that it is also possible to navigate from one document to another through a custom shortcut designed to allow the user to change documents relying only on the keyboard. The <italic>Delete</italic> button deletes the annotations for the current document, while the <italic>Assertion</italic> button creates a new assertion as a document-level annotation.</p>
      <p id="Par37">By clicking on the <italic>Annotation</italic> button, a drop-down menu appears, enabling the user to choose one or more annotation types. For each selected annotation type, the list of annotations is then displayed on the right-hand side of the annotation interface, in the <italic>annotation panel</italic> (3). The user can view, add, modify, or delete their annotations using this panel. The largest portion of the page is taken by the textual document (4) the user annotates. The <italic>vertical toolbar</italic> (5) provides a set of functionalities illustrated in Fig. <xref rid="Fig3" ref-type="fig">3</xref> from (A) to (K), that can be accessed directly from the main annotation interface improving, and speeding up the annotation procedure, and minimizing the number of actions to be performed. In (A), the list of documents of the collections is shown. The user can filter documents based on whether they contain at least one annotation and search for a specific document to annotate using its ID. In (B), the user can view a list of document collections available for annotation. Each collection is represented by a button displaying the collection name and the percentage of annotated documents. The button color varies based on the percentage of documents that have been annotated—e.g., green color is used when the user annotated more than 80% of documents, while red color is used when the user annotated less than 20% of documents. By clicking on a collection, it is possible to start annotating its documents. In (C), the users who annotated the current document are listed. It is possible to load the annotation of one of the users in the list by clicking on the associated username. (D) allows the user to open two tables containing <italic>personal</italic> and <italic>global</italic> annotations statistics overview for the current document; the former concerns the annotations of the current user, while the latter those of all the annotators. Each table contains the number of mentions, associations, mentions-concepts, relationships, assertions, and labels annotated. In the <italic>global</italic> overview, the agreement computed with the Fleiss’ kappa measure is provided. (E) allows the user to customize <italic>MetaTron</italic>. It is possible to: hide or display specific sections of the document, increase or decrease the font size and the line height, and set the color associated with each concept type. These settings have been defined to facilitate and speed up the annotation workflow and improve document readability. (F) enables to download of the annotations. The user has to specify the file format—e.g., JSON, CSV, BioC, the annotation type—e.g., mentions, concepts, relationships, and the annotator username, choosing between all the users who annotated that document. (G) opens an upload panel where only the user who created the collection can upload new lists of documents and ontological concepts. Documents can be uploaded in several formats: CSV, TXT, JSON, and PDF. The user can search for a specific publication in PubMed (by providing the related PMID), in Semantic Scholar, and OpenAIRE (by providing the related DOI): <italic>MetaTron</italic> takes advantage of their REST APIs to retrieve the title, the abstract, the authors, the date of publication, and the venue information. The user can upload a new set of annotations in CSV or JSON format that will be automatically loaded in <italic>MetaTron</italic>. In (H), the user can hide the ontological concepts associated with the mentions and visualize only the annotated mentions increasing the document’s readability. In (I), it is possible to rely on <italic>AutoTron</italic> to annotate the current document automatically for specific cases. (J) and (K) open new tabs with the <italic>MetaTron</italic> instructions, and the credits respectively.</p>
      <sec id="Sec7">
        <title>Manual annotation</title>
        <p id="Par38"><italic>Mention annotation</italic> Mentions are textual spans that can be linked to one or more ontological concepts. In <italic>MetaTron</italic>, a mention can either consist of one or more consecutive tokens (or words), where a token is a sequence of characters between two spaces or a substring of one or more contiguous tokens. In this case, the first or last character of the mention does not necessarily coincide with the character that follows or precedes a space. It is possible to select a mention that consists of two or more consecutive tokens by clicking on the first and the last words, respectively: all the words comprised between the two selected will be part of the new mention. To create a mention containing a single token, it is possible to double-click on the desired token. To annotate a substring, hence selecting a part of the token (or two or more consecutive tokens), it is possible to drag and drop the mouse from the first to the last characters of the substring. <italic>MetaTron</italic> allows for the selection of overlapping mentions, meaning that a piece of text already included in a mention can be selected.</p>
        <p id="Par39">This implementation is based on our direct experience with other annotation tools, which has allowed us to assess the pros and cons of various possible implementations. We have decided to implement the annotation of mentions both through drag and drop and by clicking on individual words to allow the user to annotate both specific textual substrings and mentions of two or more words quickly, streamlining and expediting the workflow.</p>
        <p id="Par40">When the textual content of a mention occurs more than once in the textual document, it is possible to annotate all the mentions simultaneously. When a mention is annotated, a new modal will appear if its content occurs more than once, and the user can decide to annotate the mentions altogether.</p>
        <p id="Par41">It is possible to access the <italic>mention panel</italic> illustrated in Fig. <xref rid="Fig4" ref-type="fig">4</xref> by performing a right-click on a mention. This panel includes all possible actions and annotations related to the selected mention. From this panel, users can get information about the mention—e.g., the date of annotation and the number of annotators who annotated that mention for that document, receive some suggestions about the concepts to link to the mention, perform new annotations—i.e., add a new concept or a new relationship, and delete the mention. The option <italic>Annotate all</italic> finds all the occurrences of a mention in the document and annotates them simultaneously.<fig id="Fig4"><label>Fig. 4</label><caption><p>MetaTron mention panel. The mention panel opens when the user right-clicks on the desired mention. It allows the user to get more information about the mention, receive suggestions about the concepts to link, add new concepts or relationships, annotate similar mentions, and delete the mention</p></caption><graphic xlink:href="12859_2024_5730_Fig4_HTML" id="MO4"/></fig></p>
        <p id="Par42"><italic>Concepts linking</italic> To link a new ontological concept to a mention, the user can open the mention panel of a mention and select the <italic>Add Concept</italic> option. A new modal will appear, allowing the user to select a concept. Each collection of documents has a list of user-defined ontological concepts, not necessarily tied to a single ontology. From the modal, the user can explore this set of concepts, filter them according to their type, name, and identifier, and view the associated description. If the list of ontological concepts of the collection is large, to aid the user in selecting a concept, <italic>MetaTron</italic> provides auto-completion facilities to find the desired concept easily.</p>
        <p id="Par43">The incorporation of concept annotations in our system mirrors the approach employed by various tools, such as <italic>brat</italic> and <italic>INCEpTION</italic>. Nevertheless, we observed that our implementation is characterized by its intuitiveness. By positioning the concept above the corresponding mention and employing distinct colors based on the type, users can readily discern between different concepts, thereby augmenting the immediacy of the annotation experience.</p>
        <p id="Par44">Alternatively, if the concept is unavailable in the provided list, the user can define a new concept that will be automatically added to the collection’s concepts list. In this case, the user must define the concept’s type, name, URI (or ID), and an optional description.<fig id="Fig5"><label>Fig. 5</label><caption><p>MetaTron concept selection modal. The concept selection modal allows the user to select a concept to link to a mention. The user can filter the concepts according to the concept type and use auto-completion facilities to filter further the list of concepts (<bold>A</bold>). Once a concept is selected, its description will automatically appear in the modal (<bold>B</bold>)</p></caption><graphic xlink:href="12859_2024_5730_Fig5_HTML" id="MO5"/></fig></p>
        <p id="Par45">In Fig. <xref rid="Fig5" ref-type="fig">5</xref>, we can see how to link a concept to a mention. The user can filter the concepts to choose from, specifying the concept type—i.e., <italic>disease</italic> in the figure. By typing the first letters of the desired concepts, the available options are automatically shown (A in Fig. <xref rid="Fig5" ref-type="fig">5</xref>). The required information typically consists of the concept type and the name to select a concept. The user is required to select the ID of the concept only if two or more concepts share the same name but with different URIs. Once a concept is selected, the related description will automatically appear (B in Fig. <xref rid="Fig5" ref-type="fig">5</xref>). If no option is shown, the concept is not on the list, and it is possible to add a new concept.</p>
        <p id="Par46">If a mention is linked to one or more concepts, by clicking on the <italic>Annotate all</italic> option of the mention panel, it is possible to locate all the instances of that mention in the document and associate them with the same set of concepts simultaneously.</p>
        <p id="Par47">Each mention can have more than one linked concept. The concepts linked to a mention are displayed above the mention; they are clickable so that the user can be provided with the information about the concept—i.e., the URI, the name, the description, and the type. In the annotation interface, each concept can have a different color depending on its type: in Fig. <xref rid="Fig3" ref-type="fig">3</xref>, for example, concepts of type <italic>Disease</italic> and the associated mentions are highlighted in pink, while <italic>Gene</italic> type concepts in dark blue.<fig id="Fig6"><label>Fig. 6</label><caption><p>Relationship annotation. When the user annotates a new relationship, the document’s content is blurred except for the mentions highlighted with different colors, which can be selected as subject, predicate, and object, respectively. The three steps to create a relationship are shown in the example examined. First, the object mention is selected from the menu (<bold>1</bold>); then, it is selected a predicate (<bold>2</bold>): by clicking on <bold>A</bold>, it is possible to manually type the predicate of the relationship, while by clicking on <bold>B</bold> it is possible to select a concept. In (<bold>3</bold>), the final relationship is represented: it comprises two mentions—the subject and the object and an ontological concept—the predicate. Each relationship element has a different color to make each element easily recognizable. The panel on the right allows the user to update and save the relationship</p></caption><graphic xlink:href="12859_2024_5730_Fig6_HTML" id="MO6"/></fig></p>
        <p id="Par48"><italic>Relationship annotation</italic> A relationship comprizes three primary components: a <italic>subject</italic>, a <italic>predicate</italic>, and an <italic>object</italic>; the relationship always starts from the subject and ends with the object. Each relationship element can be represented by either an ontological concept or a textual mention (with or without linked concepts). At least one of the three components of a relationship must be a mention.<xref ref-type="fn" rid="Fn3">3</xref> A new relationship can be added through the <italic>Add Relationship</italic> option of the mention panel of a mention. By default, this mention will be the subject of the relationship. This action automatically shows a <italic>relationship panel</italic> that provides a comprehensive overview of the relationship and its components. All the other mentions composing the relationship (if any) can be added by clicking on each mention, or by right-clicking on the desired mention, it is possible to select its role. It is worth noting that it is always possible to change the role of a mention—e.g., a mention which was the subject can become the object. From the relationship panel, it is possible to select the concepts of the relationship by clicking on the <italic>Add predicate</italic>, <italic>Add subject</italic>, or <italic>Add object</italic> buttons. In Fig. <xref rid="Fig6" ref-type="fig">6</xref>, the creation of a new relationship is illustrated. The relationship comprises two mentions (the subject BRCA1 and the object breast cancer) and an ontological concept (the predicate Oncogenes). In <bold>1</bold>, the object mention is selected by declaring its role from the menu. In <bold>2</bold>, the predicate is selected. There are two ways to select a predicate: from <bold>A</bold>, it is possible to input a string manually that represents the predicate, while from <bold>B</bold>, it is possible to select a predicate concept. In <bold>3</bold>, the created relationship is shown. In the textual document, each relationship component has a different color depending on its role: the subject is highlighted in red, the predicate in green, and the object in orange. The three components are linked via arrows whose positions can be changed by the user to facilitate the document readability—i.e., each mention is surrounded by four points that determine the points where the arrows can start or end.</p>
        <p id="Par49">On the right, the <italic>relationship panel</italic>, for each relationship component, shows the type of the component—e.g., mention or concept, and its role—e.g., subject, predicate, object. The annotations panel provides an overview of all the relationships the user annotates; each relationship can be viewed, edited, and deleted directly from the interface.</p>
        <p id="Par50">In this implementation, each relationship component is manually chosen, whether a mention selected in the text or a concept chosen relying on the right panel. This approach, even though it may require more steps than previous annotations, allows the users to designate which element within the relationship should be identified as a mention and which as a concept. Unlike <italic>brat</italic> and <italic>INCEpTION</italic> that enable the annotation of relationships, <italic>MetaTron</italic> does not necessitate the subject and object to be mentions within the textual document. Relationships may span multiple sentences; at least one among subject, predicate and object must be a mention, while the remaining components can be concepts.<fig id="Fig7"><label>Fig. 7</label><caption><p>Relationships list. Overview of the relationships annotated by the user. The list is subdivided into subject, predicate, and object, each into concept types. It is also possible to filter the concepts composing the relationships according to the type and the name, taking advantage of auto-completion facilities</p></caption><graphic xlink:href="12859_2024_5730_Fig7_HTML" id="MO7"/></fig></p>
        <p id="Par51">In Fig. <xref rid="Fig7" ref-type="fig">7</xref>, we can see the relationships list. It is subdivided into three categories: subject, predicate, and object; each category is further subdivided into concept types. This layout provides the user with an overview of the different concept types characterizing subjects, predicates, and objects of the annotated relationships.</p>
        <p id="Par52"><italic>Assertions annotation</italic> Like relationships, assertions consist of a subject, predicate, and object, each represented by an ontological concept unlinked to any mention in the document. To add a new assertion (see Fig. <xref rid="Fig8" ref-type="fig">8</xref>), a dedicated button in the document header opens an <italic>assertion panel</italic> similar to the one provided for relationships. Users can create a new assertion by specifying the types, names, and URIs of the subject, predicate, and object concepts. The annotated assertions can be viewed, edited, and removed via the annotation panel by selecting the assertion annotation type; the annotation panel contains the assertions the user created. For each assertion, the subject, the predicate, and the object concepts are provided with information, including the date and number of annotators. Furthermore, each assertion can be viewed, edited, or deleted. As far as our current knowledge extends, <italic>MetaTron</italic> stands out as the initial tool to introduce the creation of document-level assertions. Therefore, we have opted to maintain a close resemblance between assertion annotations and relationship annotations. This decision aims to facilitate a seamless user experience, enabling users to bypass the need to acquaint themselves with a novel annotation methodology and expedite the overall annotation workflow.<fig id="Fig8"><label>Fig. 8</label><caption><p>Assertion details. The annotation panel contains an overview of the annotated assertions. For each assertion, the subject, the predicate, and the object concepts are provided; each assertion can be edited or deleted via the two buttons placed near the <italic>Assertion 1</italic> title. The date of annotation and the number of annotators are shown below the assertion</p></caption><graphic xlink:href="12859_2024_5730_Fig8_HTML" id="MO8"/></fig></p>
        <p id="Par53"><italic>Labels annotation</italic> Labels allow the user to classify the document. Each collection has its own set of labels specified at its creation. To add one or more labels to the document, the user must open the annotation panel and select the labels annotation type. Each label is a button that can be selected or selected by a click. In Fig. <xref rid="Fig9" ref-type="fig">9</xref>, the annotation panel shows an example of the labels that can be associated with the displayed document for the selected collection. The selected labels have a light blue background.<fig id="Fig9"><label>Fig. 9</label><caption><p>Labels annotation. The annotation panel contains the labels to assign to the document. Each label is a button that can be selected or deselected. All the selected labels have a colored background; the others have a transparent background</p></caption><graphic xlink:href="12859_2024_5730_Fig9_HTML" id="MO9"/></fig></p>
      </sec>
      <sec id="Sec8">
        <title>AutoTron: automatic annotations</title>
        <p id="Par54"><italic>AutoTron</italic> represents the automatic annotation component of <italic>MetaTron</italic>. As an automatic component, <italic>AutoTron</italic> can be implemented by the user at will. The only requirement lies on the I/O structure, where specific I/O data are required to integrate the component within <italic>MetaTron</italic>. Below, we first describe the general architecture of the <italic>AutoTron</italic> component and then present two different implementations used in two annotation tasks: document-level Gene-Disease Association (GDA) extraction and sentence-level Gene expression-Cancer Association (GCA) extraction. Both implementations are currently available in <italic>MetaTron</italic> and can be used by the user on the corresponding annotation tasks. Figure <xref rid="Fig10" ref-type="fig">10</xref> shows the <italic>AutoTron</italic> workflow to extract GCAs from a sample PubMed article (i.e., PubMed id: 24662820). The workflow involves three steps. First, users select the desired task (1), which in this case is GCA. Once the user clicks the <italic>annotate</italic> button, a loading icon indicates that the automatic annotation process is underway (2). Finally, the annotations generated by <italic>AutoTron</italic> are displayed to the user (3).<fig id="Fig10"><label>Fig. 10</label><caption><p>AutoTron workflow. Overview of the workflow to extract GCAs from PubMed article 24662820. In (1), users select the task (i.e., GCA). Then, once the annotation process starts, a loading icon indicates the process is underway (2). Finally, the generated annotations are displayed to users in (3)</p></caption><graphic xlink:href="12859_2024_5730_Fig10_HTML" id="MO10"/></fig></p>
        <p id="Par55"><italic>Architecture</italic><italic>AutoTron</italic> consists of two main components and specific I/O data requirements. The main components are EL and RE. EL assigns unique meanings to entities mentioned within text [<xref ref-type="bibr" rid="CR65">65</xref>, <xref ref-type="bibr" rid="CR66">66</xref>], whereas RE identifies and extract relations between linked entities mentioned in text [<xref ref-type="bibr" rid="CR67">67</xref>, <xref ref-type="bibr" rid="CR68">68</xref>]. The EL and RE modules are containers where different methods can be plugged in/out, but which must adhere to specific I/O data. In this regard, the I/O data consists of specific fields. In input, <italic>AutoTron</italic> requires a field containing the text to annotate. In the output, <italic>AutoTron</italic> provides, for each extracted relationship or assertion, the subject, predicate, and object concept IDs, names, and types, as well as the corresponding mention positions within the text (if any). Together, these fields allow <italic>MetaTron</italic> to seamlessly integrate any implementation of the <italic>AutoTron</italic> component. In other words, <italic>AutoTron</italic> represents a framework for automatically annotating relationships and assertions that users can implement at will.</p>
        <p id="Par56"><italic>Entity linking</italic> We consider different EL systems depending on the input text. When the input text comes from PubMed, we use the PubTator system [<xref ref-type="bibr" rid="CR69">69</xref>–<xref ref-type="bibr" rid="CR71">71</xref>]. PubTator provides automated annotations from state-of-the-art text mining systems for genes/proteins, genetic variants, diseases, chemicals, species, and cell lines. In particular, PubTator normalizes annotated genes to NCBI Gene [<xref ref-type="bibr" rid="CR72">72</xref>] identifiers and annotated diseases to MeSH [<xref ref-type="bibr" rid="CR73">73</xref>] identifiers. When the input text comes from sources different than PubMed, we use the MetaMapLite system [<xref ref-type="bibr" rid="CR74">74</xref>], a near real-time EL tool that identifies UMLS [<xref ref-type="bibr" rid="CR36">36</xref>] concepts within the biomedical text. MetaMapLite returns, among other information, the CUI, the preferred term, and the location in the text of the identified UMLS concepts.</p>
        <p id="Par57">The text annotated by EL systems is then passed to RE methods to perform GDA/GCA extraction.</p>
        <p id="Par58"><italic>GDA extraction</italic> The discovery of GDAs is one of the most pressing challenges to advance precision medicine and drug discovery [<xref ref-type="bibr" rid="CR75">75</xref>]. Therefore, the automatic extraction and curation of GDAs is pivotal to advancing precision medicine and providing knowledge to assist disease diagnostics, drug discovery, and therapeutic decision-making. To this end, we use a document-level RE method that adopts Multi-Instance Learning (MIL) to extract GDA assertions from text [<xref ref-type="bibr" rid="CR76">76</xref>, <xref ref-type="bibr" rid="CR77">77</xref>]. Under MIL, text sentences are divided into bags based on pairs of concepts, and the prediction of relations (i.e., predicates) occurs at the bag level. The use of MIL well suits the assertion annotation task, where subject, predicate, and object are not associated with mentions. As the underlying ML model, the RE method exploits Piecewise Convolutional Neural Network (PCNN) model [<xref ref-type="bibr" rid="CR21">21</xref>]. PCNN first encodes sentences using a CNN and then applies a piecewise max pooling operation. This operation divides each sentence into three segments based on the positions of the two given entities and returns the maximum value in each segment instead of a single maximum value over the entire sentence. For MIL, the RE method performs average-based aggregation. This aggregation strategy assumes that all sentences within the same bag contribute equally to the bag-level representation. In other words, the bag representation is the average of all its sentence representations.</p>
        <p id="Par59">We trained the method on the TBGA dataset [<xref ref-type="bibr" rid="CR78">78</xref>], a large-scale, semi-automatically annotated dataset for GDA extraction. TBGA contains over 200,000 instances and 100,000 bags, divided into four GDA types: Therapeutic, Biomarker, Genomic Alterations, and NA. Once trained, the RE method is deployed within <italic>AutoTron</italic>.</p>
        <p id="Par60"><italic>GCA extraction</italic> Cancer prevention is one of the century’s most pressing challenges that public health needs to face. In the last few years, the rise of microarray and next-generation sequencing technologies triggered the generation of large amounts of raw experimental data about gene expression-cancer interactions. These raw data require investigation, processing, and validation by experts to be used to guide diagnosis, assess prognosis, or predict therapy response [<xref ref-type="bibr" rid="CR75">75</xref>, <xref ref-type="bibr" rid="CR79">79</xref>]. The outcomes of experts’ analyses are (often) described in scientific publications in the form of GCAs. However, manual knowledge extraction requires high economic and time costs [<xref ref-type="bibr" rid="CR80">80</xref>–<xref ref-type="bibr" rid="CR82">82</xref>]. Thus, it is of paramount importance to assist manual GCA extraction through the use of automated methods. In this regard, we use a sentence-level RE method that combines the outcomes of different models to obtain the corresponding GCA. Specifically, the method combines three ML models, each of which predicts a specific aspect associated with GCAs. The considered aspects are the Change of Gene Expression (CGE), the Change of Cancer Status (CCS), and the Gene-Cancer Interaction (GCI). Once predicted, the different aspects are combined—following a set of inference rules defined in [<xref ref-type="bibr" rid="CR47">47</xref>, <xref ref-type="bibr" rid="CR83">83</xref>]—to infer the role the given gene has on the specific cancer disease. All the ML models adopt SciBERT [<xref ref-type="bibr" rid="CR84">84</xref>], a pre-trained language model based on BERT [<xref ref-type="bibr" rid="CR85">85</xref>]. SciBERT addresses the lack of high-quality, large-scale labeled scientific data by pretraining on scientific papers from Semantic Scholar [<xref ref-type="bibr" rid="CR86">86</xref>]. On top of it, a linear layer takes SciBERT pooled output. Predictions are scores in [0, 1]; the higher the score for an aspect value, the more the model believes the sentence expresses that particular (aspect) value.</p>
        <p id="Par61">The method has been used to build a large-scale Knowledge Base (KB) on GCAs [<xref ref-type="bibr" rid="CR87">87</xref>, <xref ref-type="bibr" rid="CR88">88</xref>]. In this work, we deploy the method as is within <italic>AutoTron</italic>.</p>
      </sec>
    </sec>
    <sec id="Sec9">
      <title>Collections and customization</title>
      <p id="Par62">
        <fig id="Fig11">
          <label>Fig. 11</label>
          <caption>
            <p>MetaTron collections interface. Overview of the main components of the collection page. The user can search for a specific collection or filter the collections list at the top-right. The <italic>Add collection</italic> button allows the user to create a new collection. The collections list takes up the largest part of the page. Each collection includes information that the collection’s creator can edit</p>
          </caption>
          <graphic xlink:href="12859_2024_5730_Fig11_HTML" id="MO11"/>
        </fig>
      </p>
      <p id="Par63">Collections are sets of documents that one or more users can annotate. The collection web page is illustrated in Fig. <xref rid="Fig11" ref-type="fig">11</xref>, and is accessible through the annotation interface by clicking the <italic>Collections</italic> button.</p>
      <p id="Par64">At the top of the collection page is a text field that allows users to search for a specific collection by its name. Additionally, the four buttons below enable users to filter the collections. The default filter is <italic>All</italic>, which provides users with a list of all the collections they can annotate. <italic>Created</italic> button shows collections created by the user, while <italic>Shared</italic> displays collections that the user can annotate and be created by another team member. Lastly, the <italic>Invited</italic> button shows collections the user has been asked to join as an annotator. A new collection can be created via the <italic>Add Collections</italic> button. Finally, in the remaining part of the page, the user’s collections (either the entire or filtered list) are listed. For each collection, the following information is provided in this order: the creator, the collection’s name, the date of creation, the number of documents, the number of documents annotated by at least one user, and the description; by clicking on <italic>Learn More</italic> button, located below the collection, the information about the annotators of the collection, and the list of labels are loaded. If the user is also the collection’s creator, they can add or remove one or more annotators and add new labels. The <italic>New Round</italic> button allows the user to create a new annotation round. Depending on the annotation task, the users can perform one or more rounds of annotation; hence, they annotate the collection documents several times. The collection’s creator can also decide on the annotators of each round so that different sets of annotators can contribute to different rounds. This option automatically duplicates the annotations of the last round and makes them available in a new collection. The annotators who access this new collection are provided with all the annotations they performed in the last annotations round for that collection. Encapsulating each round on a separate collection of documents and annotations allows each round to be independent of all the other rounds and facilitates the annotators’ work. The <italic>Documents</italic> button redirects the user to the collection’s documents web page containing the list of documents of the collection together with its annotations—a more detailed description of this table is provided in Sect. <xref rid="Sec10" ref-type="sec">2.4</xref>. The <italic>Annotate</italic> button allows the user to annotate that collection: the user will be automatically redirected to the annotation interface and provided with the last annotated document of the collection (if any, otherwise with the first document available). Only the creator is provided with the <italic>Delete</italic> button, which allows for the deletion of the entire collection and the related annotations.<fig id="Fig12"><label>Fig. 12</label><caption><p>MetaTron new collection form. This figure shows the information a user must provide to create a new collection of documents. The form must provide the collection’s name and description, a list of concepts, labels, and documents</p></caption><graphic xlink:href="12859_2024_5730_Fig12_HTML" id="MO12"/></fig></p>
      <p id="Par65">In Fig. <xref rid="Fig12" ref-type="fig">12</xref>, the form to add a new collection is illustrated. A user must provide the collection’s name and description to create a new collection. Then, the user is asked to provide a list of members authorized to annotate the new collection. This is not mandatory since a user may decide to work independently on a collection. The user is asked to add a list of labels necessary to perform label annotation. Also, in this case, adding a list of labels is not mandatory at the moment of collection creation. The creator can edit the collection’s annotators and labels at any moment. Uploading a set of ontological concepts is highly recommended to perform concept linking, relationship annotation, and assertion annotation. Ontological concepts must be uploaded in CSV or JSON files, and for each concept, it is mandatory to provide the URI (or ID), the name, and the type; a description is not mandatory but recommended. Since the files introducing new concepts must follow predefined structures, we provided two downloadable templates (one for the CSV and one for the JSON formats). It is worth noting that the set of provided ontological concepts is not tied to a specific ontology, allowing the user to fully customize the collection’s configuration with concepts belonging to different ontologies, which may also not be publicly available. The insertion of new concepts is always possible at any moment.</p>
      <p id="Par66">A collection must contain at least one document. The creator can upload one or more files in the following formats: JSON, CSV, TXT. The keys in JSON files and the CSVs’ headers will also be available for annotation. <italic>MetaTron</italic> supports uploading PDF files automatically parsed by GROBID.</p>
      <p id="Par67">To retrieve information about articles having a PMID, <italic>MetaTron</italic> relies on the PubMed REST API to obtain the article’s title, abstract, date of publication, authors, and venue. For publications with a DOI, <italic>MetaTron</italic> utilizes the REST APIs of OpenAIRE and Semantic Scholar to extract the same information. The information obtained from the REST APIs is extracted, processed, and presented to the collection users as documents that can be annotated.</p>
      <p id="Par68">By default, <italic>MetaTron</italic> provides the user with the entire document to annotate. However, the user can select specific parts of the document using the <italic>settings</italic> option in the vertical toolbar of the annotation interface.</p>
    </sec>
    <sec id="Sec10">
      <title>Collaborative features</title>
      <p id="Par69">
        <fig id="Fig13">
          <label>Fig. 13</label>
          <caption>
            <p>MetaTron documents collection. Overview of the documents web page related to a collection of interest. Each document is the row of a table; each row contains the information about a document, the annotators, and the total count of annotations performed for each annotation type. For each annotation type, it is possible to see an overview of the related annotations. The last column contains buttons allowing the user to download, visualize and delete the document</p>
          </caption>
          <graphic xlink:href="12859_2024_5730_Fig13_HTML" id="MO13"/>
        </fig>
        <fig id="Fig14">
          <label>Fig. 14</label>
          <caption>
            <p>MetaTron relationships overview modal. The modal shows the subject, predicate, and object elements for each relationship annotated in the document. The mention text and the location in the text are reported; for each concept, the type, name, and URI are reported. Finally, the modal shows the number of annotators and their usernames</p>
          </caption>
          <graphic xlink:href="12859_2024_5730_Fig14_HTML" id="MO14"/>
        </fig>
        <fig id="Fig15">
          <label>Fig. 15</label>
          <caption>
            <p>Linked concepts suggestions. The suggestion modal provides a list of concepts linked to a specific mention by the other annotators of the document. For each concept, the related information is provided; the user can link the suggested concept(s) by clicking the <italic>Accept</italic> button associated with the desired concepts</p>
          </caption>
          <graphic xlink:href="12859_2024_5730_Fig15_HTML" id="MO15"/>
        </fig>
      </p>
      <p id="Par70"><italic>The documents web page. </italic> The user can keep track of the collection’s annotation state via the documents web page, accessible by clicking on the <italic>Documents</italic> button under each collection’s set of information. The documents web page, illustrated in Fig. <xref rid="Fig13" ref-type="fig">13</xref>, contains a dynamic table where, for each document, general information is provided—e.g., the ID, the batch number, and the number of annotations categorized by annotation type. Finally, the last column is the same for all the documents and allows one to visualize the text of the document, download its annotations, and, if the user is the creator of the collection, delete the document and the related annotations. In addition, it is possible to view all the annotations performed for each type, together with the related annotators. An example is provided in Fig. <xref rid="Fig14" ref-type="fig">14</xref>, where the overview of the relationships annotated for a document in the list is shown. Each relationship is subdivided into subject, predicate, and object components. If one of the components is a mention, the related text is reported along with the section in the text where it has been found—e.g., the abstract, the starting, and ending indices (this information is under the <italic>location</italic> field). If one of the components is an ontological concept—unlinked to any mention, instead, it is reported its type, its name (named as <italic>concept</italic>), and its URI (or ID). Finally, the number of annotators and their usernames are reported. This feature allows the user to have a complete overview of the relationship. Furthermore, the information about the annotators is an important indicator of the annotator’s expertise and the reliability of the annotation.</p>
      <p id="Par71"><italic>Load the teammates’ annotations</italic> The user can load a particular teammate’s annotation for a document directly from the vertical toolbar (C in Fig. <xref rid="Fig3" ref-type="fig">3</xref>). Once loaded, the user can copy one or more annotations from the teammate, resulting in both the user and the teammate having the same (or partial) set of annotations.</p>
      <p id="Par72">This feature has been implemented to allow the user to visualize and interact with other members’ annotations. The possibility to copy other members’ annotations facilitates and speeds up the annotation process as the user does not have to create new annotations from scratch.</p>
      <p id="Par73"><italic>Receiving suggestions</italic> By accessing the <italic>Suggestion</italic> option in the panel of a mention, it is possible to visualize the list of ontological concepts that the other annotators linked to that mention. Figure <xref rid="Fig15" ref-type="fig">15</xref> shows an example of a suggestion modal. The concept type, name, URI (or ID), and the number of annotators are provided for each concept. The <italic>Accept</italic> button under placed a concept allows the user to assign that ontological concept to the mention; the <italic>Close</italic> button discards the suggestion. <italic>MetaTron</italic> allows users to link more than one suggested ontological concept to the same mention.</p>
    </sec>
    <sec id="Sec11">
      <title>Inter annotator agreement (IAA) and statistics</title>
      <p id="Par74"><italic>Inter annotator agreement (IAA)</italic><italic>MetaTron</italic> implements two IAA methods: majority voting, Fleiss’ kappa and Cohen’s kappa. The first method selects all annotated annotations by more than half of the document’s annotators. In <italic>MetaTron</italic>, viewing and editing the annotations selected through majority voting is possible. The majority voting-based annotations have two goals: (i) providing the user with information about the most frequent annotations, and (ii) facilitate and speed up the annotation process. The user can copy all the annotations selected via majority voting and edit them if needed; this allows the annotator to receive an initial set of annotations, consequently saving time. It is possible to load these annotations directly from the vertical toolbar (C in Fig. <xref rid="Fig3" ref-type="fig">3</xref>): they can be loaded by clicking on the user called <italic>IAA - Inter Annotator Agreement</italic>.</p>
      <p id="Par75">Fleiss’ kappa is a statistical measure that assesses the level of agreement between two or more annotators [<xref ref-type="bibr" rid="CR89">89</xref>]. In <italic>MetaTron</italic>, two Fleiss’ kappa agreement values are computed for each annotation type: one concerns the entire collection of documents, and one concerns each single document. It is possible to check Fleiss’ kappa agreement values on the <italic>Statistics</italic> web page, whose details are provided in the paragraph below.</p>
      <p id="Par76">Cohen’s kappa is a statistical measure ranging between <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-1$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2024_5730_Article_IEq1.gif"/></alternatives></inline-formula> and 1 that assesses the level of agreement between two raters rating the same set of elements [<xref ref-type="bibr" rid="CR90">90</xref>]. Similarly to Fleiss’ kappa, we provide the Cohen’s kappa for each annotation type; it is computed for the entire collection or each document, according to the users’ needs.</p>
      <p id="Par77">It is always possible to introduce new agreement functions according to the users’ needs and requirements: <italic>MetaTron</italic> is indeed flexible, and its modular design allows for integrating new functions and measures.</p>
      <p id="Par78">Having Fleiss’ kappa and Cohen’s kappa values provides a more complete view of annotation quality. Fleiss’ kappa can show how agreement varies with a different number of annotators, while Cohen’s kappa offers a more specific assessment between pairs of annotators. Using both coefficients ensures a more accurate evaluation of annotation reliability, considering the variety of annotators involved and the specific agreement between pairs of annotators.<fig id="Fig16"><label>Fig. 16</label><caption><p>Statistics tool bar. The statistics tool bar allows to select different types of statistics. In 1 and 2 it is possible to switch between personal and global statistics. In 3 it is possible to select two annotators and get the Cohen’s kappa agreement between them; in 4 the agreement amongo multiple annotation rounds is provided; in 5 it is possible to select one document and check its statistics</p></caption><graphic xlink:href="12859_2024_5730_Fig16_HTML" id="MO16"/></fig><fig id="Fig17"><label>Fig. 17</label><caption><p>Cohen’s kappa agreement. The Cohen’s kappa agreement is provided for a pair of annotators selected by the user. The user can decide to compute the Cohen’s kappa basing on a single document or on the entire collection</p></caption><graphic xlink:href="12859_2024_5730_Fig17_HTML" id="MO17"/></fig><fig id="Fig18"><label>Fig. 18</label><caption><p>Fleiss’ kappa rounds agreement. This modal provides a table where, for each round, it is provided the Fleiss’ kappa for each type. The user can select the document or, alternatively, can check the agreement computed on the entire collection</p></caption><graphic xlink:href="12859_2024_5730_Fig18_HTML" id="MO18"/></fig><fig id="Fig19"><label>Fig. 19</label><caption><p>General statistics. The table at the left shows each annotation type, the total count of annotations, and the inter-annotator agreement. The second table concerns the concepts linking annotation type and for each concept type, it is possible to see how many concepts have been linked and the related name. Finally, the last table shows, for each concept type, the number of concepts (both unlinked and linked to a mention) taking part in a relationship (or an assertion) and whose role is subject, predicate, and object, respectively</p></caption><graphic xlink:href="12859_2024_5730_Fig19_HTML" id="MO19"/></fig><fig id="Fig20"><label>Fig. 20</label><caption><p>Annotations and annotators statistics. The first histogram illustrates for each document of the collection, how many annotations have been performed for each annotation type. The second instead, shows the number of annotators for each document</p></caption><graphic xlink:href="12859_2024_5730_Fig20_HTML" id="MO20"/></fig><fig id="Fig21"><label>Fig. 21</label><caption><p>Concept types distribution. The <italic>Global</italic> pie chart illustrates the concepts type distribution of the concepts assigned in all the annotation types. The other pie charts illustrate the concept types distributions of the concepts (unlinked and linked to the mentions) taking part in relationships and assertions and whose role was subject, predicate, and object, respectively</p></caption><graphic xlink:href="12859_2024_5730_Fig21_HTML" id="MO21"/></fig></p>
      <p id="Par79"><italic>Collections’ statistics</italic> Detailed statistics are provided on the statistics web page. <italic>MetaTron</italic> provides two types of statistics: (i) <italic>personal</italic> statistics concern the single user, and (ii) <italic>global</italic> statistics concern the entire set of annotators of the collection. The two buttons at the top of the page allow the user to switch between personal ((1) in Fig. <xref rid="Fig16" ref-type="fig">16</xref>) and global ((2) in Fig. <xref rid="Fig16" ref-type="fig">16</xref>) statistics. (3) in Fig. <xref rid="Fig16" ref-type="fig">16</xref> allows the user to be provided with the Cohen’s kappa agreement between two users they selected and the agreement is computed for each annotation type. An example is provided in Fig. <xref rid="Fig17" ref-type="fig">17</xref>. (4) in Fig. <xref rid="Fig16" ref-type="fig">16</xref> allows the user to be provided with the Fleiss’ kappa agreement for each annotation type on each round of annotation. An example is provided in Fig. <xref rid="Fig18" ref-type="fig">18</xref>: for each round it is possible to see how the agreement evolves. The text area (5) in Fig. <xref rid="Fig16" ref-type="fig">16</xref>) allows the user to select a document to check the statistics of—by default, the statistics concern the entire set of collection documents An example of global statistics is illustrated in Figs. <xref rid="Fig19" ref-type="fig">19</xref>, <xref rid="Fig20" ref-type="fig">20</xref>, <xref rid="Fig21" ref-type="fig">21</xref>. <italic>General</italic> statistics, (A) in Fig. <xref rid="Fig19" ref-type="fig">19</xref> include the number of annotated documents, the number of annotators, the number of annotations for each annotation type, and the inter-annotator agreement computed basing on the Fleiss’ kappa measure. The <italic>Linked concepts overview</italic> section (B) displays the count of how many times an ontological concept has been linked to a mention. The list is subdivided into concept types. Additionally, in section (<bold>C</bold>), there are three lists: subject, predicate, and object lists. Similarly to the previous case, each list contains the concepts, grouped by concept type, that were annotated as a subject, predicate, or object, respectively. The <italic>Documents annotations overview</italic> section ((D) in Fig. <xref rid="Fig20" ref-type="fig">20</xref>) provides for each document of the collection the total number of annotations for each annotation type. In <italic>Annotators per document count</italic> ((E) in Fig. <xref rid="Fig20" ref-type="fig">20</xref>) the number of annotators is provided for each document. Finally, the pie chart in (F) in Fig. <xref rid="Fig21" ref-type="fig">21</xref> provides a global overview of the distribution of the concept types annotated in concepts linking, relationships, and assertions annotations. The three pie charts in (G) instead exclusively concern relationships and assertions and provide an overview of the concept types assigned to subjects, predicates, and objects, respectively. <italic>Personal</italic> statistics show the same set of statistics, except for the <italic>Annotators overview</italic>—which is not considered since personal statistics exclusively concern a single user. If a document is selected instead, the <italic>Documents annotations overview</italic> and the <italic>Annotators overview</italic> are not shown since these statistics concern the entire collection of documents. It is worth noting that it is always possible to access the statistics of the documents a user annotates via the <italic>statistics</italic> button in the vertical toolbar, which shows both the personal and the global statistics.</p>
    </sec>
  </sec>
  <sec id="Sec12">
    <title>Results</title>
    <p id="Par80">This section compares a subset of the annotation tools illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. The online tools selected for comparison are <italic>TeamTat</italic>, <italic>MedTAG</italic>, <italic>LightTag</italic>, and <italic>MetaTron</italic>. The offline tools instead are <italic>MetaTron (dockerized)</italic>, <italic>INCEpTION</italic>, and <italic>brat</italic>. While <italic>TeamTat</italic> and <italic>MetaTron</italic> target the biomedical domain, the other annotation tools are general purpose.</p>
    <p id="Par81">We provide a <italic>qualitative comparison</italic> where we outline the core functionalities of each tool. Furthermore, we provide a <italic>quantitative analysis</italic> by conducting experiments to assess the performance of each tool in tasks including mentions annotations and concepts linking, and relationships annotation. In the quantitative analysis, we did not consider <italic>MedTAG</italic>—as it does not support relationship annotation, and <italic>brat</italic>—as it has not been possible to automatically test it with a web agent.</p>
    <p id="Par82">We planned to include in our comparison also <italic>tagtog</italic> [<xref ref-type="bibr" rid="CR50">50</xref>], however, as of May 2023, the online version of <italic>tagtog</italic> did not allow us to add new documents to a collection; as a consequence, it has been impossible to qualitatively and quantitatively assess its performances. Furthermore, <italic>LightTag</italic> does not support entity linking, hence we cannot link a concept to a mention, we can only tag the mention with a concept type. However, we included this tool in the quantitative comparison as it is one of the newest tools of the past years and allows for relationships annotation.</p>
    <p id="Par83">In the last section, we describe a user study conducted on two tasks, namely GCA and GDA. GCA focuses on the annotation of relationships where subject and object are mentions and the predicate is an ontological concept. GDA focuses on the annotation of assertions. The user study involved 10 PubMed abstracts per task annotated by three experts in the biomedical domain. All the users were initially provided with the automatic annotations performed by AutoTron. We analyzed the results <italic>quantitatively</italic>, measuring the agreement among the annotators, and <italic>qualitatively</italic> via a questionnaire involving the annotators’ experience. Finally, we studied how AutoTron impacted on the annotations analyzing how many annotations generated automatically have been updated, removed, added or confirmed.</p>
    <sec id="Sec13">
      <title>Qualitative analysis</title>
      <p id="Par84">Figure <xref rid="Fig1" ref-type="fig">1</xref> illustrates an overview of the features characterizing a set of annotation tools. In the qualitative analysis we compare: <italic>MetaTron</italic>, <italic>MedTAG</italic>, <italic>TeamTat</italic>, <italic>brat</italic>, <italic>LightTag</italic>, and <italic>INCEpTION</italic>. The qualitative comparison is based on our direct experience with all these tools.</p>
      <p id="Par85"><italic>MetaTron</italic> is the unique tool that fully satisfies 23 of 24 criteria—active learning, and built-in prediction is only partially satisfied; specifically, it is the unique tool that satisfies the connection to ORCID (for login purposes) (F14) external libraries integration (F15). We see that <italic>TeamTat</italic>, and <italic>INCEpTION</italic> are the most complete tools: they fully satisfy 17 and 19 criteria, respectively, 5 criteria and 1 criterion, respectively, are only partially satisfied, and the remaining are not satisfied at all. Conversely, <italic>LightTag</italic>, is the least complete: 13 criteria are fully satisfied, 3 are partially satisfied, and 8 are not. Among the tools we compared <italic>MetaTron</italic> with, <italic>LightTag</italic> is the unique one without source code available (T2), does not show the date of the last version (or commit) (T1), does not allow for modification and redistribution (T6), is not free of charge (T7), and does not support ontologies (F4). <italic>brat</italic>, instead, is the unique tool that does not support document-level annotation; hence, it is impossible to associate one or more classes to a document. Finally, only <italic>TeamTat</italic> fully supports active learning (F5), and only <italic>TeamTat</italic> and <italic>MedTAG</italic> support the annotation of PubMed abstracts. Online availability (T3) is satisfied by <italic>TeamTat</italic> and <italic>LightTag</italic>; <italic>INCEpTION</italic> and <italic>brat</italic> are available offline—the online demo is available for <italic>INCEpTION</italic>, and <italic>MetaTron</italic> and <italic>MedTAG</italic> are available online and can be locally installed. Online tools are usually easier and faster to configure than locally installable ones: online tools usually require the user to upload a set of documents in predefined formats and a schema for the annotation, which usually includes the definition of the labels to classify the documents, or the definition of the entity types to associate. However, online tools might suffer from network delays that may occur when a large amount of data is uploaded/downloaded. Offline tools guarantee isolation and preserve data privacy; simultaneously, their installation might be difficult, and the configuration is time-demanding for someone with little technical expertise.</p>
      <p id="Par86">As of our experience with the compared tools, the offline tools—<italic>brat</italic> and <italic>INCEpTION</italic>, were the least intuitive and required the most time to be configured. In particular, <italic>INCEpTION</italic> required a deep study of the documentation as the notion of annotation layers that characterize the tool is not intuitive. On the other hand, <italic>TeamTat</italic> has the fastest and easiest configuration: it requires the definition of a collection and the upload of a set of documents. <italic>LightTag</italic>, together with the set of documents to annotate, required the user to define one or more annotation schemas and relation schemas: the former allows to define the tags to associate to the entities, and the classes to classify the documents with, while the latter allows user to specify the relation types—i.e., the labels to be assigned to the edge between two entities of the relationship. <italic>MedTAG</italic> instead requires uploading a set of documents, labels for the document-level annotation, and a set of ontological concepts.</p>
      <p id="Par87">The compared tools report substantial differences in the supported document formats: <italic>MetaTron</italic>, <italic>INCEpTION</italic>, and <italic>TeamTat</italic> are the unique tools supporting the upload of PDFs and TXT files. <italic>MetaTron</italic>, <italic>MedTAG</italic>, <italic>LigthTag</italic> support JSON and CSV. <italic>TeamTat</italic>, <italic>MetaTron</italic>, and <italic>MedTAG</italic> allow the user to upload PubMed abstract. Only <italic>MetaTron</italic> allows the user to specify a DOI and annotate the related abstract extracted from Semantic Scholar or OpenAIRE. In this respect <italic>MetaTron</italic> is the only tool that supports all the aforementioned formats and is integrated with three different APIs to abstracts upload.</p>
      <p id="Par88">We analyzed the annotation procedure for what concerns: labels annotation, mentions annotation, concepts linking, and relationship annotation. All the analyzed tools implement mention annotation via drag and drop, except for <italic>MedTAG</italic> where mentions are selected by clicking on each token composing the mention. <italic>LightTag</italic> is the only tool that allows users to perform entity tagging and does not support entity linking. <italic>MetaTron</italic> is the unique tool providing three modalities to select mentions, enhancing the annotation experience. In all the other tools, the concepts linked to a mention are always selected, specifying the related type and URI or name. Also, label annotation is similar in all the examined tools and can be achieved by clicking on the labels to be associated with the document. Relationships annotation may vary depending on the examined tool: <italic>LightTag</italic> and <italic>TeamTat</italic> for example, support n-ary relationships; in <italic>MetaTron</italic> instead, a relationship always has three components, and only one of them must be a mention annotated in the document. In <italic>INCEpTION</italic> and <italic>brat</italic>, instead, the source and the target in the relationships must be mentions. <italic>MetaTron</italic> is the most versatile tool among those described, as in a relationship, subject and object are not required to be mentions in the text. Additionally, <italic>MetaTron</italic> is the only tool to propose assertions annotation, not necessarily tied to sentences in the text. In this respect, the availability of document-level annotations is a relevant feature for <italic>MetaTron</italic> as, according to [<xref ref-type="bibr" rid="CR10">10</xref>], the most adopted biomedical annotation tool does not implement this feature.</p>
      <p id="Par89">We compared <italic>MetaTron</italic> and the tools in terms of collaborative features and agreement; <italic>MetaTron, MedTAG, TeamTat, INCEpTION and LightTag</italic> support the collaboration between multiple annotators. Specifically, <italic>TeamTat</italic> supports multiple rounds of annotation and provides the annotators with agreement and disagreement between the annotators, as well as disagreement resolution. <italic>LightTag</italic> implements task management features, assigning tasks to different groups of annotators based on specific needs—e.g., language, and allowing project managers to keep track of annotations and agreement. <italic>tagtog</italic> implements user roles and allows for the definition of a set of custom annotation guidelines. In this respect, <italic>MetaTron</italic> implements different annotation rounds and provides some additional collaborative features to facilitate and speed up the annotators’ work. Specifically, it allows annotators to copy other members’ annotations, and the annotation with the highest agreement is computed via majority voting. In addition, <italic>MetaTron</italic> implements annotation suggestions: given a mention, users can see what the concepts assigned by the other annotators are and select one of them accordingly, depending, for example, on how many users have linked a specific ontological concept. For each annotation performed, the user can keep track of the number of users who performed the same annotation and change it accordingly. In <italic>MetaTron</italic>, the collection’s creator can keep track of the annotation progress and is responsible for selecting the annotators of each round. All the users can see the entire sets of annotations of each collection document and the related annotators. <italic>MetaTron</italic> is the unique tool providing different agreement measures (it implements both Fleiss’ kappa and Cohen’s kappa) computed on the entire collection or on single documents.</p>
      <p id="Par90"><italic>brat</italic> and <italic>INCEpTION</italic> are included in our comparison even if they do not target the biomedical domain.</p>
    </sec>
    <sec id="Sec14">
      <title>Quantitative analysis</title>
      <p id="Par91">To compare the performance of the selected manual annotation tools, we conducted a series of experiments on two different tasks: concepts linking, and relationships annotation. We did not treat mention annotation as a separate task because the annotation method was the same across all the annotation tools. Our experiments concerned the time elapsed and the number of clicks required to annotate a collection of the same 15 documents.</p>
      <p id="Par92">To evaluate the performances of the selected tools we relied on Selenium,<xref ref-type="fn" rid="Fn4">4</xref> an open-source testing framework used to automate web browsers. We designed four web agents, one for each annotation tool—the same web agent has been used for the two instances of <italic>MetaTron</italic>, and we used them to simulate the concepts liking and relationships annotations task on a collection of 15 abstracts extracted from PubMed. In order to simulate the annotator activity, we selected abstracts of various lengths; the mentions, the linked concepts, and the relationships have been extracted using <italic>AutoTron</italic>. Overall, <italic>AutoTron</italic> extracted 94 mentions and 71 relationships; specifically, for each abstract, a minimum of 2 mentions and 1 relationship, and a maximum of 13 mentions and 9 relationships were found. Each mention has been linked to exactly one concept. Variable delays were introduced based on the examined tool to prevent errors caused by server overload and to allow sufficient time for request processing. To provide a fair analysis, we treated online tools and offline tools separately, as the performance of online tools is influenced by the server hosting the application, while offline tools rely on the individual machine running the test.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Overview of the time spent to perform mentions annotation (MA), concepts linking (CL), and relationship annotation (RA) on a set of 15 documents. The reported value of average (AVG), standard deviation (STD), median (MED), and 5<italic>th</italic> and 95<italic>th</italic> percentiles refer to the time spent annotating 15 documents 50 times</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left"/><th align="left" colspan="5">MA + CL + RE</th></tr><tr><th align="left"/><th align="left"/><th align="left">AVG</th><th align="left">STD</th><th align="left">5th</th><th align="left">MED</th><th align="left">95th</th></tr></thead><tbody><tr><td align="left" rowspan="3">Online</td><td align="left">MetaTron</td><td align="left"><bold>638</bold>.<bold>73</bold></td><td align="left">1.67</td><td align="left">636.97</td><td align="left"><bold>638</bold>.<bold>22</bold></td><td align="left">642.49</td></tr><tr><td align="left">TeamTat</td><td align="left">842.93</td><td align="left">0.76</td><td align="left">841.67</td><td align="left">843.03</td><td align="left">844.18</td></tr><tr><td align="left">LightTag</td><td align="left">661.13</td><td align="left">1.23</td><td align="left">659.35</td><td align="left">660.85</td><td align="left">663.07</td></tr><tr><td align="left" rowspan="2">Offline</td><td align="left">MetaTron</td><td align="left"><bold>642</bold>.<bold>39</bold></td><td align="left">1.30</td><td align="left">640.35</td><td align="left"><bold>642</bold>.<bold>28</bold></td><td align="left">644.54</td></tr><tr><td align="left">INCEpTION</td><td align="left">704.85</td><td align="left">2.14</td><td align="left">701.53</td><td align="left">705.12</td><td align="left">707.33</td></tr></tbody></table><table-wrap-foot><p>The boldface values represent avg and median results of the tools with the best performances, i.e., the lowest time taken to annotate 15 documents 50 times</p></table-wrap-foot></table-wrap></p>
      <p id="Par93">We conducted an analysis concerning the time spent annotating the entire collection. The results are shown in Tables <xref rid="Tab1" ref-type="table">1</xref> and <xref rid="Tab2" ref-type="table">2</xref>. We compared the annotation tools on the average (AVG), the median (MED), standard deviation (STD), the 5<italic>th</italic> and the 95<italic>th</italic> percentiles of the time required to annotate the entire collection 50 times. We studied the time taken to perform two tasks: (i) performing an entire annotation, which comprehends mentions annotation (MA), concepts linking (CL), and relationships annotation (RA); and (ii) annotating the mentions (MA) and linking the concepts (CL). We remark that <italic>LightTag</italic> does not support concept linking, it allows only to tag the mentions with concept types.</p>
      <p id="Par94">Looking at the results achieved by the online tools in the entire annotation (Table <xref rid="Tab1" ref-type="table">1</xref>), we see that <italic>MetaTron</italic> is the most efficient tool in terms of average time, as <italic>MetaTron</italic> required 638.73 s on average (while <italic>TeamTat</italic> requires 842.93 s). <italic>LightTag</italic>’s performance falls between <italic>TeamTat</italic> and <italic>MetaTron</italic>. However, <italic>TeamTat</italic> does not reveal significant fluctuations during the 50 annotation rounds; this means that it is a tool with good stability that performs well during long annotation sessions. The substantial difference between the performance of <italic>TeamTat</italic> and those of <italic>MetaTron</italic> should be attributed to how the annotation is performed: we noticed <italic>TeamTat</italic> needed longer delays to save each annotation correctly. These aspects might not be visible to a human annotator who performs slower than the web agent; as such, a human annotator will take more time than an automatic one to annotate the selected set of documents. In the comparison involving the offline tools, the dockerized instance of <italic>MetaTron</italic> achieved better results than <italic>INCEpTION</italic>, as <italic>MetaTron</italic> required 642.39 s on average and <italic>INCEpTION</italic> 704.85.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Overview of the time spent to perform mentions annotation (MA) and concepts linking (CL) on a set of 15 documents. The reported value of average (AVG), standard deviation (STD), median (MED), and 5<italic>th</italic> and 95<italic>th</italic> percentiles refer to the time spent annotating 15 documents 50 times</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left"/><th align="left" colspan="5">MA + CL</th></tr><tr><th align="left"/><th align="left"/><th align="left">AVG</th><th align="left">STD</th><th align="left">5th</th><th align="left">MED</th><th align="left">95th</th></tr></thead><tbody><tr><td align="left" rowspan="3">Online</td><td align="left">MetaTron</td><td align="left"><bold>329</bold>.<bold>25</bold></td><td align="left">1.53</td><td align="left">327.13</td><td align="left"><bold>328</bold>.<bold>96</bold></td><td align="left">331.93</td></tr><tr><td align="left">TeamTat</td><td align="left">446.33</td><td align="left">1.34</td><td align="left">444.28</td><td align="left">446.53</td><td align="left">448.53</td></tr><tr><td align="left">LightTag</td><td align="left">412.61</td><td align="left">1.92</td><td align="left">409.48</td><td align="left">412.68</td><td align="left">415.65</td></tr><tr><td align="left" rowspan="2">Offline</td><td align="left">MetaTron</td><td align="left">330.35</td><td align="left">1.37</td><td align="left">327.98</td><td align="left">330.28</td><td align="left">331.96</td></tr><tr><td align="left">INCEpTION</td><td align="left"><bold>326</bold>.<bold>85</bold></td><td align="left">1.91</td><td align="left">323.60</td><td align="left"><bold>327</bold>.<bold>00</bold></td><td align="left">329.92</td></tr></tbody></table><table-wrap-foot><p>The boldface values represent avg and median results of the tools with the best performances, i.e., the lowest time taken to annotate 15 documents 50 times</p></table-wrap-foot></table-wrap></p>
      <p id="Par95">For what concerns the second task, mentions annotation and concepts linking, whose results are shown in Table <xref rid="Tab2" ref-type="table">2</xref>, similarly to the previous case, the best online tool is <italic>MetaTron</italic>, which required 329.25 s on average while the one that requires the highest average time is <italic>TeamTat</italic>, that required 446.33 s. Also in this case the performances of <italic>LightTag</italic>, 412.61 s, fall between those of <italic>MetaTron</italic> and <italic>TeamTat</italic>. <italic>INCEpTION</italic> and <italic>MetaTron (offline)</italic> achieved comparable performances. In both the analyzed task, all the tools had a standard deviation lower than 2.5. The 5<italic>th</italic> and 95<italic>th</italic> percentiles indicate that in all the examined tools all the computed times are uniformly distributed around the median.</p>
      <p id="Par96">The online and offline instances of <italic>MetaTron</italic> achieved similar performances in both tasks. This can be attributed to the absence of any differences between the code running locally and the code of the online instance. Furthermore, the server hosting <italic>MetaTron</italic> was underutilized when we ran the automatic agents, leading to performances comparable to the docker-based instance. It is notable the case of the <italic>INCEpTION</italic>: it achieved the worst performances in the first task (MA + CL + RE), and the best in the second one (MA + CL). This aspect points out that the annotation of relationships is the most expensive in terms of time, while <italic>INCEpTION</italic> is the most efficient tool in mentions annotation and linking. <italic>MetaTron</italic> and <italic>TeamTat</italic> present similar behaviors: the average time taken in the second task is half the time taken in the first: this indicates that annotating the relationships takes the same amount of time required to annotate the mentions and link them to the concepts. Conversely, in <italic>LightTag</italic>, mentions annotation and linking require more than half of the time: this is partially related to the implementation of the web agent in the relationship part, and partially depends on how mentions selection and linking have been implemented in the tool.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Overview of the number of clicks required to perform the annotation of 15 documents in the two selected tasks: mentions annotation and concepts linking (MA + CL) and an entire annotation—mentions annotation, concepts linking, relationship annotation (MA + CL + RA)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">MA + CL</th><th align="left">MA + CL + RA</th></tr></thead><tbody><tr><td align="left">MetaTron</td><td align="left">485</td><td char="." align="char">1028</td></tr><tr><td align="left">TeamTat</td><td align="left">500</td><td char="." align="char">1423</td></tr><tr><td align="left">LightTag</td><td align="left">327</td><td char="." align="char">611</td></tr><tr><td align="left">INCePTION</td><td align="left">391</td><td char="." align="char">746</td></tr></tbody></table></table-wrap></p>
      <p id="Par97">We analyzed the number of clicks performed to annotate the collection. The results include the count of clicks required to annotate every single document of the collection and the clicks necessary to change the document. The results are reported in Table <xref rid="Tab3" ref-type="table">3</xref>. We see that <italic>MetaTron</italic> and <italic>TeamTat</italic> are the tools that require the highest number of clicks to annotate the collection of 15 documents. This aspect is motivated by how the tools implement relationship annotation. In both the aforementioned tools, to annotate a new relationship it is required to set the two mentions and select the predicate and the associated ontological concept: all these actions makes the total number of clicks increase. <italic>LightTag</italic> instead, is the most efficient, however, it does not support entity linking, and this aspect motivates the lower number of clicks with respect to the other tools, since only the concept type—i.e., gene or disease, is required. The annotation of relationships in <italic>LightTag</italic> is the fastest compared to the other tools; the first reason is that <italic>LightTag</italic> requires the user to provide a schema configuration for relationship annotation and this allows the user to save clicks and time; in addition, in <italic>LightTag</italic> each mention/concept composing the relationship is selected by dragging and dropping it into a box hosting the relationship components, and this allows the user to save clicks—a drag and drop action is performed in a single click. <italic>INCEpTION</italic> is most efficient after <italic>LightTag</italic>, and requires a half of the clicks compared to <italic>TeamTat</italic>.</p>
    </sec>
    <sec id="Sec15">
      <title>Discussion</title>
      <p id="Par98">The qualitative and quantitative analyses conducted to assess the performances of <italic>MetaTron</italic>, <italic>MedTAG</italic>, <italic>TeamTat</italic>, <italic>LightTag</italic>, <italic>brat</italic>, and <italic>INCEpTION</italic> allowed us to draw some conclusion.</p>
      <p id="Par99">The results deriving from the analyses showed that <italic>MetaTron</italic> emerges as a competitive annotation tool in the biomedical domain, as it is the only one that fulfills all the analyzed features and achieves the best results in terms of time spent in the annotation of a set of documents. <italic>MetaTron</italic> provides an environment where one or more annotators can collaborate in annotating documents in five different ways, both at the document level and mention level and can leverage automatic annotations to expedite the annotation process. Additionally, <italic>MetaTron</italic> is open-source, free of charge, and supports a wide range of input and output formats. The tool is released as an online and offline instance, making <italic>MetaTron</italic> a versatile tool that can be adapted to different needs and use cases. The online instance of <italic>MetaTron</italic> is valuable to test its features, take advantage of <italic>AutoTron</italic>’s automatic annotations, and collaboratively annotate PubMed, Semantic Scholar, and OpenAIRE abstracts; the offline release guarantees data privacy and allows users to locally deploy <italic>MetaTron</italic> and share the tool with a controlled number of users. In addition, the offline tool is useful when dealing with large volumes of data that would require a significant amount of time for uploading.</p>
    </sec>
    <sec id="Sec16">
      <title>User study</title>
      <p id="Par100">The user study consisted of sentence-level tasks for Gene-Cancer Associations (GCA) and document-level tasks for Gene-Disease Associations (GDA). The GCA task required annotating relationships where the subject and object, representing gene and cancer mentions, respectively, are involved. The predicate corresponds to one of the following ontological concepts: (i) biomarker, indicating whether the gene associated with the disease is altered in conjunction with the disease; (ii) tumor suppressor, indicating whether the gene plays a role in preventing the disease; and (iii) oncogene, indicating whether the gene promotes the progression of the disease.</p>
      <p id="Par101">The GDA task encompassed annotating assertions where the subject, predicate, and object are ontological concepts unrelated to specific textual mentions. The subject and object represent gene and disease, respectively. At the same time, the predicate is categorized into one of the following concepts: (i) biomarker, (ii) genomic alteration, indicating a connection between a genomic alteration and the gene associated with the disease phenotype, and (iii) therapeutic, signifying the gene’s therapeutic role in ameliorating the disease.</p>
      <p id="Par102">We chose ten pertinent PubMed abstracts for each task, which three experts in the biomedical domain annotated. To streamline and expedite the annotation process for the annotators, we furnished them with automatic annotations generated by running <italic>AutoTron</italic> on each document.</p>
      <p id="Par103">The annotators performed two annotation rounds for each task. In round 1, the annotators had to annotate each document relying exclusively on the auxiliary information provided by the automatic annotations. The users were allowed to add new mentions, link concepts to them, and add new relationships. However, they could not rely on collaborative features to annotate the documents—i.e., the annotators could not check other members’ annotations or documents’ statistics. In round 2, instead, the annotators were asked to rely on the collaborative features of <italic>MetaTron</italic> to update their annotations: as a consequence, they had access to other members’ annotations and the annotation obtained via majority voting. At the end of the annotation rounds, we provided all the annotators with a questionnaire with 15 questions about their annotation experience.</p>
      <p id="Par104">In the following sections, we analyze how the annotations change after each round and how the collaborative features impact the agreement among annotators, we summarize the answers to the questionnaire, and finally, we study the quality of the annotations of <italic>AutoTron</italic> as a means to speed up and facilitate annotators’ work.</p>
      <p id="Par105"><italic>Quantitative results</italic> In Table <xref rid="Tab4" ref-type="table">4</xref>, we report the Fleiss’ kappa agreement among the annotators for each task after each round. Specifically, we provide the agreement computed on concepts and relationship annotations for the GCA task. Instead, we provide the agreement computed on assertions annotation for the GDA task. Our goal is to investigate the extent to which the presence of annotations from other annotators impacts the work of the annotators. The highest agreement has been achieved by concept annotations in all the rounds. At the end of round 1, the agreement obtained in relationships (GCA) and assertions (GDA) annotation is negative, indicating that there is no agreement among the annotators. At the end of round 2, the agreement on concept annotations did not change from round 1, while the agreement on relationships increased from <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-$$\end{document}</tex-math><mml:math id="M4"><mml:mo>-</mml:mo></mml:math><inline-graphic xlink:href="12859_2024_5730_Article_IEq2.gif"/></alternatives></inline-formula>0.2179 to <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-$$\end{document}</tex-math><mml:math id="M6"><mml:mo>-</mml:mo></mml:math><inline-graphic xlink:href="12859_2024_5730_Article_IEq3.gif"/></alternatives></inline-formula>0.0872 and on assertions from <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-$$\end{document}</tex-math><mml:math id="M8"><mml:mo>-</mml:mo></mml:math><inline-graphic xlink:href="12859_2024_5730_Article_IEq4.gif"/></alternatives></inline-formula>0.0364 to 0.2490. These results confirmed that the collaborative features provided by <italic>MetaTron</italic> play a key role in improving the results of the annotation process.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Overview of Fleiss’s kappa agreement at the end of each round. The agreement has been computed for concepts, relationships, and assertion annotations for each task’s entire set of documents</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left" colspan="2">GCA</th><th align="left">GDA</th></tr><tr><th align="left"/><th align="left">Concepts annotation</th><th align="left">Relationships annotation</th><th align="left">Assertions annotation</th></tr></thead><tbody><tr><td align="left">Round 1</td><td align="left">0.3312</td><td align="left">− 0.2179</td><td char="." align="char">− 0.0364</td></tr><tr><td align="left">Round 2</td><td align="left">0.3312</td><td align="left">− 0.0872</td><td char="." align="char">0.2490</td></tr></tbody></table></table-wrap></p>
      <p id="Par106"><italic>Qualitative results</italic> At the end of round 2, each annotator compiled a questionnaire concerning their experience using <italic>MetaTron</italic>. The questionnaire consisted of 15 questions about annotation experience, GCA and GDA tasks, and collaborative experience, with responses ranging from 1 to 5.</p>
      <p id="Par107">About the annotation experience, the annotators identified the annotation of mentions, concepts, relationships, and assertions as a straightforward process (all the questions about the complexity of annotations received scores equal to 1 and 2). Only one annotator needed to contact the developers to clarify how to annotate. All the annotators agreed that the automatic annotations generated via <italic>AutoTron</italic> are a useful starting point and contributed to speeding up and facilitating the annotation process. All the annotators considered <italic>MetaTron</italic> had a positive annotation experience and will use <italic>MetaTron</italic> in the future.</p>
      <p id="Par108">Considering task complexity, all the annotators found the GCA task more complex than the GDA one. Two over three annotators assigned a score equal to 3 in the complexity of the GCA task, while only one assigned 4. In the GDA task, instead, the annotators assigned 2, 3, and 4.</p>
      <p id="Par109">One relevant aspect is the annotators’ collaboration. All the annotators found <italic>MetaTron</italic> effectively supports collaboration among annotators. Specifically, all the annotators agreed that the possibility of copying one or more annotations from another annotator is an important feature that speeds up the annotation process. One annotator found useful the annotation generated via majority voting (score equal to 4), while the other two annotators assigned a score equal to 3 hence this annotation did not play a key role in their annotations. Finally, two out of three annotators admitted that those of the other annotators did not significantly influence their annotations. The remaining annotator, however, made numerous changes to the performed annotations in round 2. Two out of three annotators found it necessary to discuss their annotations and determine which relationship’s predicate to apply. This highlights the difficulty of the proposed tasks.</p>
      <p id="Par110">The annotators were required to point out the most useful features of <italic>MetaTron</italic> according to their experience. In Fig. <xref rid="Fig22" ref-type="fig">22</xref> we provide the results of this analysis. According to our results, three features have not been selected, specifically ontology support, statistics and collection’s agreement availability. These three features had minimal impact on annotators’ work as they do not offer direct support in the annotations; instead, they prove their key role in providing insights into overall agreements. One annotator considers five features relevant features, while six features by two. The possibility to copy the annotations of another annotator has been considered the most valuable feature, as all the annotators agreed on its importance.<fig id="Fig22"><label>Fig. 22</label><caption><p><italic>MetaTron</italic> features qualitative analysis. The overview concerns the features the annotators considered important to perform the two annotation tasks. The most significant features concern the collaboration among multiple annotators, while the least used concern the ontology support and the availability statistics</p></caption><graphic xlink:href="12859_2024_5730_Fig22_HTML" id="MO22"/></fig></p>
      <p id="Par111"><italic>AutoTron results</italic> In Table <xref rid="Tab5" ref-type="table">5</xref>, we report the total number of annotations at the end of each round. The first row, <italic>AutoTron</italic> refers to the total number of automatic annotations generated via <italic>AutoTron</italic> each annotator has been provided with at the beginning of round 1. Each annotator started with 228 concepts, and 57 relationships for the GCA task, and 17 assertions for the GDA task. At the end of round 1, the number of concepts increased to 235, the number of relationships increased to 115, and the number of assertions increased to 32. At the end of round 2, the number of concepts increased to 237, the number of relationships decreased to 111, and the number of assertions decreased to 24. The most significant change was identified in the GCA task for the doubled relationships at the end of round 1.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Overview of the total number of linked concepts, relationships, assertions annotations identified at each round. The first row, <italic>AutoTron</italic>, represents the starting point for each annotator, indicating the set of automatic annotations provided</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left" colspan="2">GCA</th><th align="left">GDA</th></tr><tr><th align="left"/><th align="left">Concepts annotation</th><th align="left">Relationships annotation</th><th align="left">Assertions annotation</th></tr></thead><tbody><tr><td align="left">AutoTron</td><td align="left">228</td><td char="." align="char">57</td><td align="left">17</td></tr><tr><td align="left">Round 1</td><td align="left">235</td><td char="." align="char">115</td><td align="left">32</td></tr><tr><td align="left">Round 2</td><td align="left">237</td><td char="." align="char">111</td><td align="left">24</td></tr></tbody></table></table-wrap></p>
      <p id="Par112">We considered the set of distinct annotations obtained at the end of round 2, and we counted how many concepts, relationships, and assertions have been updated, added to, and deleted from the automatically generated set of annotations provided for round 1. We considered an update when, in a relationship or assertion, the predicate assigned by the annotators is different from the one automatically assigned. We have a concept update, instead, when the linked concept changes.</p>
      <p id="Par113">In the GCA task, we detected that 9 concepts had been added, 2 were updated, 0 were removed, and 226 were confirmed. For what concerns relationships instead, 29 relationships have been added, 27 updated, 0 deleted, and 30 confirmed. In the GDA task, we detected that four assertions have been added, three updated, 0 deleted, and 14 confirmed.</p>
      <p id="Par114">The absence of deleted annotations confirms that <italic>AutoTron</italic> overall generates accurate annotations. Only in the GCA task in the relationships annotation the number of relationships added, updated, and confirmed remains the same.</p>
      <p id="Par115">Our results indicate that relying on <italic>AutoTron</italic> to generate a set of annotations used as a starting point is useful to facilitate and speed up the entire annotation process of the annotators. However, especially for relationships, the intervention of a human annotator is crucial for identifying and updating all existing relationships in a document. In this respect, according to the results obtained in the qualitative analyses, we see that the annotators found the GDA task easier than the GCA one: this results not only in a higher agreement but also in a lower number of updates and additions with respect to the automatic annotations.</p>
    </sec>
  </sec>
  <sec id="Sec17">
    <title>Conclusions</title>
    <p id="Par116">This paper presents <italic>MetaTron</italic>, a collaborative web-based annotation tool designed specifically for the biomedical domain. The tool facilitates the annotation of mentions, relationships, and document-level labels. It supports various document formats, including PDF, TXT, JSON, and CSV. Additionally, users can utilize the PubMed, Semantic Scholar, and OpenAIRE REST APIs to upload PMIDs or DOIs and annotate corresponding abstracts. Furthermore, <italic>MetaTron</italic> allows users to leverage their teammates’ annotations and incorporate annotations generated by <italic>AutoTron</italic> for fast annotation creation.</p>
    <p id="Par117">To ensure data privacy and limit tool usage to specific research groups, the <italic>MetaTron</italic> docker image enables local deployment on personal servers. Conversely, the online instance of <italic>MetaTron</italic> is designed for online annotation of PubMed, Semantic Scholar, and OpenAIRE abstracts, with the added advantage of utilizing AutoTron’s automatic prediction capabilities.</p>
    <p id="Par118">Noteworthy features of <italic>MetaTron</italic> include support for multiple ontologies, multilingual capabilities, login via ORCID ID, and the option to download annotations in JSON, CSV, and BioC/XML formats.</p>
    <p id="Par119">In our evaluation, we compared <italic>MetaTron</italic> to five other annotation tools, both general purpose and specifically tailored to the biomedical domain. We assessed them against 24 criteria classified into three categories: <italic>Data</italic>, <italic>Technical</italic>, and <italic>Functionalities</italic>. The qualitative analysis revealed that <italic>MetaTron</italic> fulfills almost all of the selected criteria. From a quantitative perspective, the online instance of <italic>MetaTron</italic> outperformed <italic>TeamTat</italic> and <italic>LightTag</italic> in terms of time elapsed and the number of clicks required. Additionally, the dockerized version of <italic>MetaTron</italic> achieved better results than <italic>INCEpTION</italic> in the task of mentions annotation, concept linking, and relationship annotation (MA + CL + RA). We conducted a user study which involved three human annotators and two tasks: relationships annotation and assertions annotation. The user study pointed out that <italic>MetaTron</italic> is an intuitive and easy to use tool. The collaborative features have been of great assistance, enabling annotators to enhance the accuracy of their annotations and improve agreement. The possibility of using <italic>AutoTron</italic> to automatically annotate documents, and to copy other members’ annotations has proven to be one of <italic>MetaTron</italic>’s most valued features, streamlining and facilitating the annotation process. In summary, <italic>MetaTron</italic> presents itself as a compelling annotation tool for the biomedical and bioinformatics community, providing collaborative and interactive features that can effectively streamline the annotation process. With a commitment to ongoing maintenance and a notable emphasis on relation annotation, often overlooked by other annotation tools, we think that <italic>MetaTron</italic> represents one of the highly recommended options for researchers in these domains.</p>
    <p id="Par120">As future work, we plan to integrate more use cases for built-in automatic predictions to allow <italic>MetaTron</italic> to widen to other domains of applications for automatic annotation of text. Moreover, two functionalities deserve to be implemented: the first one is to introduce a new annotation type, which is entity tagging, and let the user associate to a mention a concept type instead of the concept itself; then, we plan to implement discontinuous mentions allowing the user to be more accurate letting them decide which tokens compose the mention. About the support for the ontologies, we plan to introduce the possibility of (i) uploading the full ontologies directly from the related files and (ii) automatically suggesting the concept to associate to a mention relying on the textual and semantic similarity between the mention and the concepts. These features would support the user in speeding up the upload and the selection of the concepts. Finally, we plan to introduce in <italic>MetaTron</italic> some large corpora of documents that one or more members can annotate: this would provide the community with a tool already configured and ready-to-use and would promote the analyses of annotators’ behavior on well-known datasets.</p>
  </sec>
  <sec id="Sec18">
    <title>Availability and requirements</title>
    <p id="Par121">
      <list list-type="bullet">
        <list-item>
          <p id="Par122">Project name: MetaTron</p>
        </list-item>
        <list-item>
          <p id="Par123">Project home page: <ext-link ext-link-type="uri" xlink:href="https://github.com/GDAMining/metatron/">https://github.com/GDAMining/metatron/</ext-link></p>
        </list-item>
        <list-item>
          <p id="Par124">Online instance: <ext-link ext-link-type="uri" xlink:href="https://metatron.dei.unipd.it">https://metatron.dei.unipd.it</ext-link></p>
        </list-item>
        <list-item>
          <p id="Par125">Archived version: not applicable</p>
        </list-item>
        <list-item>
          <p id="Par126">Operating system(s): Platform independent</p>
        </list-item>
        <list-item>
          <p id="Par127">Programming language: Python, JavaScript, HTML, CSS</p>
        </list-item>
        <list-item>
          <p id="Par128">Other requirements: Docker and docker-compose (for the dockerized version) for the offline version</p>
        </list-item>
        <list-item>
          <p id="Par129">License: MIT License</p>
        </list-item>
        <list-item>
          <p id="Par130">Any restrictions to use by non-academics: No</p>
        </list-item>
      </list>
    </p>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p id="Par134"><ext-link ext-link-type="uri" xlink:href="https://www.djangoproject.com">https://www.djangoproject.com</ext-link>.</p>
    </fn>
    <fn id="Fn2">
      <label>2</label>
      <p id="Par135">By accessing with the username <italic>demo</italic> and password <italic>demo</italic>, it is possible to annotate a test collection.</p>
    </fn>
    <fn id="Fn3">
      <label>3</label>
      <p id="Par136">A relationship that solely consists of ontological concepts is treated as a document-level assertion.</p>
    </fn>
    <fn id="Fn4">
      <label>4</label>
      <p id="Par137"><ext-link ext-link-type="uri" xlink:href="https://www.selenium.dev">https://www.selenium.dev</ext-link>.</p>
    </fn>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank Fabio Giachelle, Niccoló Marini, and Laura Menotti for contributing to useful discussions and tool improvements. We appreciate their time, effort, and valuable input, significantly enriching our research. We also thank the anonymous reviewers for their insightful suggestions, which improved the paper and the described tool.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>O.I. was the designer and developer of MetaTron, wrote the code, designed the user study, wrote the main parts of the manuscript, and led the work. S.M. was the designer and developer of AutoTron and the relation extraction methods, contributed to the design of the user study, suggested some functionalities, wrote the parts describing AutoTron and revised the whole paper. G.S. coordinated the work, contributed to the design of MetaTron, suggested some functionalities, and contributed to the writing and revision of the paper.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Open access funding provided by Università degli Studi di Padova. This work is partially supported by the HEREDITARY Project, as part of the European Union’s Horizon Europe research and innovation programme under Grant Agreement No GA 101137074. The work of O.I. was partially funded by the EC H2020 project OpenAIRE-Nexus (Grant Agreement No. 101017452).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The code used in this study is publicly available on GitHub <ext-link ext-link-type="uri" xlink:href="https://github.com/GDAMining/metatron/">https://github.com/GDAMining/metatron/</ext-link>. All the underlying libraries used in this work are open-source. The complete list of libraries and their versions are reported in the GitHub repository.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1">
      <title>Ethics approval and consent to participate</title>
      <p id="Par131">Not applicable.</p>
    </notes>
    <notes id="FPar2">
      <title>Consent for publication</title>
      <p id="Par132">Not applicable.</p>
    </notes>
    <notes id="FPar3" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par133">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Costa</surname>
            <given-names>FF</given-names>
          </name>
        </person-group>
        <article-title>Big data in biomedicine</article-title>
        <source>Drug Discov Today</source>
        <year>2014</year>
        <volume>19</volume>
        <issue>4</issue>
        <fpage>433</fpage>
        <lpage>440</lpage>
        <pub-id pub-id-type="doi">10.1016/j.drudis.2013.10.012</pub-id>
        <?supplied-pmid 24183925?>
        <pub-id pub-id-type="pmid">24183925</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Murdoch</surname>
            <given-names>TB</given-names>
          </name>
          <name>
            <surname>Detsky</surname>
            <given-names>AS</given-names>
          </name>
        </person-group>
        <article-title>The inevitable application of big data to health care</article-title>
        <source>JAMA</source>
        <year>2013</year>
        <volume>309</volume>
        <issue>13</issue>
        <fpage>1351</fpage>
        <lpage>1352</lpage>
        <pub-id pub-id-type="doi">10.1001/jama.2013.393</pub-id>
        <?supplied-pmid 23549579?>
        <pub-id pub-id-type="pmid">23549579</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>CC</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Community challenges in biomedical text mining over 10 years: success, failure and the future</article-title>
        <source>Brief Bioinform</source>
        <year>2016</year>
        <volume>17</volume>
        <issue>1</issue>
        <fpage>132</fpage>
        <lpage>144</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbv024</pub-id>
        <?supplied-pmid 25935162?>
        <pub-id pub-id-type="pmid">25935162</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jovanović</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bagheri</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Semantic annotation in biomedicine: the current landscape</article-title>
        <source>J Biomed Semant</source>
        <year>2017</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>18</lpage>
        <pub-id pub-id-type="doi">10.1186/s13326-017-0153-x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Patumcharoenpol</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Meechai</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Vongsangnak</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Biomedical text mining and its applications in cancer research</article-title>
        <source>J Biomed Inform</source>
        <year>2013</year>
        <volume>46</volume>
        <issue>2</issue>
        <fpage>200</fpage>
        <lpage>211</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2012.10.007</pub-id>
        <?supplied-pmid 23159498?>
        <pub-id pub-id-type="pmid">23159498</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lindvall</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>C-Y</given-names>
          </name>
          <name>
            <surname>Moseley</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Agaronnik</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>El-Jawahri</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Paasche-Orlow</surname>
            <given-names>MK</given-names>
          </name>
          <name>
            <surname>Lakin</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Volandes</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tulsky</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Investigators</surname>
            <given-names>A-P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Natural language processing to identify advance care planning documentation in a multisite pragmatic clinical trial</article-title>
        <source>J Pain Symptom Manag</source>
        <year>2022</year>
        <volume>63</volume>
        <issue>1</issue>
        <fpage>29</fpage>
        <lpage>36</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jpainsymman.2021.06.025</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cohen</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Hersh</surname>
            <given-names>WR</given-names>
          </name>
        </person-group>
        <article-title>A survey of current work in biomedical text mining</article-title>
        <source>Brief Bioinform</source>
        <year>2005</year>
        <volume>6</volume>
        <issue>1</issue>
        <fpage>57</fpage>
        <lpage>71</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/6.1.57</pub-id>
        <?supplied-pmid 15826357?>
        <pub-id pub-id-type="pmid">15826357</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kersloot</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>van Putten</surname>
            <given-names>FJ</given-names>
          </name>
          <name>
            <surname>Abu-Hanna</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cornet</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Arts</surname>
            <given-names>DL</given-names>
          </name>
        </person-group>
        <article-title>Natural language processing algorithms for mapping clinical text fragments onto ontology concepts: a systematic review and recommendations for future studies</article-title>
        <source>J Biomed Semant</source>
        <year>2020</year>
        <volume>11</volume>
        <fpage>1</fpage>
        <lpage>21</lpage>
        <pub-id pub-id-type="doi">10.1186/s13326-020-00231-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lacson</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Pitzer</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Hinske</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Galante</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Ohno-Machado</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Evaluation of a large-scale biomedical data annotation initiative</article-title>
        <source>BMC Bioinform</source>
        <year>2009</year>
        <volume>10</volume>
        <fpage>1</fpage>
        <lpage>6</lpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-10-S9-S10</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Neves</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ševa</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>An extensive review of tools for manual annotation of documents</article-title>
        <source>Brief Bioinform</source>
        <year>2021</year>
        <volume>22</volume>
        <issue>1</issue>
        <fpage>146</fpage>
        <lpage>163</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbz130</pub-id>
        <?supplied-pmid 31838514?>
        <pub-id pub-id-type="pmid">31838514</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koleck</surname>
            <given-names>TA</given-names>
          </name>
          <name>
            <surname>Dreisbach</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Bourne</surname>
            <given-names>PE</given-names>
          </name>
          <name>
            <surname>Bakken</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Natural language processing of symptoms documented in free-text narratives of electronic health records: a systematic review</article-title>
        <source>J Am Med Inform Assoc</source>
        <year>2019</year>
        <volume>26</volume>
        <issue>4</issue>
        <fpage>364</fpage>
        <lpage>379</lpage>
        <pub-id pub-id-type="doi">10.1093/jamia/ocy173</pub-id>
        <?supplied-pmid 30726935?>
        <pub-id pub-id-type="pmid">30726935</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yim</surname>
            <given-names>W-W</given-names>
          </name>
          <name>
            <surname>Yetisgen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Harris</surname>
            <given-names>WP</given-names>
          </name>
          <name>
            <surname>Kwan</surname>
            <given-names>SW</given-names>
          </name>
        </person-group>
        <article-title>Natural language processing in oncology: a review</article-title>
        <source>JAMA Oncol</source>
        <year>2016</year>
        <volume>2</volume>
        <issue>6</issue>
        <fpage>797</fpage>
        <lpage>804</lpage>
        <pub-id pub-id-type="doi">10.1001/jamaoncol.2016.0213</pub-id>
        <?supplied-pmid 27124593?>
        <pub-id pub-id-type="pmid">27124593</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>PourNejatian</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Shin</surname>
            <given-names>HC</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>KE</given-names>
          </name>
          <name>
            <surname>Parisien</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Compas</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Costa</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Flores</surname>
            <given-names>MG</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A large language model for electronic health records</article-title>
        <source>NPJ Digit Med</source>
        <year>2022</year>
        <volume>5</volume>
        <issue>1</issue>
        <fpage>194</fpage>
        <pub-id pub-id-type="doi">10.1038/s41746-022-00742-2</pub-id>
        <?supplied-pmid 36572766?>
        <pub-id pub-id-type="pmid">36572766</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Meij E, Balog K, Odijk D. Entity linking and retrieval. In: Proceedings of the 36th international ACM SIGIR conference on research and development in information retrieval; 2013. p. 1127.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Recent advances in biomedical literature mining</article-title>
        <source>Brief Bioinform</source>
        <year>2021</year>
        <volume>22</volume>
        <issue>3</issue>
        <fpage>057</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbaa057</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hong</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wan</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A novel machine learning framework for automated biomedical relation extraction from large-scale literature repositories</article-title>
        <source>Nat Mach Intell</source>
        <year>2020</year>
        <volume>2</volume>
        <issue>6</issue>
        <fpage>347</fpage>
        <lpage>355</lpage>
        <pub-id pub-id-type="doi">10.1038/s42256-020-0189-y</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Biomedical relation extraction via knowledge-enhanced reading comprehension</article-title>
        <source>BMC Bioinform</source>
        <year>2022</year>
        <volume>23</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>19</lpage>
        <pub-id pub-id-type="doi">10.1186/s12859-021-04534-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xing</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>BioRel: towards large-scale biomedical relation extraction</article-title>
        <source>BMC Bioinform</source>
        <year>2020</year>
        <volume>21</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1186/s12859-020-03889-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Zhou P, Shi W, Tian J, Qi Z, Li B, Hao H, Xu B. Attention-based bidirectional long short-term memory networks for relation classification. In: Proceedings of the 54th annual meeting of the Association for Computational Linguistics (volume 2: short papers); 2016. p. 207–12.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Zhang D, Wang D. Relation classification via recurrent neural network. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1508.01006">arXiv:1508.01006</ext-link> (2015).</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Zeng D, Liu K, Chen Y, Zhao J. Distant supervision for relation extraction via piecewise convolutional neural networks. In: Proceedings of the 2015 conference on empirical methods in natural language processing, EMNLP 2015, Lisbon, Portugal, September 17–21, 2015. p. 1753–62.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Exploring semi-supervised variational autoencoders for biomedical relation extraction</article-title>
        <source>Methods</source>
        <year>2019</year>
        <volume>166</volume>
        <fpage>112</fpage>
        <lpage>119</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ymeth.2019.02.021</pub-id>
        <?supplied-pmid 30822516?>
        <pub-id pub-id-type="pmid">30822516</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Krauthammer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nenadic</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Term identification in the biomedical literature</article-title>
        <source>J Biomed Inform</source>
        <year>2004</year>
        <volume>37</volume>
        <issue>6</issue>
        <fpage>512</fpage>
        <lpage>526</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2004.08.004</pub-id>
        <?supplied-pmid 15542023?>
        <pub-id pub-id-type="pmid">15542023</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Matthews</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Distinguishing the species of biomedical named entities for term identification</article-title>
        <source>BMC Bioinform</source>
        <year>2008</year>
        <volume>9</volume>
        <issue>11</issue>
        <fpage>1</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Elhadad</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Unsupervised biomedical named entity recognition: experiments with clinical and biological texts</article-title>
        <source>J Biomed Inform</source>
        <year>2013</year>
        <volume>46</volume>
        <issue>6</issue>
        <fpage>1088</fpage>
        <lpage>1098</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2013.08.004</pub-id>
        <?supplied-pmid 23954592?>
        <pub-id pub-id-type="pmid">23954592</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Gorrell G, Song X, Roberts A. Bio-yodie: a named entity linking system for biomedical text. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1811.04860">arXiv:1811.04860</ext-link> (2018)</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vashishth</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Newman-Griffis</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Joshi</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Dutt</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Rosé</surname>
            <given-names>CP</given-names>
          </name>
        </person-group>
        <article-title>Improving broad-coverage medical entity linking with semantic type prediction and large-scale datasets</article-title>
        <source>J Biomed Inform</source>
        <year>2021</year>
        <volume>121</volume>
        <fpage>103880</fpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2021.103880</pub-id>
        <?supplied-pmid 34390853?>
        <pub-id pub-id-type="pmid">34390853</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">D’Souza J, Ng V. Sieve-based entity linking for the biomedical domain. In: Proceedings of the 53rd annual meeting of the Association for Computational Linguistics and the 7th international joint conference on natural language processing (volume 2: short papers); 2015. p. 297–302.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Jiang X, Ringwald M, Blake J, Shatkay H. Effective biomedical document classification for identifying publications relevant to the mouse Gene Expression Database (GXD). Database 2017;2017.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pérez-Pérez</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ferreira</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Lourenço</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Igrejas</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Fdez-Riverola</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Boosting biomedical document classification through the use of domain entity recognizers and semantic ontologies for document representation: the case of gluten bibliome</article-title>
        <source>Neurocomputing</source>
        <year>2022</year>
        <volume>484</volume>
        <fpage>223</fpage>
        <lpage>237</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neucom.2021.10.100</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Trabucco</surname>
            <given-names>JT</given-names>
          </name>
          <name>
            <surname>Raciti</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ringwald</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Marai</surname>
            <given-names>GE</given-names>
          </name>
          <name>
            <surname>Arighi</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Shatkay</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Utilizing image and caption information for biomedical document classification</article-title>
        <source>Bioinformatics</source>
        <year>2021</year>
        <volume>37</volume>
        <issue>Supplement-1</issue>
        <fpage>468</fpage>
        <lpage>476</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btab331</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Burns GA, Li X, Peng N. Building deep learning models for evidence classification from the open access biomedical literature. Database 2019; 2019.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dramé</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Mougin</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Diallo</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Large scale biomedical texts classification: a kNN and an ESA-based approaches</article-title>
        <source>Journal of biomedical semantics</source>
        <year>2016</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1186/s13326-016-0073-1</pub-id>
        <pub-id pub-id-type="pmid">26759709</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Simon</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Davidsen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Hansen</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Seymour</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Barnkob</surname>
            <given-names>MB</given-names>
          </name>
          <name>
            <surname>Olsen</surname>
            <given-names>LR</given-names>
          </name>
        </person-group>
        <article-title>BioReader: a text mining tool for performing classification of biomedical literature</article-title>
        <source>BMC Bioinform</source>
        <year>2019</year>
        <volume>19</volume>
        <fpage>165</fpage>
        <lpage>170</lpage>
        <pub-id pub-id-type="doi">10.1186/s12859-019-2607-x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Jiang X, M, Blake JA, Arighi C, Zhang G, Shatkay H. An effective biomedical document classification scheme in support of biocuration: addressing class imbalance. Database 2019; 2019.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bodenreider</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>The unified medical language system (UMLS): integrating biomedical terminology</article-title>
        <source>Nucleic Acids Res</source>
        <year>2004</year>
        <volume>32</volume>
        <issue>suppl–1</issue>
        <fpage>267</fpage>
        <lpage>270</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkh061</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>J-D</given-names>
          </name>
          <name>
            <surname>Ohta</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tsujii</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Corpus annotation for mining biomedical events from literature</article-title>
        <source>BMC Bioinform</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>1</fpage>
        <lpage>25</lpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-9-10</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bada</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Eckert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Evans</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Garcia</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Shipley</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Sitnikov</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Baumgartner</surname>
            <given-names>WA</given-names>
          </name>
          <name>
            <surname>Cohen</surname>
            <given-names>KB</given-names>
          </name>
          <name>
            <surname>Verspoor</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Blake</surname>
            <given-names>JA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Concept annotation in the CRAFT corpus</article-title>
        <source>BMC Bioinform</source>
        <year>2012</year>
        <volume>13</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>20</lpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-13-161</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Doğan</surname>
            <given-names>RI</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>NCBI disease corpus: a resource for disease name recognition and concept normalization</article-title>
        <source>J Biomed Inform</source>
        <year>2014</year>
        <volume>47</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2013.12.006</pub-id>
        <?supplied-pmid 24393765?>
        <pub-id pub-id-type="pmid">24393765</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Krallinger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rabal</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Leitner</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Vazquez</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Salgado</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lowe</surname>
            <given-names>DM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The CHEMDNER corpus of chemicals and drugs and its annotation principles</article-title>
        <source>J Cheminform</source>
        <year>2015</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="doi">10.1186/1758-2946-7-S1-S1</pub-id>
        <pub-id pub-id-type="pmid">25705261</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Ohta</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tateisi</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Tsujii</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>GENIA corpus—a semantically annotated corpus for bio-textmining</article-title>
        <source>Bioinformatics</source>
        <year>2003</year>
        <volume>19</volume>
        <issue>suppl-1</issue>
        <fpage>180</fpage>
        <lpage>182</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btg1023</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Li J, Sun Y, Johnson RJ, Sciaky D, Wei CH, Leaman R, Davis AP, Mattingly CJ, Wiegers TC, Lu Z. Biocreative V CDR task corpus: a resource for chemical disease relation extraction. Database 2016; 2016.</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Mohan S, Li D. Medmentions: a large biomedical corpus annotated with UMLS concepts. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1902.09476">arXiv:1902.09476</ext-link> (2019)</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Uzuner</surname>
            <given-names>Ö</given-names>
          </name>
          <name>
            <surname>South</surname>
            <given-names>BR</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>DuVall</surname>
            <given-names>SL</given-names>
          </name>
        </person-group>
        <article-title>2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text</article-title>
        <source>J Am Med Inform Assoc</source>
        <year>2011</year>
        <volume>18</volume>
        <issue>5</issue>
        <fpage>552</fpage>
        <lpage>556</lpage>
        <pub-id pub-id-type="doi">10.1136/amiajnl-2011-000203</pub-id>
        <?supplied-pmid 21685143?>
        <pub-id pub-id-type="pmid">21685143</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Johnson</surname>
            <given-names>AE</given-names>
          </name>
          <name>
            <surname>Pollard</surname>
            <given-names>TJ</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Lehman</surname>
            <given-names>LH</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ghassemi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Moody</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Szolovits</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Celi</surname>
            <given-names>LA</given-names>
          </name>
          <name>
            <surname>Mark</surname>
            <given-names>RG</given-names>
          </name>
        </person-group>
        <article-title>MIMIC-III, a freely accessible critical care database</article-title>
        <source>Sci Data</source>
        <year>2016</year>
        <volume>3</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1038/sdata.2016.35</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van Mulligen</surname>
            <given-names>EM</given-names>
          </name>
          <name>
            <surname>Fourrier-Reglat</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gurwitz</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Molokhia</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nieto</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Trifiro</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Kors</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Furlong</surname>
            <given-names>LI</given-names>
          </name>
        </person-group>
        <article-title>The EU-ADR corpus: annotated drugs, diseases, targets, and their relationships</article-title>
        <source>J Biomed Inform</source>
        <year>2012</year>
        <volume>45</volume>
        <issue>5</issue>
        <fpage>879</fpage>
        <lpage>884</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2012.04.004</pub-id>
        <?supplied-pmid 22554700?>
        <pub-id pub-id-type="pmid">22554700</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>HJ</given-names>
          </name>
          <name>
            <surname>Shim</surname>
            <given-names>SH</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>CoMAGC: a corpus with multi-faceted annotations of gene-cancer relations</article-title>
        <source>BMC Bioinform</source>
        <year>2013</year>
        <volume>14</volume>
        <fpage>323</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-14-323</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Giachelle</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Irrera</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Silvello</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>MedTAG: a portable and customizable annotation tool for biomedical documents</article-title>
        <source>BMC Med Inform Decis Mak</source>
        <year>2021</year>
        <volume>21</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>19</lpage>
        <pub-id pub-id-type="doi">10.1186/s12911-021-01706-4</pub-id>
        <pub-id pub-id-type="pmid">33388057</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Islamaj</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Kwon</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>TeamTat: a collaborative text annotation tool</article-title>
        <source>Nucleic Acids Res</source>
        <year>2020</year>
        <volume>48</volume>
        <issue>W1</issue>
        <fpage>5</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkaa333</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Cejuela JM, McQuilton P, Ponting L, Marygold SJ, Stefancsik R, Millburn GH, Rost B, Consortium F, et al. tagtog: interactive and text-mining-assisted annotation of gene mentions in PLOS full-text articles. Database 2014; 2014.</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Salgado</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Krallinger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Depaule</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Drula</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Tendulkar</surname>
            <given-names>AV</given-names>
          </name>
          <name>
            <surname>Leitner</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Valencia</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Marcelle</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>MyMiner: a web application for computer-assisted biocuration and text annotation</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>28</volume>
        <issue>17</issue>
        <fpage>2285</fpage>
        <lpage>2287</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts435</pub-id>
        <?supplied-pmid 22789588?>
        <pub-id pub-id-type="pmid">22789588</pub-id>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other">Kwon D, Kim S, Shin S, Wilbur WJ. BioQRator: a web-based interactive biomedical literature curating system. In: Proceedings of the fourth biocreative challenge evaluation workshop, vol 1; 2013. p. 241–46.</mixed-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kwon</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>ezTag: tagging biomedical concepts via interactive learning</article-title>
        <source>Nucleic Acids Res</source>
        <year>2018</year>
        <volume>46</volume>
        <issue>W1</issue>
        <fpage>523</fpage>
        <lpage>529</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gky428</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <mixed-citation publication-type="other">Stenetorp P, Pyysalo S, Topić G, Ohta T, Ananiadou S, Tsujii J. BRAT: a web-based tool for NLP-assisted text annotation. In: Proceedings of the demonstrations at the 13th conference of the European chapter of the Association for Computational Linguistics; 2012. p. 102–7.</mixed-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <mixed-citation publication-type="other">Giachelle F, Irrera O, Silvello G. DocTAG: a customizable annotation tool for ground truth creation. In: Advances in information retrieval: 44th European conference on IR Research, ECIR 2022, Stavanger, Norway, April 10–14, 2022, proceedings, part II. Springer; 2022. p. 288–93.</mixed-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <mixed-citation publication-type="other">Klie J-C, Bugert M, Boullosa B, de Castilho RE, Gurevych I. The inception platform: machine-assisted and knowledge-oriented interactive annotation. In: Proceedings of the 27th international conference on computational linguistics: system demonstrations; 2018. p. 5–9.</mixed-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <mixed-citation publication-type="other">Perry T. Lighttag: text annotation platform. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2109.02320">arXiv:2109.02320</ext-link> (2021)</mixed-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <mixed-citation publication-type="other">Muhie SY, Gurevych I, de Castilho RE, Biemann C. Webanno: a flexible, web-based and visually supported system for distributed annotations. In: Proceedings of the 51st annual meeting of the Association for Computational Linguistics: system demonstrations; 2013. p. 1–6.</mixed-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <mixed-citation publication-type="other">Jazayeri M. Some trends in web application development. In: Future of software engineering (FOSE’07). IEEE; 2007. p. 199–213.</mixed-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dobbie</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Strafford</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Pickrell</surname>
            <given-names>WO</given-names>
          </name>
          <name>
            <surname>Fonferko-Shadrach</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Akbari</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Thompson</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lacey</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Markup: a web-based annotation tool powered by active learning</article-title>
        <source>Front Digit Health</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.3389/fdgth.2021.598916</pub-id>
        <?supplied-pmid 34713086?>
        <pub-id pub-id-type="pmid">34713086</pub-id>
      </element-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Wen</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Moon</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Towards a better understanding of annotation tools for medical imaging: a survey</article-title>
        <source>Multimed Tools Appl</source>
        <year>2022</year>
        <volume>81</volume>
        <issue>18</issue>
        <fpage>25877</fpage>
        <lpage>25911</lpage>
        <pub-id pub-id-type="doi">10.1007/s11042-022-12100-1</pub-id>
        <pub-id pub-id-type="pmid">35350630</pub-id>
      </element-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Piad-Morffis</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gutiérrez</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Almeida-Cruz</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Munoz</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>A computational ecosystem to support ehealth knowledge discovery technologies in Spanish</article-title>
        <source>J Biomed Inform</source>
        <year>2020</year>
        <volume>109</volume>
        <fpage>103517</fpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2020.103517</pub-id>
        <?supplied-pmid 32712157?>
        <pub-id pub-id-type="pmid">32712157</pub-id>
      </element-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reinanda</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Meij</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>de Rijke</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Knowledge graphs: an information retrieval perspective</article-title>
        <source>Found Trends® Inf Retr</source>
        <year>2020</year>
        <volume>14</volume>
        <issue>4</issue>
        <fpage>289</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="doi">10.1561/1500000063</pub-id>
      </element-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <mixed-citation publication-type="other">Lopez P. GROBID: combining automatic bibliographic data recognition and term extraction for scholarship publications. In: Research and advanced technology for digital libraries: 13th European conference, ECDL 2009, Corfu, Greece, September 27–October 2, 2009. Proceedings 2009, vol 13. Springer. p. 473–4.</mixed-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>French</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>McInnes</surname>
            <given-names>BT</given-names>
          </name>
        </person-group>
        <article-title>An overview of biomedical entity linking throughout the years</article-title>
        <source>J Biomed Inform</source>
        <year>2023</year>
        <volume>137</volume>
        <fpage>104252</fpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2022.104252</pub-id>
        <?supplied-pmid 36464228?>
        <pub-id pub-id-type="pmid">36464228</pub-id>
      </element-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sevgili</surname>
            <given-names>Ö</given-names>
          </name>
          <name>
            <surname>Shelmanov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Arkhipov</surname>
            <given-names>MY</given-names>
          </name>
          <name>
            <surname>Panchenko</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Biemann</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Neural entity linking: a survey of models based on deep learning</article-title>
        <source>Semant Web</source>
        <year>2022</year>
        <volume>13</volume>
        <issue>3</issue>
        <fpage>527</fpage>
        <lpage>570</lpage>
        <pub-id pub-id-type="doi">10.3233/SW-222986</pub-id>
      </element-citation>
    </ref>
    <ref id="CR67">
      <label>67.</label>
      <mixed-citation publication-type="other">Aydar M, Bozal O, Özbay F. Neural relation extraction: a survey. CoRR <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2007.04247">arXiv: 2007.04247</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR68">
      <label>68.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Smirnova</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cudré-Mauroux</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Relation extraction using distant supervision: a survey</article-title>
        <source>ACM Comput Surv</source>
        <year>2019</year>
        <volume>51</volume>
        <issue>5</issue>
        <fpage>106</fpage>
        <lpage>110635</lpage>
        <pub-id pub-id-type="doi">10.1145/3241741</pub-id>
      </element-citation>
    </ref>
    <ref id="CR69">
      <label>69.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Kao</surname>
            <given-names>HY</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>PubTator: a web-based text mining tool for assisting biocuration</article-title>
        <source>Nucleic Acids Res</source>
        <year>2013</year>
        <volume>41</volume>
        <issue>Webserver-Issue</issue>
        <fpage>518</fpage>
        <lpage>522</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkt441</pub-id>
        <pub-id pub-id-type="pmid">23125361</pub-id>
      </element-citation>
    </ref>
    <ref id="CR70">
      <label>70.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Beyond accuracy: creating interoperable and scalable text-mining web services</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <issue>12</issue>
        <fpage>1907</fpage>
        <lpage>1910</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btv760</pub-id>
        <?supplied-pmid 26883486?>
        <pub-id pub-id-type="pmid">26883486</pub-id>
      </element-citation>
    </ref>
    <ref id="CR71">
      <label>71.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Allot</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>PubTator central: automated concept annotation for biomedical full text articles</article-title>
        <source>Nucleic Acids Res</source>
        <year>2019</year>
        <volume>47</volume>
        <issue>Webserver-Issue</issue>
        <fpage>587</fpage>
        <lpage>593</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkz389</pub-id>
      </element-citation>
    </ref>
    <ref id="CR72">
      <label>72.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maglott</surname>
            <given-names>DR</given-names>
          </name>
          <name>
            <surname>Ostell</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Pruitt</surname>
            <given-names>KD</given-names>
          </name>
          <name>
            <surname>Tatusova</surname>
            <given-names>TA</given-names>
          </name>
        </person-group>
        <article-title>Entrez gene: gene-centered information at NCBI</article-title>
        <source>Nucleic Acids Res</source>
        <year>2011</year>
        <volume>39</volume>
        <issue>Database-Issue</issue>
        <fpage>52</fpage>
        <lpage>57</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkq1237</pub-id>
      </element-citation>
    </ref>
    <ref id="CR73">
      <label>73.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lipscomb</surname>
            <given-names>CE</given-names>
          </name>
        </person-group>
        <article-title>Medical subject headings (MeSH)</article-title>
        <source>Bull Med Libr Assoc</source>
        <year>2000</year>
        <volume>88</volume>
        <issue>3</issue>
        <fpage>265</fpage>
        <?supplied-pmid 10928714?>
        <pub-id pub-id-type="pmid">10928714</pub-id>
      </element-citation>
    </ref>
    <ref id="CR74">
      <label>74.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Demner-Fushman</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Rogers</surname>
            <given-names>WJ</given-names>
          </name>
          <name>
            <surname>Aronson</surname>
            <given-names>AR</given-names>
          </name>
        </person-group>
        <article-title>MetaMap lite: an evaluation of a new Java implementation of MetaMap</article-title>
        <source>J Am Med Inform Assoc</source>
        <year>2017</year>
        <volume>24</volume>
        <issue>4</issue>
        <fpage>841</fpage>
        <lpage>844</lpage>
        <pub-id pub-id-type="doi">10.1093/jamia/ocw177</pub-id>
        <?supplied-pmid 28130331?>
        <pub-id pub-id-type="pmid">28130331</pub-id>
      </element-citation>
    </ref>
    <ref id="CR75">
      <label>75.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dugger</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Platt</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Goldstein</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Drug development in the era of precision medicine</article-title>
        <source>Nat Rev Drug Discov</source>
        <year>2018</year>
        <volume>17</volume>
        <fpage>183</fpage>
        <lpage>196</lpage>
        <pub-id pub-id-type="doi">10.1038/nrd.2017.226</pub-id>
        <?supplied-pmid 29217837?>
        <pub-id pub-id-type="pmid">29217837</pub-id>
      </element-citation>
    </ref>
    <ref id="CR76">
      <label>76.</label>
      <mixed-citation publication-type="other">Surdeanu M, Tibshirani J, Nallapati R, Manning CD. Multi-instance multi-label learning for relation extraction. In: Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning, EMNLP-CoNLL 2012, July 12–14, 2012, Jeju Island, Korea; 2012. p. 455–65.</mixed-citation>
    </ref>
    <ref id="CR77">
      <label>77.</label>
      <mixed-citation publication-type="other">Riedel S, Yao L, McCallum A. Modeling relations and their mentions without labeled text. In: Proceedings of machine learning and knowledge discovery in databases, European conference, ECML PKDD 2010, Barcelona, Spain, September 20–24, 2010. LNCS, vol 6323; 2010. p. 148–63.</mixed-citation>
    </ref>
    <ref id="CR78">
      <label>78.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marchesin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Silvello</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>TBGA: a large-scale gene-disease association dataset for biomedical relation extraction</article-title>
        <source>BMC Bioinform</source>
        <year>2022</year>
        <volume>23</volume>
        <issue>1</issue>
        <fpage>111</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-022-04646-6</pub-id>
      </element-citation>
    </ref>
    <ref id="CR79">
      <label>79.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Neary</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Identifying gene expression patterns associated with drug-specific survival in cancer patients</article-title>
        <source>Sci Rep</source>
        <year>2021</year>
        <volume>11</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1038/s41598-021-84211-y</pub-id>
        <pub-id pub-id-type="pmid">33414495</pub-id>
      </element-citation>
    </ref>
    <ref id="CR80">
      <label>80.</label>
      <mixed-citation publication-type="other">Liu F, Chen J, Jagannatha A, Yu H. Learning for biomedical information extraction: methodological review of recent advances. CoRR abs/1606.07993 (2016)</mixed-citation>
    </ref>
    <ref id="CR81">
      <label>81.</label>
      <mixed-citation publication-type="other">Krallinger M, Rabal O, Akhondi SA, Pérez MP, Santamaría J, Rodríguez GP, Tsatsaronis G, Intxaurrondo A, Lopez JA, Nandal UK, van Buel EM, Chandrasekhar A, Rodenburg M, Lægreid A, Doornenbal MA, Oyarzábal J, Lourenço A, Valencia A. Overview of the BioCreative VI chemical-protein interaction Track. In: Proceedings of the sixth biocreative challenge evaluation workshop; 2017.</mixed-citation>
    </ref>
    <ref id="CR82">
      <label>82.</label>
      <mixed-citation publication-type="other">Miranda A, Mehryary F, Luoma J, Pyysalo S, Valencia A, Krallinger M. Overview of DrugProt BioCreative VII track: quality evaluation and large scale text mining of drug-gene/protein relations. In: Proceedings of the seventh biocreative challenge evaluation workshop; 2021.</mixed-citation>
    </ref>
    <ref id="CR83">
      <label>83.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>HJ</given-names>
          </name>
          <name>
            <surname>Dang</surname>
            <given-names>TC</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>OncoSearch: cancer gene search engine with literature evidence</article-title>
        <source>Nucleic Acids Res</source>
        <year>2014</year>
        <volume>42</volume>
        <issue>Webserver-Issue</issue>
        <fpage>416</fpage>
        <lpage>421</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gku368</pub-id>
      </element-citation>
    </ref>
    <ref id="CR84">
      <label>84.</label>
      <mixed-citation publication-type="other">Beltagy I, Lo K, Cohan A. SciBERT: a pretrained language model for scientific text. In: Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3–7, 2019; 2019. p. 3613–18.</mixed-citation>
    </ref>
    <ref id="CR85">
      <label>85.</label>
      <mixed-citation publication-type="other">Devlin J, Chang MW, Lee K, Toutanova K. BERT: pre-training of deep bidirectional transformers for language understanding. In: Proceedings of the 2019 conference of the North American Chapter of the Association for Computational Linguistics: human language technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2–7, 2019, volume 1 (long and short papers); 2019. p. 4171–86.</mixed-citation>
    </ref>
    <ref id="CR86">
      <label>86.</label>
      <mixed-citation publication-type="other">Ammar W, Groeneveld D, Bhagavatula C, Beltagy I, Crawford M, Downey D, Dunkelberger J, Elgohary A, Feldman S, Ha V, Kinney R, Kohlmeier S, Lo K, Murray T, Ooi HH, Peters ME, Power J, Skjonsberg S, Wang LL, Wilhelm C, Yuan Z, van Zuylen M, Etzioni O. Construction of the literature graph in semantic scholar. In: Proceedings of the 2018 conference of the North American Chapter of the Association for Computational Linguistics: human language technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1–6, 2018, volume 3 (industry papers); 2018. p. 84–91.</mixed-citation>
    </ref>
    <ref id="CR87">
      <label>87.</label>
      <mixed-citation publication-type="other">Giachelle F, Marchesin S, Silvello G, Alonso O. Searching for reliable facts over a medical knowledge base. In: Proceedings of the 46th international ACM SIGIR conference on research and development in information retrieval, SIGIR 2023, Taipei, Taiwan, July 23–27, 2023; 2023. p. 23–7. 10.1145/3539618.3591822.</mixed-citation>
    </ref>
    <ref id="CR88">
      <label>88.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marchesin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Menotti</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Giachelle</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Silvello</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Alonso</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>Building a large gene expression-cancer knowledge base with limited human annotations</article-title>
        <source>Database J Biol Databases Curation</source>
        <year>2023</year>
        <pub-id pub-id-type="doi">10.1093/DATABASE/BAAD061</pub-id>
      </element-citation>
    </ref>
    <ref id="CR89">
      <label>89.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fleiss</surname>
            <given-names>JL</given-names>
          </name>
        </person-group>
        <article-title>Measuring nominal scale agreement among many raters</article-title>
        <source>Psychol Bull</source>
        <year>1971</year>
        <volume>76</volume>
        <issue>5</issue>
        <fpage>378</fpage>
        <pub-id pub-id-type="doi">10.1037/h0031619</pub-id>
      </element-citation>
    </ref>
    <ref id="CR90">
      <label>90.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McHugh</surname>
            <given-names>ML</given-names>
          </name>
        </person-group>
        <article-title>Interrater reliability: the kappa statistic</article-title>
        <source>Biochem Med</source>
        <year>2012</year>
        <volume>22</volume>
        <issue>3</issue>
        <fpage>276</fpage>
        <lpage>282</lpage>
        <pub-id pub-id-type="doi">10.11613/BM.2012.031</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
